{"author":{"name":null,"type":"card","url":"https://ohmyweekly.github.io"},"children":[{"content":{"html":"\u003ch1 id=\"hive-读写\"\u003eHive 读写\u003c/h1\u003e\n\u003cp\u003e使用 HiveCatalog 和 Flink 与 Hive 的连接器，Flink 可以从 Hive 数据中读取和写入数据，作为 Hive 批处理引擎的替代。请务必按照说明在你的应用中加入正确的\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#depedencies\"\u003e依赖关系\u003c/a\u003e。同时请注意，Hive 连接器只适用于 blink planner。\u003c/p\u003e\n\u003ch2 id=\"从-hive-读取数据\"\u003e从 Hive 读取数据\u003c/h2\u003e\n\u003cp\u003e假设 Hive 在其默认的数据库中包含一个名为 people 的单表，该表包含多条记录。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003ehive\u0026gt; show databases\u003cspan class=\"p\"\u003e;\u003c/span\u003e\nOK\ndefault\nTime taken: 0.841 seconds, Fetched: \u003cspan class=\"m\"\u003e1\u003c/span\u003e row\u003cspan class=\"o\"\u003e(\u003c/span\u003es\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\nhive\u0026gt; show tables\u003cspan class=\"p\"\u003e;\u003c/span\u003e\nOK\nTime taken: 0.087 seconds\n\nhive\u0026gt; CREATE TABLE mytable\u003cspan class=\"o\"\u003e(\u003c/span\u003ename string, value double\u003cspan class=\"o\"\u003e)\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\nOK\nTime taken: 0.127 seconds\n\nhive\u0026gt; SELECT * FROM mytable\u003cspan class=\"p\"\u003e;\u003c/span\u003e\nOK\nTom   4.72\nJohn  8.0\nTom   24.2\nBob   3.14\nBob   4.72\nTom   34.9\nMary  4.79\nTiff  2.72\nBill  4.33\nMary  77.7\nTime taken: 0.097 seconds, Fetched: \u003cspan class=\"m\"\u003e10\u003c/span\u003e row\u003cspan class=\"o\"\u003e(\u003c/span\u003es\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e数据准备好后，你可以\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#connecting-to-hive\"\u003e连接到现有的 Hive 安装\u003c/a\u003e并开始查询。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003eFlink SQL\u0026gt; show catalogs\u003cspan class=\"p\"\u003e;\u003c/span\u003e\nmyhive\ndefault_catalog\n\n\u003cspan class=\"c1\"\u003e# ------ Set the current catalog to be \u0026#39;myhive\u0026#39; catalog if you haven\u0026#39;t set it in the yaml file ------\u003c/span\u003e\n\nFlink SQL\u0026gt; use catalog myhive\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# ------ See all registered database in catalog \u0026#39;mytable\u0026#39; ------\u003c/span\u003e\n\nFlink SQL\u0026gt; show databases\u003cspan class=\"p\"\u003e;\u003c/span\u003e\ndefault\n\n\u003cspan class=\"c1\"\u003e# ------ See the previously registered table \u0026#39;mytable\u0026#39; ------\u003c/span\u003e\n\nFlink SQL\u0026gt; show tables\u003cspan class=\"p\"\u003e;\u003c/span\u003e\nmytable\n\n\u003cspan class=\"c1\"\u003e# ------ The table schema that Flink sees is the same that we created in Hive, two columns - name as string and value as double ------ \u003c/span\u003e\nFlink SQL\u0026gt; describe mytable\u003cspan class=\"p\"\u003e;\u003c/span\u003e\nroot\n \u003cspan class=\"p\"\u003e|\u003c/span\u003e-- name: name\n \u003cspan class=\"p\"\u003e|\u003c/span\u003e-- type: STRING\n \u003cspan class=\"p\"\u003e|\u003c/span\u003e-- name: value\n \u003cspan class=\"p\"\u003e|\u003c/span\u003e-- type: DOUBLE\n\n\u003cspan class=\"c1\"\u003e# ------ Select from hive table or hive view ------ \u003c/span\u003e\nFlink SQL\u0026gt; SELECT * FROM mytable\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n   name      value\n__________ __________\n\n    Tom      4.72\n    John     8.0\n    Tom      24.2\n    Bob      3.14\n    Bob      4.72\n    Tom      34.9\n    Mary     4.79\n    Tiff     2.72\n    Bill     4.33\n    Mary     77.7\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"查询-hive-视图\"\u003e查询 Hive 视图\u003c/h3\u003e\n\u003cp\u003e如果你需要查询 Hive 视图，请注意。\u003c/p\u003e\n\u003cp\u003e在查询该目录中的视图之前，必须先使用 Hive 目录作为当前目录。可以通过 Table API 中的 \u003ccode\u003etableEnv.useCatalog(...)\u003c/code\u003e 或者 SQL Client 中的 USE CATALOG \u0026hellip;来实现。\nHive 和 Flink SQL 有不同的语法，例如，不同的保留关键字和字元。请确保视图的查询与 Flink 语法兼容。\u003c/p\u003e\n\u003ch2 id=\"写入-hive\"\u003e写入 Hive\u003c/h2\u003e\n\u003cp\u003e同样，也可以使用 INSERT 子句将数据写入 hive 中。\u003c/p\u003e\n\u003cp\u003e考虑有一个名为 \u0026ldquo;mytable \u0026ldquo;的示例表，表中有两列：name 和 age，类型为 string 和 int。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"c1\"\u003e# ------ INSERT INTO will append to the table or partition, keeping the existing data intact ------ \u003c/span\u003e\nFlink SQL\u0026gt; INSERT INTO mytable SELECT \u003cspan class=\"s1\"\u003e\u0026#39;Tom\u0026#39;\u003c/span\u003e, 25\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# ------ INSERT OVERWRITE will overwrite any existing data in the table or partition ------ \u003c/span\u003e\nFlink SQL\u0026gt; INSERT OVERWRITE mytable SELECT \u003cspan class=\"s1\"\u003e\u0026#39;Tom\u0026#39;\u003c/span\u003e, 25\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e我们也支持分区表，考虑有一个名为 myparttable 的分区表，有四列：name、age、my_type 和 my_date，在 type 中\u0026hellip;\u003ccode\u003emy_type\u003c/code\u003e 和 \u003ccode\u003emy_date\u003c/code\u003e 是分区键。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"c1\"\u003e# ------ Insert with static partition ------ \u003c/span\u003e\nFlink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION \u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003emy_type\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;type_1\u0026#39;\u003c/span\u003e, \u003cspan class=\"nv\"\u003emy_date\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;2019-08-08\u0026#39;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e SELECT \u003cspan class=\"s1\"\u003e\u0026#39;Tom\u0026#39;\u003c/span\u003e, 25\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# ------ Insert with dynamic partition ------ \u003c/span\u003e\nFlink SQL\u0026gt; INSERT OVERWRITE myparttable SELECT \u003cspan class=\"s1\"\u003e\u0026#39;Tom\u0026#39;\u003c/span\u003e, 25, \u003cspan class=\"s1\"\u003e\u0026#39;type_1\u0026#39;\u003c/span\u003e, \u003cspan class=\"s1\"\u003e\u0026#39;2019-08-08\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# ------ Insert with static(my_type) and dynamic(my_date) partition ------ \u003c/span\u003e\nFlink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION \u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003emy_type\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;type_1\u0026#39;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e SELECT \u003cspan class=\"s1\"\u003e\u0026#39;Tom\u0026#39;\u003c/span\u003e, 25, \u003cspan class=\"s1\"\u003e\u0026#39;2019-08-08\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"格式\"\u003e格式\u003c/h2\u003e\n\u003cp\u003e我们测试了以下表格存储格式：文本、csv、SequenceFile、ORC 和 Parquet。\u003c/p\u003e\n\u003ch2 id=\"优化\"\u003e优化\u003c/h2\u003e\n\u003ch3 id=\"分区修剪\"\u003e分区修剪\u003c/h3\u003e\n\u003cp\u003eFlink 使用分区修剪作为一种性能优化，以限制 Flink 在查询 Hive 表时读取的文件和分区的数量。当你的数据被分区后，当查询符合某些过滤条件时，Flink 只会读取 Hive 表中的分区子集。\u003c/p\u003e\n\u003ch3 id=\"投影下推\"\u003e投影下推\u003c/h3\u003e\n\u003cp\u003eFlink 利用投影下推，通过从表扫描中省略不必要的字段，最大限度地减少 Flink 和 Hive 表之间的数据传输。\u003c/p\u003e\n\u003cp\u003e当一个表包含许多列时，它尤其有利。\u003c/p\u003e\n\u003ch3 id=\"限制下推\"\u003e限制下推\u003c/h3\u003e\n\u003cp\u003e对于带有 LIMIT 子句的查询，Flink 会尽可能地限制输出记录的数量，以减少跨网络传输的数据量。\u003c/p\u003e\n\u003ch3 id=\"读取时的向量优化\"\u003e读取时的向量优化\u003c/h3\u003e\n\u003cp\u003e当满足以下条件时，会自动使用优化功能。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e格式： ORC 或 Parquet。\u003c/li\u003e\n\u003cli\u003e没有复杂数据类型的列，如 hive 类型: List, Map, Struct, Union。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这个功能默认是开启的。如果出现问题，可以使用这个配置选项来关闭 Vectorized Optimization。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etable.exec.hive.fallback-mapred-reader=true\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"source-并行性推断\"\u003eSource 并行性推断\u003c/h3\u003e\n\u003cp\u003e默认情况下，Flink 根据分割次数来推断 hive 源的并行度，分割次数是根据文件的数量和文件中的块数来推断的。\u003c/p\u003e\n\u003cp\u003eFlink 允许你灵活配置并行度推断的策略。你可以在 TableConfig 中配置以下参数（注意，这些参数会影响作业的所有源）。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align:left\"\u003eKey\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eDefault\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eType\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003etable.exec.hive.infer-source-parallelism\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003etrue\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eBoolean\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e如果为真，则根据分割数来推断源的并行度，如果为假，则根据配置来设置源的并行度。如果为 false，则通过配置来设置源的并行度。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003etable.exec.hive.infer-source-parallelism.max\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e1000\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eInteger\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e设置源运算符的最大推断并行度。\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"路线图\"\u003e路线图\u003c/h2\u003e\n\u003cp\u003e我们正在规划并积极开发支持功能，如:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eACID 表\u003c/li\u003e\n\u003cli\u003e分桶表\u003c/li\u003e\n\u003cli\u003e更多格式\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e更多功能需求请联系社区 \u003ca href=\"https://flink.apache.org/community.html#mailing-lists\"\u003ehttps://flink.apache.org/community.html#mailing-lists\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e原文链接: \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_read_write.html\"\u003ehttps://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_read_write.html\u003c/a\u003e\u003c/p\u003e\n","text":"Hive 读写 使用 HiveCatalog 和 Flink 与 Hive 的连接器，Flink 可以从 Hive 数据中读取和写入数据，作为 Hive 批处理引擎的替代。请务必按照说明在你的应用中加入正确的依赖关系。同时请注意，Hive 连接器只适用于 blink planner。\n从 Hive 读取数据 假设 Hive 在其默认的数据库中包含一个名为 people 的单表，该表包含多条记录。\nhive\u0026gt; show databases; OK default Time taken: 0.841 seconds, Fetched: 1 row(s) hive\u0026gt; show tables; OK Time taken: 0.087 seconds hive\u0026gt; CREATE TABLE mytable(name string, value double); OK Time taken: 0.127 seconds hive\u0026gt; SELECT * FROM mytable; OK Tom 4.72 John 8.0 Tom 24.2 Bob 3.14 Bob 4.72 Tom 34.9 Mary 4.79 Tiff 2.72 Bill 4.33 Mary 77.7 Time taken: 0.097 seconds, Fetched: 10 row(s) 数据准备好后，你可以连接到现有的 Hive 安装并开始查询。\nFlink SQL\u0026gt; show catalogs; myhive default_catalog # ------ Set the current catalog to be \u0026#39;myhive\u0026#39; catalog if you haven\u0026#39;t set it in the yaml file ------ Flink SQL\u0026gt; use catalog myhive; # ------ See all registered database in catalog \u0026#39;mytable\u0026#39; ------ Flink SQL\u0026gt; show databases; default # ------ See the previously registered table \u0026#39;mytable\u0026#39; ------ Flink SQL\u0026gt; show tables; mytable # ------ The table schema that Flink sees is the same that we created in Hive, two columns - name as string and value as double ------  Flink SQL\u0026gt; describe mytable; root |-- name: name |-- type: STRING |-- name: value |-- type: DOUBLE # ------ Select from hive table or hive view ------  Flink SQL\u0026gt; SELECT * FROM mytable; name value __________ __________ Tom 4.72 John 8.0 Tom 24.2 Bob 3.14 Bob 4.72 Tom 34.9 Mary 4.79 Tiff 2.72 Bill 4.33 Mary 77.7 查询 Hive 视图 如果你需要查询 Hive 视图，请注意。\n在查询该目录中的视图之前，必须先使用 Hive 目录作为当前目录。可以通过 Table API 中的 tableEnv.useCatalog(...) 或者 SQL Client 中的 USE CATALOG \u0026hellip;来实现。 Hive 和 Flink SQL 有不同的语法，例如，不同的保留关键字和字元。请确保视图的查询与 Flink 语法兼容。\n写入 Hive 同样，也可以使用 INSERT 子句将数据写入 hive 中。\n考虑有一个名为 \u0026ldquo;mytable \u0026ldquo;的示例表，表中有两列：name 和 age，类型为 string 和 int。\n# ------ INSERT INTO will append to the table or partition, keeping the existing data intact ------  Flink SQL\u0026gt; INSERT INTO mytable SELECT \u0026#39;Tom\u0026#39;, 25; # ------ INSERT OVERWRITE will overwrite any existing data in the table or partition ------  Flink SQL\u0026gt; INSERT OVERWRITE mytable SELECT \u0026#39;Tom\u0026#39;, 25; 我们也支持分区表，考虑有一个名为 myparttable 的分区表，有四列：name、age、my_type 和 my_date，在 type 中\u0026hellip;my_type 和 my_date 是分区键。\n# ------ Insert with static partition ------  Flink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION (my_type=\u0026#39;type_1\u0026#39;, my_date=\u0026#39;2019-08-08\u0026#39;) SELECT \u0026#39;Tom\u0026#39;, 25; # ------ Insert with dynamic partition ------  Flink SQL\u0026gt; INSERT OVERWRITE myparttable SELECT \u0026#39;Tom\u0026#39;, 25, \u0026#39;type_1\u0026#39;, \u0026#39;2019-08-08\u0026#39;; # ------ Insert with static(my_type) and dynamic(my_date) partition ------  Flink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION (my_type=\u0026#39;type_1\u0026#39;) SELECT \u0026#39;Tom\u0026#39;, 25, \u0026#39;2019-08-08\u0026#39;; 格式 我们测试了以下表格存储格式：文本、csv、SequenceFile、ORC 和 Parquet。\n优化 分区修剪 Flink 使用分区修剪作为一种性能优化，以限制 Flink 在查询 Hive 表时读取的文件和分区的数量。当你的数据被分区后，当查询符合某些过滤条件时，Flink 只会读取 Hive 表中的分区子集。\n投影下推 Flink 利用投影下推，通过从表扫描中省略不必要的字段，最大限度地减少 Flink 和 Hive 表之间的数据传输。\n当一个表包含许多列时，它尤其有利。\n限制下推 对于带有 LIMIT 子句的查询，Flink 会尽可能地限制输出记录的数量，以减少跨网络传输的数据量。\n读取时的向量优化 当满足以下条件时，会自动使用优化功能。\n 格式： ORC 或 Parquet。 没有复杂数据类型的列，如 hive 类型: List, Map, Struct, Union。  这个功能默认是开启的。如果出现问题，可以使用这个配置选项来关闭 Vectorized Optimization。\ntable.exec.hive.fallback-mapred-reader=true Source 并行性推断 默认情况下，Flink 根据分割次数来推断 hive 源的并行度，分割次数是根据文件的数量和文件中的块数来推断的。\nFlink 允许你灵活配置并行度推断的策略。你可以在 TableConfig 中配置以下参数（注意，这些参数会影响作业的所有源）。\n   Key Default Type Description     table.exec.hive.infer-source-parallelism true Boolean 如果为真，则根据分割数来推断源的并行度，如果为假，则根据配置来设置源的并行度。如果为 false，则通过配置来设置源的并行度。   table.exec.hive.infer-source-parallelism.max 1000 Integer 设置源运算符的最大推断并行度。    路线图 我们正在规划并积极开发支持功能，如:\n ACID 表 分桶表 更多格式  更多功能需求请联系社区 https://flink.apache.org/community.html#mailing-lists\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_read_write.html\n"},"name":"Hive Read and Write","published":"2020-08-25T00:00:00+08:00","summary":"Hive Read and Write","type":"entry","url":"https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/"},{"content":{"html":"\u003ch1 id=\"hive-流\"\u003eHive 流\u003c/h1\u003e\n\u003cp\u003e一个典型的 hive 作业是周期性地安排执行的，所以会有较大的延迟。\u003c/p\u003e\n\u003cp\u003eFlink 支持以流式的形式写入、读取和加入 hive 表。\u003c/p\u003e\n\u003cp\u003e流式数据有三种类型。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将流式数据写入 Hive 表。\u003c/li\u003e\n\u003cli\u003e以流的形式增量读取 Hive 表。\u003c/li\u003e\n\u003cli\u003e流式表使用 \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table\"\u003eTemporal 表\u003c/a\u003e连接 Hive 表。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"流式写入\"\u003e流式写入\u003c/h2\u003e\n\u003cp\u003eHive 表支持流式写入，基于 \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#streaming-sink\"\u003eFilesystem Streaming Sink\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003eHive Streaming Sink 重用 Filesystem Streaming Sink，将 Hadoop OutputFormat/RecordWriter 整合到流式写入。Hadoop RecordWriters 是 Bulk-encoded Formats，Bulk Formats 在每个检查点上滚动文件。\u003c/p\u003e\n\u003cp\u003e默认情况下，现在只有重命名提交者，这意味着 S3 文件系统不能支持精确的 once，如果你想在 S3 文件系统中使用 Hive 流媒体汇，你可以在 TableConfig 中把以下参数配置为 false，以使用 Flink 原生写入器（只对 parquet 和 orc 有效）（注意这些参数会影响所有作业的汇）。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align:left\"\u003eKey\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eDefault\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eType\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003etable.exec.hive.fallback-mapred-writer\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003etrue\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eBoolean\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e如果是假的，用 flink native writer 写 parquet 和 orc 文件；如果是真的，用 hadoop mapred record writer 写 parquet 和 orc 文件。\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e下面展示了如何使用流接收器写一个流式查询，将数据从 Kafka 写到一个有 partition-commit 的 Hive 表中，并运行一个批处理查询将这些数据读回。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"k\"\u003etable\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003esql\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003edialect\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ehive\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003cspan class=\"k\"\u003eCREATE\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"n\"\u003ehive_table\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\n  \u003cspan class=\"n\"\u003euser_id\u003c/span\u003e \u003cspan class=\"n\"\u003eSTRING\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eorder_amount\u003c/span\u003e \u003cspan class=\"n\"\u003eDOUBLE\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"n\"\u003ePARTITIONED\u003c/span\u003e \u003cspan class=\"k\"\u003eBY\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edt\u003c/span\u003e \u003cspan class=\"n\"\u003eSTRING\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ehr\u003c/span\u003e \u003cspan class=\"n\"\u003eSTRING\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"n\"\u003eSTORED\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003eparquet\u003c/span\u003e \u003cspan class=\"n\"\u003eTBLPROPERTIES\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\n  \u003cspan class=\"s1\"\u003e\u0026#39;partition.time-extractor.timestamp-pattern\u0026#39;\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;$dt $hr:00:00\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"s1\"\u003e\u0026#39;sink.partition-commit.trigger\u0026#39;\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;partition-time\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"s1\"\u003e\u0026#39;sink.partition-commit.delay\u0026#39;\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;1 h\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"s1\"\u003e\u0026#39;sink.partition-commit.policy.kind\u0026#39;\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;metastore,success-file\u0026#39;\u003c/span\u003e\n\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\n\u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"k\"\u003etable\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003esql\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003edialect\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"k\"\u003edefault\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003cspan class=\"k\"\u003eCREATE\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"n\"\u003ekafka_table\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\n  \u003cspan class=\"n\"\u003euser_id\u003c/span\u003e \u003cspan class=\"n\"\u003eSTRING\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eorder_amount\u003c/span\u003e \u003cspan class=\"n\"\u003eDOUBLE\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"n\"\u003elog_ts\u003c/span\u003e \u003cspan class=\"k\"\u003eTIMESTAMP\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eWATERMARK\u003c/span\u003e \u003cspan class=\"k\"\u003eFOR\u003c/span\u003e \u003cspan class=\"n\"\u003elog_ts\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003elog_ts\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"nb\"\u003eINTERVAL\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;5\u0026#39;\u003c/span\u003e \u003cspan class=\"k\"\u003eSECOND\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eWITH\u003c/span\u003e \u003cspan class=\"p\"\u003e(...);\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e-- streaming sql, insert into hive table\n\u003c/span\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"k\"\u003eINSERT\u003c/span\u003e \u003cspan class=\"k\"\u003eINTO\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"n\"\u003ehive_table\u003c/span\u003e \u003cspan class=\"k\"\u003eSELECT\u003c/span\u003e \u003cspan class=\"n\"\u003euser_id\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eorder_amount\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eDATE_FORMAT\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elog_ts\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;yyyy-MM-dd\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"n\"\u003eDATE_FORMAT\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elog_ts\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;HH\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eFROM\u003c/span\u003e \u003cspan class=\"n\"\u003ekafka_table\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e-- batch sql, select with partition pruning\n\u003c/span\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"k\"\u003eSELECT\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"k\"\u003eFROM\u003c/span\u003e \u003cspan class=\"n\"\u003ehive_table\u003c/span\u003e \u003cspan class=\"k\"\u003eWHERE\u003c/span\u003e \u003cspan class=\"n\"\u003edt\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;2020-05-20\u0026#39;\u003c/span\u003e \u003cspan class=\"k\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ehr\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;12\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"流式读取\"\u003e流式读取\u003c/h2\u003e\n\u003cp\u003e为了提高 hive 读取的实时性，Flink 支持实时 Hive 表流读取。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e分区表，监控分区的生成，并逐步读取新分区。\u003c/li\u003e\n\u003cli\u003e非分区表，监控文件夹中新文件的生成，并增量读取新文件。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e甚至可以采用 10 分钟级别的分区策略，利用 Flink 的 Hive 流式读取和 Hive 流式写入，大大提高 Hive 数据仓库的实时性能，达到准实时分钟级别。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align:left\"\u003eKey\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eDefault\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eType\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003estreaming-source.enable\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003efalse\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eBoolean\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e是否启用流媒体源。注意：请确保每个分区/文件都是以原子方式写入，否则读者可能会得到不完整的数据。请确保每个分区/文件都应该以原子方式写入，否则读者可能会得到不完整的数据。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003estreaming-source.monitor-interval\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e1 m\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eDuration\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e连续监控分区/文件的时间间隔。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003estreaming-source.consume-order\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003ecreate-time\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eString\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e流源的消耗顺序，支持 create-time 和 partition-time。create-time 比较的是分区/文件的创建时间，这不是 Hive metaStore 中的分区创建时间，而是文件系统中的文件夹/文件修改时间；partition-time 比较的是分区名称所代表的时间，如果分区文件夹以某种方式得到更新，比如在文件夹中添加新文件，就会影响数据的消耗方式。对于非分区表，这个值应该一直是 \u0026ldquo;创建时间\u0026rdquo;。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003estreaming-source.consume-start-offset\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e1970-00-00\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eString\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e流式消费的起始偏移量。如何解析和比较偏移量取决于你的顺序。对于创建时间和分区时间，应该是一个时间戳字符串（yyyy-[m]m-[d]d [hh:mm:ss]）。对于分区时间，将使用分区时间提取器从分区中提取时间。\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e注意:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e监控策略是现在扫描位置路径中的所有目录/文件。如果分区太多，会出现性能问题。\u003c/li\u003e\n\u003cli\u003e非分区的流式读取需要将每个文件原子地放入目标目录中。\u003c/li\u003e\n\u003cli\u003e分区的流式读取要求在 hive metastore 的视图中原子地添加每个分区。这意味着新添加到现有分区的数据不会被消耗掉。\u003c/li\u003e\n\u003cli\u003e流读取不支持 Flink DDL 中的水印语法。所以它不能用于窗口操作符。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e下面展示了如何增量读取 Hive 表。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eSELECT\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"k\"\u003eFROM\u003c/span\u003e \u003cspan class=\"n\"\u003ehive_table\u003c/span\u003e \u003cspan class=\"cm\"\u003e/*+ OPTIONS(\u0026#39;streaming-source.enable\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;streaming-source.consume-start-offset\u0026#39;=\u0026#39;2020-05-20\u0026#39;) */\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"hive-表作为临时表\"\u003eHive 表作为临时表\u003c/h2\u003e\n\u003cp\u003e您可以使用 Hive 表作为时态表，并将流式数据加入其中。请按照\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table\"\u003e示例\u003c/a\u003e来了解如何连接一个时态表。\u003c/p\u003e\n\u003cp\u003e在执行 join 时，Hive 表将被缓存在 TM 内存中，并在 Hive 表中查找来自流的每一条记录，以决定是否找到匹配。你不需要任何额外的设置就可以使用 Hive 表作为时态表。但可以选择用以下属性配置 Hive 表缓存的 TTL。缓存过期后，将再次扫描 Hive 表以加载最新的数据。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align:left\"\u003eKey\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eDefault\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eType\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003elookup.join.cache.ttl\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e60 min\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eDuration\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e在查找连接中构建表的缓存 TTL（例如 10 分钟）。默认情况下，TTL 为 60 分钟。\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e注意:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e每个加入子任务都需要保留自己的 Hive 表的缓存。请确保 Hive 表可以放入 TM 任务槽的内存中。\u003c/li\u003e\n\u003cli\u003e你应该为 lookup.join.cache.ttl 设置一个相对较大的值。如果你的 Hive 表需要太频繁的更新和重载，你可能会有性能问题。\u003c/li\u003e\n\u003cli\u003e目前，每当缓存需要刷新时，我们只是简单地加载整个 Hive 表。没有办法区分新数据和旧数据。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e原文链接: \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html\"\u003ehttps://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html\u003c/a\u003e\u003c/p\u003e\n","text":"Hive 流 一个典型的 hive 作业是周期性地安排执行的，所以会有较大的延迟。\nFlink 支持以流式的形式写入、读取和加入 hive 表。\n流式数据有三种类型。\n 将流式数据写入 Hive 表。 以流的形式增量读取 Hive 表。 流式表使用 Temporal 表连接 Hive 表。  流式写入 Hive 表支持流式写入，基于 Filesystem Streaming Sink。\nHive Streaming Sink 重用 Filesystem Streaming Sink，将 Hadoop OutputFormat/RecordWriter 整合到流式写入。Hadoop RecordWriters 是 Bulk-encoded Formats，Bulk Formats 在每个检查点上滚动文件。\n默认情况下，现在只有重命名提交者，这意味着 S3 文件系统不能支持精确的 once，如果你想在 S3 文件系统中使用 Hive 流媒体汇，你可以在 TableConfig 中把以下参数配置为 false，以使用 Flink 原生写入器（只对 parquet 和 orc 有效）（注意这些参数会影响所有作业的汇）。\n   Key Default Type Description     table.exec.hive.fallback-mapred-writer true Boolean 如果是假的，用 flink native writer 写 parquet 和 orc 文件；如果是真的，用 hadoop mapred record writer 写 parquet 和 orc 文件。    下面展示了如何使用流接收器写一个流式查询，将数据从 Kafka 写到一个有 partition-commit 的 Hive 表中，并运行一个批处理查询将这些数据读回。\nSET table.sql-dialect=hive; CREATE TABLE hive_table ( user_id STRING, order_amount DOUBLE ) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES ( \u0026#39;partition.time-extractor.timestamp-pattern\u0026#39;=\u0026#39;$dt $hr:00:00\u0026#39;, \u0026#39;sink.partition-commit.trigger\u0026#39;=\u0026#39;partition-time\u0026#39;, \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;metastore,success-file\u0026#39; ); SET table.sql-dialect=default; CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, log_ts TIMESTAMP(3), WATERMARK FOR log_ts AS log_ts - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (...); -- streaming sql, insert into hive table INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, \u0026#39;yyyy-MM-dd\u0026#39;), DATE_FORMAT(log_ts, \u0026#39;HH\u0026#39;) FROM kafka_table; -- batch sql, select with partition pruning SELECT * FROM hive_table WHERE dt=\u0026#39;2020-05-20\u0026#39; and hr=\u0026#39;12\u0026#39;; 流式读取 为了提高 hive 读取的实时性，Flink 支持实时 Hive 表流读取。\n 分区表，监控分区的生成，并逐步读取新分区。 非分区表，监控文件夹中新文件的生成，并增量读取新文件。  甚至可以采用 10 分钟级别的分区策略，利用 Flink 的 Hive 流式读取和 Hive 流式写入，大大提高 Hive 数据仓库的实时性能，达到准实时分钟级别。\n   Key Default Type Description     streaming-source.enable false Boolean 是否启用流媒体源。注意：请确保每个分区/文件都是以原子方式写入，否则读者可能会得到不完整的数据。请确保每个分区/文件都应该以原子方式写入，否则读者可能会得到不完整的数据。   streaming-source.monitor-interval 1 m Duration 连续监控分区/文件的时间间隔。   streaming-source.consume-order create-time String 流源的消耗顺序，支持 create-time 和 partition-time。create-time 比较的是分区/文件的创建时间，这不是 Hive metaStore 中的分区创建时间，而是文件系统中的文件夹/文件修改时间；partition-time 比较的是分区名称所代表的时间，如果分区文件夹以某种方式得到更新，比如在文件夹中添加新文件，就会影响数据的消耗方式。对于非分区表，这个值应该一直是 \u0026ldquo;创建时间\u0026rdquo;。   streaming-source.consume-start-offset 1970-00-00 String 流式消费的起始偏移量。如何解析和比较偏移量取决于你的顺序。对于创建时间和分区时间，应该是一个时间戳字符串（yyyy-[m]m-[d]d [hh:mm:ss]）。对于分区时间，将使用分区时间提取器从分区中提取时间。    注意:\n 监控策略是现在扫描位置路径中的所有目录/文件。如果分区太多，会出现性能问题。 非分区的流式读取需要将每个文件原子地放入目标目录中。 分区的流式读取要求在 hive metastore 的视图中原子地添加每个分区。这意味着新添加到现有分区的数据不会被消耗掉。 流读取不支持 Flink DDL 中的水印语法。所以它不能用于窗口操作符。  下面展示了如何增量读取 Hive 表。\nSELECT * FROM hive_table /*+ OPTIONS(\u0026#39;streaming-source.enable\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;streaming-source.consume-start-offset\u0026#39;=\u0026#39;2020-05-20\u0026#39;) */; Hive 表作为临时表 您可以使用 Hive 表作为时态表，并将流式数据加入其中。请按照示例来了解如何连接一个时态表。\n在执行 join 时，Hive 表将被缓存在 TM 内存中，并在 Hive 表中查找来自流的每一条记录，以决定是否找到匹配。你不需要任何额外的设置就可以使用 Hive 表作为时态表。但可以选择用以下属性配置 Hive 表缓存的 TTL。缓存过期后，将再次扫描 Hive 表以加载最新的数据。\n   Key Default Type Description     lookup.join.cache.ttl 60 min Duration 在查找连接中构建表的缓存 TTL（例如 10 分钟）。默认情况下，TTL 为 60 分钟。    注意:\n 每个加入子任务都需要保留自己的 Hive 表的缓存。请确保 Hive 表可以放入 TM 任务槽的内存中。 你应该为 lookup.join.cache.ttl 设置一个相对较大的值。如果你的 Hive 表需要太频繁的更新和重载，你可能会有性能问题。 目前，每当缓存需要刷新时，我们只是简单地加载整个 Hive 表。没有办法区分新数据和旧数据。  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html\n"},"name":"Hive Streaming","published":"2020-08-25T00:00:00+08:00","summary":"Hive Streaming","type":"entry","url":"https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/"},{"content":{"html":"\u003ch1 id=\"hive-函数\"\u003eHive 函数\u003c/h1\u003e\n\u003ch2 id=\"通过-hivemodule-使用-hive-内置功能\"\u003e通过 HiveModule 使用 Hive 内置功能\u003c/h2\u003e\n\u003cp\u003eHiveModule 将 Hive 内置函数作为 Flink 系统（内置）函数提供给 Flink SQL 和 Table API 用户。\u003c/p\u003e\n\u003cp\u003e具体信息请参考 \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/modules.html#hivemodule\"\u003eHiveModule\u003c/a\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eScala\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-scala\" data-lang=\"scala\"\u003e\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e            \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;myhive\u0026#34;\u003c/span\u003e\n\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003eversion\u003c/span\u003e         \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;2.3.4\u0026#34;\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003etableEnv\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eloadModue\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"o\"\u003e,\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nc\"\u003eHiveModule\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eversion\u003c/span\u003e\u003cspan class=\"o\"\u003e));\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eYAML\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan class=\"nt\"\u003emodules\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ecore\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"w\"\u003e     \u003c/span\u003e\u003cspan class=\"nt\"\u003etype\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ecore\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003emyhive\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"w\"\u003e     \u003c/span\u003e\u003cspan class=\"nt\"\u003etype\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ehive\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e注意，旧版本中的一些 Hive 内置功能存在\u003ca href=\"https://issues.apache.org/jira/browse/HIVE-16183\"\u003e线程安全问题\u003c/a\u003e。我们建议用户给自己的 Hive 打上补丁来修复它们。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"hive-用户定义的函数\"\u003eHive 用户定义的函数\u003c/h2\u003e\n\u003cp\u003e用户可以在 Flink 中使用他们现有的 Hive 用户定义函数。\u003c/p\u003e\n\u003cp\u003e支持的 UDF 类型包括:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUDF\u003c/li\u003e\n\u003cli\u003eGenericUDF\u003c/li\u003e\n\u003cli\u003eGenericUDTF\u003c/li\u003e\n\u003cli\u003eUDAF\u003c/li\u003e\n\u003cli\u003eGenericUDAFResolver2\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在查询规划和执行时，Hive 的 UDF 和 GenericUDF 会自动翻译成 Flink 的 ScalarFunction，Hive 的 GenericUDTF 会自动翻译成 Flink 的 TableFunction，Hive 的 UDAF 和 GenericUDAFResolver2 会翻译成 Flink 的 AggregateFunction。\u003c/p\u003e\n\u003cp\u003e要使用 Hive 的用户定义函数，用户必须做到:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e设置一个由 Hive Metastore 支持的 HiveCatalog 作为会话的当前目录，其中包含该函数。\u003c/li\u003e\n\u003cli\u003e在 Flink 的 classpath 中加入一个包含该函数的 jar。\u003c/li\u003e\n\u003cli\u003e使用 Blink 计划器。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"使用-hive-用户定义函数\"\u003e使用 Hive 用户定义函数\u003c/h2\u003e\n\u003cp\u003e假设我们在 Hive Metastore 中注册了以下 Hive 函数。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003cspan class=\"cm\"\u003e * Test simple udf. Registered under name \u0026#39;myudf\u0026#39;\n\u003c/span\u003e\u003cspan class=\"cm\"\u003e */\u003c/span\u003e\n\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eTestHiveSimpleUDF\u003c/span\u003e \u003cspan class=\"kd\"\u003eextends\u003c/span\u003e \u003cspan class=\"n\"\u003eUDF\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\n\t\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"n\"\u003eIntWritable\u003c/span\u003e \u003cspan class=\"nf\"\u003eevaluate\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eIntWritable\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"n\"\u003eIntWritable\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"o\"\u003e());\u003c/span\u003e\n\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\n\t\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"n\"\u003eText\u003c/span\u003e \u003cspan class=\"nf\"\u003eevaluate\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eText\u003c/span\u003e \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"n\"\u003eText\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003etoString\u003c/span\u003e\u003cspan class=\"o\"\u003e());\u003c/span\u003e\n\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003cspan class=\"cm\"\u003e * Test generic udf. Registered under name \u0026#39;mygenericudf\u0026#39;\n\u003c/span\u003e\u003cspan class=\"cm\"\u003e */\u003c/span\u003e\n\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eTestHiveGenericUDF\u003c/span\u003e \u003cspan class=\"kd\"\u003eextends\u003c/span\u003e \u003cspan class=\"n\"\u003eGenericUDF\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\n\t\u003cspan class=\"nd\"\u003e@Override\u003c/span\u003e\n\t\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"n\"\u003eObjectInspector\u003c/span\u003e \u003cspan class=\"nf\"\u003einitialize\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eObjectInspector\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e \u003cspan class=\"n\"\u003earguments\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"kd\"\u003ethrows\u003c/span\u003e \u003cspan class=\"n\"\u003eUDFArgumentException\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003echeckArgument\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003earguments\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elength\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\n\t\t\u003cspan class=\"n\"\u003echeckArgument\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003earguments\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003einstanceof\u003c/span\u003e \u003cspan class=\"n\"\u003eConstantObjectInspector\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003eObject\u003c/span\u003e \u003cspan class=\"n\"\u003econstant\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e((\u003c/span\u003e\u003cspan class=\"n\"\u003eConstantObjectInspector\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"n\"\u003earguments\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]).\u003c/span\u003e\u003cspan class=\"na\"\u003egetWritableConstantValue\u003c/span\u003e\u003cspan class=\"o\"\u003e();\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003echeckArgument\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003econstant\u003c/span\u003e \u003cspan class=\"k\"\u003einstanceof\u003c/span\u003e \u003cspan class=\"n\"\u003eIntWritable\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003echeckArgument\u003c/span\u003e\u003cspan class=\"o\"\u003e(((\u003c/span\u003e\u003cspan class=\"n\"\u003eIntWritable\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"n\"\u003econstant\u003c/span\u003e\u003cspan class=\"o\"\u003e).\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"o\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\n\t\t\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003earguments\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003einstanceof\u003c/span\u003e \u003cspan class=\"n\"\u003eIntObjectInspector\u003c/span\u003e \u003cspan class=\"o\"\u003e||\u003c/span\u003e\n\t\t\t\t\u003cspan class=\"n\"\u003earguments\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003einstanceof\u003c/span\u003e \u003cspan class=\"n\"\u003eStringObjectInspector\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\t\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003earguments\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"o\"\u003e];\u003c/span\u003e\n\t\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\t\t\u003cspan class=\"k\"\u003ethrow\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"n\"\u003eRuntimeException\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;Not support argument: \u0026#34;\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003earguments\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"o\"\u003e]);\u003c/span\u003e\n\t\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\n\t\u003cspan class=\"nd\"\u003e@Override\u003c/span\u003e\n\t\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"n\"\u003eObject\u003c/span\u003e \u003cspan class=\"nf\"\u003eevaluate\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eDeferredObject\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e \u003cspan class=\"n\"\u003earguments\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"kd\"\u003ethrows\u003c/span\u003e \u003cspan class=\"n\"\u003eHiveException\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003earguments\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"o\"\u003e].\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"o\"\u003e();\u003c/span\u003e\n\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\n\t\u003cspan class=\"nd\"\u003e@Override\u003c/span\u003e\n\t\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"n\"\u003eString\u003c/span\u003e \u003cspan class=\"nf\"\u003egetDisplayString\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e \u003cspan class=\"n\"\u003echildren\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;TestHiveGenericUDF\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e;\u003c/span\u003e\n\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003cspan class=\"cm\"\u003e * Test split udtf. Registered under name \u0026#39;mygenericudtf\u0026#39;\n\u003c/span\u003e\u003cspan class=\"cm\"\u003e */\u003c/span\u003e\n\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eTestHiveUDTF\u003c/span\u003e \u003cspan class=\"kd\"\u003eextends\u003c/span\u003e \u003cspan class=\"n\"\u003eGenericUDTF\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\n\t\u003cspan class=\"nd\"\u003e@Override\u003c/span\u003e\n\t\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"n\"\u003eStructObjectInspector\u003c/span\u003e \u003cspan class=\"nf\"\u003einitialize\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eObjectInspector\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e \u003cspan class=\"n\"\u003eargOIs\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"kd\"\u003ethrows\u003c/span\u003e \u003cspan class=\"n\"\u003eUDFArgumentException\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003echeckArgument\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eargOIs\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elength\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\n\t\t\u003cspan class=\"c1\"\u003e// TEST for constant arguments\n\u003c/span\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\t\t\u003cspan class=\"n\"\u003echeckArgument\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eargOIs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003einstanceof\u003c/span\u003e \u003cspan class=\"n\"\u003eConstantObjectInspector\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003eObject\u003c/span\u003e \u003cspan class=\"n\"\u003econstant\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e((\u003c/span\u003e\u003cspan class=\"n\"\u003eConstantObjectInspector\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"n\"\u003eargOIs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]).\u003c/span\u003e\u003cspan class=\"na\"\u003egetWritableConstantValue\u003c/span\u003e\u003cspan class=\"o\"\u003e();\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003echeckArgument\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003econstant\u003c/span\u003e \u003cspan class=\"k\"\u003einstanceof\u003c/span\u003e \u003cspan class=\"n\"\u003eIntWritable\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003echeckArgument\u003c/span\u003e\u003cspan class=\"o\"\u003e(((\u003c/span\u003e\u003cspan class=\"n\"\u003eIntWritable\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"n\"\u003econstant\u003c/span\u003e\u003cspan class=\"o\"\u003e).\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"o\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\n\t\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eObjectInspectorFactory\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003egetStandardStructObjectInspector\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\n\t\t\t\u003cspan class=\"n\"\u003eCollections\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003esingletonList\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;col1\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e),\u003c/span\u003e\n\t\t\t\u003cspan class=\"n\"\u003eCollections\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003esingletonList\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ePrimitiveObjectInspectorFactory\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ejavaStringObjectInspector\u003c/span\u003e\u003cspan class=\"o\"\u003e));\u003c/span\u003e\n\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\n\t\u003cspan class=\"nd\"\u003e@Override\u003c/span\u003e\n\t\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eObject\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e \u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"kd\"\u003ethrows\u003c/span\u003e \u003cspan class=\"n\"\u003eHiveException\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003eString\u003c/span\u003e \u003cspan class=\"n\"\u003estr\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e \u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"o\"\u003e];\u003c/span\u003e\n\t\t\u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e \u003cspan class=\"n\"\u003es\u003c/span\u003e \u003cspan class=\"o\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003estr\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003esplit\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;,\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e))\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\t\t\u003cspan class=\"n\"\u003eforward\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\t\t\t\u003cspan class=\"n\"\u003eforward\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\t\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\n\t\u003cspan class=\"nd\"\u003e@Override\u003c/span\u003e\n\t\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e \u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eclose\u003c/span\u003e\u003cspan class=\"o\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\t\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e从 Hive CLI 中，我们可以看到他们已经注册了。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003ehive\u0026gt; show functions\u003cspan class=\"p\"\u003e;\u003c/span\u003e\nOK\n......\nmygenericudf\nmyudf\nmyudtf\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e然后，用户可以在 SQL 中使用它们作为。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003eFlink SQL\u0026gt; \u003cspan class=\"k\"\u003eselect\u003c/span\u003e mygenericudf\u003cspan class=\"o\"\u003e(\u003c/span\u003emyudf\u003cspan class=\"o\"\u003e(\u003c/span\u003ename\u003cspan class=\"o\"\u003e)\u003c/span\u003e, 1\u003cspan class=\"o\"\u003e)\u003c/span\u003e as a, mygenericudf\u003cspan class=\"o\"\u003e(\u003c/span\u003emyudf\u003cspan class=\"o\"\u003e(\u003c/span\u003eage\u003cspan class=\"o\"\u003e)\u003c/span\u003e, 1\u003cspan class=\"o\"\u003e)\u003c/span\u003e as b, s from mysourcetable, lateral table\u003cspan class=\"o\"\u003e(\u003c/span\u003emyudtf\u003cspan class=\"o\"\u003e(\u003c/span\u003ename, 1\u003cspan class=\"o\"\u003e))\u003c/span\u003e as T\u003cspan class=\"o\"\u003e(\u003c/span\u003es\u003cspan class=\"o\"\u003e)\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e原文链接: \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_functions.html\"\u003ehttps://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_functions.html\u003c/a\u003e\u003c/p\u003e\n","text":"Hive 函数 通过 HiveModule 使用 Hive 内置功能 HiveModule 将 Hive 内置函数作为 Flink 系统（内置）函数提供给 Flink SQL 和 Table API 用户。\n具体信息请参考 HiveModule。\n Scala  val name = \u0026#34;myhive\u0026#34; val version = \u0026#34;2.3.4\u0026#34; tableEnv.loadModue(name, new HiveModule(version));  YAML  modules:- name:coretype:core- name:myhivetype:hive 注意，旧版本中的一些 Hive 内置功能存在线程安全问题。我们建议用户给自己的 Hive 打上补丁来修复它们。  Hive 用户定义的函数 用户可以在 Flink 中使用他们现有的 Hive 用户定义函数。\n支持的 UDF 类型包括:\n UDF GenericUDF GenericUDTF UDAF GenericUDAFResolver2  在查询规划和执行时，Hive 的 UDF 和 GenericUDF 会自动翻译成 Flink 的 ScalarFunction，Hive 的 GenericUDTF 会自动翻译成 Flink 的 TableFunction，Hive 的 UDAF 和 GenericUDAFResolver2 会翻译成 Flink 的 AggregateFunction。\n要使用 Hive 的用户定义函数，用户必须做到:\n 设置一个由 Hive Metastore 支持的 HiveCatalog 作为会话的当前目录，其中包含该函数。 在 Flink 的 classpath 中加入一个包含该函数的 jar。 使用 Blink 计划器。  使用 Hive 用户定义函数 假设我们在 Hive Metastore 中注册了以下 Hive 函数。\n/** * Test simple udf. Registered under name \u0026#39;myudf\u0026#39; */ public class TestHiveSimpleUDF extends UDF { public IntWritable evaluate(IntWritable i) { return new IntWritable(i.get()); } public Text evaluate(Text text) { return new Text(text.toString()); } } /** * Test generic udf. Registered under name \u0026#39;mygenericudf\u0026#39; */ public class TestHiveGenericUDF extends GenericUDF { @Override public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException { checkArgument(arguments.length == 2); checkArgument(arguments[1] instanceof ConstantObjectInspector); Object constant = ((ConstantObjectInspector) arguments[1]).getWritableConstantValue(); checkArgument(constant instanceof IntWritable); checkArgument(((IntWritable) constant).get() == 1); if (arguments[0] instanceof IntObjectInspector || arguments[0] instanceof StringObjectInspector) { return arguments[0]; } else { throw new RuntimeException(\u0026#34;Not support argument: \u0026#34; + arguments[0]); } } @Override public Object evaluate(DeferredObject[] arguments) throws HiveException { return arguments[0].get(); } @Override public String getDisplayString(String[] children) { return \u0026#34;TestHiveGenericUDF\u0026#34;; } } /** * Test split udtf. Registered under name \u0026#39;mygenericudtf\u0026#39; */ public class TestHiveUDTF extends GenericUDTF { @Override public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException { checkArgument(argOIs.length == 2); // TEST for constant arguments \tcheckArgument(argOIs[1] instanceof ConstantObjectInspector); Object constant = ((ConstantObjectInspector) argOIs[1]).getWritableConstantValue(); checkArgument(constant instanceof IntWritable); checkArgument(((IntWritable) constant).get() == 1); return ObjectInspectorFactory.getStandardStructObjectInspector( Collections.singletonList(\u0026#34;col1\u0026#34;), Collections.singletonList(PrimitiveObjectInspectorFactory.javaStringObjectInspector)); } @Override public void process(Object[] args) throws HiveException { String str = (String) args[0]; for (String s : str.split(\u0026#34;,\u0026#34;)) { forward(s); forward(s); } } @Override public void close() { } } 从 Hive CLI 中，我们可以看到他们已经注册了。\nhive\u0026gt; show functions; OK ...... mygenericudf myudf myudtf 然后，用户可以在 SQL 中使用它们作为。\nFlink SQL\u0026gt; select mygenericudf(myudf(name), 1) as a, mygenericudf(myudf(age), 1) as b, s from mysourcetable, lateral table(myudtf(name, 1)) as T(s); 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_functions.html\n"},"name":"Hive 函数","published":"2020-08-25T00:00:00+08:00","summary":"Hive Functions","type":"entry","url":"https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/"},{"content":{"html":"\u003ch1 id=\"hive-方言\"\u003eHive 方言\u003c/h1\u003e\n\u003cp\u003e从 1.11.0 开始，当使用 Hive 方言时，Flink 允许用户用 Hive 语法编写 SQL 语句。通过提供与 Hive 语法的兼容性，我们旨在提高与 Hive 的互操作性，减少用户为了执行不同的语句而需要在 Flink 和 Hive 之间切换的情况。\u003c/p\u003e\n\u003ch2 id=\"使用-hive-方言\"\u003e使用 Hive 方言\u003c/h2\u003e\n\u003cp\u003eFlink 目前支持两种 SQL 方言：默认和 Hive。在使用 Hive 语法编写之前，需要先切换到 Hive 方言。下面介绍如何通过 SQL Client 和 Table API 来设置方言。同时注意，你可以为你执行的每一条语句动态切换方言。不需要重新启动会话来使用不同的方言。\u003c/p\u003e\n\u003ch3 id=\"sql-客户端\"\u003eSQL 客户端\u003c/h3\u003e\n\u003cp\u003eSQL 方言可以通过 table.sql-dialect 属性来指定，因此你可以在你的 SQL 客户端的 yaml 文件的配置部分设置初始方言。因此，你可以在 SQL 客户端的 yaml 文件的配置部分设置要使用的初始方言。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan class=\"nt\"\u003eexecution\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003eplanner\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eblink\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003etype\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ebatch\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003eresult-mode\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003etable\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"nt\"\u003econfiguration\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003etable.sql-dialect\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ehive\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e你也可以在 SQL 客户端启动后设置方言。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003eFlink SQL\u0026gt; \u003cspan class=\"nb\"\u003eset\u003c/span\u003e table.sql-dialect\u003cspan class=\"o\"\u003e=\u003c/span\u003ehive\u003cspan class=\"p\"\u003e;\u003c/span\u003e -- to use hive dialect\n\u003cspan class=\"o\"\u003e[\u003c/span\u003eINFO\u003cspan class=\"o\"\u003e]\u003c/span\u003e Session property has been set.\n\nFlink SQL\u0026gt; \u003cspan class=\"nb\"\u003eset\u003c/span\u003e table.sql-dialect\u003cspan class=\"o\"\u003e=\u003c/span\u003edefault\u003cspan class=\"p\"\u003e;\u003c/span\u003e -- to use default dialect\n\u003cspan class=\"o\"\u003e[\u003c/span\u003eINFO\u003cspan class=\"o\"\u003e]\u003c/span\u003e Session property has been set.\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"table-api\"\u003eTable API\u003c/h3\u003e\n\u003cp\u003eYou can set dialect for your TableEnvironment with Table API.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003epyflink.table\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003esettings\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEnvironmentSettings\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enew_instance\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ein_batch_mode\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003euse_blink_planner\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebuild\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003cspan class=\"n\"\u003et_env\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eBatchTableEnvironment\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eenvironment_settings\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003esettings\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# to use hive dialect\u003c/span\u003e\n\u003cspan class=\"n\"\u003et_env\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget_config\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset_sql_dialect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eSqlDialect\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eHIVE\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# to use default dialect\u003c/span\u003e\n\u003cspan class=\"n\"\u003et_env\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget_config\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset_sql_dialect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eSqlDialect\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eDEFAULT\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ddl\"\u003eDDL\u003c/h2\u003e\n\u003cp\u003e本节列出了 Hive 方言支持的 DDL。在这里我们将主要关注语法。关于每个 DDL 语句的语义，你可以参考 \u003ca href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL\"\u003eHive 文档\u003c/a\u003e。\u003c/p\u003e\n\u003ch3 id=\"database\"\u003eDATABASE\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eShow\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eSHOW\u003c/span\u003e \u003cspan class=\"n\"\u003eDATABASES\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eCreate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eCREATE\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003eDATABASE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eSCHEMA\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eIF\u003c/span\u003e \u003cspan class=\"k\"\u003eNOT\u003c/span\u003e \u003cspan class=\"k\"\u003eEXISTS\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"n\"\u003edatabase_name\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCOMMENT\u003c/span\u003e \u003cspan class=\"n\"\u003edatabase_comment\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eLOCATION\u003c/span\u003e \u003cspan class=\"n\"\u003efs_path\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eWITH\u003c/span\u003e \u003cspan class=\"n\"\u003eDBPROPERTIES\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...)];\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eAlter\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e更新属性\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003eDATABASE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eSCHEMA\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"n\"\u003edatabase_name\u003c/span\u003e \u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"n\"\u003eDBPROPERTIES\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e更新所有者\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003eDATABASE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eSCHEMA\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"n\"\u003edatabase_name\u003c/span\u003e \u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"k\"\u003eOWNER\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eUSER\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eROLE\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"n\"\u003euser_or_role\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e更新位置\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003eDATABASE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eSCHEMA\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"n\"\u003edatabase_name\u003c/span\u003e \u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"k\"\u003eLOCATION\u003c/span\u003e \u003cspan class=\"n\"\u003efs_path\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eDrop\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eDROP\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003eDATABASE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eSCHEMA\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eIF\u003c/span\u003e \u003cspan class=\"k\"\u003eEXISTS\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"n\"\u003edatabase_name\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eRESTRICT\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eCASCADE\u003c/span\u003e\u003cspan class=\"p\"\u003e];\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eUse\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"n\"\u003eUSE\u003c/span\u003e \u003cspan class=\"n\"\u003edatabase_name\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"table\"\u003eTABLE\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eShow\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eSHOW\u003c/span\u003e \u003cspan class=\"n\"\u003eTABLES\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eCreate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eCREATE\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eEXTERNAL\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eIF\u003c/span\u003e \u003cspan class=\"k\"\u003eNOT\u003c/span\u003e \u003cspan class=\"k\"\u003eEXISTS\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[(\u003c/span\u003e\u003cspan class=\"n\"\u003ecol_name\u003c/span\u003e \u003cspan class=\"n\"\u003edata_type\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ecolumn_constraint\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCOMMENT\u003c/span\u003e \u003cspan class=\"n\"\u003ecol_comment\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"p\"\u003e...\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003etable_constraint\u003c/span\u003e\u003cspan class=\"p\"\u003e])]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCOMMENT\u003c/span\u003e \u003cspan class=\"n\"\u003etable_comment\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ePARTITIONED\u003c/span\u003e \u003cspan class=\"k\"\u003eBY\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecol_name\u003c/span\u003e \u003cspan class=\"n\"\u003edata_type\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCOMMENT\u003c/span\u003e \u003cspan class=\"n\"\u003ecol_comment\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"p\"\u003e...)]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eROW\u003c/span\u003e \u003cspan class=\"n\"\u003eFORMAT\u003c/span\u003e \u003cspan class=\"n\"\u003erow_format\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eSTORED\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003efile_format\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eLOCATION\u003c/span\u003e \u003cspan class=\"n\"\u003efs_path\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eTBLPROPERTIES\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...)]\u003c/span\u003e\n  \n\u003cspan class=\"n\"\u003erow_format\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eDELIMITED\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eFIELDS\u003c/span\u003e \u003cspan class=\"n\"\u003eTERMINATED\u003c/span\u003e \u003cspan class=\"k\"\u003eBY\u003c/span\u003e \u003cspan class=\"nb\"\u003echar\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eESCAPED\u003c/span\u003e \u003cspan class=\"k\"\u003eBY\u003c/span\u003e \u003cspan class=\"nb\"\u003echar\u003c/span\u003e\u003cspan class=\"p\"\u003e]]\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eCOLLECTION\u003c/span\u003e \u003cspan class=\"n\"\u003eITEMS\u003c/span\u003e \u003cspan class=\"n\"\u003eTERMINATED\u003c/span\u003e \u003cspan class=\"k\"\u003eBY\u003c/span\u003e \u003cspan class=\"nb\"\u003echar\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n      \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eMAP\u003c/span\u003e \u003cspan class=\"n\"\u003eKEYS\u003c/span\u003e \u003cspan class=\"n\"\u003eTERMINATED\u003c/span\u003e \u003cspan class=\"k\"\u003eBY\u003c/span\u003e \u003cspan class=\"nb\"\u003echar\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eLINES\u003c/span\u003e \u003cspan class=\"n\"\u003eTERMINATED\u003c/span\u003e \u003cspan class=\"k\"\u003eBY\u003c/span\u003e \u003cspan class=\"nb\"\u003echar\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n      \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eNULL\u003c/span\u003e \u003cspan class=\"k\"\u003eDEFINED\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"nb\"\u003echar\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n  \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"n\"\u003eSERDE\u003c/span\u003e \u003cspan class=\"n\"\u003eserde_name\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eWITH\u003c/span\u003e \u003cspan class=\"n\"\u003eSERDEPROPERTIES\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...)]\u003c/span\u003e\n  \n\u003cspan class=\"n\"\u003efile_format\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eSEQUENCEFILE\u003c/span\u003e\n  \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"n\"\u003eTEXTFILE\u003c/span\u003e\n  \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"n\"\u003eRCFILE\u003c/span\u003e\n  \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"n\"\u003eORC\u003c/span\u003e\n  \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"n\"\u003ePARQUET\u003c/span\u003e\n  \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"n\"\u003eAVRO\u003c/span\u003e\n  \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"n\"\u003eINPUTFORMAT\u003c/span\u003e \u003cspan class=\"n\"\u003einput_format_classname\u003c/span\u003e \u003cspan class=\"n\"\u003eOUTPUTFORMAT\u003c/span\u003e \u003cspan class=\"n\"\u003eoutput_format_classname\u003c/span\u003e\n  \n\u003cspan class=\"n\"\u003ecolumn_constraint\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"k\"\u003eNOT\u003c/span\u003e \u003cspan class=\"k\"\u003eNULL\u003c/span\u003e \u003cspan class=\"p\"\u003e[[\u003c/span\u003e\u003cspan class=\"n\"\u003eENABLE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"n\"\u003eDISABLE\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eVALIDATE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"n\"\u003eNOVALIDATE\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eRELY\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"n\"\u003eNORELY\u003c/span\u003e\u003cspan class=\"p\"\u003e]]\u003c/span\u003e\n  \n\u003cspan class=\"n\"\u003etable_constraint\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCONSTRAINT\u003c/span\u003e \u003cspan class=\"k\"\u003econstraint_name\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003ePRIMARY\u003c/span\u003e \u003cspan class=\"k\"\u003eKEY\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecol_name\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...)\u003c/span\u003e \u003cspan class=\"p\"\u003e[[\u003c/span\u003e\u003cspan class=\"n\"\u003eENABLE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"n\"\u003eDISABLE\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eVALIDATE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"n\"\u003eNOVALIDATE\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eRELY\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"n\"\u003eNORELY\u003c/span\u003e\u003cspan class=\"p\"\u003e]]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eAlter\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e重命名\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-ssql\" data-lang=\"ssql\"\u003eALTER TABLE table_name RENAME TO new_table_name;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e更新属性\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e \u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"n\"\u003eTBLPROPERTIES\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_name\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eproperty_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eproperty_name\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eproperty_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...\u003c/span\u003e \u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e更新位置\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ePARTITION\u003c/span\u003e \u003cspan class=\"n\"\u003epartition_spec\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"k\"\u003eLOCATION\u003c/span\u003e \u003cspan class=\"n\"\u003efs_path\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003epartition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。\u003c/p\u003e\n\u003cp\u003e更新文件格式\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ePARTITION\u003c/span\u003e \u003cspan class=\"n\"\u003epartition_spec\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"n\"\u003eFILEFORMAT\u003c/span\u003e \u003cspan class=\"n\"\u003efile_format\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003epartition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。\u003c/p\u003e\n\u003cp\u003e更新 SerDe 属性\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ePARTITION\u003c/span\u003e \u003cspan class=\"n\"\u003epartition_spec\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"n\"\u003eSERDE\u003c/span\u003e \u003cspan class=\"n\"\u003eserde_class_name\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eWITH\u003c/span\u003e \u003cspan class=\"n\"\u003eSERDEPROPERTIES\u003c/span\u003e \u003cspan class=\"n\"\u003eserde_properties\u003c/span\u003e\u003cspan class=\"p\"\u003e];\u003c/span\u003e\n \n\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ePARTITION\u003c/span\u003e \u003cspan class=\"n\"\u003epartition_spec\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"n\"\u003eSERDEPROPERTIES\u003c/span\u003e \u003cspan class=\"n\"\u003eserde_properties\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n \n\u003cspan class=\"n\"\u003eserde_properties\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_name\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eproperty_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eproperty_name\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eproperty_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...\u003c/span\u003e \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003epartition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。\u003c/p\u003e\n\u003cp\u003e添加分区\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e \u003cspan class=\"k\"\u003eADD\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eIF\u003c/span\u003e \u003cspan class=\"k\"\u003eNOT\u003c/span\u003e \u003cspan class=\"k\"\u003eEXISTS\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ePARTITION\u003c/span\u003e \u003cspan class=\"n\"\u003epartition_spec\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eLOCATION\u003c/span\u003e \u003cspan class=\"n\"\u003efs_path\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eDrop Partitions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e \u003cspan class=\"k\"\u003eDROP\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eIF\u003c/span\u003e \u003cspan class=\"k\"\u003eEXISTS\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"n\"\u003ePARTITION\u003c/span\u003e \u003cspan class=\"n\"\u003epartition_spec\u003c/span\u003e\u003cspan class=\"p\"\u003e[,\u003c/span\u003e \u003cspan class=\"n\"\u003ePARTITION\u003c/span\u003e \u003cspan class=\"n\"\u003epartition_spec\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...];\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e新增/替换 列\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e\n  \u003cspan class=\"k\"\u003eADD\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eREPLACE\u003c/span\u003e \u003cspan class=\"n\"\u003eCOLUMNS\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecol_name\u003c/span\u003e \u003cspan class=\"n\"\u003edata_type\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCOMMENT\u003c/span\u003e \u003cspan class=\"n\"\u003ecol_comment\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"p\"\u003e...)\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCASCADE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eRESTRICT\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eChange Column\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e \u003cspan class=\"n\"\u003eCHANGE\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCOLUMN\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"n\"\u003ecol_old_name\u003c/span\u003e \u003cspan class=\"n\"\u003ecol_new_name\u003c/span\u003e \u003cspan class=\"n\"\u003ecolumn_type\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCOMMENT\u003c/span\u003e \u003cspan class=\"n\"\u003ecol_comment\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eFIRST\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eAFTER\u003c/span\u003e \u003cspan class=\"k\"\u003ecolumn_name\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCASCADE\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"k\"\u003eRESTRICT\u003c/span\u003e\u003cspan class=\"p\"\u003e];\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eDrop\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eDROP\u003c/span\u003e \u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eIF\u003c/span\u003e \u003cspan class=\"k\"\u003eEXISTS\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"view\"\u003eVIEW\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCreate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eCREATE\u003c/span\u003e \u003cspan class=\"k\"\u003eVIEW\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eIF\u003c/span\u003e \u003cspan class=\"k\"\u003eNOT\u003c/span\u003e \u003cspan class=\"k\"\u003eEXISTS\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"n\"\u003eview_name\u003c/span\u003e \u003cspan class=\"p\"\u003e[(\u003c/span\u003e\u003cspan class=\"k\"\u003ecolumn_name\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...)\u003c/span\u003e \u003cspan class=\"p\"\u003e]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eCOMMENT\u003c/span\u003e \u003cspan class=\"n\"\u003eview_comment\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eTBLPROPERTIES\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_name\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eproperty_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...)]\u003c/span\u003e\n  \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"k\"\u003eSELECT\u003c/span\u003e \u003cspan class=\"p\"\u003e...;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eAlter\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e注意：改变视图只在表 API 中工作，但不支持通过 SQL 客户端。\u003c/p\u003e\n\u003cp\u003e重命名\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eVIEW\u003c/span\u003e \u003cspan class=\"n\"\u003eview_name\u003c/span\u003e \u003cspan class=\"k\"\u003eRENAME\u003c/span\u003e \u003cspan class=\"k\"\u003eTO\u003c/span\u003e \u003cspan class=\"n\"\u003enew_view_name\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e更新属性\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eVIEW\u003c/span\u003e \u003cspan class=\"n\"\u003eview_name\u003c/span\u003e \u003cspan class=\"k\"\u003eSET\u003c/span\u003e \u003cspan class=\"n\"\u003eTBLPROPERTIES\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperty_name\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eproperty_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e...\u003c/span\u003e \u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eUpdate As Select\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eVIEW\u003c/span\u003e \u003cspan class=\"n\"\u003eview_name\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003eselect_statement\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eDrop\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eDROP\u003c/span\u003e \u003cspan class=\"k\"\u003eVIEW\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eIF\u003c/span\u003e \u003cspan class=\"k\"\u003eEXISTS\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"n\"\u003eview_name\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"function\"\u003eFUNCTION\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eShow\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eSHOW\u003c/span\u003e \u003cspan class=\"n\"\u003eFUNCTIONS\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eCreate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eCREATE\u003c/span\u003e \u003cspan class=\"k\"\u003eFUNCTION\u003c/span\u003e \u003cspan class=\"n\"\u003efunction_name\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003eclass_name\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eDrop\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eDROP\u003c/span\u003e \u003cspan class=\"k\"\u003eFUNCTION\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eIF\u003c/span\u003e \u003cspan class=\"k\"\u003eEXISTS\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"n\"\u003efunction_name\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"dml\"\u003eDML\u003c/h2\u003e\n\u003ch3 id=\"nsert\"\u003eNSERT\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"k\"\u003eINSERT\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003eINTO\u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"n\"\u003eOVERWRITE\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"k\"\u003eTABLE\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003etable_name\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ePARTITION\u003c/span\u003e \u003cspan class=\"n\"\u003epartition_spec\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003eSELECT\u003c/span\u003e \u003cspan class=\"p\"\u003e...;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003epartition_spec，如果存在的话，可以是完整规格或部分规格。如果 partition_spec 是部分规格，动态分区列名可以省略。\u003c/p\u003e\n\u003ch2 id=\"dql\"\u003eDQL\u003c/h2\u003e\n\u003cp\u003e目前，Hive 方言支持的 DQL 语法与 Flink SQL 相同。详情请参考 Flink SQL 查询。而且建议切换到默认方言来执行 DQL。\u003c/p\u003e\n\u003ch2 id=\"注意事项\"\u003e注意事项\u003c/h2\u003e\n\u003cp\u003e以下是使用 Hive 方言的一些注意事项。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHive 方言只能用于操作 Hive 表，而不是通用表。而且 Hive 方言应该和 \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_catalog.html\"\u003eHiveCatalog\u003c/a\u003e 一起使用。\u003c/li\u003e\n\u003cli\u003e虽然所有的 Hive 版本都支持相同的语法，但是否有特定的功能还是取决于你使用的 \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#supported-hive-versions\"\u003eHive 版本\u003c/a\u003e。例如，更新数据库位置只在 Hive-2.4.0 或更高版本中支持。\u003c/li\u003e\n\u003cli\u003eHive 和 Calcite 有不同的保留关键字集。例如，在 Calcite 中默认是保留关键字，而在 Hive 中是非保留关键字。即使是 Hive 方言，你也必须用反引号（`）来引用这些关键字，才能将它们作为标识符使用。\u003c/li\u003e\n\u003cli\u003e由于扩展查询不兼容，在 Flink 中创建的视图不能在 Hive 中查询。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e原文链接: \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_dialect.html\"\u003ehttps://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_dialect.html\u003c/a\u003e\u003c/p\u003e\n","text":"Hive 方言 从 1.11.0 开始，当使用 Hive 方言时，Flink 允许用户用 Hive 语法编写 SQL 语句。通过提供与 Hive 语法的兼容性，我们旨在提高与 Hive 的互操作性，减少用户为了执行不同的语句而需要在 Flink 和 Hive 之间切换的情况。\n使用 Hive 方言 Flink 目前支持两种 SQL 方言：默认和 Hive。在使用 Hive 语法编写之前，需要先切换到 Hive 方言。下面介绍如何通过 SQL Client 和 Table API 来设置方言。同时注意，你可以为你执行的每一条语句动态切换方言。不需要重新启动会话来使用不同的方言。\nSQL 客户端 SQL 方言可以通过 table.sql-dialect 属性来指定，因此你可以在你的 SQL 客户端的 yaml 文件的配置部分设置初始方言。因此，你可以在 SQL 客户端的 yaml 文件的配置部分设置要使用的初始方言。\nexecution:planner:blinktype:batchresult-mode:tableconfiguration:table.sql-dialect:hive你也可以在 SQL 客户端启动后设置方言。\nFlink SQL\u0026gt; set table.sql-dialect=hive; -- to use hive dialect [INFO] Session property has been set. Flink SQL\u0026gt; set table.sql-dialect=default; -- to use default dialect [INFO] Session property has been set. Table API You can set dialect for your TableEnvironment with Table API.\nfrom pyflink.table import * settings = EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build() t_env = BatchTableEnvironment.create(environment_settings=settings) # to use hive dialect t_env.get_config().set_sql_dialect(SqlDialect.HIVE) # to use default dialect t_env.get_config().set_sql_dialect(SqlDialect.DEFAULT) DDL 本节列出了 Hive 方言支持的 DDL。在这里我们将主要关注语法。关于每个 DDL 语句的语义，你可以参考 Hive 文档。\nDATABASE  Show  SHOW DATABASES;  Create  CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION fs_path] [WITH DBPROPERTIES (property_name=property_value, ...)];  Alter  更新属性\nALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); 更新所有者\nALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; 更新位置\nALTER (DATABASE|SCHEMA) database_name SET LOCATION fs_path;  Drop  DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];  Use  USE database_name; TABLE  Show  SHOW TABLES;  Create  CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [column_constraint] [COMMENT col_comment], ... [table_constraint])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [ [ROW FORMAT row_format] [STORED AS file_format] ] [LOCATION fs_path] [TBLPROPERTIES (property_name=property_value, ...)] row_format: : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, ...)] file_format: : SEQUENCEFILE | TEXTFILE | RCFILE | ORC | PARQUET | AVRO | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname column_constraint: : NOT NULL [[ENABLE|DISABLE] [VALIDATE|NOVALIDATE] [RELY|NORELY]] table_constraint: : [CONSTRAINT constraint_name] PRIMARY KEY (col_name, ...) [[ENABLE|DISABLE] [VALIDATE|NOVALIDATE] [RELY|NORELY]]  Alter  重命名\nALTER TABLE table_name RENAME TO new_table_name; 更新属性\nALTER TABLE table_name SET TBLPROPERTIES (property_name = property_value, property_name = property_value, ... ); 更新位置\nALTER TABLE table_name [PARTITION partition_spec] SET LOCATION fs_path; partition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。\n更新文件格式\nALTER TABLE table_name [PARTITION partition_spec] SET FILEFORMAT file_format; partition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。\n更新 SerDe 属性\nALTER TABLE table_name [PARTITION partition_spec] SET SERDE serde_class_name [WITH SERDEPROPERTIES serde_properties]; ALTER TABLE table_name [PARTITION partition_spec] SET SERDEPROPERTIES serde_properties; serde_properties: : (property_name = property_value, property_name = property_value, ... ) partition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。\n添加分区\nALTER TABLE table_name ADD [IF NOT EXISTS] (PARTITION partition_spec [LOCATION fs_path])+;  Drop Partitions  ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...];  新增/替换 列  ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) [CASCADE|RESTRICT]  Change Column  ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] [CASCADE|RESTRICT];  Drop  DROP TABLE [IF EXISTS] table_name; VIEW  Create  CREATE VIEW [IF NOT EXISTS] view_name [(column_name, ...) ] [COMMENT view_comment] [TBLPROPERTIES (property_name = property_value, ...)] AS SELECT ...;  Alter  注意：改变视图只在表 API 中工作，但不支持通过 SQL 客户端。\n重命名\nALTER VIEW view_name RENAME TO new_view_name; 更新属性\nALTER VIEW view_name SET TBLPROPERTIES (property_name = property_value, ... );  Update As Select  ALTER VIEW view_name AS select_statement;  Drop  DROP VIEW [IF EXISTS] view_name; FUNCTION  Show  SHOW FUNCTIONS;  Create  CREATE FUNCTION function_name AS class_name; Drop\nDROP FUNCTION [IF EXISTS] function_name; DML NSERT INSERT (INTO|OVERWRITE) [TABLE] table_name [PARTITION partition_spec] SELECT ...; partition_spec，如果存在的话，可以是完整规格或部分规格。如果 partition_spec 是部分规格，动态分区列名可以省略。\nDQL 目前，Hive 方言支持的 DQL 语法与 Flink SQL 相同。详情请参考 Flink SQL 查询。而且建议切换到默认方言来执行 DQL。\n注意事项 以下是使用 Hive 方言的一些注意事项。\n Hive 方言只能用于操作 Hive 表，而不是通用表。而且 Hive 方言应该和 HiveCatalog 一起使用。 虽然所有的 Hive 版本都支持相同的语法，但是否有特定的功能还是取决于你使用的 Hive 版本。例如，更新数据库位置只在 Hive-2.4.0 或更高版本中支持。 Hive 和 Calcite 有不同的保留关键字集。例如，在 Calcite 中默认是保留关键字，而在 Hive 中是非保留关键字。即使是 Hive 方言，你也必须用反引号（`）来引用这些关键字，才能将它们作为标识符使用。 由于扩展查询不兼容，在 Flink 中创建的视图不能在 Hive 中查询。  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_dialect.html\n"},"name":"Hive 方言","published":"2020-08-25T00:00:00+08:00","summary":"Hive Dialect","type":"entry","url":"https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/"},{"content":{"html":"\u003ch1 id=\"hive-集成\"\u003eHive 集成\u003c/h1\u003e\n\u003cp\u003eApache Hive 已经确立了自己作为数据仓库生态系统的焦点。它不仅是大数据分析和 ETL 的 SQL 引擎，也是一个数据管理平台，在这里，数据被发现、定义和发展。\u003c/p\u003e\n\u003cp\u003eFlink 与 Hive 提供了两方面的整合。\u003c/p\u003e\n\u003cp\u003e第一是利用 Hive 的 Metastore 作为一个持久性目录，与 Flink 的 HiveCatalog 进行跨会话存储 Flink 特定的元数据。例如，用户可以通过使用 HiveCatalog 将 Kafka 或 ElasticSearch 表存储在 Hive Metastore 中，并在以后的 SQL 查询中重复使用。\u003c/p\u003e\n\u003cp\u003e二是提供 Flink 作为读写 Hive 表的替代引擎。\u003c/p\u003e\n\u003cp\u003eHiveCatalog 的设计是 \u0026ldquo;开箱即用\u0026rdquo;，与现有的 Hive 安装兼容。您不需要修改现有的 Hive Metastore，也不需要改变数据位置或表的分区。\u003c/p\u003e\n\u003ch2 id=\"支持的-hive-版本\"\u003e支持的 Hive 版本\u003c/h2\u003e\n\u003cp\u003eFlink 支持以下 Hive 版本。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1.0\n\u003cul\u003e\n\u003cli\u003e1.0.0\u003c/li\u003e\n\u003cli\u003e1.0.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e1.1\n\u003cul\u003e\n\u003cli\u003e1.1.0\u003c/li\u003e\n\u003cli\u003e1.1.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e1.2\n\u003cul\u003e\n\u003cli\u003e1.2.0\u003c/li\u003e\n\u003cli\u003e1.2.1\u003c/li\u003e\n\u003cli\u003e1.2.2\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2.0\n\u003cul\u003e\n\u003cli\u003e2.0.0\u003c/li\u003e\n\u003cli\u003e2.0.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2.1\n\u003cul\u003e\n\u003cli\u003e2.1.0\u003c/li\u003e\n\u003cli\u003e2.1.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2.2\n\u003cul\u003e\n\u003cli\u003e2.2.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2.3\n\u003cul\u003e\n\u003cli\u003e2.3.0\u003c/li\u003e\n\u003cli\u003e2.3.1\u003c/li\u003e\n\u003cli\u003e2.3.2\u003c/li\u003e\n\u003cli\u003e2.3.3\u003c/li\u003e\n\u003cli\u003e2.3.4\u003c/li\u003e\n\u003cli\u003e2.3.5\u003c/li\u003e\n\u003cli\u003e2.3.6\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e3.1\n\u003cul\u003e\n\u003cli\u003e3.1.0\u003c/li\u003e\n\u003cli\u003e3.1.1\u003c/li\u003e\n\u003cli\u003e3.1.2\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e请注意 Hive 本身在不同的版本有不同的功能，这些问题不是 Flink 造成的。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1.2.0 及以后版本支持 Hive 内置函数。\u003c/li\u003e\n\u003cli\u003e3.1.0 及以后版本支持列约束，即 PRIMARY KEY 和 NOT NULL。\u003c/li\u003e\n\u003cli\u003e在 1.2.0 及以后的版本中，支持修改表的统计数据。\u003c/li\u003e\n\u003cli\u003e在 1.2.0 及以后的版本中支持 DATE 列统计。\u003c/li\u003e\n\u003cli\u003e在 2.0.x 中不支持写入 ORC 表。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"依赖性\"\u003e依赖性\u003c/h3\u003e\n\u003cp\u003e为了与 Hive 集成，你需要在 Flink 发行版的 \u003ccode\u003e/lib/\u003c/code\u003e 目录下添加一些额外的依赖关系，以使集成工作在 Table API 程序或 SQL 客户端中。另外，你也可以将这些依赖项放在一个专门的文件夹中，并分别用 \u003ccode\u003e-C\u003c/code\u003e 或 \u003ccode\u003e-l\u003c/code\u003e 选项将它们添加到 \u003ccode\u003eclasspath\u003c/code\u003e 中，用于 Table API 程序或 SQL Client。\u003c/p\u003e\n\u003cp\u003eApache Hive 是建立在 Hadoop 上的，所以首先需要 Hadoop 依赖，请参考提供 Hadoop 类。\u003c/p\u003e\n\u003cp\u003e有两种方法可以添加 Hive 依赖。首先是使用 Flink 的捆绑式 Hive jars。你可以根据你使用的 metastore 的版本来选择捆绑的 Hive jar。第二种是分别添加每个所需的 jar。如果你使用的 Hive 版本没有在这里列出，第二种方式就会很有用。\u003c/p\u003e\n\u003ch4 id=\"使用捆绑的-hive-jar\"\u003e使用捆绑的 Hive jar\u003c/h4\u003e\n\u003cp\u003e下表列出了所有可用的捆绑的 hive jar，你可以选择一个到 Flink 发行版的 \u003ccode\u003e/lib/\u003c/code\u003e 目录下。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align:left\"\u003eMetastore version\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eMaven dependency\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eSQL Client JAR\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e1.0.0 - 1.2.2\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eflink-sql-connector-hive-1.2.2\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003ca href=\"https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-1.2.2_2.11/1.11.0/flink-sql-connector-hive-1.2.2_2.11-1.11.0.jar\"\u003eDownload\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e2.0.0 - 2.2.0\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eflink-sql-connector-hive-2.2.0\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003ca href=\"https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-2.2.0_2.11/1.11.0/flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar\"\u003eDownload\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e2.3.0 - 2.3.6\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eflink-sql-connector-hive-2.3.6\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003ca href=\"https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-2.3.6_2.11/1.11.0/flink-sql-connector-hive-2.3.6_2.11-1.11.0.jar\"\u003eDownload\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e3.0.0 - 3.1.2\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eflink-sql-connector-hive-3.1.2\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003ca href=\"https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-3.1.2_2.11/1.11.0/flink-sql-connector-hive-3.1.2_2.11-1.11.0.jar\"\u003eDownload\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch4 id=\"用户定义的依赖性\"\u003e用户定义的依赖性\u003c/h4\u003e\n\u003cp\u003e请在下面找到不同 Hive 主要版本所需的依赖关系。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHive 2.3.4\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink's Hive connector.Contains flink-hadoop-compatibility and flink-orc jars\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-exec-2.3.4.jar\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 1.0.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink's Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-metastore-1.0.0.jar\n       hive-exec-1.0.0.jar\n       libfb303-0.9.0.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately\n       \n       // Orc dependencies -- required by the ORC vectorized optimizations\n       orc-core-1.4.3-nohive.jar\n       aircompressor-0.8.jar // transitive dependency of orc-core\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 1.1.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink's Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-metastore-1.1.0.jar\n       hive-exec-1.1.0.jar\n       libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately\n\n       // Orc dependencies -- required by the ORC vectorized optimizations\n       orc-core-1.4.3-nohive.jar\n       aircompressor-0.8.jar // transitive dependency of orc-core\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 1.2.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink's Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-metastore-1.2.1.jar\n       hive-exec-1.2.1.jar\n       libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately\n\n       // Orc dependencies -- required by the ORC vectorized optimizations\n       orc-core-1.4.3-nohive.jar\n       aircompressor-0.8.jar // transitive dependency of orc-core\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 2.0.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink's Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-exec-2.0.0.jar\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 2.1.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink's Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-exec-2.1.0.jar\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 2.2.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink's Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-exec-2.2.0.jar\n\n       // Orc dependencies -- required by the ORC vectorized optimizations\n       orc-core-1.4.3.jar\n       aircompressor-0.8.jar // transitive dependency of orc-core\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 3.1.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink's Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-exec-3.1.0.jar\n       libfb303-0.9.3.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"program-maven\"\u003eProgram maven\u003c/h3\u003e\n\u003cp\u003e如果你正在构建你自己的程序，你需要在你的 mvn 文件中加入以下依赖关系。建议不要在生成的 jar 文件中包含这些依赖关系。你应该在运行时添加上面所说的依赖关系。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-xml\" data-lang=\"xml\"\u003e\u003cspan class=\"c\"\u003e\u0026lt;!-- Flink Dependency --\u0026gt;\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n  \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.flink\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n  \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003eflink-connector-hive_2.11\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n  \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e1.11.0\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n  \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\n\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n  \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.flink\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n  \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003eflink-table-api-java-bridge_2.11\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n  \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e1.11.0\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n  \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\n\u003cspan class=\"c\"\u003e\u0026lt;!-- Hive Dependency --\u0026gt;\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n    \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.hive\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n    \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003ehive-exec\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n    \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e${hive.version}\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n    \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"连接到-hive\"\u003e连接到 Hive\u003c/h2\u003e\n\u003cp\u003e通过表环境或 YAML 配置，使用目录接口和 HiveCatalog 连接到现有的 Hive 安装。\u003c/p\u003e\n\u003cp\u003e如果 \u003ccode\u003ehive-conf/hive-site.xml\u003c/code\u003e 文件存储在远程存储系统中，用户应先将 hive 配置文件下载到本地环境中。\u003c/p\u003e\n\u003cp\u003e请注意，虽然 HiveCatalog 不需要特定的规划师，但读/写 Hive 表只适用于 blink 规划师。因此强烈建议您在连接 Hive 仓库时使用 blink planner。\u003c/p\u003e\n\u003cp\u003eHiveCatalog 能够自动检测使用中的 Hive 版本。建议不要指定 Hive 版本，除非自动检测失败。\u003c/p\u003e\n\u003cp\u003e以 Hive 2.3.4 版本为例。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"chroma\"\u003e\u003ccode class=\"language-scala\" data-lang=\"scala\"\u003e\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003esettings\u003c/span\u003e \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eEnvironmentSettings\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enewInstance\u003c/span\u003e\u003cspan class=\"o\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003einBatchMode\u003c/span\u003e\u003cspan class=\"o\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003ebuild\u003c/span\u003e\u003cspan class=\"o\"\u003e()\u003c/span\u003e\n\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003etableEnv\u003c/span\u003e \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eTableEnvironment\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esettings\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e            \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;myhive\u0026#34;\u003c/span\u003e\n\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003edefaultDatabase\u003c/span\u003e \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;mydatabase\u0026#34;\u003c/span\u003e\n\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003ehiveConfDir\u003c/span\u003e     \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;/opt/hive-conf\u0026#34;\u003c/span\u003e \u003cspan class=\"c1\"\u003e// a local path\n\u003c/span\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\n\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003ehive\u003c/span\u003e \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nc\"\u003eHiveCatalog\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"o\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edefaultDatabase\u003c/span\u003e\u003cspan class=\"o\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ehiveConfDir\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003etableEnv\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eregisterCatalog\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;myhive\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ehive\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e// set the HiveCatalog as the current catalog of the session\n\u003c/span\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003etableEnv\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003euseCatalog\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;myhive\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ddl\"\u003eDDL\u003c/h2\u003e\n\u003cp\u003e建议使用 \u003ca href=\"https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect\"\u003eHive 方言\u003c/a\u003e在 Flink 中执行 DDL 来创建 Hive 表、视图、分区、函数。\u003c/p\u003e\n\u003ch2 id=\"dml\"\u003eDML\u003c/h2\u003e\n\u003cp\u003eFlink 支持 DML 写入 Hive 表。请参考\u003ca href=\"https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write\"\u003e读写 Hive 表\u003c/a\u003e的细节。\u003c/p\u003e\n\u003cp\u003e原文链接: \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/\"\u003ehttps://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/\u003c/a\u003e\u003c/p\u003e\n","text":"Hive 集成 Apache Hive 已经确立了自己作为数据仓库生态系统的焦点。它不仅是大数据分析和 ETL 的 SQL 引擎，也是一个数据管理平台，在这里，数据被发现、定义和发展。\nFlink 与 Hive 提供了两方面的整合。\n第一是利用 Hive 的 Metastore 作为一个持久性目录，与 Flink 的 HiveCatalog 进行跨会话存储 Flink 特定的元数据。例如，用户可以通过使用 HiveCatalog 将 Kafka 或 ElasticSearch 表存储在 Hive Metastore 中，并在以后的 SQL 查询中重复使用。\n二是提供 Flink 作为读写 Hive 表的替代引擎。\nHiveCatalog 的设计是 \u0026ldquo;开箱即用\u0026rdquo;，与现有的 Hive 安装兼容。您不需要修改现有的 Hive Metastore，也不需要改变数据位置或表的分区。\n支持的 Hive 版本 Flink 支持以下 Hive 版本。\n 1.0  1.0.0 1.0.1   1.1  1.1.0 1.1.1   1.2  1.2.0 1.2.1 1.2.2   2.0  2.0.0 2.0.1   2.1  2.1.0 2.1.1   2.2  2.2.0   2.3  2.3.0 2.3.1 2.3.2 2.3.3 2.3.4 2.3.5 2.3.6   3.1  3.1.0 3.1.1 3.1.2    请注意 Hive 本身在不同的版本有不同的功能，这些问题不是 Flink 造成的。\n 1.2.0 及以后版本支持 Hive 内置函数。 3.1.0 及以后版本支持列约束，即 PRIMARY KEY 和 NOT NULL。 在 1.2.0 及以后的版本中，支持修改表的统计数据。 在 1.2.0 及以后的版本中支持 DATE 列统计。 在 2.0.x 中不支持写入 ORC 表。  依赖性 为了与 Hive 集成，你需要在 Flink 发行版的 /lib/ 目录下添加一些额外的依赖关系，以使集成工作在 Table API 程序或 SQL 客户端中。另外，你也可以将这些依赖项放在一个专门的文件夹中，并分别用 -C 或 -l 选项将它们添加到 classpath 中，用于 Table API 程序或 SQL Client。\nApache Hive 是建立在 Hadoop 上的，所以首先需要 Hadoop 依赖，请参考提供 Hadoop 类。\n有两种方法可以添加 Hive 依赖。首先是使用 Flink 的捆绑式 Hive jars。你可以根据你使用的 metastore 的版本来选择捆绑的 Hive jar。第二种是分别添加每个所需的 jar。如果你使用的 Hive 版本没有在这里列出，第二种方式就会很有用。\n使用捆绑的 Hive jar 下表列出了所有可用的捆绑的 hive jar，你可以选择一个到 Flink 发行版的 /lib/ 目录下。\n   Metastore version Maven dependency SQL Client JAR     1.0.0 - 1.2.2 flink-sql-connector-hive-1.2.2 Download   2.0.0 - 2.2.0 flink-sql-connector-hive-2.2.0 Download   2.3.0 - 2.3.6 flink-sql-connector-hive-2.3.6 Download   3.0.0 - 3.1.2 flink-sql-connector-hive-3.1.2 Download    用户定义的依赖性 请在下面找到不同 Hive 主要版本所需的依赖关系。\n Hive 2.3.4  /flink-1.11.0 /lib // Flink's Hive connector.Contains flink-hadoop-compatibility and flink-orc jars flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.3.4.jar  Hive 1.0.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-metastore-1.0.0.jar hive-exec-1.0.0.jar libfb303-0.9.0.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core  Hive 1.1.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-metastore-1.1.0.jar hive-exec-1.1.0.jar libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core  Hive 1.2.1  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-metastore-1.2.1.jar hive-exec-1.2.1.jar libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core  Hive 2.0.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.0.0.jar  Hive 2.1.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.1.0.jar  Hive 2.2.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.2.0.jar // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3.jar aircompressor-0.8.jar // transitive dependency of orc-core  Hive 3.1.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-3.1.0.jar libfb303-0.9.3.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately Program maven 如果你正在构建你自己的程序，你需要在你的 mvn 文件中加入以下依赖关系。建议不要在生成的 jar 文件中包含这些依赖关系。你应该在运行时添加上面所说的依赖关系。\n\u0026lt;!-- Flink Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-hive_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Hive Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hive-exec\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${hive.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 连接到 Hive 通过表环境或 YAML 配置，使用目录接口和 HiveCatalog 连接到现有的 Hive 安装。\n如果 hive-conf/hive-site.xml 文件存储在远程存储系统中，用户应先将 hive 配置文件下载到本地环境中。\n请注意，虽然 HiveCatalog 不需要特定的规划师，但读/写 Hive 表只适用于 blink 规划师。因此强烈建议您在连接 Hive 仓库时使用 blink planner。\nHiveCatalog 能够自动检测使用中的 Hive 版本。建议不要指定 Hive 版本，除非自动检测失败。\n以 Hive 2.3.4 版本为例。\nval settings = EnvironmentSettings.newInstance().inBatchMode().build() val tableEnv = TableEnvironment.create(settings) val name = \u0026#34;myhive\u0026#34; val defaultDatabase = \u0026#34;mydatabase\u0026#34; val hiveConfDir = \u0026#34;/opt/hive-conf\u0026#34; // a local path  val hive = new HiveCatalog(name, defaultDatabase, hiveConfDir) tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, hive) // set the HiveCatalog as the current catalog of the session tableEnv.useCatalog(\u0026#34;myhive\u0026#34;) DDL 建议使用 Hive 方言在 Flink 中执行 DDL 来创建 Hive 表、视图、分区、函数。\nDML Flink 支持 DML 写入 Hive 表。请参考读写 Hive 表的细节。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/\n"},"name":"Hive 集成 - 概览","published":"2020-08-25T00:00:00+08:00","summary":"Overview","type":"entry","url":"https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/"}],"name":"Hive","type":"feed","url":"https://ohmyweekly.github.io/tags/hive/"}