<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-us">
    <generator uri="https://gohugo.io/" version="0.79.0">Hugo</generator><title type="html"><![CDATA[Guide on 焉知非鱼]]></title>
    
        <subtitle type="html"><![CDATA[rakulang, dartlang, nimlang, golang, rustlang, lang lang no see]]></subtitle>
    
    
    
            <link href="https://ohmyweekly.github.io/tags/guide/" rel="alternate" type="text/html" title="HTML" />
            <link href="https://ohmyweekly.github.io/tags/guide/index.xml" rel="alternate" type="application/rss+xml" title="RSS" />
            <link href="https://ohmyweekly.github.io/tags/guide/atom.xml" rel="self" type="application/atom+xml" title="Atom" />
            <link href="https://ohmyweekly.github.io/tags/guide/jf2feed.json" rel="alternate" type="application/jf2feed+json" title="jf2feed" />
    <updated>2020-12-23T23:14:06+08:00</updated>
    
    
    
    
        <id>https://ohmyweekly.github.io/tags/guide/</id>
    
        
        <entry>
            <title type="html"><![CDATA[Flink Dataset API 编程指南]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Flink Dataset Api Programming Guide</blockquote><h2 id="flink-dataset-api-编程指南">Flink DataSet API 编程指南</h2>
<p>Flink 中的数据集程序是对数据集实现转换（如过滤、映射、加入、分组）的常规程序。数据集最初是从某些来源创建的（例如，通过读取文件，或从本地集合中创建）。结果通过汇返回，例如可以将数据写入（分布式）文件，或标准输出（例如命令行终端）。Flink 程序可以在各种环境下运行，独立运行，或者嵌入其他程序中。执行可以发生在本地 JVM 中，也可以发生在许多机器的集群中。</p>
<p>请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">DataStream API 概述</a>，了解 Flink API 的基本概念。该概述是针对 DataStream API 的，但这两个 API 的基本概念是一样的。</p>
<p>为了创建你自己的 Flink DataSet 程序，我们鼓励你从 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html#anatomy-of-a-flink-program">Flink 程序的骨架</a>开始，并逐步添加你自己的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#dataset-transformations">转换</a>。其余部分作为附加操作和高级功能的参考。</p>
<h3 id="程序示例">程序示例</h3>
<p>下面的程序是一个完整的、可以使用的 WordCount 的例子，你可以复制和粘贴代码在本地运行。你可以复制和粘贴代码在本地运行它。你只需要在你的项目中加入正确的 Flink 的库（参见与 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html">Flink 的链接</a>部分）并指定导入。然后你就可以开始了</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala._</span>

<span class="k">object</span> <span class="nc">WordCount</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
    <span class="k">val</span> <span class="n">text</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span>
      <span class="s">&#34;Who&#39;s there?&#34;</span><span class="o">,</span>
      <span class="s">&#34;I think I hear them. Stand, ho! Who&#39;s there?&#34;</span><span class="o">)</span>

    <span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">text</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">toLowerCase</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34;\\W+&#34;</span><span class="o">)</span> <span class="n">filter</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">nonEmpty</span> <span class="o">}</span> <span class="o">}</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">}</span>
      <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
      <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

    <span class="n">counts</span><span class="o">.</span><span class="n">print</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="dataset-转换">DataSet 转换</h3>
<p>数据转换将一个或多个 DataSet 转换为一个新的 DataSet。程序可以将多个转换组合成复杂的集合。</p>
<p>本节简要介绍了可用的转换。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html">转换文档</a>中有所有变换的完整描述和示例。</p>
<ul>
<li>Map</li>
</ul>
<p>接受一个元素，产生一个元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">toInt</span> <span class="o">}</span>
</code></pre></div><ul>
<li>FlatMap</li>
</ul>
<p>接受一个元素并产生零、一个或多个元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="n">str</span> <span class="k">=&gt;</span> <span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34; &#34;</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><ul>
<li>MapPartition</li>
</ul>
<p>在一个函数调用中转换一个并行分区。该函数以&quot;迭代器&quot;的形式获取分区，并可产生任意数量的结果值。每个分区的元素数量取决于平行度和之前的操作。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="n">in</span> <span class="k">=&gt;</span> <span class="n">in</span> <span class="n">map</span> <span class="o">{</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">}</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Filter</li>
</ul>
<p>对每个元素进行布尔函数评估，并保留那些函数返回真的元素。
重要：系统假设函数不会修改应用谓词的元素。违反这个假设会导致错误的结果。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">_</span> <span class="o">&gt;</span> <span class="mi">1000</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Reduce</li>
</ul>
<p>通过重复将两个元素合并为一个元素，将一组元素合并为一个元素。换算可以应用于一个完整的数据集，也可以应用于一个分组的数据集。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="k">_</span> <span class="o">+</span> <span class="k">_</span> <span class="o">}</span>
</code></pre></div><ul>
<li>ReduceGroup</li>
</ul>
<p>将一组元素合并成一个或多个元素。<code>ReduceGroup</code> 可以应用在一个完整的数据集上，也可以应用在一个分组的数据集上。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduceGroup</span> <span class="o">{</span> <span class="n">elements</span> <span class="k">=&gt;</span> <span class="n">elements</span><span class="o">.</span><span class="n">sum</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Aggregate</li>
</ul>
<p>将一组值聚合成一个值。<code>Aggregation</code> 函数可以被认为是内置的 <code>reduce</code> 函数。<code>Aggregate</code> 可以应用于一个完整的数据集，也可以应用于一个分组的数据集。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">SUM</span><span class="o">,</span> <span class="mi">0</span><span class="o">).</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">MIN</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
</code></pre></div><p>你也可以使用简写语法来进行 <code>minimum</code>, <code>maximum</code> 和 <code>sum</code> 的聚合。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">min</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
</code></pre></div><ul>
<li>Distinct</li>
</ul>
<p>返回数据集的不同元素。它从输入的 DataSet 中删除元素的所有字段或字段子集的重复条目。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">distinct</span><span class="o">()</span>
</code></pre></div><ul>
<li>Join</li>
</ul>
<p>通过创建所有键值相等的元素对来连接两个数据集。可以选择使用 <code>JoinFunction</code> 将一对元素变成一个元素，或者使用 <code>FlatJoinFunction</code> 将一对元素变成任意多个（包括无）元素。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#specifying-keys">键</a>部分了解如何定义连接键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// In this case tuple fields are used as keys. &#34;0&#34; is the join field on the first tuple
</span><span class="c1">// &#34;1&#34; is the join field on the second tuple.
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">input2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>你可以通过 Join Hints 指定运行时执行连接的方式。这些提示描述了连接是通过分区还是广播进行的，以及它是使用基于排序还是基于散列的算法。请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#join-algorithm-hints">转换指南</a>，了解可能的提示列表和示例。
如果没有指定提示，系统将尝试对输入大小进行估计，并根据这些估计选择最佳策略。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// This executes a join by broadcasting the first data set
</span><span class="c1">// using a hash table for the broadcast data
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">input2</span><span class="o">,</span> <span class="nc">JoinHint</span><span class="o">.</span><span class="nc">BROADCAST_HASH_FIRST</span><span class="o">)</span>
                   <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>请注意，连接转换只适用于等价连接。其他的连接类型需要使用 OuterJoin 或 CoGroup 来表达。</p>
<ul>
<li>OuterJoin</li>
</ul>
<p>在两个数据集上执行左联接、右联接或完全外联接。外联接与常规（内联接）类似，创建所有键值相同的元素对。此外，如果在另一侧没有找到匹配的键，&ldquo;外侧&quot;的记录（左、右或全联接时两者都有）将被保留。匹配的元素对（或一个元素和另一个输入的 <code>null</code> 值）被交给 <code>JoinFunction</code> 将这对元素变成单个元素，或交给 <code>FlatJoinFunction</code> 将这对元素变成任意多个（包括无）元素。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#specifying-keys">键</a>部分，了解如何定义连接键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">joined</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span><span class="n">right</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="o">{</span>
   <span class="o">(</span><span class="n">left</span><span class="o">,</span> <span class="n">right</span><span class="o">)</span> <span class="k">=&gt;</span>
     <span class="k">val</span> <span class="n">a</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">left</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="s">&#34;none&#34;</span> <span class="k">else</span> <span class="n">left</span><span class="o">.</span><span class="n">_1</span>
     <span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">right</span><span class="o">)</span>
  <span class="o">}</span>
</code></pre></div><ul>
<li>CoGroup</li>
</ul>
<p>减少操作的二维变体。在一个或多个字段上对每个输入进行分组，然后将分组合并。每一对组都会调用转换函数。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#specifying-keys">键</a>部分，了解如何定义 <code>coGroup</code> 键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data1</span><span class="o">.</span><span class="n">coGroup</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><ul>
<li>Cross</li>
</ul>
<p>建立两个输入的笛卡尔乘积（交叉乘积），创建所有元素对。可选择使用交叉函数将一对元素变成一个单一元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">data2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">data1</span><span class="o">.</span><span class="n">cross</span><span class="o">(</span><span class="n">data2</span><span class="o">)</span>
</code></pre></div><p>注意：<code>Cross</code> 可能是一个非常耗费计算的操作，甚至可以挑战大型计算集群！建议使用 <code>crossWithTiny()</code> 和 <code>crossWithHuge()</code> 来提示系统数据集的大小。</p>
<ul>
<li>
<p>Union</p>
</li>
<li>
<p>产生两个数据集的并集。</p>
</li>
</ul>
<pre><code class="language-raku" data-lang="raku">data.union(data2)
</code></pre><ul>
<li>Rebalance</li>
</ul>
<p>均匀地重新平衡数据集的并行分区，以消除数据倾斜。只有类似于 Map 的变换才可以跟随重新平衡(rebalance)变换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">data1</span><span class="o">.</span><span class="n">rebalance</span><span class="o">().</span><span class="n">map</span><span class="o">(...)</span>
</code></pre></div><ul>
<li>Hash-Partition</li>
</ul>
<p>在给定的键上对数据集进行散列分区。键可以被指定为位置键、表达式键和键选择函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">partitionByHash</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Range-Partition</li>
</ul>
<p>在给定的键上按照范围分割数据集。键可以被指定为位置键、表达式键和键选择函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">partitionByRange</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>自定义分区</li>
</ul>
<p>使用自定义的 <code>Partitioner</code> 函数，根据键将记录分配到特定的分区。键可以指定为位置键、表达式键和键选择函数。
注意：此方法仅适用于单个字段键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span>
  <span class="o">.</span><span class="n">partitionCustom</span><span class="o">(</span><span class="n">partitioner</span><span class="o">,</span> <span class="n">key</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Sort Partition</li>
</ul>
<p>按照指定的顺序对数据集的所有分区进行本地排序。字段可以指定为元组位置或字段表达式。对多个字段的排序是通过链式 <code>sortPartition()</code> 调用完成的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>First-n</li>
</ul>
<p>返回一个数据集的前 n 个（任意）元素。First-n 可以应用于一个常规数据集、一个分组数据集或一个分组排序数据集。分组键可以指定为键选择函数、元组位置或 case 类字段。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// regular data set
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
<span class="c1">// grouped data set
</span><span class="c1"></span><span class="k">val</span> <span class="n">result2</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
<span class="c1">// grouped-sorted data set
</span><span class="c1"></span><span class="k">val</span> <span class="n">result3</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">sortGroup</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
</code></pre></div><p>以下转换可用于元组的数据集。</p>
<ul>
<li>MinBy / MaxBy</li>
</ul>
<p>从一组元组中选择一个元组，这些元组的一个或多个字段的值是最小的（最大的）。用于比较的字段必须是有效的键字段，即可比较。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。MinBy (MaxBy)可以应用于一个完整的数据集或一个分组数据集。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// a data set with a single tuple with minimum values for the Int and String fields.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
<span class="c1">// a data set with one tuple for each group with the minimum value for the Double field.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
                                             <span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>通过匿名模式匹配从 tuple、case 类和集合中提取，比如下面。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">temperature</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="o">}</span>
</code></pre></div><p>不受 API 开箱即用的支持。要使用这个功能，你应该使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/scala_api_extensions.html">Scala API 扩展</a>。</p>
<p>变换的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/parallel.html">并行度</a>可以通过 <code>setParallelism(int)</code> 来定义，而 <code>name(String)</code> 可以给变换指定一个自定义的名称，这对调试很有帮助。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#data-sources">数据源</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#data-sinks">数据接收器</a>也是如此。</p>
<p><code>withParameters(Configuration)</code> 传递 Configuration 对象，这些对象可以从用户函数里面的 <code>open()</code> 方法访问。</p>
<h2 id="指定键">指定键</h2>
<p>一些转换（join、coGroup、groupBy）需要在元素集合上定义一个键。其他转换（Reduce、GroupReduce、Aggregate）允许在应用之前将数据按键分组。</p>
<p>一个 DataSet 被分组为：</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;...&gt;</span> <span class="n">input</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;...&gt;</span> <span class="n">reduced</span> <span class="o">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="cm">/*define key here*/</span><span class="o">)</span>
  <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="cm">/*do something*/</span><span class="o">);</span>
</code></pre></div><p>Flink 的数据模型不是基于键值对的。因此，你不需要将数据集类型物理地打包成键和值。键是&quot;虚拟的&rdquo;：它们被定义为实际数据上的函数，以指导分组操作符。</p>
<h3 id="为元组定义键">为元组定义键</h3>
<p>最简单的情况是对 Tuple 的一个或多个字段进行分组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">keyed</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
</code></pre></div><p>元组在第一个字段（整数类型的字段）上进行分组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">grouped</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>在这里，我们将元组放在一个由第一个字段和第二个字段组成的复合键上。</p>
<p>关于嵌套 Tuple 的说明。如果你的 DataSet 有一个嵌套的元组，比如：</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple3</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Float</span><span class="o">&gt;,</span><span class="n">String</span><span class="o">,</span><span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">ds</span><span class="o">;</span>
</code></pre></div><p>指定 <code>groupBy(0)</code> 将使系统使用完整的 Tuple2 作为键（以 Integer 和 Float 为键）。如果要&quot;导航&quot;到嵌套的 Tuple2 中，就必须使用字段表达式键，下面将对其进行说明。</p>
<h3 id="使用字段表达式定义键">使用字段表达式定义键</h3>
<p>你可以使用基于字符串的字段表达式来引用嵌套的字段，并为分组、排序、连接(join)或 coGrouping 定义键。</p>
<p>字段表达式可以非常容易地选择（嵌套的）复合类型中的字段，如 Tuple 和 POJO 类型。</p>
<p>在下面的例子中，我们有一个有两个字段 &ldquo;word&rdquo; 和 &ldquo;count&rdquo; 的 WC POJO。要按字段 <code>word</code> 进行分组，我们只需将其名称传递给 <code>groupBy()</code> 函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO (Plain old Java Object)
</span><span class="c1"></span><span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="k">var</span> <span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="k">var</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span> <span class="k">this</span><span class="o">(</span><span class="s">&#34;&#34;</span><span class="o">,</span> <span class="mi">0L</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">)</span>

<span class="c1">// or, as a case class, which is less typing
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="字段表达式语法">字段表达式语法</h3>
<ul>
<li>
<p>通过字段名选择 POJO 字段。例如 &ldquo;user&rdquo; 指的是 POJO 类型的 &ldquo;user&rdquo; 字段。</p>
</li>
<li>
<p>通过 1-offset 字段名或 0-offset 字段索引来选择 Tuple 字段。例如 &ldquo;_1&rdquo; 和 &ldquo;5&rdquo; 分别指 Scala Tuple 类型的第一和第六字段。</p>
</li>
</ul>
<p>你可以在 POJO 和 Tuple 中选择嵌套字段。例如 &ldquo;user.zip&rdquo; 指的是 POJO 的 &ldquo;zip&rdquo; 字段，它存储在 POJO 类型的 &ldquo;user&rdquo; 字段中。支持 POJO 和 Tuple 的任意嵌套和混合，如 &ldquo;_2.user.zip&rdquo; 或 &ldquo;user._4.1.zip&rdquo;。</p>
<p>你可以使用 &ldquo;_&rdquo; 通配符表达式选择完整的类型。这也适用于不是 Tuple 或 POJO 类型的类型。</p>
<p>字段表达式示例:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="k">var</span> <span class="n">complex</span><span class="k">:</span> <span class="kt">ComplexNestedClass</span><span class="o">,</span> <span class="k">var</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span> <span class="k">this</span><span class="o">(</span><span class="kc">null</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span>

<span class="k">class</span> <span class="nc">ComplexNestedClass</span><span class="o">(</span>
    <span class="k">var</span> <span class="n">someNumber</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>
    <span class="n">someFloat</span><span class="k">:</span> <span class="kt">Float</span><span class="o">,</span>
    <span class="n">word</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">,</span> <span class="nc">String</span><span class="o">),</span>
    <span class="n">hadoopCitizen</span><span class="k">:</span> <span class="kt">IntWritable</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span> <span class="k">this</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="s">&#34;&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">IntWritable</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span> <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>这些都是上面例子代码的有效字段表达式。</p>
<ul>
<li>
<p>&ldquo;count&rdquo;: WC 类中的计数字段</p>
</li>
<li>
<p>&ldquo;complex&rdquo;: 递归选择 POJO 类型 <code>ComplexNestedClass</code> 的 <code>complex</code> 字段的所有字段。</p>
</li>
<li>
<p>&ldquo;complex.word._3&rdquo;: 选择嵌套的 Tuple3 的最后一个字段。</p>
</li>
<li>
<p>&ldquo;complex.hadoopCitizen&rdquo;: 选择 Hadoop <code>IntWritable</code> 类型。</p>
</li>
</ul>
<h3 id="使用键选择函数定义键">使用键选择函数定义键</h3>
<p>另一种定义键的方法是&quot;键选择器&quot;函数。键选择器函数将一个元素作为输入，并返回该元素的键。键可以是任何类型的，并且可以从确定性计算中得到。</p>
<p>下面的例子显示了一个简单返回对象字段的键选择函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary case class
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">keyed</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span> <span class="k">_</span><span class="o">.</span><span class="n">word</span> <span class="o">)</span>
</code></pre></div><h2 id="数据源">数据源</h2>
<p>数据源创建初始数据集，例如从文件或 Java 集合中创建。创建数据集的一般机制是在 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/io/InputFormat.java">InputFormat</a> 后面抽象出来的。Flink 自带了几种内置的格式来从常见的文件格式创建数据集。其中许多格式在 ExecutionEnvironment 上有快捷方法。</p>
<p>基于文件的:</p>
<ul>
<li>
<p><code>readTextFile(path) / TextInputFormat</code> - 读取文件并以字符串形式返回。</p>
</li>
<li>
<p><code>readTextFileWithValue(path) / TextValueInputFormat</code> - 以行的方式读取文件并以 <code>StringValues</code> 的形式返回。<code>StringValues</code> 是可变字符串。</p>
</li>
<li>
<p><code>readCsvFile(path) / CsvInputFormat</code> - 解析以逗号（或其他字符）分隔的文件。返回一个由 tuple、case 类对象或 POJOs 组成的 DataSet。支持基本的 java 类型及其对应的 Value 类型作为字段类型。</p>
</li>
<li>
<p><code>readFileOfPrimitives(path, delimiter) / PrimitiveInputFormat</code> - 使用给定的定界符，解析新行（或其他字符序列）定界的基元数据类型的文件，如 String 或 Integer。</p>
</li>
<li>
<p><code>readSequenceFile(Key, Value, path) / SequenceFileInputFormat</code> - 创建一个 JobConf 并从指定的路径读取文件，文件类型为 <code>SequenceFileInputFormat</code>，Key 类和 Value 类，并以 <code>Tuple2&lt;Key, Value&gt;</code> 的形式返回。</p>
</li>
</ul>
<p>基于集合的:</p>
<ul>
<li>
<p><code>fromCollection(Iterable)</code> - 从一个 Iterable 创建一个数据集。Iterable 返回的所有元素必须是相同的类型。</p>
</li>
<li>
<p><code>fromCollection(Iterator)</code> - 从一个 Iterator 创建一个数据集。该类指定了迭代器返回的元素的数据类型。</p>
</li>
<li>
<p><code>fromElements(elements: _*)</code> - 从给定的对象序列中创建一个数据集。所有对象必须是相同的类型。</p>
</li>
<li>
<p><code>fromParallelCollection(SplittableIterator)</code> - 从迭代器中并行创建一个数据集。该类指定了迭代器返回的元素的数据类型。</p>
</li>
<li>
<p><code>generateSequence(from, to)</code> - 在给定的区间内并行生成数字序列。</p>
</li>
</ul>
<p>通用的:</p>
<ul>
<li>
<p><code>readFile(inputFormat, path) / FileInputFormat</code> - 接受一个文件输入格式。</p>
</li>
<li>
<p><code>createInput(inputFormat) / InputFormat</code> - 接受一个通用的输入格式。</p>
</li>
</ul>
<p>示例:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span>  <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// read text file from local files system
</span><span class="c1"></span><span class="k">val</span> <span class="n">localLines</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;file:///path/to/my/textfile&#34;</span><span class="o">)</span>

<span class="c1">// read text file from an HDFS running at nnHost:nnPort
</span><span class="c1"></span><span class="k">val</span> <span class="n">hdfsLines</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;hdfs://nnHost:nnPort/path/to/my/textfile&#34;</span><span class="o">)</span>

<span class="c1">// read a CSV file with three fields
</span><span class="c1"></span><span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)](</span><span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">)</span>

<span class="c1">// read a CSV file with five fields, taking only two of them
</span><span class="c1"></span><span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)](</span>
  <span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">,</span>
  <span class="n">includedFields</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">3</span><span class="o">))</span> <span class="c1">// take the first and the fourth field
</span><span class="c1"></span>
<span class="c1">// CSV input can also be used with Case Classes
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">MyCaseClass</span><span class="o">(</span><span class="n">str</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">dbl</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>
<span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[</span><span class="kt">MyCaseClass</span><span class="o">](</span>
  <span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">,</span>
  <span class="n">includedFields</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">3</span><span class="o">))</span> <span class="c1">// take the first and the fourth field
</span><span class="c1"></span>
<span class="c1">// read a CSV file with three fields into a POJO (Person) with corresponding fields
</span><span class="c1"></span><span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[</span><span class="kt">Person</span><span class="o">](</span>
  <span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">,</span>
  <span class="n">pojoFields</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">&#34;name&#34;</span><span class="o">,</span> <span class="s">&#34;age&#34;</span><span class="o">,</span> <span class="s">&#34;zipcode&#34;</span><span class="o">))</span>

<span class="c1">// create a set from some given elements
</span><span class="c1"></span><span class="k">val</span> <span class="n">values</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="s">&#34;Foo&#34;</span><span class="o">,</span> <span class="s">&#34;bar&#34;</span><span class="o">,</span> <span class="s">&#34;foobar&#34;</span><span class="o">,</span> <span class="s">&#34;fubar&#34;</span><span class="o">)</span>

<span class="c1">// generate a number sequence
</span><span class="c1"></span><span class="k">val</span> <span class="n">numbers</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">generateSequence</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">10000000</span><span class="o">)</span>

<span class="c1">// read a file from the specified path of type SequenceFileInputFormat
</span><span class="c1"></span><span class="k">val</span> <span class="n">tuples</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">createInput</span><span class="o">(</span><span class="nc">HadoopInputs</span><span class="o">.</span><span class="n">readSequenceFile</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">IntWritable</span><span class="o">],</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">Text</span><span class="o">],</span>
 <span class="s">&#34;hdfs://nnHost:nnPort/path/to/file&#34;</span><span class="o">))</span>
</code></pre></div><h3 id="配置-csv-解析">配置 CSV 解析</h3>
<p>Flink 为 CSV 解析提供了许多配置选项。</p>
<ul>
<li>
<p>lineDelimiter: 字符串指定单个记录的定界符。默认的行定界符是新行字符 <code>'/n'</code>。</p>
</li>
<li>
<p>fieldDelimiter: 字符串指定分隔记录字段的定界符。默认的字段定界符是逗号字符 <code>','</code>。</p>
</li>
<li>
<p>includeFields: <code>Array[Int]</code> 定义从输入文件中读取哪些字段（以及忽略哪些字段）。默认情况下，前 n 个字段（由 <code>type()</code> 调用中的类型数定义）会被解析。</p>
</li>
<li>
<p>pojoFields: <code>Array[String]</code> 指定 POJO 的字段，这些字段被映射到 CSV 字段。CSV 字段的解析器会根据 POJO 字段的类型和顺序自动初始化。</p>
</li>
<li>
<p>parseQuotedStrings: 启用引号字符串解析的字符。如果字符串字段的第一个字符是引号字符，那么字符串将被解析为引号字符串（前导或尾部的空白不被修剪）。引号字符串中的字段定界符会被忽略。如果引号字符串字段的最后一个字符不是引号字符，则引号字符串解析失败。如果启用了引号字符串解析，且字段的第一个字符不是引号字符串，则该字符串将被解析为未引号字符串。默认情况下，引号字符串解析被禁用。</p>
</li>
<li>
<p>ignoreComments: 字符串指定一个注解前缀。所有以指定注解前缀开始的行都不会被解析和忽略。默认情况下，没有行被忽略。</p>
</li>
<li>
<p>lenient：布尔值，启用宽松解析。也就是说，不能正确解析的行会被忽略。默认情况下，禁用宽松解析，无效行会引发异常。</p>
</li>
<li>
<p>ignoreFirstLine: Boolean 配置 InputFormat 忽略输入文件的第一行。默认情况下，没有行被忽略。</p>
</li>
</ul>
<h3 id="input-path-的递归遍历">Input Path 的递归遍历</h3>
<p>对于基于文件的输入，当输入路径是一个目录时，默认情况下不会枚举嵌套文件。取而代之的是，只读取基础目录内的文件，而忽略嵌套文件。嵌套文件的递归枚举可以通过 <code>recursive.file.enumeration</code> 配置参数启用，就像下面的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// enable recursive enumeration of nested input files
</span><span class="c1"></span><span class="k">val</span> <span class="n">env</span>  <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// create a configuration object
</span><span class="c1"></span><span class="k">val</span> <span class="n">parameters</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span>

<span class="c1">// set the recursive enumeration parameter
</span><span class="c1"></span><span class="n">parameters</span><span class="o">.</span><span class="n">setBoolean</span><span class="o">(</span><span class="s">&#34;recursive.file.enumeration&#34;</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>

<span class="c1">// pass the configuration to the data source
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;file:///path/with.nested/files&#34;</span><span class="o">).</span><span class="n">withParameters</span><span class="o">(</span><span class="n">parameters</span><span class="o">)</span>
</code></pre></div><h3 id="读取压缩文件">读取压缩文件</h3>
<p>Flink 目前支持输入文件的透明解压，如果这些文件被标记为适当的文件扩展名。特别是，这意味着无需进一步配置输入格式，任何 <code>FileInputFormat</code> 都支持压缩，包括自定义输入格式。请注意，压缩文件可能不会被并行读取，从而影响作业的可扩展性。</p>
<p>下表列出了当前支持的压缩方法。</p>
<table>
<thead>
<tr>
<th style="text-align:left">压缩方法</th>
<th style="text-align:left">文件后缀</th>
<th style="text-align:left">并行性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">DEFLATE</td>
<td style="text-align:left">.deflate</td>
<td style="text-align:left">no</td>
</tr>
<tr>
<td style="text-align:left">GZip</td>
<td style="text-align:left">.gz, .gzip</td>
<td style="text-align:left">no</td>
</tr>
<tr>
<td style="text-align:left">Bzip2</td>
<td style="text-align:left">.bz2</td>
<td style="text-align:left">no</td>
</tr>
<tr>
<td style="text-align:left">XZ</td>
<td style="text-align:left">.xz</td>
<td style="text-align:left">no</td>
</tr>
</tbody>
</table>
<h2 id="数据接收器">数据接收器</h2>
<p>数据接收器消费 DataSet 并用于存储或返回它们。数据接收器的操作是用 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/io/OutputFormat.java">OutputFormat</a> 来描述的。Flink 带有各种内置的输出格式，这些格式被封装在对 DataSet 的操作后面。</p>
<ul>
<li><code>writeAsText() / TextOutputFormat</code> &ndash;将元素逐行写成 Strings。通过调用每个元素的 <code>toString()</code> 方法获得字符串。</li>
<li><code>writeAsCsv(...) / CsvOutputFormat</code> - 将元组写成逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的 <code>toString()</code> 方法。</li>
<li><code>print() / printToErr()</code> - 在标准输出/标准错误流上打印每个元素的 <code>toString()</code> 值。</li>
<li><code>write() / FileOutputFormat</code> - 用于自定义文件输出的方法和基类。支持自定义对象到字节的转换。</li>
<li><code>output()/ OutputFormat</code> - 最通用的输出方法，用于非基于文件的数据接收器（如将结果存储在数据库中）。</li>
</ul>
<p>一个 DataSet 可以被输入到多个操作中。程序可以写入或打印一个数据集，同时还可以对其进行额外的转换。</p>
<p><strong>示例</strong></p>
<p>标准数据接收器方法:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// text data
</span><span class="c1"></span><span class="k">val</span> <span class="n">textData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// write DataSet to a file on the local file system
</span><span class="c1"></span><span class="n">textData</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///my/result/on/localFS&#34;</span><span class="o">)</span>

<span class="c1">// write DataSet to a file on an HDFS with a namenode running at nnHost:nnPort
</span><span class="c1"></span><span class="n">textData</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;hdfs://nnHost:nnPort/my/result/on/localFS&#34;</span><span class="o">)</span>

<span class="c1">// write DataSet to a file and overwrite the file if it exists
</span><span class="c1"></span><span class="n">textData</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///my/result/on/localFS&#34;</span><span class="o">,</span> <span class="nc">WriteMode</span><span class="o">.</span><span class="nc">OVERWRITE</span><span class="o">)</span>

<span class="c1">// tuples as lines with pipe as the separator &#34;a|b|c&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">values</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">values</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="s">&#34;file:///path/to/the/result/file&#34;</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34;|&#34;</span><span class="o">)</span>

<span class="c1">// this writes tuples in the text formatting &#34;(a, b, c)&#34;, rather than as CSV lines
</span><span class="c1"></span><span class="n">values</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///path/to/the/result/file&#34;</span><span class="o">)</span>

<span class="c1">// this writes values as strings using a user-defined formatting
</span><span class="c1"></span><span class="n">values</span> <span class="n">map</span> <span class="o">{</span> <span class="n">tuple</span> <span class="k">=&gt;</span> <span class="n">tuple</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="s">&#34; - &#34;</span> <span class="o">+</span> <span class="n">tuple</span><span class="o">.</span><span class="n">_2</span> <span class="o">}</span>
  <span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///path/to/the/result/file&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="本地排序输出">本地排序输出</h3>
<p>数据接收器的输出可以使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-for-tuples">元组字段位置</a>或<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>对指定字段按指定顺序进行本地排序。这适用于每一种输出格式。</p>
<p>下面的示例展示了如何使用该功能。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">tData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">pData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">BookPojo</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">sData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// sort output on String field in ascending order
</span><span class="c1"></span><span class="n">tData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>

<span class="c1">// sort output on Double field in descending and Int field in ascending order
</span><span class="c1"></span><span class="n">tData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">).</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>

<span class="c1">// sort output on the &#34;author&#34; field of nested BookPojo in descending order
</span><span class="c1"></span><span class="n">pData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="s">&#34;_1.author&#34;</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">).</span><span class="n">writeAsText</span><span class="o">(...)</span>

<span class="c1">// sort output on the full tuple in ascending order
</span><span class="c1"></span><span class="n">tData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="s">&#34;_&#34;</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">writeAsCsv</span><span class="o">(...)</span>

<span class="c1">// sort atomic type (String) output in descending order
</span><span class="c1"></span><span class="n">sData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="s">&#34;_&#34;</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">).</span><span class="n">writeAsText</span><span class="o">(...)</span>
</code></pre></div><p>目前还不支持全局排序输出。</p>
<h2 id="迭代运算符">迭代运算符</h2>
<p>迭代在  Flink 程序中实现了循环。迭代运算符封装了程序的一部分，并反复执行，将一次迭代的结果（部分解）反馈到下一次迭代中。Flink 中的迭代有两种类型。<code>BulkIteration</code> 和 <code>DeltaIteration</code>。</p>
<p>本节提供了如何使用这两种运算符的快速示例。查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">迭代介绍页面</a>可以获得更详细的介绍。</p>
<h3 id="批量迭代">批量迭代</h3>
<p>要创建一个 <code>BulkIteration</code>，调用迭代开始的 DataSet 的 <code>iterate(int)</code> 方法，同时指定一个 <code>step</code> 函数。<code>step</code> 函数获取当前迭代的输入 DataSet，并且必须返回一个新的 DataSet。迭代调用的参数是最大的迭代次数，迭代过后要停止。</p>
<p>还有 <code>iterateWithTermination(int)</code> 函数，接受 <code>step</code> 函数，返回两个 DataSets。迭代步骤的结果和一个终止标准。一旦终止准则 DataSet 为空，就会停止迭代。</p>
<p>下面的例子是迭代估计数字 Pi。目标是计算随机点的数量，这些随机点落入单位圆中。在每一次迭代中，都会挑选一个随机点。如果这个点位于单位圆内，我们就递增计数。然后，Pi 的估计值是所得到的计数除以迭代次数乘以 4。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>

<span class="c1">// Create initial DataSet
</span><span class="c1"></span><span class="k">val</span> <span class="n">initial</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="n">count</span> <span class="k">=</span> <span class="n">initial</span><span class="o">.</span><span class="n">iterate</span><span class="o">(</span><span class="mi">10000</span><span class="o">)</span> <span class="o">{</span> <span class="n">iterationInput</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">iterationInput</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">i</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">x</span> <span class="k">=</span> <span class="nc">Math</span><span class="o">.</span><span class="n">random</span><span class="o">()</span>
    <span class="k">val</span> <span class="n">y</span> <span class="k">=</span> <span class="nc">Math</span><span class="o">.</span><span class="n">random</span><span class="o">()</span>
    <span class="n">i</span> <span class="o">+</span> <span class="o">(</span><span class="k">if</span> <span class="o">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="o">)</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="n">result</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">count</span> <span class="n">map</span> <span class="o">{</span> <span class="n">c</span> <span class="k">=&gt;</span> <span class="n">c</span> <span class="o">/</span> <span class="mf">10000.0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">}</span>

<span class="n">result</span><span class="o">.</span><span class="n">print</span><span class="o">()</span>

<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">(</span><span class="s">&#34;Iterative Pi Example&#34;</span><span class="o">)</span>
</code></pre></div><p>你也可以查看 <a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/clustering/KMeans.scala">K-Means</a> 的例子，它使用 <code>BulkIteration</code> 来聚类一组未标记的点。</p>
<h3 id="增量迭代">增量迭代</h3>
<p>增量迭代利用了某些算法在每次迭代中不改变解的每个数据点的事实。</p>
<p>除了在每次迭代中反馈的部分解（称为 workset），delta 迭代还保持着跨迭代的状态（称为解集），可以通过 delta 更新。迭代计算的结果是最后一次迭代后的状态。关于 delta 迭代的基本原理，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">迭代简介</a>。</p>
<p>定义 <code>DeltaIteration</code> 与定义 <code>BulkIteration</code> 类似。对于 delta 迭代，两个数据集构成了每次迭代的输入（工作集和解集），并且在每次迭代中产生两个数据集作为结果（新工作集，解集 delta）。</p>
<p>要创建一个 DeltaIteration 在初始解集上调用 <code>iterateDelta(initialWorkset，maxIterations，key)</code>。<code>step</code> 函数需要两个参数。(solutionSet, workset), 并且必须返回两个值: (solutionSetDelta, newWorkset).</p>
<p>下面是一个 delta 迭代语法的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// read the initial data sets
</span><span class="c1"></span><span class="k">val</span> <span class="n">initialSolutionSet</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">initialWorkset</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">maxIterations</span> <span class="k">=</span> <span class="mi">100</span>
<span class="k">val</span> <span class="n">keyPosition</span> <span class="k">=</span> <span class="mi">0</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">initialSolutionSet</span><span class="o">.</span><span class="n">iterateDelta</span><span class="o">(</span><span class="n">initialWorkset</span><span class="o">,</span> <span class="n">maxIterations</span><span class="o">,</span> <span class="nc">Array</span><span class="o">(</span><span class="n">keyPosition</span><span class="o">))</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">solution</span><span class="o">,</span> <span class="n">workset</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">candidateUpdates</span> <span class="k">=</span> <span class="n">workset</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="nc">ComputeCandidateChanges</span><span class="o">())</span>
    <span class="k">val</span> <span class="n">deltas</span> <span class="k">=</span> <span class="n">candidateUpdates</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">solution</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)(</span><span class="k">new</span> <span class="nc">CompareChangesToCurrent</span><span class="o">())</span>

    <span class="k">val</span> <span class="n">nextWorkset</span> <span class="k">=</span> <span class="n">deltas</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">new</span> <span class="nc">FilterByThreshold</span><span class="o">())</span>

    <span class="o">(</span><span class="n">deltas</span><span class="o">,</span> <span class="n">nextWorkset</span><span class="o">)</span>
<span class="o">}</span>

<span class="n">result</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">outputPath</span><span class="o">)</span>

<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><h2 id="在函数中对数据对象进行操作">在函数中对数据对象进行操作</h2>
<p>Flink 的运行时以 Java 对象的形式与用户函数交换数据。函数从运行时接收输入对象作为方法参数，并返回输出对象作为结果。因为这些对象是由用户函数和运行时代码访问的，所以理解和遵循用户代码如何访问，即读取和修改这些对象的规则是非常重要的。</p>
<p>用户函数以常规方法参数（如 MapFunction）或通过 Iterable 参数（如 GroupReduceFunction）从 Flink 的运行时接收对象。我们把运行时传递给用户函数的对象称为输入对象。用户函数可以将对象作为方法返回值（像 MapFunction）或通过 Collector（像 FlatMapFunction）发射给 Flink 运行时。我们将用户函数向运行时发射的对象称为输出对象。</p>
<p>Flink 的 DataSet API 具有两种模式，它们在 Flink 的运行时如何创建或重用输入对象方面有所不同。这种行为会影响用户函数如何与输入和输出对象交互的保证和约束。下面的章节定义了这些规则，并给出了编写安全用户函数代码的编码指南。</p>
<h3 id="禁用对象重用default">禁用对象重用(DEFAULT)</h3>
<p>默认情况下，Flink 在禁用对象重用模式下运行。这种模式可以保证函数在函数调用中总是接收新的输入对象。对象重用禁用模式能提供更好的保证，使用起来也更安全。但是，它有一定的处理开销，可能会引起较高的 Java 垃圾收集活动。下表解释了在禁用对象重用模式下，用户函数如何访问输入和输出对象。</p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">保证和限制</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">读取输入对象</td>
<td style="text-align:left">在一个方法调用中，保证输入对象的值不会改变。这包括由 Iterable 服务的对象。例如，在 List 或 Map 中收集由 Iterable 服务的输入对象是安全的。请注意，在方法调用离开后，对象可能会被修改。跨函数调用记忆对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">修改输入对象</td>
<td style="text-align:left">你可以修改输入对象。</td>
</tr>
<tr>
<td style="text-align:left">发射输入对象</td>
<td style="text-align:left">你可以发射输入对象。输入对象的值可能在发射后发生变化。读取发射后的输入对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">读取输出对象</td>
<td style="text-align:left">给予收集器的对象或作为方法结果返回的对象可能已经改变了其值。读取输出对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">修改输出对象</td>
<td style="text-align:left">你可以在对象被发射后对其进行修改，然后再次发射。</td>
</tr>
</tbody>
</table>
<p>对象重用禁用（默认）模式的编码准则。</p>
<ul>
<li>不要跨方法调用记忆和读取输入对象。</li>
<li>不要在发出对象后读取对象。</li>
</ul>
<h3 id="启用对象重用">启用对象重用</h3>
<p>在启用对象重用模式下，Flink 的运行时会尽量减少对象实例化的数量。这可以提高性能，并且可以减少 Java 垃圾收集的压力。通过调用 <code>ExecutionConfig.enableObjectReuse()</code> 激活对象重用启用模式。下表解释了在启用对象重用模式下，用户函数如何访问输入和输出对象。</p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">保证和限制</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">读取作为常规方法参数接收的输入对象</td>
<td style="text-align:left">作为常规方法参数接收的输入对象在一次函数调用中不被修改。对象可能在方法调用结束后被修改。跨函数调用记忆对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">读取从 Iterable 参数中接收到的输入对象</td>
<td style="text-align:left">从 Iterable 中接收到的输入对象只在调用 next()方法之前有效。一个 Iterable 或 Iterator 可以多次服务于同一个对象实例。记住从 Iterable 接收的输入对象是不安全的，例如，把它们放在 List 或 Map 中。</td>
</tr>
<tr>
<td style="text-align:left">修改输入对象</td>
<td style="text-align:left">除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(reuse)的输入对象外，你不得修改输入对象。</td>
</tr>
<tr>
<td style="text-align:left">发射输入对象</td>
<td style="text-align:left">除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(重用)的输入对象外，你不得发射输入对象。</td>
</tr>
<tr>
<td style="text-align:left">读取输出对象</td>
<td style="text-align:left">一个被交给 Collector 或作为方法结果返回的对象可能已经改变了它的值。读取输出对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">修改输出对象</td>
<td style="text-align:left">你可以修改一个输出对象并再次发出它。</td>
</tr>
</tbody>
</table>
<p>启用对象重用的编码准则。</p>
<ul>
<li>不记忆从 Iterable 接收的输入对象。</li>
<li>不记忆和读取跨方法调用的输入对象。</li>
<li>除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(reuse)的输入对象外，不要修改或发出输入对象。</li>
<li>为了减少对象实例化，你总是可以发出一个专门的输出对象，这个对象被反复修改，但从不读取。</li>
</ul>
<h2 id="调试">调试</h2>
<p>在分布式集群中的大型数据集上运行数据分析程序之前，最好确保所实现的算法能够按照预期的方式运行。因此，实现数据分析程序通常是一个检查结果、调试和改进的渐进过程。</p>
<p>Flink 提供了一些不错的功能，通过支持 IDE 内的本地调试、注入测试数据和收集结果数据，大大简化了数据分析程序的开发过程。本节给大家一些提示，如何简化 Flink 程序的开发。</p>
<h3 id="本地执行环境">本地执行环境</h3>
<p>LocalEnvironment 在它创建的同一个 JVM 进程中启动 Flink 系统。如果你从 IDE 中启动 LocalEnvironment，你可以在代码中设置断点，轻松调试你的程序。</p>
<p>LocalEnvironment 的创建和使用方法如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">createLocalEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="n">pathToTextFile</span><span class="o">)</span>
<span class="c1">// build your program
</span><span class="c1"></span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><h3 id="收集数据源和接收器">收集数据源和接收器</h3>
<p>为分析程序提供输入并检查其输出，如果通过创建输入文件和读取输出文件来完成，是很麻烦的。Flink 具有特殊的数据源和接收器，这些数据源和接收器由 Java 集合支持，以方便测试。一旦程序经过测试，源和接收器可以很容易地被从 HDFS 等外部数据存储中读取/写入的源和接收器所替代。</p>
<p>集合数据源可以使用以下方式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">createLocalEnvironment</span><span class="o">()</span>

<span class="c1">// Create a DataSet from a list of elements
</span><span class="c1"></span><span class="k">val</span> <span class="n">myInts</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">)</span>

<span class="c1">// Create a DataSet from any Collection
</span><span class="c1"></span><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">myTuples</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="c1">// Create a DataSet from an Iterator
</span><span class="c1"></span><span class="k">val</span> <span class="n">longIt</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">myLongs</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="n">longIt</span><span class="o">)</span>
</code></pre></div><p>注：目前，集合数据源要求数据类型和迭代器实现 Serializable。此外，集合数据源不能并行执行（ parallelism = 1）。</p>
<h2 id="语义注解">语义注解</h2>
<p>语义注解可以用来给 Flink 提供关于函数行为的提示。它们告诉系统，函数读取并评估了函数输入的哪些字段，以及它将哪些字段从输入转发到输出，而没有进行修改。语义注解是加快执行速度的有力手段，因为它们允许系统推理出在多个操作中重复使用排序顺序或分区的问题。使用语义注解最终可能会使程序免于不必要的数据洗牌或不必要的排序，并显著提高程序的性能。</p>
<p>注意：语义注解的使用是可选的。然而，在提供语义注解时，保守地使用语义注解是绝对关键的! 不正确的语义注解将导致 Flink 对你的程序做出不正确的假设，并可能最终导致不正确的结果。如果一个操作符的行为不是明确可预测的，就不应该提供注解。请仔细阅读文档。</p>
<p>目前支持以下语义注解。</p>
<p><strong>转发字段注解</strong></p>
<p>转发字段信息声明了未被修改的输入字段被函数转发到输出中的同一位置或另一位置。该信息被优化器用来推断数据属性（如排序或分区）是否被函数保留。对于对输入元素组进行操作的函数，如 GroupReduce、GroupCombine、CoGroup 和 MapPartition，所有被定义为转发字段的字段必须总是从同一个输入元素联合转发。由组智函数发出的每个元素的转发字段可能来源于函数的输入组的不同元素。</p>
<p>字段转发信息使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>来指定。在输出中转发到同一位置的字段可以通过其位置来指定。指定的位置必须对输入和输出的数据类型有效，并具有相同的类型。例如字符串 &ldquo;f2 &ldquo;声明 Java 输入元组的第三个字段总是等于输出元组中的第三个字段。</p>
<p>将输入中的源字段和输出中的目标字段指定为字段表达式，就可以声明未修改的字段转发到输出中的另一个位置。字符串 <code>&quot;f0-&gt;f2&quot;</code> 表示将 Java 输入元组的第一个字段不变的复制到 Java 输出元组的第三个字段。通配符表达式 <code>*</code> 可以用来指代整个输入或输出类型，即 <code>&quot;f0-&gt;*&quot;</code> 表示一个函数的输出总是等于其 Java 输入元组的第一个字段。</p>
<p>多个转发字段可以在一个字符串中用分号隔开声明为 <code>&quot;f0; f2-&gt;f1; f3-&gt;f2&quot;</code>，也可以在单独的字符串中声明为 &ldquo;f0&rdquo;、&ldquo;f2-&gt;f1&rdquo;、&ldquo;f3-&gt;f2&rdquo;。当指定转发字段时，不要求所有的转发字段都声明，但所有的声明必须正确。</p>
<p>转发字段信息可以通过在函数类定义上附加 Java 注解来声明，或者在调用 DataSet 上的函数后将其作为操作符参数传递，如下图所示。</p>
<p><strong>函数类注解</strong></p>
<ul>
<li><code>@ForwardedFields</code> 用于单输入的函数，如 Map 和 Reduce。</li>
<li><code>@ForwardedFieldsFirst</code> 代表有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>@ForwardedFieldsSecond</code> 代表有两个输入的函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p><strong>操作符参数</strong></p>
<ul>
<li><code>data.map(myMapFnc).withForwardedFields()</code> 用于单输入的函数，如 Map 和 Reduce。</li>
<li><code>data1.join(data2).where().equalTo().with(myJoinFnc).withForwardFieldsFirst()</code> 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>data1.join(data2).where().equalTo().with(myJoinFnc).withForwardFieldsSecond()</code> 用于有两个输入的函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p>请注意，不可能覆盖通过操作符参数指定为类注解的字段前向信息。</p>
<p>例子：在函数的第二个输入端，如 Join 和 CoGroup，请注意不能覆盖通过运算符参数指定的类注解的字段前向信息。</p>
<p>下面的例子显示了如何使用函数类注解来声明转发的字段信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nd">@ForwardedFields</span><span class="o">(</span><span class="s">&#34;_1-&gt;_3&#34;</span><span class="o">)</span>
<span class="k">class</span> <span class="nc">MyMap</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]{</span>
   <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">))</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">,</span> <span class="nc">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">return</span> <span class="o">(</span><span class="s">&#34;foo&#34;</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span> <span class="o">/</span> <span class="mi">2</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>非转发字段</strong></p>
<p>非转发字段信息声明了所有在函数输出中不保留在同一位置的字段。所有其他字段的值都被认为保留在输出的同一位置。因此，非转发字段信息与转发字段信息是相反的。分组运算符（如 GroupReduce、GroupCombine、CoGroup 和 MapPartition）的非转发字段信息必须满足与转发字段信息相同的要求。</p>
<p>重要：非转发字段信息的规范是可选的。但是如果使用，必须指定 <strong>ALL!</strong> 非转发字段，因为所有其他字段都被认为是原地转发的。将一个转发字段声明为非转发字段是安全的。</p>
<p>非转发字段被指定为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>的列表。这个列表既可以是由分号分隔的字段表达式组成的单个字符串，也可以是多个字符串。例如 &ldquo;f1; f3&rdquo; 和 &ldquo;f1&rdquo;、&ldquo;f3&rdquo; 都声明 Java 元组的第二个和第四个字段不保留在原地，其他所有字段都保留在原地。非前向字段信息只能为输入和输出类型相同的函数指定。</p>
<p>非转发字段信息是作为函数类注解使用以下注解来指定的。</p>
<ul>
<li><code>@NonForwardedFields</code> 用于单个输入函数，如 Map 和 Reduce。</li>
<li><code>@NonForwardedFieldsFirst</code> 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>@NonForwardedFieldsSecond</code> 用于函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p>例子</p>
<p>下面的例子显示了如何声明非转发字段信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nd">@NonForwardedFields</span><span class="o">(</span><span class="s">&#34;_2&#34;</span><span class="o">)</span> <span class="c1">// second field is not forwarded
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyMap</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]{</span>
  <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">))</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">return</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span> <span class="o">/</span> <span class="mi">2</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>读取字段</strong></p>
<p>读取字段信息声明了所有被函数访问和评估的字段，也就是说，所有被函数用来计算结果的字段。例如，在条件语句中被评估的字段或用于计算的字段必须在指定读取字段信息时被标记为读取。仅仅是未经修改就转发到输出而不评估其值的字段，或者根本没有被访问的字段都不被认为是读。</p>
<p>重要：读取字段信息的指定是可选的。但是如果使用，必须指定 <strong>ALL!</strong> 读取字段。将一个非读字段声明为读字段是安全的。</p>
<p>读取字段被指定为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>的列表。这个列表可以是一个由分号分隔的字段表达式组成的单个字符串，也可以是多个字符串。例如 &ldquo;f1; f3&rdquo; 和 &ldquo;f1&rdquo;、&ldquo;f3&rdquo; 都声明 Java 元组的第二和第四字段被函数读取和评估。</p>
<p>读取字段信息是以函数类注解的形式指定的，使用以下注解。</p>
<ul>
<li><code>@ReadFields</code> 用于单输入函数，如 Map 和 Reduce。</li>
<li><code>@ReadFieldsFirst</code> 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>@ReadFieldsSecond</code> 用于有两个输入的函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p>示例：</p>
<p>下面的例子显示了如何声明读取字段信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nd">@ReadFields</span><span class="o">(</span><span class="s">&#34;_1; _4&#34;</span><span class="o">)</span> <span class="c1">// _1 and _4 are read and evaluated by the function.
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyMap</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]{</span>
   <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">,</span> <span class="nc">Int</span><span class="o">,</span> <span class="nc">Int</span><span class="o">))</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_1</span> <span class="o">==</span> <span class="mi">42</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">return</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="k">return</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_4</span> <span class="o">+</span> <span class="mi">10</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h2 id="广播变量">广播变量</h2>
<p>广播变量允许你在操作的常规输入之外，将一个数据集提供给操作的所有并行实例。这对辅助数据集或数据依赖性参数化很有用。然后，该数据集将作为一个集合在操作者处被访问。</p>
<ul>
<li>广播：广播集通过 <code>withBroadcastSet(DataSet，String)</code> 按名称注册，并通过</li>
<li>访问方式：通过目标操作者处的 <code>getRuntimeContext().getBroadcastVariable(String)</code> 访问。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// 1. The DataSet to be broadcast
</span><span class="c1"></span><span class="k">val</span> <span class="n">toBroadcast</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>

<span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="s">&#34;b&#34;</span><span class="o">)</span>

<span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">new</span> <span class="nc">RichMapFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]()</span> <span class="o">{</span>
    <span class="k">var</span> <span class="n">broadcastSet</span><span class="k">:</span> <span class="kt">Traversable</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">config</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="c1">// 3. Access the broadcast DataSet as a Collection
</span><span class="c1"></span>      <span class="n">broadcastSet</span> <span class="k">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="n">getBroadcastVariable</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;broadcastSetName&#34;</span><span class="o">).</span><span class="n">asScala</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">in</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">{</span>
        <span class="o">...</span>
    <span class="o">}</span>
<span class="o">}).</span><span class="n">withBroadcastSet</span><span class="o">(</span><span class="n">toBroadcast</span><span class="o">,</span> <span class="s">&#34;broadcastSetName&#34;</span><span class="o">)</span> <span class="c1">// 2. Broadcast the DataSet
</span></code></pre></div><p>在注册和访问广播数据集时，确保名称（前面例子中的 <code>broadcastSetName</code>）匹配。关于完整的示例程序，可以看一下 <a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/clustering/KMeans.scala">KMeans 算法</a>。</p>
<p>注意：由于广播变量的内容在每个节点上都保存在内存中，所以它不应该变得太大。对于像标量值这样简单的东西，你可以简单地将参数作为函数闭包的一部分，或者使用 <code>withParameters(...)</code> 方法来传递配置。</p>
<h2 id="分布式缓存">分布式缓存</h2>
<p>Flink 提供了一个类似于 Apache Hadoop 的分布式缓存，以使用户函数的并行实例可以在本地访问文件。该功能可用于共享包含静态外部数据的文件，如字典或机器学习的回归模型。</p>
<p>缓存的工作原理如下。程序在其 <code>ExecutionEnvironment</code> 中以特定的名称将<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/connectors.html#reading-from-file-systems">本地或远程文件系统（如 HDFS 或 S3）</a>的文件或目录注册为缓存文件。当程序执行时，Flink 会自动将该文件或目录复制到所有工作者的本地文件系统中。用户函数可以查找指定名称下的文件或目录，并从工作者的本地文件系统中访问它。</p>
<p>分布式缓存的使用方法如下。</p>
<p>在 <code>ExecutionEnvironment</code> 中注册文件或目录。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// register a file from HDFS
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">registerCachedFile</span><span class="o">(</span><span class="s">&#34;hdfs:///path/to/your/file&#34;</span><span class="o">,</span> <span class="s">&#34;hdfsFile&#34;</span><span class="o">)</span>

<span class="c1">// register a local executable file (script, executable, ...)
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">registerCachedFile</span><span class="o">(</span><span class="s">&#34;file:///path/to/exec/file&#34;</span><span class="o">,</span> <span class="s">&#34;localExecFile&#34;</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>

<span class="c1">// define your program and execute
</span><span class="c1"></span><span class="o">...</span>
<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyMapper</span><span class="o">())</span>
<span class="o">...</span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><p>在一个用户函数（这里是 MapFunction）中访问缓存文件。该函数必须扩展一个 RichFunction 类，因为它需要访问 RuntimeContext。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// extend a RichFunction to have access to the RuntimeContext
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyMapper</span> <span class="k">extends</span> <span class="nc">RichMapFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">config</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>

    <span class="c1">// access cached file via RuntimeContext and DistributedCache
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">myFile</span><span class="k">:</span> <span class="kt">File</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">.</span><span class="n">getDistributedCache</span><span class="o">.</span><span class="n">getFile</span><span class="o">(</span><span class="s">&#34;hdfsFile&#34;</span><span class="o">)</span>
    <span class="c1">// read the file (or navigate the directory)
</span><span class="c1"></span>    <span class="o">...</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// use content of cached file
</span><span class="c1"></span>    <span class="o">...</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h2 id="向函数传递参数">向函数传递参数</h2>
<p>可以使用构造函数或 <code>withParameters(Configuration)</code> 方法将参数传递给函数。参数会被序列化为函数对象的一部分，并传送给所有并行任务实例。</p>
<p><strong>通过构造函数</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">toFilter</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>

<span class="n">toFilter</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyFilter</span><span class="o">(</span><span class="mi">2</span><span class="o">))</span>

<span class="k">class</span> <span class="nc">MyFilter</span><span class="o">(</span><span class="n">limit</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">FilterFunction</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">filter</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">value</span> <span class="o">&gt;</span> <span class="n">limit</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>通过 withParameters(配置)</strong></p>
<p>本方法以一个 Configuration 对象作为参数，它将被传递给<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html#rich-functions">富函数</a>的 <code>open()</code> 方法。配置对象是一个从 String 键到不同值类型的 Map。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">toFilter</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>

<span class="k">val</span> <span class="n">c</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">()</span>
<span class="n">c</span><span class="o">.</span><span class="n">setInteger</span><span class="o">(</span><span class="s">&#34;limit&#34;</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>

<span class="n">toFilter</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">new</span> <span class="nc">RichFilterFunction</span><span class="o">[</span><span class="kt">Int</span><span class="o">]()</span> <span class="o">{</span>
    <span class="k">var</span> <span class="n">limit</span> <span class="k">=</span> <span class="mi">0</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">config</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="n">limit</span> <span class="k">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getInteger</span><span class="o">(</span><span class="s">&#34;limit&#34;</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">filter</span><span class="o">(</span><span class="n">in</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">in</span> <span class="o">&gt;</span> <span class="n">limit</span>
    <span class="o">}</span>
<span class="o">}).</span><span class="n">withParameters</span><span class="o">(</span><span class="n">c</span><span class="o">)</span>
</code></pre></div><p><strong>在全局范围内通过 ExecutionConfig</strong></p>
<p>Flink 还允许将自定义配置值传递到环境的 ExecutionConfig 接口。由于执行配置可以在所有（丰富的）用户函数中访问，因此自定义配置将在所有函数中全局可用。</p>
<p>设置一个自定义的全局配置：</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">()</span>
<span class="n">conf</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;mykey&#34;</span><span class="o">,</span> <span class="s">&#34;myvalue&#34;</span><span class="o">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">getConfig</span><span class="o">.</span><span class="n">setGlobalJobParameters</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span>
</code></pre></div><p>请注意，你也可以传递一个扩展 <code>ExecutionConfig.GlobalJobParameters</code> 类的自定义类作为全局作业参数给执行配置。该接口允许实现 <code>Map&lt;String, String&gt; toMap()</code> 方法，该方法将在 web 前端显示来自配置的值。</p>
<p><strong>从全局配置中访问值</strong></p>
<p>全局工作参数中的对象在系统中的很多地方都可以访问。所有实现 RichFunction 接口的用户函数都可以通过运行时上下文访问。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kd">class</span> <span class="nc">Tokenizer</span> <span class="kd">extends</span> <span class="n">RichFlatMapFunction</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="o">{</span>

    <span class="kd">private</span> <span class="n">String</span> <span class="n">mykey</span><span class="o">;</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">open</span><span class="o">(</span><span class="n">Configuration</span> <span class="n">parameters</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
      <span class="kd">super</span><span class="o">.</span><span class="na">open</span><span class="o">(</span><span class="n">parameters</span><span class="o">);</span>
      <span class="n">ExecutionConfig</span><span class="o">.</span><span class="na">GlobalJobParameters</span> <span class="n">globalParams</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getExecutionConfig</span><span class="o">().</span><span class="na">getGlobalJobParameters</span><span class="o">();</span>
      <span class="n">Configuration</span> <span class="n">globConf</span> <span class="o">=</span> <span class="o">(</span><span class="n">Configuration</span><span class="o">)</span> <span class="n">globalParams</span><span class="o">;</span>
      <span class="n">mykey</span> <span class="o">=</span> <span class="n">globConf</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="s">&#34;mykey&#34;</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>
    <span class="o">}</span>
    <span class="c1">// ... more here ...
</span></code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/guide" term="guide" label="Guide" />
                            
                        
                    
                
            
        </entry>
    
</feed>
