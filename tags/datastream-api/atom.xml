<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-us">
    <generator uri="https://gohugo.io/" version="0.85.0">Hugo</generator><title type="html"><![CDATA[DataStream API on 焉知非鱼]]></title>
    
        <subtitle type="html"><![CDATA[rakulang, dartlang, nimlang, golang, rustlang, lang lang no see]]></subtitle>
    
    
    
            <link href="https://ohmyweekly.github.io/tags/datastream-api/" rel="alternate" type="text/html" title="HTML" />
            <link href="https://ohmyweekly.github.io/tags/datastream-api/index.xml" rel="alternate" type="application/rss+xml" title="RSS" />
            <link href="https://ohmyweekly.github.io/tags/datastream-api/atom.xml" rel="self" type="application/atom+xml" title="Atom" />
            <link href="https://ohmyweekly.github.io/tags/datastream-api/jf2feed.json" rel="alternate" type="application/jf2feed+json" title="jf2feed" />
    <updated>2021-07-12T22:13:54+08:00</updated>
    
    
    
    
        <id>https://ohmyweekly.github.io/tags/datastream-api/</id>
    
        
        <entry>
            <title type="html"><![CDATA[Java Lambda 表达式]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/?utm_source=atom_feed" rel="related" type="text/html" title="可查询状态" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Java Lambda Expressions</blockquote><h2 id="java-lambda-表达式">Java Lambda 表达式</h2>
<p>Java 8 引入了一些新的语言功能，旨在实现更快、更清晰的编码。其中最重要的功能是所谓的&quot;Lambda 表达式&quot;，它打开了函数式编程的大门。Lambda 表达式允许以一种直接的方式实现和传递函数，而无需声明额外的（匿名）类。</p>
<p>注意 Flink 支持对 Java API 的所有操作符使用 lambda 表达式，但是，每当 lambda 表达式使用 Java 属的时候，你需要明确地声明类型信息。</p>
<p>本文档展示了如何使用 lambda 表达式并描述了当前的限制。关于 Flink API 的一般介绍，请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">DataSteam API 概述</a>。</p>
<h3 id="例子和限制">例子和限制</h3>
<p>下面的例子说明了如何实现一个简单的内联 <code>map()</code> 函数，该函数使用 lambda 表达式对其输入进行平方化。<code>map()</code> 函数的输入 <code>i</code> 和输出参数的类型不需要声明，因为它们是由 Java 编译器推断的。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
<span class="c1">// returns the squared i
</span><span class="c1"></span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">i</span> <span class="o">-&gt;</span> <span class="n">i</span><span class="o">*</span><span class="n">i</span><span class="o">)</span>
<span class="o">.</span><span class="na">print</span><span class="o">();</span>
</code></pre></div><p>Flink 可以从方法签名 <code>OUT map(IN value)</code> 的实现中自动提取结果类型信息，因为 OUT 不是通用的，而是 Integer。</p>
<p>遗憾的是，像 <code>flatMap()</code> 这样签名为 <code>void flatMap(IN value, Collector&lt;OUT&gt; out)</code> 的函数被 Java 编译器编译成 <code>void flatMap(IN value, Collector out)</code>。这使得 Flink 无法自动推断输出类型的类型信息。</p>
<p>Flink 很可能会抛出一个类似下面的异常。</p>
<pre><code>org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of 'Collector' are missing.
    In many cases lambda methods don't provide enough information for automatic type extraction when Java generics are involved.
    An easy workaround is to use an (anonymous) class instead that implements the 'org.apache.flink.api.common.functions.FlatMapFunction' interface.
    Otherwise the type has to be specified explicitly using type information.
</code></pre><p>在这种情况下，需要明确指定类型信息，否则输出将被视为类型为 Object，导致序列化效率低下。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.typeinfo.Types</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.api.java.DataSet</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.util.Collector</span><span class="o">;</span>

<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">input</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">);</span>

<span class="c1">// collector type must be declared
</span><span class="c1"></span><span class="n">input</span><span class="o">.</span><span class="na">flatMap</span><span class="o">((</span><span class="n">Integer</span> <span class="n">number</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">StringBuilder</span> <span class="n">builder</span> <span class="o">=</span> <span class="k">new</span> <span class="n">StringBuilder</span><span class="o">();</span>
    <span class="k">for</span><span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">number</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
        <span class="n">builder</span><span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">);</span>
        <span class="n">out</span><span class="o">.</span><span class="na">collect</span><span class="o">(</span><span class="n">builder</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
    <span class="o">}</span>
<span class="o">})</span>
<span class="c1">// provide type information explicitly
</span><span class="c1"></span><span class="o">.</span><span class="na">returns</span><span class="o">(</span><span class="n">Types</span><span class="o">.</span><span class="na">STRING</span><span class="o">)</span>
<span class="c1">// prints &#34;a&#34;, &#34;a&#34;, &#34;aa&#34;, &#34;a&#34;, &#34;aa&#34;, &#34;aaa&#34;
</span><span class="c1"></span><span class="o">.</span><span class="na">print</span><span class="o">();</span>
</code></pre></div><p>当使用具有通用返回类型的 <code>map()</code> 函数时，也会出现类似的问题。在下面的例子中，一个方法签名 <code>Tuple2&lt;Integer, Integer&gt; map(Integer value)</code> 被擦除为 <code>Tuple2 map(Integer value)</code>。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.functions.MapFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.api.java.tuple.Tuple2</span><span class="o">;</span>

<span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">i</span> <span class="o">-&gt;</span> <span class="n">Tuple2</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span><span class="o">))</span>    <span class="c1">// no information about fields of Tuple2
</span><span class="c1"></span>    <span class="o">.</span><span class="na">print</span><span class="o">();</span>
</code></pre></div><p>一般来说，这些问题可以通过多种方式解决。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.typeinfo.Types</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.api.java.tuple.Tuple2</span><span class="o">;</span>

<span class="c1">// use the explicit &#34;.returns(...)&#34;
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">i</span> <span class="o">-&gt;</span> <span class="n">Tuple2</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span><span class="o">))</span>
    <span class="o">.</span><span class="na">returns</span><span class="o">(</span><span class="n">Types</span><span class="o">.</span><span class="na">TUPLE</span><span class="o">(</span><span class="n">Types</span><span class="o">.</span><span class="na">INT</span><span class="o">,</span> <span class="n">Types</span><span class="o">.</span><span class="na">INT</span><span class="o">))</span>
    <span class="o">.</span><span class="na">print</span><span class="o">();</span>

<span class="c1">// use a class instead
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="k">new</span> <span class="n">MyTuple2Mapper</span><span class="o">())</span>
    <span class="o">.</span><span class="na">print</span><span class="o">();</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">MyTuple2Mapper</span> <span class="kd">extends</span> <span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="o">{</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="nf">map</span><span class="o">(</span><span class="n">Integer</span> <span class="n">i</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="n">Tuple2</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// use an anonymous class instead
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="k">new</span> <span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="o">{</span>
        <span class="nd">@Override</span>
        <span class="kd">public</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="nf">map</span><span class="o">(</span><span class="n">Integer</span> <span class="n">i</span><span class="o">)</span> <span class="o">{</span>
            <span class="k">return</span> <span class="n">Tuple2</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span><span class="o">);</span>
        <span class="o">}</span>
    <span class="o">})</span>
    <span class="o">.</span><span class="na">print</span><span class="o">();</span>

<span class="c1">// or in this example use a tuple subclass instead
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">i</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">DoubleTuple</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span><span class="o">))</span>
    <span class="o">.</span><span class="na">print</span><span class="o">();</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">DoubleTuple</span> <span class="kd">extends</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="nf">DoubleTuple</span><span class="o">(</span><span class="kt">int</span> <span class="n">f0</span><span class="o">,</span> <span class="kt">int</span> <span class="n">f1</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">this</span><span class="o">.</span><span class="na">f0</span> <span class="o">=</span> <span class="n">f0</span><span class="o">;</span>
        <span class="k">this</span><span class="o">.</span><span class="na">f1</span> <span class="o">=</span> <span class="n">f1</span><span class="o">;</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/java_lambdas.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/java_lambdas.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Joining]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-asynchronous-io-for-external-data-access/?utm_source=atom_feed" rel="related" type="text/html" title="用于外部数据访问的异步 I/O" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-windows/?utm_source=atom_feed" rel="related" type="text/html" title="窗口" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-joining/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Joining</blockquote><h1 id="窗口连接join">窗口连接(Join)</h1>
<p>窗口连接(window join)将两个流的元素连接起来，这两个流有一个共同的键，并且位于同一个窗口中。这些窗口可以通过使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-assigners">窗口分配器</a>来定义，并对两个流的元素进行评估。</p>
<p>然后，来自双方的元素被传递到一个用户定义的 JoinFunction 或 FlatJoinFunction 中，用户可以发出符合加入标准的结果。</p>
<p>一般的用法可以归纳为以下几点。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">stream</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">otherStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(&lt;</span><span class="nc">KeySelector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">equalTo</span><span class="o">(&lt;</span><span class="nc">KeySelector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="nc">WindowAssigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">apply</span><span class="o">(&lt;</span><span class="nc">JoinFunction</span><span class="o">&gt;)</span>
</code></pre></div><p>关于语义的一些说明:</p>
<ul>
<li>两个流中元素的成对组合的创建就像一个内部连接，这意味着一个流中的元素如果没有另一个流中的相应元素与之连接，就不会发出。</li>
<li>那些被加入的元素将以各自窗口中最大的时间戳作为它们的时间戳。例如，一个以 <code>[5, 10)</code> 为边界的窗口将导致加入的元素以9作为它们的时间戳。</li>
</ul>
<p>在下面的章节中，我们将使用一些示例性的场景来概述不同类型的窗口连接是如何进行的。</p>
<h2 id="滚动窗口连接">滚动窗口连接</h2>
<p>当执行滚动窗口连接时，所有具有共同的键和共同的滚动窗口的元素都会以成对组合的方式进行连接，并传递给 <code>JoinFunction</code> 或 <code>FlatJoinFunction</code>。因为这表现得像一个内连接，所以一个流的元素如果在其滚动窗口中没有来自另一个流的元素，就不会被发出去！这就是为什么我们要把一个流的元素加入到滚动窗口中。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/tumbling-window-join.svg" alt="img"></p>
<p>如图所示，我们定义了一个大小为2毫秒的滚动窗口，其结果是 <code>[0,1]</code>，<code>[2,3]</code>，&hellip;形式的窗口。图中显示了每个窗口中所有元素的配对组合，这些元素将被传递给 <code>JoinFunction</code>。请注意，在翻滚窗口 <code>[6,7]</code> 中，没有任何元素发出，因为绿色流中没有元素存在，要与橙色元素⑥和⑦连接。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.time.Time</span><span class="o">;</span>

<span class="o">...</span>

<span class="k">val</span> <span class="n">orangeStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">greenStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">orangeStream</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">greenStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">equalTo</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(</span><span class="mi">2</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="o">(</span><span class="n">e1</span><span class="o">,</span> <span class="n">e2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">e1</span> <span class="o">+</span> <span class="s">&#34;,&#34;</span> <span class="o">+</span> <span class="n">e2</span> <span class="o">}</span>
</code></pre></div><h2 id="滑动窗连接">滑动窗连接</h2>
<p>在执行滑动窗口连接时，所有具有共同键和共同滑动窗口的元素都会以成对组合的方式连接，并传递给 <code>JoinFunction</code> 或 <code>FlatJoinFunction</code>。一个流的元素如果在当前的滑动窗口中没有来自另一个流的元素，则不会被发出! 请注意，有些元素可能在一个滑动窗口中被加入，但在另一个滑动窗口中却没有!</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/sliding-window-join.svg" alt="img"></p>
<p>在这个例子中，我们使用的是大小为两毫秒的滑动窗口，并将它们滑动一毫秒，结果是滑动窗口 <code>[-1，0]，[0，1]，[1，2]，[2，3]</code>，&hellip;。x轴下面的加入元素就是每个滑动窗口传递给 <code>JoinFunction</code> 的元素。这里你也可以看到，例如橙色的②与绿色的③在窗口 <code>[2,3]</code> 中连接，但与窗口 <code>[1,2]</code> 中的任何元素都没有连接。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.time.Time</span><span class="o">;</span>

<span class="o">...</span>

<span class="k">val</span> <span class="n">orangeStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">greenStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">orangeStream</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">greenStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">equalTo</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">SlidingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span> <span class="cm">/* size */</span><span class="o">,</span> <span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="cm">/* slide */</span><span class="o">))</span>
    <span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="o">(</span><span class="n">e1</span><span class="o">,</span> <span class="n">e2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">e1</span> <span class="o">+</span> <span class="s">&#34;,&#34;</span> <span class="o">+</span> <span class="n">e2</span> <span class="o">}</span>
</code></pre></div><h2 id="会议窗口连接">会议窗口连接</h2>
<p>当执行会话窗口连接时，所有具有相同键的元素，当&quot;组合&quot;满足会话标准时，将以成对组合的方式连接，并传递给 <code>JoinFunction</code> 或 <code>FlatJoinFunction</code>。同样，这也是执行内部连接，所以如果有一个会话窗口只包含来自一个流的元素，就不会有输出。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/session-window-join.svg" alt="img"></p>
<p>在这里，我们定义了一个会话窗口加入，其中每个会话被至少1ms的间隙所分割。有三个会话，在前两个会话中，两个流中的加入元素都会传递给 <code>JoinFunction</code>。在第三个会话中，绿色流中没有元素，所以⑧和⑨没有加入！在第三个会话中，绿色流中没有元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.time.Time</span><span class="o">;</span>
 
<span class="o">...</span>

<span class="k">val</span> <span class="n">orangeStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">greenStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">orangeStream</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">greenStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">equalTo</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">EventTimeSessionWindows</span><span class="o">.</span><span class="n">withGap</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(</span><span class="mi">1</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="o">(</span><span class="n">e1</span><span class="o">,</span> <span class="n">e2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">e1</span> <span class="o">+</span> <span class="s">&#34;,&#34;</span> <span class="o">+</span> <span class="n">e2</span> <span class="o">}</span>
</code></pre></div><h2 id="间隔连接">间隔连接</h2>
<p>间隔连接将两个流的元素（我们暂且称它们为A和B）用一个共同的键连接起来，流B中的元素的时间戳与流A中元素的时间戳处于一个相对的时间间隔。</p>
<p>这也可以更正式地表达为 <code>b.timestamp∈[a.timestamp + lowerBound; a.timestamp + upperBound]</code> 或 <code>a.timestamp + lowerBound &lt;= b.timestamp &lt;= a.timestamp + upperBound</code>。</p>
<p>其中a和b是A和B的元素，它们有一个共同的键。下界和上界都可以是负的或正的，只要下界总是小于或等于上界。区间连接目前只执行内连接。</p>
<p>当一对元素传递给 <code>ProcessJoinFunction</code> 时，它们将被分配为两个元素中较大的时间戳（可以通过 <code>ProcessJoinFunction.Context</code> 访问）。</p>
<p>注意：间隔连接目前只支持事件时间。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/interval-join.svg" alt="img"></p>
<p>在上面的例子中，我们将两个流&quot;橙色&quot;和&quot;绿色&quot;连接起来，下界为-2毫秒，上界为+1毫秒。默认情况下，这些边界是包容的，但可以应用 <code>.lowerBoundExclusive()</code> 和 <code>.upperBoundExclusive</code> 来改变行为。</p>
<p>再次使用更正式的符号，这将被翻译为:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">orangeElem</span><span class="o">.</span><span class="n">ts</span> <span class="o">+</span> <span class="n">lowerBound</span> <span class="o">&lt;=</span> <span class="n">greenElem</span><span class="o">.</span><span class="n">ts</span> <span class="o">&lt;=</span> <span class="n">orangeElem</span><span class="o">.</span><span class="n">ts</span> <span class="o">+</span> <span class="n">upperBound</span>
</code></pre></div><p>as indicated by the triangles.</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.functions.co.ProcessJoinFunction</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.time.Time</span><span class="o">;</span>

<span class="o">...</span>

<span class="k">val</span> <span class="n">orangeStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">greenStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">orangeStream</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">intervalJoin</span><span class="o">(</span><span class="n">greenStream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">))</span>
    <span class="o">.</span><span class="n">between</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(-</span><span class="mi">2</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">ProcessJoinFunction</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">Integer</span>, <span class="kt">String</span><span class="o">]</span> <span class="o">{</span>
        <span class="k">override</span> <span class="k">def</span> <span class="n">processElement</span><span class="o">(</span><span class="n">left</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">,</span> <span class="n">right</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">,</span> <span class="n">ctx</span><span class="k">:</span> <span class="kt">ProcessJoinFunction</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">Integer</span>, <span class="kt">String</span><span class="o">]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
         <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">left</span> <span class="o">+</span> <span class="s">&#34;,&#34;</span> <span class="o">+</span> <span class="n">right</span><span class="o">);</span> 
        <span class="o">}</span>
      <span class="o">});</span>
    <span class="o">});</span>
</code></pre></div><p>原文连接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/joining.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/joining.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/operators" term="operators" label="Operators" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/joining" term="joining" label="Joining" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Operators]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/?utm_source=atom_feed" rel="related" type="text/html" title="可查询状态" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-operators/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Operators</blockquote><h2 id="操作符">操作符</h2>
<p>操作符将一个或多个 DataStream 转换为一个新的 DataStream。程序可以将多个变换组合成复杂的数据流拓扑。</p>
<p>本节给出了基本变换的描述，应用这些变换后的有效物理分区，以及对 Flink 的操作符链的见解。</p>
<h2 id="datastream-转换">DataStream 转换</h2>
<ul>
<li>Map</li>
</ul>
<p>DataStream → DataStream</p>
<p>接受一个元素并产生一个元素。一个将输入流的值翻倍的 <code>map</code> 函数:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">}</span>
</code></pre></div><ul>
<li>FlatMap</li>
</ul>
<p>DataStream → DataStream</p>
<p>接受一个元素并产生零个、一个或多个元素。一个将句子分割成单词的 <code>flatMap</code> 函数:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="n">str</span> <span class="k">=&gt;</span> <span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34; &#34;</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Filter</li>
</ul>
<p>DataStream → DataStream</p>
<p>评估每个元素的布尔函数，并保留那些函数返回值为真的元素。一个过滤掉零值的 <code>filter</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">_</span> <span class="o">!=</span> <span class="mi">0</span> <span class="o">}</span>
</code></pre></div><ul>
<li>KeyBy</li>
</ul>
<p>DataStream → KeyedStream</p>
<p>在逻辑上将一个流划分为互斥的分区，每个分区包含相同键的元素。在内部，这是通过哈希分区实现的。关于如何指定键，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html#keyed-state">keys</a>。这个转换会返回一个 <code>KeyedStream</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="s">&#34;someKey&#34;</span><span class="o">)</span> <span class="c1">// Key by field &#34;someKey&#34;
</span><span class="c1"></span><span class="n">dataStream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>         <span class="c1">// Key by the first element of a Tuple
</span></code></pre></div><ul>
<li>Reduce</li>
</ul>
<p>KeyedStream → DataStream</p>
<p>在 keyed 数据流上进行&quot;滚动&quot;换算(reduce)。将当前元素与最后一个换算的值合并，并发出新的值。</p>
<p>一个创建部分和(sum)流的 <code>reduce</code> 函数:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">keyedStream</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="k">_</span> <span class="o">+</span> <span class="k">_</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Fold</li>
</ul>
<p>KeyedStream → DataStream</p>
<p>在一个带有初始值的 keyed 数据流上进行&quot;滚动&quot;折叠。将当前元素与最后一个折叠的值结合起来，并发出新的值。</p>
<p>一个折叠函数，当应用于序列(1,2,3,4,5)时，发出序列 &ldquo;start-1&rdquo;、&ldquo;start-1-2&rdquo;、&ldquo;start-1-2-3&rdquo;、&hellip;</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span>
    <span class="n">keyedStream</span><span class="o">.</span><span class="n">fold</span><span class="o">(</span><span class="s">&#34;start&#34;</span><span class="o">)((</span><span class="n">str</span><span class="o">,</span> <span class="n">i</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span> <span class="n">str</span> <span class="o">+</span> <span class="s">&#34;-&#34;</span> <span class="o">+</span> <span class="n">i</span> <span class="o">})</span>
</code></pre></div><ul>
<li>Aggregations</li>
</ul>
<p>KeyedStream → DataStream</p>
<p>在 keyed 数据流上进行滚动聚合。<code>min</code> 和 <code>minBy</code> 的区别在于 <code>min</code> 返回最小值，而 <code>minBy</code> 则返回该字段中具有最小值的元素（<code>max</code> 和 <code>maxBy</code> 也一样）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">keyedStream</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">min</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">min</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Window</li>
</ul>
<p>KeyedStream → WindowedStream</p>
<p>可以在已经分区的 <code>KeyedStream</code> 上定义 <code>Window</code>。窗口根据一些特征（例如，最近5秒内到达的数据）对每个键中的数据进行分组。关于窗口的描述，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html">窗口</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span> <span class="c1">// Last 5 seconds of data
</span></code></pre></div><ul>
<li>WindowAll</li>
</ul>
<p>DataStream → AllWindowedStream</p>
<p>可以在常规的 DataStream 上定义窗口。窗口根据一些特征（例如，在过去5秒内到达的数据）对所有流事件进行分组。关于窗口的完整描述，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html">窗口</a>。</p>
<p>警告：在许多情况下，这是一个非并行的转换。所有的记录将被收集在 <code>windowAll</code> 操作符的一个任务(task)中。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">windowAll</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span> <span class="c1">// Last 5 seconds of data
</span></code></pre></div><ul>
<li>Window Apply</li>
</ul>
<p>WindowedStream → DataStream</p>
<p>AllWindowedStream → DataStream</p>
<p>将一般函数应用于整个窗口。下面是一个手动求和窗口元素的函数。</p>
<p>注意：如果您使用的是 <code>windowAll</code> 转换，您需要使用 <code>AllWindowFunction</code> 来代替。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">windowedStream</span><span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="nc">WindowFunction</span> <span class="o">}</span>

<span class="c1">// applying an AllWindowFunction on non-keyed window stream
</span><span class="c1"></span><span class="n">allWindowedStream</span><span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="nc">AllWindowFunction</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Window Reduce</li>
</ul>
<p>WindowedStream → DataStream</p>
<p>对窗口应用函数式的 <code>reduce </code> 函数，并返回换算后的值:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">windowedStream</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="k">_</span> <span class="o">+</span> <span class="k">_</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Window Fold</li>
</ul>
<p>WindowedStream → DataStream</p>
<p>对窗口应用功能 <code>fold</code> 函数并返回折叠后的值。示例函数应用于序列 <code>(1,2,3,4,5)</code> 时，将序列折叠成字符串 &ldquo;start-1-2-3-4-5&rdquo;:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span>
    <span class="n">windowedStream</span><span class="o">.</span><span class="n">fold</span><span class="o">(</span><span class="s">&#34;start&#34;</span><span class="o">,</span> <span class="o">(</span><span class="n">str</span><span class="o">,</span> <span class="n">i</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span> <span class="n">str</span> <span class="o">+</span> <span class="s">&#34;-&#34;</span> <span class="o">+</span> <span class="n">i</span> <span class="o">})</span>
</code></pre></div><ul>
<li>窗口上的聚合</li>
</ul>
<p>WindowedStream → DataStream</p>
<p>聚合一个窗口的内容。<code>min</code> 和 <code>minBy</code> 的区别在于 <code>min</code> 返回最小值，而 <code>minBy</code> 返回在该字段中具有最小值的元素（<code>max</code> 和 <code>maxBy</code> 相同）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">windowedStream</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">min</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">min</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Union</li>
</ul>
<p>DataStream* → DataStream</p>
<p>联合两个或多个数据流，创建一个新的流，包含所有流的所有元素。注意：如果你把一个数据流和它自己联合起来，你将在生成的数据流中得到每个元素两次。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">union</span><span class="o">(</span><span class="n">otherStream1</span><span class="o">,</span> <span class="n">otherStream2</span><span class="o">,</span> <span class="o">...)</span>
</code></pre></div><ul>
<li>Window Join</li>
</ul>
<p>DataStream,DataStream → DataStream</p>
<p>在一个给定的键和一个公共窗口上连接(join)两个数据流。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">otherStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;).</span><span class="n">equalTo</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">3</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Window CoGroup</li>
</ul>
<p>DataStream,DataStream → DataStream</p>
<p>在一个给定的键和一个共同的窗口上将两个数据流串联(Cogroups)起来。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">coGroup</span><span class="o">(</span><span class="n">otherStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">3</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">apply</span> <span class="o">{}</span>
</code></pre></div><ul>
<li>Connect</li>
</ul>
<p>DataStream,DataStream → ConnectedStreams</p>
<p>&ldquo;连接&rdquo;(connect)两个数据流，保留其类型，允许两个数据流之间共享状态。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">someStream</span> <span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="n">otherStream</span> <span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">connectedStreams</span> <span class="k">=</span> <span class="n">someStream</span><span class="o">.</span><span class="n">connect</span><span class="o">(</span><span class="n">otherStream</span><span class="o">)</span>
</code></pre></div><ul>
<li>CoMap, CoFlatMap</li>
</ul>
<p>ConnectedStreams → DataStream</p>
<p>类似于连接(connected)数据流上的 <code>map</code> 和 <code>flatMap</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">connectedStreams</span><span class="o">.</span><span class="n">map</span><span class="o">(</span>
    <span class="o">(</span><span class="k">_</span> <span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="kc">true</span><span class="o">,</span>
    <span class="o">(</span><span class="k">_</span> <span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="kc">false</span>
<span class="o">)</span>
<span class="n">connectedStreams</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span>
    <span class="o">(</span><span class="k">_</span> <span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="kc">true</span><span class="o">,</span>
    <span class="o">(</span><span class="k">_</span> <span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="kc">false</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>Split</li>
</ul>
<p>DataStream → SplitStream</p>
<p>根据某种标准，将流分成两个或多个流。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">split</span> <span class="k">=</span> <span class="n">someDataStream</span><span class="o">.</span><span class="n">split</span><span class="o">(</span>
  <span class="o">(</span><span class="n">num</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="o">(</span><span class="n">num</span> <span class="o">%</span> <span class="mi">2</span><span class="o">)</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="mi">0</span> <span class="k">=&gt;</span> <span class="nc">List</span><span class="o">(</span><span class="s">&#34;even&#34;</span><span class="o">)</span>
      <span class="k">case</span> <span class="mi">1</span> <span class="k">=&gt;</span> <span class="nc">List</span><span class="o">(</span><span class="s">&#34;odd&#34;</span><span class="o">)</span>
    <span class="o">}</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>Select</li>
</ul>
<p>SplitStream → DataStream</p>
<p>从分割流中选择一个或多个流。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">even</span> <span class="k">=</span> <span class="n">split</span> <span class="n">select</span> <span class="s">&#34;even&#34;</span>
<span class="k">val</span> <span class="n">odd</span> <span class="k">=</span> <span class="n">split</span> <span class="n">select</span> <span class="s">&#34;odd&#34;</span>
<span class="k">val</span> <span class="n">all</span> <span class="k">=</span> <span class="n">split</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="s">&#34;even&#34;</span><span class="o">,</span><span class="s">&#34;odd&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Iterate</li>
</ul>
<p>DataStream → IterativeStream → DataStream</p>
<p>在流(flow)中创建一个&quot;反馈&quot;循环，将一个操作符的输出重定向到之前的某个操作符。这对于定义持续更新模型的算法特别有用。下面的代码从一个流(stream)开始，连续应用迭代体。大于0的元素被送回反馈通道，其余元素被转发到下游。参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/#iterations">迭代</a>的完整描述。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">initialStream</span><span class="o">.</span><span class="n">iterate</span> <span class="o">{</span>
  <span class="n">iteration</span> <span class="k">=&gt;</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">iterationBody</span> <span class="k">=</span> <span class="n">iteration</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span><span class="cm">/*do something*/</span><span class="o">}</span>
    <span class="o">(</span><span class="n">iterationBody</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="o">),</span> <span class="n">iterationBody</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="o">))</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>通过匿名模式匹配从 tuple、case 类和集合中提取，比如下面:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">temperature</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="o">}</span>
</code></pre></div><p>不受 API 开箱即用的支持。要使用这个功能，你应该使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/scala_api_extensions.html">Scala API 扩展</a>。</p>
<p>以下转换可用于 Tuples 的数据流:</p>
<ul>
<li>Project</li>
</ul>
<p>DataStream → DataStream</p>
<p>从元组中选择一个字段的子集。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataStream</span><span class="o">&lt;</span><span class="n">Tuple3</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">in</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">DataStream</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="na">project</span><span class="o">(</span><span class="n">2</span><span class="o">,</span><span class="n">0</span><span class="o">);</span>
</code></pre></div><h2 id="物理分区">物理分区</h2>
<p>Flink 还可以通过以下函数对转换后的准确流分区进行低级控制（如果需要）。</p>
<ul>
<li>自定义分区</li>
</ul>
<p>DataStream → DataStream</p>
<p>使用用户定义的 Partitioner 为每个元素选择目标任务。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">partitionCustom</span><span class="o">(</span><span class="n">partitioner</span><span class="o">,</span> <span class="s">&#34;someKey&#34;</span><span class="o">)</span>
<span class="n">dataStream</span><span class="o">.</span><span class="n">partitionCustom</span><span class="o">(</span><span class="n">partitioner</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span>
</code></pre></div><ul>
<li>随机分区</li>
</ul>
<p>DataStream → DataStream</p>
<p>将元素按照均匀分布随机分区。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">shuffle</span><span class="o">()</span>
</code></pre></div><ul>
<li>Rebalancing (循环分区)</li>
</ul>
<p>DataStream → DataStream</p>
<p>对元素进行循环分区，使每个分区的负载相等。在数据倾斜的情况下，对性能优化很有用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">rebalance</span><span class="o">()</span>
</code></pre></div><ul>
<li>Rescaling</li>
</ul>
<p>DataStream → DataStream</p>
<p>将元素，轮回分区到下游操作的子集。如果你想拥有管道，例如，从源的每个并行实例向几个映射器(mappers)的子集扇出，以分配负载，但又不想进行 <code>rebalance()</code> 会引起的完全再平衡，那么这就很有用。这将只需要本地数据传输，而不是通过网络传输数据，这取决于其他配置值，如 TaskManagers 的槽数(slots)。</p>
<p>上游操作向其发送元素的下游操作子集取决于上游和下游操作的并行程度。例如，如果上游操作的并行度为2，下游操作的并行度为4，那么一个上游操作将向两个下游操作分发元素，而另一个上游操作将向另外两个下游操作分发。另一方面，如果下游操作具有并行度2，而上游操作具有并行度4，那么两个上游操作将分配给一个下游操作，而其他两个上游操作将分配给其他下游操作。</p>
<p>在不同的并行度不是彼此的倍数的情况下，一个或几个下游操作将从上游操作中获得不同数量的输入。</p>
<p>请看此图，可以直观地看到上例中的连接(connection)模式。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/rescale.svg" alt="img"></p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">rescale</span><span class="o">()</span>
</code></pre></div><ul>
<li>Broadcasting</li>
</ul>
<p>DataStream → DataStream</p>
<p>将元素广播到每个分区。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">broadcast</span><span class="o">()</span>
</code></pre></div><h2 id="任务链和资源组">任务链和资源组</h2>
<p>链式的两个后续变换意味着将它们共同放置在同一个线程中以获得更好的性能。如果可能的话，Flink 默认会将操作符链起来（例如，两个后续的 map 变换）。如果需要的话，API 提供了对链式操作的精细控制。</p>
<p>如果你想在整个作业(job)中禁用链，请使用 <code>StreamExecutionEnvironment.disableOperatorChaining()</code>。对于更细粒度的控制，以下函数是可用的。请注意，这些函数只能在 DataStream 转换之后使用，因为它们引用了之前的转换。例如，你可以使用 <code>someStream.map(...).startNewChain()</code>，但你不能使用 <code>someStream.startNewChain()</code>。</p>
<p>资源组是 Flink 中的一个槽，参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html#configuring-taskmanager-processing-slots">slots</a>。如果需要，你可以在单独的槽中手动隔离操作符。</p>
<ul>
<li>Start new chain</li>
</ul>
<p>开始一个新的链，从这个操作符开始。两个映射器(mappers)将被连锁，<code>filter</code> 将不会被连锁到第一个映射器(mapper)。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">someStream</span><span class="o">.</span><span class="n">filter</span><span class="o">(...).</span><span class="n">map</span><span class="o">(...).</span><span class="n">startNewChain</span><span class="o">().</span><span class="n">map</span><span class="o">(...)</span>
</code></pre></div><ul>
<li>Disable chaining</li>
</ul>
<p>不将 <code>map</code> 运算符连锁化。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">someStream</span><span class="o">.</span><span class="n">map</span><span class="o">(...).</span><span class="n">disableChaining</span><span class="o">()</span>
</code></pre></div><ul>
<li>Set slot sharing group</li>
</ul>
<p>设置操作的槽位共享组。Flink 会将具有相同槽位共享组的操作放入同一个槽位，而将没有槽位共享组的操作保留在其他槽位。这可以用来隔离槽位。如果所有的输入操作都在同一个槽共享组中，槽共享组就会从输入操作中继承。缺省槽共享组的名称是 &ldquo;default&rdquo;，操作可以通过调用 <code>slotSharingGroup(&quot;default&quot;)</code> 来明确地放入这个组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">someStream</span><span class="o">.</span><span class="n">filter</span><span class="o">(...).</span><span class="n">slotSharingGroup</span><span class="o">(</span><span class="s">&#34;name&#34;</span><span class="o">)</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/operator" term="operator" label="Operator" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Scala API 扩展]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/?utm_source=atom_feed" rel="related" type="text/html" title="可查询状态" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Scala API Extensions</blockquote><h2 id="scala-api-扩展">Scala API 扩展</h2>
<p>为了在 Scala 和 Java API 之间保持相当程度的一致性，一些允许在 Scala 中进行高级表达的功能被从标准 API 中省略了，包括批处理和流式处理。</p>
<p>如果你想享受完整的 Scala 体验，你可以选择加入通过隐式转换来增强 Scala API 的扩展。</p>
<p>要使用所有可用的扩展，您只需为 DataSet API 添加一个简单的导入即可。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala.extensions._</span>
</code></pre></div><p>或者 DataStream API:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala.extensions._</span>
</code></pre></div><p>另外，你也可以按顺序导入单个扩展，只使用你喜欢的扩展。</p>
<h2 id="接受部分函数">接受部分函数</h2>
<p>通常情况下，DataSet 和 DataStream API 都不接受匿名模式匹配函数来解构 tuple、case 类或集合，比如下面。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">temperature</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="c1">// [...]
</span><span class="c1"></span>  <span class="c1">// The previous line causes the following compilation error:
</span><span class="c1"></span>  <span class="c1">// &#34;The argument types of an anonymous function must be fully known. (SLS 8.5)&#34;
</span><span class="c1"></span><span class="o">}</span>
</code></pre></div><p>该扩展在 DataSet 和 DataStream Scala API 中引入了新的方法，这些方法在扩展的 API 中具有一对一的对应关系。这些代理方法确实支持匿名模式匹配函数。</p>
<h3 id="dataset-api">DataSet API</h3>
<ul>
<li>mapWith	方法和原来的 map (DataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">value</span><span class="o">.</span><span class="n">toString</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>mapPartitionWith 方法和原来的	mapPartition (DataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapPartitionWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="n">head</span> <span class="o">#</span><span class="k">:</span><span class="kt">:</span> <span class="k">_</span> <span class="o">=&gt;</span> <span class="n">head</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>flatMapWith	方法和原来的 flatMap (DataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">flatMapWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">visitTimes</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">visitTimes</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">name</span> <span class="o">-&gt;</span> <span class="k">_</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>filterWith 方法和原来的	filter (DataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">filterWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="nc">Train</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">isOnTime</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">isOnTime</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>reduceWith 方法和原来的	reduce (DataSet, GroupedDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduceWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">((</span><span class="k">_</span><span class="o">,</span> <span class="n">amount1</span><span class="o">),</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">amount2</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="n">amount1</span> <span class="o">+</span> <span class="n">amount2</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>reduceGroupWith	方法和原来的 reduceGroup (GroupedDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduceGroupWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="n">id</span> <span class="o">#</span><span class="k">:</span><span class="kt">:</span> <span class="kt">value</span> <span class="k">#</span><span class="kt">::</span> <span class="k">_</span> <span class="o">=&gt;</span> <span class="n">id</span> <span class="o">-&gt;</span> <span class="n">value</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>groupingBy 方法和原来的	groupBy (DataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">groupingBy</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">id</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>sortGroupWith	方法和原来的 sortGroup (GroupedDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">grouped</span><span class="o">.</span><span class="n">sortGroupWith</span><span class="o">(</span><span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">case</span> <span class="nc">House</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">value</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>combineGroupWith 方法和原来的	combineGroup (GroupedDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">grouped</span><span class="o">.</span><span class="n">combineGroupWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="n">header</span> <span class="o">#</span><span class="k">:</span><span class="kt">:</span> <span class="kt">amounts</span> <span class="o">=&gt;</span> <span class="n">amounts</span><span class="o">.</span><span class="n">sum</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>projecting 方法和原来的	apply (JoinDataSet, CrossDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span>
  <span class="n">whereClause</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="n">pk</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">pk</span><span class="o">).</span>
  <span class="n">isEqualTo</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">fk</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">fk</span><span class="o">).</span>
  <span class="n">projecting</span> <span class="o">{</span>
    <span class="k">case</span> <span class="o">((</span><span class="n">pk</span><span class="o">,</span> <span class="n">tx</span><span class="o">),</span> <span class="o">(</span><span class="n">products</span><span class="o">,</span> <span class="n">fk</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="n">tx</span> <span class="o">-&gt;</span> <span class="n">products</span>
  <span class="o">}</span>

<span class="n">data1</span><span class="o">.</span><span class="n">cross</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span><span class="n">projecting</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="k">_</span><span class="o">),</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">-&gt;</span> <span class="n">b</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>projecting 方法和原来的	apply (CoGroupDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data1</span><span class="o">.</span><span class="n">coGroup</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span>
  <span class="n">whereClause</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="n">pk</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">pk</span><span class="o">).</span>
  <span class="n">isEqualTo</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">fk</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">fk</span><span class="o">).</span>
  <span class="n">projecting</span> <span class="o">{</span>
    <span class="k">case</span> <span class="o">(</span><span class="n">head1</span> <span class="o">#</span><span class="k">:</span><span class="kt">:</span> <span class="k">_</span><span class="o">,</span> <span class="n">head2</span> <span class="o">#</span><span class="k">:</span><span class="kt">:</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">head1</span> <span class="o">-&gt;</span> <span class="n">head2</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="datastream-api">DataStream API</h3>
<ul>
<li>mapWith	方法和原来的 map (DataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">value</span><span class="o">.</span><span class="n">toString</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>flatMapWith	方法和原来的 flatMap (DataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">flatMapWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">visits</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">visits</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">name</span> <span class="o">-&gt;</span> <span class="k">_</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>filterWith 方法和原来的	filter (DataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">filterWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="nc">Train</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">isOnTime</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">isOnTime</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>keyingBy 方法和原来的	keyBy (DataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">keyingBy</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">id</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>mapWith 方法和原来的 map (ConnectedDataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapWith</span><span class="o">(</span>
  <span class="n">map1</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">value</span><span class="o">.</span><span class="n">toString</span><span class="o">,</span>
  <span class="n">map2</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">,</span> <span class="n">value</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">value</span> <span class="o">+</span> <span class="mi">1</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>flatMapWith	方法和原来的 flatMap (ConnectedDataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">flatMapWith</span><span class="o">(</span>
  <span class="n">flatMap1</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">json</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">parse</span><span class="o">(</span><span class="n">json</span><span class="o">),</span>
  <span class="n">flatMap2</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">,</span> <span class="n">json</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">parse</span><span class="o">(</span><span class="n">json</span><span class="o">)</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>keyingBy 方法和原来的	keyBy (ConnectedDataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">keyingBy</span><span class="o">(</span>
  <span class="n">key1</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">timestamp</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">timestamp</span><span class="o">,</span>
  <span class="n">key2</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">id</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>reduceWith 方法和原来的 reduce (KeyedStream, WindowedStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduceWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">((</span><span class="k">_</span><span class="o">,</span> <span class="n">sum1</span><span class="o">),</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">sum2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">sum1</span> <span class="o">+</span> <span class="n">sum2</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>foldWith 方法和原来的	fold (KeyedStream, WindowedStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">foldWith</span><span class="o">(</span><span class="nc">User</span><span class="o">(</span><span class="n">bought</span> <span class="k">=</span> <span class="mi">0</span><span class="o">))</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="nc">User</span><span class="o">(</span><span class="n">b</span><span class="o">),</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">items</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="nc">User</span><span class="o">(</span><span class="n">b</span> <span class="o">+</span> <span class="n">items</span><span class="o">.</span><span class="n">size</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>applyWith	方法和原来的 apply (WindowedStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">applyWith</span><span class="o">(</span><span class="mi">0</span><span class="o">)(</span>
  <span class="n">foldFunction</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="n">sum</span><span class="o">,</span> <span class="n">amount</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">sum</span> <span class="o">+</span> <span class="n">amount</span>
  <span class="n">windowFunction</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="n">k</span><span class="o">,</span> <span class="n">w</span><span class="o">,</span> <span class="n">sum</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="o">)</span>
</code></pre></div><ul>
<li>projecting 方法和原来的	apply (JoinedStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span>
  <span class="n">whereClause</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="n">pk</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">pk</span><span class="o">).</span>
  <span class="n">isEqualTo</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">fk</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">fk</span><span class="o">).</span>
  <span class="n">projecting</span> <span class="o">{</span>
    <span class="k">case</span> <span class="o">((</span><span class="n">pk</span><span class="o">,</span> <span class="n">tx</span><span class="o">),</span> <span class="o">(</span><span class="n">products</span><span class="o">,</span> <span class="n">fk</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="n">tx</span> <span class="o">-&gt;</span> <span class="n">products</span>
  <span class="o">}</span>
</code></pre></div><p>关于每个方法的语义的更多信息，请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">DataSet</a> 和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">DataStream API</a> 文档。</p>
<p>要专门使用这个扩展，可以添加以下导入。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala.extensions.acceptPartialFunctions</span>
</code></pre></div><p>对于 DataSet 扩展和</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions</span>
</code></pre></div><p>下面的代码段展示了一个最小的例子，说明如何一起使用这些扩展方法（与 DataSet API 一起）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">object</span> <span class="nc">Main</span> <span class="o">{</span>
  <span class="k">import</span> <span class="nn">org.apache.flink.api.scala.extensions._</span>
  <span class="k">case</span> <span class="k">class</span> <span class="nc">Point</span><span class="o">(</span><span class="n">x</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">y</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>
  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
    <span class="k">val</span> <span class="n">ds</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="nc">Point</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="nc">Point</span><span class="o">(</span><span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">),</span> <span class="nc">Point</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">6</span><span class="o">))</span>
    <span class="n">ds</span><span class="o">.</span><span class="n">filterWith</span> <span class="o">{</span>
      <span class="k">case</span> <span class="nc">Point</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="o">}.</span><span class="n">reduceWith</span> <span class="o">{</span>
      <span class="k">case</span> <span class="o">(</span><span class="nc">Point</span><span class="o">(</span><span class="n">x1</span><span class="o">,</span> <span class="n">y1</span><span class="o">),</span> <span class="o">(</span><span class="nc">Point</span><span class="o">(</span><span class="n">x2</span><span class="o">,</span> <span class="n">y2</span><span class="o">)))</span> <span class="k">=&gt;</span> <span class="nc">Point</span><span class="o">(</span><span class="n">x1</span> <span class="o">+</span> <span class="n">y1</span><span class="o">,</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">y2</span><span class="o">)</span>
    <span class="o">}.</span><span class="n">mapWith</span> <span class="o">{</span>
      <span class="k">case</span> <span class="nc">Point</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">y</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">y</span><span class="o">)</span>
    <span class="o">}.</span><span class="n">flatMapWith</span> <span class="o">{</span>
      <span class="k">case</span> <span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">y</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&#34;x&#34;</span> <span class="o">-&gt;</span> <span class="n">x</span><span class="o">,</span> <span class="s">&#34;y&#34;</span> <span class="o">-&gt;</span> <span class="n">y</span><span class="o">)</span>
    <span class="o">}.</span><span class="n">groupingBy</span> <span class="o">{</span>
      <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">id</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/scala_api_extensions.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/scala_api_extensions.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[侧输出]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/?utm_source=atom_feed" rel="related" type="text/html" title="可查询状态" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Side Outputs</blockquote><h2 id="side-output">Side Output</h2>
<p>除了 DataStream 操作产生的主流(main stream)外，还可以产生任意数量的附加侧输出结果流。结果流中的数据类型不必与主流中的数据类型相匹配，不同侧输出的类型也可以不同。当您要分割数据流时，这种操作非常有用，通常您必须复制数据流，然后从每个数据流中过滤掉您不想要的数据。</p>
<p>在使用侧输出时，首先需要定义一个 <code>OutputTag</code>，用来识别侧输出流。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">outputTag</span> <span class="k">=</span> <span class="nc">OutputTag</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;side-output&#34;</span><span class="o">)</span>
</code></pre></div><p>请注意 <code>OutputTag</code> 是如何根据侧输出流所包含的元素类型进行类型化的。</p>
<p>可以通过以下函数向侧输出发送数据。</p>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/process_function.html">ProcessFunction</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/process_function.html#the-keyedprocessfunction">KeyedProcessFunction</a></li>
<li>CoProcessFunction</li>
<li>KeyedCoProcessFunction</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#processwindowfunction">ProcessWindowFunction</a></li>
<li>ProcessAllWindowFunction</li>
</ul>
<p>你可以使用 <code>Context</code> 参数（在上面的函数中暴露给用户）向一个由 <code>OutputTag</code> 标识的侧输出发送数据。下面是一个从 <code>ProcessFunction</code> 中发射侧输出数据的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">outputTag</span> <span class="k">=</span> <span class="nc">OutputTag</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;side-output&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">mainDataStream</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">ProcessFunction</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">processElement</span><span class="o">(</span>
        <span class="n">value</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>
        <span class="n">ctx</span><span class="k">:</span> <span class="kt">ProcessFunction</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span>
        <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="c1">// emit data to regular output
</span><span class="c1"></span>      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">value</span><span class="o">)</span>

      <span class="c1">// emit data to side output
</span><span class="c1"></span>      <span class="n">ctx</span><span class="o">.</span><span class="n">output</span><span class="o">(</span><span class="n">outputTag</span><span class="o">,</span> <span class="s">&#34;sideout-&#34;</span> <span class="o">+</span> <span class="nc">String</span><span class="o">.</span><span class="n">valueOf</span><span class="o">(</span><span class="n">value</span><span class="o">))</span>
    <span class="o">}</span>
  <span class="o">})</span>
</code></pre></div><p>为了检索侧输出流，你可以在 DataStream 操作的结果上使用 <code>getSideOutput(OutputTag)</code>。这将给你一个 DataStream，它的类型是侧输出流的结果。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">outputTag</span> <span class="k">=</span> <span class="nc">OutputTag</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;side-output&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">mainDataStream</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">sideOutputStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">mainDataStream</span><span class="o">.</span><span class="n">getSideOutput</span><span class="o">(</span><span class="n">outputTag</span><span class="o">)</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/side_output.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/side_output.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/side-outputs" term="side-outputs" label="Side Outputs" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[可查询状态]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Queryable State Beta</blockquote><p>注意：可查询状态的客户端 API 目前处于不断发展的状态，对所提供接口的稳定性不做保证。在即将到来的 Flink 版本中，客户端的 API 很可能会有突破性的变化。</p>
<p>简而言之，这个功能将 Flink 的 managed keyed (partitioned) state（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">Working with State</a>）暴露给外界，并允许用户从 Flink 外部查询作业的状态。对于某些场景来说，可查询状态消除了与外部系统（如键值存储）进行分布式操作/交易的需求，而这往往是实践中的瓶颈。此外，该功能对于调试目的可能特别有用。</p>
<p>注意事项: 当查询一个状态对象时，该对象是在没有任何同步或复制的情况下从一个并发线程访问的。这是一个设计上的选择，因为上述任何一种情况都会导致作业延迟的增加，这是我们想要避免的。因为任何使用 Java 堆空间的状态后端，如 MemoryStateBackend 或 FsStateBackend，在检索值时都不会使用副本，而是直接引用存储的值，所以读-修改-写模式是不安全的，可能会导致可查询状态服务器因并发修改而失败。RocksDBStateBackend 则可以避免这些问题。</p>
<h2 id="架构">架构</h2>
<p>在展示如何使用可查询状态之前，先简单介绍一下构成它的实体。Queryable State 功能由三个主要实体组成。</p>
<ol>
<li>QueryableStateClient，它（可能）运行在 Flink 集群之外，并提交用户查询。</li>
<li>QueryableStateClientProxy，它运行在每个 TaskManager 上（即 Flink 集群内部），负责接收客户端的查询，代表他从负责的 TaskManager 中获取所请求的状态，并将其返回给客户端，以及</li>
<li>QueryableStateServer，它运行在每个 TaskManager 上，负责为本地存储的状态提供服务。</li>
</ol>
<p>客户端连接到其中一个代理，并发送一个与特定键 <em>k</em> 相关联的状态的请求。正如在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">使用状态</a>中所述，keyed state 被组织在键组(Key Groups)中，每个 TaskManager 都被分配了一些这样的键组(Key Groups)。为了发现哪个 TaskManager 负责持有 <em>k</em> 的键组，代理将询问 JobManager。根据答案，代理将查询运行在该 TaskManager 上的 QueryableStateServer，以获取与 <em>k</em> 相关联的状态，并将响应转发到客户端。</p>
<h2 id="激活可查询状态">激活可查询状态</h2>
<p>要在 Flink 集群上启用可查询状态，你需要做以下工作。</p>
<ol>
<li>将 <code>flink-queryable-state-runtime_2.11-1.11.0.jar</code> 从 <a href="https://flink.apache.org/downloads.html">Flink 发行版</a>的 <code>opt/</code> 文件夹中复制到 <code>lib/</code>  文件夹中。</li>
<li>设置属性 <code>queryable-state.enable</code> 为 <code>true</code>。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html#queryable-state">配置</a>文档了解详情和附加参数。</li>
</ol>
<p>要验证您的群集是否在启用可查询状态后运行，请检查任何 TaskManager 的日志中的行。&ldquo;Started the Queryable State Proxy Server @ &hellip;&quot;。</p>
<h3 id="使状态可查询">使状态可查询</h3>
<p>现在你已经在集群上激活了可查询状态，现在是时候看看如何使用它了。为了使一个状态对外界可见，它需要通过使用以下方式明确地成为可查询状态。</p>
<ul>
<li>QueryableStateStream, 一个方便的对象，它作为一个接收器(sink)，并把它的传入值作为可查询的状态提供，或者是</li>
<li>stateDescriptor.setQueryable(String queryableStateName) 方法，使得状态描述符所代表的 keyed state，可以查询。</li>
</ul>
<p>下面的章节将解释这两种方法的使用。</p>
<h3 id="可查询的状态流">可查询的状态流</h3>
<p>在 KeyedStream 上调用 <code>.asQueryableState(stateName, stateDescriptor)</code> 会返回一个 <code>QueryableStateStream</code>，它将其值作为可查询状态提供。根据状态的类型，<code>asQueryableState()</code> 方法有以下几种变体。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="c1">// ValueState
</span><span class="c1"></span><span class="n">QueryableStateStream</span> <span class="nf">asQueryableState</span><span class="o">(</span>
    <span class="n">String</span> <span class="n">queryableStateName</span><span class="o">,</span>
    <span class="n">ValueStateDescriptor</span> <span class="n">stateDescriptor</span><span class="o">)</span>

<span class="c1">// Shortcut for explicit ValueStateDescriptor variant
</span><span class="c1"></span><span class="n">QueryableStateStream</span> <span class="nf">asQueryableState</span><span class="o">(</span><span class="n">String</span> <span class="n">queryableStateName</span><span class="o">)</span>

<span class="c1">// FoldingState
</span><span class="c1"></span><span class="n">QueryableStateStream</span> <span class="nf">asQueryableState</span><span class="o">(</span>
    <span class="n">String</span> <span class="n">queryableStateName</span><span class="o">,</span>
    <span class="n">FoldingStateDescriptor</span> <span class="n">stateDescriptor</span><span class="o">)</span>

<span class="c1">// ReducingState
</span><span class="c1"></span><span class="n">QueryableStateStream</span> <span class="nf">asQueryableState</span><span class="o">(</span>
    <span class="n">String</span> <span class="n">queryableStateName</span><span class="o">,</span>
    <span class="n">ReducingStateDescriptor</span> <span class="n">stateDescriptor</span><span class="o">)</span>
</code></pre></div><p>注意：没有可查询的 <code>ListState</code> 接收器，因为这会导致一个不断增长的列表，可能无法清理，因此最终会消耗过多的内存。</p>
<p>返回的 <code>QueryableStateStream</code> 可以被看作是一个接收器(sink)，不能被进一步转换。在内部，一个 <code>QueryableStateStream</code> 会被翻译成一个操作符，它使用所有传入的记录来更新可查询状态实例。更新逻辑是由 <code>asQueryableState</code> 调用中提供的 <code>StateDescriptor</code> 的类型暗示的。在像下面这样的程序中，keyed stream 的所有记录将通过 <code>ValueState.update(value)</code> 来更新状态实例:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">stream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">asQueryableState</span><span class="o">(</span><span class="s">&#34;query-name&#34;</span><span class="o">)</span>
</code></pre></div><p>这就像 Scala API 的 <code>flatMapWithState</code> 一样。</p>
<h3 id="管理的-keyed-state">管理的 Keyed State</h3>
<p>通过 <code>StateDescriptor.setQueryable(String queryableStateName)</code> 使相应的状态描述符成为可查询的状态，可以使操作符的托管键控状态(参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html#using-managed-keyed-state">使用 Managed Keyed State)</a>)成为可查询的状态，如下面的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">ValueStateDescriptor</span><span class="o">&lt;</span><span class="nc">Tuple2</span><span class="o">&lt;</span><span class="nc">Long</span><span class="o">,</span> <span class="nc">Long</span><span class="o">&gt;&gt;</span> <span class="n">descriptor</span> <span class="k">=</span>
        <span class="k">new</span> <span class="nc">ValueStateDescriptor</span><span class="o">&lt;&gt;(</span>
                <span class="s">&#34;average&#34;</span><span class="o">,</span> <span class="c1">// the state name
</span><span class="c1"></span>                <span class="nc">TypeInformation</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="k">new</span> <span class="nc">TypeHint</span><span class="o">&lt;</span><span class="nc">Tuple2</span><span class="o">&lt;</span><span class="nc">Long</span><span class="o">,</span> <span class="nc">Long</span><span class="o">&gt;&gt;()</span> <span class="o">{}));</span> <span class="c1">// type information
</span><span class="c1"></span>
<span class="n">descriptor</span><span class="o">.</span><span class="n">setQueryable</span><span class="o">(</span><span class="s">&#34;query-name&#34;</span><span class="o">);</span> <span class="c1">// queryable state name
</span></code></pre></div><p>注意：<code>queryableStateName</code> 参数可以任意选择，并且只用于查询。它不一定要与状态本身的名称相同。</p>
<p>这个变体对于哪种类型的状态可以被查询没有限制。这意味着它可以用于任何 ValueState、ReduceState、ListState、MapState、AggregatingState 以及目前已被废弃的 FoldingState。</p>
<h3 id="查询状态">查询状态</h3>
<p>到目前为止，你已经设置了你的集群以可查询的状态运行，并且你已经将你的（部分）状态声明为可查询。现在是时候看看如何查询这个状态了。</p>
<p>为此，你可以使用 <code>QueryableStateClient</code> 辅助类。它可以在 <code>flink-queryable-state-client jar</code> 中找到，它必须和 flink-core 一起被显式地包含在项目的 pom.xml 中作为依赖，如下所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-core<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-queryable-state-client-java<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>更多的内容，可以查看如何<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html">设置 Flink 程序</a>。</p>
<p><code>QueryableStateClient</code> 会将你的查询提交给内部代理，然后代理会处理你的查询并返回最终结果。初始化客户端的唯一要求是提供一个有效的 TaskManager 主机名（记住每个 TaskManager 上都有一个可查询状态代理运行）和代理监听的端口。更多关于如何配置代理和状态服务器端口的信息请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/queryable_state.html#configuration">配置部分</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">QueryableStateClient</span> <span class="n">client</span> <span class="o">=</span> <span class="k">new</span> <span class="n">QueryableStateClient</span><span class="o">(</span><span class="n">tmHostname</span><span class="o">,</span> <span class="n">proxyPort</span><span class="o">)</span>
</code></pre></div><p>客户端准备好后，要查询一个类型为 V 的状态，与类型为 K 的键相关联，可以使用该方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">CompletableFuture</span><span class="o">&lt;</span><span class="n">S</span><span class="o">&gt;</span> <span class="nf">getKvState</span><span class="o">(</span>
    <span class="n">JobID</span> <span class="n">jobId</span><span class="o">,</span>
    <span class="n">String</span> <span class="n">queryableStateName</span><span class="o">,</span>
    <span class="n">K</span> <span class="n">key</span><span class="o">,</span>
    <span class="n">TypeInformation</span><span class="o">&lt;</span><span class="n">K</span><span class="o">&gt;</span> <span class="n">keyTypeInfo</span><span class="o">,</span>
    <span class="n">StateDescriptor</span><span class="o">&lt;</span><span class="n">S</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="n">stateDescriptor</span><span class="o">)</span>
</code></pre></div><p>以上返回一个 CompletableFuture，最终持有 ID 为 jobID 的作业的 <code>queryableStateName</code> 所标识的可查询状态实例的状态值。key 是你对其状态感兴趣的键，keyTypeInfo 将告诉 Flink 如何序列化/解序列化它。最后，<code>stateDescriptor</code> 包含了关于所请求的状态的必要信息，即它的类型（Value、Reduce 等）和如何序列化/解序列化它的必要信息。</p>
<p>细心的读者会注意到，返回的 future 包含一个 S 类型的值，即一个包含实际值的 <code>State</code> 对象。这可以是 Flink 支持的任何一种状态类型。ValueState，ReduceState，ListState，MapState，AggregatingState，以及目前已经废弃的 FoldingState。</p>
<p>注意：这些状态对象不允许对包含的状态进行修改。您可以使用它们来获取状态的实际值，例如使用 <code>valueState.get()</code>，或者迭代包含的 <code>&lt;K，V&gt;</code> 条目，例如使用 <code>mapState.entry()</code>，但您不能修改它们。举个例子，在返回的列表状态上调用 <code>add()</code> 方法会抛出一个 <code>UnsupportedOperationException</code>。</p>
<p>注意：客户端是异步的，可以被多个线程共享。在未使用时需要通过 <code>QueryableStateClient.shutdown()</code> 来关闭它，以释放资源。</p>
<h3 id="例子">例子</h3>
<p>下面的例子扩展了 <code>CountWindowAverage</code> 的例子(请看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html#using-managed-keyed-state">使用 Managed Keyed State</a>)，使其可查询，并展示了如何查询这个值。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">CountWindowAverage</span> <span class="kd">extends</span> <span class="n">RichFlatMapFunction</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="o">{</span>

    <span class="kd">private</span> <span class="kd">transient</span> <span class="n">ValueState</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">sum</span><span class="o">;</span> <span class="c1">// a tuple containing the count and the sum
</span><span class="c1"></span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">flatMap</span><span class="o">(</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="n">input</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
        <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="n">currentSum</span> <span class="o">=</span> <span class="n">sum</span><span class="o">.</span><span class="na">value</span><span class="o">();</span>
        <span class="n">currentSum</span><span class="o">.</span><span class="na">f0</span> <span class="o">+=</span> <span class="n">1</span><span class="o">;</span>
        <span class="n">currentSum</span><span class="o">.</span><span class="na">f1</span> <span class="o">+=</span> <span class="n">input</span><span class="o">.</span><span class="na">f1</span><span class="o">;</span>
        <span class="n">sum</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="n">currentSum</span><span class="o">);</span>

        <span class="k">if</span> <span class="o">(</span><span class="n">currentSum</span><span class="o">.</span><span class="na">f0</span> <span class="o">&gt;=</span> <span class="n">2</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">out</span><span class="o">.</span><span class="na">collect</span><span class="o">(</span><span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;&gt;(</span><span class="n">input</span><span class="o">.</span><span class="na">f0</span><span class="o">,</span> <span class="n">currentSum</span><span class="o">.</span><span class="na">f1</span> <span class="o">/</span> <span class="n">currentSum</span><span class="o">.</span><span class="na">f0</span><span class="o">));</span>
            <span class="n">sum</span><span class="o">.</span><span class="na">clear</span><span class="o">();</span>
        <span class="o">}</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">open</span><span class="o">(</span><span class="n">Configuration</span> <span class="n">config</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">ValueStateDescriptor</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">descriptor</span> <span class="o">=</span>
                <span class="k">new</span> <span class="n">ValueStateDescriptor</span><span class="o">&lt;&gt;(</span>
                        <span class="s">&#34;average&#34;</span><span class="o">,</span> <span class="c1">// the state name
</span><span class="c1"></span>                        <span class="n">TypeInformation</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="k">new</span> <span class="n">TypeHint</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;()</span> <span class="o">{}));</span> <span class="c1">// type information
</span><span class="c1"></span>        <span class="n">descriptor</span><span class="o">.</span><span class="na">setQueryable</span><span class="o">(</span><span class="s">&#34;query-name&#34;</span><span class="o">);</span>
        <span class="n">sum</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getState</span><span class="o">(</span><span class="n">descriptor</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
<span class="n">Once</span> <span class="n">used</span> <span class="n">in</span> <span class="n">a</span> <span class="n">job</span><span class="o">,</span> <span class="n">you</span> <span class="n">can</span> <span class="n">retrieve</span> <span class="n">the</span> <span class="n">job</span> <span class="n">ID</span> <span class="n">and</span> <span class="n">then</span> <span class="n">query</span> <span class="n">any</span> <span class="n">key</span><span class="err">’</span><span class="n">s</span> <span class="n">current</span> <span class="n">state</span> <span class="n">from</span> <span class="k">this</span> <span class="n">operator</span><span class="o">:</span>

<span class="n">QueryableStateClient</span> <span class="n">client</span> <span class="o">=</span> <span class="k">new</span> <span class="n">QueryableStateClient</span><span class="o">(</span><span class="n">tmHostname</span><span class="o">,</span> <span class="n">proxyPort</span><span class="o">);</span>

<span class="c1">// the state descriptor of the state to be fetched.
</span><span class="c1"></span><span class="n">ValueStateDescriptor</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">descriptor</span> <span class="o">=</span>
        <span class="k">new</span> <span class="n">ValueStateDescriptor</span><span class="o">&lt;&gt;(</span>
          <span class="s">&#34;average&#34;</span><span class="o">,</span>
          <span class="n">TypeInformation</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="k">new</span> <span class="n">TypeHint</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;()</span> <span class="o">{}));</span>

<span class="n">CompletableFuture</span><span class="o">&lt;</span><span class="n">ValueState</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;&gt;</span> <span class="n">resultFuture</span> <span class="o">=</span>
        <span class="n">client</span><span class="o">.</span><span class="na">getKvState</span><span class="o">(</span><span class="n">jobId</span><span class="o">,</span> <span class="s">&#34;query-name&#34;</span><span class="o">,</span> <span class="n">key</span><span class="o">,</span> <span class="n">BasicTypeInfo</span><span class="o">.</span><span class="na">LONG_TYPE_INFO</span><span class="o">,</span> <span class="n">descriptor</span><span class="o">);</span>

<span class="c1">// now handle the returned value
</span><span class="c1"></span><span class="n">resultFuture</span><span class="o">.</span><span class="na">thenAccept</span><span class="o">(</span><span class="n">response</span> <span class="o">-&gt;</span> <span class="o">{</span>
        <span class="k">try</span> <span class="o">{</span>
            <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="n">res</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="na">get</span><span class="o">();</span>
        <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
        <span class="o">}</span>
<span class="o">});</span>
</code></pre></div><h2 id="配置">配置</h2>
<p>以下配置参数会影响可查询状态服务器和客户端的行为，它们被定义在 <code>QueryableStateOptions</code> 中。</p>
<h3 id="状态服务器">状态服务器</h3>
<ul>
<li><code>queryable-state.server.ports</code>：可查询状态服务器的服务器端口范围。如果在同一台机器上运行多个 task manager，这对避免端口冲突很有用。指定的范围可以是：一个端口: &ldquo;9123&rdquo;，一个端口范围: &ldquo;50100-50200&rdquo;，或者一个范围和或点的列表: &ldquo;50100-50200,50300-50400,51234&rdquo;。默认端口为 9067。</li>
<li><code>queryable-state.server.network-threads</code>: 接收状态服务器传入请求的网络（事件循环）线程数（0 =&gt; #slots）。</li>
<li><code>queryable-state.server.query-threads</code>: 为状态服务器处理/服务传入请求的线程数（0 =&gt; #slots）。</li>
</ul>
<h3 id="代理">代理</h3>
<ul>
<li><code>queryable-state.proxy.ports</code>：可查询状态代理服务器的端口范围。如果在同一台机器上运行多个 task manager，这对避免端口冲突很有用。指定的范围可以是：一个端口: &ldquo;9123&rdquo;，一个端口范围: &ldquo;50100-50200&rdquo;，或者一个范围和或点的列表: &ldquo;50100-50200,50300-50400,51234&rdquo;。默认端口为 9069。</li>
<li><code>queryable-state.proxy.network-threads</code>：为客户端代理接收传入请求的网络（事件循环）线程数（0 =&gt; #slots）。</li>
<li><code>queryable-state.proxy.query-threads</code>：为客户端代理处理/服务传入请求的线程数（0 =&gt; #slots）。</li>
</ul>
<h3 id="限制条件">限制条件</h3>
<ul>
<li>可查询状态的生命周期与任务的生命周期绑定，例如，任务在启动时注册可查询状态，在处置时取消注册。在未来的版本中，我们希望将其解耦，以便在任务完成后允许查询，并通过状态复制加快恢复速度。</li>
<li>关于可用 KvState 的通知是通过一个简单的告诉发生的。将来应该改进这个功能，使其更加强大，包括询问和确认。</li>
<li>服务器和客户端会跟踪查询的统计数据。目前默认情况下，这些数据是被禁用的，因为它们不会暴露在任何地方。一旦有更好的支持通过 Metrics 系统发布这些数字，我们应该启用统计。</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/queryable_state.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/queryable_state.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[处理应用程序参数]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-handling-application-parameters/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-handling-application-parameters/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Handling Application Parameters</blockquote><h1 id="处理应用程序参数">处理应用程序参数</h1>
<p>几乎所有的 Flink 应用，包括批处理和流式应用，都依赖于外部配置参数，它们用于指定输入和输出源（如路径或地址）、系统参数（并行性、运行时配置）和应用特定参数（通常在用户函数中使用）。它们用于指定输入和输出源（如路径或地址）、系统参数（并行性、运行时配置）和应用程序特定参数（通常在用户函数中使用）。</p>
<p>Flink 提供了一个名为 ParameterTool 的简单工具，为解决这些问题提供一些基本的工具。请注意，你不一定要使用这里描述的 ParameterTool。其他框架如 Commons CLI和argparse4j 也能很好地与 Flink 一起工作。</p>
<p>将你的配置值导入 ParameterTool 之中</p>
<p>ParameterTool 提供了一组预定义的静态方法来读取配置。该工具内部期待的是一个 <code>Map&lt;String，String&gt;</code>，所以很容易将其与自己的配置风格整合在一起。</p>
<p>从 <code>.properties</code> 文件中</p>
<p>下面的方法将读取一个属性文件并提供键/值对。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">String</span> <span class="n">propertiesFilePath</span> <span class="o">=</span> <span class="s">&#34;/home/sam/flink/myjob.properties&#34;</span><span class="o">;</span>
<span class="n">ParameterTool</span> <span class="n">parameter</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromPropertiesFile</span><span class="o">(</span><span class="n">propertiesFilePath</span><span class="o">);</span>

<span class="n">File</span> <span class="n">propertiesFile</span> <span class="o">=</span> <span class="k">new</span> <span class="n">File</span><span class="o">(</span><span class="n">propertiesFilePath</span><span class="o">);</span>
<span class="n">ParameterTool</span> <span class="n">parameter</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromPropertiesFile</span><span class="o">(</span><span class="n">propertiesFile</span><span class="o">);</span>

<span class="n">InputStream</span> <span class="n">propertiesFileInputStream</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FileInputStream</span><span class="o">(</span><span class="n">file</span><span class="o">);</span>
<span class="n">ParameterTool</span> <span class="n">parameter</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromPropertiesFile</span><span class="o">(</span><span class="n">propertiesFileInputStream</span><span class="o">);</span>
</code></pre></div><p>从命令行参数来看</p>
<p>这就允许从命令行中获取 <code>--input hdfs://mydata --elements 42</code> 这样的参数。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">ParameterTool</span> <span class="n">parameter</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromArgs</span><span class="o">(</span><span class="n">args</span><span class="o">);</span>
    <span class="c1">// .. regular code ..
</span></code></pre></div><p>从系统属性</p>
<p>当启动 JVM 时，你可以将系统属性传递给它。<code>-Dinput=hdfs://mydata</code>。你也可以从这些系统属性中初始化 ParameterTool。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ParameterTool</span> <span class="n">parameter</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromSystemProperties</span><span class="o">();</span>
</code></pre></div><p>在 Flink 程序中使用参数</p>
<p>现在我们已经从某个地方得到了参数（见上文），我们可以以各种方式使用它们。</p>
<p>直接从 ParameterTool 中使用</p>
<p>ParameterTool 本身有访问值的方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ParameterTool</span> <span class="n">parameters</span> <span class="o">=</span> <span class="c1">// ...
</span><span class="c1"></span><span class="n">parameter</span><span class="o">.</span><span class="na">getRequired</span><span class="o">(</span><span class="s">&#34;input&#34;</span><span class="o">);</span>
<span class="n">parameter</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="s">&#34;output&#34;</span><span class="o">,</span> <span class="s">&#34;myDefaultValue&#34;</span><span class="o">);</span>
<span class="n">parameter</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="s">&#34;expectedCount&#34;</span><span class="o">,</span> <span class="o">-</span><span class="n">1L</span><span class="o">);</span>
<span class="n">parameter</span><span class="o">.</span><span class="na">getNumberOfParameters</span><span class="o">()</span>
<span class="c1">// .. there are more methods available.
</span></code></pre></div><p>你可以在客户端提交应用程序的 <code>main()</code> 方法中直接使用这些方法的返回值。例如，你可以这样设置一个操作符的并行性。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ParameterTool</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromArgs</span><span class="o">(</span><span class="n">args</span><span class="o">);</span>
<span class="kt">int</span> <span class="n">parallelism</span> <span class="o">=</span> <span class="n">parameters</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="s">&#34;mapParallelism&#34;</span><span class="o">,</span> <span class="n">2</span><span class="o">);</span>
<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">Tokenizer</span><span class="o">()).</span><span class="na">setParallelism</span><span class="o">(</span><span class="n">parallelism</span><span class="o">);</span>
</code></pre></div><p>由于 ParameterTool 是可序列化的，所以你可以把它传递给函数本身。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ParameterTool</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromArgs</span><span class="o">(</span><span class="n">args</span><span class="o">);</span>
<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">Tokenizer</span><span class="o">(</span><span class="n">parameters</span><span class="o">));</span>
</code></pre></div><p>然后在函数内部使用它从命令行获取值。</p>
<p>全局注册参数</p>
<p>在 ExecutionConfig 中注册为全局作业参数的参数可以作为配置值从 JobManager Web 界面和用户定义的所有功能中访问。</p>
<p>全局注册参数。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ParameterTool</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromArgs</span><span class="o">(</span><span class="n">args</span><span class="o">);</span>

<span class="c1">// set up the execution environment
</span><span class="c1"></span><span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>
<span class="n">env</span><span class="o">.</span><span class="na">getConfig</span><span class="o">().</span><span class="na">setGlobalJobParameters</span><span class="o">(</span><span class="n">parameters</span><span class="o">);</span>
</code></pre></div><p>在任何丰富的用户功能中访问它们。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kd">class</span> <span class="nc">Tokenizer</span> <span class="kd">extends</span> <span class="n">RichFlatMapFunction</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="o">{</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">flatMap</span><span class="o">(</span><span class="n">String</span> <span class="n">value</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="o">{</span>
	<span class="n">ParameterTool</span> <span class="n">parameters</span> <span class="o">=</span> <span class="o">(</span><span class="n">ParameterTool</span><span class="o">)</span>
	    <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getExecutionConfig</span><span class="o">().</span><span class="na">getGlobalJobParameters</span><span class="o">();</span>
	<span class="n">parameters</span><span class="o">.</span><span class="na">getRequired</span><span class="o">(</span><span class="s">&#34;input&#34;</span><span class="o">);</span>
	<span class="c1">// .. do more ..
</span></code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/application_parameters.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/application_parameters.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/parameters" term="parameters" label="Parameters" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[实验特性]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-experimental-features/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-experimental-features/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Experimental Features</blockquote><h2 id="实验特性">实验特性</h2>
<p>本节介绍 DataStream API 中的实验性功能。实验性功能仍在不断发展，可能是不稳定的、不完整的，或者在未来的版本中会有很大的变化。</p>
<h3 id="将预先分割的数据流重新解释为-keyed-流">将预先分割的数据流重新解释为 keyed 流</h3>
<p>我们可以将一个预分区的数据流重新解释为一个 keyed 流，以避免洗牌。</p>
<p>警告：重新解释的数据流必须已经被预分区了，其方式与 Flink 的 keyBy 在洗牌中对数据的分区方式完全相同，即键组分配。</p>
<p>一个用例是两个作业之间的物化洗牌：第一个作业执行 keyBy 洗牌，并将每个输出物化为一个分区。第二个作业有源，对于每个并行实例，从第一个作业创建的相应分区中读取。现在可以将这些源重新解释为 keyed 流，例如应用窗口化。请注意，这个技巧使得第二个作业的并行性很尴尬，这对细粒度的恢复方案很有帮助。</p>
<p>这个重新解释的功能是通过 DataStreamUtils 暴露的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">static</span> <span class="o">&lt;</span><span class="n">T</span><span class="o">,</span> <span class="n">K</span><span class="o">&gt;</span> <span class="nc">KeyedStream</span><span class="o">&lt;</span><span class="n">T</span><span class="o">,</span> <span class="n">K</span><span class="o">&gt;</span> <span class="n">reinterpretAsKeyedStream</span><span class="o">(</span>
    <span class="nc">DataStream</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="n">stream</span><span class="o">,</span>
    <span class="nc">KeySelector</span><span class="o">&lt;</span><span class="n">T</span><span class="o">,</span> <span class="n">K</span><span class="o">&gt;</span> <span class="n">keySelector</span><span class="o">,</span>
    <span class="nc">TypeInformation</span><span class="o">&lt;</span><span class="n">K</span><span class="o">&gt;</span> <span class="n">typeInfo</span><span class="o">)</span>
</code></pre></div><p>给定一个基流(base stream)、一个键选择器和类型信息，该方法从基流创建一个 keyed 流。</p>
<p>代码示例:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
<span class="k">val</span> <span class="n">source</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">new</span> <span class="nc">DataStreamUtils</span><span class="o">(</span><span class="n">source</span><span class="o">).</span><span class="n">reinterpretAsKeyedStream</span><span class="o">((</span><span class="n">in</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">in</span><span class="o">)</span>
  <span class="o">.</span><span class="n">timeWindow</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
  <span class="o">.</span><span class="n">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span>
  <span class="o">.</span><span class="n">addSink</span><span class="o">(</span><span class="k">new</span> <span class="nc">DiscardingSink</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/experimental.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/experimental.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[广播状态模式]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-the-broadcast-state-pattern/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-the-broadcast-state-pattern/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>The Broadcast State Pattern</blockquote><p>在本节中，您将了解如何在实践中使用广播状态。请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/concepts/stateful-stream-processing.html">Stateful Stream Processing</a> 来了解有状态流处理背后的概念。</p>
<h2 id="提供的-api">提供的 API</h2>
<p>为了展示所提供的 API，我们将在介绍它们的全部功能之前先举一个例子。作为我们的运行示例，我们将使用这样的情况：我们有一个不同颜色和形状的对象流，我们希望找到相同颜色的对象对，并遵循特定的模式，例如，一个矩形和一个三角形。我们假设有趣的模式集会随着时间的推移而演变。</p>
<p>在这个例子中，第一个流将包含具有 <code>Color</code> 和 <code>Shape</code> 属性的 <code>Item</code> 类型的元素。另一个流将包含 <code>Rules</code>。</p>
<p>从 <code>Items</code> 流开始，我们只需要按 <code>Color</code> keyBy，因为我们想要相同颜色的对。这将确保相同颜色的元素最终会出现在同一个物理机上。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="c1">// key the items by color
</span><span class="c1"></span><span class="n">KeyedStream</span><span class="o">&lt;</span><span class="n">Item</span><span class="o">,</span> <span class="n">Color</span><span class="o">&gt;</span> <span class="n">colorPartitionedStream</span> <span class="o">=</span> <span class="n">itemStream</span>
                        <span class="o">.</span><span class="na">keyBy</span><span class="o">(</span><span class="k">new</span> <span class="n">KeySelector</span><span class="o">&lt;</span><span class="n">Item</span><span class="o">,</span> <span class="n">Color</span><span class="o">&gt;(){...});</span>
</code></pre></div><p>继续讨论规则，包含规则的流应该被广播到所有下游任务，这些任务应该将它们存储在本地，以便它们可以根据所有传入的项目评估它们。下面的代码段将i)广播规则流，ii)使用提供的 <code>MapStateDescriptor</code>，它将创建规则将被存储的广播状态。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// a map descriptor to store the name of the rule (string) and the rule itself.
</span><span class="c1"></span><span class="nc">MapStateDescriptor</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">Rule</span><span class="o">&gt;</span> <span class="n">ruleStateDescriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MapStateDescriptor</span><span class="o">&lt;&gt;(</span>
			<span class="s">&#34;RulesBroadcastState&#34;</span><span class="o">,</span>
			<span class="nc">BasicTypeInfo</span><span class="o">.</span><span class="nc">STRING_TYPE_INFO</span><span class="o">,</span>
			<span class="nc">TypeInformation</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="k">new</span> <span class="nc">TypeHint</span><span class="o">&lt;</span><span class="nc">Rule</span><span class="o">&gt;()</span> <span class="o">{}));</span>
		
<span class="c1">// broadcast the rules and create the broadcast state
</span><span class="c1"></span><span class="nc">BroadcastStream</span><span class="o">&lt;</span><span class="nc">Rule</span><span class="o">&gt;</span> <span class="n">ruleBroadcastStream</span> <span class="k">=</span> <span class="n">ruleStream</span>
                        <span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="n">ruleStateDescriptor</span><span class="o">);</span>
</code></pre></div><p>最后，为了根据从 <code>Item</code> 流传入的元素来评估 <code>Rules</code>，我们需要。</p>
<ul>
<li>连接(connect)两个流，并且</li>
<li>指定我们的匹配检测逻辑。</li>
</ul>
<p>将一个流（keyed or non-keyed）与 <code>BroadcastStream</code> 连接起来，可以通过在非广播流上调用 <code>connect()</code> 来完成，并将 <code>BroadcastStream</code> 作为一个参数。这将返回一个 <code>BroadcastConnectedStream</code>，我们可以在这个 Stream 上调用一个特殊类型的 <code>CoProcessFunction</code> 来处理。该函数将包含我们的匹配逻辑。该函数的具体类型取决于非广播流的类型。</p>
<ul>
<li>如果它是 <strong>keyed</strong>，那么这个函数就是 <code>KeyedBroadcastProcessFunction</code>。</li>
<li>如果是 <strong>non-keyed,</strong>，那么该函数就是一个 <code>BroadcastProcessFunction</code>。</li>
</ul>
<p>鉴于我们的非广播流是 keyed 的，下面的代码段包含了上述调用。</p>
<p>注意： 连接(connect)应该被调用在非广播流上， 以 <code>BroadcastStream</code> 作为参数。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">colorPartitionedStream</span>
                 <span class="o">.</span><span class="na">connect</span><span class="o">(</span><span class="n">ruleBroadcastStream</span><span class="o">)</span>
                 <span class="o">.</span><span class="na">process</span><span class="o">(</span>
                     
                     <span class="c1">// type arguments in our KeyedBroadcastProcessFunction represent: 
</span><span class="c1"></span>                     <span class="c1">//   1. the key of the keyed stream
</span><span class="c1"></span>                     <span class="c1">//   2. the type of elements in the non-broadcast side
</span><span class="c1"></span>                     <span class="c1">//   3. the type of elements in the broadcast side
</span><span class="c1"></span>                     <span class="c1">//   4. the type of the result, here a string
</span><span class="c1"></span>                     
                     <span class="k">new</span> <span class="n">KeyedBroadcastProcessFunction</span><span class="o">&lt;</span><span class="n">Color</span><span class="o">,</span> <span class="n">Item</span><span class="o">,</span> <span class="n">Rule</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;()</span> <span class="o">{</span>
                         <span class="c1">// my matching logic
</span><span class="c1"></span>                     <span class="o">}</span>
                 <span class="o">);</span>
</code></pre></div><h3 id="broadcastprocessfunction-和-keyedbroadcastprocessfunction">BroadcastProcessFunction 和 KeyedBroadcastProcessFunction</h3>
<p>与 <code>CoProcessFunction</code> 一样，这些函数有两个处理方法要实现；<code>processBroadcastElement()</code> 负责处理广播流中的传入元素，<code>processElement()</code> 用于处理非广播流。这些方法的完整签名如下:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">abstract</span> <span class="kd">class</span> <span class="nc">BroadcastProcessFunction</span><span class="o">&lt;</span><span class="n">IN1</span><span class="o">,</span> <span class="n">IN2</span><span class="o">,</span> <span class="n">OUT</span><span class="o">&gt;</span> <span class="kd">extends</span> <span class="n">BaseBroadcastProcessFunction</span> <span class="o">{</span>

    <span class="kd">public</span> <span class="kd">abstract</span> <span class="kt">void</span> <span class="nf">processElement</span><span class="o">(</span><span class="n">IN1</span> <span class="n">value</span><span class="o">,</span> <span class="n">ReadOnlyContext</span> <span class="n">ctx</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>

    <span class="kd">public</span> <span class="kd">abstract</span> <span class="kt">void</span> <span class="nf">processBroadcastElement</span><span class="o">(</span><span class="n">IN2</span> <span class="n">value</span><span class="o">,</span> <span class="n">Context</span> <span class="n">ctx</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>
<span class="o">}</span>
<span class="kd">public</span> <span class="kd">abstract</span> <span class="kd">class</span> <span class="nc">KeyedBroadcastProcessFunction</span><span class="o">&lt;</span><span class="n">KS</span><span class="o">,</span> <span class="n">IN1</span><span class="o">,</span> <span class="n">IN2</span><span class="o">,</span> <span class="n">OUT</span><span class="o">&gt;</span> <span class="o">{</span>

    <span class="kd">public</span> <span class="kd">abstract</span> <span class="kt">void</span> <span class="nf">processElement</span><span class="o">(</span><span class="n">IN1</span> <span class="n">value</span><span class="o">,</span> <span class="n">ReadOnlyContext</span> <span class="n">ctx</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>

    <span class="kd">public</span> <span class="kd">abstract</span> <span class="kt">void</span> <span class="nf">processBroadcastElement</span><span class="o">(</span><span class="n">IN2</span> <span class="n">value</span><span class="o">,</span> <span class="n">Context</span> <span class="n">ctx</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onTimer</span><span class="o">(</span><span class="kt">long</span> <span class="n">timestamp</span><span class="o">,</span> <span class="n">OnTimerContext</span> <span class="n">ctx</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>
<span class="o">}</span>
</code></pre></div><p>首先需要注意的是，这两个函数在处理广播端元素时都需要实现 <code>processBroadcastElement()</code> 方法，在处理非广播端元素时需要实现 <code>processElement()</code> 方法。</p>
<p>这两个方法在提供的上下文中有所不同。非广播侧有一个 <code>ReadOnlyContext</code>，而广播侧有一个 <code>Context</code>。</p>
<p>这两个上下文（以下枚举中的 <code>ctx</code>）:</p>
<ol>
<li>提供对广播状态的访问：<code>ctx.getBroadcastState(MapStateDescriptor&lt;K, V&gt; stateDescriptor)</code>。</li>
<li>允许查询元素的时间戳：<code>ctx.timestamp()</code>。</li>
<li>获取当前水印：<code>ctx.currentWatermark()</code>。</li>
<li>获取当前处理时间：<code>ctx.currentProcessingTime()</code>，以及</li>
<li>将元素发射到侧输出：<code>ctx.output(OutputTag&lt;X&gt; outputTag, X value)</code>。</li>
</ol>
<p><code>getBroadcastState()</code> 中的 <code>stateDescriptor</code> 应该和上面的 <code>.broadcast(ruleStateDescriptor)</code> 中的 <code>stateDescriptor</code> 是一样的。</p>
<p>区别在于各自对广播状态的访问类型。广播端对其有读写访问权，而非广播端则只有读的访问权（因此才有这些名字）。原因是在 Flink 中，不存在跨任务通信。所以，为了保证广播状态中的内容在我们操作符的所有并行实例中都是相同的，我们只给广播侧读写访问权，而广播侧在所有任务中看到的元素都是相同的，并且我们要求该侧每个传入元素的计算在所有任务中都是相同的。忽略这个规则会打破状态的一致性保证，导致结果不一致，而且往往难以调试。</p>
<p>注意 <code>processBroadcastElement()</code> 中实现的逻辑必须在所有并行实例中具有相同的确定性行为!</p>
<p>最后，由于 <code>KeyedBroadcastProcessFunction</code> 是在 keyed stream 上运行的，它暴露了一些 <code>BroadcastProcessFunction</code> 无法实现的功能。那就是</p>
<ol>
<li><code>processElement()</code> 方法中的 <code>ReadOnlyContext</code> 允许访问 Flink 的底层定时器服务，它允许注册事件和/或处理时间定时器。当一个定时器发射时， <code>onTimer()</code> (如上所示)被调用一个 <code>OnTimerContext</code>，它暴露了与 <code>ReadOnlyContext</code> 相同的功能，再加上</li>
</ol>
<ul>
<li>能够询问发射的定时器是事件还是处理时间, 和</li>
<li>来查询与定时器相关联的键。</li>
</ul>
<ol start="2">
<li><code>processBroadcastElement()</code> 方法中的 <code>Context</code> 包含 <code>applyToKeyedState(StateDescriptor&lt;S, VS&gt; stateDescriptor, KeyedStateFunction&lt;KS, S&gt; function)</code> 方法。这允许注册一个 <code>KeyedStateFunction</code>，以应用于与提供的 <code>stateDescriptor</code> 相关联的所有键的所有状态。</li>
</ol>
<p>注意。注册定时器只能在 <code>KeyedBroadcastProcessFunction</code> 的 <code>processElement()</code> 处进行，而且只能在那里进行。在 <code>processBroadcastElement()</code> 方法中是不可能的，因为没有键与广播元素相关联。</p>
<p>回到我们原来的例子，我们的 <code>KeyedBroadcastProcessFunction</code> 可以是如下的样子。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="k">new</span> <span class="n">KeyedBroadcastProcessFunction</span><span class="o">&lt;</span><span class="n">Color</span><span class="o">,</span> <span class="n">Item</span><span class="o">,</span> <span class="n">Rule</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;()</span> <span class="o">{</span>

    <span class="c1">// store partial matches, i.e. first elements of the pair waiting for their second element
</span><span class="c1"></span>    <span class="c1">// we keep a list as we may have many first elements waiting
</span><span class="c1"></span>    <span class="kd">private</span> <span class="kd">final</span> <span class="n">MapStateDescriptor</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Item</span><span class="o">&gt;&gt;</span> <span class="n">mapStateDesc</span> <span class="o">=</span>
        <span class="k">new</span> <span class="n">MapStateDescriptor</span><span class="o">&lt;&gt;(</span>
            <span class="s">&#34;items&#34;</span><span class="o">,</span>
            <span class="n">BasicTypeInfo</span><span class="o">.</span><span class="na">STRING_TYPE_INFO</span><span class="o">,</span>
            <span class="k">new</span> <span class="n">ListTypeInfo</span><span class="o">&lt;&gt;(</span><span class="n">Item</span><span class="o">.</span><span class="na">class</span><span class="o">));</span>

    <span class="c1">// identical to our ruleStateDescriptor above
</span><span class="c1"></span>    <span class="kd">private</span> <span class="kd">final</span> <span class="n">MapStateDescriptor</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Rule</span><span class="o">&gt;</span> <span class="n">ruleStateDescriptor</span> <span class="o">=</span> 
        <span class="k">new</span> <span class="n">MapStateDescriptor</span><span class="o">&lt;&gt;(</span>
            <span class="s">&#34;RulesBroadcastState&#34;</span><span class="o">,</span>
            <span class="n">BasicTypeInfo</span><span class="o">.</span><span class="na">STRING_TYPE_INFO</span><span class="o">,</span>
            <span class="n">TypeInformation</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="k">new</span> <span class="n">TypeHint</span><span class="o">&lt;</span><span class="n">Rule</span><span class="o">&gt;()</span> <span class="o">{}));</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">processBroadcastElement</span><span class="o">(</span><span class="n">Rule</span> <span class="n">value</span><span class="o">,</span>
                                        <span class="n">Context</span> <span class="n">ctx</span><span class="o">,</span>
                                        <span class="n">Collector</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
        <span class="n">ctx</span><span class="o">.</span><span class="na">getBroadcastState</span><span class="o">(</span><span class="n">ruleStateDescriptor</span><span class="o">).</span><span class="na">put</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="na">name</span><span class="o">,</span> <span class="n">value</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">processElement</span><span class="o">(</span><span class="n">Item</span> <span class="n">value</span><span class="o">,</span>
                               <span class="n">ReadOnlyContext</span> <span class="n">ctx</span><span class="o">,</span>
                               <span class="n">Collector</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>

        <span class="kd">final</span> <span class="n">MapState</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Item</span><span class="o">&gt;&gt;</span> <span class="n">state</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getMapState</span><span class="o">(</span><span class="n">mapStateDesc</span><span class="o">);</span>
        <span class="kd">final</span> <span class="n">Shape</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="na">getShape</span><span class="o">();</span>
    
        <span class="k">for</span> <span class="o">(</span><span class="n">Map</span><span class="o">.</span><span class="na">Entry</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Rule</span><span class="o">&gt;</span> <span class="n">entry</span> <span class="o">:</span>
                <span class="n">ctx</span><span class="o">.</span><span class="na">getBroadcastState</span><span class="o">(</span><span class="n">ruleStateDescriptor</span><span class="o">).</span><span class="na">immutableEntries</span><span class="o">())</span> <span class="o">{</span>
            <span class="kd">final</span> <span class="n">String</span> <span class="n">ruleName</span> <span class="o">=</span> <span class="n">entry</span><span class="o">.</span><span class="na">getKey</span><span class="o">();</span>
            <span class="kd">final</span> <span class="n">Rule</span> <span class="n">rule</span> <span class="o">=</span> <span class="n">entry</span><span class="o">.</span><span class="na">getValue</span><span class="o">();</span>
    
            <span class="n">List</span><span class="o">&lt;</span><span class="n">Item</span><span class="o">&gt;</span> <span class="n">stored</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">ruleName</span><span class="o">);</span>
            <span class="k">if</span> <span class="o">(</span><span class="n">stored</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">stored</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;();</span>
            <span class="o">}</span>
    
            <span class="k">if</span> <span class="o">(</span><span class="n">shape</span> <span class="o">==</span> <span class="n">rule</span><span class="o">.</span><span class="na">second</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">stored</span><span class="o">.</span><span class="na">isEmpty</span><span class="o">())</span> <span class="o">{</span>
                <span class="k">for</span> <span class="o">(</span><span class="n">Item</span> <span class="n">i</span> <span class="o">:</span> <span class="n">stored</span><span class="o">)</span> <span class="o">{</span>
                    <span class="n">out</span><span class="o">.</span><span class="na">collect</span><span class="o">(</span><span class="s">&#34;MATCH: &#34;</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="s">&#34; - &#34;</span> <span class="o">+</span> <span class="n">value</span><span class="o">);</span>
                <span class="o">}</span>
                <span class="n">stored</span><span class="o">.</span><span class="na">clear</span><span class="o">();</span>
            <span class="o">}</span>
    
            <span class="c1">// there is no else{} to cover if rule.first == rule.second
</span><span class="c1"></span>            <span class="k">if</span> <span class="o">(</span><span class="n">shape</span><span class="o">.</span><span class="na">equals</span><span class="o">(</span><span class="n">rule</span><span class="o">.</span><span class="na">first</span><span class="o">))</span> <span class="o">{</span>
                <span class="n">stored</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">value</span><span class="o">);</span>
            <span class="o">}</span>
    
            <span class="k">if</span> <span class="o">(</span><span class="n">stored</span><span class="o">.</span><span class="na">isEmpty</span><span class="o">())</span> <span class="o">{</span>
                <span class="n">state</span><span class="o">.</span><span class="na">remove</span><span class="o">(</span><span class="n">ruleName</span><span class="o">);</span>
            <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
                <span class="n">state</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="n">ruleName</span><span class="o">,</span> <span class="n">stored</span><span class="o">);</span>
            <span class="o">}</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="重要的考虑因素">重要的考虑因素</h3>
<p>在介绍完提供的 API 之后，本节重点介绍使用广播状态时需要注意的重要事项。这些事项是</p>
<ul>
<li>
<p>没有跨任务通信。如前所述，这就是为什么只有 (Keyed)-BroadcastProcessFunction 的广播端可以修改广播状态的内容的原因。此外，用户必须确保所有的任务对每一个传入元素都以同样的方式修改广播状态的内容。否则，不同的任务可能有不同的内容，导致结果不一致。</p>
</li>
<li>
<p>不同任务的广播状态中事件的顺序可能不同。虽然广播流的元素保证了所有元素将（最终）进入所有下游任务，但元素可能会以不同的顺序到达每个任务。因此，每个传入元素的状态更新必须不依赖于传入事件的顺序。</p>
</li>
<li>
<p>所有的任务都会对其广播状态进行 checkpoint。虽然当 checkpoint 发生时，所有任务的广播状态中都有相同的元素（checkpoint 屏障不会超过元素），但所有任务都会 checkpoint 他们的广播状态，而不仅仅是其中一个。这是一个设计决定，以避免在还原过程中让所有任务从同一个文件中读取（从而避免热点），尽管它的代价是将检查点状态的大小增加了p的系数（=并行性）。Flink 保证在恢复/缩放时，不会有重复和丢失的数据。在以相同或更小的并行度进行恢复时，每个任务读取其检查点状态。扩容后，每个任务读取自己的状态，其余任务（p_new-p_old）以循环的方式读取之前任务的检查点。</p>
</li>
<li>
<p>没有 RocksDB 状态后端。广播状态在运行时保存在内存中，内存供应也应相应进行。这对所有的操作符状态都适用。</p>
</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[数据源]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-data-sources/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-data-sources/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Data Sources</blockquote><p>注：这描述了新的数据源 API ，作为 FLIP-27 的一部分在 Flink 1.11 中引入。这个新的 API 目前处于 BETA 状态。
大多数现有的源连接器还没有（截至 Flink 1.11 ）使用这个新的 API 实现，而是使用以前的 API ，基于 SourceFunction 。
本页介绍了 Flink 的数据源 API 及其背后的概念和架构。如果你对 Flink 中的数据源是如何工作的，或者你想实现一个新的数据源，请阅读本页面。</p>
<p>如果您正在寻找预定义的源连接器，请查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/">连接器文档</a>。</p>
<h2 id="数据源概念">数据源概念</h2>
<p><strong>核心部件</strong></p>
<p>一个数据源有三个核心组件。<code>Split</code>、<code>SplitEnumerator</code> 和 <code>SourceReader</code>。</p>
<ul>
<li>
<p><code>Split</code> 是数据源所消耗的一部分数据，就像一个文件或一个日志分区。<code>Split</code> 是源分配工作和并行读取数据的粒度。</p>
</li>
<li>
<p><code>SourceReader</code> 请求 <code>Split</code> 并进行处理，例如读取 <code>Split</code> 所代表的文件或日志分区。<code>SourceReader</code> 在 <code>SourceOperators</code> 的 Task Manager 上并行运行，并产生事件/记录的并行流。</p>
</li>
<li>
<p><code>SplitEnumerator</code> 生成 <code>Split</code> 并将它们分配给 <code>SourceReader</code> 。它作为单个实例在任务管理器上运行，负责维护待处理的 <code>Split</code> 的积压，并以平衡的方式将它们分配给读者。</p>
</li>
</ul>
<p><code>Source</code> 类是将上述三个组件联系在一起的 API 入口点。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/source_components.svg" alt="img"></p>
<p><strong>统一的跨流和批处理</strong></p>
<p>数据源 API 以统一的方式支持无界流源和有界批处理源。</p>
<p>这两种情况的区别很小：在有界/批处理的情况下，枚举器生成一组固定的 split ，而且每个 split 必然是有限的。在无界流的情况下，这两种情况中的一种是不正确的（ split 不是有限的，或者枚举器不断产生新的 split ）。</p>
<h3 id="例子">例子</h3>
<p>下面是一些简化的概念性例子，以说明在流式和批处理情况下，数据源组件如何交互。</p>
<p>请注意，这并不能准确地描述 Kafka 和 File 源的实现是如何工作的；部分内容是简化的，用于说明目的。</p>
<p><strong>绑定的文件源</strong></p>
<p>源有一个要读取的目录的 URI/路径，以及一个定义如何解析文件的格式。</p>
<ul>
<li><code>Split</code> 是一个文件，或者一个文件的一个区域（如果数据格式支持分割文件）。</li>
<li><code>SplitEnumerator</code> 列出了给定目录路径下的所有文件。它将 <code>Split</code> 分配给下一个请求 <code>Split</code> 的读者。一旦所有的 <code>Split</code> 都分配完毕，它就会用 <code>NoMoreSplits</code> 来响应请求。</li>
<li>SourceReader 请求一个 <code>Split</code> ，并读取被分配的 <code>Split</code> （文件或文件区域），并使用给定的格式进行解析。如果它没有得到另一个 <code>Split</code> ，而是得到一个 <code>NoMoreSplits</code> 消息，它就结束了。</li>
</ul>
<p><strong>非绑定流文件源</strong></p>
<p>这个源的工作方式和上面描述的一样，除了 <code>SplitEnumerator</code> 从不响应 <code>NoMoreSplits</code> ，而是周期性地列出给定 <code>URI/Path</code> 下的内容以检查新文件。一旦发现新文件，它就会为它们生成新的 <code>Splits</code> ，并可以将它们分配给可用的 <code>SourceReaders</code>。</p>
<p><strong>无界流 Kafka 源</strong></p>
<p>该源有一个 Kafka Topic （或 Topic 列表或 Topic regex ）和一个 Deserializer 来解析记录。</p>
<ul>
<li>一个 Split 就是一个 Kafka Topic 分区。</li>
<li>SplitEnumerator 连接到 brokers ，以列出所有涉及订阅的主题分区。枚举器可以选择重复这个操作来发现新添加的主题/分区。</li>
<li>SourceReader 使用 KafkaConsumer 读取分配的 split （主题分区），并使用提供的 Deserializer 反序列化记录。分割(Topic Partitions) 没有终点，所以读取器永远不会到达数据的终点。</li>
</ul>
<p><strong>绑定的 Kafka 源</strong></p>
<p>和上面一样，只是每个 Split （主题分区）有一个定义的结束偏移量。一旦 SourceReader 达到一个 Split 的结束偏移量，它就会完成该 Split 。一旦所有分配的 Split 结束， SourceReader 就结束了。</p>
<h2 id="数据源-api">数据源 API</h2>
<p>本节介绍了 FLIP-27 中新引入的 Source API 的主要接口，并为开发者提供了 Source 开发的技巧。</p>
<h3 id="source">Source</h3>
<p><a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/Source.java">Source</a> API 是一个工厂风格的接口，用于创建以下组件。</p>
<ul>
<li>Split Enumerator</li>
<li>源读取器</li>
<li>分离式序列器</li>
<li>枚举器检查点序列器</li>
</ul>
<p>除此之外， Source 还提供了源的<a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/Boundedness.java">边界</a>属性，这样 Flink 可以选择合适的模式来运行 Flink 作业。</p>
<p>Source 的实现应该是可序列化的，因为 Source 实例在运行时被序列化并上传到 Flink 集群。</p>
<h3 id="splitenumerator">SplitEnumerator</h3>
<p><a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/SplitEnumerator.java">SplitEnumerator</a> 有望成为 Source 的&quot;大脑&quot;。SplitEnumerator 的典型实现会做以下工作。</p>
<ul>
<li>SourceReader 注册处理</li>
<li>SourceReader 失败处理
<ul>
<li>当 SourceReader 失败时，将调用 <code>addSplitsBack()</code> 方法。SplitEnumerator 应该收回未被失败的 SourceReader 承认的分割分配。</li>
</ul>
</li>
<li>SourceEvent 处理
<ul>
<li>SourceEvents 是在 SplitEnumerator 和 SourceReader 之间发送的自定义事件。实现可以利用这种机制来进行复杂的协调。</li>
</ul>
</li>
<li>分割发现和分配
<ul>
<li>SplitEnumerator 可以根据各种事件将 split 分配给 SourceReaders ，包括发现新的 split 、新的 SourceReader 注册、 SourceReader 失败等。</li>
</ul>
</li>
</ul>
<p>SplitEnumerator 可以借助 <a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/SplitEnumeratorContext.java">SplitEnumeratorContext</a> 完成上述工作， SplitEnumeratorContext 是在创建或恢复 SplitEnumerator 时提供给 Source 的。SplitEnumeratorContext 允许 SplitEnumerator 检索读取器的必要信息并执行协调动作。Source 实现应该将 SplitEnumeratorContext 传递给 SplitEnumerator 实例。</p>
<p>虽然 SplitEnumerator 实现可以通过只在它的方法被调用时采取协调动作的被动方式很好地工作，但一些 SplitEnumerator 实现可能希望主动采取行动。例如，一个 SplitEnumerator 可能希望定期运行 split discovery ，并将新的 split 分配给 SourceReaders 。这样的实现可能会发现调用 Async() 方法 SplitEnumeratorContext 很方便。下面的代码片段展示了 SplitEnumerator 实现如何在不维护自己的线程的情况下实现这一点。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">MySplitEnumerator</span> <span class="n">implements</span> <span class="nc">SplitEnumerator</span><span class="o">&lt;</span><span class="nc">MySplit</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="k">private</span> <span class="k">final</span> <span class="n">long</span> <span class="nc">DISCOVER_INTERVAL</span> <span class="k">=</span> <span class="mi">60</span><span class="n">_000L</span><span class="o">;</span>

    <span class="cm">/**
</span><span class="cm">     * A method to discover the splits.
</span><span class="cm">     */</span>
    <span class="k">private</span> <span class="nc">List</span><span class="o">&lt;</span><span class="nc">MySplit</span><span class="o">&gt;</span> <span class="n">discoverSplits</span><span class="o">()</span> <span class="o">{...}</span>
    
    <span class="nd">@Override</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">start</span><span class="o">()</span> <span class="o">{</span>
        <span class="o">...</span>
        <span class="n">enumContext</span><span class="o">.</span><span class="n">callAsync</span><span class="o">(</span><span class="k">this:</span><span class="kt">:discoverSplits</span><span class="o">,</span> <span class="n">splits</span> <span class="o">-&gt;</span> <span class="o">{</span>
            <span class="nc">Map</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">,</span> <span class="nc">List</span><span class="o">&lt;</span><span class="nc">MockSourceSplit</span><span class="o">&gt;&gt;</span> <span class="n">assignments</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HashMap</span><span class="o">&lt;&gt;();</span>
            <span class="n">int</span> <span class="n">parallelism</span> <span class="k">=</span> <span class="n">enumContext</span><span class="o">.</span><span class="n">currentParallelism</span><span class="o">();</span>
            <span class="k">for</span> <span class="o">(</span><span class="nc">MockSourceSplit</span> <span class="n">split</span> <span class="k">:</span> <span class="kt">splits</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">int</span> <span class="n">owner</span> <span class="k">=</span> <span class="n">split</span><span class="o">.</span><span class="n">splitId</span><span class="o">().</span><span class="n">hashCode</span><span class="o">()</span> <span class="o">%</span> <span class="n">parallelism</span><span class="o">;</span>
                <span class="n">assignments</span><span class="o">.</span><span class="n">computeIfAbsent</span><span class="o">(</span><span class="n">owner</span><span class="o">,</span> <span class="k">new</span> <span class="nc">ArrayList</span><span class="o">&lt;&gt;()).</span><span class="n">add</span><span class="o">(</span><span class="n">split</span><span class="o">);</span>
            <span class="o">}</span>
            <span class="n">enumContext</span><span class="o">.</span><span class="n">assignSplits</span><span class="o">(</span><span class="k">new</span> <span class="nc">SplitsAssignment</span><span class="o">&lt;&gt;(</span><span class="n">assignments</span><span class="o">));</span>
        <span class="o">},</span> <span class="mi">0L</span><span class="o">,</span> <span class="nc">DISCOVER_INTERVAL</span><span class="o">);</span>
        <span class="o">...</span>
    <span class="o">}</span>
    <span class="o">...</span>
<span class="o">}</span>
</code></pre></div><h2 id="sourcereader">SourceReader</h2>
<p>SourceReader 是一个运行在 Task Manager 中的组件，用于消耗来自 Splits 的记录。</p>
<p>SourceReader 暴露了一个基于拉的消费接口。一个 Flink 任务在循环中不断调用 pollNext(ReaderOutput) 来轮询 SourceReader 的记录。pollNext(ReaderOutput) 方法的返回值表示源阅读器的状态。</p>
<ul>
<li>MORE_AVAILABLE - SourceReader 立即有更多的记录可用。</li>
<li>NOTHING_AVAILABLE - SourceReader 此时没有更多的记录可用，但将来可能会有更多的记录。</li>
<li>END_OF_INPUT - SourceReader 已经用完了所有的记录，达到了数据的终点。这意味着 SourceReader 可以被关闭。</li>
</ul>
<p>为了保证性能，会给 pollNext(ReaderOutput) 方法提供一个 ReaderOutput ，所以如果有必要， SourceReader 可以在一次调用 pollNext() 的过程中发出多条记录。例如，有时外部系统的工作粒度是块。一个块可能包含多条记录，但源码只能在块的边界处进行检查点。在这种情况下， SourceReader 可以一次将一个块中的所有记录排放到 ReaderOutput 。但是，除非必要， SourceReader 的实现应该避免在一次 pollNext(ReaderOutput) 的调用中发射多条记录。这是因为从 SourceReader 中进行轮询的任务线程是在事件循环中工作的，不能阻塞。</p>
<p>SourceReader 的所有状态都应该维护在 SourceSplits 里面，这些状态在 snapshotState() 调用时返回。这样做可以在需要时将 SourceSplits 重新分配给其他 SourceReaders 。</p>
<p>在创建 SourceReader 时，会向 Source 提供一个 SourceReaderContext 。预计 Source 将把上下文传递给 SourceReader 实例。SourceReader 可以通过 SourceReaderContext 向其 SplitEnumerator 发送 SourceEvent 。Source 的一个典型的设计模式是让 SourceReaders 向 SplitEnumerator 报告它们的本地信息， SplitEnumerator 有一个全局视图来做决策。</p>
<p>SourceReader API 是一个低级的 API ，它允许用户手动处理 split ，并有自己的线程模型来获取和交接记录。为了方便 SourceReader 的实现， Flink 提供了一个 <a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/SourceReaderBase.java">SourceReaderBase</a> 类，大大减少了编写 SourceReader 的工作量。强烈建议连接器开发人员利用 SourceReaderBase ，而不是从头开始编写 SourceReaders 。更多细节请查看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/sources.html#the-split-reader-api">Split Reader API</a> 部分。</p>
<h3 id="使用-source">使用 Source</h3>
<p>为了从 Source 创建 DataStream ，需要将 Source 传递给 StreamExecutionEnvironment。例如:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="n">mySource</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MySource</span><span class="o">(...)</span>

<span class="k">val</span> <span class="n">stream</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromSource</span><span class="o">(</span>
      <span class="n">mySource</span><span class="o">,</span>
      <span class="nc">WatermarkStrategy</span><span class="o">.</span><span class="n">noWatermarks</span><span class="o">(),</span>
      <span class="s">&#34;MySourceName&#34;</span><span class="o">)</span>
<span class="o">...</span>
</code></pre></div><h2 id="split-读取器-api">Split 读取器 API</h2>
<p>核心的 SourceReader API 是完全异步的，需要实现者手动管理异步拆分读取。然而，在实践中，大多数 Source 使用执行阻塞操作，比如在客户端（例如 KafkaConsumer ）上阻塞 poll() 调用，或者在分布式文件系统（ HDFS ， S3 ，&hellip;）上阻塞 I/O 操作。为了与异步的 Source API 兼容，这些阻塞（同步）操作需要发生在单独的线程中，线程将数据交给异步部分的阅读器。</p>
<p><a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/splitreader/SplitReader.java">SplitReader</a> 是用于简单的基于同步读取/轮询的源码实现的高级 API ，比如文件读取、 Kafka 等。</p>
<p>核心是 SourceReaderBase 类，它接收一个 SplitReader 并创建运行 SplitReader 的 fetcher 线程，支持不同的消费线程模型。</p>
<h3 id="splitreader">SplitReader</h3>
<p>SplitReader API 只有三个方法。</p>
<ul>
<li>一个阻塞获取方法，返回一个 <a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/RecordsWithSplitIds.java">RecordsWithSplitIds</a> 。</li>
<li>一种非阻塞方法，用于处理拆分变化。</li>
<li>一个非阻塞的唤醒方法，用于唤醒阻塞的获取操作。</li>
</ul>
<p>SplitReader 只专注于从外部系统读取记录，因此比 SourceReader 简单得多。详情请查看该类的 Java 文档。</p>
<h3 id="sourcereaderbase">SourceReaderBase</h3>
<p>SourceReader 的实现很常见，它做了以下工作。</p>
<ul>
<li>拥有一个线程池，以阻塞的方式从外部系统的分割处获取数据。</li>
<li>处理内部获取线程和其他方法调用之间的同步，如 pollNext(ReaderOutput) 。</li>
<li>维护每个 split 的水印，以便进行水印对齐。</li>
<li>维护每个分身的状态，以便检查点。</li>
</ul>
<p>为了减少编写一个新的 SourceReader 的工作， Flink 提供了一个 <a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/SourceReaderBase.java">SourceReaderBase</a> 类作为 SourceReader 的基础实现。SourceReaderBase 开箱即完成了上述所有工作。如果要编写一个新的 SourceReader ，只需要让 SourceReader 实现继承 SourceReaderBase ，填充一些方法，然后实现一个高级的 <a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/splitreader/SplitReader.java">SplitReader</a> 就可以了。</p>
<h3 id="splitfetchermanager">SplitFetcherManager</h3>
<p>SourceReaderBase 支持一些开箱即用的线程模型，这取决于与之合作的 <a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/fetcher/SplitFetcherManager.java">SplitFetcherManager</a> 的行为。SplitFetcherManager 帮助创建和维护一个 SplitFetcher 池，每个 SplitFetcher 用一个 SplitReader 来获取。它还决定了如何将 split 分配给每个 split fetcher 。</p>
<p>举个例子，如下图所示，一个 SplitFetcherManager 可能有固定数量的线程，每个线程从分配给 SourceReader 的一些 split 中获取。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/source_reader.svg" alt="img"></p>
<p>下面的代码片段实现了这个线程模型。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="cm">/**
</span><span class="cm"> * A SplitFetcherManager that has a fixed size of split fetchers and assign splits 
</span><span class="cm"> * to the split fetchers based on the hash code of split IDs.
</span><span class="cm"> */</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">FixedSizeSplitFetcherManager</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">SplitT</span> <span class="kd">extends</span> <span class="n">SourceSplit</span><span class="o">&gt;</span> 
        <span class="kd">extends</span> <span class="n">SplitFetcherManager</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">SplitT</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">private</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">numFetchers</span><span class="o">;</span>

    <span class="kd">public</span> <span class="nf">FixedSizeSplitFetcherManager</span><span class="o">(</span>
            <span class="kt">int</span> <span class="n">numFetchers</span><span class="o">,</span>
            <span class="n">FutureNotifier</span> <span class="n">futureNotifier</span><span class="o">,</span>
            <span class="n">FutureCompletingBlockingQueue</span><span class="o">&lt;</span><span class="n">RecordsWithSplitIds</span><span class="o">&lt;</span><span class="n">E</span><span class="o">&gt;&gt;</span> <span class="n">elementsQueue</span><span class="o">,</span>
            <span class="n">Supplier</span><span class="o">&lt;</span><span class="n">SplitReader</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">SplitT</span><span class="o">&gt;&gt;</span> <span class="n">splitReaderSupplier</span><span class="o">)</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span><span class="n">futureNotifier</span><span class="o">,</span> <span class="n">elementsQueue</span><span class="o">,</span> <span class="n">splitReaderSupplier</span><span class="o">);</span>
        <span class="k">this</span><span class="o">.</span><span class="na">numFetchers</span> <span class="o">=</span> <span class="n">numFetchers</span><span class="o">;</span>
        <span class="c1">// Create numFetchers split fetchers.
</span><span class="c1"></span>        <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numFetchers</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
            <span class="n">startFetcher</span><span class="o">(</span><span class="n">createSplitFetcher</span><span class="o">());</span>
        <span class="o">}</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">addSplits</span><span class="o">(</span><span class="n">List</span><span class="o">&lt;</span><span class="n">SplitT</span><span class="o">&gt;</span> <span class="n">splitsToAdd</span><span class="o">)</span> <span class="o">{</span>
        <span class="c1">// Group splits by their owner fetchers.
</span><span class="c1"></span>        <span class="n">Map</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">SplitT</span><span class="o">&gt;&gt;</span> <span class="n">splitsByFetcherIndex</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashMap</span><span class="o">&lt;&gt;();</span>
        <span class="n">splitsToAdd</span><span class="o">.</span><span class="na">forEach</span><span class="o">(</span><span class="n">split</span> <span class="o">-&gt;</span> <span class="o">{</span>
            <span class="kt">int</span> <span class="n">ownerFetcherIndex</span> <span class="o">=</span> <span class="n">split</span><span class="o">.</span><span class="na">hashCode</span><span class="o">()</span> <span class="o">%</span> <span class="n">numFetchers</span><span class="o">;</span>
            <span class="n">splitsByFetcherIndex</span>
                    <span class="o">.</span><span class="na">computeIfAbsent</span><span class="o">(</span><span class="n">ownerFetcherIndex</span><span class="o">,</span> <span class="n">s</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;())</span>
                    <span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">split</span><span class="o">);</span>
        <span class="o">});</span>
        <span class="c1">// Assign the splits to their owner fetcher.
</span><span class="c1"></span>        <span class="n">splitsByFetcherIndex</span><span class="o">.</span><span class="na">forEach</span><span class="o">((</span><span class="n">fetcherIndex</span><span class="o">,</span> <span class="n">splitsForFetcher</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">{</span>
            <span class="n">fetchers</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">fetcherIndex</span><span class="o">).</span><span class="na">addSplits</span><span class="o">(</span><span class="n">splitsForFetcher</span><span class="o">);</span>
        <span class="o">});</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>而使用这个线程模型的 SourceReader 可以创建如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">FixedFetcherSizeSourceReader</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">T</span><span class="o">,</span> <span class="n">SplitT</span> <span class="kd">extends</span> <span class="n">SourceSplit</span><span class="o">,</span> <span class="n">SplitStateT</span><span class="o">&gt;</span>
        <span class="kd">extends</span> <span class="n">SourceReaderBase</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">T</span><span class="o">,</span> <span class="n">SplitT</span><span class="o">,</span> <span class="n">SplitStateT</span><span class="o">&gt;</span> <span class="o">{</span>

    <span class="kd">public</span> <span class="nf">FixedFetcherSizeSourceReader</span><span class="o">(</span>
            <span class="n">FutureNotifier</span> <span class="n">futureNotifier</span><span class="o">,</span>
            <span class="n">FutureCompletingBlockingQueue</span><span class="o">&lt;</span><span class="n">RecordsWithSplitIds</span><span class="o">&lt;</span><span class="n">E</span><span class="o">&gt;&gt;</span> <span class="n">elementsQueue</span><span class="o">,</span>
            <span class="n">Supplier</span><span class="o">&lt;</span><span class="n">SplitReader</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">SplitT</span><span class="o">&gt;&gt;</span> <span class="n">splitFetcherSupplier</span><span class="o">,</span>
            <span class="n">RecordEmitter</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">T</span><span class="o">,</span> <span class="n">SplitStateT</span><span class="o">&gt;</span> <span class="n">recordEmitter</span><span class="o">,</span>
            <span class="n">Configuration</span> <span class="n">config</span><span class="o">,</span>
            <span class="n">SourceReaderContext</span> <span class="n">context</span><span class="o">)</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span>
                <span class="n">futureNotifier</span><span class="o">,</span>
                <span class="n">elementsQueue</span><span class="o">,</span>
                <span class="k">new</span> <span class="n">FixedSizeSplitFetcherManager</span><span class="o">&lt;&gt;(</span>
                        <span class="n">config</span><span class="o">.</span><span class="na">getInteger</span><span class="o">(</span><span class="n">SourceConfig</span><span class="o">.</span><span class="na">NUM_FETCHERS</span><span class="o">),</span>
                        <span class="n">futureNotifier</span><span class="o">,</span>
                        <span class="n">elementsQueue</span><span class="o">,</span>
                        <span class="n">splitFetcherSupplier</span><span class="o">),</span>
                <span class="n">recordEmitter</span><span class="o">,</span>
                <span class="n">config</span><span class="o">,</span>
                <span class="n">context</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">onSplitFinished</span><span class="o">(</span><span class="n">Collection</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">finishedSplitIds</span><span class="o">)</span> <span class="o">{</span>
        <span class="c1">// Do something in the callback for the finished splits.
</span><span class="c1"></span>    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">SplitStateT</span> <span class="nf">initializedState</span><span class="o">(</span><span class="n">SplitT</span> <span class="n">split</span><span class="o">)</span> <span class="o">{</span>
        <span class="o">...</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">SplitT</span> <span class="nf">toSplitType</span><span class="o">(</span><span class="n">String</span> <span class="n">splitId</span><span class="o">,</span> <span class="n">SplitStateT</span> <span class="n">splitState</span><span class="o">)</span> <span class="o">{</span>
        <span class="o">...</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>显然， SourceReader 的实现也可以在 SplitFetcherManager 和 SourceReaderBase 之上轻松实现自己的线程模型。</p>
<h2 id="事件时间和水印">事件时间和水印</h2>
<p>事件时间分配和水印生成作为数据源的一部分发生。离开源读取器的事件流具有事件时间戳，并且（在流执行期间）包含水印。有关事件时间和水印的介绍，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/concepts/timely-stream-processing.html">及时流处理</a>。</p>
<p>重要事项 基于传统 <a href="https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java">SourceFunction</a> 的应用程序通常会在后面单独的步骤中通过 stream.assignTimestampsAndWatermarks(WatermarkStrategy) 生成时间戳和水印。这个函数不应该被用于新的源，因为时间戳将被分配，并且它将覆盖之前的分割感知水印。</p>
<h3 id="api">API</h3>
<p>在 DataStream API 创建期间， WatermarkStrategy 被传递给 Source，并创建 <a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/common/eventtime/TimestampAssigner.java">TimestampAssigner</a> 和 <a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/common/eventtime/WatermarkGenerator.java">WatermarkGenerator</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">environment</span><span class="o">.</span><span class="n">fromSource</span><span class="o">(</span>
    <span class="nc">Source</span><span class="o">&lt;</span><span class="nc">OUT</span><span class="o">,</span> <span class="o">?,</span> <span class="o">?&gt;</span> <span class="n">source</span><span class="o">,</span>
    <span class="nc">WatermarkStrategy</span><span class="o">&lt;</span><span class="nc">OUT</span><span class="o">&gt;</span> <span class="n">timestampsAndWatermarks</span><span class="o">,</span>
    <span class="nc">String</span> <span class="n">sourceName</span><span class="o">)</span>
</code></pre></div><p>TimestampAssigner 和 WatermarkGenerator 作为 ReaderOutput(或 SourceOutput) 的一部分透明地运行，因此源码实现者不必实现任何时间戳提取和水印生成代码。</p>
<h3 id="事件时间戳">事件时间戳</h3>
<p>事件时间戳的分配有两个步骤。</p>
<ol>
<li>
<p>SourceReader 可以通过调用 SourceOutput.collect(event, timestamp) 将源记录时间戳附加到事件上。这只与基于记录且有时间戳的数据源有关，如 Kafka 、 Kinesis 、 Pulsar 或 Pravega 。不基于记录且有时间戳的数据源（如文件）没有源记录时间戳。这一步是源连接器实现的一部分，而不是由使用源的应用程序参数化。</p>
</li>
<li>
<p>由应用程序配置的 TimestampAssigner 分配最终的时间戳。TimestampAssigner 看到原始源记录时间戳和事件。分配者可以使用源记录时间戳或访问事件的一个字段获得最终的事件时间戳。</p>
</li>
</ol>
<p>这种两步法允许用户同时引用源系统的时间戳和事件数据中的时间戳作为事件时间戳。</p>
<p>注意：当使用没有源记录时间戳的数据源（如文件），并选择源记录时间戳作为最终的事件时间戳时，事件将得到一个默认的时间戳，等于 LONG_MIN （=-9,223,372,036,854,775,808 ）。</p>
<h3 id="水印生成">水印生成</h3>
<p>水印生成器仅在流式执行期间激活。批量执行会停用水印生成器；下面描述的所有相关操作都将成为有效的无操作。</p>
<p>数据源 API 支持每次拆分单独运行水印生成器。这使得 Flink 可以单独观察每个分体的事件时间进度，这对于正确处理事件时间偏斜和防止空闲分区拖累整个应用的事件时间进度非常重要。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/per_split_watermarks.svg" alt="img"></p>
<p>当使用 Split Reader API 实现一个源连接器时，会自动处理这个问题。所有基于 Split Reader API 的实现都具有开箱即用的 split-aware 水印。</p>
<p>对于一个低级别的 SourceReader API 的实现来说，要使用 split-aware 水印的生成，该实现必须将不同的 split 事件输出到不同的输出中： Split-local SourceOutputs 。分割本地输出可以通过 createOutputForSplit(splitId) 和 releaseOutputForSplit(splitId) 方法在主 <a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/ReaderOutput.java">ReaderOutput</a> 上创建和释放。详情请参考该类和方法的 JavaDocs 。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/sources.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/sources.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/data-sources" term="data-sources" label="Data Sources" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[检查点]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-checkpointing/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-checkpointing/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Checkpointing</blockquote><p>Flink 中的每一个函数和操作符都可以是有状态的（详情请看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">使用状态</a>）。有状态的函数在单个元素/事件的处理过程中存储数据，使得状态成为任何类型的更复杂操作的关键构建模块。</p>
<p>为了使状态具有容错性，Flink 需要对状态进行 <strong>checkpoint</strong>。检查点允许 Flink 恢复流中的状态和位置，使应用程序具有与无故障执行相同的语义。</p>
<p>关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/learn-flink/fault_tolerance.html">流式容错的文档</a>详细描述了 Flink 的流式容错机制背后的技术。</p>
<h2 id="前提条件">前提条件</h2>
<p>Flink 的检查点机制与流和状态的持久存储交互。一般来说，它需要:</p>
<ul>
<li>一个能在一定时间内重放记录(replay records)的持久（或耐用）数据源。这种源的例子是持久性消息队列（如 Apache Kafka、RabbitMQ、Amazon Kinesis、Google PubSub）或文件系统（如 HDFS、S3、GFS、NFS、Ceph&hellip;）。</li>
<li>状态的持久性存储，通常是一个分布式文件系统（如 HDFS、S3、GFS、NFS、Ceph&hellip;）。</li>
</ul>
<h2 id="启用和配置检查点">启用和配置检查点</h2>
<p>默认情况下，检查点被禁用。要启用检查点，在 <code>StreamExecutionEnvironment</code> 上调用 <code>enableCheckpointing(n)</code>，其中 <em>n</em> 是检查点间隔，单位为毫秒。</p>
<p>检查点的其他参数包括:</p>
<ul>
<li>
<p>exactly-once vs. at-least-once：你可以选择向 <code>enableCheckpointing(n)</code> 方法传递一个模式，以便在两个保证级别之间进行选择。对于大多数应用来说，exactly-once 是比较好的。At-least-once 可能适用于某些超低延迟（持续几毫秒）的应用。</p>
</li>
<li>
<p>检查点超时。如果一个正在进行中的检查点没有完成，那么它被中止的时间。</p>
</li>
<li>
<p>检查点之间的最小时间。为了确保流应用在检查点之间有一定的进度，可以定义检查点之间需要经过多少时间。例如，如果这个值设置为5000，那么下一个检查点将在上一个检查点完成后不早于5秒开始，无论检查点持续时间和检查点间隔如何。请注意，这意味着检查点间隔永远不会小于这个参数。</p>
</li>
</ul>
<p>通过定义&quot;检查点之间的时间&quot;(time between checkpoints)通常比检查点间隔更容易配置应用程序，因为&quot;检查点之间的时间&quot;不容易受到检查点有时可能比平均时间长的事实的影响（例如，如果目标存储系统暂时缓慢）。</p>
<p>请注意，这个值也意味着并发检查点的数量为1。</p>
<ul>
<li>并发检查点的数量。默认情况下，当一个检查点仍在进行时，系统不会触发另一个检查点。这可以确保拓扑不会在检查点上花费太多时间，而使处理流的工作没有进展。可以允许多个重叠的检查点，这对于那些有一定处理延迟（例如因为函数调用外部服务，需要一些时间来响应），但仍然希望做非常频繁的检查点（100s毫秒），以便在故障时重新处理很少的管道来说是很有意思的。</li>
</ul>
<p>当定义了检查点之间的最小时间时，不能使用这个选项。</p>
<ul>
<li>
<p>外部化检查点。您可以配置周期性检查点，使其在外部持久化。外部化的检查点会将它们的元数据写入持久化存储中，当作业失败时不会自动清理。这样一来，如果你的工作失败了，你身边就会有一个检查点来恢复。关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/state/checkpoints.html#externalized-checkpoints">外部化检查点的部署说明</a>中有更多细节。</p>
</li>
<li>
<p>Fail/checkpoint 错误时继续执行任务。这决定了如果在执行任务的检查点过程中出现错误，任务是否会失败。这是默认行为。另外，当禁用该功能时，任务将简单地拒绝向检查点协调器提供检查点并继续运行。</p>
</li>
<li>
<p>更喜欢用于恢复的检查点。这决定了即使有更近的保存点可用时，任务是否会回退到最新的检查点，以减少恢复时间。</p>
</li>
<li>
<p>不对齐的检查点。你可以启用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/state/checkpoints.html#unaligned-checkpoints">不对齐的检查点</a>，以大大减少背压下的检查点时间。仅适用于精确的一次检查点，且并发检查点数量为1。</p>
</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>

<span class="c1">// start a checkpoint every 1000 ms
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">enableCheckpointing</span><span class="o">(</span><span class="mi">1000</span><span class="o">)</span>

<span class="c1">// 高级选项:
</span><span class="c1"></span>
<span class="c1">// 设置模式为 exactly-once (这是默认的)
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">setCheckpointingMode</span><span class="o">(</span><span class="nc">CheckpointingMode</span><span class="o">.</span><span class="nc">EXACTLY_ONCE</span><span class="o">)</span>

<span class="c1">// make sure 500 ms of progress happen between checkpoints
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">setMinPauseBetweenCheckpoints</span><span class="o">(</span><span class="mi">500</span><span class="o">)</span>

<span class="c1">// checkpoints have to complete within one minute, or are discarded
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">setCheckpointTimeout</span><span class="o">(</span><span class="mi">60000</span><span class="o">)</span>

<span class="c1">// prevent the tasks from failing if an error happens in their checkpointing, the checkpoint will just be declined.
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">setFailTasksOnCheckpointingErrors</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="c1">// allow only one checkpoint to be in progress at the same time
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">setMaxConcurrentCheckpoints</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

<span class="c1">// enables the experimental unaligned checkpoints
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">enableUnalignedCheckpoints</span><span class="o">()</span>
</code></pre></div><h3 id="相关配置选项">相关配置选项</h3>
<p>更多的参数和/或默认值可以通过 <code>conf/flink-conf.yaml</code> 来设置（参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html">配置</a>的完整指南）。</p>
<table>
<thead>
<tr>
<th style="text-align:left">键</th>
<th style="text-align:left">默认值</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">state.backend</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">String</td>
<td style="text-align:left">用于存储和 checkpoint 状态的状态后端。</td>
</tr>
<tr>
<td style="text-align:left">state.backend.async</td>
<td style="text-align:left">true</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">状态后端是否应该在可能的情况下使用异步快照方法的选项，可配置。有些状态后端可能不支持异步快照，或者只支持异步快照，而忽略这个选项。</td>
</tr>
<tr>
<td style="text-align:left">state.backend.fs.memory-threshold</td>
<td style="text-align:left">20 kb</td>
<td style="text-align:left">MemorySize</td>
<td style="text-align:left">状态数据文件的最小尺寸。小于这个大小的所有状态块都内嵌存储在根检查点元数据文件中。该配置的最大内存阈值为1MB。</td>
</tr>
<tr>
<td style="text-align:left">state.backend.fs.write-buffer-size</td>
<td style="text-align:left">4096</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">写入文件系统的检查点流的默认写缓冲区大小。实际的写缓冲区大小是由这个选项和选项 &lsquo;state.backend.fs.memory-threshold&rsquo; 的最大值决定的。</td>
</tr>
<tr>
<td style="text-align:left">state.backend.incremental</td>
<td style="text-align:left">false</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">如果可能，状态后端是否应该创建增量检查点。对于增量检查点，只存储与前一个检查点的差异，而不是完整的检查点状态。一旦启用，在 Web UI 中显示的状态大小或从 rest API 中获取的状态大小只代表 delta 检查点大小，而不是完整的检查点大小。一些状态后端可能不支持增量检查点而忽略这个选项。</td>
</tr>
<tr>
<td style="text-align:left">state.backend.local-recovery</td>
<td style="text-align:left">false</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">这个选项可以配置这个状态后端的本地恢复。默认情况下，本地恢复是被停用的。本地恢复目前只覆盖 keyed state 后端。目前，MemoryStateBackend 不支持本地恢复，忽略此选项。</td>
</tr>
<tr>
<td style="text-align:left">state.checkpoints.dir</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">String</td>
<td style="text-align:left">在 Flink 支持的文件系统中，用于存储检查点数据文件和元数据的默认目录。该存储路径必须可以从所有参与进程/节点（即所有 TaskManager 和 JobManager）访问。</td>
</tr>
<tr>
<td style="text-align:left">state.checkpoints.num-retained</td>
<td style="text-align:left">1</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">保留已完成的检查点的最大数量。</td>
</tr>
<tr>
<td style="text-align:left">state.savepoints.dir</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">String</td>
<td style="text-align:left">保存点的默认目录。由将保存点写入文件系统的状态后端（MemoryStateBackend, FsStateBackend, RocksDBStateBackend）使用。</td>
</tr>
<tr>
<td style="text-align:left">taskmanager.state.local.root-dirs</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">String</td>
<td style="text-align:left">配置参数，定义本地恢复中存储基于文件的状态的根目录。本地恢复目前只覆盖 keyed state 后端。目前，MemoryStateBackend 不支持本地恢复，忽略这个选项。</td>
</tr>
</tbody>
</table>
<h3 id="选择状态后端">选择状态后端</h3>
<p>Flink 的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/learn-flink/fault_tolerance.html">检查点机制</a>在定时器和有状态的操作符中存储所有状态的一致快照，包括连接器、窗口和任何<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">用户定义的状态</a>。检查点存储的位置（例如，JobManager内存、文件系统、数据库）取决于配置的状态后端。</p>
<p>默认情况下，状态保存在 TaskManager 的内存中，检查点保存在 JobManager 的内存中。为了正确地持久化大状态，Flink 支持各种方法在其他状态后端存储和检查点状态。状态后端的选择可以通过 <code>StreamExecutionEnvironment.setStateBackend(...)</code> 进行配置。</p>
<p>有关可用的状态后端以及作业范围(job-wide)和集群范围(cluster-wide)配置选项的更多细节，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/state/state_backends.html">状态后端</a>。</p>
<h3 id="迭代作业中的状态检查点">迭代作业中的状态检查点</h3>
<p>Flink 目前只为没有迭代的作业提供处理保证。在迭代作业上启用检查点会导致异常。为了在迭代程序上强制检查点，用户需要在启用检查点时设置一个特殊标志：<code>env.enableCheckpointing(interval, CheckpointingMode.EXACTLY_ONCE, force = true)</code>。</p>
<p>请注意，循环边缘中飞行中的记录（以及与之相关的状态变化）将在失败时丢失。</p>
<h3 id="重新启动策略">重新启动策略</h3>
<p>Flink 支持不同的重启策略，这些策略可以控制作业(job)在发生故障时如何重启。更多信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/task_failure_recovery.html">重启策略</a>。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[测试]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-testing/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-testing/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Testing</blockquote><h1 id="测试">测试</h1>
<p>测试是每个软件开发过程中不可缺少的一部分，因此 Apache Flink 提供的工具可以在测试金字塔的多个层次上测试你的应用程序代码。</p>
<h2 id="测试用户自定义函数">测试用户自定义函数</h2>
<p>通常，我们可以假设 Flink 在用户定义的函数之外产生正确的结果。因此，建议尽可能用单元测试来测试那些包含主要业务逻辑的类。</p>
<h3 id="单元测试无状态timeless-udfs">单元测试无状态、Timeless UDFs。</h3>
<p>例如，我们来看看下面的无状态 MapFunction。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">IncrementMapFunction</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">]</span> <span class="o">{</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">record</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">record</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>通过传递合适的参数和验证输出，用你最喜欢的测试框架对这样的函数进行单元测试是非常容易的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">IncrementMapFunctionTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">Matchers</span> <span class="o">{</span>

    <span class="s">&#34;IncrementMapFunction&#34;</span> <span class="n">should</span> <span class="s">&#34;increment values&#34;</span> <span class="n">in</span> <span class="o">{</span>
        <span class="c1">// instantiate your function
</span><span class="c1"></span>        <span class="k">val</span> <span class="n">incrementer</span><span class="k">:</span> <span class="kt">IncrementMapFunction</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">IncrementMapFunction</span><span class="o">()</span>

        <span class="c1">// call the methods that you have implemented
</span><span class="c1"></span>        <span class="n">incremeter</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span> <span class="n">should</span> <span class="n">be</span> <span class="o">(</span><span class="mi">3</span><span class="o">)</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>同样，使用 org.apache.flink.util.Collector 的用户定义函数（例如 FlatMapFunction 或 ProcessFunction）可以通过提供一个模拟对象而不是真实的 Collector 来轻松测试。一个与 IncrementMapFunction 功能相同的 FlatMapFunction 可以进行如下单元测试。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">IncrementFlatMapFunctionTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">MockFactory</span> <span class="o">{</span>

    <span class="s">&#34;IncrementFlatMapFunction&#34;</span> <span class="n">should</span> <span class="s">&#34;increment values&#34;</span> <span class="n">in</span> <span class="o">{</span>
       <span class="c1">// instantiate your function
</span><span class="c1"></span>      <span class="k">val</span> <span class="n">incrementer</span> <span class="k">:</span> <span class="kt">IncrementFlatMapFunction</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">IncrementFlatMapFunction</span><span class="o">()</span>

      <span class="k">val</span> <span class="n">collector</span> <span class="k">=</span> <span class="n">mock</span><span class="o">[</span><span class="kt">Collector</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]]</span>

      <span class="c1">//verify collector was called with the right output
</span><span class="c1"></span>      <span class="o">(</span><span class="n">collector</span><span class="o">.</span><span class="n">collect</span> <span class="k">_</span><span class="o">).</span><span class="n">expects</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>

      <span class="c1">// call the methods that you have implemented
</span><span class="c1"></span>      <span class="n">flattenFunction</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="n">collector</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="单元测试-有状态或及时的-udf-和自定义操作符">单元测试 有状态或及时的 UDF 和自定义操作符</h3>
<p>测试一个用户定义函数的功能是比较困难的，因为它涉及到测试用户代码和 Flink 运行时之间的交互。为此，Flink 提供了一个所谓的测试线束的集合，它可以用来测试这样的用户定义函数以及自定义操作符。</p>
<ul>
<li>OneInputStreamOperatorTestHarness(用于 DataStreams 上的操作符)</li>
<li>KeyedOneInputStreamOperatorTestHarness(用于 KeyedStreams 上的操作者)</li>
<li>TwoInputStreamOperatorTestHarness (适用于两个 DataStreams 的 ConnectedStreams 操作者)</li>
<li>KeyedTwoInputStreamOperatorTestHarness (用于两个 KeyedStream 的 ConnectedStreams 上的操作员)</li>
</ul>
<p>为了使用测试套件，需要一组额外的依赖关系（测试范围）。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-test-utils_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>test<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-runtime_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>test<span class="nt">&lt;/scope&gt;</span>
  <span class="nt">&lt;classifier&gt;</span>tests<span class="nt">&lt;/classifier&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-streaming-java_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>test<span class="nt">&lt;/scope&gt;</span>
  <span class="nt">&lt;classifier&gt;</span>tests<span class="nt">&lt;/classifier&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>现在，测试线束可以用来将记录和水印推送到你的用户定义函数或自定义运算符中，控制处理时间，最后对运算符的输出进行断言（包括侧输出）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">StatefulFlatMapFunctionTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">Matchers</span> <span class="k">with</span> <span class="nc">BeforeAndAfter</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">var</span> <span class="n">testHarness</span><span class="k">:</span> <span class="kt">OneInputStreamOperatorTestHarness</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span>
  <span class="k">private</span> <span class="k">var</span> <span class="n">statefulFlatMap</span><span class="k">:</span> <span class="kt">StatefulFlatMapFunction</span> <span class="o">=</span> <span class="kc">null</span>

  <span class="n">before</span> <span class="o">{</span>
    <span class="c1">//instantiate user-defined function
</span><span class="c1"></span>    <span class="n">statefulFlatMap</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StatefulFlatMap</span>

    <span class="c1">// wrap user defined function into a the corresponding operator
</span><span class="c1"></span>    <span class="n">testHarness</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">OneInputStreamOperatorTestHarness</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">](</span><span class="k">new</span> <span class="nc">StreamFlatMap</span><span class="o">(</span><span class="n">statefulFlatMap</span><span class="o">))</span>

    <span class="c1">// optionally configured the execution environment
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">getExecutionConfig</span><span class="o">().</span><span class="n">setAutoWatermarkInterval</span><span class="o">(</span><span class="mi">50</span><span class="o">);</span>

    <span class="c1">// open the test harness (will also call open() on RichFunctions)
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">open</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="s">&#34;StatefulFlatMap&#34;</span> <span class="n">should</span> <span class="s">&#34;do some fancy stuff with timers and state&#34;</span> <span class="n">in</span> <span class="o">{</span>


    <span class="c1">//push (timestamped) elements into the operator (and hence user defined function)
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">processElement</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">100</span><span class="o">);</span>

    <span class="c1">//trigger event time timers by advancing the event time of the operator with a watermark
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">processWatermark</span><span class="o">(</span><span class="mi">100</span><span class="o">);</span>

    <span class="c1">//trigger proccesign time timers by advancing the processing time of the operator directly
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">setProcessingTime</span><span class="o">(</span><span class="mi">100</span><span class="o">);</span>

    <span class="c1">//retrieve list of emitted records for assertions
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">getOutput</span> <span class="n">should</span> <span class="n">contain</span> <span class="o">(</span><span class="mi">3</span><span class="o">)</span>

    <span class="c1">//retrieve list of records emitted to a specific side output for assertions (ProcessFunction only)
</span><span class="c1"></span>    <span class="c1">//testHarness.getSideOutput(new OutputTag[Int](&#34;invalidRecords&#34;)) should have size 0
</span><span class="c1"></span>  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>KeyedOneInputStreamOperatorTestHarness 和 KeyedTwoInputStreamOperatorTestHarness 是通过额外提供一个包括键类的 TypeInformation 的 KeySelector 来实例化的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">StatefulFlatMapTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">Matchers</span> <span class="k">with</span> <span class="nc">BeforeAndAfter</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">var</span> <span class="n">testHarness</span><span class="k">:</span> <span class="kt">OneInputStreamOperatorTestHarness</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Long</span>, <span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span>
  <span class="k">private</span> <span class="k">var</span> <span class="n">statefulFlatMapFunction</span><span class="k">:</span> <span class="kt">FlattenFunction</span> <span class="o">=</span> <span class="kc">null</span>

  <span class="n">before</span> <span class="o">{</span>
    <span class="c1">//instantiate user-defined function
</span><span class="c1"></span>    <span class="n">statefulFlatMapFunction</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StateFulFlatMap</span>

    <span class="c1">// wrap user defined function into a the corresponding operator
</span><span class="c1"></span>    <span class="n">testHarness</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">KeyedOneInputStreamOperatorTestHarness</span><span class="o">(</span><span class="k">new</span> <span class="nc">StreamFlatMap</span><span class="o">(</span><span class="n">statefulFlatMapFunction</span><span class="o">),</span><span class="k">new</span> <span class="nc">MyStringKeySelector</span><span class="o">(),</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">STRING</span><span class="o">())</span>

    <span class="c1">// open the test harness (will also call open() on RichFunctions)
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">open</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="c1">//tests
</span><span class="c1"></span>
<span class="o">}</span>
</code></pre></div><p>在 Flink 代码库中还可以找到更多使用这些测试线束的例子，例如。</p>
<ul>
<li>org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest 是一个很好的例子，用于测试依赖于处理或事件时间的操作员和用户定义的函数。</li>
<li>org.apache.flink.streaming.api.function.sink.filesystem.LocalStreamingFileSinkTest 展示了如何使用 AbstractStreamOperatorTestHarness 测试自定义的 sink。具体来说，它使用 AbstractStreamOperatorTestHarness.snapshot 和 AbstractStreamOperatorTestHarness.initializeState 来测试它与 Flink 的检查点机制的交互。</li>
</ul>
<p>注意: AbstractStreamOperatorTestHarness 和它的派生类目前不是公共 API 的一部分，可能会发生变化。</p>
<h3 id="单元测试-processfunction">单元测试 ProcessFunction</h3>
<p>鉴于其重要性，除了之前的测试线束可以直接用于测试 ProcessFunction 外，Flink 还提供了一个名为 ProcessFunctionTestHarnesses 的测试线束工厂，可以更方便地进行测试线束实例化。考虑到这个例子。</p>
<p>注意: 要使用这个测试线束，你还需要引入上一节中提到的依赖关系。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">PassThroughProcessFunction</span> <span class="k">extends</span> <span class="nc">ProcessFunction</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">Integer</span><span class="o">]</span> <span class="o">{</span>

    <span class="nd">@throws</span><span class="o">[</span><span class="kt">Exception</span><span class="o">]</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">processElement</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">,</span> <span class="n">ctx</span><span class="k">:</span> <span class="kt">ProcessFunction</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">Integer</span><span class="o">]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Integer</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">value</span><span class="o">)</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>使用 ProcessFunctionTestHarnesses 对这样的函数进行单元测试是非常容易的，通过传递合适的参数并验证输出。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">PassThroughProcessFunctionTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">Matchers</span> <span class="o">{</span>

  <span class="s">&#34;PassThroughProcessFunction&#34;</span> <span class="n">should</span> <span class="s">&#34;forward values&#34;</span> <span class="n">in</span> <span class="o">{</span>

    <span class="c1">//instantiate user-defined function
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">processFunction</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">PassThroughProcessFunction</span>

    <span class="c1">// wrap user defined function into a the corresponding operator
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">harness</span> <span class="k">=</span> <span class="nc">ProcessFunctionTestHarnesses</span><span class="o">.</span><span class="n">forProcessFunction</span><span class="o">(</span><span class="n">processFunction</span><span class="o">)</span>

    <span class="c1">//push (timestamped) elements into the operator (and hence user defined function)
</span><span class="c1"></span>    <span class="n">harness</span><span class="o">.</span><span class="n">processElement</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">10</span><span class="o">)</span>

    <span class="c1">//retrieve list of emitted records for assertions
</span><span class="c1"></span>    <span class="n">harness</span><span class="o">.</span><span class="n">extractOutputValues</span><span class="o">()</span> <span class="n">should</span> <span class="n">contain</span> <span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>关于如何使用 ProcessFunctionTestHarnesses 来测试 ProcessFunction 的不同风味，如 KeyedProcessFunction、KeyedCoProcessFunction、BroadcastProcessFunction 等的更多例子，鼓励用户查看 ProcessFunctionTestHarnessesTest。</p>
<h2 id="测试-flink-作业">测试 Flink 作业</h2>
<h3 id="junit-规则-miniclusterwithclientresource">JUnit 规则 MiniClusterWithClientResource</h3>
<p>Apache Flink 提供了一个名为 MiniClusterWithClientResource 的 JUnit 规则，用于针对本地的、嵌入式的迷你集群测试完整的作业，名为 MiniClusterWithClientResource。</p>
<p>要使用 MiniClusterWithClientResource，需要一个额外的依赖（测试范围）。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-test-utils_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>让我们以前面几节中同样简单的 MapFunction 为例。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">IncrementMapFunction</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">]</span> <span class="o">{</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">record</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">record</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>现在可以在本地 Flink 集群中测试使用该 MapFunction 的简单管道，具体如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">StreamingJobIntegrationTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">Matchers</span> <span class="k">with</span> <span class="nc">BeforeAndAfter</span> <span class="o">{</span>

  <span class="k">val</span> <span class="n">flinkCluster</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MiniClusterWithClientResource</span><span class="o">(</span><span class="k">new</span> <span class="nc">MiniClusterResourceConfiguration</span><span class="o">.</span><span class="nc">Builder</span><span class="o">()</span>
    <span class="o">.</span><span class="n">setNumberSlotsPerTaskManager</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
    <span class="o">.</span><span class="n">setNumberTaskManagers</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
    <span class="o">.</span><span class="n">build</span><span class="o">)</span>

  <span class="n">before</span> <span class="o">{</span>
    <span class="n">flinkCluster</span><span class="o">.</span><span class="n">before</span><span class="o">()</span>
  <span class="o">}</span>

  <span class="n">after</span> <span class="o">{</span>
    <span class="n">flinkCluster</span><span class="o">.</span><span class="n">after</span><span class="o">()</span>
  <span class="o">}</span>


  <span class="s">&#34;IncrementFlatMapFunction pipeline&#34;</span> <span class="n">should</span> <span class="s">&#34;incrementValues&#34;</span> <span class="n">in</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

    <span class="c1">// configure your test environment
</span><span class="c1"></span>    <span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>

    <span class="c1">// values are collected in a static variable
</span><span class="c1"></span>    <span class="nc">CollectSink</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">clear</span><span class="o">()</span>

    <span class="c1">// create a stream of custom elements and apply transformations
</span><span class="c1"></span>    <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">21</span><span class="o">,</span> <span class="mi">22</span><span class="o">)</span>
       <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">new</span> <span class="nc">IncrementMapFunction</span><span class="o">())</span>
       <span class="o">.</span><span class="n">addSink</span><span class="o">(</span><span class="k">new</span> <span class="nc">CollectSink</span><span class="o">())</span>

    <span class="c1">// execute
</span><span class="c1"></span>    <span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>

    <span class="c1">// verify your results
</span><span class="c1"></span>    <span class="nc">CollectSink</span><span class="o">.</span><span class="n">values</span> <span class="n">should</span> <span class="n">contain</span> <span class="n">allOf</span> <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">22</span><span class="o">,</span> <span class="mi">23</span><span class="o">)</span>
    <span class="o">}</span>
<span class="o">}</span>
<span class="c1">// create a testing sink
</span><span class="c1"></span><span class="k">class</span> <span class="nc">CollectSink</span> <span class="k">extends</span> <span class="nc">SinkFunction</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">invoke</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">synchronized</span> <span class="o">{</span>
      <span class="nc">CollectSink</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">value</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">object</span> <span class="nc">CollectSink</span> <span class="o">{</span>
    <span class="c1">// must be static
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">values</span><span class="k">:</span> <span class="kt">util.List</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="k">new</span> <span class="n">util</span><span class="o">.</span><span class="nc">ArrayList</span><span class="o">()</span>
<span class="o">}</span>
</code></pre></div><p>关于 MiniClusterWithClientResource 的集成测试的几点说明。</p>
<ul>
<li>
<p>为了不把你的整个流水线代码从生产中复制到测试中，请在你的生产代码中使源和汇可插拔，并在你的测试中注入特殊的测试源和测试汇。</p>
</li>
<li>
<p>这里使用了 CollectSink 中的静态变量，因为 Flink 在将所有操作符分布在集群中之前，会将它们序列化。通过静态变量与本地 Flink 迷你集群实例化的运算符进行通信是解决这个问题的一种方法。另外，你可以将数据写到与你的测试汇的临时目录中的文件中。</p>
</li>
<li>
<p>如果你的作业使用事件时间计时器，你可以实现一个自定义的并行源函数来发射水印。</p>
</li>
<li>
<p>建议始终以并行度 <code>&gt;1</code> 的方式在本地测试你的流水线，以识别只有并行执行的流水线才会出现的错误。</p>
</li>
<li>
<p>优先选择 <code>@ClassRule</code> 而不是 <code>@Rule</code>，这样多个测试可以共享同一个 Flink 集群。这样做可以节省大量的时间，因为 Flink 集群的启动和关闭通常会支配实际测试的执行时间。</p>
</li>
<li>
<p>如果你的管道包含自定义状态处理，你可以通过启用检查点并在迷你集群内重新启动作业来测试其正确性。为此，你需要通过从你的管道中的（仅测试的）用户定义函数中抛出一个异常来触发失败。</p>
</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/testing" term="testing" label="testing" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[状态后端]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-state-backends/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-state-backends/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>State Backends</blockquote><h2 id="状态后端">状态后端</h2>
<p>Flink 提供了不同的状态后端，指定状态的存储方式和位置。</p>
<p>状态可以位于 Java 的堆上或离堆(off-heap)。根据你的状态后端，Flink 还可以为应用程序管理状态，这意味着 Flink 处理内存管理（必要时可能会溢出到磁盘），以允许应用程序持有非常大的状态。默认情况下，配置文件 <em>flink-conf.yaml</em> 决定了所有 Flink 作业(job)的状态后端。</p>
<p>然而，默认的状态后端可以在每个作业(per-job)的基础上被重写，如下所示。</p>
<p>有关可用的状态后端、其优势、限制和配置参数的更多信息，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/state/state_backends.html">部署与操作</a>中的相应章节。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>
<span class="n">env</span><span class="o">.</span><span class="n">setStateBackend</span><span class="o">(...)</span>
</code></pre></div><p>状态后端: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state_backends.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state_backends.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[状态模式的演变]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-state-schema-evolution/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-state-schema-evolution/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>State Schema Evolution</blockquote><p>Apache Flink 流媒体应用通常被设计为无限期或长时间运行。与所有长期运行的服务一样，应用程序需要更新以适应不断变化的需求。这对于应用程序所针对的数据模式(data schema)也是一样的，它们会随着应用程序的发展而发展。</p>
<p>本页提供了关于如何演进状态类型的数据模式(data schema)的概述。当前的限制在不同的类型和状态结构（<code>ValueState</code>、<code>ListState</code> 等）中有所不同。</p>
<p>请注意，本页面上的信息仅在您使用由 Flink 自己的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html">类型序列化框架</a>生成的状态序列化器时相关。也就是说，在声明你的状态时，所提供的状态描述符并没有被配置为使用特定的 <code>TypeSerializer</code> 或 <code>TypeInformation</code>，在这种情况下，Flink 会推导出状态类型的信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ListStateDescriptor</span><span class="o">&lt;</span><span class="n">MyPojoType</span><span class="o">&gt;</span> <span class="n">descriptor</span> <span class="o">=</span>
    <span class="k">new</span> <span class="n">ListStateDescriptor</span><span class="o">&lt;&gt;(</span>
        <span class="s">&#34;state-name&#34;</span><span class="o">,</span>
        <span class="n">MyPojoType</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

<span class="n">checkpointedState</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getListState</span><span class="o">(</span><span class="n">descriptor</span><span class="o">);</span>
</code></pre></div><p>在底层，状态的模式(schema)是否可以被演化取决于用于读取/写入持久化状态字节的序列化器。简单地说，只有当它的序列化器正确地支持时，一个注册状态的模式才能被演化。这是由 Flink 的类型序列化框架生成的序列化器透明地处理的（当前的支持范围列在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/schema_evolution.html#supported-data-types-for-schema-evolution">下面</a>）。</p>
<p>如果你打算为你的状态类型实现一个自定义的 <code>TypeSerializer</code>，并想了解如何实现序列化器以支持状态模式演化，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html">自定义状态序列化</a>。那里的文档还涵盖了关于状态序列化器和 Flink 的状态后端之间的相互作用的必要内部细节，以支持状态模式(state schema)演化。</p>
<h2 id="状态模式的演化">状态模式的演化</h2>
<p>要演化给定状态类型的模式，您需要采取以下步骤。</p>
<ol>
<li>保存你的 Flink 流作业(job)的保存点。</li>
<li>更新您的应用程序中的状态类型（例如，修改您的 Avro 类型模式）。</li>
<li>从保存点恢复作业(job)。当第一次访问状态时，Flink 将评估是否已经改变了状态的模式(schema)，并在必要时迁移状态模式。</li>
</ol>
<p>迁移状态以适应已更改的模式的过程是自动发生的，并且对每个状态都是独立的。这个过程由 Flink 内部执行，首先检查状态的新序列器是否与之前的序列器有不同的序列化模式，如果有，则用之前的序列器将状态读到对象，再用新的序列器写回字节。</p>
<p>关于迁移过程的进一步细节不在本文档的范围内，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html">这里</a>。</p>
<h2 id="支持的模式演化数据类型">支持的模式演化数据类型</h2>
<p>目前，模式演化只支持 POJO 和 Avro 类型。因此，如果你关心状态的模式演化，目前建议始终使用 POJO 或 Avro 作为状态数据类型。</p>
<p>有计划扩展对更多复合类型的支持；更多细节请参考 <a href="https://issues.apache.org/jira/browse/FLINK-10896">FLINK-10896</a>。</p>
<h3 id="pojo-类型">POJO 类型</h3>
<p>Flink 支持 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html#rules-for-pojo-types">POJO 类型</a>的演化模式，基于以下一组规则。</p>
<ol>
<li>字段可以被删除。一旦被删除，在未来的检查点和保存点中，被删除字段的之前值将被丢弃。</li>
<li>可以添加新字段。新字段将被初始化为其类型的默认值，正如 <a href="https://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html">Java 所定义的</a>那样。</li>
<li>已声明的字段类型不能改变。</li>
<li>POJO 类型的类名不能改变，包括类的命名空间。</li>
</ol>
<p>请注意，POJO 类型状态的模式只能在 Flink 版本大于 1.8.0 的情况下，从以前的保存点恢复时才能进化。当使用比 1.8.0 更老的 Flink 版本进行还原时，模式不能被改变。</p>
<h3 id="avro-类型">Avro 类型</h3>
<p>Flink 完全支持 Avro 类型状态的演变模式，只要模式变化被 <a href="http://avro.apache.org/docs/current/spec.html#Schema+Resolution">Avro 的模式解析规则</a>认为是兼容的。</p>
<p>一个限制是作为状态类型使用的 Avro 生成的类在恢复作业时不能被重新定位或具有不同的命名空间。</p>
<p>注意: 不支持键的模式演变。</p>
<p>举个例子。RocksDB 状态后端依赖于二进制对象的标识，而不是 hashCode 方法实现。对 keys 对象结构的任何改变都可能导致非确定性行为。</p>
<p>注意: Kryo 不能用于模式演化。</p>
<p>当使用 Kryo 时，框架没有可能验证是否有任何不兼容的变化。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/schema_evolution.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/schema_evolution.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[用于外部数据访问的异步 I/O]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-asynchronous-io-for-external-data-access/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-windows/?utm_source=atom_feed" rel="related" type="text/html" title="窗口" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-asynchronous-io-for-external-data-access/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Asynchronous Io for External Data Access</blockquote><p>本页解释了如何使用 Flink 的 API 与外部数据存储进行异步 I/O。对于不熟悉异步或事件驱动编程的用户来说，一篇关于 Futures 和事件驱动编程的文章可能是有用的准备。</p>
<p>注：关于异步 I/O 实用程序的设计和实现的细节可以在提案和设计文件 <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65870673">FLIP-12：异步I/O设计和实现中找到</a>。</p>
<h2 id="异步io操作的必要性">异步I/O操作的必要性</h2>
<p>在与外部系统交互时（例如用存储在数据库中的数据来丰富流事件时），需要注意与外部系统的通信延迟不会主导流应用的总工作。</p>
<p>奈何访问外部数据库中的数据，例如在 <code>MapFunction</code> 中，通常意味着同步交互。一个请求被发送到数据库，<code>MapFunction</code> 等待直到收到响应。在许多情况下，这种等待占据了函数的绝大部分时间。</p>
<p>与数据库的异步交互意味着一个并行函数实例可以同时处理许多请求，并同时接收响应。这样一来，等待时间就可以与发送其他请求和接收响应叠加起来。最起码，等待时间可以摊在多个请求上。这在大多数情况下会导致更高的流吞吐量。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/async_io.svg" alt="img"></p>
<p>注意：通过仅仅将 <code>MapFunction</code> 扩展到很高的并行度来提高吞吐量，在某些情况下也是可行的，但通常要付出很高的资源代价：拥有更多的并行 <code>MapFunction</code> 实例意味着更多的任务、线程、Flink 内部网络连接、与数据库的网络连接、缓冲区以及一般的内部记账开销。</p>
<h2 id="前提条件">前提条件</h2>
<p>如上节所述，要实现对数据库（或键/值存储）的适当异步 I/O，需要向该数据库提供一个支持异步请求的客户端。许多流行的数据库都提供了这样的客户端。</p>
<p>在没有这样的客户端的情况下，可以尝试通过创建多个客户端，并用线程池处理同步调用，将同步客户端变成有限的并发客户端。然而，这种方法通常比一个合适的异步客户端效率低。</p>
<h2 id="异步-io-api">异步 I/O API</h2>
<p>Flink 的 Async I/O API 允许用户使用异步请求客户端与数据流。该 API 处理与数据流的集成，以及处理顺序、事件时间、容错等。</p>
<p>假设自己有一个目标数据库的异步客户端，需要三个部分来实现对数据库的异步 I/O 的流转换。</p>
<ul>
<li>一个 AsyncFunction 的实现，用来调度请求。</li>
<li>一个回调，获取操作结果并将其交给 <code>ResultFuture</code>。</li>
<li>在 DataStream 上应用异步 I/O 操作作为转换。</li>
</ul>
<p>下面的代码示例说明了基本模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="cm">/**
</span><span class="cm"> * An implementation of the &#39;AsyncFunction&#39; that sends requests and sets the callback.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">AsyncDatabaseRequest</span> <span class="k">extends</span> <span class="nc">AsyncFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)]</span> <span class="o">{</span>

    <span class="cm">/** The database specific client that can issue concurrent requests with callbacks */</span>
    <span class="k">lazy</span> <span class="k">val</span> <span class="n">client</span><span class="k">:</span> <span class="kt">DatabaseClient</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">DatabaseClient</span><span class="o">(</span><span class="n">host</span><span class="o">,</span> <span class="n">post</span><span class="o">,</span> <span class="n">credentials</span><span class="o">)</span>

    <span class="cm">/** The context used for the future callbacks */</span>
    <span class="k">implicit</span> <span class="k">lazy</span> <span class="k">val</span> <span class="n">executor</span><span class="k">:</span> <span class="kt">ExecutionContext</span> <span class="o">=</span> <span class="nc">ExecutionContext</span><span class="o">.</span><span class="n">fromExecutor</span><span class="o">(</span><span class="nc">Executors</span><span class="o">.</span><span class="n">directExecutor</span><span class="o">())</span>


    <span class="k">override</span> <span class="k">def</span> <span class="n">asyncInvoke</span><span class="o">(</span><span class="n">str</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">resultFuture</span><span class="k">:</span> <span class="kt">ResultFuture</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>

        <span class="c1">// issue the asynchronous request, receive a future for the result
</span><span class="c1"></span>        <span class="k">val</span> <span class="n">resultFutureRequested</span><span class="k">:</span> <span class="kt">Future</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">client</span><span class="o">.</span><span class="n">query</span><span class="o">(</span><span class="n">str</span><span class="o">)</span>

        <span class="c1">// set the callback to be executed once the request by the client is complete
</span><span class="c1"></span>        <span class="c1">// the callback simply forwards the result to the result future
</span><span class="c1"></span>        <span class="n">resultFutureRequested</span><span class="o">.</span><span class="n">onSuccess</span> <span class="o">{</span>
            <span class="k">case</span> <span class="n">result</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="n">resultFuture</span><span class="o">.</span><span class="n">complete</span><span class="o">(</span><span class="nc">Iterable</span><span class="o">((</span><span class="n">str</span><span class="o">,</span> <span class="n">result</span><span class="o">)))</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// create the original stream
</span><span class="c1"></span><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// apply the async I/O transformation
</span><span class="c1"></span><span class="k">val</span> <span class="n">resultStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span>
    <span class="nc">AsyncDataStream</span><span class="o">.</span><span class="n">unorderedWait</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="k">new</span> <span class="nc">AsyncDatabaseRequest</span><span class="o">(),</span> <span class="mi">1000</span><span class="o">,</span> <span class="nc">TimeUnit</span><span class="o">.</span><span class="nc">MILLISECONDS</span><span class="o">,</span> <span class="mi">100</span><span class="o">)</span>
</code></pre></div><p>重要提示：<code>ResultFuture.complete</code> 的第一次调用就完成了。所有后续的完成调用将被忽略。</p>
<p>以下两个参数控制异步操作。</p>
<ul>
<li>
<p>超时: 超时定义了异步请求在被认为失败之前可能需要的时间。这个参数可以防范死机/失败的请求。</p>
</li>
<li>
<p>Capacity（容量）：该参数定义了异步请求在被认为失败之前可能需要的时间。这个参数定义了多少个异步请求可以同时进行。尽管异步I/O方法通常会带来更好的吞吐量，但操作者仍然可以成为流应用的瓶颈。限制并发请求的数量可以确保操作者不会积累越来越多的待处理请求的积压，但一旦容量耗尽，就会触发背压。</p>
</li>
</ul>
<h3 id="超时处理">超时处理</h3>
<p>当一个异步 I/O 请求超时时，默认情况下会抛出一个异常并重新启动作业。如果你想处理超时，你可以重写 <code>AsyncFunction#timeout</code> 方法。</p>
<h3 id="结果的顺序">结果的顺序</h3>
<p><code>AsyncFunction</code> 发出的并发请求经常以某种未定义的顺序完成，基于哪个请求先完成。为了控制结果记录以何种顺序发出，Flink 提供了两种模式。</p>
<ul>
<li>
<p>Unordered: 异步请求一结束，结果记录就会被发出。在异步 I/O 操作符之后，流中记录的顺序与之前不同。这种模式以处理时间为基本时间特性时，延迟最低，开销最小。使用 <code>AsyncDataStream.unorderedWait(...)</code> 来实现这种模式。</p>
</li>
<li>
<p>Ordered: 在这种情况下，流的顺序被保留下来。结果记录的发出顺序与异步请求被触发的顺序相同（运算符输入记录的顺序）。为了达到这个目的，操作符会缓冲一个结果记录，直到它前面的所有记录都被发出来（或定时发出来）。这通常会在检查点中引入一些额外的延迟和一些开销，因为与无序模式相比，记录或结果在检查点状态下维持的时间更长。使用 <code>AsyncDataStream.orderedWait(...)</code> 来处理这种模式。</p>
</li>
</ul>
<h2 id="事件时间">事件时间</h2>
<p>当流媒体应用程序使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间</a>工作时，水印将由异步 I/O 操作符正确处理。具体来说，这意味着两种顺序模式的以下内容。</p>
<ul>
<li>无序的：水印不会超越记录，反之亦然，这意味着水印会建立一个顺序边界。只有在水印之间才会发出无序的记录。发生在某一水印之后的记录，只有在该水印被发射之后才会被发射。而水印则只有在该水印之前的所有输入的结果记录被发出之后才会被发出。</li>
</ul>
<p>这意味着在有水印的情况下，无序模式会引入一些与有序模式相同的延迟和管理开销。该开销的数量取决于水印的频率。</p>
<ul>
<li>有序的: 水印和记录的顺序被保留下来 就像记录之间的顺序被保留一样 与处理时间相比，开销没有明显变化。</li>
</ul>
<p>请记住，摄取时间是事件时间的一种特殊情况，其自动生成的水印是基于源处理时间的。</p>
<h2 id="容错保证">容错保证</h2>
<p>异步 I/O 操作符提供了完全精确的一次容错保证，它将飞行中的异步请求记录存储在检查点中，并在故障恢复时恢复/重新触发请求。它将飞行中的异步请求记录存储在检查点中，并在从故障中恢复时恢复/重新触发请求。</p>
<h2 id="实现技巧">实现技巧</h2>
<p>对于有 Executor（或 Scala 中的 ExecutionContext）用于回调的 Futures 实现，我们建议使用  DirectExecutor，因为回调通常只做最少的工作，而且DirectExecutor 避免了额外的线程间交接开销。回调通常只将结果交给 <code>ResultFuture</code>，后者将其添加到输出缓冲区。从那里开始，包括记录排放和与检查点记账的交互在内的繁重逻辑无论如何都发生在一个专用线程池中。</p>
<p>可以通过 <code>org.apache.flink.runtime.concurrent.Executors.directExecutor()</code> 或 <code>com.google.common.util.concurrent.MoreExecutors.directExecutor()</code> 获得 DirectExecutor。</p>
<h2 id="注意事项">注意事项</h2>
<p>AsyncFunction 不叫多线程。</p>
<p>我们在这里要明确指出的一个常见的困惑是，AsyncFunction 不是以多线程的方式调用的。AsyncFunction 只存在一个实例，并且对于流的各个分区中的每一条记录，它都会被依次调用。除非 <code>asyncInvoke(...)</code> 方法快速返回并依赖于回调（由客户端），否则不会导致正确的异步 I/O。</p>
<p>例如，以下模式会导致阻塞 <code>asyncInvoke(...)</code> 函数，从而使异步行为无效。</p>
<ul>
<li>
<p>使用一个数据库客户端，其查找/查询方法的调用会被阻塞，直到结果被接收回来为止</p>
</li>
<li>
<p>在 <code>asyncInvoke(...)</code> 方法中阻止/等待异步客户端返回的未来型对象。</p>
</li>
</ul>
<p>出于一致性的考虑，AsyncFunction 的操作符（AsyncWaitOperator）目前必须位于操作符链的头部。</p>
<p>由于在 FLINK-13063 问题中给出的原因，我们目前必须打破 AsyncWaitOperator 的操作符链，以防止潜在的一致性问题。这是对以前支持链的行为的改变。需要旧行为并接受潜在的违反一致性保证的用户可以手动实例化并将 AsyncWaitOperator 添加到作业图中，并通过 AsyncWaitOperator#setChainingStrategy(ChainingStrategy.ALWAYS) 将链式策略设置回链式。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/operators" term="operators" label="Operators" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/io" term="io" label="IO" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[用户定义函数]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-user-defined-functions/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-user-defined-functions/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>User Defined Functions</blockquote><h1 id="用户自定义函数">用户自定义函数</h1>
<p>大多数操作符都需要用户定义的函数。本节列出了如何指定这些函数的不同方法。我们还涵盖了累加器，它可以用来深入了解您的 Flink 应用程序。</p>
<h2 id="lambda-函数">Lambda 函数</h2>
<p>在前面的例子中已经看到，所有的操作符都接受 lambda 函数来描述操作。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">startsWith</span><span class="o">(</span><span class="s">&#34;http://&#34;</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="o">(</span><span class="n">i1</span><span class="o">,</span><span class="n">i2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i2</span> <span class="o">}</span>
<span class="c1">// 或
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="k">_</span> <span class="o">+</span> <span class="k">_</span> <span class="o">}</span>
</code></pre></div><h3 id="富函数rich-functions">富函数(Rich functions)</h3>
<p>所有以 lambda 函数作为参数的变换都可以以富函数作为参数。例如，我们可以不使用:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">toInt</span> <span class="o">}</span>
</code></pre></div><p>你可以编写:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">MyMapFunction</span> <span class="k">extends</span> <span class="nc">RichMapFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">in</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span><span class="kt">Int</span> <span class="o">=</span> <span class="o">{</span> <span class="n">in</span><span class="o">.</span><span class="n">toInt</span> <span class="o">}</span>
<span class="o">};</span>
</code></pre></div><p>并将该函数传递给 map 转换:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyMapFunction</span><span class="o">())</span>
</code></pre></div><p>丰富的函数也可以定义为匿名类:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">(</span><span class="k">new</span> <span class="nc">RichMapFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">in</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span><span class="kt">Int</span> <span class="o">=</span> <span class="o">{</span> <span class="n">in</span><span class="o">.</span><span class="n">toInt</span> <span class="o">}</span>
<span class="o">})</span>
</code></pre></div><p>丰富的函数除了提供用户定义的函数（map、reduce等）外，还提供了四个方法：<code>open</code>、<code>close</code>、<code>getRuntimeContext</code> 和 <code>setRuntimeContext</code>。这些方法可以用于为函数设置参数（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#passing-parameters-to-functions">Passing Parameters to Functions</a>）、创建和最终确定局部状态、访问广播变量（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#broadcast-variables">Broadcast Variables</a>）、访问运行时信息，如累加器和计数器（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html#accumulators--counters">Accumulators and Counters</a>）以及迭代信息（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">Iterations</a>）。</p>
<h3 id="累积器和计数器">累积器和计数器</h3>
<p>累积器是一个简单的构造，有一个加法运算和一个最终的累积结果，在作业结束后就可以使用。</p>
<p>最直接的累加器是一个计数器，你可以使用 <code>Accumulator.add(V value)</code> 方法对它进行增量。在作业结束时，Flink 将对所有部分结果进行加总（合并）并将结果发送给客户端。累积器在调试期间或如果你快速想了解更多的数据时是很有用的。</p>
<p>Flink 目前有以下内置的累加器。它们每个都实现了 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java">Accumulator</a> 接口。</p>
<ul>
<li><a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/IntCounter.java">IntCounter</a>、<a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/LongCounter.java">LongCounter</a> 和 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/DoubleCounter.java">DoubleCounter</a>。请看下面一个使用计数器的例子。</li>
<li><a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Histogram.java">直方图</a>。一个离散数量的直方块的直方图实现。在内部，它只是一个从 Integer 到 Integer 的映射。你可以用它来计算值的分布，例如字数程序的每行字数分布。</li>
</ul>
<p><strong>如何使用累加器:</strong></p>
<p>首先你必须在用户定义的转换函数中创建一个累加器对象(这里是一个计数器)，在你想使用它的地方。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">private</span> <span class="n">IntCounter</span> <span class="n">numLines</span> <span class="o">=</span> <span class="k">new</span> <span class="n">IntCounter</span><span class="o">();</span>
</code></pre></div><p>其次，你必须注册累加器对象，通常是在富函数的 <code>open()</code> 方法中。在这里你还需要定义名称。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">getRuntimeContext</span><span class="o">().</span><span class="n">addAccumulator</span><span class="o">(</span><span class="s">&#34;num-lines&#34;</span><span class="o">,</span> <span class="k">this</span><span class="o">.</span><span class="n">numLines</span><span class="o">);</span>
</code></pre></div><p>现在你可以在运算函数的任何地方使用累加器，包括在 <code>open()</code> 和 <code>close()</code> 方法中。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">this</span><span class="o">.</span><span class="n">numLines</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
</code></pre></div><p>整体结果将存储在 <code>JobExecutionResult</code> 对象中，该对象由执行环境的 <code>execute()</code> 方法返回（目前只有在执行等待作业完成的情况下才有效）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">myJobExecutionResult</span><span class="o">.</span><span class="n">getAccumulatorResult</span><span class="o">(</span><span class="s">&#34;num-lines&#34;</span><span class="o">)</span>
</code></pre></div><p>所有的累加器在每个作业中共享一个命名空间。因此你可以在你的工作的不同操作函数中使用同一个累加器。Flink 会在内部合并所有同名的累加器。</p>
<p>关于累加器和迭代的说明。目前，累加器的结果只有在整个作业结束后才会出现。我们计划在下一次迭代中也能获得上一次迭代的结果。你可以使用 <a href="v">Aggregators</a> 来计算每次迭代的统计数据，并根据这些统计数据来终止迭代。</p>
<p><strong>自定义累加器:</strong></p>
<p>要实现你自己的累加器，你只需要编写你的 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java">Accumulator</a> 接口的实现。如果你认为你的自定义累加器应该和Flink一起发布，请随时创建一个pull request。</p>
<p>你可以选择实现 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java">Accumulator</a> 或 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/SimpleAccumulator.java">SimpleAccumulator</a>。</p>
<p><code>Accumulator&lt;V,R&gt;</code> 是最灵活的。它为要添加的值定义了一个类型 V，为最终结果定义了一个结果类型 R。例如，对于一个直方图，V 是一个数字，R 是一个直方图。 <code>SimpleAccumulator</code> 适用于两种类型都相同的情况，例如计数器。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[窗口]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-windows/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-asynchronous-io-for-external-data-access/?utm_source=atom_feed" rel="related" type="text/html" title="用于外部数据访问的异步 I/O" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-windows/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Windows</blockquote><h1 id="窗口">窗口</h1>
<p>窗口是处理无限流的核心。窗口将流分割成有限大小的&quot;桶&quot;，我们可以对其应用计算。本文档主要介绍 Flink 中如何进行窗口化，以及程序员如何从其提供的功能中最大限度地受益。</p>
<p>下面介绍了一个窗口化 Flink 程序的一般结构。第一个片段指的是 keyed 流，而第二个片段指的是 non-keyed 流。正如人们所看到的那样，唯一的区别是 keyed 流的 <code>keyBy(...)</code> 调用和 non-keyed 流的 <code>window(...)</code> 变成了 <code>windowAll(...)</code>。这也将作为本页面其他内容的路线图。</p>
<p><strong>Keyed 窗口</strong></p>
<pre><code>stream
       .keyBy(...)               &lt;-  keyed 与 non-keyed 窗口的对比
       .window(...)              &lt;-  必须的: &quot;assigner&quot;
      [.trigger(...)]            &lt;-  可选的: &quot;trigger&quot; (否则使用默认的 trigger)
      [.evictor(...)]            &lt;-  可选的: &quot;evictor&quot; (否则没有 evictor)
      [.allowedLateness(...)]    &lt;-  可选的: &quot;lateness&quot; (否则为零)
      [.sideOutputLateData(...)] &lt;-  可选的: &quot;output tag&quot; (否则迟到数据无侧输出)
       .reduce/aggregate/fold/apply()      &lt;-  必须的: &quot;function&quot;
      [.getSideOutput(...)]      &lt;-  可选的: &quot;output tag&quot;
</code></pre><p><strong>Non-Keyed 窗口</strong></p>
<pre><code>stream
       .windowAll(...)           &lt;-  必须的: &quot;assigner&quot;
      [.trigger(...)]            &lt;-  可选的: &quot;trigger&quot; (否则使用默认的 trigger)
      [.evictor(...)]            &lt;-  可选的: &quot;evictor&quot; (否则没有 evictor)
      [.allowedLateness(...)]    &lt;-  可选的: &quot;lateness&quot; (否则为零)
      [.sideOutputLateData(...)] &lt;-  可选的: &quot;output tag&quot; (否则迟到数据无侧输出)
       .reduce/aggregate/fold/apply()      &lt;-  必须的: &quot;function&quot;
      [.getSideOutput(...)]      &lt;-  可选的: &quot;output tag&quot;
</code></pre><p>在上面，方括号中的命令(<code>[...]</code>)是可选的。这表明 Flink 允许你以多种不同的方式定制你的窗口逻辑，以便它最适合你的需求。</p>
<h2 id="窗口生命周期">窗口生命周期</h2>
<p>简而言之，当第一个应该属于这个窗口的元素到达时，就会创建一个窗口，当时间（事件时间或处理时间）经过(passes)它的结束时间戳加上用户指定的允许延迟时，这个窗口就会被完全移除（见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#allowed-lateness">允许延迟</a>）。Flink 只保证对基于时间的窗口进行移除，而不保证对其他类型的窗口，如全局窗口进行移除（见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-assigners">窗口分配器</a>）。例如，基于事件-时间的窗口策略每5分钟创建一个非重叠（或翻滚）的窗口，并且允许的延迟为1分钟，当第一个具有时间戳的元素落入这个区间时，Flink 将为 12:00 和 12:05 之间的区间创建一个新的窗口，当水印通过 12:06 的时间戳时，它将删除它。</p>
<p>此外，每个窗口将有一个触发器(见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#triggers">触发器</a>)和一个函数(ProcessWindowFunction、ReduceFunction、AggregateFunction或FoldFunction)(见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-functions">窗口函数</a>)。函数将包含要应用于窗口内容的计算，而触发器则指定了窗口被认为可以应用函数的条件。触发策略可能是&quot;当窗口中的元素数量超过4时&quot;，或者&quot;当水印经过窗口的末端时&quot;。触发器还可以决定在创建和删除窗口之间的任何时间(any time between its creation and removal)清除窗口的内容。在这种情况下，清除只指窗口中的元素，而不是窗口元数据。这意味着新的数据仍然可以被添加到该窗口中。</p>
<p>除上述之外，您还可以指定一个 Evictor(见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#evictors">Evictors</a>)，它将能够在触发器触发后以及在函数应用之前和/或之后从窗口中删除元素。</p>
<p>在下文中，我们将对上述每个组件进行更详细的介绍。我们先从上述代码段中必须的部分开始(参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#keyed-vs-non-keyed-windows">Keyed vs Non-Keyed 窗口</a>、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-assigner">窗口分配器</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-function">窗口函数</a>)，然后再转向可选部分。</p>
<h2 id="keyed-与-non-keyed-窗口的对比">Keyed 与 Non-Keyed 窗口的对比</h2>
<p>首先要指定的是您的流是否应该是 keyed 的。这必须在定义窗口之前完成。使用 <code>keyBy(...)</code> 将把您的无限流分割成逻辑 keyed 流。如果没有调用 <code>keyBy(...)</code>，那么您的流就不是 keyed 流。</p>
<p>在 keyed 流的情况下，传入事件的任何属性都可以被用作键（更多细节在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html#keyed-datastream">这里</a>）。拥有一个 keyed 流将允许你的窗口计算由多个任务并行执行，因为每个逻辑 keyed 流可以独立于其他流进行处理。所有指向同一键的元素将被发送到同一个并行任务(task)。</p>
<p>在 non-keyed 流的情况下，您的原始流不会被分割成多个逻辑流，所有的窗口化逻辑将由一个任务(task)来执行，即并行度为1。</p>
<h2 id="窗口分配器">窗口分配器</h2>
<p>在指定流是否是 keyed 流之后，下一步是定义窗口分配器。窗口分配器定义了如何将元素分配给窗口。这是通过在 <code>window(...)</code>（对于 keyed 流）或 <code>windowAll()</code>（对于 non-keyed 流）调用中指定您所选择的 <code>WindowAssigner</code> 来实现的。</p>
<p><code>WindowAssigner</code> 负责将每个传入的元素分配给一个或多个窗口。Flink 为最常见的用例提供了预定义的窗口分配器，即滚动窗口、滑动窗口、会话窗口和全局窗口。您也可以通过扩展 <code>WindowAssigner</code> 类来实现自定义窗口分配器。所有内置的窗口分配器（除了全局窗口）都是基于时间将元素分配给窗口，时间可以是处理时间，也可以是事件时间。请查看我们关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间</a>的部分，了解处理时间和事件时间之间的区别，以及时间戳和水印是如何生成的。</p>
<p>基于时间的窗口有一个开始时间戳（包括）和结束时间戳（不包括），共同描述窗口的大小。在代码中，Flink 在处理基于时间的窗口时使用了 <code>TimeWindow</code>，它有查询开始和结束时间戳的方法，还有一个额外的方法 <code>maxTimestamp()</code>，可以返回给定窗口的最大允许时间戳。</p>
<p>在下文中，我们将展示 Flink 的预定义窗口分配器是如何工作的，以及如何在 DataStream 程序中使用它们。下图直观地展示了每个分配器的工作情况。紫色的圆圈代表流的元素，这些元素被某个键（在本例中是用户1、用户2和用户3）分割。x轴显示的是时间的进度。</p>
<h2 id="滚动窗口">滚动窗口</h2>
<p>滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口。滚动窗口有一个固定的大小，并且不重叠。例如，如果你指定了一个大小为5分钟的滚动窗口，那么当前的窗口将被评估，并且每隔5分钟就会启动一个新的窗口，如下图所示。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/tumbling-windows.svg" alt="img"></p>
<p>以下代码片段展示了如何使用滚动窗口。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// tumbling event-time windows
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// tumbling processing-time windows
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingProcessingTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// daily tumbling event-time windows offset by -8 hours.
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">days</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(-</span><span class="mi">8</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>
</code></pre></div><p>时间间隔可以使用 <code>Time.milliseconds(x)</code>, <code>Time.seconds(x)</code>, <code>Time.minutes(x)</code> 等中的一种来指定。</p>
<p>如最后一个例子所示，滚动窗口分配器还可以采用一个可选的偏移量(<code>offset</code>)参数，用于改变窗口的对齐方式。例如，在没有偏移量的情况下，每小时的滚动窗口与纪元对齐，也就是说，你会得到诸如 <code>1:00:00.000 - 1:59:59.999</code>，<code>2:00:00.000 - 2:59:59.999</code> 等窗口。如果你想改变这一点，你可以给出一个偏移量。例如，如果偏移量为15分钟，您将得到 <code>1:15:00.000 - 2:14:59.999</code>，<code>2:15:00.000 - 3:14:59.999</code> 等。偏移量的一个重要用途是调整窗口到 UTC-0 以外的时区。例如，在中国，你必须指定一个 <code>Time.hours(-8)</code> 的偏移量。</p>
<h2 id="滑动窗口">滑动窗口</h2>
<p>滑动窗口分配器将元素分配给固定长度的窗口。与滚动窗口分配器类似，窗口的大小由窗口大小(window size)参数配置。一个额外的窗口滑动(window slide)参数控制滑动窗口的启动频率。因此，如果滑动窗口的滑块小于窗口大小，滑动窗口可以重叠。在这种情况下，元素被分配到多个窗口。</p>
<p>例如，你可以有10分钟大小的窗口，滑动5分钟。这样，每隔5分钟就会有一个窗口，包含过去10分钟内到达的事件，如下图所示。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/sliding-windows.svg" alt="img"></p>
<p>以下代码片段展示了如何使用滑动窗口。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// sliding event-time windows
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">SlidingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// sliding processing-time windows
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">SlidingProcessingTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// sliding processing-time windows offset by -8 hours
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">SlidingProcessingTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(</span><span class="mi">12</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(-</span><span class="mi">8</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>
</code></pre></div><p>时间间隔可以通过使用 <code>Time.milliseconds(x)</code>, <code>Time.seconds(x)</code>, <code>Time.minutes(x)</code> 等中的一个来指定。</p>
<p>如上一个例子所示，滑动窗口分配器还可以采取一个可选的偏移量(<code>offset</code>)参数，用于改变窗口的对齐方式。例如，在没有偏移量的情况下，每小时滑动30分钟的窗口与纪元对齐，也就是说，你将得到 <code>1:00:00.000 - 1:59:59.999</code>，<code>1:30:00.000 - 2:29:59.999</code> 等窗口。如果你想改变这一点，你可以给出一个偏移量。例如，如果偏移量为15分钟，您将得到 <code>1:15:00.000 - 2:14:59.999</code>，<code>1:45:00.000 - 2:44:59.999</code> 等。偏移量的一个重要用途是调整窗口到 UTC-0 以外的时区。例如，在中国，你必须指定一个 <code>Time.hours(-8)</code> 的偏移。</p>
<h2 id="会话窗口">会话窗口</h2>
<p>会话窗口分配器按活动的会话对元素进行分组。与滚动窗口和滑动窗口不同，会话窗口不重叠，也没有固定的开始和结束时间。相反，当会话窗口在一定时间内没有接收到元素时，也就是在不活动的间隙发生时，会话窗口就会关闭。会话窗口分配器可以配置一个静态的会话间隙(session gap)，也可以配置一个会话间隙提取函数，该函数定义了多长时间的不活动期。当这个时间段(period)到期(expires)时，当前会话关闭，后续元素被分配到一个新的会话窗口。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/session-windows.svg" alt="img"></p>
<p>以下代码片段展示了如何使用会话窗口。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// event-time session windows with static gap
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">EventTimeSessionWindows</span><span class="o">.</span><span class="n">withGap</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">minutes</span><span class="o">(</span><span class="mi">10</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// event-time session windows with dynamic gap
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">EventTimeSessionWindows</span><span class="o">.</span><span class="n">withDynamicGap</span><span class="o">(</span><span class="k">new</span> <span class="nc">SessionWindowTimeGapExtractor</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="o">{</span>
      <span class="k">override</span> <span class="k">def</span> <span class="n">extract</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="o">{</span>
        <span class="c1">// determine and return session gap
</span><span class="c1"></span>      <span class="o">}</span>
    <span class="o">}))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// processing-time session windows with static gap
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">ProcessingTimeSessionWindows</span><span class="o">.</span><span class="n">withGap</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">minutes</span><span class="o">(</span><span class="mi">10</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>


<span class="c1">// processing-time session windows with dynamic gap
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">DynamicProcessingTimeSessionWindows</span><span class="o">.</span><span class="n">withDynamicGap</span><span class="o">(</span><span class="k">new</span> <span class="nc">SessionWindowTimeGapExtractor</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="o">{</span>
      <span class="k">override</span> <span class="k">def</span> <span class="n">extract</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="o">{</span>
        <span class="c1">// determine and return session gap
</span><span class="c1"></span>      <span class="o">}</span>
    <span class="o">}))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>
</code></pre></div><p>静态间隙可以通过使用 <code>Time.milliseconds(x)</code>, <code>Time.seconds(x)</code>, <code>Time.minutes(x)</code> 等之一来指定。</p>
<p>动态间隙可以通过实现 <code>SessionWindowTimeGapExtractor</code> 接口来指定。</p>
<p>注意: 由于会话窗口没有固定的开始和结束，所以它们的评估方式与滚动和滑动窗口不同。在内部，会话窗口操作符为每个到达的记录创建一个新的窗口，如果它们彼此之间的距离比定义的间隙更近，就会将窗口合并在一起。为了能够合并，会话窗口操作符需要一个合并<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#triggers">触发器</a>和一个合并<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-functions">窗口函数</a>，如 ReduceFunction、AggregateFunction 或 ProcessWindowFunction(FoldFunction 不能合并)。</p>
<h2 id="全局窗口">全局窗口</h2>
<p>全局窗口分配器将具有相同键的所有元素分配到同一个全局窗口。只有当你还指定了一个自定义<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#triggers">触发器</a>时，这种窗口方案才有用。否则，任何计算都不会被执行，因为全局窗口没有一个自然的终点，我们可以在那里处理聚集的元素。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/non-windowed.svg" alt="img"></p>
<p>下面的代码片段展示了如何使用全局窗口。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">GlobalWindows</span><span class="o">.</span><span class="n">create</span><span class="o">())</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>
</code></pre></div><h2 id="窗口函数">窗口函数</h2>
<p>在定义了窗口分配器之后，我们需要指定我们要对这些窗口中的每一个窗口进行的计算。这是窗口函数的责任，一旦系统确定一个窗口准备好进行处理，它就会用来处理每个（可能是 keyed 的）窗口的元素（关于 Flink 如何确定窗口准备好，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#triggers">触发器</a>）。</p>
<p>窗口函数可以是 <code>ReduceFunction</code>、<code>AggregateFunction</code>、<code>FoldFunction</code> 或 <code>ProcessWindowFunction</code> 中的一种。前两个可以更有效地执行（见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#state%20size">状态大小</a>部分），因为 Flink 可以在每个窗口到达时增量地聚合元素。<code>ProcessWindowFunction</code> 可以为一个窗口中包含的所有元素获取一个 <code>Iterable</code>，以及关于元素所属窗口的附加元信息。</p>
<p>带有 <code>ProcessWindowFunction</code> 的窗口化转换不能像其他情况一样高效执行，因为 Flink 在调用函数之前必须在内部缓冲一个窗口的所有元素。通过将 <code>ProcessWindowFunction</code> 与 <code>ReduceFunction</code>、<code>AggregateFunction</code> 或 <code>FoldFunction</code> 结合起来，既可以得到窗口元素的增量聚合，也可以得到 <code>ProcessWindowFunction</code> 接收到的额外的窗口元数据，从而缓解这种情况。我们将查看这些变体的每个例子。</p>
<h3 id="reducefunction">ReduceFunction</h3>
<p><code>ReduceFunction</code> 指定了如何将输入的两个元素组合起来以产生相同类型的输出元素。Flink 使用 <code>ReduceFunction</code> 来增量聚合一个窗口的元素。</p>
<p><code>ReduceFunction</code> 可以这样定义和使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="o">(</span><span class="n">v1</span><span class="o">,</span> <span class="n">v2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">v1</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">v1</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">v2</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><p>上面的例子把一个窗口中所有元素的元组的第二个字段相加起来。</p>
<h3 id="aggregatefunction">AggregateFunction</h3>
<p><code>AggregateFunction</code> 是 <code>ReduceFunction</code> 的通用版本，它有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型是输入流中元素的类型，AggregateFunction 有一个方法用于将一个输入元素添加到累加器中。该接口还有创建一个初始累加器、将两个累加器合并成一个累加器以及从一个累加器中提取一个输出（类型为 OUT）的方法。我们将在下面的例子中看到这些方法是如何工作的。</p>
<p>和 ReduceFunction 一样，Flink 会在窗口的输入元素到达时，对它们进行增量聚合。</p>
<p>AggregateFunction 可以这样定义和使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="cm">/**
</span><span class="cm"> * The accumulator is used to keep a running sum and a count. The [getResult] method
</span><span class="cm"> * computes the average.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">AverageAggregate</span> <span class="k">extends</span> <span class="nc">AggregateFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="kt">Double</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span> <span class="k">=</span> <span class="o">(</span><span class="mi">0L</span><span class="o">,</span> <span class="mi">0L</span><span class="o">)</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">add</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Long</span><span class="o">),</span> <span class="n">accumulator</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span>
    <span class="o">(</span><span class="n">accumulator</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="mi">1L</span><span class="o">)</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getResult</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_1</span> <span class="o">/</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_2</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">),</span> <span class="n">b</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span>
    <span class="o">(</span><span class="n">a</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">a</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="k">new</span> <span class="nc">AverageAggregate</span><span class="o">)</span>
</code></pre></div><p>上面的例子是计算窗口中元素的第二个字段的平均值。</p>
<h3 id="foldfunction">FoldFunction</h3>
<p>FoldFunction 指定了窗口的输入元素如何与输出类型的元素相结合。对于添加到窗口的每个元素和当前的输出值，都会递增地调用 FoldFunction。第一个元素与输出类型的预定义初始值相结合。</p>
<p>可以这样定义和使用 FoldFunction。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">fold</span><span class="o">(</span><span class="s">&#34;&#34;</span><span class="o">)</span> <span class="o">{</span> <span class="o">(</span><span class="n">acc</span><span class="o">,</span> <span class="n">v</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">acc</span> <span class="o">+</span> <span class="n">v</span><span class="o">.</span><span class="n">_2</span> <span class="o">}</span>
</code></pre></div><p>上面的例子将所有输入的 Long 值追加到一个初始的空字符串中。</p>
<p>注意 <code>fold()</code> 不能用于会话窗口或其他可合并窗口。</p>
<h3 id="processwindowfunction">ProcessWindowFunction</h3>
<p>ProcessWindowFunction 得到一个包含窗口所有元素的 Iterable，以及一个可以访问时间和状态信息的 Context 对象，这使得它能够提供比其他窗口函数更多的灵活性。这是以性能和资源消耗为代价的，因为元素不能增量聚合，而是需要在内部缓冲，直到窗口被认为可以处理为止。</p>
<p>ProcessWindowFunction 的签名如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">abstract</span> <span class="k">class</span> <span class="nc">ProcessWindowFunction</span><span class="o">[</span><span class="kt">IN</span>, <span class="kt">OUT</span>, <span class="kt">KEY</span>, <span class="kt">W</span> <span class="k">&lt;:</span> <span class="kt">Window</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">Function</span> <span class="o">{</span>

  <span class="cm">/**
</span><span class="cm">    * Evaluates the window and outputs none or several elements.
</span><span class="cm">    *
</span><span class="cm">    * @param key      The key for which this window is evaluated.
</span><span class="cm">    * @param context  The context in which the window is being evaluated.
</span><span class="cm">    * @param elements The elements in the window being evaluated.
</span><span class="cm">    * @param out      A collector for emitting elements.
</span><span class="cm">    * @throws Exception The function may throw exceptions to fail the program and trigger recovery.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">process</span><span class="o">(</span>
      <span class="n">key</span><span class="k">:</span> <span class="kt">KEY</span><span class="o">,</span>
      <span class="n">context</span><span class="k">:</span> <span class="kt">Context</span><span class="o">,</span>
      <span class="n">elements</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[</span><span class="kt">IN</span><span class="o">],</span>
      <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">OUT</span><span class="o">])</span>

  <span class="cm">/**
</span><span class="cm">    * The context holding window metadata
</span><span class="cm">    */</span>
  <span class="k">abstract</span> <span class="k">class</span> <span class="nc">Context</span> <span class="o">{</span>
    <span class="cm">/**
</span><span class="cm">      * Returns the window that is being evaluated.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">window</span><span class="k">:</span> <span class="kt">W</span>

    <span class="cm">/**
</span><span class="cm">      * Returns the current processing time.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">currentProcessingTime</span><span class="k">:</span> <span class="kt">Long</span>

    <span class="cm">/**
</span><span class="cm">      * Returns the current event-time watermark.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">currentWatermark</span><span class="k">:</span> <span class="kt">Long</span>

    <span class="cm">/**
</span><span class="cm">      * State accessor for per-key and per-window state.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">windowState</span><span class="k">:</span> <span class="kt">KeyedStateStore</span>

    <span class="cm">/**
</span><span class="cm">      * State accessor for per-key global state.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">globalState</span><span class="k">:</span> <span class="kt">KeyedStateStore</span>
  <span class="o">}</span>

<span class="o">}</span>
</code></pre></div><p>注意 <code>key</code> 参数是通过为 <code>keyBy()</code> 调用指定的 <code>KeySelector</code> 提取的键。如果是元组索引键或字符串字段引用，这个键的类型总是 Tuple，你必须手动将其转换为一个正确大小的元组来提取键字段。</p>
<p><code>ProcessWindowFunction</code> 可以这样定义和使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
  <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
  <span class="o">.</span><span class="n">timeWindow</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">minutes</span><span class="o">(</span><span class="mi">5</span><span class="o">))</span>
  <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyProcessWindowFunction</span><span class="o">())</span>

<span class="cm">/* ... */</span>

<span class="k">class</span> <span class="nc">MyProcessWindowFunction</span> <span class="k">extends</span> <span class="nc">ProcessWindowFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">TimeWindow</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">def</span> <span class="n">process</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">context</span><span class="k">:</span> <span class="kt">Context</span><span class="o">,</span> <span class="n">input</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)],</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">var</span> <span class="n">count</span> <span class="k">=</span> <span class="mi">0L</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">in</span> <span class="k">&lt;-</span> <span class="n">input</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">count</span> <span class="k">=</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="o">}</span>
    <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="s">s&#34;Window </span><span class="si">${</span><span class="n">context</span><span class="o">.</span><span class="n">window</span><span class="si">}</span><span class="s"> count: </span><span class="si">$count</span><span class="s">&#34;</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>这个例子显示了一个 <code>ProcessWindowFunction</code>，它可以计算一个窗口中的元素。此外，窗口函数还将窗口的信息添加到输出中。</p>
<p>注意，使用 ProcessWindowFunction 进行简单的聚合，如 <code>count</code>，效率相当低。下一节将展示如何将 <code>ReduceFunction</code> 或 <code>AggregateFunction</code> 与 <code>ProcessWindowFunction</code> 结合起来，以获得增量聚合和 <code>ProcessWindowFunction</code> 的附加信息。</p>
<h3 id="具有增量聚合功能的-processwindowfunction">具有增量聚合功能的 ProcessWindowFunction</h3>
<p><code>ProcessWindowFunction</code> 可以与 <code>ReduceFunction</code>、<code>AggregateFunction</code> 或 <code>FoldFunction</code> 相结合，以在元素到达窗口时进行增量聚合。当窗口关闭时，<code>ProcessWindowFunction</code> 将被提供聚合的结果。这使得它可以增量计算窗口，同时可以访问 <code>ProcessWindowFunction</code> 的附加窗口元信息。</p>
<p>注意 您也可以使用 legacy WindowFunction 代替 ProcessWindowFunction 进行增量窗口聚合。</p>
<h4 id="使用-reducefunction-进行增量窗口聚合">使用 ReduceFunction 进行增量窗口聚合</h4>
<p>下面的例子展示了如何将增量 ReduceFunction 与 ProcessWindowFunction 相结合，以返回窗口中最小的事件以及窗口的开始时间。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">SensorReading</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
  <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
  <span class="o">.</span><span class="n">timeWindow</span><span class="o">(&lt;</span><span class="n">duration</span><span class="o">&gt;)</span>
  <span class="o">.</span><span class="n">reduce</span><span class="o">(</span>
    <span class="o">(</span><span class="n">r1</span><span class="k">:</span> <span class="kt">SensorReading</span><span class="o">,</span> <span class="n">r2</span><span class="k">:</span> <span class="kt">SensorReading</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span> <span class="k">if</span> <span class="o">(</span><span class="n">r1</span><span class="o">.</span><span class="n">value</span> <span class="o">&gt;</span> <span class="n">r2</span><span class="o">.</span><span class="n">value</span><span class="o">)</span> <span class="n">r2</span> <span class="k">else</span> <span class="n">r1</span> <span class="o">},</span>
    <span class="o">(</span> <span class="n">key</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
      <span class="n">context</span><span class="k">:</span> <span class="kt">ProcessWindowFunction</span><span class="o">[</span><span class="k">_</span>, <span class="k">_</span>, <span class="k">_</span>, <span class="kt">TimeWindow</span><span class="o">]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span>
      <span class="n">minReadings</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[</span><span class="kt">SensorReading</span><span class="o">],</span>
      <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">SensorReading</span><span class="o">)]</span> <span class="o">)</span> <span class="k">=&gt;</span>
      <span class="o">{</span>
        <span class="k">val</span> <span class="n">min</span> <span class="k">=</span> <span class="n">minReadings</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">context</span><span class="o">.</span><span class="n">window</span><span class="o">.</span><span class="n">getStart</span><span class="o">,</span> <span class="n">min</span><span class="o">))</span>
      <span class="o">}</span>
  <span class="o">)</span>
</code></pre></div><h4 id="用-aggregatefunction-进行增量窗口聚合">用 AggregateFunction 进行增量窗口聚合</h4>
<p>下面的例子展示了如何将增量的 AggregateFunction 与 ProcessWindowFunction 结合起来，计算平均值，同时将键和窗口与平均值一起发出。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
  <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
  <span class="o">.</span><span class="n">timeWindow</span><span class="o">(&lt;</span><span class="n">duration</span><span class="o">&gt;)</span>
  <span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="k">new</span> <span class="nc">AverageAggregate</span><span class="o">(),</span> <span class="k">new</span> <span class="nc">MyProcessWindowFunction</span><span class="o">())</span>

<span class="c1">// Function definitions
</span><span class="c1"></span>
<span class="cm">/**
</span><span class="cm"> * The accumulator is used to keep a running sum and a count. The [getResult] method
</span><span class="cm"> * computes the average.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">AverageAggregate</span> <span class="k">extends</span> <span class="nc">AggregateFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="kt">Double</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span> <span class="k">=</span> <span class="o">(</span><span class="mi">0L</span><span class="o">,</span> <span class="mi">0L</span><span class="o">)</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">add</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Long</span><span class="o">),</span> <span class="n">accumulator</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span>
    <span class="o">(</span><span class="n">accumulator</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="mi">1L</span><span class="o">)</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getResult</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_1</span> <span class="o">/</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_2</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">),</span> <span class="n">b</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span>
    <span class="o">(</span><span class="n">a</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">a</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">class</span> <span class="nc">MyProcessWindowFunction</span> <span class="k">extends</span> <span class="nc">ProcessWindowFunction</span><span class="o">[</span><span class="kt">Double</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)</span>, <span class="kt">String</span>, <span class="kt">TimeWindow</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">def</span> <span class="n">process</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">context</span><span class="k">:</span> <span class="kt">Context</span><span class="o">,</span> <span class="n">averages</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)])</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">average</span> <span class="k">=</span> <span class="n">averages</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
    <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">key</span><span class="o">,</span> <span class="n">average</span><span class="o">))</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h4 id="用-foldfunction-进行增量窗口聚合">用 FoldFunction 进行增量窗口聚合</h4>
<p>下面的例子展示了如何将增量式 FoldFunction 与 ProcessWindowFunction 相结合，以提取窗口中的事件数量，并返回窗口的键和结束时间。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">SensorReading</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
 <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
 <span class="o">.</span><span class="n">timeWindow</span><span class="o">(&lt;</span><span class="n">duration</span><span class="o">&gt;)</span>
 <span class="o">.</span><span class="n">fold</span> <span class="o">(</span>
    <span class="o">(</span><span class="s">&#34;&#34;</span><span class="o">,</span> <span class="mi">0L</span><span class="o">,</span> <span class="mi">0</span><span class="o">),</span>
    <span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Long</span><span class="o">,</span> <span class="nc">Int</span><span class="o">),</span> <span class="n">r</span><span class="k">:</span> <span class="kt">SensorReading</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span> <span class="o">(</span><span class="s">&#34;&#34;</span><span class="o">,</span> <span class="mi">0L</span><span class="o">,</span> <span class="n">acc</span><span class="o">.</span><span class="n">_3</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="o">},</span>
    <span class="o">(</span> <span class="n">key</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
      <span class="n">window</span><span class="k">:</span> <span class="kt">TimeWindow</span><span class="o">,</span>
      <span class="n">counts</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span>, <span class="kt">Int</span><span class="o">)],</span>
      <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="o">)</span> <span class="k">=&gt;</span>
      <span class="o">{</span>
        <span class="k">val</span> <span class="n">count</span> <span class="k">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">key</span><span class="o">,</span> <span class="n">window</span><span class="o">.</span><span class="n">getEnd</span><span class="o">,</span> <span class="n">count</span><span class="o">.</span><span class="n">_3</span><span class="o">))</span>
      <span class="o">}</span>
  <span class="o">)</span>
</code></pre></div><h4 id="在-processwindowfunction-中使用-per-窗口状态">在 ProcessWindowFunction 中使用 per-窗口状态</h4>
<p>除了访问 keyed 状态（任何富函数都可以），ProcessWindowFunction 还可以使用 keyed 状态，该状态的作用域是函数当前正在处理的窗口。在这种情况下，理解每个窗口状态所指的窗口是什么很重要。这里涉及到不同的&quot;窗口&quot;。</p>
<ul>
<li>窗口是在指定窗口操作时定义的。这可能是1小时的滚动窗口或者2小时的滑动窗口，滑动1小时。</li>
<li>一个给定的键的定义窗口的实际实例。这可能是 12: 00 到 13: 00 的时间窗口，用户 ID xyz. 这是基于窗口定义的，会有很多窗口，基于作业当前正在处理的键的数量，基于事件属于什么时间段。</li>
</ul>
<p>每个窗口的状态与这两者中的后一种挂钩。意思是说，如果我们处理了1000个不同键的事件，并且所有键的事件当前都属于 <code>[12:00，13:00)</code> 时间窗口，那么将有1000个窗口实例，每个窗口都有自己的键的per-窗口状态。</p>
<p><code>process()</code> 调用接收到的 Context 对象上有两个方法允许访问这两种类型的状态。</p>
<ul>
<li><code>globalState()</code>，允许访问不在窗口范围内的 keyed 状态。</li>
<li><code>windowState()</code>，它允许访问同样作用于窗口的 keyed 状态。</li>
</ul>
<p>如果你预计同一窗口会有多次发射，那么这个功能是很有帮助的，因为当你对晚到的数据有晚发射的情况，或者当你有一个自定义的触发器，做投机性的早期发射时，可能会发生这种情况。在这种情况下，你会在每个窗口状态下存储之前的发射信息或发射次数。</p>
<p>当使用窗口状态时，重要的是当窗口被清除时也要清理该状态。这应该发生在 <code>clear()</code> 方法中。</p>
<h3 id="windowfunctionlegacy">WindowFunction(Legacy)</h3>
<p>在一些可以使用 <code>ProcessWindowFunction</code> 的地方，你也可以使用 <code>WindowFunction</code>。这是 <code>ProcessWindowFunction</code> 的旧版本，它提供的上下文信息较少，而且没有一些先进的功能，比如每个窗口的 keyed 状态。这个接口在某些时候会被废弃。</p>
<p>WindowFunction 的签名如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">trait</span> <span class="nc">WindowFunction</span><span class="o">[</span><span class="kt">IN</span>, <span class="kt">OUT</span>, <span class="kt">KEY</span>, <span class="kt">W</span> <span class="k">&lt;:</span> <span class="kt">Window</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">Function</span> <span class="k">with</span> <span class="nc">Serializable</span> <span class="o">{</span>

  <span class="cm">/**
</span><span class="cm">    * Evaluates the window and outputs none or several elements.
</span><span class="cm">    *
</span><span class="cm">    * @param key    The key for which this window is evaluated.
</span><span class="cm">    * @param window The window that is being evaluated.
</span><span class="cm">    * @param input  The elements in the window being evaluated.
</span><span class="cm">    * @param out    A collector for emitting elements.
</span><span class="cm">    * @throws Exception The function may throw exceptions to fail the program and trigger recovery.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">KEY</span><span class="o">,</span> <span class="n">window</span><span class="k">:</span> <span class="kt">W</span><span class="o">,</span> <span class="n">input</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[</span><span class="kt">IN</span><span class="o">],</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">OUT</span><span class="o">])</span>
<span class="o">}</span>
</code></pre></div><p>可以这样使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyWindowFunction</span><span class="o">())</span>
</code></pre></div><h3 id="触发器">触发器</h3>
<p>触发器决定一个窗口（由窗口分配器形成）何时可以被窗口函数处理。每个 <code>WindowAssigner</code> 都有一个默认的触发器。如果默认的触发器不符合你的需求，你可以使用 <code>trigger(...)</code> 指定一个自定义的触发器。</p>
<p>触发器接口有五个方法，允许 Trigger 对不同的事件做出反应。</p>
<ul>
<li><code>onElement()</code> 方法对每个添加到窗口的元素都会被调用。</li>
<li><code>onEventTime()</code> 方法在注册的事件时间定时器启动时被调用。</li>
<li><code>onProcessingTime()</code> 方法在注册的处理时间计时器启动时被调用。</li>
<li><code>onMerge()</code> 方法与有状态的触发器相关，当两个触发器的对应窗口合并时，例如使用会话窗口时，就会合并两个触发器的状态。</li>
<li>最后 <code>clear()</code> 方法在删除相应窗口时执行任何需要的操作。</li>
</ul>
<p>关于以上方法有两点需要注意。</p>
<p>1）前三个方法通过返回一个 <code>TriggerResult</code> 来决定如何对其调用事件采取行动。动作可以是以下之一。</p>
<ul>
<li>CONTINUE：什么也不做。</li>
<li>FIRE：触发计算。</li>
<li>PURGE：清除窗口中的元素，以及</li>
<li>FIRE_AND_PURGE：触发计算，之后清除窗口中的元素。</li>
</ul>
<ol start="2">
<li>这些方法中的任何一种都可以用来注册处理时间或事件时间的定时器，以备将来的操作。</li>
</ol>
<h3 id="fire-和-purge">Fire 和 Purge</h3>
<p>一旦触发器确定一个窗口可以处理，它就会发射，即返回 FIRE 或 FIRE_AND_PURGE。这是窗口操作者发出当前窗口结果的信号。给定一个带有 ProcessWindowFunction 的窗口，所有的元素都会被传递给 ProcessWindowFunction（可能是在将它们传递给 evictor 之后）。带有 ReduceFunction、AggregateFunction 或 FoldFunction 的窗口只是简单地发出它们急切的聚合结果。</p>
<p>当一个触发器发射时，它可以是 FIRE 或 FIRE_AND_PURGE。FIRE 保留窗口的内容，而 FIRE_AND_PURGE 则删除其内容。默认情况下，预先实现的触发器只是 FIRE 而不清除窗口状态。</p>
<p>注意 Purging 将简单地删除窗口的内容，并将完整地保留任何关于窗口和任何触发状态的潜在元信息。</p>
<h3 id="窗口分配器的默认触发器">窗口分配器的默认触发器</h3>
<p>WindowAssigner 的默认触发器适合于许多用例。例如，所有的事件时间窗口分配器都有一个 EventTimeTrigger 作为默认触发器。这个触发器仅仅是在水印通过窗口结束后就会触发。</p>
<p>注意：GlobalWindow 的默认触发器是 NeverTrigger，它永远不会触发。因此，在使用 GlobalWindow 时，您必须定义一个自定义的触发器。</p>
<p>注意：通过使用 trigger() 指定一个触发器，您将覆盖一个 WindowAssigner 的默认触发器。例如，如果你为 TumblingEventTimeWindows 指定了一个 CountTrigger，你将不再获得基于时间进度的窗口启动，而只能通过计数来获得。现在，如果你想同时基于时间和计数做出反应，你必须编写自己的自定义触发器。</p>
<h3 id="内置和自定义触发器">内置和自定义触发器</h3>
<p>Flink 内置了一些触发器。</p>
<ul>
<li>前面已经提到过的, EventTimeTrigger 会根据水印测量的事件时间的进展而触发。</li>
<li>处理时间触发器（ProcessingTimeTrigger）基于处理时间而触发。</li>
<li>CountTrigger 在一个窗口中的元素数量超过给定的限制时触发。</li>
<li>PurgingTrigger 将另一个触发器作为参数，并将其转换为一个清洗触发器。</li>
</ul>
<p>如果你需要实现一个自定义的触发器，你应该查看抽象的 <a href="https://github.com/apache/flink/blob/master//flink-streaming-java/src/main/java/org/apache/flink/streaming/api/windowing/triggers/Trigger.java">Trigger</a> 类。请注意，API 仍在不断发展，可能会在 Flink 的未来版本中改变。</p>
<h3 id="evictors">Evictors</h3>
<p>Flink 的窗口模型允许在 WindowAssigner 和 Trigger 之外指定一个可选的 Evictor。这可以通过 <code>evictor(...)</code> 方法来完成（如本文开头所示）。Evictor 能够在触发器触发后和应用窗口函数之前和/或之后从窗口中移除元素。要做到这一点，Evictor 接口有两个方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="cm">/**
</span><span class="cm"> * Optionally evicts elements. Called before windowing function.
</span><span class="cm"> *
</span><span class="cm"> * @param elements The elements currently in the pane.
</span><span class="cm"> * @param size The current number of elements in the pane.
</span><span class="cm"> * @param window The {@link Window}
</span><span class="cm"> * @param evictorContext The context for the Evictor
</span><span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">evictBefore</span><span class="o">(</span><span class="n">Iterable</span><span class="o">&lt;</span><span class="n">TimestampedValue</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;&gt;</span> <span class="n">elements</span><span class="o">,</span> <span class="kt">int</span> <span class="n">size</span><span class="o">,</span> <span class="n">W</span> <span class="n">window</span><span class="o">,</span> <span class="n">EvictorContext</span> <span class="n">evictorContext</span><span class="o">);</span>

<span class="cm">/**
</span><span class="cm"> * Optionally evicts elements. Called after windowing function.
</span><span class="cm"> *
</span><span class="cm"> * @param elements The elements currently in the pane.
</span><span class="cm"> * @param size The current number of elements in the pane.
</span><span class="cm"> * @param window The {@link Window}
</span><span class="cm"> * @param evictorContext The context for the Evictor
</span><span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">evictAfter</span><span class="o">(</span><span class="n">Iterable</span><span class="o">&lt;</span><span class="n">TimestampedValue</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;&gt;</span> <span class="n">elements</span><span class="o">,</span> <span class="kt">int</span> <span class="n">size</span><span class="o">,</span> <span class="n">W</span> <span class="n">window</span><span class="o">,</span> <span class="n">EvictorContext</span> <span class="n">evictorContext</span><span class="o">);</span>
</code></pre></div><p><code>evictBefore()</code> 包含在窗口函数之前应用的驱逐逻辑，而 <code>evictAfter()</code> 包含在窗口函数之后应用的逻辑。在应用窗口函数之前被驱逐的元素将不会被它处理。</p>
<p>Flink 自带了三个预先实现的驱逐器。这三个是:</p>
<ul>
<li>CountEvictor：从窗口中保留最多用户指定数量的元素，并从窗口缓冲区开始丢弃剩余的元素。</li>
<li>DeltaEvictor：取 DeltaFunction 和阈值，计算窗口缓冲区中最后一个元素和剩余元素之间的 delta，并删除 delta 大于或等于阈值的元素。</li>
<li>TimeEvictor：以毫秒为单位的时间间隔作为参数，对于一个给定的窗口，它在其元素中找到最大的时间戳 max_ts，并删除所有时间戳小于 max_ts - interval 的元素。</li>
</ul>
<p>默认情况下，所有预先实现的 evictor 都会在 window 函数之前应用其逻辑。</p>
<p>注意: 指定一个 evictor 可以防止任何预聚集，因为一个窗口的所有元素都必须在应用计算之前传递给 evictor。</p>
<p>注意 Flink 不保证窗口内元素的顺序。这意味着，虽然 evictor 可以从窗口的开头移除元素，但这些元素不一定是最先或最后到达的。</p>
<h2 id="允许的延迟">允许的延迟</h2>
<p>当使用事件时间窗口时，可能会发生元素迟到的情况，也就是说，Flink 用来跟踪事件时间进度的水印已经超过了元素所属窗口的结束时间戳。关于 Flink 如何处理事件时间，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间</a>，尤其是<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html#late-elements">迟到元素</a>。</p>
<p>默认情况下，当水印超过窗口的结束时间时，晚期元素就会被删除。然而，Flink 允许为窗口操作者指定一个最大允许延迟。允许延迟指定了元素在被丢弃之前可以迟到多少时间，其默认值为0。 在水印通过窗口结束后但在其通过窗口结束前加上允许延迟之前到达的元素，仍然会被添加到窗口中。根据所使用的触发器，一个迟到但未被丢弃的元素可能会导致窗口再次启动。EventTimeTrigger 就属于这种情况。</p>
<p>为了使这个工作，Flink 会保持窗口的状态，直到它们的允许延迟过期。一旦发生这种情况，Flink 就会删除窗口并删除其状态，这一点在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-lifecycle">窗口生命周期</a>部分也有描述。</p>
<p>默认情况下，允许的延迟被设置为0，也就是说，到达水印后面的元素将被丢弃。</p>
<p>您可以像这样指定允许的延迟。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">allowedLateness</span><span class="o">(&lt;</span><span class="n">time</span><span class="o">&gt;)</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>
</code></pre></div><p>注意 当使用 GlobalWindows 窗口分配器时，由于全局窗口的结束时间戳是 Long.MAX_VALUE，因此没有数据被认为是迟到数据。</p>
<h3 id="作为侧输出获取迟到数据">作为侧输出获取迟到数据</h3>
<p>使用 Flink 的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/side_output.html">侧输出</a>功能，你可以得到一个被丢弃的迟到数据流。</p>
<p>首先，你需要在窗口化的数据流上使用 <code>sideOutputLateData(OutputTag)</code> 来指定你要获取迟到的数据。然后，你就可以在窗口化操作的结果上得到侧输出流。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">lateOutputTag</span> <span class="k">=</span> <span class="nc">OutputTag</span><span class="o">[</span><span class="kt">T</span><span class="o">](</span><span class="s">&#34;late-data&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">allowedLateness</span><span class="o">(&lt;</span><span class="n">time</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">sideOutputLateData</span><span class="o">(</span><span class="n">lateOutputTag</span><span class="o">)</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="k">val</span> <span class="n">lateStream</span> <span class="k">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getSideOutput</span><span class="o">(</span><span class="n">lateOutputTag</span><span class="o">)</span>
</code></pre></div><h4 id="迟到元素的考虑">迟到元素的考虑</h4>
<p>当指定允许的延迟大于0时，在水印通过窗口结束后，窗口及其内容将被保留。在这些情况下，当一个迟到但未被丢弃的元素到达时，它可能会触发窗口的另一次发射。这些发射被称为晚期发射，因为它们是由晚期事件触发的，与主发射相反，主发射是窗口的第一次发射。在会话窗口的情况下，迟发可能会进一步导致窗口的合并，因为它们可能会&quot;弥合&quot;两个已经存在的、未合并的窗口之间的差距。</p>
<p>注意：你应该意识到，晚点发射的元素应该被视为之前计算的更新结果，也就是说，你的数据流将包含同一计算的多个结果。根据你的应用，你需要考虑到这些重复的结果，或者对它们进行重复复制。</p>
<h3 id="处理窗口结果">处理窗口结果</h3>
<p>窗口化操作的结果又是一个 DataStream，在结果元素中没有保留任何关于窗口化操作的信息，所以如果你想保留窗口的元信息，你必须在你的 <code>ProcessWindowFunction</code> 的结果元素中手动编码这些信息。在结果元素上设置的唯一相关信息是元素的时间戳。这被设置为处理过的窗口的最大允许时间戳，也就是结束时间戳-1，因为窗口结束时间戳是独占的。注意，这对事件时间窗口和处理时间窗口都是如此，即在窗口化操作后元素总是有一个时间戳，但这个时间戳可以是事件时间时间戳，也可以是处理时间时间戳。对于处理时间窗口来说，这没有特别的影响，但是对于事件时间窗口来说，加上水印与窗口的交互方式，使得<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#consecutive-windowed-operations">连续的窗口化操作</a>具有相同的窗口大小。我们将在看完水印如何与窗口交互后再谈这个问题。</p>
<h4 id="水印和窗口的交互">水印和窗口的交互</h4>
<p>在继续本节之前，你可能想看看我们关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间和水印</a>的章节。</p>
<p>当水印到达窗口操作符时，会触发两件事。</p>
<ul>
<li>水印会触发计算所有窗口的最大时间戳（就是结束时间戳-1）小于新水印的窗口。</li>
<li>水印被转发到下游的操作中</li>
</ul>
<p>直观地说，水印会&quot;冲掉&quot;任何在下游操作中被认为是晚期的窗口，一旦它们收到该水印。</p>
<h4 id="连续的窗口操作">连续的窗口操作</h4>
<p>如前所述，计算窗口化结果的时间戳的方式以及水印与窗口的交互方式允许将连续的窗口化操作串在一起。当你想进行两个连续的窗口化操作时，如果你想使用不同的键，但仍然希望来自同一个上游窗口的元素最终出现在同一个下游窗口中，这就很有用。考虑这个例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">resultsPerKey</span> <span class="k">=</span> <span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">reduce</span><span class="o">(</span><span class="k">new</span> <span class="nc">Summer</span><span class="o">())</span>

<span class="k">val</span> <span class="n">globalResults</span> <span class="k">=</span> <span class="n">resultsPerKey</span>
    <span class="o">.</span><span class="n">windowAll</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">TopKWindowFunction</span><span class="o">())</span>
</code></pre></div><p>在这个例子中，第一次操作的时间窗口 <code>[0，5)</code> 的结果也会在随后的窗口操作中最终出现在时间窗口 <code>[0，5)</code>。这样就可以计算每个键的和，然后在第二个操作中计算同一窗口内的 top-k 元素。</p>
<h3 id="有用的状态大小考虑">有用的状态大小考虑</h3>
<p>窗口可以在很长一段时间内（如几天、几周或几个月）被定义，因此会积累非常大的状态。在估算窗口计算的存储需求时，有几个规则需要牢记。</p>
<ol>
<li>
<p>Flink 为每个元素所属的窗口创建一个副本。鉴于此，翻滚窗口为每个元素保留一个副本（一个元素正好属于一个窗口，除非它被后期丢弃）。相比之下，滑动窗口会给每个元素创建若干个，这一点在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-assigners">窗口分配器</a>部分有解释。因此，大小为1天，滑动1秒的滑动窗口可能不是一个好主意。</p>
</li>
<li>
<p>ReduceFunction、AggregateFunction 和 FoldFunction 可以显著降低存储要求，因为它们热衷于聚合元素，每个窗口只存储一个值。相比之下，仅仅使用 ProcessWindowFunction 就需要累积所有元素。</p>
</li>
<li>
<p>使用 Evictor 可以防止任何预聚集，因为一个窗口的所有元素都必须在应用计算之前通过 evictor（见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#evictors">Evictor</a>）。</p>
</li>
</ol>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/operators" term="operators" label="Operators" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/windows" term="windows" label="Windows" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[自定义序列化管理状态]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-custom-serialization-for-managed-state/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-custom-serialization-for-managed-state/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Custom Serialization for Managed State</blockquote><p>本页面的目标是为需要使用自定义状态序列化的用户提供指导，涵盖了如何提供自定义状态序列化器，以及实现允许状态模式演化的序列化器的指南和最佳实践。</p>
<p>如果你只是简单地使用 Flink 自带的序列化器，这个页面是不相关的，可以忽略。</p>
<h2 id="使用自定义状态序列化器">使用自定义状态序列化器</h2>
<p>当注册一个 managed operator 或 keyed state时，需要一个 <code>StateDescriptor</code> 来指定状态的名称，以及状态的类型信息。类型信息被 Flink 的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html">类型序列化框架</a>用来为状态创建合适的序列化器。</p>
<p>也可以完全绕过这一点，让 Flink 使用自己的自定义序列化器来序列化被管理的状态，只需用自己的 <code>TypeSerializer</code> 实现直接实例化 <code>StateDescriptor</code> 即可。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">CustomTypeSerializer</span> <span class="k">extends</span> <span class="nc">TypeSerializer</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Integer</span><span class="o">)]</span> <span class="o">{...}</span>

<span class="k">val</span> <span class="n">descriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ListStateDescriptor</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Integer</span><span class="o">)](</span>
    <span class="s">&#34;state-name&#34;</span><span class="o">,</span>
    <span class="k">new</span> <span class="nc">CustomTypeSerializer</span><span class="o">)</span>
<span class="o">)</span>

<span class="n">checkpointedState</span> <span class="k">=</span> <span class="n">getRuntimeContext</span><span class="o">.</span><span class="n">getListState</span><span class="o">(</span><span class="n">descriptor</span><span class="o">)</span>
</code></pre></div><h3 id="状态序列化器和模式演进">状态序列化器和模式演进</h3>
<p>本节解释了与状态序列化和模式演进相关的面向用户的抽象，以及关于 Flink 如何与这些抽象交互的必要内部细节。</p>
<p>当从保存点恢复时，Flink 允许改变用于读取和写入先前注册状态的序列化器，因此用户不会被锁定在任何特定的序列化模式上。当状态被还原时，将为该状态注册一个新的序列化器（即在还原作业中用于访问状态的 <code>StateDescriptor</code> 所附带的序列化器）。这个新的序列化器可能与之前的序列化器的模式不同。因此，在实现状态序列化器时，除了读取/写入数据的基本逻辑外，另一个需要注意的重要问题是未来如何改变序列化模式。</p>
<p>说到 schema，在这里，这个术语可以互换，指的是状态类型的数据模型和状态类型的序列化二进制格式。一般来说，模式，可以为少数情况而改变。</p>
<ol>
<li>状态类型的数据模式发生了变化，即从 POJO 中增加或删除一个作为状态的字段。</li>
<li>一般来说，数据模式发生变化后，需要升级序列器的序列化格式。</li>
<li>序列器的配置发生了变化。</li>
</ol>
<p>为了让新的执行有状态的写入模式的信息，并检测模式是否发生了变化，在对操作符的状态进行保存点时，需要将状态序列器的快照和状态字节一起写入。这就是抽象出来的一个 <code>TypeSerializerSnapshot</code>，在下一小节解释。</p>
<h3 id="typeserializersnapshot-抽象"><code>TypeSerializerSnapshot</code> 抽象</h3>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">TypeSerializerSnapshot</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kt">int</span> <span class="nf">getCurrentVersion</span><span class="o">();</span>
    <span class="kt">void</span> <span class="nf">writeSnapshot</span><span class="o">(</span><span class="n">DataOuputView</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="o">;</span>
    <span class="kt">void</span> <span class="nf">readSnapshot</span><span class="o">(</span><span class="kt">int</span> <span class="n">readVersion</span><span class="o">,</span> <span class="n">DataInputView</span> <span class="n">in</span><span class="o">,</span> <span class="n">ClassLoader</span> <span class="n">userCodeClassLoader</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="o">;</span>
    <span class="n">TypeSerializerSchemaCompatibility</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="nf">resolveSchemaCompatibility</span><span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="n">newSerializer</span><span class="o">);</span>
    <span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="nf">restoreSerializer</span><span class="o">();</span>
<span class="o">}</span>
<span class="kd">public</span> <span class="kd">abstract</span> <span class="kd">class</span> <span class="nc">TypeSerializer</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="o">{</span>    
    
    <span class="c1">// ...
</span><span class="c1"></span>    
    <span class="kd">public</span> <span class="kd">abstract</span> <span class="n">TypeSerializerSnapshot</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="nf">snapshotConfiguration</span><span class="o">();</span>
<span class="o">}</span>
</code></pre></div><p>序列器的 TypeSerializerSnapshot 是一个时间点信息，它作为状态序列器的写模式的唯一真理来源，以及还原一个序列器所必须的任何额外信息，这些信息将与给定的时间点相同。关于在还原时应该写入和读取什么作为序列器快照的逻辑是在 <code>writeSnapshot和readSnapshot</code> 方法中定义的。</p>
<p>请注意，快照本身的写模式也可能需要随着时间的推移而改变（例如，当你希望在快照中添加更多关于序列器的信息时）。为了方便，快照是有版本的，在 <code>getCurrentVersion</code> 方法中定义了当前的版本号。在还原时，当从保存点读取序列器快照时，将向 <code>readSnapshot</code> 方法提供写入快照的模式的版本，以便读取实现可以处理不同的版本。</p>
<p>在还原时，检测新的序列器的模式是否改变的逻辑应该在 <code>resolveSchemaCompatibility</code> 方法中实现。当之前的注册状态在还原执行的操作符中再次注册新的序列化器时，新的序列化器会通过这个方法提供给之前序列化器的快照。该方法返回一个代表兼容性解决结果的 <code>TypeSerializerSchemaCompatibility</code>，它可以是以下之一。</p>
<ol>
<li><code>TypeSerializerSchemaCompatibility.compatibleAsIs()</code>：这个结果标志着新的序列化器是兼容的，这意味着新的序列化器与之前的序列化器具有相同的模式。有可能在resolveSchemaCompatibility方法中重新配置了新的序列化器，使其兼容。</li>
<li><code>TypeSerializerSchemaCompatibility.compatibleAfterMigration()</code>：这个结果标志着新的序列化器具有不同的序列化模式，可以从旧的模式迁移，使用之前的序列化器（识别旧的模式）将字节读入状态对象，然后用新的序列化器（识别新的模式）将对象重新写回字节。</li>
<li><code>TypeSerializerSchemaCompatibility.incompatible()</code>：这个结果标志着新的序列化器有不同的序列化模式，但不可能从旧模式迁移。</li>
</ol>
<p>最后一点细节是在需要迁移的情况下，如何获得之前的序列化器。序列化器的 <code>TypeSerializerSnapshot</code> 的另一个重要作用是，它可以作为一个工厂来恢复以前的序列化器。更具体地说，<code>TypeSerializerSnapshot</code> 应该实现 <code>restoreSerializer</code> 方法来实例化一个序列化器实例，该实例能够识别之前序列化器的模式和配置，因此可以安全地读取之前序列化器写入的数据。</p>
<h4 id="flink-如何与-typeserializer-和-typeserializersnapshot-抽象进行交互">Flink 如何与 TypeSerializer 和 TypeSerializerSnapshot 抽象进行交互</h4>
<p>总结一下，本节总结了 Flink，或者更具体地说，状态后端如何与抽象进行交互。根据状态后端的不同，交互略有不同，但这与状态序列化器及其序列化器快照的实现是正交的。</p>
<p><strong>离堆状态后端(如 RocksDBStateBackend)</strong></p>
<ol>
<li>用具有模式A的状态序列器注册新的状态。</li>
</ol>
<ul>
<li>注册的 TypeSerializer 用于在每次状态访问时读取/写入状态。</li>
<li>状态被写入模式A中。</li>
</ul>
<ol start="2">
<li>拍摄一个保存点</li>
</ol>
<ul>
<li>序列器快照是通过 <code>TypeSerializer#snapshotConfiguration</code> 方法提取的。</li>
<li>序列器快照被写入保存点，以及已经序列化的状态字节（模式A）。</li>
</ul>
<ol start="3">
<li>恢复的执行用新的状态序列化器重新访问恢复的状态字节，新的状态序列化器具有模式B。</li>
</ol>
<ul>
<li>前一个状态序列器的快照被还原。</li>
<li>状态字节在还原时不被反序列化，只被加载回状态后端（因此，仍在模式A中）。</li>
<li>接收到新的序列化器后，通过 <code>TypeSerializer#resolveSchemaCompatibility</code> 提供给被还原的前一个序列化器的快照，检查模式是否兼容。</li>
</ul>
<ol start="4">
<li>将后端中的状态字节从模式A迁移到模式B。</li>
</ol>
<ul>
<li>如果兼容性决议反映模式已经改变，并且可以进行迁移，则进行模式迁移。通过 <code>TypeSerializerSnapshot#restoreSerializer()</code>，将从序列化器快照中获取之前识别模式A的状态序列化器，并用于反序列化状态字节到对象，进而用新的序列化器再次重写，识别模式B，完成迁移。在继续处理之前，所有访问状态的条目全部迁移完毕。</li>
<li>如果解析信号为不兼容，则状态访问失败，出现异常。</li>
</ul>
<p><strong>堆状态后端（如 MemoryStateBackend、FsStateBackend）</strong>:</p>
<ol>
<li>用具有模式A的状态序列器注册新的状态。</li>
</ol>
<ul>
<li>注册的 TypeSerializer 由状态后端维护。</li>
</ul>
<ol start="2">
<li>拍摄一个保存点，将所有状态用模式A序列化。</li>
</ol>
<ul>
<li>序列器快照是通过 <code>TypeSerializer#snapshotConfiguration</code> 方法提取的。</li>
<li>序列化器快照被写入保存点。</li>
<li>现在状态对象被序列化到保存点，写入模式A中。</li>
</ul>
<ol start="3">
<li>在还原时，将状态反序列化为堆中的对象。</li>
</ol>
<ul>
<li>前一个状态序列器的快照被恢复。</li>
<li>通过 <code>TypeSerializerSnapshot#restoreSerializer()</code> 从序列化器快照中获取之前的序列化器，该序列化器识别模式A，用于将状态字节反序列化为对象。</li>
<li>从现在开始，所有的状态都已经被反序列化了。</li>
</ul>
<ol start="4">
<li>恢复后的执行用新的状态序列化器重新访问以前的状态，新的状态序列化器具有模式B。</li>
</ol>
<ul>
<li>在接收到新的序列化器后，通过 <code>TypeSerializer#resolveSchemaCompatibility</code> 提供给恢复之前序列化器的快照，以检查模式的兼容性。</li>
<li>如果兼容性检查发出需要迁移的信号，在这种情况下什么都不会发生，因为对于堆后端来说，所有的状态已经被反序列化为对象。</li>
<li>如果解析信号为不兼容，则状态访问失败，出现异常。</li>
</ul>
<ol start="5">
<li>再拍摄一个保存点，将所有状态用模式B序列化。</li>
</ol>
<ul>
<li>与步骤2.相同，但现在状态字节都在模式B中。</li>
</ul>
<h3 id="预先定义方便的-typeserializersnapshot-类">预先定义方便的 TypeSerializerSnapshot 类</h3>
<p>Flink 提供了两个抽象的基础 TypeSerializerSnapshot 类，可以用于典型场景。SimpleTypeSerializerSnapshot 和 CompositeTypeSerializerSnapshot。</p>
<p>提供这些预定义快照作为其序列化器快照的序列化器必须始终有自己独立的子类实现。这与不在不同的序列化器之间共享快照类的最佳实践相对应，这将在下一节中得到更详尽的解释。</p>
<h4 id="实现-simpletypeserializersnapshot">实现 SimpleTypeSerializerSnapshot</h4>
<p>SimpleTypeSerializerSnapshot 是为没有任何状态或配置的序列化器准备的，本质上意味着序列化器的序列化模式完全由序列化器的类来定义。</p>
<p>当使用 SimpleTypeSerializerSnapshot 作为你的序列化器的快照类时，兼容性解决只有2种可能的结果。</p>
<ul>
<li>TypeSerializerSchemaCompatibility.compatibleAsIs()，如果新的序列化器类保持相同，或</li>
<li>TypeSerializerSchemaCompatibility.incompatible()，如果新的序列化器类与之前的序列化器类不同。</li>
</ul>
<p>下面以 Flink 的 <code>IntSerializer</code> 为例，介绍 <code>SimpleTypeSerializerSnapshot</code> 的使用方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">IntSerializerSnapshot</span> <span class="kd">extends</span> <span class="n">SimpleTypeSerializerSnapshot</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="nf">IntSerializerSnapshot</span><span class="o">()</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(()</span> <span class="o">-&gt;</span> <span class="n">IntSerializer</span><span class="o">.</span><span class="na">INSTANCE</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>IntSerializer 没有状态或配置。序列化格式完全由序列化器类自己定义，只能由另一个 IntSerializer 读取。因此，它适合 SimpleTypeSerializerSnapshot 的使用情况。</p>
<p>SimpleTypeSerializerSnapshot 的基础超级构造函数期望得到一个相应序列器实例的 Supplier，不管快照当前是在还原还是在快照期间写入。该 Supplier 用于创建还原序列化器，以及类型检查，以验证新序列化器是否属于相同的预期序列化器类。</p>
<h4 id="实现-compositetypeserializersnapshot">实现 CompositeTypeSerializerSnapshot</h4>
<p>CompositeTypeSerializerSnapshot 是为那些依赖于多个嵌套序列化器的序列化器而设计的。</p>
<p>在进一步解释之前，我们将依赖于多个嵌套序列化器的序列化器称为此上下文中的&quot;外部&quot;序列化器。这方面的例子可以是 MapSerializer、ListSerializer、GenericArraySerializer 等。例如，考虑 MapSerializer &ndash;键和值序列化器将是嵌套序列化器，而MapSerializer本身是 &ldquo;外部 &ldquo;序列化器。</p>
<p>在这种情况下，外层序列化器的快照也应该包含嵌套序列化器的快照，这样就可以独立检查嵌套序列化器的兼容性。在解决外层序列化器的兼容性时，需要考虑每个嵌套序列化器的兼容性。</p>
<p>提供 CompositeTypeSerializerSnapshot 是为了协助实现这类复合序列器的快照。它处理嵌套序列化器快照的读写，以及考虑到所有嵌套序列化器的兼容性，解析最终的兼容性结果。</p>
<p>下面以 Flink 的 MapSerializer 为例，介绍如何使用 CompositeTypeSerializerSnapshot。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">MapSerializerSnapshot</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="kd">extends</span> <span class="n">CompositeTypeSerializerSnapshot</span><span class="o">&lt;</span><span class="n">Map</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;,</span> <span class="n">MapSerializer</span><span class="o">&gt;</span> <span class="o">{</span>

    <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">CURRENT_VERSION</span> <span class="o">=</span> <span class="n">1</span><span class="o">;</span>

    <span class="kd">public</span> <span class="nf">MapSerializerSnapshot</span><span class="o">()</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span><span class="n">MapSerializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="nf">MapSerializerSnapshot</span><span class="o">(</span><span class="n">MapSerializer</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="n">mapSerializer</span><span class="o">)</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span><span class="n">mapSerializer</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">int</span> <span class="nf">getCurrentOuterSnapshotVersion</span><span class="o">()</span> <span class="o">{</span>
        <span class="k">return</span> <span class="n">CURRENT_VERSION</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">MapSerializer</span> <span class="nf">createOuterSerializerWithNestedSerializers</span><span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="n">nestedSerializers</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">K</span><span class="o">&gt;</span> <span class="n">keySerializer</span> <span class="o">=</span> <span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">K</span><span class="o">&gt;)</span> <span class="n">nestedSerializers</span><span class="o">[</span><span class="n">0</span><span class="o">];</span>
        <span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">V</span><span class="o">&gt;</span> <span class="n">valueSerializer</span> <span class="o">=</span> <span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">V</span><span class="o">&gt;)</span> <span class="n">nestedSerializers</span><span class="o">[</span><span class="n">1</span><span class="o">];</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">MapSerializer</span><span class="o">&lt;&gt;(</span><span class="n">keySerializer</span><span class="o">,</span> <span class="n">valueSerializer</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="n">getNestedSerializers</span><span class="o">(</span><span class="n">MapSerializer</span> <span class="n">outerSerializer</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="o">{</span> <span class="n">outerSerializer</span><span class="o">.</span><span class="na">getKeySerializer</span><span class="o">(),</span> <span class="n">outerSerializer</span><span class="o">.</span><span class="na">getValueSerializer</span><span class="o">()</span> <span class="o">};</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>当实现一个新的序列器快照作为 CompositeTypeSerializerSnapshot 的子类时，必须实现以下三个方法。</p>
<ul>
<li><code>#getCurrentOuterSnapshotVersion()</code>。该方法定义了当前外部序列化器快照的序列化二进制格式的版本。</li>
<li><code>#getNestedSerializers(TypeSerializer)</code>。给定外部序列化器，返回其嵌套的序列化器。</li>
<li><code>#createOuterSerializerWithNestedSerializers(TypeSerializer[])</code>。给定嵌套的序列化器，创建一个外部序列化器的实例。</li>
</ul>
<p>上面的例子是一个 CompositeTypeSerializerSnapshot，除了嵌套的序列化器的快照外，没有额外的信息需要快照。因此，可以预期其外部快照版本永远不需要上报。然而，其他一些序列化器，包含一些额外的静态配置，需要和嵌套的组件序列化器一起持久化。一个例子是 Flink 的 GenericArraySerializer，除了嵌套的元素序列化器之外，它还包含了数组元素类型的类作为配置。</p>
<p>在这些情况下，需要在 CompositeTypeSerializerSnapshot 上实现另外三个方法。</p>
<ul>
<li><code>#writeOuterSnapshot(DataOutputView)</code>：定义如何写入外部快照信息。</li>
<li><code>#readOuterSnapshot(int, DataInputView, ClassLoader)</code>：定义如何读取外部快照信息。</li>
<li><code>#resolveOuterSchemaCompatibility(TypeSerializer)</code>：根据外部快照信息检查兼容性。</li>
</ul>
<p>默认情况下，CompositeTypeSerializerSnapshot 假设没有任何外部快照信息可读/可写，因此上述方法的默认实现为空。如果子类有外部快照信息，那么这三个方法必须全部实现。</p>
<p>下面以 Flink 的 GenericArraySerializer 为例，说明 CompositeTypeSerializerSnapshot 如何用于确实有外部快照信息的复合序列器快照。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">final</span> <span class="kd">class</span> <span class="nc">GenericArraySerializerSnapshot</span><span class="o">&lt;</span><span class="n">C</span><span class="o">&gt;</span> <span class="kd">extends</span> <span class="n">CompositeTypeSerializerSnapshot</span><span class="o">&lt;</span><span class="n">C</span><span class="o">[],</span> <span class="n">GenericArraySerializer</span><span class="o">&gt;</span> <span class="o">{</span>

    <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">CURRENT_VERSION</span> <span class="o">=</span> <span class="n">1</span><span class="o">;</span>

    <span class="kd">private</span> <span class="n">Class</span><span class="o">&lt;</span><span class="n">C</span><span class="o">&gt;</span> <span class="n">componentClass</span><span class="o">;</span>

    <span class="kd">public</span> <span class="nf">GenericArraySerializerSnapshot</span><span class="o">()</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span><span class="n">GenericArraySerializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="nf">GenericArraySerializerSnapshot</span><span class="o">(</span><span class="n">GenericArraySerializer</span><span class="o">&lt;</span><span class="n">C</span><span class="o">&gt;</span> <span class="n">genericArraySerializer</span><span class="o">)</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span><span class="n">genericArraySerializer</span><span class="o">);</span>
        <span class="k">this</span><span class="o">.</span><span class="na">componentClass</span> <span class="o">=</span> <span class="n">genericArraySerializer</span><span class="o">.</span><span class="na">getComponentClass</span><span class="o">();</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">int</span> <span class="nf">getCurrentOuterSnapshotVersion</span><span class="o">()</span> <span class="o">{</span>
        <span class="k">return</span> <span class="n">CURRENT_VERSION</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">writeOuterSnapshot</span><span class="o">(</span><span class="n">DataOutputView</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
        <span class="n">out</span><span class="o">.</span><span class="na">writeUTF</span><span class="o">(</span><span class="n">componentClass</span><span class="o">.</span><span class="na">getName</span><span class="o">());</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">readOuterSnapshot</span><span class="o">(</span><span class="kt">int</span> <span class="n">readOuterSnapshotVersion</span><span class="o">,</span> <span class="n">DataInputView</span> <span class="n">in</span><span class="o">,</span> <span class="n">ClassLoader</span> <span class="n">userCodeClassLoader</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
        <span class="k">this</span><span class="o">.</span><span class="na">componentClass</span> <span class="o">=</span> <span class="n">InstantiationUtil</span><span class="o">.</span><span class="na">resolveClassByName</span><span class="o">(</span><span class="n">in</span><span class="o">,</span> <span class="n">userCodeClassLoader</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">boolean</span> <span class="nf">resolveOuterSchemaCompatibility</span><span class="o">(</span><span class="n">GenericArraySerializer</span> <span class="n">newSerializer</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="na">componentClass</span> <span class="o">==</span> <span class="n">newSerializer</span><span class="o">.</span><span class="na">getComponentClass</span><span class="o">())</span>
            <span class="o">?</span> <span class="n">OuterSchemaCompatibility</span><span class="o">.</span><span class="na">COMPATIBLE_AS_IS</span>
            <span class="o">:</span> <span class="n">OuterSchemaCompatibility</span><span class="o">.</span><span class="na">INCOMPATIBLE</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">GenericArraySerializer</span> <span class="nf">createOuterSerializerWithNestedSerializers</span><span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="n">nestedSerializers</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">C</span><span class="o">&gt;</span> <span class="n">componentSerializer</span> <span class="o">=</span> <span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">C</span><span class="o">&gt;)</span> <span class="n">nestedSerializers</span><span class="o">[</span><span class="n">0</span><span class="o">];</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">GenericArraySerializer</span><span class="o">&lt;&gt;(</span><span class="n">componentClass</span><span class="o">,</span> <span class="n">componentSerializer</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="n">getNestedSerializers</span><span class="o">(</span><span class="n">GenericArraySerializer</span> <span class="n">outerSerializer</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="o">{</span> <span class="n">outerSerializer</span><span class="o">.</span><span class="na">getComponentSerializer</span><span class="o">()</span> <span class="o">};</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>在上面的代码片段中，有两个重要的事情需要注意。首先，由于这个 <code>CompositeTypeSerializerSnapshot</code> 实现的外快照信息是作为快照的一部分写入的，所以每当外快照信息的序列化格式发生变化时，由 <code>getCurrentOuterSnapshotVersion()</code> 定义的外快照版本必须被上调。</p>
<p>其次，请注意我们在写组件类时避免使用 Java 序列化，只写类名，在读回快照时动态加载。避免使用 Java 序列化来编写序列化器快照的内容，总的来说是一个很好的做法。关于这方面的更多细节将在下一节介绍。</p>
<h3 id="实施说明和最佳实践">实施说明和最佳实践</h3>
<ol>
<li>Flink 通过将序列器快照实例化，恢复序列器快照，其类名为</li>
</ol>
<p>序列器的快照，是注册状态如何被序列化的唯一真实来源，是读取保存点中状态的入口。为了能够恢复和访问以前的状态，必须能够恢复以前状态序列化器的快照。</p>
<p>Flink 通过首先实例化 TypeSerializerSnapshot 与其类名（与快照字节一起写入）来恢复序列器快照。因此，为了避免受到意外的类名更改或实例化失败， TypeSerializerSnapshot 类应该。</p>
<ul>
<li>避免被实现为匿名类或嵌套类。</li>
<li>有一个公共的空值构造函数用于实例化。</li>
</ul>
<ol start="2">
<li>避免在不同的序列化器之间共享同一个 TypeSerializerSnapshot 类。</li>
</ol>
<p>由于模式兼容性检查要通过序列化器快照，让多个序列化器返回同一个 TypeSerializerSnapshot 类作为它们的快照，会使 <code>TypeSerializerSnapshot#resolveSchemaCompatibility</code> 和 <code>TypeSerializerSnapshot#restoreSerializer()</code> 方法的实现变得复杂。</p>
<p>这也将是一个不好的分离关注点，一个单一序列化器的序列化模式、配置以及如何恢复它，应该合并在自己专门的TypeSerializerSnapshot类中。</p>
<ol start="3">
<li>避免使用 Java 序列化来制作序列化器快照内容</li>
</ol>
<p>在编写持久化的序列化器快照的内容时，完全不应该使用 Java 序列化。例如，一个序列化器需要持久化一个目标类型的类作为其快照的一部分。关于类的信息应该通过写入类名来持久化，而不是直接使用 Java 将类序列化。在读取快照时，会读取类名，并通过名称来动态加载类。</p>
<p>这种做法保证了序列化器快照总是可以安全读取。在上面的例子中，如果类型类是使用 Java 序列化来持久化的，一旦类的实现发生了变化，根据 Java 序列化的具体规定，快照可能不再可读，不再二进制兼容。</p>
<h3 id="从-flink-17-之前的废弃序列化快照-api-迁移">从 Flink 1.7 之前的废弃序列化快照 API 迁移</h3>
<p>本节是一个从 Flink 1.7 之前存在的序列化器和序列化器快照的 API 迁移指南。</p>
<p>在 Flink 1.7 之前，序列化器快照是以 TypeSerializerConfigSnapshot 的形式实现的（现在已经被废弃了，将来最终会被移除，完全被新的 TypeSerializerSnapshot 接口取代）。此外，序列化器模式兼容性检查的责任住在 TypeSerializer  内部，在 <code>TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot)</code> 方法中实现。</p>
<p>新旧抽象之间的另一个主要区别是，被废弃的 <code>TypeSerializerConfigSnapshot</code> 不具备实例化之前的序列化器的能力。因此，在你的序列化器仍然返回 <code>TypeSerializerConfigSnapshot</code> 的子类作为它的快照的情况下，序列化器实例本身将总是使用 Java 序列化写入 savepoints，以便在还原时可以使用以前的序列化器。这是很不可取的，因为还原作业是否成功，很容易受到前一个序列化器类的可用性的影响，或者说，一般来说，序列化器实例是否可以在还原时使用 Java 序列化读回。这意味着你的状态只能使用同一个序列化器，一旦你想升级序列化器类或进行模式迁移，可能会出现问题。</p>
<p>为了面向未来，并能灵活地迁移你的状态序列器和模式，强烈建议从旧的抽象中迁移。做到这一点的步骤如下。</p>
<ol>
<li>实现 TypeSerializerSnapshot 的新子类。这将是你的序列化器的新快照。</li>
<li>在 <code>TypeSerializer#snapshotConfiguration()</code> 方法中返回新的 <code>TypeSerializerSnapshot</code> 作为你的 serializer 快照。</li>
<li>从 Flink 1.7 之前存在的保存点恢复作业，然后再取一个保存点。注意，在这一步，旧的序列化器的 <code>TypeSerializerConfigSnapshot</code> 必须仍然存在于 classpath 中，并且不能删除 <code>TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot)</code> 方法的实现。这个过程的目的是将旧保存点中写的 <code>TypeSerializerConfigSnapshot</code> 替换为序列化器新实现的 <code>TypeSerializerSnapshot</code>。</li>
<li>一旦你有一个用 Flink 1.7 拍摄的保存点，保存点将包含 TypeSerializerSnapshot 作为状态序列化器快照，序列化器实例将不再写入保存点中。在这一点上，现在可以安全地删除旧抽象的所有实现（从序列化器中删除旧的 TypeSerializerConfigSnapshot 实现，因为将作为 TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot)）。</li>
</ol>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[项目配置]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-project-configuration/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-project-configuration/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Project Configuration</blockquote><h2 id="项目配置">项目配置</h2>
<p>每个 Flink 应用都依赖于一组 Flink 库。最起码，应用程序依赖于 Flink APIs。许多应用还依赖于某些连接器库（如 Kafka、Cassandra 等）。当运行 Flink 应用时（无论是在分布式部署中，还是在 IDE 中进行测试），Flink 运行时库也必须是可用的。</p>
<h3 id="flink-核心和应用依赖性">Flink 核心和应用依赖性</h3>
<p>与大多数运行用户定义应用的系统一样，Flink 中的依赖和库有两大类。</p>
<ul>
<li>Flink 核心依赖。Flink 本身由一组运行系统所需的类和依赖关系组成，例如协调、网络、检查点、故障转移、API、操作（如窗口化）、资源管理等。所有这些类和依赖项的集合构成了 Flink 运行时的核心，在 Flink 应用启动时必须存在。</li>
</ul>
<p>这些核心类和依赖项被打包在 flink-dist jar 中。它们是 Flink 的 lib 文件夹的一部分，也是基本的 Flink 容器镜像的一部分。把这些依赖关系想象成类似于 Java 的核心库（rt.jar，charsets.jar 等），其中包含了 String 和 List 等类。</p>
<p>Flink Core Dependencies 不包含任何连接器或库（CEP、SQL、ML 等），以避免默认情况下 classpath 中的依赖关系和类数量过多。事实上，我们尽量让核心依赖关系保持纤细，以保持默认 classpath 小，避免依赖冲突。</p>
<ul>
<li>用户应用依赖是指特定用户应用所需要的所有连接器、格式或库。</li>
</ul>
<p>用户应用程序通常被打包成一个应用程序 jar，其中包含了应用程序代码和所需的连接器和库依赖。</p>
<p>用户应用依赖关系明确不包括 Flink DataStream API 和运行时依赖关系，因为这些已经是 Flink 核心依赖关系的一部分。</p>
<h3 id="设置一个项目-基本依赖性">设置一个项目: 基本依赖性</h3>
<p>每一个 Flink 应用都需要最低限度的 API 依赖关系，来进行开发。</p>
<p>当手动设置项目时，你需要为 Java/Scala API 添加以下依赖关系（这里用 Maven 语法表示，但同样的依赖关系也适用于其他构建工具（Gradle、SBT 等）。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-streaming-scala_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>重要：请注意，所有这些依赖关系的范围都被设置为 <em>provided</em>。这意味着它们需要被编译，但它们不应该被打包到项目的应用程序 jar 文件中&ndash;这些依赖是 Flink 核心依赖，在任何设置中都是可用的。</p>
<p>强烈建议将这些依赖关系保持在 <em>provid</em> 的作用域内。如果它们没有被设置为 <em>provided</em>，最好的情况是生成的 JAR 变得过大，因为它也包含了所有 Flink 核心依赖。最坏的情况是，添加到应用程序的 jar 文件中的 Flink 核心依赖与你自己的一些依赖版本发生冲突（通常通过倒类加载来避免）。</p>
<p>关于 IntelliJ 的说明：要使应用程序在 IntelliJ IDEA 中运行，就必须在运行配置中勾选 Include dependencies with &ldquo;Provided&rdquo; scope box。如果这个选项不可用（可能是由于使用了旧的 IntelliJ IDEA 版本），那么一个简单的变通方法是创建一个调用应用程序 <code>main()</code> 方法的测试。</p>
<h3 id="添加连接器和库依赖性">添加连接器和库依赖性</h3>
<p>大多数应用都需要特定的连接器或库来运行，例如与 Kafka、Cassandra 等的连接器。这些连接器不是 Flink 核心依赖的一部分，必须作为依赖关系添加到应用程序中。</p>
<p>下面是一个将 Kafka 的连接器作为依赖项添加的例子（Maven 语法）。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>flink-connector-kafka_2.11<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>我们建议将应用程序代码和所有需要的依赖关系打包成一个带有依赖关系的 jar，我们称之为应用 jar。应用 jar 可以提交给一个已经运行的 Flink 集群，或者添加到 Flink 应用容器镜像中。</p>
<p>从 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html">Java 项目模板</a>或 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration">Scala 项目模板</a>创建的项目被配置为在运行 <code>mvn clean package</code> 时自动将应用依赖关系包含到应用 jar 中。对于没有从这些模板中设置的项目，我们建议添加 Maven Shade Plugin（如下文附录中所列）来构建包含所有所需依赖项的应用 jar。</p>
<p>重要的是。为了让 Maven（和其他构建工具）正确地将依赖关系打包到应用 jar 中，这些应用依赖关系必须在编译范围中指定（与核心依赖关系不同，后者必须在提供的范围中指定）。</p>
<h3 id="scala-版本">Scala 版本</h3>
<p>Scala 版本(2.11, 2.12 等)彼此之间不是二进制兼容的。因此，Flink for Scala 2.11 不能用于使用 Scala 2.12 的应用程序。</p>
<p>所有的 Flink 依赖性都是以 Scala 版本为后缀的，例如 flink-streaming-scala_2.11。</p>
<p>只使用 Java 的开发者可以选择任何 Scala 版本，Scala 开发者需要选择与其应用的 Scala 版本相匹配的 Scala 版本。</p>
<p>请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/flinkDev/building.html#scala-versions">构建指南</a>，了解如何为特定的 Scala 版本构建 Flink。</p>
<h3 id="hadoop-依赖性">Hadoop 依赖性</h3>
<p>一般规则：永远不需要直接将 Hadoop 依赖关系添加到您的应用程序中。唯一的例外是当使用现有的 Hadoop 输入/输出格式和 Flink 的 Hadoop  兼容性包装时。</p>
<p>如果您想将 Flink 与 Hadoop 一起使用，您需要有一个包含 Hadoop 依赖的 Flink 设置，而不是将 Hadoop 添加为应用程序依赖。详情请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html">Hadoop 设置指南</a>。</p>
<p>这种设计主要有两个原因。</p>
<ul>
<li>
<p>一些 Hadoop 交互发生在 Flink 的核心中，可能是在用户应用启动之前，例如为检查点设置 HDFS，通过 Hadoop 的 Kerberos 令牌进行认证，或者在 YARN 上进行部署。</p>
</li>
<li>
<p>Flink 的倒类加载方法将许多过渡性依赖从核心依赖中隐藏起来。这不仅适用于 Flink 自身的核心依赖，也适用于 Hadoop 在设置中存在的依赖。这样一来，应用程序可以使用相同依赖的不同版本，而不会遇到依赖冲突（相信我们，这是一个大问题，因为 Hadoop 的依赖树是巨大的）。</p>
</li>
</ul>
<p>如果你在 IDE 内部的测试或开发过程中需要 Hadoop 依赖关系（例如用于 HDFS 访问），请将这些依赖关系配置成类似于要测试或提供的依赖关系的范围。</p>
<h3 id="maven-快速入门">Maven 快速入门</h3>
<p><strong>所需</strong></p>
<p>唯一的要求是工作中的 Maven 3.0.4（或更高）和 Java 8.x 的安装。</p>
<p><strong>创建项目</strong></p>
<p>使用以下命令之一来创建项目。</p>
<ul>
<li>使用 Maven 原型</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ mvn archetype:generate                           <span class="se">\
</span><span class="se"></span>  -DarchetypeGroupId<span class="o">=</span>org.apache.flink              <span class="se">\
</span><span class="se"></span>  -DarchetypeArtifactId<span class="o">=</span>flink-quickstart-java      <span class="se">\
</span><span class="se"></span>  -DarchetypeVersion<span class="o">=</span>1.11.0
</code></pre></div><p>这可以让你为新创建的项目命名，它将交互式地要求你提供 groupId、artifactId 和包名。</p>
<ul>
<li>运行快速启动脚本</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ curl https://flink.apache.org/q/quickstart.sh <span class="p">|</span> bash -s 1.11.0
</code></pre></div><p>我们建议您将该项目导入到您的 IDE 中进行开发和测试。IntelliJ IDEA 支持开箱即用的 Maven 项目。如果您使用 Eclipse，<a href="http://www.eclipse.org/m2e/">m2e 插件</a>允许<a href="http://books.sonatype.com/m2eclipse-book/reference/creating-sect-importing-projects.html#fig-creating-import">导入 Maven 项目</a>。有些 Eclipse 捆绑包默认包含该插件，有些则需要您手动安装。</p>
<p>请注意：Java 默认的 JVM 堆大小对 Flink 来说可能太小。你必须手动增加它。在 Eclipse 中，选择 Run Configurations -&gt; Arguments，并在 VM Arguments 框中写下 -Xmx800m。在 IntelliJ IDEA 中推荐的改变 JVM 选项的方法是来自 Help | Edit Custom VM Options 菜单。详情请看<a href="https://intellij-support.jetbrains.com/hc/en-us/articles/206544869-Configuring-JVM-options-and-platform-properties">这篇文章</a>。</p>
<h4 id="构建项目">构建项目</h4>
<p>如果你想构建/打包你的项目，进入你的项目目录并运行 &ldquo;mvn clean package&rdquo; 命令。你会发现一个 JAR 文件，其中包含了你的应用程序，加上你可能已经添加的连接器和库作为应用程序的依赖关系：<code>target/&lt;artifact-id&gt;-&lt;version&gt;.jar</code>。</p>
<p>注意：如果您使用与 StreamingJob 不同的类作为应用程序的主类/入口点，我们建议您相应地更改 pom.xml 文件中的 mainClass 设置。这样，Flink 就可以从 JAR 文件中运行应用程序，而不需要额外指定主类。</p>
<h3 id="gradle">Gradle</h3>
<p><strong>需求</strong></p>
<p>唯一的要求是工作的 Gradle 3.x（或更高）和 Java 8.x 安装。</p>
<p><strong>创建项目</strong></p>
<p>使用以下命令之一来创建一个项目。</p>
<ul>
<li>Gradle 例子</li>
</ul>
<p><strong>build.gradle</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="err">buildscript</span> <span class="p">{</span>
    <span class="err">repositories</span> <span class="err">{</span>
        <span class="err">jcenter()</span> <span class="err">//</span> <span class="err">this</span> <span class="err">applies</span> <span class="err">only</span> <span class="err">to</span> <span class="err">the</span> <span class="err">Gradle</span> <span class="err">&#39;Shadow&#39;</span> <span class="err">plugin</span>
    <span class="p">}</span>
    <span class="err">dependencies</span> <span class="p">{</span>
        <span class="err">classpath</span> <span class="err">&#39;com.github.jengelman.gradle.plugins:shadow:2.0.4&#39;</span>
    <span class="p">}</span>
<span class="err">}</span>

<span class="err">plugins</span> <span class="p">{</span>
    <span class="err">id</span> <span class="err">&#39;java&#39;</span>
    <span class="err">id</span> <span class="err">&#39;application&#39;</span>
    <span class="err">//</span> <span class="err">shadow</span> <span class="err">plugin</span> <span class="err">to</span> <span class="err">produce</span> <span class="err">fat</span> <span class="err">JARs</span>
    <span class="err">id</span> <span class="err">&#39;com.github.johnrengelman.shadow&#39;</span> <span class="err">version</span> <span class="err">&#39;2.0.4&#39;</span>
<span class="p">}</span>


<span class="err">//</span> <span class="err">artifact</span> <span class="err">properties</span>
<span class="err">group</span> <span class="err">=</span> <span class="err">&#39;org.myorg.quickstart&#39;</span>
<span class="err">version</span> <span class="err">=</span> <span class="err">&#39;</span><span class="mf">0.1</span><span class="err">-SNAPSHOT&#39;</span>
<span class="err">mainClassName</span> <span class="err">=</span> <span class="err">&#39;org.myorg.quickstart.StreamingJob&#39;</span>
<span class="err">description</span> <span class="err">=</span> <span class="s2">&#34;&#34;&#34;Flink Quickstart Job&#34;&#34;&#34;</span>

<span class="err">ext</span> <span class="p">{</span>
    <span class="err">javaVersion</span> <span class="err">=</span> <span class="err">&#39;1.8&#39;</span>
    <span class="err">flinkVersion</span> <span class="err">=</span> <span class="err">&#39;1.11.0&#39;</span>
    <span class="err">scalaBinaryVersion</span> <span class="err">=</span> <span class="err">&#39;2.11&#39;</span>
    <span class="err">slf4jVersion</span> <span class="err">=</span> <span class="err">&#39;1.7.15&#39;</span>
    <span class="err">log4jVersion</span> <span class="err">=</span> <span class="err">&#39;2.12.1&#39;</span>
<span class="p">}</span>


<span class="err">sourceCompatibility</span> <span class="err">=</span> <span class="err">javaVersion</span>
<span class="err">targetCompatibility</span> <span class="err">=</span> <span class="err">javaVersion</span>
<span class="err">tasks.withType(JavaCompile)</span> <span class="p">{</span>
	<span class="err">options.encoding</span> <span class="err">=</span> <span class="err">&#39;UTF-8&#39;</span>
<span class="p">}</span>

<span class="err">applicationDefaultJvmArgs</span> <span class="err">=</span> <span class="p">[</span><span class="s2">&#34;-Dlog4j.configurationFile=log4j2.properties&#34;</span><span class="p">]</span>

<span class="err">task</span> <span class="err">wrapper(type:</span> <span class="err">Wrapper)</span> <span class="p">{</span>
    <span class="err">gradleVersion</span> <span class="err">=</span> <span class="err">&#39;3.1&#39;</span>
<span class="p">}</span>

<span class="err">//</span> <span class="err">declare</span> <span class="err">where</span> <span class="err">to</span> <span class="err">find</span> <span class="err">the</span> <span class="err">dependencies</span> <span class="err">of</span> <span class="err">your</span> <span class="err">project</span>
<span class="err">repositories</span> <span class="p">{</span>
    <span class="err">mavenCentral()</span>
    <span class="err">maven</span> <span class="err">{</span> <span class="err">url</span> <span class="nt">&#34;https://repository.apache.org/content/repositories/snapshots/&#34;</span> <span class="p">}</span>
<span class="err">}</span>

<span class="err">//</span> <span class="err">NOTE:</span> <span class="err">We</span> <span class="err">cannot</span> <span class="err">use</span> <span class="s2">&#34;compileOnly&#34;</span> <span class="err">or</span> <span class="s2">&#34;shadow&#34;</span> <span class="err">configurations</span> <span class="err">since</span> <span class="err">then</span> <span class="err">we</span> <span class="err">could</span> <span class="err">not</span> <span class="err">run</span> <span class="err">code</span>
<span class="err">//</span> <span class="err">in</span> <span class="err">the</span> <span class="err">IDE</span> <span class="err">or</span> <span class="err">with</span> <span class="s2">&#34;gradle run&#34;</span><span class="err">.</span> <span class="err">We</span> <span class="err">also</span> <span class="err">cannot</span> <span class="err">exclude</span> <span class="err">transitive</span> <span class="err">dependencies</span> <span class="err">from</span> <span class="err">the</span>
<span class="err">//</span> <span class="err">shadowJar</span> <span class="err">yet</span> <span class="err">(see</span> <span class="err">https://github.com/johnrengelman/shadow/issues/</span><span class="mi">159</span><span class="err">).</span>
<span class="err">//</span> <span class="err">-&gt;</span> <span class="err">Explicitly</span> <span class="err">define</span> <span class="err">the</span> <span class="err">//</span> <span class="err">libraries</span> <span class="err">we</span> <span class="err">want</span> <span class="err">to</span> <span class="err">be</span> <span class="err">included</span> <span class="err">in</span> <span class="err">the</span> <span class="s2">&#34;flinkShadowJar&#34;</span> <span class="err">configuration!</span>
<span class="err">configurations</span> <span class="p">{</span>
    <span class="err">flinkShadowJar</span> <span class="err">//</span> <span class="err">dependencies</span> <span class="err">which</span> <span class="err">go</span> <span class="err">into</span> <span class="err">the</span> <span class="err">shadowJar</span>

    <span class="err">//</span> <span class="err">always</span> <span class="err">exclude</span> <span class="err">these</span> <span class="err">(also</span> <span class="err">from</span> <span class="err">transitive</span> <span class="err">dependencies)</span> <span class="err">since</span> <span class="err">they</span> <span class="err">are</span> <span class="err">provided</span> <span class="err">by</span> <span class="err">Flink</span>
    <span class="err">flinkShadowJar.exclude</span> <span class="err">group:</span> <span class="err">&#39;org.apache.flink&#39;,</span> <span class="err">module:</span> <span class="err">&#39;force-shading&#39;</span>
    <span class="err">flinkShadowJar.exclude</span> <span class="err">group:</span> <span class="err">&#39;com.google.code.findbugs&#39;,</span> <span class="err">module:</span> <span class="err">&#39;jsr305&#39;</span>
    <span class="err">flinkShadowJar.exclude</span> <span class="err">group:</span> <span class="err">&#39;org.slf4j&#39;</span>
    <span class="err">flinkShadowJar.exclude</span> <span class="err">group:</span> <span class="err">&#39;org.apache.logging.log4j&#39;</span>
<span class="p">}</span>

<span class="err">//</span> <span class="err">declare</span> <span class="err">the</span> <span class="err">dependencies</span> <span class="err">for</span> <span class="err">your</span> <span class="err">production</span> <span class="err">and</span> <span class="err">test</span> <span class="err">code</span>
<span class="err">dependencies</span> <span class="p">{</span>
    <span class="err">//</span> <span class="err">--------------------------------------------------------------</span>
    <span class="err">//</span> <span class="err">Compile-time</span> <span class="err">dependencies</span> <span class="err">that</span> <span class="err">should</span> <span class="err">NOT</span> <span class="err">be</span> <span class="err">part</span> <span class="err">of</span> <span class="err">the</span>
    <span class="err">//</span> <span class="err">shadow</span> <span class="err">jar</span> <span class="err">and</span> <span class="err">are</span> <span class="err">provided</span> <span class="err">in</span> <span class="err">the</span> <span class="err">lib</span> <span class="err">folder</span> <span class="err">of</span> <span class="err">Flink</span>
    <span class="err">//</span> <span class="err">--------------------------------------------------------------</span>
    <span class="err">compile</span> <span class="nt">&#34;org.apache.flink:flink-streaming-java_${scalaBinaryVersion}:${flinkVersion}&#34;</span>

    <span class="err">//</span> <span class="err">--------------------------------------------------------------</span>
    <span class="err">//</span> <span class="err">Dependencies</span> <span class="err">that</span> <span class="err">should</span> <span class="err">be</span> <span class="err">part</span> <span class="err">of</span> <span class="err">the</span> <span class="err">shadow</span> <span class="err">jar</span><span class="p">,</span> <span class="err">e.g.</span>
    <span class="err">//</span> <span class="err">connectors.</span> <span class="err">These</span> <span class="err">must</span> <span class="err">be</span> <span class="err">in</span> <span class="err">the</span> <span class="err">flinkShadowJar</span> <span class="err">configuration!</span>
    <span class="err">//</span> <span class="err">--------------------------------------------------------------</span>
    <span class="err">//flinkShadowJar</span> <span class="nt">&#34;org.apache.flink:flink-connector-kafka-0.11_${scalaBinaryVersion}:${flinkVersion}&#34;</span>

    <span class="err">compile</span> <span class="s2">&#34;org.apache.logging.log4j:log4j-api:${log4jVersion}&#34;</span>
    <span class="err">compile</span> <span class="s2">&#34;org.apache.logging.log4j:log4j-core:${log4jVersion}&#34;</span>
    <span class="err">compile</span> <span class="s2">&#34;org.apache.logging.log4j:log4j-slf4j-impl:${log4jVersion}&#34;</span>
    <span class="err">compile</span> <span class="s2">&#34;org.slf4j:slf4j-log4j12:${slf4jVersion}&#34;</span>

    <span class="err">//</span> <span class="err">Add</span> <span class="err">test</span> <span class="err">dependencies</span> <span class="err">here.</span>
    <span class="err">//</span> <span class="err">testCompile</span> <span class="s2">&#34;junit:junit:4.12&#34;</span>
<span class="p">}</span>

<span class="err">//</span> <span class="err">make</span> <span class="err">compileOnly</span> <span class="err">dependencies</span> <span class="err">available</span> <span class="err">for</span> <span class="err">tests:</span>
<span class="err">sourceSets</span> <span class="p">{</span>
    <span class="err">main.compileClasspath</span> <span class="err">+=</span> <span class="err">configurations.flinkShadowJar</span>
    <span class="err">main.runtimeClasspath</span> <span class="err">+=</span> <span class="err">configurations.flinkShadowJar</span>

    <span class="err">test.compileClasspath</span> <span class="err">+=</span> <span class="err">configurations.flinkShadowJar</span>
    <span class="err">test.runtimeClasspath</span> <span class="err">+=</span> <span class="err">configurations.flinkShadowJar</span>

    <span class="err">javadoc.classpath</span> <span class="err">+=</span> <span class="err">configurations.flinkShadowJar</span>
<span class="p">}</span>

<span class="err">run.classpath</span> <span class="err">=</span> <span class="err">sourceSets.main.runtimeClasspath</span>

<span class="err">jar</span> <span class="p">{</span>
    <span class="err">manifest</span> <span class="err">{</span>
        <span class="err">attributes</span> <span class="err">&#39;Built-By&#39;:</span> <span class="err">System.getProperty(&#39;user.name&#39;),</span>
                <span class="err">&#39;Build-Jdk&#39;:</span> <span class="err">System.getProperty(&#39;java.version&#39;)</span>
    <span class="p">}</span>
<span class="err">}</span>

<span class="err">shadowJar</span> <span class="p">{</span>
    <span class="err">configurations</span> <span class="err">=</span> <span class="err">[project.configurations.flinkShadowJar]</span>
<span class="p">}</span>
</code></pre></div><p><strong>setting.gradle</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="err">rootProject.name</span> <span class="err">=</span> <span class="err">&#39;quickstart&#39;</span>
</code></pre></div><p>这允许你为你新创建的项目命名，它将交互式地询问你项目的名称、组织（也用于包名）、项目版本、Scala 和 Flink。它将交互式地要求你提供项目名称、组织（也用于包名）、项目版本、Scala 和 Flink 版本。</p>
<ul>
<li>运行快速启动脚本</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">bash -c <span class="s2">&#34;</span><span class="k">$(</span>curl https://flink.apache.org/q/gradle-quickstart.sh<span class="k">)</span><span class="s2">&#34;</span> -- 1.11.0 2.11
</code></pre></div><p>我们建议你将这个项目导入到你的 IDE 中进行开发和测试。IntelliJ IDEA 在安装 Gradle 插件后，支持 Gradle 项目。Eclipse 通过 <a href="https://projects.eclipse.org/projects/tools.buildship">Eclipse Buildship</a> 插件来实现（确保在导入向导的最后一步指定 Gradle 版本&gt;=3.0，影子插件需要它）。你也可以使用 <a href="https://docs.gradle.org/current/userguide/userguide.html#ide-integration">Gradle 的 IDE 集成</a>来从 Gradle 创建项目文件。</p>
<p>请注意：Java 默认的 JVM 堆大小对 Flink 来说可能太小。你必须手动增加它。在 Eclipse 中，选择 Run Configurations -&gt; Arguments，并在 VM Arguments 框中写下 <code>-Xmx800m</code>。在 IntelliJ IDEA 中推荐的改变 JVM 选项的方法是来自 Help | Edit Custom VM Options 菜单。详情请看<a href="https://intellij-support.jetbrains.com/hc/en-us/articles/206544869-Configuring-JVM-options-and-platform-properties">这篇文章</a>。</p>
<h4 id="构建项目-1">构建项目</h4>
<p>如果你想构建/打包你的项目，去你的项目目录下运行 &ldquo;gradle clean shadowJar&rdquo; 命令，你会发现一个 JAR 文件，其中包含了你的应用程序，以及你可能已经添加到应用程序中作为依赖的连接器和库：<code>build/libs/&lt;project-name&gt;-&lt;version&gt;-all.jar</code>。</p>
<p>注意：如果你使用与 StreamingJob 不同的类作为应用程序的主类/入口点，我们建议你相应地更改 build.gradle 文件中的 mainClassName 设置。这样，Flink 就可以从 JAR 文件中运行应用程序，而无需额外指定主类。</p>
<h3 id="sbt">SBT</h3>
<h4 id="创建项目">创建项目</h4>
<p>您可以通过以下两种方法中的任何一种来构建一个新项目。</p>
<ul>
<li>使用 sbt 模板</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ sbt new tillrohrmann/flink-project.g8
</code></pre></div><ul>
<li>运行快速启动脚本</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ bash &lt;<span class="o">(</span>curl https://flink.apache.org/q/sbt-quickstart.sh<span class="o">)</span>
</code></pre></div><p>这将在指定的项目目录下创建一个 Flink 项目。</p>
<h4 id="构建项目-2">构建项目</h4>
<p>为了建立你的项目，你只需要发出 sbt clean assembly 命令。这将在 <code>target/scala_your-major-scala-version/</code> 目录下创建 fat-jar <code>your-project-name-assembly-0.1-SNAPSHOT.jar</code>。</p>
<p><strong>运行项目</strong></p>
<p>为了运行你的项目，你必须发出 sbt 运行命令。</p>
<p>默认情况下，这将在 sbt 运行的同一个 JVM 中运行你的工作。为了在不同的 JVM 中运行你的工作，请在 build.sbt 中添加以下行。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">fork in run :<span class="o">=</span> <span class="nb">true</span>
</code></pre></div><h4 id="intellij">IntelliJ</h4>
<p>我们推荐您使用 <a href="https://www.jetbrains.com/idea/">IntelliJ</a> 进行 Flink 作业开发。为了开始，您必须将新创建的项目导入到 IntelliJ 中。您可以通过 File -&gt; New -&gt; Project from Existing Sources&hellip;然后选择您的项目目录。IntelliJ 会自动检测 build.sbt 文件，并设置好一切。</p>
<p>为了运行 Flink 作业，建议选择 mainRunner 模块作为运行/调试配置的 classpath。这将确保所有被设置为提供的依赖关系在执行时都是可用的。您可以通过 Run -&gt; Edit Configurations&hellip;配置 Run/Debug 配置，然后从 Use classpath of module dropbox 中选择 mainRunner。</p>
<h4 id="eclipse">Eclipse</h4>
<p>为了将新创建的项目导入到 <a href="https://eclipse.org/">Eclipse</a> 中，首先必须为其创建 Eclipse 项目文件。这些项目文件可以通过  <a href="https://github.com/typesafehub/sbteclipse">sbteclipse</a> 插件来创建。在 PROJECT_DIR/project/plugins.sbt 文件中添加以下一行。</p>
<pre><code>addSbtPlugin(&quot;com.typeafe.sbteclipse&quot; % &quot;sbteclipse-plugin&quot; % &quot;4.0.0&quot;)
</code></pre><p>在 sbt 中使用下面的命令来创建 Eclipse 项目文件</p>
<pre><code>&gt; eclipse
</code></pre><p>现在你可以通过 File-&gt;Import&hellip;-&gt;Existing Projects into Workspace 导入 Eclipse，然后选择项目目录。</p>
<h2 id="附录-用依赖关系构建-jar-的模板">附录: 用依赖关系构建 Jar 的模板</h2>
<p>要构建一个包含声明的连接器和库所需的所有依赖关系的应用程序 JAR，可以使用以下 shade 插件定义。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;build&gt;</span>
    <span class="nt">&lt;plugins&gt;</span>
        <span class="nt">&lt;plugin&gt;</span>
            <span class="nt">&lt;groupId&gt;</span>org.apache.maven.plugins<span class="nt">&lt;/groupId&gt;</span>
            <span class="nt">&lt;artifactId&gt;</span>maven-shade-plugin<span class="nt">&lt;/artifactId&gt;</span>
            <span class="nt">&lt;version&gt;</span>3.1.1<span class="nt">&lt;/version&gt;</span>
            <span class="nt">&lt;executions&gt;</span>
                <span class="nt">&lt;execution&gt;</span>
                    <span class="nt">&lt;phase&gt;</span>package<span class="nt">&lt;/phase&gt;</span>
                    <span class="nt">&lt;goals&gt;</span>
                        <span class="nt">&lt;goal&gt;</span>shade<span class="nt">&lt;/goal&gt;</span>
                    <span class="nt">&lt;/goals&gt;</span>
                    <span class="nt">&lt;configuration&gt;</span>
                        <span class="nt">&lt;artifactSet&gt;</span>
                            <span class="nt">&lt;excludes&gt;</span>
                                <span class="nt">&lt;exclude&gt;</span>com.google.code.findbugs:jsr305<span class="nt">&lt;/exclude&gt;</span>
                                <span class="nt">&lt;exclude&gt;</span>org.slf4j:*<span class="nt">&lt;/exclude&gt;</span>
                                <span class="nt">&lt;exclude&gt;</span>log4j:*<span class="nt">&lt;/exclude&gt;</span>
                            <span class="nt">&lt;/excludes&gt;</span>
                        <span class="nt">&lt;/artifactSet&gt;</span>
                        <span class="nt">&lt;filters&gt;</span>
                            <span class="nt">&lt;filter&gt;</span>
                                <span class="c">&lt;!-- Do not copy the signatures in the META-INF folder.
</span><span class="c">                                Otherwise, this might cause SecurityExceptions when using the JAR. --&gt;</span>
                                <span class="nt">&lt;artifact&gt;</span>*:*<span class="nt">&lt;/artifact&gt;</span>
                                <span class="nt">&lt;excludes&gt;</span>
                                    <span class="nt">&lt;exclude&gt;</span>META-INF/*.SF<span class="nt">&lt;/exclude&gt;</span>
                                    <span class="nt">&lt;exclude&gt;</span>META-INF/*.DSA<span class="nt">&lt;/exclude&gt;</span>
                                    <span class="nt">&lt;exclude&gt;</span>META-INF/*.RSA<span class="nt">&lt;/exclude&gt;</span>
                                <span class="nt">&lt;/excludes&gt;</span>
                            <span class="nt">&lt;/filter&gt;</span>
                        <span class="nt">&lt;/filters&gt;</span>
                        <span class="nt">&lt;transformers&gt;</span>
                            <span class="nt">&lt;transformer</span> <span class="na">implementation=</span><span class="s">&#34;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&#34;</span><span class="nt">&gt;</span>
                                <span class="nt">&lt;mainClass&gt;</span>my.programs.main.clazz<span class="nt">&lt;/mainClass&gt;</span>
                            <span class="nt">&lt;/transformer&gt;</span>
                        <span class="nt">&lt;/transformers&gt;</span>
                    <span class="nt">&lt;/configuration&gt;</span>
                <span class="nt">&lt;/execution&gt;</span>
            <span class="nt">&lt;/executions&gt;</span>
        <span class="nt">&lt;/plugin&gt;</span>
    <span class="nt">&lt;/plugins&gt;</span>
<span class="nt">&lt;/build&gt;</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Event Time]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-event-time/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-flink-datastream-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Datastream API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-generating-watermarks/?utm_source=atom_feed" rel="related" type="text/html" title="Generating Watermarks" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/?utm_source=atom_feed" rel="related" type="text/html" title="使用状态" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-built-in-watermark-generators/?utm_source=atom_feed" rel="related" type="text/html" title="内置的水印生成器" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/?utm_source=atom_feed" rel="related" type="text/html" title="状态和容错性" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-event-time/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-21T00:00:00+08:00</published>
            <updated>2020-08-21T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Event Time</blockquote><h2 id="event-timehttpsciapacheorgprojectsflinkflink-docs-release-111devevent_timehtml"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">Event Time</a></h2>
<p>在本节中，您将学习如何编写时间感知(time-aware)的 Flink 程序。请看一下<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/concepts/timely-stream-processing.html">及时流处理</a>，了解及时流处理背后的概念。</p>
<p>关于如何在 Flink 程序中使用时间的信息请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html">windowing</a> 和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/process_function.html">ProcessFunction</a>。</p>
<p>使用事件时间处理的先决条件是设置正确的时间特性(<em>time characteristic</em>)。该设置定义了数据流源的行为（例如，它们是否会分配时间戳），以及像 <code>KeyedStream.timeWindow(Time.seconds(30))</code> 这样的窗口操作应该使用什么时间概念。</p>
<p>你可以使用 <code>StreamExecutionEnvironment.setStreamTimeCharacteristic()</code> 设置时间特性:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="n">env</span><span class="o">.</span><span class="n">setStreamTimeCharacteristic</span><span class="o">(</span><span class="nc">TimeCharacteristic</span><span class="o">.</span><span class="nc">EventTime</span><span class="o">)</span>

<span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">MyEvent</span><span class="o">]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">addSource</span><span class="o">(</span><span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">[</span><span class="kt">MyEvent</span><span class="o">](</span><span class="n">topic</span><span class="o">,</span> <span class="n">schema</span><span class="o">,</span> <span class="n">props</span><span class="o">))</span>

<span class="n">stream</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span> <span class="k">_</span><span class="o">.</span><span class="n">getUser</span> <span class="o">)</span>
    <span class="o">.</span><span class="n">timeWindow</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">reduce</span><span class="o">(</span> <span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">b</span><span class="o">)</span> <span class="o">)</span>
    <span class="o">.</span><span class="n">addSink</span><span class="o">(...)</span>
</code></pre></div><p>需要注意的是，为了在事件时间(<em>event time</em>)中运行这个例子，程序需要使用直接为数据定义事件时间并自己发射水印的源，或者程序必须在源之后注入一个时间戳分配器(<em>Timestamp Assigner</em>)与水印生成器(<em>Watermark Generator</em>)。这些函数描述了如何访问事件时间戳，以及事件流表现出何种程度的无序性。</p>
<h2 id="下一步该怎么走">下一步该怎么走？</h2>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_timestamps_watermarks.html">生成水印</a>。展示了如何编写时间戳分配器和水印生成器，这些都是事件时间(event-time)感知 Flink 应用所需要的。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_timestamp_extractors.html">内置的水印生成器</a>。概述了内置的水印生成器。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/debugging_event_time.html">调试窗口和事件时间</a>：展示如何调试事件时间 Flink 应用程序中围绕水印和时间戳的问题。</li>
</ul>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/programming" term="programming" label="programming" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Flink Datastream API 编程指南]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-flink-datastream-api-programming-guide/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-event-time/?utm_source=atom_feed" rel="related" type="text/html" title="Event Time" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-generating-watermarks/?utm_source=atom_feed" rel="related" type="text/html" title="Generating Watermarks" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/?utm_source=atom_feed" rel="related" type="text/html" title="使用状态" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-built-in-watermark-generators/?utm_source=atom_feed" rel="related" type="text/html" title="内置的水印生成器" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/?utm_source=atom_feed" rel="related" type="text/html" title="状态和容错性" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-flink-datastream-api-programming-guide/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-21T00:00:00+08:00</published>
            <updated>2020-08-21T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Flink Datastream API Programming Guide</blockquote><h2 id="flink-datastream-api-编程指南httpsciapacheorgprojectsflinkflink-docs-release-111devdatastream_apihtml"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">Flink DataStream API 编程指南</a></h2>
<p>Flink 中的 DataStream 程序是对数据流实现转换的常规程序（如过滤、更新状态、定义窗口、聚合）。数据流最初是由各种源（如消息队列、套接字流、文件）创建的。结果通过接收器(sink)返回，例如可以将数据写入文件，或标准输出（例如命令行终端）。Flink 程序可以在各种环境下运行，独立运行，或者嵌入到其他程序中。执行可以发生在本地 JVM 中，也可以发生在许多机器的集群中。</p>
<p>为了创建你自己的 Flink DataStream 程序，我们鼓励你从<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html#anatomy-of-a-flink-program">一个 Flink 程序的骨架</a>开始，并逐步添加你自己的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/index.html">流转换</a>。其余部分作为额外操作和高级功能的参考。</p>
<h2 id="什么是-datastream">什么是 DataStream？</h2>
<p>DataStream API 的名字来自于特殊的 <code>DataStream</code> 类，它用于表示 Flink 程序中的数据集合。你可以把它们看作是不可改变的数据集合，可以包含重复的数据。这些数据既可以是有限的，也可以是无边界的，你用来处理它们的 API 是一样的。</p>
<p>DataStream 在用法上与普通的 Java Collection 类似，但在一些关键方面却有很大不同。它们是不可改变的，这意味着一旦它们被创建，你就不能添加或删除元素。你也不能简单地检查里面的元素，而只能使用 DataStream API 操作对它们进行操作，这也被称为转换。</p>
<p>你可以通过在 Flink 程序中添加一个源来创建一个初始的 DataStream。然后你可以从中派生新的流，并通过使用 API 方法，如 <code>map</code>、<code>filter</code> 等来组合它们。</p>
<h2 id="flink-程序的骨架">Flink 程序的骨架</h2>
<p>Flink 程序看起来就像转换 DataStream 的普通程序。每个程序由相同的基本部分组成。</p>
<ol>
<li>获取一个执行环境</li>
<li>加载/创建初始数据。</li>
<li>指定该数据的转换。</li>
<li>指定计算结果的位置。</li>
<li>触发程序执行</li>
</ol>
<p>现在我们将对其中的每一个步骤进行概述，更多细节请参考相关章节。注意，Scala DataStream API 的所有核心类都可以在 <a href="https://github.com/apache/flink/blob/master//flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala">org.apache.flink.stream.api.scala</a> 中找到。</p>
<p><code>StreamExecutionEnvironment</code> 是所有 Flink 程序的基础。你可以使用 <code>StreamExecutionEnvironment</code> 上的这些静态方法获得一个。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">getExecutionEnvironment</span><span class="o">()</span>

<span class="n">createLocalEnvironment</span><span class="o">()</span>

<span class="n">createRemoteEnvironment</span><span class="o">(</span><span class="n">host</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">port</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">jarFiles</span><span class="k">:</span> <span class="kt">String*</span><span class="o">)</span>
</code></pre></div><p>通常情况下，你只需要使用 <code>getExecutionEnvironment()</code>，因为这将根据上下文做正确的事情：如果你在 IDE 里面执行你的程序，或者作为一个普通的 Java 程序，它将创建一个本地环境，在你的本地机器上执行你的程序。如果你从你的程序中创建了一个 JAR 文件，并通过<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/cli.html">命令行</a>调用它，Flink 集群管理器将执行你的主方法，并且 <code>getExecutionEnvironment()</code> 将返回一个在集群上执行你的程序的执行环境。</p>
<p>对于指定数据源，执行环境有几种方法可以使用不同的方法从文件中读取数据：你可以只是逐行读取，作为 CSV 文件，或者使用任何其他提供的数据源。如果只是将文本文件作为一个行的序列来读取，你可以使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="n">text</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;file:///path/to/file&#34;</span><span class="o">)</span>
</code></pre></div><p>这将为您提供一个 DataStream，然后您可以在其上应用转换来创建新的派生 DataStream。</p>
<p>你可以通过调用 DataStream 上的方法和转换函数来应用转换。例如，一个 <code>map</code> 转换看起来像这样。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">mapped</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">toInt</span> <span class="o">}</span>
</code></pre></div><p>这将通过将原始集合中的每一个字符串转换为一个 Integer 来创建一个新的 DataStream。</p>
<p>一旦你有了一个包含最终结果的 DataStream，你就可以通过创建一个接收器(sink)将其写入外部系统。这些只是创建接收器的一些示例方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">writeAsText</span><span class="o">(</span><span class="n">path</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="n">print</span><span class="o">()</span>
</code></pre></div><p>一旦你指定了完整的程序，你需要通过调用 <code>StreamExecutionEnvironment</code> 上的 <code>execution()</code> 来触发程序的执行。根据 <code>ExecutionEnvironment</code> 的类型，将在你的本地机器上触发执行，或者将你的程序提交到集群上执行。</p>
<p><code>execute()</code> 方法将等待作业完成，然后返回一个 <code>JobExecutionResult</code>，这个包含执行时间和累加器结果。</p>
<p>如果你不想等待作业完成，你可以在 <code>StreamExecutionEnvironment</code> 上调用 <code>executeAysnc()</code> 来触发异步作业执行。它将返回一个 <code>JobClient</code>，你可以用它与刚刚提交的作业进行通信。例如，下面是如何通过使用 <code>executeAsync()</code> 来实现 <code>execute()</code> 的语义。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">final</span> <span class="n">JobClient</span> <span class="n">jobClient</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">executeAsync</span><span class="o">();</span>

<span class="kd">final</span> <span class="n">JobExecutionResult</span> <span class="n">jobExecutionResult</span> <span class="o">=</span> <span class="n">jobClient</span><span class="o">.</span><span class="na">getJobExecutionResult</span><span class="o">(</span><span class="n">userClassloader</span><span class="o">).</span><span class="na">get</span><span class="o">();</span>
</code></pre></div><p>最后这部分关于程序执行的内容对于理解 Flink 操作何时以及如何执行至关重要。所有的 Flink 程序都是懒惰地执行的。当程序的主方法被执行时，数据加载和转换不会直接发生。相反，每个操作都被创建并添加到一个数据流图(dataflow graph)中。当执行环境上的 <code>execute()</code> 调用明确触发执行时，这些操作才会被实际执行。程序是在本地执行还是在集群上执行，取决于执行环境的类型</p>
<p>惰性求值可以让您构建复杂的程序，Flink 作为一个整体规划的单元来执行。</p>
<h2 id="示例程序">示例程序</h2>
<p>下面的程序是一个完整的，工作的流媒体窗口单词计数应用程序的例子，它可以在5秒的窗口中计算来自 Web Socket 的单词。你可以复制和粘贴代码在本地运行它。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.time.Time</span>

<span class="k">object</span> <span class="nc">WindowWordCount</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
    <span class="k">val</span> <span class="n">text</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">socketTextStream</span><span class="o">(</span><span class="s">&#34;localhost&#34;</span><span class="o">,</span> <span class="mi">9999</span><span class="o">)</span>

    <span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">text</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">toLowerCase</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34;\\W+&#34;</span><span class="o">)</span> <span class="n">filter</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">nonEmpty</span> <span class="o">}</span> <span class="o">}</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">}</span>
      <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
      <span class="o">.</span><span class="n">timeWindow</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">))</span>
      <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

    <span class="n">counts</span><span class="o">.</span><span class="n">print</span><span class="o">()</span>

    <span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">(</span><span class="s">&#34;Window Stream WordCount&#34;</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>To run the example program, start the input stream with netcat first from a terminal:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">nc -lk <span class="m">9999</span>
</code></pre></div><p>只需输入一些单词，按回车键输入一个新单词。这些词将被输入到单词计数程序中。如果你想看到大于1的计数，请在5秒内反复输入同一个单词（如果你打字没那么快，请从5秒开始增加窗口大小☺）。</p>
<h2 id="数据源">数据源</h2>
<p>源是你的程序读取其输入的地方。你可以通过使用 <code>StreamExecutionEnvironment.addSource(sourceFunction)</code> 将一个源附加到你的程序中。Flink 提供了许多预先实现的 <code>SourceFunction</code>，但是你可以通过实现非并行源的 <code>SourceFunction</code>，或者实现并行源的 <code>ParallelSourceFunction</code> 接口或扩展 <code>RichParallelSourceFunction</code> 来编写自己的自定义源。</p>
<p>有几种预定义的流源(stream sources)可以从 <code>StreamExecutionEnvironment</code> 中访问。</p>
<p>基于文件的。</p>
<ul>
<li>
<p><code>readTextFile(path)</code> - 逐行读取文本文件，即遵循 <code>TextInputFormat</code> 规范的文件，并将其作为字符串返回。</p>
</li>
<li>
<p><code>readFile(fileInputFormat, path)</code> - 根据指定的文件输入格式读取（一次）文件。</p>
</li>
<li>
<p><code>readFile(fileInputFormat, path, watchType, interval, pathFilter)</code> - 这是前面两个方法内部调用的方法。它根据给定的 <code>fileInputFormat</code> 读取路径中的文件。根据所提供的 <code>watchType</code>，这个源可能会周期性地监视(每隔 interval 毫秒)路径中的新数据(<code>FileProcessingMode.PROCESS_CONTINUOUSLY</code>)，或者处理一次当前路径中的数据并退出(<code>FileProcessingMode.PROCESS_ONCE</code>)。使用 <code>pathFilter</code>，用户可以进一步排除被处理的文件。</p>
</li>
</ul>
<p><em>实现</em>:</p>
<p>在底层下，Flink 将文件读取过程分成两个子任务(sub-tasks)，即目录监控和数据读取。这些子任务中的每一个都是由一个单独的实体实现的。监控由一个单一的、非并行（并行度=1）的任务实现，而读取则由多个任务(task)并行运行。后者的并行度等于作业的并行度(job parallelism)。单个监控任务的作用是扫描目录（根据 <code>watchType</code> 的不同，定期或只扫描一次），找到要处理的文件，将其分割，并将这些分割的文件分配给下游的读取器。读取器是那些将读取实际数据的东西。每个分片只能由一个读取器读取，而一个读取器可以读取多个分片，一个接一个。</p>
<p><em>重要提示</em>:</p>
<ol>
<li>
<p>如果 <code>watchType</code> 被设置为 <code>FileProcessingMode.PROCESS_CONTINUOUSLY</code>，当一个文件被修改时，它的内容会被完全重新处理。这可能会打破&quot;精确地一次&quot;(exactly-once)的语义，因为在文件末尾追加数据会导致其所有内容被重新处理。</p>
</li>
<li>
<p>如果 <code>watchType</code> 被设置为 <code>FileProcessingMode.PROCESS_ONCE</code>，那么源就会对路径扫描一次并退出，而不会等待读取器完成对文件内容的读取。当然，读取器会继续读取，直到读取完所有文件内容。关闭源会导致在这之后不再有检查点。这可能会导致节点故障后的恢复速度变慢，因为作业(job)将从最后一个检查点开始恢复读取。</p>
</li>
</ol>
<p>基于 Socket 的:</p>
<ul>
<li><code>socketTextStream</code> - 从套接字读取。元素可以用定界符分开。</li>
</ul>
<p>基于集合的:</p>
<ul>
<li>
<p><code>fromCollection(Seq)</code> - 从 <code>Java Java.util.Collection</code> 中创建数据流。集合中的所有元素必须是相同的类型。</p>
</li>
<li>
<p><code>fromCollection(Iterator)</code> - 从迭代器中创建一个数据流。该类指定迭代器返回的元素的数据类型。</p>
</li>
<li>
<p><code>fromElements(elements: _*)</code> - 从给定的对象序列中创建一个数据流。所有对象必须是相同的类型。</p>
</li>
<li>
<p><code>fromParallelCollection(SplittableIterator)</code> - 从迭代器中并行创建数据流。该类指定了迭代器返回的元素的数据类型。</p>
</li>
<li>
<p><code>generateSequence(from, to)</code> - 在给定的区间内并行生成数字序列。</p>
</li>
</ul>
<p>自定义的:</p>
<ul>
<li><code>addSource</code> - 附加一个新的源函数。例如，要从 Apache Kafka 读取数据，你可以使用 <code>addSource(new FlinkKafkaConsumer010&lt;&gt;(...))</code>。更多细节请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/">连接器</a>。</li>
</ul>
<h2 id="数据流转换">数据流转换</h2>
<p>请参阅 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/index.html">operators</a> 以了解可用的流转换的概述。</p>
<h2 id="数据接收器">数据接收器</h2>
<p>数据接收器消耗 DataStream，并将其转发到文件、套接字、外部系统或打印。Flink 带有各种内置的输出格式，这些格式被封装在 DataStream 的操作后面。</p>
<ul>
<li>
<p><code>writeAsText()</code> / <code>TextOutputFormat</code> - 将元素逐行写入字符串。这些字符串是通过调用每个元素的 <code>toString()</code> 方法获得的。</p>
</li>
<li>
<p><code>writeAsCsv(...)</code> / <code>CsvOutputFormat</code> - 将元组写成逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的 <code>toString()</code> 方法。</p>
</li>
<li>
<p><code>print()</code> / <code>printToErr()</code> - 将每个元素的 <code>toString()</code> 值打印在标准输出/标准错误流上。可以选择提供一个前缀(msg)，这个前缀被添加到输出中。这可以帮助区分不同的 <code>print</code> 调用。如果并行度大于1，输出也将被预置为产生输出的任务(task)的标识符。</p>
</li>
<li>
<p><code>writeUsingOutputFormat()</code> / <code>FileOutputFormat</code> - 用于自定义文件输出的方法和基类。支持自定义对象到字节的转换。</p>
</li>
<li>
<p><code>writeToSocket</code> - 根据 <code>SerializationSchema</code> 将元素写入 socket。</p>
</li>
<li>
<p>addSink - 调用一个自定义的 sink 函数。Flink 捆绑了连接其他系统（如 Apache Kafka）的连接器，这些连接器被实现为 sink 函数。</p>
</li>
</ul>
<p>请注意，DataStream 上的 <code>write*()</code> 方法主要是为了调试的目的。它们不参与 Flink 的检查点，这意味着这些函数通常具有最多一次(at-least-once)的语义。数据冲洗到目标系统取决于 <code>OutputFormat</code> 的实现。这意味着并非所有发送到 <code>OutputFormat</code> 的元素都会立即在目标系统中显示出来。另外，在失败的情况下，这些记录可能会丢失。</p>
<p>为了可靠地、精确地一次性将流传送到文件系统中，请使用 <code>flink-connector-filesystem</code>。此外，通过 <code>.addSink(...)</code> 方法的自定义实现可以参与 Flink 的检查点，以实现精确的一次语义。</p>
<h2 id="迭代">迭代</h2>
<p>迭代流程序实现了一个步骤函数，并将其嵌入到 <code>IterativeStream</code> 中。由于 DataStream 程序可能永远不会结束，所以没有最大的迭代次数。相反，你需要指定流的哪一部分被馈入到迭代中，哪一部分使用 <code>split</code> 转换或 <code>filter</code> 转发到下游。在这里，我们展示了一个迭代的例子，其中主体（重复计算的部分）是一个简单的 <code>map</code> 转换，而反馈回来的元素是通过使用 <code>filter</code> 转发到下游的元素来区分的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">iteratedStream</span> <span class="k">=</span> <span class="n">someDataStream</span><span class="o">.</span><span class="n">iterate</span><span class="o">(</span>
  <span class="n">iteration</span> <span class="k">=&gt;</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">iterationBody</span> <span class="k">=</span> <span class="n">iteration</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="cm">/* this is executed many times */</span><span class="o">)</span>
    <span class="o">(</span><span class="n">iterationBody</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="cm">/* one part of the stream */</span><span class="o">),</span> <span class="n">iterationBody</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="cm">/* some other part of the stream */</span><span class="o">))</span>
<span class="o">})</span>
</code></pre></div><p>例如，这里的程序是从一系列整数中连续减去1，直到它们达到零。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">someIntegers</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">generateSequence</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1000</span><span class="o">)</span>

<span class="k">val</span> <span class="n">iteratedStream</span> <span class="k">=</span> <span class="n">someIntegers</span><span class="o">.</span><span class="n">iterate</span><span class="o">(</span>
  <span class="n">iteration</span> <span class="k">=&gt;</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">minusOne</span> <span class="k">=</span> <span class="n">iteration</span><span class="o">.</span><span class="n">map</span><span class="o">(</span> <span class="n">v</span> <span class="k">=&gt;</span> <span class="n">v</span> <span class="o">-</span> <span class="mi">1</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">stillGreaterThanZero</span> <span class="k">=</span> <span class="n">minusOne</span><span class="o">.</span><span class="n">filter</span> <span class="o">(</span><span class="k">_</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">lessThanZero</span> <span class="k">=</span> <span class="n">minusOne</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="o">)</span>
    <span class="o">(</span><span class="n">stillGreaterThanZero</span><span class="o">,</span> <span class="n">lessThanZero</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">)</span>
</code></pre></div><h2 id="执行参数">执行参数</h2>
<p><code>StreamExecutionEnvironment</code> 包含了 <code>ExecutionConfig</code>，它允许为运行时设置作业特定(job specific)的配置值。</p>
<p>请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/execution_configuration.html">执行配置</a>，了解大多数参数的解释。这些参数专门与 DataStream API 有关。</p>
<ul>
<li><code>setAutoWatermarkInterval(long milliseconds)</code>: 设置自动发射水印的时间间隔。你可以通过 <code>long getAutoWatermarkInterval()</code> 来获取当前值。</li>
</ul>
<h3 id="容错">容错</h3>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/checkpointing.html">状态和检查点</a>介绍了如何启用和配置 Flink 的检查点机制。</p>
<h3 id="控制延迟">控制延迟</h3>
<p>默认情况下，元素不会在网络上逐一传输（会造成不必要的网络流量），而是被缓冲。缓冲区（实际在机器之间传输）的大小可以在 Flink 配置文件中设置。虽然这种方法有利于优化吞吐量，但当传入的数据流速度不够快时，会造成延迟问题。为了控制吞吐量和延迟，你可以在执行环境上（或者在单个 operator 上）使用 <code>env.setBufferTimeout(timeoutMillis)</code> 来设置缓冲区填满的最大等待时间。过了这个时间，即使缓冲区没有满，也会自动发送。该超时的默认值为 100 ms。</p>
<p>使用方法:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span><span class="k">:</span> <span class="kt">LocalStreamEnvironment</span> <span class="o">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">createLocalEnvironment</span>
<span class="n">env</span><span class="o">.</span><span class="n">setBufferTimeout</span><span class="o">(</span><span class="n">timeoutMillis</span><span class="o">)</span>

<span class="n">env</span><span class="o">.</span><span class="n">generateSequence</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">10</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">myMap</span><span class="o">).</span><span class="n">setBufferTimeout</span><span class="o">(</span><span class="n">timeoutMillis</span><span class="o">)</span>
</code></pre></div><p>为了最大限度地提高吞吐量，设置 <code>setBufferTimeout(-1)</code>，这将消除超时，缓冲区只有在满时才会被刷新。为了最大限度地减少延迟，将超时设置为接近0的值（例如5或10毫秒）。应该避免缓冲区超时为0，因为它会导致严重的性能下降。</p>
<h2 id="调试">调试</h2>
<p>在分布式集群中运行一个流程序之前，最好先确保实现的算法能够按照预期的方式运行。因此，实现数据分析程序通常是一个检查结果、调试和改进的渐进过程。</p>
<p>Flink 提供了一些功能，通过支持 IDE 内的本地调试、测试数据的注入和结果数据的收集，大大简化了数据分析程序的开发过程。本节给出一些提示，如何简化 Flink 程序的开发。</p>
<h3 id="本地执行环境">本地执行环境</h3>
<p><code>LocalStreamEnvironment</code> 在它创建的同一个 JVM 进程中启动 Flink 系统。如果你从 IDE 中启动 <code>LocalEnvironment</code>，你可以在代码中设置断点，轻松调试你的程序。</p>
<p><code>LocalEnvironment</code> 的创建和使用方法如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">createLocalEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">addSource</span><span class="o">(</span><span class="cm">/* some source */</span><span class="o">)</span>
<span class="c1">// build your program
</span><span class="c1"></span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><h3 id="收集数据源">收集数据源</h3>
<p>Flink 提供了特殊的数据源，这些数据源由 Java 集合支持，以方便测试。一旦程序被测试，源和接收器就可以很容易地被从外部系统读取/写入的源和接收器所替代。</p>
<p>集合数据源的使用方法如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">createLocalEnvironment</span><span class="o">()</span>

<span class="c1">// 从元素列表中创建一个 DataStream
</span><span class="c1"></span><span class="k">val</span> <span class="n">myInts</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">)</span>

<span class="c1">// 从任何集合中创建一个 DataStream
</span><span class="c1"></span><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">myTuples</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="c1">// 从迭代器中创建一个 DataStream
</span><span class="c1"></span><span class="k">val</span> <span class="n">longIt</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">myLongs</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="n">longIt</span><span class="o">)</span>
</code></pre></div><p>注：目前，集合数据源要求数据类型和迭代器实现 <code>Serializable</code>。此外，集合数据源不能并行执行( parallelism = 1)。</p>
<h3 id="迭代器数据接收器">迭代器数据接收器</h3>
<p>Flink 还提供了一个收集 DataStream 结果的接收器(sink)，用于测试和调试目的。它的使用方法如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.experimental.DataStreamUtils</span>
<span class="k">import</span> <span class="nn">scala.collection.JavaConverters.asScalaIteratorConverter</span>

<span class="k">val</span> <span class="n">myResult</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">myOutput</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="nc">DataStreamUtils</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">myResult</span><span class="o">.</span><span class="n">javaStream</span><span class="o">).</span><span class="n">asScala</span>
</code></pre></div><p>注意：<code>flink-streaming-contrib</code> 模块从 Flink 1.5.0 中移除。它的类被移到 <code>flink-streaming-java</code> 和 <code>flink-streaming-scala</code> 中。</p>
<h2 id="下一步怎么走">下一步怎么走？</h2>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/index.html">运算符</a>: 规范可用的流式运算符。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间</a>: 介绍 Flink 的时间概念。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/index.html">状态和容错</a>: 解释如何开发有状态的应用。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/index.html">连接器</a>: 描述可用的输入和输出连接器。</li>
</ul>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/programming" term="programming" label="programming" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Generating Watermarks]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-generating-watermarks/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-event-time/?utm_source=atom_feed" rel="related" type="text/html" title="Event Time" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-flink-datastream-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Datastream API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/?utm_source=atom_feed" rel="related" type="text/html" title="使用状态" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-built-in-watermark-generators/?utm_source=atom_feed" rel="related" type="text/html" title="内置的水印生成器" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/?utm_source=atom_feed" rel="related" type="text/html" title="状态和容错性" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-generating-watermarks/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-21T00:00:00+08:00</published>
            <updated>2020-08-21T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Generating Watermarks</blockquote><h2 id="生成水印httpsciapacheorgprojectsflinkflink-docs-release-111devevent_timestamps_watermarkshtml"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_timestamps_watermarks.html">生成水印</a></h2>
<p>在本节中，您将了解 Flink 提供的 API，用于处理事件时间时间戳和水印。关于事件时间、处理时间和摄取时间的介绍，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间的介绍</a>。</p>
<h3 id="水印策略介绍">水印策略介绍</h3>
<p>为了使用事件时间，Flink 需要知道事件的时间戳，这意味着流中的每个元素都需要分配其事件时间戳(event timestamp)。这通常是通过使用 <code>TimestampAssigner</code> 从元素中的某个字段访问/提取时间戳(timestamp)来完成的。</p>
<p>时间戳分配与生成水印是同步进行的，水印告诉系统事件时间的进展。你可以通过指定一个 <code>WatermarkGenerator</code> 来配置。</p>
<p>Flink API 期望一个 <code>WatermarkStrategy</code>，其中包含一个 <code>TimestampAssigner</code> 和 <code>WatermarkGenerator</code>。一些常见的策略作为 <code>WatermarkStrategy</code> 上的静态方法是开箱即用的，但用户也可以在需要时建立自己的策略。</p>
<p>为了完整起见，这里是接口:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">WatermarkStrategy</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="kd">extends</span> <span class="n">TimestampAssignerSupplier</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;,</span> <span class="n">WatermarkGeneratorSupplier</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;{</span>

    <span class="cm">/**
</span><span class="cm">     * Instantiates a {@link TimestampAssigner} for assigning timestamps according to this
</span><span class="cm">     * strategy.
</span><span class="cm">     */</span>
    <span class="nd">@Override</span>
    <span class="n">TimestampAssigner</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="nf">createTimestampAssigner</span><span class="o">(</span><span class="n">TimestampAssignerSupplier</span><span class="o">.</span><span class="na">Context</span> <span class="n">context</span><span class="o">);</span>

    <span class="cm">/**
</span><span class="cm">     * Instantiates a WatermarkGenerator that generates watermarks according to this strategy.
</span><span class="cm">     */</span>
    <span class="nd">@Override</span>
    <span class="n">WatermarkGenerator</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="nf">createWatermarkGenerator</span><span class="o">(</span><span class="n">WatermarkGeneratorSupplier</span><span class="o">.</span><span class="na">Context</span> <span class="n">context</span><span class="o">);</span>
<span class="o">}</span>
</code></pre></div><p>如前所述，你通常不会自己实现这个接口，而是使用 <code>WatermarkStrategy</code> 上的静态帮助方法来实现常见的水印策略，或者将自定义的 <code>TimestampAssigner</code> 与 <code>WatermarkGenerator</code> 捆绑在一起。例如，要使用有界无序水印和 lambda 函数作为时间戳分配器，你可以使用这个方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">WatermarkStrategy</span>
  <span class="o">.</span><span class="n">forBoundedOutOfOrderness</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span><span class="o">)](</span><span class="nc">Duration</span><span class="o">.</span><span class="n">ofSeconds</span><span class="o">(</span><span class="mi">20</span><span class="o">))</span>
  <span class="o">.</span><span class="n">withTimestampAssigner</span><span class="o">(</span><span class="k">new</span> <span class="nc">SerializableTimestampAssigner</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span><span class="o">)]</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">extractTimestamp</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">String</span><span class="o">),</span> <span class="n">recordTimestamp</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="n">element</span><span class="o">.</span><span class="n">_1</span>
  <span class="o">})</span>
</code></pre></div><p>(在这里使用 Scala Lambdas 目前是行不通的，因为 Scala 很笨，很难支持这个。#fus)</p>
<p>指定一个 <code>TimestampAssigner</code> 是可选的，在大多数情况下，你其实并不想指定一个。例如，当使用 Kafka 或 Kinesis 时，你会直接从 Kafka/Kinesis 记录中获取时间戳。</p>
<p>我们将在后面的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_timestamps_watermarks.html#writing-watermarkgenerators">Writing WatermarkGenerator</a>中查看 <code>WatermarkGenerator</code> 接口。</p>
<p>注意：时间戳和水印都被指定为自 1970-01-01T00:00:00Z 的 Java 纪元以来的毫秒。</p>
<h3 id="使用水印策略">使用水印策略</h3>
<p>在 Flink 应用中，有两个地方可以使用 <code>WatermarkStrategy</code>。1）直接在源上使用，2）在非源操作后使用。</p>
<p>第一个选项是比较好的，因为它允许源在水印逻辑中利用关于碎片/分区/分割的知识。源通常可以更精细地跟踪水印，源产生的整体水印也会更准确。直接在源上指定 <code>WatermarkStrategy</code> 通常意味着你必须使用源的特定接口/请参阅 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_timestamps_watermarks.html#watermark-strategies-and-the-kafka-connector">Watermark Strategies 和 Kafka Connector</a>，以了解在 Kafka Connector 上如何工作，以及关于每个分区水印如何工作的更多细节。</p>
<p>第二个选项（在任意操作后设置 <code>WatermarkStrategy</code>）只应在不能直接在源上设置策略时使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="n">env</span><span class="o">.</span><span class="n">setStreamTimeCharacteristic</span><span class="o">(</span><span class="nc">TimeCharacteristic</span><span class="o">.</span><span class="nc">EventTime</span><span class="o">)</span>

<span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">MyEvent</span><span class="o">]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readFile</span><span class="o">(</span>
         <span class="n">myFormat</span><span class="o">,</span> <span class="n">myFilePath</span><span class="o">,</span> <span class="nc">FileProcessingMode</span><span class="o">.</span><span class="nc">PROCESS_CONTINUOUSLY</span><span class="o">,</span> <span class="mi">100</span><span class="o">,</span>
         <span class="nc">FilePathFilter</span><span class="o">.</span><span class="n">createDefaultFilter</span><span class="o">())</span>

<span class="k">val</span> <span class="n">withTimestampsAndWatermarks</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">MyEvent</span><span class="o">]</span> <span class="k">=</span> <span class="n">stream</span>
        <span class="o">.</span><span class="n">filter</span><span class="o">(</span> <span class="k">_</span><span class="o">.</span><span class="n">severity</span> <span class="o">==</span> <span class="nc">WARNING</span> <span class="o">)</span>
        <span class="o">.</span><span class="n">assignTimestampsAndWatermarks</span><span class="o">(&lt;</span><span class="n">watermark</span> <span class="n">strategy</span><span class="o">&gt;)</span>

<span class="n">withTimestampsAndWatermarks</span>
        <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span> <span class="k">_</span><span class="o">.</span><span class="n">getGroup</span> <span class="o">)</span>
        <span class="o">.</span><span class="n">timeWindow</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">))</span>
        <span class="o">.</span><span class="n">reduce</span><span class="o">(</span> <span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">b</span><span class="o">)</span> <span class="o">)</span>
        <span class="o">.</span><span class="n">addSink</span><span class="o">(...)</span>
</code></pre></div><p>以这种方式使用 <code>WatermarkStrategy</code>，可以获取一个流并生成一个带有时间戳元素和水印的新流。如果原始流已经有时间戳和/或水印了，时间戳分配器就会覆盖它们。</p>
<h3 id="处理闲置源">处理闲置源</h3>
<p>如果其中一个输入分割/分区/碎片在一段时间内没有携带事件，这意味着 <code>WatermarkGenerator</code> 也没有得到任何新的信息来作为水印的基础。我们称之为空闲输入或空闲源。这是一个问题，因为有可能发生你的一些分区仍然携带事件。在这种情况下，水印将被保留下来，因为它是作为所有不同的并行水印的最小值计算的。</p>
<p>为了处理这个问题，你可以使用 <code>WatermarkStrategy</code> 来检测空闲，并将一个输入标记为空闲。<code>WatermarkStrategy</code> 为此提供了一个方便的助手。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">WatermarkStrategy</span>
  <span class="o">.</span><span class="n">forBoundedOutOfOrderness</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span><span class="o">)](</span><span class="nc">Duration</span><span class="o">.</span><span class="n">ofSeconds</span><span class="o">(</span><span class="mi">20</span><span class="o">))</span>
  <span class="o">.</span><span class="n">withIdleness</span><span class="o">(</span><span class="nc">Duration</span><span class="o">.</span><span class="n">ofMinutes</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
</code></pre></div><h3 id="编写水印生成器">编写水印生成器</h3>
<p>时间戳分配器(TimestampAssigner)是一个从事件中提取字段的简单函数，因此我们不需要详细研究它们。而 <code>WatermarkGenerator</code> 的编写就比较复杂了，我们将在接下来的两节中看如何做。这就是 <code>WatermarkGenerator</code> 的接口。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="cm">/**
</span><span class="cm"> * The {@code WatermarkGenerator} generates watermarks either based on events or
</span><span class="cm"> * periodically (in a fixed interval).
</span><span class="cm"> *
</span><span class="cm"> * &lt;p&gt;&lt;b&gt;Note:&lt;/b&gt; This WatermarkGenerator subsumes the previous distinction between the
</span><span class="cm"> * {@code AssignerWithPunctuatedWatermarks} and the {@code AssignerWithPeriodicWatermarks}.
</span><span class="cm"> */</span>
<span class="nd">@Public</span>
<span class="kd">public</span> <span class="kd">interface</span> <span class="nc">WatermarkGenerator</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="o">{</span>

    <span class="cm">/**
</span><span class="cm">     * Called for every event, allows the watermark generator to examine and remember the
</span><span class="cm">     * event timestamps, or to emit a watermark based on the event itself.
</span><span class="cm">     */</span>
    <span class="kt">void</span> <span class="nf">onEvent</span><span class="o">(</span><span class="n">T</span> <span class="n">event</span><span class="o">,</span> <span class="kt">long</span> <span class="n">eventTimestamp</span><span class="o">,</span> <span class="n">WatermarkOutput</span> <span class="n">output</span><span class="o">);</span>

    <span class="cm">/**
</span><span class="cm">     * Called periodically, and might emit a new watermark, or not.
</span><span class="cm">     *
</span><span class="cm">     * &lt;p&gt;The interval in which this method is called and Watermarks are generated
</span><span class="cm">     * depends on {@link ExecutionConfig#getAutoWatermarkInterval()}.
</span><span class="cm">     */</span>
    <span class="kt">void</span> <span class="nf">onPeriodicEmit</span><span class="o">(</span><span class="n">WatermarkOutput</span> <span class="n">output</span><span class="o">);</span>
<span class="o">}</span>
</code></pre></div><p>有两种不同风格的水印生成器：周期性和打点式。</p>
<p>周期性生成器通常通过 <code>onEvent()</code> 观察到传入的事件，然后当框架调用 <code>onPeriodicEmit()</code> 时，发射水印。</p>
<p>标点式生成器会观察 <code>onEvent()</code> 中的事件，并等待流中携带水印信息的特殊标记事件或标点。当它看到这些事件之一时，就会立即发出一个水印。通常，标点生成器不会从 <code>onPeriodicEmit()</code> 发出水印。</p>
<p>接下来我们将看看如何实现每种样式的生成器。</p>
<h4 id="编写周期性水印生成器">编写周期性水印生成器</h4>
<p>周期性生成器观察流事件并周期性地生成水印（可能取决于流元素，或者纯粹基于处理时间）。</p>
<p>生成水印的间隔（每n毫秒）通过 <code>ExecutionConfig.setAutoWatermarkInterval(...)</code> 来定义。每次都会调用生成器的 <code>onPeriodicEmit()</code> 方法，如果返回的水印是非空的，并且大于前一个水印，就会发出一个新的水印。</p>
<p>这里我们展示了两个使用周期性水印生成器的简单例子。请注意，Flink 提供了 <code>BoundedOutfOrdernessWatermarks</code>，这是一个 <code>WatermarkGenerator</code>，它的工作原理与下面所示的 <code>BoundedOutfOrdernessGenerator</code> 类似。你可以在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_timestamp_extractors.html#assigners-allowing-a-fixed-amount-of-lateness">这里</a>阅读关于如何使用它。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="cm">/**
</span><span class="cm"> * This generator generates watermarks assuming that elements arrive out of order,
</span><span class="cm"> * but only to a certain degree. The latest elements for a certain timestamp t will arrive
</span><span class="cm"> * at most n milliseconds after the earliest elements for timestamp t.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">BoundedOutOfOrdernessGenerator</span> <span class="k">extends</span> <span class="nc">AssignerWithPeriodicWatermarks</span><span class="o">[</span><span class="kt">MyEvent</span><span class="o">]</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">maxOutOfOrderness</span> <span class="k">=</span> <span class="mi">3500L</span> <span class="c1">// 3.5 seconds
</span><span class="c1"></span>
    <span class="k">var</span> <span class="n">currentMaxTimestamp</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="k">_</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">onEvent</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">MyEvent</span><span class="o">,</span> <span class="n">eventTimestamp</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">currentMaxTimestamp</span> <span class="k">=</span> <span class="n">max</span><span class="o">(</span><span class="n">eventTimestamp</span><span class="o">,</span> <span class="n">currentMaxTimestamp</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">onPeriodicEmit</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
        <span class="c1">// emit the watermark as current highest timestamp minus the out-of-orderness bound
</span><span class="c1"></span>        <span class="n">output</span><span class="o">.</span><span class="n">emitWatermark</span><span class="o">(</span><span class="k">new</span> <span class="nc">Watermark</span><span class="o">(</span><span class="n">currentMaxTimestamp</span> <span class="o">-</span> <span class="n">maxOutOfOrderness</span> <span class="o">-</span> <span class="mi">1</span><span class="o">));</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="cm">/**
</span><span class="cm"> * This generator generates watermarks that are lagging behind processing time by a fixed amount.
</span><span class="cm"> * It assumes that elements arrive in Flink after a bounded delay.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">TimeLagWatermarkGenerator</span> <span class="k">extends</span> <span class="nc">AssignerWithPeriodicWatermarks</span><span class="o">[</span><span class="kt">MyEvent</span><span class="o">]</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">maxTimeLag</span> <span class="k">=</span> <span class="mi">5000L</span> <span class="c1">// 5 seconds
</span><span class="c1"></span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">onEvent</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">MyEvent</span><span class="o">,</span> <span class="n">eventTimestamp</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
        <span class="c1">// don&#39;t need to do anything because we work on processing time
</span><span class="c1"></span>    <span class="o">}</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">onPeriodicEmit</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">output</span><span class="o">.</span><span class="n">emitWatermark</span><span class="o">(</span><span class="k">new</span> <span class="nc">Watermark</span><span class="o">(</span><span class="nc">System</span><span class="o">.</span><span class="n">currentTimeMillis</span><span class="o">()</span> <span class="o">-</span> <span class="n">maxTimeLag</span><span class="o">));</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h4 id="编写一个标点水印生成器">编写一个标点水印生成器</h4>
<p>标点水印生成器将观察事件流，每当它看到一个携带水印信息的特殊元素时，就会发出一个水印。</p>
<p>这就是如何实现一个标点水印生成器，每当一个事件表明它携带某个标记时，它就会发射一个水印。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">PunctuatedAssigner</span> <span class="k">extends</span> <span class="nc">AssignerWithPunctuatedWatermarks</span><span class="o">[</span><span class="kt">MyEvent</span><span class="o">]</span> <span class="o">{</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">onEvent</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">MyEvent</span><span class="o">,</span> <span class="n">eventTimestamp</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">event</span><span class="o">.</span><span class="n">hasWatermarkMarker</span><span class="o">())</span> <span class="o">{</span>
            <span class="n">output</span><span class="o">.</span><span class="n">emitWatermark</span><span class="o">(</span><span class="k">new</span> <span class="nc">Watermark</span><span class="o">(</span><span class="n">event</span><span class="o">.</span><span class="n">getWatermarkTimestamp</span><span class="o">()))</span>
        <span class="o">}</span>
    <span class="o">}</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">onPeriodicEmit</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
        <span class="c1">// don&#39;t need to do anything because we emit in reaction to events above
</span><span class="c1"></span>    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>注：可以对每个事件生成一个水印。然而，由于每个水印都会引起下游的一些计算，因此过多的水印会降低性能。</p>
<h3 id="水印策略和-kafka-连接器">水印策略和 Kafka 连接器</h3>
<p>当使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/kafka.html">Apache Kafka</a> 作为数据源时，每个 Kafka 分区可能有一个简单的事件时间模式（升序时间戳或有界失序）。然而，当消耗来自 Kafka 的流时，多个分区经常会被并行消耗，交织来自分区的事件，并破坏每个分区的模式（这是 Kafka 的消费者客户端的固有工作方式）。</p>
<p>在这种情况下，你可以使用 Flink 的 Kafka-partition-aware 水印生成功能。使用该功能，在 Kafka 消费者内部，按 Kafka 分区生成水印，每个分区水印的合并方式与流洗牌的水印合并方式相同。</p>
<p>例如，如果每个 Kafka 分区的事件时间戳是严格的升序，那么用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_timestamp_extractors.html#assigners-with-ascending-timestamps">升序时间戳水印生成器</a>生成每个分区的水印，会得到完美的整体水印。请注意，我们在示例中并没有提供 TimestampAssigner，而是使用 Kafka 记录本身的时间戳。</p>
<p>下面的插图展示了如何使用 per-Kafka-partition 水印生成器，以及在这种情况下水印如何通过流式数据流传播。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">kafkaSource</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">[</span><span class="kt">MyType</span><span class="o">](</span><span class="s">&#34;myTopic&#34;</span><span class="o">,</span> <span class="n">schema</span><span class="o">,</span> <span class="n">props</span><span class="o">)</span>
<span class="n">kafkaSource</span><span class="o">.</span><span class="n">assignTimestampsAndWatermarks</span><span class="o">(</span>
  <span class="nc">WatermarkStrategy</span>
    <span class="o">.</span><span class="n">forBoundedOutOfOrderness</span><span class="o">(</span><span class="nc">Duration</span><span class="o">.</span><span class="n">ofSeconds</span><span class="o">(</span><span class="mi">20</span><span class="o">)))</span>

<span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">MyType</span><span class="o">]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">addSource</span><span class="o">(</span><span class="n">kafkaSource</span><span class="o">)</span>
</code></pre></div><p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/parallel_kafka_watermarks.svg" alt="img"></p>
<h3 id="运算符如何处理水印">运算符如何处理水印</h3>
<p>作为一般规则，运算符(operator)在向下游转发一个给定的水印之前，需要对其进行完全处理。例如，<code>WindowOperator</code> 将首先评估所有应该被发射的窗口，只有在产生所有由水印触发的输出之后，水印本身才会被发送到下游。换句话说，所有因发生水印而产生的元素将在水印之前被发射。</p>
<p>同样的规则也适用于 <code>TwoInputStreamOperator</code>。然而，在这种情况下，运算符的当前水印被定义为其两个输入的最小值。</p>
<p>这种行为的细节由 <code>OneInputStreamOperator#processWatermark</code>、<code>TwoInputStreamOperator#processWatermark1</code> 和 <code>TwoInputStreamOperator#processWatermark2</code> 方法的实现来定义。</p>
<h3 id="废弃的-assignerwithperiodicwatermarks-和-assignerwithpunctuatedwatermarks-方法">废弃的 AssignerWithPeriodicWatermarks 和 AssignerWithPunctuatedWatermarks 方法</h3>
<p>在引入当前的 <code>WatermarkStrategy</code>、<code>TimestampAssigner</code> 和 <code>WatermarkGenerator</code> 抽象之前，Flink 使用了 <code>AssignerWithPeriodicWatermarks</code> 和 <code>AssignerWithPeriodicWatermarks</code>。你仍然会在 API 中看到它们，但建议使用新的接口，因为它们提供了更清晰的分离关注点，也统一了水印生成的周期和标点样式。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/programming" term="programming" label="programming" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/event-time" term="event-time" label="Event Time" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[使用状态]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-event-time/?utm_source=atom_feed" rel="related" type="text/html" title="Event Time" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-flink-datastream-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Datastream API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-generating-watermarks/?utm_source=atom_feed" rel="related" type="text/html" title="Generating Watermarks" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-built-in-watermark-generators/?utm_source=atom_feed" rel="related" type="text/html" title="内置的水印生成器" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/?utm_source=atom_feed" rel="related" type="text/html" title="状态和容错性" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-21T00:00:00+08:00</published>
            <updated>2020-08-21T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Working With State</blockquote><h2 id="使用状态httpsciapacheorgprojectsflinkflink-docs-release-111devstreamstatestatehtml"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">使用状态</a></h2>
<p>在本节中，您将了解 Flink 为编写有状态程序提供的 API。请看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/concepts/stateful-stream-processing.html">Stateful Stream Processing</a> 来了解有状态流处理背后的概念。</p>
<h3 id="keyed-datastream">Keyed DataStream</h3>
<p>如果要使用 keyed state，首先需要在 DataStream 上指定一个键，这个键应该用来分隔(partition)状态（也包括流中的记录本身）。你可以在 DataStream 上使用 <code>keyBy(KeySelector)</code> 指定一个键。这将产生一个 <code>KeyedDataStream</code>，然后允许使用 keyed state 的操作。</p>
<p>键选择函数将一条记录作为输入，并返回该记录的键。键可以是任何类型的，并且必须从确定性计算中导出。</p>
<p>Flink 的数据模型不是基于键值对的。因此，您不需要将数据集类型物理地打包成键和值。键是&quot;虚拟&quot;的：它们被定义为实际数据上的函数，以指导分组操作符。</p>
<p>下面的例子显示了一个键选择函数，它只是返回对象的字段。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// 普通的 case 类
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">keyed</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span> <span class="k">_</span><span class="o">.</span><span class="n">word</span> <span class="o">)</span>
</code></pre></div><h4 id="元组键和表达式键">元组键和表达式键</h4>
<p>Flink 还有两种定义键的方法：元组键和表达式键。有了它，你可以使用元组字段索引或表达式来指定键，用于选择对象的字段。我们今天不推荐使用这些，但你可以参考 DataStream 的 Javadoc 来了解它们。严格来说，使用 <code>KeySelector</code> 函数更胜一筹：使用 Java lambdas，它们很容易使用，而且它们在运行时的开销可能更少。</p>
<h3 id="使用-keyed-state">使用 Keyed State</h3>
<p>keyed State 接口提供了对不同类型的状态的访问，这些状态的作用域都是当前输入元素的键。这意味着，这种类型的状态只能在 <code>KeyedStream</code> 上使用，它可以通过 <code>stream.keyBy(...)</code> 来创建。</p>
<p>现在，我们将首先看看不同类型的状态有哪些，然后我们会看看如何在程序中使用它们。可用的状态原语有:</p>
<ul>
<li>
<p><code>ValueState&lt;T&gt;</code>：它保留了一个可更新和检索的值（如上所述，作用域为输入元素的键，因此操作符所看到的每个键都可能有一个值）。这个值可以使用 <code>update(T)</code> 来设置，也可以使用 <code>T value()</code> 来检索。</p>
</li>
<li>
<p><code>ListState&lt;T&gt;</code>：这保存了一个元素列表。你可以在所有当前存储的元素上追加元素和检索一个 <code>Iterable</code>。使用 <code>add(T)</code> 或 <code>addAll(List&lt;T&gt;)</code> 添加元素，可以使用 <code>Iterable&lt;T&gt; get()</code> 检索 <code>Iterable</code>。你也可以用 <code>update(List&lt;T&gt;)</code> 覆盖现有的列表。</p>
</li>
<li>
<p><code>ReducingState&lt;T&gt;</code>: 这保留了一个单一的值，代表所有添加到状态的值的集合。该接口类似于 <code>ListState</code>，但使用 <code>add(T)</code> 添加的元素会使用指定的 <code>ReduceFunction</code> 被化简成一个总计。</p>
</li>
<li>
<p><code>AggregatingState&lt;IN，OUT&gt;</code>：这保留了一个单一的值，代表所有添加到状态的值的聚合。与 <code>ReducingState</code> 相反，<code>aggregate</code> 类型可能与添加到状态中的元素类型不同。接口与 <code>ListState</code> 相同，但使用 <code>add(IN)</code> 添加的元素会使用指定的 <code>AggregateFunction</code> 进行聚合。</p>
</li>
<li>
<p><code>MapState&lt;UK, UV&gt;</code>: 它保存了一个映射列表。你可以将键值对放入状态中，并在所有当前存储的映射上检索一个 <code>Iterable</code>。使用 <code>put(UK, UV)</code> 或 <code>putAll(Map&lt;UK, UV&gt;)</code> 可以添加映射。与用户键相关联的值可以使用 <code>get(UK)</code> 来检索。可以分别使用 <code>entries()</code>、<code>keys()</code> 和 <code>values()</code> 检索映射、键和值的可迭代视图。你也可以使用 <code>isEmpty()</code> 来检查这个映射是否包含任何键值映射。</p>
</li>
</ul>
<p>所有类型的状态也都有一个方法 <code>clear()</code>，可以清除当前活动键的状态，也就是输入元素的键。</p>
<p>需要注意的是，这些状态对象只用于带状态的接口。状态不一定存储在里面，而可能驻留在磁盘或其他地方。第二件要记住的事情是，你从状态中得到的值取决于输入元素的键。因此，如果所涉及的键不同，你在用户函数的一次调用中得到的值可能与另一次调用中的值不同。</p>
<p>为了得到一个状态句柄，你必须创建一个 <code>StateDescriptor</code>。这里面包含了状态的名称(我们稍后会看到，你可以创建多个状态，而且它们必须有独特的名称，这样你才能引用它们)，状态所拥有的值的类型，可能还有一个用户指定的函数，比如 <code>ReduceFunction</code>。根据你要检索的状态类型，你可以创建一个 <code>ValueStateDescriptor</code>、一个 <code>ListStateDescriptor</code>、一个 <code>ReducingStateDescriptor</code> 或一个 <code>MapStateDescriptor</code>。</p>
<p>状态是使用 <code>RuntimeContext</code> 访问的，所以只有在富函数(<em>rich functions</em>)中才有可能。请看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html#rich-functions">这里</a>了解相关信息，但我们也会很快看到一个例子。<code>RichFunction</code> 中可用的 <code>RuntimeContext</code> 有这些方法来访问状态。</p>
<ul>
<li>ValueState<!-- raw HTML omitted --> getState(ValueStateDescriptor<!-- raw HTML omitted -->)</li>
<li>ReducingState<!-- raw HTML omitted --> getReducingState(ReducingStateDescriptor<!-- raw HTML omitted -->)</li>
<li>ListState<!-- raw HTML omitted --> getListState(ListStateDescriptor<!-- raw HTML omitted -->)</li>
<li>AggregatingState&lt;IN, OUT&gt; getAggregatingState(AggregatingStateDescriptor&lt;IN, ACC, OUT&gt;)</li>
<li>MapState&lt;UK, UV&gt; getMapState(MapStateDescriptor&lt;UK, UV&gt;)</li>
</ul>
<p>这是一个 <code>FlatMapFunction</code> 的例子，它展示了所有的部分是如何结合在一起的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">CountWindowAverage</span> <span class="k">extends</span> <span class="nc">RichFlatMapFunction</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">var</span> <span class="n">sum</span><span class="k">:</span> <span class="kt">ValueState</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="k">_</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">flatMap</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">),</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>

    <span class="c1">// 访问状态值
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">tmpCurrentSum</span> <span class="k">=</span> <span class="n">sum</span><span class="o">.</span><span class="n">value</span>

    <span class="c1">// 如果之前没有使用过，则为 null。
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">currentSum</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">tmpCurrentSum</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">tmpCurrentSum</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="o">(</span><span class="mi">0L</span><span class="o">,</span> <span class="mi">0L</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="c1">// 更新次数
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">newSum</span> <span class="k">=</span> <span class="o">(</span><span class="n">currentSum</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="n">currentSum</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">input</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>

    <span class="c1">// 更新状态
</span><span class="c1"></span>    <span class="n">sum</span><span class="o">.</span><span class="n">update</span><span class="o">(</span><span class="n">newSum</span><span class="o">)</span>

    <span class="c1">// 如果计数达到2，则发出平均数，并清除状态。
</span><span class="c1"></span>    <span class="k">if</span> <span class="o">(</span><span class="n">newSum</span><span class="o">.</span><span class="n">_1</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">input</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">newSum</span><span class="o">.</span><span class="n">_2</span> <span class="o">/</span> <span class="n">newSum</span><span class="o">.</span><span class="n">_1</span><span class="o">))</span>
      <span class="n">sum</span><span class="o">.</span><span class="n">clear</span><span class="o">()</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">parameters</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">sum</span> <span class="k">=</span> <span class="n">getRuntimeContext</span><span class="o">.</span><span class="n">getState</span><span class="o">(</span>
      <span class="k">new</span> <span class="nc">ValueStateDescriptor</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)](</span><span class="s">&#34;average&#34;</span><span class="o">,</span> <span class="n">createTypeInformation</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)])</span>
    <span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>


<span class="k">object</span> <span class="nc">ExampleCountWindowAverage</span> <span class="k">extends</span> <span class="nc">App</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

  <span class="n">env</span><span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="nc">List</span><span class="o">(</span>
    <span class="o">(</span><span class="mi">1L</span><span class="o">,</span> <span class="mi">3L</span><span class="o">),</span>
    <span class="o">(</span><span class="mi">1L</span><span class="o">,</span> <span class="mi">5L</span><span class="o">),</span>
    <span class="o">(</span><span class="mi">1L</span><span class="o">,</span> <span class="mi">7L</span><span class="o">),</span>
    <span class="o">(</span><span class="mi">1L</span><span class="o">,</span> <span class="mi">4L</span><span class="o">),</span>
    <span class="o">(</span><span class="mi">1L</span><span class="o">,</span> <span class="mi">2L</span><span class="o">)</span>
  <span class="o">)).</span><span class="n">keyBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
    <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="nc">CountWindowAverage</span><span class="o">())</span>
    <span class="o">.</span><span class="n">print</span><span class="o">()</span>
  <span class="c1">// the printed output will be (1,4) and (1,5)
</span><span class="c1"></span>
  <span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">(</span><span class="s">&#34;ExampleKeyedState&#34;</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><p>这个例子实现了一个穷人的计数窗口。我们用第一个字段对元组进行 keyed 操作（在本例中，所有元组都有相同的键 <code>1</code>）。该函数将计数和运行的总和存储在一个 <code>ValueState</code>  中。一旦计数达到 2，它就会发出平均数并清除状态，这样我们就可以从 0 开始。注意，如果我们在第一个字段中的元组具有不同的值，那么这将为每个不同的输入键保持不同的状态值。</p>
<h4 id="状态存活时间ttl">状态存活时间(TTL)</h4>
<p>可以为任何类型的 keyed state 分配一个生存时间（TTL）。如果配置了 TTL，并且状态值已经过期，存储的值将在尽力的基础上进行清理，这将在下面详细讨论。</p>
<p>所有状态集合类型都支持每个条目的 TTL。这意味着列表元素和映射条目独立过期。</p>
<p>为了使用状态 TTL，必须首先建立一个 <code>StateTtlConfig</code> 配置对象。然后可以通过传递配置在任何状态描述符中启用 TTL 功能。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.StateTtlConfig</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.ValueStateDescriptor</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.common.time.Time</span>

<span class="k">val</span> <span class="n">ttlConfig</span> <span class="k">=</span> <span class="nc">StateTtlConfig</span>
    <span class="o">.</span><span class="n">newBuilder</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">setUpdateType</span><span class="o">(</span><span class="nc">StateTtlConfig</span><span class="o">.</span><span class="nc">UpdateType</span><span class="o">.</span><span class="nc">OnCreateAndWrite</span><span class="o">)</span>
    <span class="o">.</span><span class="n">setStateVisibility</span><span class="o">(</span><span class="nc">StateTtlConfig</span><span class="o">.</span><span class="nc">StateVisibility</span><span class="o">.</span><span class="nc">NeverReturnExpired</span><span class="o">)</span>
    <span class="o">.</span><span class="n">build</span>
    
<span class="k">val</span> <span class="n">stateDescriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ValueStateDescriptor</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;text state&#34;</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span>
<span class="n">stateDescriptor</span><span class="o">.</span><span class="n">enableTimeToLive</span><span class="o">(</span><span class="n">ttlConfig</span><span class="o">)</span>
</code></pre></div><p>配置有几个选项需要考虑。</p>
<p><code>newBuilder</code> 方法的第一个参数是强制性的，它是存活的时间值。</p>
<p>更新类型配置状态 TTL 何时被刷新（默认为 <code>OnCreateAndWrite</code>）。</p>
<ul>
<li>StateTtlConfig.UpdateType.OnCreateAndWrite - 仅在创建和写入访问时才会出现</li>
<li>StateTtlConfig.UpdateType.OnReadAndWrite - 也是在读的时候。</li>
</ul>
<p>状态可见性配置如果过期值尚未清理，是否在读取访问时返回（默认为 <code>NeverReturnExpired</code>）。</p>
<ul>
<li>StateTtlConfig.StateVisibility.NeverReturnExpired - 过期值永不返回</li>
<li>StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp - 如果仍然可用则返回。</li>
</ul>
<p>在 <code>NeverReturnExpired</code> 的情况下，过期状态就像不存在一样，即使它仍然必须被删除。这个选项对于数据在 TTL 之后必须严格地成为不可读的访问状态的用例是很有用的，例如处理隐私敏感数据的应用程序。</p>
<p>另一个选项 <code>ReturnExpiredIfNotCleanedUp</code> 允许在清理之前返回过期状态。</p>
<p>注意:</p>
<ul>
<li>
<p>状态后端存储最后一次修改的时间戳和用户值，这意味着启用该功能会增加状态存储的消耗。Heap 状态后端在内存中存储了一个额外的 Java 对象，该对象有一个对用户状态对象的引用和一个原始的长值。RocksDB 状态后端每存储一个值、列表项或映射项增加8个字节。</p>
</li>
<li>
<p>目前只支持参考处理时间的 TTL。</p>
</li>
<li>
<p>试图使用启用 TTL 的描述符来恢复之前没有配置 TTL 的状态，或者反之，将导致兼容性失败和 <code>StateMigrationException</code>。</p>
</li>
<li>
<p>TTL 配置不是检查点或保存点的一部分，而是 Flink 在当前运行的作业中如何处理的一种方式。</p>
</li>
<li>
<p>带 TTL 的映射状态目前只有在用户值序列化器能够处理 null 值的情况下才支持 null 用户值。如果序列化器不支持空值，可以用 <code>NullableSerializer</code> 包装，代价是在序列化形式中多出一个字节。</p>
</li>
</ul>
<h4 id="过期状态的清理">过期状态的清理</h4>
<p>默认情况下，过期的值会在读取时显式删除，如 <code>ValueState#value</code>，如果配置的状态后台支持，则会定期在后台进行垃圾回收。后台清理可以在 <code>StateTtlConfig</code> 中禁用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.StateTtlConfig</span>
<span class="k">val</span> <span class="n">ttlConfig</span> <span class="k">=</span> <span class="nc">StateTtlConfig</span>
    <span class="o">.</span><span class="n">newBuilder</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">disableCleanupInBackground</span>
    <span class="o">.</span><span class="n">build</span>
</code></pre></div><p>如果想对后台的一些特殊清理进行更精细的控制，可以按照下面的描述单独配置。目前，堆状态后台依靠增量清理，RocksDB 后台使用压实过滤器进行后台清理。</p>
<h5 id="全快照中的清理">全快照中的清理</h5>
<p>此外，您可以在拍摄完整状态快照的瞬间激活清理，这将减少其大小。在当前的实现下，本地状态不会被清理，但在从上一个快照恢复的情况下，它将不包括删除的过期状态。可以在 <code>StateTtlConfig</code> 中进行配置。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.StateTtlConfig</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.common.time.Time</span>

<span class="k">val</span> <span class="n">ttlConfig</span> <span class="k">=</span> <span class="nc">StateTtlConfig</span>
    <span class="o">.</span><span class="n">newBuilder</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">cleanupFullSnapshot</span>
    <span class="o">.</span><span class="n">build</span>
</code></pre></div><p>此选项不适用于 RocksDB 状态后端的增量检查点。</p>
<p><strong>注意:</strong></p>
<ul>
<li>对于现有的作业，这个清理策略可以在 <code>StateTtlConfig</code> 中随时激活或停用，例如从保存点重新启动后。</li>
</ul>
<h5 id="增量清理">增量清理</h5>
<p>另一种选择是逐步触发一些状态条目的清理。触发器可以是每次状态访问或/和每次记录处理的回调。如果这种清理策略对某些状态是激活的，存储后端就会为这个状态的所有条目保留一个惰性的全局迭代器。每次触发增量清理时，迭代器都会被提前。对遍历过的状态条目进行检查，对过期的条目进行清理。</p>
<p>这个功能可以在 <code>StateTtlConfig</code> 中配置。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.StateTtlConfig</span>
<span class="k">val</span> <span class="n">ttlConfig</span> <span class="k">=</span> <span class="nc">StateTtlConfig</span>
    <span class="o">.</span><span class="n">newBuilder</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">cleanupIncrementally</span><span class="o">(</span><span class="mi">10</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
    <span class="o">.</span><span class="n">build</span>
</code></pre></div><p>这个策略有两个参数。第一个是每次清理触发的检查状态条目数。它总是在每次状态访问时触发。第二个参数定义是否在每次记录处理中额外触发清理。堆后端默认的后台清理每次记录处理检查5个条目而不进行清理。</p>
<p><strong>注意:</strong></p>
<ul>
<li>如果没有发生对状态的访问或者没有处理记录，过期状态将持续存在。</li>
<li>增量清理所花费的时间会增加记录处理的延迟。</li>
<li>目前，增量清理只在堆状态后端实现。对 RocksDB 的设置不会有影响。</li>
<li>如果堆状态后端与同步快照一起使用，全局迭代器在迭代的时候会保留所有键的副本，因为它的具体实现不支持并发修改。那么启用这个功能会增加内存消耗。异步快照则不存在这个问题。</li>
<li>对于现有的作业，这个清理策略可以在 <code>StateTtlConfig</code> 中随时激活或停用，例如从保存点重新启动后。</li>
</ul>
<h5 id="rocksdb-压缩过程中的清理">RocksDB 压缩过程中的清理</h5>
<p>如果使用 RocksDB 状态后端，将调用 Flink 特定的压实过滤器进行后台清理。RocksDB  会定期运行异步压实来合并状态更新，减少存储量。Flink 压实过滤器通过 TTL 检查状态条目的过期时间戳，排除过期值。</p>
<p>这个功能可以在 <code>StateTtlConfig</code> 中配置。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.StateTtlConfig</span>

<span class="k">val</span> <span class="n">ttlConfig</span> <span class="k">=</span> <span class="nc">StateTtlConfig</span>
    <span class="o">.</span><span class="n">newBuilder</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">cleanupInRocksdbCompactFilter</span><span class="o">(</span><span class="mi">1000</span><span class="o">)</span>
    <span class="o">.</span><span class="n">build</span>
</code></pre></div><p>RocksDB 压实过滤器在处理一定数量的状态条目后，每次都会从 Flink 中查询当前的时间戳，用于检查过期情况，你可以改变它，并传递自定义值给 <code>StateTtlConfig.newBuilder(...).cleanupInRocksdbCompactFilter(long queryTimeAfterNumEntries)</code> 方法。更频繁地更新时间戳可以提高清理速度，但由于它使用了来自本地代码的 JNI 调用，因此降低了压缩性能。RocksDB 后台默认的清理方式是每次处理1000个条目后查询当前时间戳。</p>
<p>你可以通过激活 <code>FlinkCompactionFilter</code> 的调试级别来激活 RocksDB 过滤器原生代码的调试日志。</p>
<pre><code>log4j.logger.org.rocksdb.FlinkCompactionFilter=DEBUG
</code></pre><p><strong>注意:</strong></p>
<ul>
<li>在压实过程中调用 TTL 过滤器会使其速度减慢。TTL 过滤器必须解析最后一次访问的时间戳，并检查每个被压缩的键的存储状态条目的到期时间。如果是集合状态类型(list 或 map)，每个存储元素的检查也会被调用。</li>
<li>如果该功能用于具有非固定字节长度元素的列表状态，则原生 TTL 过滤器必须额外调用每个至少第一个元素已过期的状态条目中元素在 JNI 上的 Flink java 类型序列化器，以确定下一个未过期元素的偏移。</li>
<li>对于现有的作业，这种清理策略可以在 <code>StateTtlConfig</code> 中随时激活或停用，例如从保存点重新启动后。</li>
</ul>
<h3 id="scala-datastream-api-中的状态">Scala DataStream API 中的状态</h3>
<p>除了上面描述的接口外，Scala API 还为 KeyedStream 上具有单个 <code>ValueState</code> 的有状态 <code>map()</code> 或 <code>flatMap()</code> 函数提供了快捷方式。用户函数在 <code>Option</code> 中获取 <code>ValueState</code> 的当前值，并且必须返回一个更新的值，该值将用于更新状态。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">counts</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="n">stream</span>
  <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
  <span class="o">.</span><span class="n">mapWithState</span><span class="o">((</span><span class="n">in</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">),</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span> <span class="k">=&gt;</span>
    <span class="n">count</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="nc">Some</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span> <span class="o">(</span><span class="n">in</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">c</span><span class="o">),</span> <span class="nc">Some</span><span class="o">(</span><span class="n">c</span> <span class="o">+</span> <span class="n">in</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">)</span>
      <span class="k">case</span> <span class="nc">None</span> <span class="k">=&gt;</span> <span class="o">(</span> <span class="o">(</span><span class="n">in</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="mi">0</span><span class="o">),</span> <span class="nc">Some</span><span class="o">(</span><span class="n">in</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">)</span>
    <span class="o">})</span>
</code></pre></div><h3 id="operator-state">Operator State</h3>
<p>Operator State（或 non-keyed state）是指绑定到一个并行操作符实例的状态。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/kafka.html">Kafka 连接器</a>是 Flink 中使用 Operator State 的一个很好的激励例子。Kafka 消费者的每个并行实例都维护着一个主题分区和偏移的映射作为其 Operator State。</p>
<p>Operator State 接口支持在并行操作符实例之间重新分配状态，当并行性发生变化时。有不同的方案来进行这种重新分配。</p>
<p>在典型的有状态的 Flink 应用中，你不需要操作符状态。它主要是一种特殊类型的状态，用于源/接收器实现和你没有键的情况下，可以通过它来分隔状态。</p>
<h3 id="广播状态">广播状态</h3>
<p>Broadcast State 是 Operator State 的一种特殊类型。引入它是为了支持这样的用例：一个流的记录(records)需要被广播到所有下游任务，它们被用来在所有子任务中保持相同的状态。然后在处理第二个流的记录时可以访问这个状态。作为一个广播状态可以自然出现的例子，我们可以想象一个低吞吐量的流，其中包含一组规则，我们希望对来自另一个流的所有元素进行评估。考虑到上述类型的用例，广播状态与其余运算符状态的不同之处在于。</p>
<ul>
<li>它有一个 map 格式。</li>
<li>它只适用于有广播流和非广播流作为输入的特定操作符，以及</li>
<li>这样的操作符可以拥有多个不同名称的广播状态。</li>
</ul>
<h3 id="使用-operator-state">使用 Operator State</h3>
<p>要使用运算符状态，有状态函数可以实现 <code>CheckpointedFunction</code> 接口。</p>
<h4 id="checkpointedfunction">CheckpointedFunction</h4>
<p><code>CheckpointedFunction</code> 接口提供了对不同重分配方案的 non-keyed 的访问。它需要实现两个方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kt">void</span> <span class="nf">snapshotState</span><span class="o">(</span><span class="n">FunctionSnapshotContext</span> <span class="n">context</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>

<span class="kt">void</span> <span class="nf">initializeState</span><span class="o">(</span><span class="n">FunctionInitializationContext</span> <span class="n">context</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>
</code></pre></div><p>每当需要执行一个检查点时，就会调用 <code>snapshotState()</code>。与之对应的 <code>initializeState()</code>，在每次用户定义的函数被初始化时都会被调用，不管是在函数首次初始化时，还是在函数实际从早期的检查点恢复时。鉴于此，<code>initializeState()</code> 不仅是初始化不同类型状态的地方，也是包含状态恢复逻辑的地方。</p>
<p>目前，支持列表式操作符状态。状态有望成为一个可序列化对象的 <code>List</code>，彼此独立，因此在重新缩放时有资格重新分配。换句话说，这些对象是 non-keyed state 可以重新分配的最细粒度。根据状态访问方法的不同，定义了以下重分布方案。</p>
<ul>
<li>
<p>均分重分配: 每个操作符都会返回一个状态元素列表。整个状态在逻辑上是所有列表的连接(concatenation)。在还原/再分配时，列表被平均分成有多少个并行操作符就有多少个子列表。每个操作符都会得到一个子列表，这个子列表可以是空的，也可以包含一个或多个元素。举个例子，如果在并行度为1的情况下，一个操作符的检查点状态包含元素1和元素2，当把并行度增加到2时，元素1可能最终进入操作符实例0，而元素2将进入操作符实例1。</p>
</li>
<li>
<p>联盟再分配。每个操作符都会返回一个状态元素列表。整个状态在逻辑上是所有 <code>List</code> 的连接(concatenation)。在还原/再分配时，每个操作符都会得到完整的状态元素列表。如果你的列表可能有很高的基数(cardinality)，请不要使用这个功能。检查点元数据将为每个列表条目存储一个偏移，这可能会导致 RPC 帧大小或内存外错误。</p>
</li>
</ul>
<p>下面是一个有状态的 <code>SinkFunction</code> 的例子，它使用 <code>CheckpointedFunction</code> 来缓冲元素，然后再将它们发送到外界。它演示了基本的均分重分配列表状态。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">BufferingSink</span><span class="o">(</span><span class="n">threshold</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">0</span><span class="o">)</span>
  <span class="k">extends</span> <span class="nc">SinkFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span>
    <span class="k">with</span> <span class="nc">CheckpointedFunction</span> <span class="o">{</span>

  <span class="nd">@transient</span>
  <span class="k">private</span> <span class="k">var</span> <span class="n">checkpointedState</span><span class="k">:</span> <span class="kt">ListState</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="k">_</span>

  <span class="k">private</span> <span class="k">val</span> <span class="n">bufferedElements</span> <span class="k">=</span> <span class="nc">ListBuffer</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]()</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">invoke</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">),</span> <span class="n">context</span><span class="k">:</span> <span class="kt">Context</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">bufferedElements</span> <span class="o">+=</span> <span class="n">value</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">bufferedElements</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">threshold</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">for</span> <span class="o">(</span><span class="n">element</span> <span class="k">&lt;-</span> <span class="n">bufferedElements</span><span class="o">)</span> <span class="o">{</span>
        <span class="c1">// send it to the sink
</span><span class="c1"></span>      <span class="o">}</span>
      <span class="n">bufferedElements</span><span class="o">.</span><span class="n">clear</span><span class="o">()</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">snapshotState</span><span class="o">(</span><span class="n">context</span><span class="k">:</span> <span class="kt">FunctionSnapshotContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">checkpointedState</span><span class="o">.</span><span class="n">clear</span><span class="o">()</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">element</span> <span class="k">&lt;-</span> <span class="n">bufferedElements</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">checkpointedState</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">element</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">initializeState</span><span class="o">(</span><span class="n">context</span><span class="k">:</span> <span class="kt">FunctionInitializationContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">descriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ListStateDescriptor</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)](</span>
      <span class="s">&#34;buffered-elements&#34;</span><span class="o">,</span>
      <span class="nc">TypeInformation</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="k">new</span> <span class="nc">TypeHint</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]()</span> <span class="o">{})</span>
    <span class="o">)</span>

    <span class="n">checkpointedState</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">getOperatorStateStore</span><span class="o">.</span><span class="n">getListState</span><span class="o">(</span><span class="n">descriptor</span><span class="o">)</span>

    <span class="k">if</span><span class="o">(</span><span class="n">context</span><span class="o">.</span><span class="n">isRestored</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">for</span><span class="o">(</span><span class="n">element</span> <span class="k">&lt;-</span> <span class="n">checkpointedState</span><span class="o">.</span><span class="n">get</span><span class="o">())</span> <span class="o">{</span>
        <span class="n">bufferedElements</span> <span class="o">+=</span> <span class="n">element</span>
      <span class="o">}</span>
    <span class="o">}</span>
  <span class="o">}</span>

<span class="o">}</span>
</code></pre></div><p><code>initializeState</code> 方法的参数是一个 <code>FunctionInitializationContext</code>。它用于初始化 non-keyed &ldquo;容器&rdquo;。这些容器是一个 <code>ListState</code> 类型的容器，在检查点时，non-keyed 对象将被存储在那里。</p>
<p>请注意如何初始化状态，类似于 keyed state，用一个 <code>StateDescriptor</code> 来初始化，这个 <code>StateDescriptor</code> 包含了状态名称和状态所持有的值的类型信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">descriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ListStateDescriptor</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)](</span>
    <span class="s">&#34;buffered-elements&#34;</span><span class="o">,</span>
    <span class="nc">TypeInformation</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="k">new</span> <span class="nc">TypeHint</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]()</span> <span class="o">{})</span>
<span class="o">)</span>

<span class="n">checkpointedState</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">getOperatorStateStore</span><span class="o">.</span><span class="n">getListState</span><span class="o">(</span><span class="n">descriptor</span><span class="o">)</span>
</code></pre></div><p>状态访问方法的命名约定包含其重分配模式，然后是其状态结构。例如，如果要在还原时使用 union 重分配方案的列表状态，则使用 <code>getUnionListState(descriptor)</code> 访问状态。如果方法名中不包含重分配模式，例如 <code>getListState(descriptor)</code>，则仅仅意味着将使用基本的均分重分配方案。</p>
<p>在初始化容器后，我们使用上下文的 <code>isRestored()</code> 方法来检查是否在故障后恢复。如果为真，即我们正在恢复，则应用还原逻辑。</p>
<p>如修改后的 <code>BufferingSink</code> 的代码所示，在状态初始化过程中恢复的这个 <code>ListState</code> 被保存在一个类变量中，以便将来在 <code>snapshotState()</code> 中使用。在那里，<code>ListState</code> 会被清除掉之前检查点所包含的所有对象，然后用我们要检查点的新对象来填充。</p>
<p>顺便说一下， keyed state 也可以在 <code>initializeState()</code> 方法中初始化。这可以使用提供的 <code>FunctionInitializationContext</code> 来完成。</p>
<h3 id="有状态的源函数">有状态的源函数</h3>
<p>与其他操作符相比，有状态的源需要更多的小心。为了使状态和输出集合的更新是原子性的（对于失败/恢复时的精确一次性语义来说是必需的），用户需要从源的上下文中获得一个锁。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">CounterSource</span>
       <span class="k">extends</span> <span class="nc">RichParallelSourceFunction</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span>
       <span class="k">with</span> <span class="nc">CheckpointedFunction</span> <span class="o">{</span>

  <span class="nd">@volatile</span>
  <span class="k">private</span> <span class="k">var</span> <span class="n">isRunning</span> <span class="k">=</span> <span class="kc">true</span>

  <span class="k">private</span> <span class="k">var</span> <span class="n">offset</span> <span class="k">=</span> <span class="mi">0L</span>
  <span class="k">private</span> <span class="k">var</span> <span class="n">state</span><span class="k">:</span> <span class="kt">ListState</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="k">_</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">run</span><span class="o">(</span><span class="n">ctx</span><span class="k">:</span> <span class="kt">SourceFunction.SourceContext</span><span class="o">[</span><span class="kt">Long</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">lock</span> <span class="k">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">getCheckpointLock</span>

    <span class="k">while</span> <span class="o">(</span><span class="n">isRunning</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// output and state update are atomic
</span><span class="c1"></span>      <span class="n">lock</span><span class="o">.</span><span class="n">synchronized</span><span class="o">({</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">offset</span><span class="o">)</span>

        <span class="n">offset</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="o">})</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">cancel</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="n">isRunning</span> <span class="k">=</span> <span class="kc">false</span>
  
  <span class="k">override</span> <span class="k">def</span> <span class="n">initializeState</span><span class="o">(</span><span class="n">context</span><span class="k">:</span> <span class="kt">FunctionInitializationContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">state</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">getOperatorStateStore</span><span class="o">.</span><span class="n">getListState</span><span class="o">(</span>
      <span class="k">new</span> <span class="nc">ListStateDescriptor</span><span class="o">[</span><span class="kt">Long</span><span class="o">](</span><span class="s">&#34;state&#34;</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">Long</span><span class="o">]))</span>

    <span class="k">for</span> <span class="o">(</span><span class="n">l</span> <span class="k">&lt;-</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="o">().</span><span class="n">asScala</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">offset</span> <span class="k">=</span> <span class="n">l</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">snapshotState</span><span class="o">(</span><span class="n">context</span><span class="k">:</span> <span class="kt">FunctionSnapshotContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">state</span><span class="o">.</span><span class="n">clear</span><span class="o">()</span>
    <span class="n">state</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">offset</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>一些运算符可能需要检查点被 Flink 完全承认时的信息来与外界沟通。在这种情况下，请参见 <code>org.apache.flink.runtime.state.CheckpointListener</code> 接口。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/state" term="state" label="State" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[内置的水印生成器]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-built-in-watermark-generators/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-event-time/?utm_source=atom_feed" rel="related" type="text/html" title="Event Time" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-flink-datastream-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Datastream API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-generating-watermarks/?utm_source=atom_feed" rel="related" type="text/html" title="Generating Watermarks" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/?utm_source=atom_feed" rel="related" type="text/html" title="使用状态" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/?utm_source=atom_feed" rel="related" type="text/html" title="状态和容错性" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-built-in-watermark-generators/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-21T00:00:00+08:00</published>
            <updated>2020-08-21T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Builtin Watermark Generators</blockquote><h2 id="内置水印生成器httpsciapacheorgprojectsflinkflink-docs-release-111devevent_timestamp_extractorshtml"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_timestamp_extractors.html">内置水印生成器</a></h2>
<p>正如在 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_timestamps_watermarks.html">Generating Watermarks</a> 一文中所描述的，Flink 提供了抽象，允许程序员分配自己的时间戳和发射自己的水印。更具体地说，可以通过实现 WatermarkGenerator 接口来实现。</p>
<p>为了进一步简化此类任务的编程工作，Flink 自带了一些预先实现的时间戳分配器。本节提供了它们的列表。除了它们的开箱即用的功能外，它们的实现可以作为自定义实现的范例。</p>
<h2 id="单调地增加时间戳">单调地增加时间戳</h2>
<p>周期性水印生成的最简单的特殊情况是当给定源任务(task)看到的时间戳以升序出现时。在这种情况下，当前的时间戳总是可以作为水印，因为不会有更早的时间戳到达。</p>
<p>请注意，只需要每个并行数据源任务的时间戳是升序的。例如，如果在一个特定的设置中，一个 Kafka 分区被一个并行数据源实例读取，那么只需要在每个 Kafka 分区中时间戳是升序的。每当并行流被洗牌、联合、连接(connected)或合并时，Flink 的水印合并机制都会生成正确的水印。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">WatermarkStrategy</span><span class="o">.</span><span class="n">forMonotonousTimestamps</span><span class="o">()</span>
</code></pre></div><h2 id="固定的延迟量">固定的延迟量</h2>
<p>周期性水印生成的另一个例子是，当水印滞后于流中看到的最大（事件时间）时间戳的固定时间量时。这种情况涵盖了预先知道流中可能遇到的最大延迟的场景，例如，当创建一个包含时间戳分布在固定时间段内的元素的自定义源进行测试时。对于这些情况，Flink 提供了 BoundedOutOfOrdernessWatermarks 生成器，它以 maxOutOfOrderness 作为参数，即在计算给定窗口的最终结果时，一个元素在被忽略之前允许迟到的最大时间。Lateness 对应于 <em>t - t_w</em> 的结果，其中 <em>t</em> 是一个元素的（事件-时间）时间戳，<em>t_w</em> 是之前的水印。如果 <em>lateness &gt; 0</em>，那么该元素被认为是迟到的，并且默认情况下，在计算其对应窗口的作业结果时被忽略。请参阅关于允许延迟的文档，以获得更多关于处理迟到元素的信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">WatermarkStrategy</span>
  <span class="o">.</span><span class="n">forBoundedOutOfOrderness</span><span class="o">(</span><span class="nc">Duration</span><span class="o">.</span><span class="n">ofSeconds</span><span class="o">(</span><span class="mi">10</span><span class="o">))</span>
</code></pre></div>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/programming" term="programming" label="programming" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[状态和容错性]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-event-time/?utm_source=atom_feed" rel="related" type="text/html" title="Event Time" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-flink-datastream-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Datastream API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-generating-watermarks/?utm_source=atom_feed" rel="related" type="text/html" title="Generating Watermarks" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/?utm_source=atom_feed" rel="related" type="text/html" title="使用状态" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-built-in-watermark-generators/?utm_source=atom_feed" rel="related" type="text/html" title="内置的水印生成器" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-21T00:00:00+08:00</published>
            <updated>2020-08-21T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>State &amp; Fault Tolerance</blockquote><h2 id="状态和容错性httpsciapacheorgprojectsflinkflink-docs-release-111devstreamstate"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/">状态和容错性</a></h2>
<p>在本节中，您将了解 Flink 为编写有状态程序提供的 API。请看一下 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/concepts/stateful-stream-processing.html">Stateful Stream Processing</a>，了解有状态流处理背后的概念。</p>
<h2 id="下一步怎么走">下一步怎么走？</h2>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">使用状态</a>。展示如何在 Flink 应用中使用状态，并解释不同类型的状态。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html">广播状态模式</a>。解释如何连接一个广播流和一个非广播流，并使用状态在它们之间交换信息。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/checkpointing.html">检查点</a>。描述了如何启用和配置检查点以实现容错。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/queryable_state.html">可查询状态</a>。说明如何在运行时从 Flink 外部访问状态。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/schema_evolution.html">状态模式演化</a>：介绍如何在运行时从外部访问状态。展示了状态类型的模式如何演化。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html">管理状态的自定义序列化</a>。讨论如何实现自定义序列化，特别是针对模式演化。</li>
</ul>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
</feed>
