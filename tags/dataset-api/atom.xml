<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-us">
    <generator uri="https://gohugo.io/" version="0.79.0">Hugo</generator><title type="html"><![CDATA[DataSet API on 焉知非鱼]]></title>
    
        <subtitle type="html"><![CDATA[rakulang, dartlang, nimlang, golang, rustlang, lang lang no see]]></subtitle>
    
    
    
            <link href="https://ohmyweekly.github.io/tags/dataset-api/" rel="alternate" type="text/html" title="HTML" />
            <link href="https://ohmyweekly.github.io/tags/dataset-api/index.xml" rel="alternate" type="application/rss+xml" title="RSS" />
            <link href="https://ohmyweekly.github.io/tags/dataset-api/atom.xml" rel="self" type="application/atom+xml" title="Atom" />
            <link href="https://ohmyweekly.github.io/tags/dataset-api/jf2feed.json" rel="alternate" type="application/jf2feed+json" title="jf2feed" />
    <updated>2021-02-08T15:20:01+08:00</updated>
    
    
    
    
        <id>https://ohmyweekly.github.io/tags/dataset-api/</id>
    
        
        <entry>
            <title type="html"><![CDATA[Dataset 变换]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Dataset Transformations</blockquote><h1 id="dataset-转换">DataSet 转换</h1>
<p>本文档深入介绍了 DataSets 上可用的转换。关于 Flink Java API 的一般介绍，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">编程指南</a>。</p>
<p>对于密集索引的数据集中的压缩元素，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/zip_elements_guide.html">压缩元素指南</a>。</p>
<h2 id="map">Map</h2>
<p>Map 转换将用户定义的映射函数应用于 DataSet 的每个元素。它实现了一对一的映射，也就是说，函数必须准确地返回一个元素。</p>
<p>下面的代码将一个由整数对组成的 DataSet 转化为一个由整数组成的 DataSet。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">intPairs</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">intSums</span> <span class="k">=</span> <span class="n">intPairs</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">pair</span> <span class="k">=&gt;</span> <span class="n">pair</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="n">pair</span><span class="o">.</span><span class="n">_2</span> <span class="o">}</span>
</code></pre></div><h2 id="flatmap">FlatMap</h2>
<p>FlatMap 转换在 DataSet 的每个元素上应用了一个用户定义的 <code>flat-map</code> 函数。这种映射函数的变体可以为每个输入元素返回任意多个结果元素（包括没有）。</p>
<p>下面的代码将一个文本行的 DataSet 转换为一个单词的 DataSet。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">textLines</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">words</span> <span class="k">=</span> <span class="n">textLines</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34; &#34;</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><h2 id="mappartition">MapPartition</h2>
<p>MapPartition 在一次函数调用中转换一个并行分区。map-partition 函数以 Iterable 的形式获取分区，并可以产生任意数量的结果值。每个分区中元素的数量取决于平行度和之前的操作。</p>
<p>下面的代码将文本行的 DataSet 转换为每个分区的计数 DataSet。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">textLines</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// Some is required because the return value must be a Collection.
</span><span class="c1">// There is an implicit conversion from Option to a Collection.
</span><span class="c1"></span><span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">texLines</span><span class="o">.</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="n">in</span> <span class="k">=&gt;</span> <span class="nc">Some</span><span class="o">(</span><span class="n">in</span><span class="o">.</span><span class="n">size</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><h2 id="filter">Filter</h2>
<p>过滤器转换将用户定义的过滤器函数应用于 DataSet 的每个元素，并且只保留那些函数返回为真的元素。</p>
<p>以下代码从数据集中删除所有小于零的整数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">intNumbers</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">naturalNumbers</span> <span class="k">=</span> <span class="n">intNumbers</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">_</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">}</span>
</code></pre></div><p>重要：系统假设函数不会修改应用谓词的元素。违反这个假设会导致错误的结果。</p>
<h2 id="元组数据集的投影projection">元组数据集的投影(Projection)</h2>
<p><code>Project</code> 转换删除或移动 Tuple DataSet 的 Tuple 字段。<code>project(int...)</code> 方法通过其索引选择应该保留的 Tuple 字段，并定义它们在输出 Tuple 中的顺序。</p>
<p>投影(Projection)不需要定义用户函数。</p>
<p>下面的代码显示了在 DataSet 上应用 <code>Project</code> 转换的不同方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple3</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">in</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1">// converts Tuple3&lt;Integer, Double, String&gt; into Tuple2&lt;String, Integer&gt;
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="na">project</span><span class="o">(</span><span class="n">2</span><span class="o">,</span><span class="n">0</span><span class="o">);</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">#</span> <span class="n">scala</span>
<span class="nc">Not</span> <span class="n">supported</span><span class="o">.</span>
</code></pre></div><h3 id="分组数据集上的变换">分组数据集上的变换</h3>
<p><code>reduce</code> 操作可以对分组的数据集进行操作。指定用于分组的键可以通过多种方式进行。</p>
<ul>
<li>键表达式</li>
<li>键选择器函数</li>
<li>一个或多个字段位置键（仅限元组数据集）。</li>
<li>case 类字段(仅 case 类)</li>
</ul>
<p>请看一下 <code>reduce</code> 的例子，看看如何指定分组键。</p>
<h3 id="换算分组数据集">换算分组数据集</h3>
<p>应用于分组数据集的 <code>Reduce</code> 转换，使用用户定义的 <code>Reduce</code> 函数将每个分组换算为一个元素。对于每一组输入元素，一个 Reduce 函数将成对的元素连续组合成一个元素，直到每组只剩下一个元素。</p>
<p>请注意，对于一个 <code>ReduceFunction</code>，返回对象的键字段应该与输入值相匹配。这是因为 <code>reduce</code> 是隐式可组合的，当传递给 <code>reduce</code> 运算符时，从 <code>combine</code> 运算符发出的对象又是按键分组的。</p>
<h4 id="在按键表达式分组的数据集上进行-reduce-操作">在按键表达式分组的数据集上进行 Reduce 操作</h4>
<p>键表达式指定了 DataSet 中每个元素的一个或多个字段。每个键表达式都是一个公共字段的名称或一个 getter 方法。点号可以用来深入到对象中。键表达式 <code>&quot;*&quot;</code> 可以选择所有字段。下面的代码展示了如何使用键表达式对 POJO 数据集进行分组，并使用 <code>reduce</code> 函数对其进行换算。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO
</span><span class="c1"></span><span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="k">val</span> <span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="k">val</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">(</span><span class="kc">null</span><span class="o">,</span> <span class="o">-</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="c1">// [...]
</span><span class="c1"></span><span class="o">}</span>

<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">).</span><span class="n">reduce</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">w1</span><span class="o">,</span> <span class="n">w2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="k">new</span> <span class="nc">WC</span><span class="o">(</span><span class="n">w1</span><span class="o">.</span><span class="n">word</span><span class="o">,</span> <span class="n">w1</span><span class="o">.</span><span class="n">count</span> <span class="o">+</span> <span class="n">w2</span><span class="o">.</span><span class="n">count</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="对按键选择器分组的数据集进行换算">对按键选择器分组的数据集进行换算</h4>
<p>键选择器函数从数据集的每个元素中提取一个键值。提取的键值用于对 DataSet 进行分组。下面的代码展示了如何使用键选择器函数对 POJO 数据集进行分组，并使用 reduce 函数对其进行换算。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO
</span><span class="c1"></span><span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="k">val</span> <span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="k">val</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">(</span><span class="kc">null</span><span class="o">,</span> <span class="o">-</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="c1">// [...]
</span><span class="c1"></span><span class="o">}</span>

<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">word</span> <span class="o">}</span> <span class="n">reduce</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">w1</span><span class="o">,</span> <span class="n">w2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="k">new</span> <span class="nc">WC</span><span class="o">(</span><span class="n">w1</span><span class="o">.</span><span class="n">word</span><span class="o">,</span> <span class="n">w1</span><span class="o">.</span><span class="n">count</span> <span class="o">+</span> <span class="n">w2</span><span class="o">.</span><span class="n">count</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="对按字段位置键分组的数据集进行换算仅元组数据集">对按字段位置键分组的数据集进行换算（仅元组数据集）</h4>
<p>字段位置键指定了一个 Tuple DataSet 的一个或多个字段，这些字段被用作分组键。下面的代码显示了如何使用字段位置键和应用 reduce 函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">tuples</span> <span class="k">=</span> <span class="nc">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// group on the first and second Tuple field
</span><span class="c1"></span><span class="k">val</span> <span class="n">reducedTuples</span> <span class="k">=</span> <span class="n">tuples</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1</span><span class="o">).</span><span class="n">reduce</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h4 id="对按-case-类字段分组的数据集进行换算">对按 case 类字段分组的数据集进行换算</h4>
<p>当使用 Case Classes 时，你也可以使用字段的名称来指定分组键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">MyClass</span><span class="o">(</span><span class="k">val</span> <span class="n">a</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">c</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>
<span class="k">val</span> <span class="n">tuples</span> <span class="k">=</span> <span class="nc">DataSet</span><span class="o">[</span><span class="kt">MyClass</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// group on the first and second field
</span><span class="c1"></span><span class="k">val</span> <span class="n">reducedTuples</span> <span class="k">=</span> <span class="n">tuples</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="s">&#34;b&#34;</span><span class="o">).</span><span class="n">reduce</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h3 id="在分组数据集上进行分组换算">在分组数据集上进行分组换算</h3>
<p>应用在分组 DataSet 上的 GroupReduce 转换，会对每个组调用用户定义的 <code>group-reduce</code> 函数。这与 Reduce 之间的区别在于，用户定义的函数可以一次性获得整个组。该函数是在一个组的所有元素上用一个 <code>Iterable</code> 调用的，并且可以返回任意数量的结果元素。</p>
<h4 id="在按字段位置键分组的数据集上进行分组-reduce只适用于元组数据集">在按字段位置键分组的数据集上进行分组 Reduce(只适用于元组数据集)</h4>
<p>下面的代码显示了如何从一个按 Integer 分组的 DataSet 中删除重复的字符串。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">reduceGroup</span> <span class="o">{</span>
      <span class="o">(</span><span class="n">in</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)])</span> <span class="k">=&gt;</span>
        <span class="n">in</span><span class="o">.</span><span class="n">toSet</span> <span class="n">foreach</span> <span class="o">(</span><span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">)</span>
    <span class="o">}</span>
</code></pre></div><h4 id="对按键表达式键选择器函数或-case-类字段分组的数据集进行分组换算">对按键表达式、键选择器函数或 case 类字段分组的数据集进行分组换算</h4>
<p>类似于 Reduce 变换中的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-dataset-grouped-by-key-expression">键表达式</a>、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-dataset-grouped-by-keyselector-function">键选择器函数</a>和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-dataset-grouped-by-case-class-fields">case 类字段</a>的工作。</p>
<h4 id="对排序组进行-groupreduce">对排序组进行 GroupReduce</h4>
<p>一个 <code>group-reduce</code> 函数使用一个 Iterable 访问一个组的元素。可选地，Iterable 可以按照指定的顺序输出一个组的元素。在许多情况下，这有助于降低用户定义的 <code>group-reduce</code> 函数的复杂性，并提高其效率。</p>
<p>下面的代码显示了另一个例子，如何在一个由整数分组并按 String 排序的 DataSet 中删除重复的 String。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">sortGroup</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">reduceGroup</span> <span class="o">{</span>
      <span class="o">(</span><span class="n">in</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)])</span> <span class="k">=&gt;</span>
        <span class="k">var</span> <span class="n">prev</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=</span> <span class="kc">null</span>
        <span class="k">for</span> <span class="o">(</span><span class="n">t</span> <span class="k">&lt;-</span> <span class="n">in</span><span class="o">)</span> <span class="o">{</span>
          <span class="k">if</span> <span class="o">(</span><span class="n">prev</span> <span class="o">==</span> <span class="kc">null</span> <span class="o">||</span> <span class="n">prev</span> <span class="o">!=</span> <span class="n">t</span><span class="o">)</span>
            <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">t</span><span class="o">)</span>
            <span class="n">prev</span> <span class="k">=</span> <span class="n">t</span>
        <span class="o">}</span>
    <span class="o">}</span>
</code></pre></div><p>注意：如果在 <code>reduce</code> 操作之前，使用运算符的基于排序的执行策略建立了分组，那么 GroupSort 通常是免费的。</p>
<h4 id="可组合的-groupreducefunctions">可组合的 GroupReduceFunctions</h4>
<p>与 reduce 函数不同，<code>group-reduce</code> 函数是不可隐式组合的。为了使一个分组换算函数可以组合，它必须实现 <code>GroupCombineFunction</code> 接口。</p>
<p>重要：<code>GroupCombineFunction</code> 接口的通用输入和输出类型必须等于 <code>GroupReduceFunction</code> 的通用输入类型，如下例所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Combinable GroupReduceFunction that computes two sums.
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyCombinableGroupReducer</span>
  <span class="k">extends</span> <span class="nc">GroupReduceFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="kt">String</span><span class="o">]</span>
  <span class="k">with</span> <span class="nc">GroupCombineFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span>
<span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">reduce</span><span class="o">(</span>
    <span class="n">in</span><span class="k">:</span> <span class="kt">java.lang.Iterable</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)],</span>
    <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span>
  <span class="o">{</span>
    <span class="k">val</span> <span class="n">r</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span>
      <span class="n">in</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">asScala</span><span class="o">.</span><span class="n">reduce</span><span class="o">(</span> <span class="o">(</span><span class="n">a</span><span class="o">,</span><span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">a</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">a</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">)</span>
    <span class="c1">// concat key and sum and emit
</span><span class="c1"></span>    <span class="n">out</span><span class="o">.</span><span class="n">collect</span> <span class="o">(</span><span class="n">r</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="s">&#34;-&#34;</span> <span class="o">+</span> <span class="n">r</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">combine</span><span class="o">(</span>
    <span class="n">in</span><span class="k">:</span> <span class="kt">java.lang.Iterable</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)],</span>
    <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span>
  <span class="o">{</span>
    <span class="k">val</span> <span class="n">r</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span>
      <span class="n">in</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">asScala</span><span class="o">.</span><span class="n">reduce</span><span class="o">(</span> <span class="o">(</span><span class="n">a</span><span class="o">,</span><span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">a</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">a</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">)</span>
    <span class="c1">// emit tuple with key and sum
</span><span class="c1"></span>    <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">r</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="在分组数据集上进行分组合并">在分组数据集上进行分组合并</h3>
<p><code>GroupCombine</code> 变换是可组合的 <code>GroupReduceFunction</code> 中 <code>combine</code> 步骤的泛化形式。与此相反，<code>GroupReduce</code> 函数中的 <code>combine</code> 步骤只允许从输入类型 I 到输出类型 I 的组合。这是因为 <code>GroupReduce</code> 函数中的 <code>reduce</code> 步骤期望输入类型 I。</p>
<p>在某些应用中，希望在执行额外的转换（例如减少数据大小）之前，将一个数据集合并成中间格式。这可以通过一个 <code>CombineGroup</code> 转换来实现，而且成本很低。</p>
<p>注意：对分组数据集的 <code>GroupCombine</code> 是在内存中以贪婪的策略执行的，它可能不会一次处理所有数据，而是分多个步骤进行。它也是在各个分区上执行的，而不像 <code>GroupReduce</code> 变换那样进行数据交换。这可能会导致部分结果。</p>
<p>下面的例子演示了如何使用 <code>CombineGroup</code> 变换来实现另一种 <code>WordCount</code>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">[</span><span class="kt">..</span><span class="o">]</span> <span class="c1">// The words received as input
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">combinedWords</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="n">combineGroup</span> <span class="o">{</span>
    <span class="o">(</span><span class="n">words</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)])</span> <span class="k">=&gt;</span>
        <span class="k">var</span> <span class="n">key</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="kc">null</span>
        <span class="k">var</span> <span class="n">count</span> <span class="k">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="o">(</span><span class="n">word</span> <span class="k">&lt;-</span> <span class="n">words</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">key</span> <span class="k">=</span> <span class="n">word</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="o">}</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">key</span><span class="o">,</span> <span class="n">count</span><span class="o">))</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="n">combinedWords</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="n">reduceGroup</span> <span class="o">{</span>
    <span class="o">(</span><span class="n">words</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)])</span> <span class="k">=&gt;</span>
        <span class="k">var</span> <span class="n">key</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="kc">null</span>
        <span class="k">var</span> <span class="n">sum</span> <span class="k">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="o">((</span><span class="n">word</span><span class="o">,</span> <span class="n">sum</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">words</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">key</span> <span class="k">=</span> <span class="n">word</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">count</span>
        <span class="o">}</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">key</span><span class="o">,</span> <span class="n">sum</span><span class="o">))</span>
<span class="o">}</span>
</code></pre></div><p>上面的另一种 <code>WordCount</code> 实现演示了 <code>GroupCombine</code> 如何在执行 <code>GroupReduce</code> 转换之前组合单词。上面的例子只是一个概念证明。请注意，组合步骤如何改变 DataSet 的类型，通常在执行 <code>GroupReduce</code> 之前需要进行额外的 Map 转换。</p>
<h4 id="在分组元组数据集上进行聚合">在分组元组数据集上进行聚合</h4>
<p>有一些常用的聚合操作是经常使用的。Aggregate 转换提供了以下内置的聚合函数。</p>
<ul>
<li>Sum,</li>
<li>Min,</li>
<li>Max.</li>
</ul>
<p>Aggregate 变换只能应用在 Tuple 数据集上，并且只支持字段位置键进行分组。</p>
<p>下面的代码显示了如何在按字段位置键分组的数据集上应用&quot;聚合&quot;变换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">SUM</span><span class="o">,</span> <span class="mi">0</span><span class="o">).</span><span class="n">and</span><span class="o">(</span><span class="nc">MIN</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
</code></pre></div><p>要在一个 DataSet 上应用多个聚合，必须在第一个聚合之后使用 <code>.and()</code> 函数，也就是说 <code>.aggregary(SUM, 0).and(MIN, 2)</code> 会产生原始 DataSet 的字段 0 和字段 2 的最小值之和。与此相反，<code>.aggregary(SUM，0).aggregary(MIN，2)</code> 将在一个聚合上应用一个聚合。在给定的示例中，它将在计算字段 0 与字段 1 分组后产生字段 2 的最小值。</p>
<p>注意：聚合函数集将在未来得到扩展。</p>
<h4 id="对分组元组数据集的-minby--maxby-函数">对分组元组数据集的 MinBy / MaxBy 函数</h4>
<p><code>MinBy (MaxBy)</code> 转换为每组元组选择一个元组。被选择的元组是一个或多个指定字段的值是最小（最大）的元组。用于比较的字段必须是有效的关键字段，即可比较的字段。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。</p>
<p>下面的代码显示了如何从 <code>DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt;</code> 中选择具有相同 String 值的每组元组的 Integer 和 Double 字段最小值的元组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span>
                                   <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>  <span class="c1">// group DataSet on second field
</span><span class="c1"></span>                                   <span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span> <span class="c1">// select tuple with minimum values for first and third field.
</span></code></pre></div><h3 id="换算整个数据集">换算整个数据集</h3>
<p>Reduce 转换将用户定义的 <code>reduce</code> 函数应用于一个数据集的所有元素。随后，<code>reduce</code> 函数将元素对组合成一个元素，直到只剩下一个元素。</p>
<p>下面的代码显示了如何对一个整数数据集的所有元素进行求和。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">intNumbers</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">)</span>
<span class="k">val</span> <span class="n">sum</span> <span class="k">=</span> <span class="n">intNumbers</span><span class="o">.</span><span class="n">reduce</span> <span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">)</span>
</code></pre></div><p>使用 Reduce 转换换算一个完整的 DataSet 意味着最后的 Reduce 操作不能并行完成。然而，<code>reduce</code> 函数是可以自动组合的，因此 Reduce 转换不会限制大多数用例的可扩展性。</p>
<h3 id="对整个数据集进行分组换算">对整个数据集进行分组换算</h3>
<p><code>GroupReduce</code> 转换将用户定义的 <code>group-reduce</code> 函数应用于 DataSet 的所有元素。<code>group-reduce</code> 可以遍历 DataSet 的所有元素，并返回任意数量的结果元素。</p>
<p>下面的示例展示了如何在一个完整的 DataSet 上应用 <code>GroupReduce</code> 转换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyGroupReducer</span><span class="o">())</span>
</code></pre></div><p>注意：如果 <code>group-reduce</code> 函数不可组合，那么在一个完整的 DataSet 上的 <code>GroupReduce</code> 转换不能并行完成。因此，这可能是一个非常耗费计算的操作。请参阅上面的&quot;可组合的 GroupReduceFunctions&quot; 部分，了解如何实现可组合的 <code>group-reduce</code> 函数。</p>
<h3 id="在完整的数据集上进行分组合并groupcombine">在完整的数据集上进行分组合并(GroupCombine)</h3>
<p>在一个完整的 DataSet 上的 GroupCombine 的工作原理类似于在一个分组的 DataSet 上的 GroupCombine。在所有节点上对数据进行分区，然后以贪婪的方式进行合并（即只有适合内存的数据才会一次性合并）。</p>
<h3 id="在完整的-tuple-数据集上进行聚合">在完整的 Tuple 数据集上进行聚合</h3>
<p>有一些常用的聚合操作是经常使用的。Aggregate 转换提供了以下内置的聚合函数。</p>
<ul>
<li>Sum,</li>
<li>Min, 和</li>
<li>Max.</li>
</ul>
<p>Aggregate 变换只能应用于 Tuple 数据集。</p>
<p>下面的代码显示了如何在一个完整的数据集上应用聚合转换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">SUM</span><span class="o">,</span> <span class="mi">0</span><span class="o">).</span><span class="n">and</span><span class="o">(</span><span class="nc">MIN</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
</code></pre></div><p>注意：扩展支持的聚合函数集是我们的路线图。</p>
<h3 id="在完整的元组数据集上实现-minby--maxby">在完整的元组数据集上实现 MinBy / MaxBy</h3>
<p><code>MinBy (MaxBy)</code> 转换从一个元组数据集中选择一个元组。被选择的元组是一个或多个指定字段的值是最小（最大）的元组。用于比较的字段必须是有效的键字段，即可比较的字段。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。</p>
<p>以下代码显示了如何从 <code>DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt;</code> 中选择具有 Integer 和 Double 字段最大值的元组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span>                          
                                   <span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span> <span class="c1">// select tuple with maximum values for first and third field.
</span></code></pre></div><h3 id="distinct">Distinct</h3>
<p>Distinct 转换计算源 DataSet 中不同元素的 DataSet。下面的代码从 DataSet 中删除所有重复的元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">distinct</span><span class="o">()</span>
</code></pre></div><p>也可以使用以下方法改变 DataSet 中元素的区分方式。</p>
<ul>
<li>一个或多个字段位置键（仅元组数据集）。</li>
<li>一个键选择器函数，或</li>
<li>一个键表达式</li>
</ul>
<h4 id="用字段位置键去重distinct">用字段位置键去重(Distinct)</h4>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">distinct</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">2</span><span class="o">)</span>
</code></pre></div><h4 id="用-keyselector-函数去重distinct">用 KeySelector 函数去重(Distinct)</h4>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">distinct</span> <span class="o">{</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nc">Math</span><span class="o">.</span><span class="n">abs</span><span class="o">(</span><span class="n">x</span><span class="o">)}</span>
</code></pre></div><h4 id="用键表达式去重distinct">用键表达式去重(Distinct)</h4>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">CustomType</span><span class="o">(</span><span class="n">aName</span> <span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">aNumber</span> <span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span> <span class="o">}</span>

<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">CustomType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">distinct</span><span class="o">(</span><span class="s">&#34;aName&#34;</span><span class="o">,</span> <span class="s">&#34;aNumber&#34;</span><span class="o">)</span>
</code></pre></div><p>也可以用通配符表示使用所有字段:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO
</span><span class="c1"></span><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">CustomType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">distinct</span><span class="o">(</span><span class="s">&#34;_&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="join">Join</h3>
<p>Join 转换将两个 DataSets 连接成一个 DataSet。两个数据集的元素在一个或多个键上进行连接(join)，这些键可以通过使用</p>
<ul>
<li>键选择器函数</li>
<li>一个或多个字段位置键（仅限 Tuple DataSet）。</li>
<li>case 类字段</li>
</ul>
<p>有几种不同的方法来执行 Join 转换，如下所示。</p>
<h4 id="默认的-join-join-into-tuple2">默认的 Join (Join into Tuple2)</h4>
<p>默认的 Join 变换会产生一个新的 Tuple DataSet，它有两个字段。每个元组在第一个元组字段中持有第一个输入 DataSet 的 join 元素，在第二个字段中持有第二个输入 DataSet 的匹配元素。</p>
<p>下面的代码显示了一个使用字段位置键的默认 Join 转换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">input2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Double</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">input2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><h4 id="用-join-函数连接">用 Join 函数连接</h4>
<p>Join 转换也可以调用用户定义的 <code>join</code> 函数来处理连接(joining)元组。<code>join</code> 函数接收第一个输入 DataSet 的一个元素和第二个输入 DataSet 的一个元素，并准确返回一个元素。</p>
<p>下面的代码使用键选择器函数执行了一个带有自定义 java 对象的 DataSet 和一个 Tuple DataSet 的连接，并展示了如何使用用户定义的连接(join)函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">Rating</span><span class="o">(</span><span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">category</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">points</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>

<span class="k">val</span> <span class="n">ratings</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Ratings</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">weights</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">weightedRatings</span> <span class="k">=</span> <span class="n">ratings</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">weights</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;category&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">rating</span><span class="o">,</span> <span class="n">weight</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">rating</span><span class="o">.</span><span class="n">name</span><span class="o">,</span> <span class="n">rating</span><span class="o">.</span><span class="n">points</span> <span class="o">*</span> <span class="n">weight</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="用-flat-join-函数连接">用 Flat-Join 函数连接</h4>
<p>类似于 Map 和 FlatMap，<code>FlatJoin</code> 的行为方式与 Join 相同，但它不是返回一个元素，而是可以返回（收集）、零个、一个或多个元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">Rating</span><span class="o">(</span><span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">category</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">points</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>

<span class="k">val</span> <span class="n">ratings</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Ratings</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">weights</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">weightedRatings</span> <span class="k">=</span> <span class="n">ratings</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">weights</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;category&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">rating</span><span class="o">,</span> <span class="n">weight</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)])</span> <span class="k">=&gt;</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">weight</span><span class="o">.</span><span class="n">_2</span> <span class="o">&gt;</span> <span class="mf">0.1</span><span class="o">)</span> <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">rating</span><span class="o">.</span><span class="n">name</span><span class="o">,</span> <span class="n">rating</span><span class="o">.</span><span class="n">points</span> <span class="o">*</span> <span class="n">weight</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="用-projection-java-only-连接">用 Projection (Java Only) 连接</h4>
<p>Join 变换可以使用投影(projection)构造结果元组，如下所示:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple3</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Byte</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">input1</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;&gt;</span> <span class="n">input2</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple4</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">String</span><span class="o">,</span> <span class="n">Double</span><span class="o">,</span> <span class="n">Byte</span><span class="o">&gt;&gt;</span>
            <span class="n">result</span> <span class="o">=</span>
            <span class="n">input1</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">input2</span><span class="o">)</span>
                  <span class="c1">// key definition on first DataSet using a field position key
</span><span class="c1"></span>                  <span class="o">.</span><span class="na">where</span><span class="o">(</span><span class="n">0</span><span class="o">)</span>
                  <span class="c1">// key definition of second DataSet using a field position key
</span><span class="c1"></span>                  <span class="o">.</span><span class="na">equalTo</span><span class="o">(</span><span class="n">0</span><span class="o">)</span>
                  <span class="c1">// select and reorder fields of matching tuples
</span><span class="c1"></span>                  <span class="o">.</span><span class="na">projectFirst</span><span class="o">(</span><span class="n">0</span><span class="o">,</span><span class="n">2</span><span class="o">).</span><span class="na">projectSecond</span><span class="o">(</span><span class="n">1</span><span class="o">).</span><span class="na">projectFirst</span><span class="o">(</span><span class="n">1</span><span class="o">);</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// scala
</span><span class="c1"></span><span class="nc">Not</span> <span class="n">supported</span><span class="o">.</span>
</code></pre></div><h4 id="用数据集大小提示-join">用数据集大小提示 Join</h4>
<p>为了引导优化器选择正确的执行策略，你可以提示要连接(join)的 DataSet 的大小，如下所示:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">input2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// hint that the second DataSet is very small
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">joinWithTiny</span><span class="o">(</span><span class="n">input2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="c1">// hint that the second DataSet is very large
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">joinWithHuge</span><span class="o">(</span><span class="n">input2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
</code></pre></div><h4 id="join-算法提示">Join 算法提示</h4>
<p>Flink 运行时可以以各种方式执行连接(join)。每一种可能的方式在不同的情况下都会优于其他方式。系统会尝试自动选择一种合理的方式，但也允许你手动选择一种策略，以防你想强制执行特定的连接(join)方式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">SomeType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">input2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">AnotherType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// hint that the second DataSet is very small
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">input2</span><span class="o">,</span> <span class="nc">JoinHint</span><span class="o">.</span><span class="nc">BROADCAST_HASH_FIRST</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;id&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
</code></pre></div><p>有以下提示:</p>
<ul>
<li>
<p>OPTIMIZER_CHOOSES: 相当于完全不给提示，让系统来选择。</p>
</li>
<li>
<p>BROADCAST_HASH_FIRST：广播第一个输入，并据此建立一个哈希表，由第二个输入探测。如果第一个输入的数据非常小，这是一个很好的策略。</p>
</li>
<li>
<p>BROADCAST_HASH_SECOND: 广播第二个输入，并从中建立一个哈希表，由第一个输入探测。如果第二个输入非常小，是一个很好的策略。</p>
</li>
<li>
<p>REPARTITION_HASH_FIRST：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第一个输入建立一个哈希表。如果第一个输入比第二个输入小，但两个输入都很大，这个策略就很好。注意：如果无法估计大小，也无法重新使用已有的分区和排序，系统就会使用这个默认的后备策略。</p>
</li>
<li>
<p>REPARTITION_HASH_SECOND：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第二个输入建立一个哈希表。如果第二个输入比第一个输入小，但两个输入仍然很大，这个策略就很好。</p>
</li>
<li>
<p>REPARTITION_SORT_MERGE：系统对每个输入进行分区（洗牌）（除非输入已经分区），并对每个输入进行排序（除非已经排序）。通过对排序后的输入进行流式合并来连接(join)这些输入。如果一个或两个输入都已经被排序，这个策略就很好。</p>
</li>
</ul>
<h3 id="外连接">外连接</h3>
<p><code>OuterJoin</code> 转换在两个数据集上执行左、右或全外连接。外连接与常规（内连接）类似，创建所有键值相等的元素对。此外，如果在另一侧没有找到匹配的键，&ldquo;外侧&quot;的记录（左、右，或者在完全的情况下两者都有）将被保留。匹配的一对元素（或一个元素和另一个输入的空值）被交给 JoinFunction 将这对元素变成一个元素，或交给 FlatJoinFunction 将这对元素变成任意多个（包括无）元素。</p>
<p>两个 DataSets 的元素都是在一个或多个键上连接的，这些键可以通过使用</p>
<ul>
<li>键选择器函数</li>
<li>一个或多个字段位置键（仅限 Tuple DataSet）。</li>
<li>case 类字段</li>
</ul>
<p>OuterJoins 只支持 Java 和 Scala DataSet API。</p>
<h4 id="用-join-函数进行外连接">用 Join 函数进行外连接</h4>
<p><code>OuterJoin</code> 转换调用一个用户定义的 <code>join</code> 函数来处理连接元组。<code>join</code> 函数接收第一个输入 DataSet 的一个元素和第二个输入 DataSet 的一个元素，并准确地返回一个元素。根据外连接的类型（左、右、全），连接函数的两个输入元素中可以有一个是空的。</p>
<p>下面的代码使用键选择器函数执行 DataSet 与自定义 java 对象和 Tuple DataSet 的左外连接，并展示了如何使用用户定义的连接函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">Rating</span><span class="o">(</span><span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">category</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">points</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>

<span class="k">val</span> <span class="n">movies</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">ratings</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Ratings</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">moviesWithPoints</span> <span class="k">=</span> <span class="n">movies</span><span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span><span class="n">ratings</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;name&#34;</span><span class="o">)</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">movie</span><span class="o">,</span> <span class="n">rating</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">movie</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="k">if</span> <span class="o">(</span><span class="n">rating</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">rating</span><span class="o">.</span><span class="n">points</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="使用-flat-join-函数进行外连接">使用 Flat-Join 函数进行外连接</h4>
<p>类似于 Map 和 FlatMap，一个带有 <code>flat-join</code> 函数的 OuterJoin 的行为与带有 <code>join</code> 函数的 OuterJoin 相同，但它不是返回一个元素，而是可以返回（收集）、零个、一个或多个元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">Not</span> <span class="n">supported</span><span class="o">.</span>
</code></pre></div><h4 id="join-算法提示-1">Join 算法提示</h4>
<p>Flink 运行时可以以各种方式执行外连接。每一种可能的方式在不同的情况下都会优于其他方式。系统试图自动选择一种合理的方式，但允许你手动选择一种策略，以防你想强制执行特定的外连接方式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">SomeType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">input2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">AnotherType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// hint that the second DataSet is very small
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span><span class="n">input2</span><span class="o">,</span> <span class="nc">JoinHint</span><span class="o">.</span><span class="nc">REPARTITION_SORT_MERGE</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;id&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">result2</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">rightOuterJoin</span><span class="o">(</span><span class="n">input2</span><span class="o">,</span> <span class="nc">JoinHint</span><span class="o">.</span><span class="nc">BROADCAST_HASH_FIRST</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;id&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
</code></pre></div><p>有以下提示:</p>
<ul>
<li>
<p>OPTIMIZER_CHOOSES: 相当于完全不给提示，让系统来选择。</p>
</li>
<li>
<p>BROADCAST_HASH_FIRST：广播第一个输入，并据此建立一个哈希表，由第二个输入探测。如果第一个输入的数据非常小，这是一个很好的策略。</p>
</li>
<li>
<p>BROADCAST_HASH_SECOND: 广播第二个输入，并从中建立一个哈希表，由第一个输入探测。如果第二个输入非常小，是一个很好的策略。</p>
</li>
<li>
<p>REPARTITION_HASH_FIRST：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第一个输入建立一个哈希表。如果第一个输入比第二个输入小，但两个输入仍然很大，这个策略就很好。</p>
</li>
<li>
<p>REPARTITION_HASH_SECOND：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第二个输入建立一个哈希表。如果第二个输入比第一个输入小，但两个输入仍然很大，这个策略就很好。</p>
</li>
<li>
<p>REPARTITION_SORT_MERGE：系统对每个输入进行分区（洗牌）（除非输入已经分区），并对每个输入进行排序（除非已经排序）。通过对排序后的输入进行流式合并来连接(join)这些输入。如果一个或两个输入都已经被排序，这个策略就很好。</p>
</li>
</ul>
<p>注意：目前还不是所有的外连接类型都支持所有的执行策略。</p>
<ul>
<li>
<p>LeftOuterJoin 支持:</p>
<ul>
<li>OPTIMIZER_CHOOSES</li>
<li>BROADCAST_HASH_SECOND</li>
<li>REPARTITION_HASH_SECOND</li>
<li>REPARTITION_SORT_MERGE</li>
</ul>
</li>
<li>
<p>RightOuterJoin 支持:</p>
<ul>
<li>OPTIMIZER_CHOOSES</li>
<li>BROADCAST_HASH_FIRST</li>
<li>REPARTITION_HASH_FIRST</li>
<li>REPARTITION_SORT_MERGE</li>
</ul>
</li>
<li>
<p>FullOuterJoin 支持:</p>
<ul>
<li>OPTIMIZER_CHOOSES</li>
<li>REPARTITION_SORT_MERGE</li>
</ul>
</li>
</ul>
<h3 id="cross">Cross</h3>
<p>Cross 变换将两个 DataSets 组合成一个 DataSet。它建立了两个输入数据集元素的所有 pairwise 组合，即建立了一个笛卡尔积。Cross 变换要么在每对元素上调用用户定义的 <code>cross</code> 函数，要么输出一个 Tuple2。这两种模式如下所示。</p>
<p>注意：Cross 是一个潜在的计算密集型操作，甚至可以挑战大型计算集群。</p>
<h4 id="使用用户定义函数进行交叉运算">使用用户定义函数进行交叉运算</h4>
<p>Cross 变换可以调用一个用户定义的 <code>cross</code> 函数。<code>cross</code> 函数接收第一个输入的一个元素和第二个输入的一个元素，并正好返回一个结果元素。</p>
<p>下面的代码展示了如何使用 <code>cross</code> 函数对两个 DataSets 进行交叉变换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">Coord</span><span class="o">(</span><span class="n">id</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">x</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">y</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>

<span class="k">val</span> <span class="n">coords1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Coord</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">coords2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Coord</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">distances</span> <span class="k">=</span> <span class="n">coords1</span><span class="o">.</span><span class="n">cross</span><span class="o">(</span><span class="n">coords2</span><span class="o">)</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">c1</span><span class="o">,</span> <span class="n">c2</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">dist</span> <span class="k">=</span> <span class="n">sqrt</span><span class="o">(</span><span class="n">pow</span><span class="o">(</span><span class="n">c1</span><span class="o">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">c2</span><span class="o">.</span><span class="n">x</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span> <span class="o">+</span> <span class="n">pow</span><span class="o">(</span><span class="n">c1</span><span class="o">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">c2</span><span class="o">.</span><span class="n">y</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
    <span class="o">(</span><span class="n">c1</span><span class="o">.</span><span class="n">id</span><span class="o">,</span> <span class="n">c2</span><span class="o">.</span><span class="n">id</span><span class="o">,</span> <span class="n">dist</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="用数据集大小提示交叉">用数据集大小提示交叉</h4>
<p>为了引导优化器选择正确的执行策略，你可以提示要交叉的 DataSet 的大小，如下所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">input2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// hint that the second DataSet is very small
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">crossWithTiny</span><span class="o">(</span><span class="n">input2</span><span class="o">)</span>

<span class="c1">// hint that the second DataSet is very large
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">crossWithHuge</span><span class="o">(</span><span class="n">input2</span><span class="o">)</span>
</code></pre></div><h3 id="cogroup">CoGroup</h3>
<p>CoGroup 转换联合(jointly)处理两个 DataSets 的组。两个 DataSets 根据定义的键进行分组，共享同一键的两个 DataSets 的组被一起交给用户定义的共组(co-group)函数。如果对于一个特定的键来说，只有一个 DataSet 有一个组，那么 <code>co-group</code> 函数就会和这个组以及一个空组一起被调用。共组(co-group)函数可以分别迭代两个组的元素，并返回任意数量的结果元素。</p>
<p>与 Reduce、GroupReduce 和 Join 类似，可以使用不同的键选择器方法来定义键。</p>
<h4 id="数据集上的-cogroup">数据集上的 CoGroup</h4>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">iVals</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">dVals</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">iVals</span><span class="o">.</span><span class="n">coGroup</span><span class="o">(</span><span class="n">dVals</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">iVals</span><span class="o">,</span> <span class="n">dVals</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">ints</span> <span class="k">=</span> <span class="n">iVals</span> <span class="n">map</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">_2</span> <span class="o">}</span> <span class="n">toSet</span>

    <span class="k">for</span> <span class="o">(</span><span class="n">dVal</span> <span class="k">&lt;-</span> <span class="n">dVals</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">for</span> <span class="o">(</span><span class="n">i</span> <span class="k">&lt;-</span> <span class="n">ints</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">dVal</span><span class="o">.</span><span class="n">_2</span> <span class="o">*</span> <span class="n">i</span><span class="o">)</span>
      <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="union">Union</h3>
<p>产生两个 DataSets 的联合(union)，这两个 DataSets 必须是同一类型。两个以上 DataSets 的联合(union)可以通过多个联合(union)调用来实现，如下所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">vals1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">vals2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">vals3</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">unioned</span> <span class="k">=</span> <span class="n">vals1</span><span class="o">.</span><span class="n">union</span><span class="o">(</span><span class="n">vals2</span><span class="o">).</span><span class="n">union</span><span class="o">(</span><span class="n">vals3</span><span class="o">)</span>
</code></pre></div><h3 id="rebalance">Rebalance</h3>
<p>均匀地重新平衡 DataSet 的并行分区，以消除数据倾斜。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// rebalance DataSet and apply a Map transformation.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">rebalance</span><span class="o">().</span><span class="n">map</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h3 id="hash-partition">Hash-Partition</h3>
<p>在给定的键上对 DataSet 进行散列分割。键可以被指定为位置键、表达式键和键选择器函数（关于如何指定键，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-grouped-dataset">Reduce 示例</a>）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// hash-partition DataSet by String value and apply a MapPartition transformation.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">partitionByHash</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h3 id="range-partition">Range-Partition</h3>
<p>在给定的键上 Range-partitions 一个 DataSet。键可以被指定为位置键、表达式键和键选择器函数（关于如何指定键，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-grouped-dataset">Reduce 示例</a>）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// range-partition DataSet by String value and apply a MapPartition transformation.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">partitionByRange</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h3 id="sort-partition">Sort Partition</h3>
<p>按照指定的顺序，在指定的字段上对 DataSet 的所有分区进行本地排序。字段可以被指定为字段表达式或字段位置（关于如何指定键，请参阅 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-grouped-dataset">Reduce 示例</a>）。通过链式 <code>sortPartition()</code> 调用，可以在多个字段上对分区进行排序。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// Locally sort partitions in ascending order on the second String field and
</span><span class="c1">// in descending order on the first String field.
</span><span class="c1">// Apply a MapPartition transformation on the sorted partitions.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">)</span>
            <span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">)</span>
            <span class="o">.</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h3 id="first-n">First-n</h3>
<p>返回一个 DataSet 的前 n 个（任意）元素。First-n 可以应用于一个常规的 DataSet、一个分组的 DataSet 或一个分组排序的 DataSet。分组键可以被指定为键选择器函数或字段位置键（关于如何指定键，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-grouped-dataset">Reduce 示例</a>）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// Return the first five (arbitrary) elements of the DataSet
</span><span class="c1"></span><span class="k">val</span> <span class="n">out1</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">first</span><span class="o">(</span><span class="mi">5</span><span class="o">)</span>

<span class="c1">// Return the first two (arbitrary) elements of each String group
</span><span class="c1"></span><span class="k">val</span> <span class="n">out2</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">first</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>

<span class="c1">// Return the first three elements of each String group ordered by the Integer field
</span><span class="c1"></span><span class="k">val</span> <span class="n">out3</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">sortGroup</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Flink Dataset API 编程指南]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Flink Dataset Api Programming Guide</blockquote><h2 id="flink-dataset-api-编程指南">Flink DataSet API 编程指南</h2>
<p>Flink 中的数据集程序是对数据集实现转换（如过滤、映射、加入、分组）的常规程序。数据集最初是从某些来源创建的（例如，通过读取文件，或从本地集合中创建）。结果通过汇返回，例如可以将数据写入（分布式）文件，或标准输出（例如命令行终端）。Flink 程序可以在各种环境下运行，独立运行，或者嵌入其他程序中。执行可以发生在本地 JVM 中，也可以发生在许多机器的集群中。</p>
<p>请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">DataStream API 概述</a>，了解 Flink API 的基本概念。该概述是针对 DataStream API 的，但这两个 API 的基本概念是一样的。</p>
<p>为了创建你自己的 Flink DataSet 程序，我们鼓励你从 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html#anatomy-of-a-flink-program">Flink 程序的骨架</a>开始，并逐步添加你自己的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#dataset-transformations">转换</a>。其余部分作为附加操作和高级功能的参考。</p>
<h3 id="程序示例">程序示例</h3>
<p>下面的程序是一个完整的、可以使用的 WordCount 的例子，你可以复制和粘贴代码在本地运行。你可以复制和粘贴代码在本地运行它。你只需要在你的项目中加入正确的 Flink 的库（参见与 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html">Flink 的链接</a>部分）并指定导入。然后你就可以开始了</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala._</span>

<span class="k">object</span> <span class="nc">WordCount</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
    <span class="k">val</span> <span class="n">text</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span>
      <span class="s">&#34;Who&#39;s there?&#34;</span><span class="o">,</span>
      <span class="s">&#34;I think I hear them. Stand, ho! Who&#39;s there?&#34;</span><span class="o">)</span>

    <span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">text</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">toLowerCase</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34;\\W+&#34;</span><span class="o">)</span> <span class="n">filter</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">nonEmpty</span> <span class="o">}</span> <span class="o">}</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">}</span>
      <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
      <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

    <span class="n">counts</span><span class="o">.</span><span class="n">print</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="dataset-转换">DataSet 转换</h3>
<p>数据转换将一个或多个 DataSet 转换为一个新的 DataSet。程序可以将多个转换组合成复杂的集合。</p>
<p>本节简要介绍了可用的转换。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html">转换文档</a>中有所有变换的完整描述和示例。</p>
<ul>
<li>Map</li>
</ul>
<p>接受一个元素，产生一个元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">toInt</span> <span class="o">}</span>
</code></pre></div><ul>
<li>FlatMap</li>
</ul>
<p>接受一个元素并产生零、一个或多个元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="n">str</span> <span class="k">=&gt;</span> <span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34; &#34;</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><ul>
<li>MapPartition</li>
</ul>
<p>在一个函数调用中转换一个并行分区。该函数以&quot;迭代器&quot;的形式获取分区，并可产生任意数量的结果值。每个分区的元素数量取决于平行度和之前的操作。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="n">in</span> <span class="k">=&gt;</span> <span class="n">in</span> <span class="n">map</span> <span class="o">{</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">}</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Filter</li>
</ul>
<p>对每个元素进行布尔函数评估，并保留那些函数返回真的元素。
重要：系统假设函数不会修改应用谓词的元素。违反这个假设会导致错误的结果。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">_</span> <span class="o">&gt;</span> <span class="mi">1000</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Reduce</li>
</ul>
<p>通过重复将两个元素合并为一个元素，将一组元素合并为一个元素。换算可以应用于一个完整的数据集，也可以应用于一个分组的数据集。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="k">_</span> <span class="o">+</span> <span class="k">_</span> <span class="o">}</span>
</code></pre></div><ul>
<li>ReduceGroup</li>
</ul>
<p>将一组元素合并成一个或多个元素。<code>ReduceGroup</code> 可以应用在一个完整的数据集上，也可以应用在一个分组的数据集上。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduceGroup</span> <span class="o">{</span> <span class="n">elements</span> <span class="k">=&gt;</span> <span class="n">elements</span><span class="o">.</span><span class="n">sum</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Aggregate</li>
</ul>
<p>将一组值聚合成一个值。<code>Aggregation</code> 函数可以被认为是内置的 <code>reduce</code> 函数。<code>Aggregate</code> 可以应用于一个完整的数据集，也可以应用于一个分组的数据集。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">SUM</span><span class="o">,</span> <span class="mi">0</span><span class="o">).</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">MIN</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
</code></pre></div><p>你也可以使用简写语法来进行 <code>minimum</code>, <code>maximum</code> 和 <code>sum</code> 的聚合。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">min</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
</code></pre></div><ul>
<li>Distinct</li>
</ul>
<p>返回数据集的不同元素。它从输入的 DataSet 中删除元素的所有字段或字段子集的重复条目。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">distinct</span><span class="o">()</span>
</code></pre></div><ul>
<li>Join</li>
</ul>
<p>通过创建所有键值相等的元素对来连接两个数据集。可以选择使用 <code>JoinFunction</code> 将一对元素变成一个元素，或者使用 <code>FlatJoinFunction</code> 将一对元素变成任意多个（包括无）元素。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#specifying-keys">键</a>部分了解如何定义连接键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// In this case tuple fields are used as keys. &#34;0&#34; is the join field on the first tuple
</span><span class="c1">// &#34;1&#34; is the join field on the second tuple.
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">input2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>你可以通过 Join Hints 指定运行时执行连接的方式。这些提示描述了连接是通过分区还是广播进行的，以及它是使用基于排序还是基于散列的算法。请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#join-algorithm-hints">转换指南</a>，了解可能的提示列表和示例。
如果没有指定提示，系统将尝试对输入大小进行估计，并根据这些估计选择最佳策略。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// This executes a join by broadcasting the first data set
</span><span class="c1">// using a hash table for the broadcast data
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">input2</span><span class="o">,</span> <span class="nc">JoinHint</span><span class="o">.</span><span class="nc">BROADCAST_HASH_FIRST</span><span class="o">)</span>
                   <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>请注意，连接转换只适用于等价连接。其他的连接类型需要使用 OuterJoin 或 CoGroup 来表达。</p>
<ul>
<li>OuterJoin</li>
</ul>
<p>在两个数据集上执行左联接、右联接或完全外联接。外联接与常规（内联接）类似，创建所有键值相同的元素对。此外，如果在另一侧没有找到匹配的键，&ldquo;外侧&quot;的记录（左、右或全联接时两者都有）将被保留。匹配的元素对（或一个元素和另一个输入的 <code>null</code> 值）被交给 <code>JoinFunction</code> 将这对元素变成单个元素，或交给 <code>FlatJoinFunction</code> 将这对元素变成任意多个（包括无）元素。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#specifying-keys">键</a>部分，了解如何定义连接键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">joined</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span><span class="n">right</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="o">{</span>
   <span class="o">(</span><span class="n">left</span><span class="o">,</span> <span class="n">right</span><span class="o">)</span> <span class="k">=&gt;</span>
     <span class="k">val</span> <span class="n">a</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">left</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="s">&#34;none&#34;</span> <span class="k">else</span> <span class="n">left</span><span class="o">.</span><span class="n">_1</span>
     <span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">right</span><span class="o">)</span>
  <span class="o">}</span>
</code></pre></div><ul>
<li>CoGroup</li>
</ul>
<p>减少操作的二维变体。在一个或多个字段上对每个输入进行分组，然后将分组合并。每一对组都会调用转换函数。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#specifying-keys">键</a>部分，了解如何定义 <code>coGroup</code> 键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data1</span><span class="o">.</span><span class="n">coGroup</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><ul>
<li>Cross</li>
</ul>
<p>建立两个输入的笛卡尔乘积（交叉乘积），创建所有元素对。可选择使用交叉函数将一对元素变成一个单一元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">data2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">data1</span><span class="o">.</span><span class="n">cross</span><span class="o">(</span><span class="n">data2</span><span class="o">)</span>
</code></pre></div><p>注意：<code>Cross</code> 可能是一个非常耗费计算的操作，甚至可以挑战大型计算集群！建议使用 <code>crossWithTiny()</code> 和 <code>crossWithHuge()</code> 来提示系统数据集的大小。</p>
<ul>
<li>
<p>Union</p>
</li>
<li>
<p>产生两个数据集的并集。</p>
</li>
</ul>
<pre><code class="language-raku" data-lang="raku">data.union(data2)
</code></pre><ul>
<li>Rebalance</li>
</ul>
<p>均匀地重新平衡数据集的并行分区，以消除数据倾斜。只有类似于 Map 的变换才可以跟随重新平衡(rebalance)变换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">data1</span><span class="o">.</span><span class="n">rebalance</span><span class="o">().</span><span class="n">map</span><span class="o">(...)</span>
</code></pre></div><ul>
<li>Hash-Partition</li>
</ul>
<p>在给定的键上对数据集进行散列分区。键可以被指定为位置键、表达式键和键选择函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">partitionByHash</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Range-Partition</li>
</ul>
<p>在给定的键上按照范围分割数据集。键可以被指定为位置键、表达式键和键选择函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">partitionByRange</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>自定义分区</li>
</ul>
<p>使用自定义的 <code>Partitioner</code> 函数，根据键将记录分配到特定的分区。键可以指定为位置键、表达式键和键选择函数。
注意：此方法仅适用于单个字段键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span>
  <span class="o">.</span><span class="n">partitionCustom</span><span class="o">(</span><span class="n">partitioner</span><span class="o">,</span> <span class="n">key</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Sort Partition</li>
</ul>
<p>按照指定的顺序对数据集的所有分区进行本地排序。字段可以指定为元组位置或字段表达式。对多个字段的排序是通过链式 <code>sortPartition()</code> 调用完成的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>First-n</li>
</ul>
<p>返回一个数据集的前 n 个（任意）元素。First-n 可以应用于一个常规数据集、一个分组数据集或一个分组排序数据集。分组键可以指定为键选择函数、元组位置或 case 类字段。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// regular data set
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
<span class="c1">// grouped data set
</span><span class="c1"></span><span class="k">val</span> <span class="n">result2</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
<span class="c1">// grouped-sorted data set
</span><span class="c1"></span><span class="k">val</span> <span class="n">result3</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">sortGroup</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
</code></pre></div><p>以下转换可用于元组的数据集。</p>
<ul>
<li>MinBy / MaxBy</li>
</ul>
<p>从一组元组中选择一个元组，这些元组的一个或多个字段的值是最小的（最大的）。用于比较的字段必须是有效的键字段，即可比较。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。MinBy (MaxBy)可以应用于一个完整的数据集或一个分组数据集。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// a data set with a single tuple with minimum values for the Int and String fields.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
<span class="c1">// a data set with one tuple for each group with the minimum value for the Double field.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
                                             <span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>通过匿名模式匹配从 tuple、case 类和集合中提取，比如下面。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">temperature</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="o">}</span>
</code></pre></div><p>不受 API 开箱即用的支持。要使用这个功能，你应该使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/scala_api_extensions.html">Scala API 扩展</a>。</p>
<p>变换的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/parallel.html">并行度</a>可以通过 <code>setParallelism(int)</code> 来定义，而 <code>name(String)</code> 可以给变换指定一个自定义的名称，这对调试很有帮助。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#data-sources">数据源</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#data-sinks">数据接收器</a>也是如此。</p>
<p><code>withParameters(Configuration)</code> 传递 Configuration 对象，这些对象可以从用户函数里面的 <code>open()</code> 方法访问。</p>
<h2 id="指定键">指定键</h2>
<p>一些转换（join、coGroup、groupBy）需要在元素集合上定义一个键。其他转换（Reduce、GroupReduce、Aggregate）允许在应用之前将数据按键分组。</p>
<p>一个 DataSet 被分组为：</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;...&gt;</span> <span class="n">input</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;...&gt;</span> <span class="n">reduced</span> <span class="o">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="cm">/*define key here*/</span><span class="o">)</span>
  <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="cm">/*do something*/</span><span class="o">);</span>
</code></pre></div><p>Flink 的数据模型不是基于键值对的。因此，你不需要将数据集类型物理地打包成键和值。键是&quot;虚拟的&rdquo;：它们被定义为实际数据上的函数，以指导分组操作符。</p>
<h3 id="为元组定义键">为元组定义键</h3>
<p>最简单的情况是对 Tuple 的一个或多个字段进行分组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">keyed</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
</code></pre></div><p>元组在第一个字段（整数类型的字段）上进行分组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">grouped</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>在这里，我们将元组放在一个由第一个字段和第二个字段组成的复合键上。</p>
<p>关于嵌套 Tuple 的说明。如果你的 DataSet 有一个嵌套的元组，比如：</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple3</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Float</span><span class="o">&gt;,</span><span class="n">String</span><span class="o">,</span><span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">ds</span><span class="o">;</span>
</code></pre></div><p>指定 <code>groupBy(0)</code> 将使系统使用完整的 Tuple2 作为键（以 Integer 和 Float 为键）。如果要&quot;导航&quot;到嵌套的 Tuple2 中，就必须使用字段表达式键，下面将对其进行说明。</p>
<h3 id="使用字段表达式定义键">使用字段表达式定义键</h3>
<p>你可以使用基于字符串的字段表达式来引用嵌套的字段，并为分组、排序、连接(join)或 coGrouping 定义键。</p>
<p>字段表达式可以非常容易地选择（嵌套的）复合类型中的字段，如 Tuple 和 POJO 类型。</p>
<p>在下面的例子中，我们有一个有两个字段 &ldquo;word&rdquo; 和 &ldquo;count&rdquo; 的 WC POJO。要按字段 <code>word</code> 进行分组，我们只需将其名称传递给 <code>groupBy()</code> 函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO (Plain old Java Object)
</span><span class="c1"></span><span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="k">var</span> <span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="k">var</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span> <span class="k">this</span><span class="o">(</span><span class="s">&#34;&#34;</span><span class="o">,</span> <span class="mi">0L</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">)</span>

<span class="c1">// or, as a case class, which is less typing
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="字段表达式语法">字段表达式语法</h3>
<ul>
<li>
<p>通过字段名选择 POJO 字段。例如 &ldquo;user&rdquo; 指的是 POJO 类型的 &ldquo;user&rdquo; 字段。</p>
</li>
<li>
<p>通过 1-offset 字段名或 0-offset 字段索引来选择 Tuple 字段。例如 &ldquo;_1&rdquo; 和 &ldquo;5&rdquo; 分别指 Scala Tuple 类型的第一和第六字段。</p>
</li>
</ul>
<p>你可以在 POJO 和 Tuple 中选择嵌套字段。例如 &ldquo;user.zip&rdquo; 指的是 POJO 的 &ldquo;zip&rdquo; 字段，它存储在 POJO 类型的 &ldquo;user&rdquo; 字段中。支持 POJO 和 Tuple 的任意嵌套和混合，如 &ldquo;_2.user.zip&rdquo; 或 &ldquo;user._4.1.zip&rdquo;。</p>
<p>你可以使用 &ldquo;_&rdquo; 通配符表达式选择完整的类型。这也适用于不是 Tuple 或 POJO 类型的类型。</p>
<p>字段表达式示例:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="k">var</span> <span class="n">complex</span><span class="k">:</span> <span class="kt">ComplexNestedClass</span><span class="o">,</span> <span class="k">var</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span> <span class="k">this</span><span class="o">(</span><span class="kc">null</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span>

<span class="k">class</span> <span class="nc">ComplexNestedClass</span><span class="o">(</span>
    <span class="k">var</span> <span class="n">someNumber</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>
    <span class="n">someFloat</span><span class="k">:</span> <span class="kt">Float</span><span class="o">,</span>
    <span class="n">word</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">,</span> <span class="nc">String</span><span class="o">),</span>
    <span class="n">hadoopCitizen</span><span class="k">:</span> <span class="kt">IntWritable</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span> <span class="k">this</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="s">&#34;&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">IntWritable</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span> <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>这些都是上面例子代码的有效字段表达式。</p>
<ul>
<li>
<p>&ldquo;count&rdquo;: WC 类中的计数字段</p>
</li>
<li>
<p>&ldquo;complex&rdquo;: 递归选择 POJO 类型 <code>ComplexNestedClass</code> 的 <code>complex</code> 字段的所有字段。</p>
</li>
<li>
<p>&ldquo;complex.word._3&rdquo;: 选择嵌套的 Tuple3 的最后一个字段。</p>
</li>
<li>
<p>&ldquo;complex.hadoopCitizen&rdquo;: 选择 Hadoop <code>IntWritable</code> 类型。</p>
</li>
</ul>
<h3 id="使用键选择函数定义键">使用键选择函数定义键</h3>
<p>另一种定义键的方法是&quot;键选择器&quot;函数。键选择器函数将一个元素作为输入，并返回该元素的键。键可以是任何类型的，并且可以从确定性计算中得到。</p>
<p>下面的例子显示了一个简单返回对象字段的键选择函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary case class
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">keyed</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span> <span class="k">_</span><span class="o">.</span><span class="n">word</span> <span class="o">)</span>
</code></pre></div><h2 id="数据源">数据源</h2>
<p>数据源创建初始数据集，例如从文件或 Java 集合中创建。创建数据集的一般机制是在 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/io/InputFormat.java">InputFormat</a> 后面抽象出来的。Flink 自带了几种内置的格式来从常见的文件格式创建数据集。其中许多格式在 ExecutionEnvironment 上有快捷方法。</p>
<p>基于文件的:</p>
<ul>
<li>
<p><code>readTextFile(path) / TextInputFormat</code> - 读取文件并以字符串形式返回。</p>
</li>
<li>
<p><code>readTextFileWithValue(path) / TextValueInputFormat</code> - 以行的方式读取文件并以 <code>StringValues</code> 的形式返回。<code>StringValues</code> 是可变字符串。</p>
</li>
<li>
<p><code>readCsvFile(path) / CsvInputFormat</code> - 解析以逗号（或其他字符）分隔的文件。返回一个由 tuple、case 类对象或 POJOs 组成的 DataSet。支持基本的 java 类型及其对应的 Value 类型作为字段类型。</p>
</li>
<li>
<p><code>readFileOfPrimitives(path, delimiter) / PrimitiveInputFormat</code> - 使用给定的定界符，解析新行（或其他字符序列）定界的基元数据类型的文件，如 String 或 Integer。</p>
</li>
<li>
<p><code>readSequenceFile(Key, Value, path) / SequenceFileInputFormat</code> - 创建一个 JobConf 并从指定的路径读取文件，文件类型为 <code>SequenceFileInputFormat</code>，Key 类和 Value 类，并以 <code>Tuple2&lt;Key, Value&gt;</code> 的形式返回。</p>
</li>
</ul>
<p>基于集合的:</p>
<ul>
<li>
<p><code>fromCollection(Iterable)</code> - 从一个 Iterable 创建一个数据集。Iterable 返回的所有元素必须是相同的类型。</p>
</li>
<li>
<p><code>fromCollection(Iterator)</code> - 从一个 Iterator 创建一个数据集。该类指定了迭代器返回的元素的数据类型。</p>
</li>
<li>
<p><code>fromElements(elements: _*)</code> - 从给定的对象序列中创建一个数据集。所有对象必须是相同的类型。</p>
</li>
<li>
<p><code>fromParallelCollection(SplittableIterator)</code> - 从迭代器中并行创建一个数据集。该类指定了迭代器返回的元素的数据类型。</p>
</li>
<li>
<p><code>generateSequence(from, to)</code> - 在给定的区间内并行生成数字序列。</p>
</li>
</ul>
<p>通用的:</p>
<ul>
<li>
<p><code>readFile(inputFormat, path) / FileInputFormat</code> - 接受一个文件输入格式。</p>
</li>
<li>
<p><code>createInput(inputFormat) / InputFormat</code> - 接受一个通用的输入格式。</p>
</li>
</ul>
<p>示例:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span>  <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// read text file from local files system
</span><span class="c1"></span><span class="k">val</span> <span class="n">localLines</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;file:///path/to/my/textfile&#34;</span><span class="o">)</span>

<span class="c1">// read text file from an HDFS running at nnHost:nnPort
</span><span class="c1"></span><span class="k">val</span> <span class="n">hdfsLines</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;hdfs://nnHost:nnPort/path/to/my/textfile&#34;</span><span class="o">)</span>

<span class="c1">// read a CSV file with three fields
</span><span class="c1"></span><span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)](</span><span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">)</span>

<span class="c1">// read a CSV file with five fields, taking only two of them
</span><span class="c1"></span><span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)](</span>
  <span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">,</span>
  <span class="n">includedFields</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">3</span><span class="o">))</span> <span class="c1">// take the first and the fourth field
</span><span class="c1"></span>
<span class="c1">// CSV input can also be used with Case Classes
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">MyCaseClass</span><span class="o">(</span><span class="n">str</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">dbl</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>
<span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[</span><span class="kt">MyCaseClass</span><span class="o">](</span>
  <span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">,</span>
  <span class="n">includedFields</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">3</span><span class="o">))</span> <span class="c1">// take the first and the fourth field
</span><span class="c1"></span>
<span class="c1">// read a CSV file with three fields into a POJO (Person) with corresponding fields
</span><span class="c1"></span><span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[</span><span class="kt">Person</span><span class="o">](</span>
  <span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">,</span>
  <span class="n">pojoFields</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">&#34;name&#34;</span><span class="o">,</span> <span class="s">&#34;age&#34;</span><span class="o">,</span> <span class="s">&#34;zipcode&#34;</span><span class="o">))</span>

<span class="c1">// create a set from some given elements
</span><span class="c1"></span><span class="k">val</span> <span class="n">values</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="s">&#34;Foo&#34;</span><span class="o">,</span> <span class="s">&#34;bar&#34;</span><span class="o">,</span> <span class="s">&#34;foobar&#34;</span><span class="o">,</span> <span class="s">&#34;fubar&#34;</span><span class="o">)</span>

<span class="c1">// generate a number sequence
</span><span class="c1"></span><span class="k">val</span> <span class="n">numbers</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">generateSequence</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">10000000</span><span class="o">)</span>

<span class="c1">// read a file from the specified path of type SequenceFileInputFormat
</span><span class="c1"></span><span class="k">val</span> <span class="n">tuples</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">createInput</span><span class="o">(</span><span class="nc">HadoopInputs</span><span class="o">.</span><span class="n">readSequenceFile</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">IntWritable</span><span class="o">],</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">Text</span><span class="o">],</span>
 <span class="s">&#34;hdfs://nnHost:nnPort/path/to/file&#34;</span><span class="o">))</span>
</code></pre></div><h3 id="配置-csv-解析">配置 CSV 解析</h3>
<p>Flink 为 CSV 解析提供了许多配置选项。</p>
<ul>
<li>
<p>lineDelimiter: 字符串指定单个记录的定界符。默认的行定界符是新行字符 <code>'/n'</code>。</p>
</li>
<li>
<p>fieldDelimiter: 字符串指定分隔记录字段的定界符。默认的字段定界符是逗号字符 <code>','</code>。</p>
</li>
<li>
<p>includeFields: <code>Array[Int]</code> 定义从输入文件中读取哪些字段（以及忽略哪些字段）。默认情况下，前 n 个字段（由 <code>type()</code> 调用中的类型数定义）会被解析。</p>
</li>
<li>
<p>pojoFields: <code>Array[String]</code> 指定 POJO 的字段，这些字段被映射到 CSV 字段。CSV 字段的解析器会根据 POJO 字段的类型和顺序自动初始化。</p>
</li>
<li>
<p>parseQuotedStrings: 启用引号字符串解析的字符。如果字符串字段的第一个字符是引号字符，那么字符串将被解析为引号字符串（前导或尾部的空白不被修剪）。引号字符串中的字段定界符会被忽略。如果引号字符串字段的最后一个字符不是引号字符，则引号字符串解析失败。如果启用了引号字符串解析，且字段的第一个字符不是引号字符串，则该字符串将被解析为未引号字符串。默认情况下，引号字符串解析被禁用。</p>
</li>
<li>
<p>ignoreComments: 字符串指定一个注解前缀。所有以指定注解前缀开始的行都不会被解析和忽略。默认情况下，没有行被忽略。</p>
</li>
<li>
<p>lenient：布尔值，启用宽松解析。也就是说，不能正确解析的行会被忽略。默认情况下，禁用宽松解析，无效行会引发异常。</p>
</li>
<li>
<p>ignoreFirstLine: Boolean 配置 InputFormat 忽略输入文件的第一行。默认情况下，没有行被忽略。</p>
</li>
</ul>
<h3 id="input-path-的递归遍历">Input Path 的递归遍历</h3>
<p>对于基于文件的输入，当输入路径是一个目录时，默认情况下不会枚举嵌套文件。取而代之的是，只读取基础目录内的文件，而忽略嵌套文件。嵌套文件的递归枚举可以通过 <code>recursive.file.enumeration</code> 配置参数启用，就像下面的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// enable recursive enumeration of nested input files
</span><span class="c1"></span><span class="k">val</span> <span class="n">env</span>  <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// create a configuration object
</span><span class="c1"></span><span class="k">val</span> <span class="n">parameters</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span>

<span class="c1">// set the recursive enumeration parameter
</span><span class="c1"></span><span class="n">parameters</span><span class="o">.</span><span class="n">setBoolean</span><span class="o">(</span><span class="s">&#34;recursive.file.enumeration&#34;</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>

<span class="c1">// pass the configuration to the data source
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;file:///path/with.nested/files&#34;</span><span class="o">).</span><span class="n">withParameters</span><span class="o">(</span><span class="n">parameters</span><span class="o">)</span>
</code></pre></div><h3 id="读取压缩文件">读取压缩文件</h3>
<p>Flink 目前支持输入文件的透明解压，如果这些文件被标记为适当的文件扩展名。特别是，这意味着无需进一步配置输入格式，任何 <code>FileInputFormat</code> 都支持压缩，包括自定义输入格式。请注意，压缩文件可能不会被并行读取，从而影响作业的可扩展性。</p>
<p>下表列出了当前支持的压缩方法。</p>
<table>
<thead>
<tr>
<th style="text-align:left">压缩方法</th>
<th style="text-align:left">文件后缀</th>
<th style="text-align:left">并行性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">DEFLATE</td>
<td style="text-align:left">.deflate</td>
<td style="text-align:left">no</td>
</tr>
<tr>
<td style="text-align:left">GZip</td>
<td style="text-align:left">.gz, .gzip</td>
<td style="text-align:left">no</td>
</tr>
<tr>
<td style="text-align:left">Bzip2</td>
<td style="text-align:left">.bz2</td>
<td style="text-align:left">no</td>
</tr>
<tr>
<td style="text-align:left">XZ</td>
<td style="text-align:left">.xz</td>
<td style="text-align:left">no</td>
</tr>
</tbody>
</table>
<h2 id="数据接收器">数据接收器</h2>
<p>数据接收器消费 DataSet 并用于存储或返回它们。数据接收器的操作是用 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/io/OutputFormat.java">OutputFormat</a> 来描述的。Flink 带有各种内置的输出格式，这些格式被封装在对 DataSet 的操作后面。</p>
<ul>
<li><code>writeAsText() / TextOutputFormat</code> &ndash;将元素逐行写成 Strings。通过调用每个元素的 <code>toString()</code> 方法获得字符串。</li>
<li><code>writeAsCsv(...) / CsvOutputFormat</code> - 将元组写成逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的 <code>toString()</code> 方法。</li>
<li><code>print() / printToErr()</code> - 在标准输出/标准错误流上打印每个元素的 <code>toString()</code> 值。</li>
<li><code>write() / FileOutputFormat</code> - 用于自定义文件输出的方法和基类。支持自定义对象到字节的转换。</li>
<li><code>output()/ OutputFormat</code> - 最通用的输出方法，用于非基于文件的数据接收器（如将结果存储在数据库中）。</li>
</ul>
<p>一个 DataSet 可以被输入到多个操作中。程序可以写入或打印一个数据集，同时还可以对其进行额外的转换。</p>
<p><strong>示例</strong></p>
<p>标准数据接收器方法:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// text data
</span><span class="c1"></span><span class="k">val</span> <span class="n">textData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// write DataSet to a file on the local file system
</span><span class="c1"></span><span class="n">textData</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///my/result/on/localFS&#34;</span><span class="o">)</span>

<span class="c1">// write DataSet to a file on an HDFS with a namenode running at nnHost:nnPort
</span><span class="c1"></span><span class="n">textData</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;hdfs://nnHost:nnPort/my/result/on/localFS&#34;</span><span class="o">)</span>

<span class="c1">// write DataSet to a file and overwrite the file if it exists
</span><span class="c1"></span><span class="n">textData</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///my/result/on/localFS&#34;</span><span class="o">,</span> <span class="nc">WriteMode</span><span class="o">.</span><span class="nc">OVERWRITE</span><span class="o">)</span>

<span class="c1">// tuples as lines with pipe as the separator &#34;a|b|c&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">values</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">values</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="s">&#34;file:///path/to/the/result/file&#34;</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34;|&#34;</span><span class="o">)</span>

<span class="c1">// this writes tuples in the text formatting &#34;(a, b, c)&#34;, rather than as CSV lines
</span><span class="c1"></span><span class="n">values</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///path/to/the/result/file&#34;</span><span class="o">)</span>

<span class="c1">// this writes values as strings using a user-defined formatting
</span><span class="c1"></span><span class="n">values</span> <span class="n">map</span> <span class="o">{</span> <span class="n">tuple</span> <span class="k">=&gt;</span> <span class="n">tuple</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="s">&#34; - &#34;</span> <span class="o">+</span> <span class="n">tuple</span><span class="o">.</span><span class="n">_2</span> <span class="o">}</span>
  <span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///path/to/the/result/file&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="本地排序输出">本地排序输出</h3>
<p>数据接收器的输出可以使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-for-tuples">元组字段位置</a>或<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>对指定字段按指定顺序进行本地排序。这适用于每一种输出格式。</p>
<p>下面的示例展示了如何使用该功能。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">tData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">pData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">BookPojo</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">sData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// sort output on String field in ascending order
</span><span class="c1"></span><span class="n">tData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>

<span class="c1">// sort output on Double field in descending and Int field in ascending order
</span><span class="c1"></span><span class="n">tData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">).</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>

<span class="c1">// sort output on the &#34;author&#34; field of nested BookPojo in descending order
</span><span class="c1"></span><span class="n">pData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="s">&#34;_1.author&#34;</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">).</span><span class="n">writeAsText</span><span class="o">(...)</span>

<span class="c1">// sort output on the full tuple in ascending order
</span><span class="c1"></span><span class="n">tData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="s">&#34;_&#34;</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">writeAsCsv</span><span class="o">(...)</span>

<span class="c1">// sort atomic type (String) output in descending order
</span><span class="c1"></span><span class="n">sData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="s">&#34;_&#34;</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">).</span><span class="n">writeAsText</span><span class="o">(...)</span>
</code></pre></div><p>目前还不支持全局排序输出。</p>
<h2 id="迭代运算符">迭代运算符</h2>
<p>迭代在  Flink 程序中实现了循环。迭代运算符封装了程序的一部分，并反复执行，将一次迭代的结果（部分解）反馈到下一次迭代中。Flink 中的迭代有两种类型。<code>BulkIteration</code> 和 <code>DeltaIteration</code>。</p>
<p>本节提供了如何使用这两种运算符的快速示例。查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">迭代介绍页面</a>可以获得更详细的介绍。</p>
<h3 id="批量迭代">批量迭代</h3>
<p>要创建一个 <code>BulkIteration</code>，调用迭代开始的 DataSet 的 <code>iterate(int)</code> 方法，同时指定一个 <code>step</code> 函数。<code>step</code> 函数获取当前迭代的输入 DataSet，并且必须返回一个新的 DataSet。迭代调用的参数是最大的迭代次数，迭代过后要停止。</p>
<p>还有 <code>iterateWithTermination(int)</code> 函数，接受 <code>step</code> 函数，返回两个 DataSets。迭代步骤的结果和一个终止标准。一旦终止准则 DataSet 为空，就会停止迭代。</p>
<p>下面的例子是迭代估计数字 Pi。目标是计算随机点的数量，这些随机点落入单位圆中。在每一次迭代中，都会挑选一个随机点。如果这个点位于单位圆内，我们就递增计数。然后，Pi 的估计值是所得到的计数除以迭代次数乘以 4。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>

<span class="c1">// Create initial DataSet
</span><span class="c1"></span><span class="k">val</span> <span class="n">initial</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="n">count</span> <span class="k">=</span> <span class="n">initial</span><span class="o">.</span><span class="n">iterate</span><span class="o">(</span><span class="mi">10000</span><span class="o">)</span> <span class="o">{</span> <span class="n">iterationInput</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">iterationInput</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">i</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">x</span> <span class="k">=</span> <span class="nc">Math</span><span class="o">.</span><span class="n">random</span><span class="o">()</span>
    <span class="k">val</span> <span class="n">y</span> <span class="k">=</span> <span class="nc">Math</span><span class="o">.</span><span class="n">random</span><span class="o">()</span>
    <span class="n">i</span> <span class="o">+</span> <span class="o">(</span><span class="k">if</span> <span class="o">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="o">)</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="n">result</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">count</span> <span class="n">map</span> <span class="o">{</span> <span class="n">c</span> <span class="k">=&gt;</span> <span class="n">c</span> <span class="o">/</span> <span class="mf">10000.0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">}</span>

<span class="n">result</span><span class="o">.</span><span class="n">print</span><span class="o">()</span>

<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">(</span><span class="s">&#34;Iterative Pi Example&#34;</span><span class="o">)</span>
</code></pre></div><p>你也可以查看 <a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/clustering/KMeans.scala">K-Means</a> 的例子，它使用 <code>BulkIteration</code> 来聚类一组未标记的点。</p>
<h3 id="增量迭代">增量迭代</h3>
<p>增量迭代利用了某些算法在每次迭代中不改变解的每个数据点的事实。</p>
<p>除了在每次迭代中反馈的部分解（称为 workset），delta 迭代还保持着跨迭代的状态（称为解集），可以通过 delta 更新。迭代计算的结果是最后一次迭代后的状态。关于 delta 迭代的基本原理，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">迭代简介</a>。</p>
<p>定义 <code>DeltaIteration</code> 与定义 <code>BulkIteration</code> 类似。对于 delta 迭代，两个数据集构成了每次迭代的输入（工作集和解集），并且在每次迭代中产生两个数据集作为结果（新工作集，解集 delta）。</p>
<p>要创建一个 DeltaIteration 在初始解集上调用 <code>iterateDelta(initialWorkset，maxIterations，key)</code>。<code>step</code> 函数需要两个参数。(solutionSet, workset), 并且必须返回两个值: (solutionSetDelta, newWorkset).</p>
<p>下面是一个 delta 迭代语法的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// read the initial data sets
</span><span class="c1"></span><span class="k">val</span> <span class="n">initialSolutionSet</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">initialWorkset</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">maxIterations</span> <span class="k">=</span> <span class="mi">100</span>
<span class="k">val</span> <span class="n">keyPosition</span> <span class="k">=</span> <span class="mi">0</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">initialSolutionSet</span><span class="o">.</span><span class="n">iterateDelta</span><span class="o">(</span><span class="n">initialWorkset</span><span class="o">,</span> <span class="n">maxIterations</span><span class="o">,</span> <span class="nc">Array</span><span class="o">(</span><span class="n">keyPosition</span><span class="o">))</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">solution</span><span class="o">,</span> <span class="n">workset</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">candidateUpdates</span> <span class="k">=</span> <span class="n">workset</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="nc">ComputeCandidateChanges</span><span class="o">())</span>
    <span class="k">val</span> <span class="n">deltas</span> <span class="k">=</span> <span class="n">candidateUpdates</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">solution</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)(</span><span class="k">new</span> <span class="nc">CompareChangesToCurrent</span><span class="o">())</span>

    <span class="k">val</span> <span class="n">nextWorkset</span> <span class="k">=</span> <span class="n">deltas</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">new</span> <span class="nc">FilterByThreshold</span><span class="o">())</span>

    <span class="o">(</span><span class="n">deltas</span><span class="o">,</span> <span class="n">nextWorkset</span><span class="o">)</span>
<span class="o">}</span>

<span class="n">result</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">outputPath</span><span class="o">)</span>

<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><h2 id="在函数中对数据对象进行操作">在函数中对数据对象进行操作</h2>
<p>Flink 的运行时以 Java 对象的形式与用户函数交换数据。函数从运行时接收输入对象作为方法参数，并返回输出对象作为结果。因为这些对象是由用户函数和运行时代码访问的，所以理解和遵循用户代码如何访问，即读取和修改这些对象的规则是非常重要的。</p>
<p>用户函数以常规方法参数（如 MapFunction）或通过 Iterable 参数（如 GroupReduceFunction）从 Flink 的运行时接收对象。我们把运行时传递给用户函数的对象称为输入对象。用户函数可以将对象作为方法返回值（像 MapFunction）或通过 Collector（像 FlatMapFunction）发射给 Flink 运行时。我们将用户函数向运行时发射的对象称为输出对象。</p>
<p>Flink 的 DataSet API 具有两种模式，它们在 Flink 的运行时如何创建或重用输入对象方面有所不同。这种行为会影响用户函数如何与输入和输出对象交互的保证和约束。下面的章节定义了这些规则，并给出了编写安全用户函数代码的编码指南。</p>
<h3 id="禁用对象重用default">禁用对象重用(DEFAULT)</h3>
<p>默认情况下，Flink 在禁用对象重用模式下运行。这种模式可以保证函数在函数调用中总是接收新的输入对象。对象重用禁用模式能提供更好的保证，使用起来也更安全。但是，它有一定的处理开销，可能会引起较高的 Java 垃圾收集活动。下表解释了在禁用对象重用模式下，用户函数如何访问输入和输出对象。</p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">保证和限制</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">读取输入对象</td>
<td style="text-align:left">在一个方法调用中，保证输入对象的值不会改变。这包括由 Iterable 服务的对象。例如，在 List 或 Map 中收集由 Iterable 服务的输入对象是安全的。请注意，在方法调用离开后，对象可能会被修改。跨函数调用记忆对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">修改输入对象</td>
<td style="text-align:left">你可以修改输入对象。</td>
</tr>
<tr>
<td style="text-align:left">发射输入对象</td>
<td style="text-align:left">你可以发射输入对象。输入对象的值可能在发射后发生变化。读取发射后的输入对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">读取输出对象</td>
<td style="text-align:left">给予收集器的对象或作为方法结果返回的对象可能已经改变了其值。读取输出对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">修改输出对象</td>
<td style="text-align:left">你可以在对象被发射后对其进行修改，然后再次发射。</td>
</tr>
</tbody>
</table>
<p>对象重用禁用（默认）模式的编码准则。</p>
<ul>
<li>不要跨方法调用记忆和读取输入对象。</li>
<li>不要在发出对象后读取对象。</li>
</ul>
<h3 id="启用对象重用">启用对象重用</h3>
<p>在启用对象重用模式下，Flink 的运行时会尽量减少对象实例化的数量。这可以提高性能，并且可以减少 Java 垃圾收集的压力。通过调用 <code>ExecutionConfig.enableObjectReuse()</code> 激活对象重用启用模式。下表解释了在启用对象重用模式下，用户函数如何访问输入和输出对象。</p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">保证和限制</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">读取作为常规方法参数接收的输入对象</td>
<td style="text-align:left">作为常规方法参数接收的输入对象在一次函数调用中不被修改。对象可能在方法调用结束后被修改。跨函数调用记忆对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">读取从 Iterable 参数中接收到的输入对象</td>
<td style="text-align:left">从 Iterable 中接收到的输入对象只在调用 next()方法之前有效。一个 Iterable 或 Iterator 可以多次服务于同一个对象实例。记住从 Iterable 接收的输入对象是不安全的，例如，把它们放在 List 或 Map 中。</td>
</tr>
<tr>
<td style="text-align:left">修改输入对象</td>
<td style="text-align:left">除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(reuse)的输入对象外，你不得修改输入对象。</td>
</tr>
<tr>
<td style="text-align:left">发射输入对象</td>
<td style="text-align:left">除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(重用)的输入对象外，你不得发射输入对象。</td>
</tr>
<tr>
<td style="text-align:left">读取输出对象</td>
<td style="text-align:left">一个被交给 Collector 或作为方法结果返回的对象可能已经改变了它的值。读取输出对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">修改输出对象</td>
<td style="text-align:left">你可以修改一个输出对象并再次发出它。</td>
</tr>
</tbody>
</table>
<p>启用对象重用的编码准则。</p>
<ul>
<li>不记忆从 Iterable 接收的输入对象。</li>
<li>不记忆和读取跨方法调用的输入对象。</li>
<li>除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(reuse)的输入对象外，不要修改或发出输入对象。</li>
<li>为了减少对象实例化，你总是可以发出一个专门的输出对象，这个对象被反复修改，但从不读取。</li>
</ul>
<h2 id="调试">调试</h2>
<p>在分布式集群中的大型数据集上运行数据分析程序之前，最好确保所实现的算法能够按照预期的方式运行。因此，实现数据分析程序通常是一个检查结果、调试和改进的渐进过程。</p>
<p>Flink 提供了一些不错的功能，通过支持 IDE 内的本地调试、注入测试数据和收集结果数据，大大简化了数据分析程序的开发过程。本节给大家一些提示，如何简化 Flink 程序的开发。</p>
<h3 id="本地执行环境">本地执行环境</h3>
<p>LocalEnvironment 在它创建的同一个 JVM 进程中启动 Flink 系统。如果你从 IDE 中启动 LocalEnvironment，你可以在代码中设置断点，轻松调试你的程序。</p>
<p>LocalEnvironment 的创建和使用方法如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">createLocalEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="n">pathToTextFile</span><span class="o">)</span>
<span class="c1">// build your program
</span><span class="c1"></span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><h3 id="收集数据源和接收器">收集数据源和接收器</h3>
<p>为分析程序提供输入并检查其输出，如果通过创建输入文件和读取输出文件来完成，是很麻烦的。Flink 具有特殊的数据源和接收器，这些数据源和接收器由 Java 集合支持，以方便测试。一旦程序经过测试，源和接收器可以很容易地被从 HDFS 等外部数据存储中读取/写入的源和接收器所替代。</p>
<p>集合数据源可以使用以下方式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">createLocalEnvironment</span><span class="o">()</span>

<span class="c1">// Create a DataSet from a list of elements
</span><span class="c1"></span><span class="k">val</span> <span class="n">myInts</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">)</span>

<span class="c1">// Create a DataSet from any Collection
</span><span class="c1"></span><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">myTuples</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="c1">// Create a DataSet from an Iterator
</span><span class="c1"></span><span class="k">val</span> <span class="n">longIt</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">myLongs</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="n">longIt</span><span class="o">)</span>
</code></pre></div><p>注：目前，集合数据源要求数据类型和迭代器实现 Serializable。此外，集合数据源不能并行执行（ parallelism = 1）。</p>
<h2 id="语义注解">语义注解</h2>
<p>语义注解可以用来给 Flink 提供关于函数行为的提示。它们告诉系统，函数读取并评估了函数输入的哪些字段，以及它将哪些字段从输入转发到输出，而没有进行修改。语义注解是加快执行速度的有力手段，因为它们允许系统推理出在多个操作中重复使用排序顺序或分区的问题。使用语义注解最终可能会使程序免于不必要的数据洗牌或不必要的排序，并显著提高程序的性能。</p>
<p>注意：语义注解的使用是可选的。然而，在提供语义注解时，保守地使用语义注解是绝对关键的! 不正确的语义注解将导致 Flink 对你的程序做出不正确的假设，并可能最终导致不正确的结果。如果一个操作符的行为不是明确可预测的，就不应该提供注解。请仔细阅读文档。</p>
<p>目前支持以下语义注解。</p>
<p><strong>转发字段注解</strong></p>
<p>转发字段信息声明了未被修改的输入字段被函数转发到输出中的同一位置或另一位置。该信息被优化器用来推断数据属性（如排序或分区）是否被函数保留。对于对输入元素组进行操作的函数，如 GroupReduce、GroupCombine、CoGroup 和 MapPartition，所有被定义为转发字段的字段必须总是从同一个输入元素联合转发。由组智函数发出的每个元素的转发字段可能来源于函数的输入组的不同元素。</p>
<p>字段转发信息使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>来指定。在输出中转发到同一位置的字段可以通过其位置来指定。指定的位置必须对输入和输出的数据类型有效，并具有相同的类型。例如字符串 &ldquo;f2 &ldquo;声明 Java 输入元组的第三个字段总是等于输出元组中的第三个字段。</p>
<p>将输入中的源字段和输出中的目标字段指定为字段表达式，就可以声明未修改的字段转发到输出中的另一个位置。字符串 <code>&quot;f0-&gt;f2&quot;</code> 表示将 Java 输入元组的第一个字段不变的复制到 Java 输出元组的第三个字段。通配符表达式 <code>*</code> 可以用来指代整个输入或输出类型，即 <code>&quot;f0-&gt;*&quot;</code> 表示一个函数的输出总是等于其 Java 输入元组的第一个字段。</p>
<p>多个转发字段可以在一个字符串中用分号隔开声明为 <code>&quot;f0; f2-&gt;f1; f3-&gt;f2&quot;</code>，也可以在单独的字符串中声明为 &ldquo;f0&rdquo;、&ldquo;f2-&gt;f1&rdquo;、&ldquo;f3-&gt;f2&rdquo;。当指定转发字段时，不要求所有的转发字段都声明，但所有的声明必须正确。</p>
<p>转发字段信息可以通过在函数类定义上附加 Java 注解来声明，或者在调用 DataSet 上的函数后将其作为操作符参数传递，如下图所示。</p>
<p><strong>函数类注解</strong></p>
<ul>
<li><code>@ForwardedFields</code> 用于单输入的函数，如 Map 和 Reduce。</li>
<li><code>@ForwardedFieldsFirst</code> 代表有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>@ForwardedFieldsSecond</code> 代表有两个输入的函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p><strong>操作符参数</strong></p>
<ul>
<li><code>data.map(myMapFnc).withForwardedFields()</code> 用于单输入的函数，如 Map 和 Reduce。</li>
<li><code>data1.join(data2).where().equalTo().with(myJoinFnc).withForwardFieldsFirst()</code> 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>data1.join(data2).where().equalTo().with(myJoinFnc).withForwardFieldsSecond()</code> 用于有两个输入的函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p>请注意，不可能覆盖通过操作符参数指定为类注解的字段前向信息。</p>
<p>例子：在函数的第二个输入端，如 Join 和 CoGroup，请注意不能覆盖通过运算符参数指定的类注解的字段前向信息。</p>
<p>下面的例子显示了如何使用函数类注解来声明转发的字段信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nd">@ForwardedFields</span><span class="o">(</span><span class="s">&#34;_1-&gt;_3&#34;</span><span class="o">)</span>
<span class="k">class</span> <span class="nc">MyMap</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]{</span>
   <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">))</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">,</span> <span class="nc">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">return</span> <span class="o">(</span><span class="s">&#34;foo&#34;</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span> <span class="o">/</span> <span class="mi">2</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>非转发字段</strong></p>
<p>非转发字段信息声明了所有在函数输出中不保留在同一位置的字段。所有其他字段的值都被认为保留在输出的同一位置。因此，非转发字段信息与转发字段信息是相反的。分组运算符（如 GroupReduce、GroupCombine、CoGroup 和 MapPartition）的非转发字段信息必须满足与转发字段信息相同的要求。</p>
<p>重要：非转发字段信息的规范是可选的。但是如果使用，必须指定 <strong>ALL!</strong> 非转发字段，因为所有其他字段都被认为是原地转发的。将一个转发字段声明为非转发字段是安全的。</p>
<p>非转发字段被指定为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>的列表。这个列表既可以是由分号分隔的字段表达式组成的单个字符串，也可以是多个字符串。例如 &ldquo;f1; f3&rdquo; 和 &ldquo;f1&rdquo;、&ldquo;f3&rdquo; 都声明 Java 元组的第二个和第四个字段不保留在原地，其他所有字段都保留在原地。非前向字段信息只能为输入和输出类型相同的函数指定。</p>
<p>非转发字段信息是作为函数类注解使用以下注解来指定的。</p>
<ul>
<li><code>@NonForwardedFields</code> 用于单个输入函数，如 Map 和 Reduce。</li>
<li><code>@NonForwardedFieldsFirst</code> 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>@NonForwardedFieldsSecond</code> 用于函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p>例子</p>
<p>下面的例子显示了如何声明非转发字段信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nd">@NonForwardedFields</span><span class="o">(</span><span class="s">&#34;_2&#34;</span><span class="o">)</span> <span class="c1">// second field is not forwarded
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyMap</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]{</span>
  <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">))</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">return</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span> <span class="o">/</span> <span class="mi">2</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>读取字段</strong></p>
<p>读取字段信息声明了所有被函数访问和评估的字段，也就是说，所有被函数用来计算结果的字段。例如，在条件语句中被评估的字段或用于计算的字段必须在指定读取字段信息时被标记为读取。仅仅是未经修改就转发到输出而不评估其值的字段，或者根本没有被访问的字段都不被认为是读。</p>
<p>重要：读取字段信息的指定是可选的。但是如果使用，必须指定 <strong>ALL!</strong> 读取字段。将一个非读字段声明为读字段是安全的。</p>
<p>读取字段被指定为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>的列表。这个列表可以是一个由分号分隔的字段表达式组成的单个字符串，也可以是多个字符串。例如 &ldquo;f1; f3&rdquo; 和 &ldquo;f1&rdquo;、&ldquo;f3&rdquo; 都声明 Java 元组的第二和第四字段被函数读取和评估。</p>
<p>读取字段信息是以函数类注解的形式指定的，使用以下注解。</p>
<ul>
<li><code>@ReadFields</code> 用于单输入函数，如 Map 和 Reduce。</li>
<li><code>@ReadFieldsFirst</code> 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>@ReadFieldsSecond</code> 用于有两个输入的函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p>示例：</p>
<p>下面的例子显示了如何声明读取字段信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nd">@ReadFields</span><span class="o">(</span><span class="s">&#34;_1; _4&#34;</span><span class="o">)</span> <span class="c1">// _1 and _4 are read and evaluated by the function.
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyMap</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]{</span>
   <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">,</span> <span class="nc">Int</span><span class="o">,</span> <span class="nc">Int</span><span class="o">))</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_1</span> <span class="o">==</span> <span class="mi">42</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">return</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="k">return</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_4</span> <span class="o">+</span> <span class="mi">10</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h2 id="广播变量">广播变量</h2>
<p>广播变量允许你在操作的常规输入之外，将一个数据集提供给操作的所有并行实例。这对辅助数据集或数据依赖性参数化很有用。然后，该数据集将作为一个集合在操作者处被访问。</p>
<ul>
<li>广播：广播集通过 <code>withBroadcastSet(DataSet，String)</code> 按名称注册，并通过</li>
<li>访问方式：通过目标操作者处的 <code>getRuntimeContext().getBroadcastVariable(String)</code> 访问。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// 1. The DataSet to be broadcast
</span><span class="c1"></span><span class="k">val</span> <span class="n">toBroadcast</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>

<span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="s">&#34;b&#34;</span><span class="o">)</span>

<span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">new</span> <span class="nc">RichMapFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]()</span> <span class="o">{</span>
    <span class="k">var</span> <span class="n">broadcastSet</span><span class="k">:</span> <span class="kt">Traversable</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">config</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="c1">// 3. Access the broadcast DataSet as a Collection
</span><span class="c1"></span>      <span class="n">broadcastSet</span> <span class="k">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="n">getBroadcastVariable</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;broadcastSetName&#34;</span><span class="o">).</span><span class="n">asScala</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">in</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">{</span>
        <span class="o">...</span>
    <span class="o">}</span>
<span class="o">}).</span><span class="n">withBroadcastSet</span><span class="o">(</span><span class="n">toBroadcast</span><span class="o">,</span> <span class="s">&#34;broadcastSetName&#34;</span><span class="o">)</span> <span class="c1">// 2. Broadcast the DataSet
</span></code></pre></div><p>在注册和访问广播数据集时，确保名称（前面例子中的 <code>broadcastSetName</code>）匹配。关于完整的示例程序，可以看一下 <a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/clustering/KMeans.scala">KMeans 算法</a>。</p>
<p>注意：由于广播变量的内容在每个节点上都保存在内存中，所以它不应该变得太大。对于像标量值这样简单的东西，你可以简单地将参数作为函数闭包的一部分，或者使用 <code>withParameters(...)</code> 方法来传递配置。</p>
<h2 id="分布式缓存">分布式缓存</h2>
<p>Flink 提供了一个类似于 Apache Hadoop 的分布式缓存，以使用户函数的并行实例可以在本地访问文件。该功能可用于共享包含静态外部数据的文件，如字典或机器学习的回归模型。</p>
<p>缓存的工作原理如下。程序在其 <code>ExecutionEnvironment</code> 中以特定的名称将<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/connectors.html#reading-from-file-systems">本地或远程文件系统（如 HDFS 或 S3）</a>的文件或目录注册为缓存文件。当程序执行时，Flink 会自动将该文件或目录复制到所有工作者的本地文件系统中。用户函数可以查找指定名称下的文件或目录，并从工作者的本地文件系统中访问它。</p>
<p>分布式缓存的使用方法如下。</p>
<p>在 <code>ExecutionEnvironment</code> 中注册文件或目录。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// register a file from HDFS
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">registerCachedFile</span><span class="o">(</span><span class="s">&#34;hdfs:///path/to/your/file&#34;</span><span class="o">,</span> <span class="s">&#34;hdfsFile&#34;</span><span class="o">)</span>

<span class="c1">// register a local executable file (script, executable, ...)
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">registerCachedFile</span><span class="o">(</span><span class="s">&#34;file:///path/to/exec/file&#34;</span><span class="o">,</span> <span class="s">&#34;localExecFile&#34;</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>

<span class="c1">// define your program and execute
</span><span class="c1"></span><span class="o">...</span>
<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyMapper</span><span class="o">())</span>
<span class="o">...</span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><p>在一个用户函数（这里是 MapFunction）中访问缓存文件。该函数必须扩展一个 RichFunction 类，因为它需要访问 RuntimeContext。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// extend a RichFunction to have access to the RuntimeContext
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyMapper</span> <span class="k">extends</span> <span class="nc">RichMapFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">config</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>

    <span class="c1">// access cached file via RuntimeContext and DistributedCache
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">myFile</span><span class="k">:</span> <span class="kt">File</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">.</span><span class="n">getDistributedCache</span><span class="o">.</span><span class="n">getFile</span><span class="o">(</span><span class="s">&#34;hdfsFile&#34;</span><span class="o">)</span>
    <span class="c1">// read the file (or navigate the directory)
</span><span class="c1"></span>    <span class="o">...</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// use content of cached file
</span><span class="c1"></span>    <span class="o">...</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h2 id="向函数传递参数">向函数传递参数</h2>
<p>可以使用构造函数或 <code>withParameters(Configuration)</code> 方法将参数传递给函数。参数会被序列化为函数对象的一部分，并传送给所有并行任务实例。</p>
<p><strong>通过构造函数</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">toFilter</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>

<span class="n">toFilter</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyFilter</span><span class="o">(</span><span class="mi">2</span><span class="o">))</span>

<span class="k">class</span> <span class="nc">MyFilter</span><span class="o">(</span><span class="n">limit</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">FilterFunction</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">filter</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">value</span> <span class="o">&gt;</span> <span class="n">limit</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>通过 withParameters(配置)</strong></p>
<p>本方法以一个 Configuration 对象作为参数，它将被传递给<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html#rich-functions">富函数</a>的 <code>open()</code> 方法。配置对象是一个从 String 键到不同值类型的 Map。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">toFilter</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>

<span class="k">val</span> <span class="n">c</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">()</span>
<span class="n">c</span><span class="o">.</span><span class="n">setInteger</span><span class="o">(</span><span class="s">&#34;limit&#34;</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>

<span class="n">toFilter</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">new</span> <span class="nc">RichFilterFunction</span><span class="o">[</span><span class="kt">Int</span><span class="o">]()</span> <span class="o">{</span>
    <span class="k">var</span> <span class="n">limit</span> <span class="k">=</span> <span class="mi">0</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">config</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="n">limit</span> <span class="k">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getInteger</span><span class="o">(</span><span class="s">&#34;limit&#34;</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">filter</span><span class="o">(</span><span class="n">in</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">in</span> <span class="o">&gt;</span> <span class="n">limit</span>
    <span class="o">}</span>
<span class="o">}).</span><span class="n">withParameters</span><span class="o">(</span><span class="n">c</span><span class="o">)</span>
</code></pre></div><p><strong>在全局范围内通过 ExecutionConfig</strong></p>
<p>Flink 还允许将自定义配置值传递到环境的 ExecutionConfig 接口。由于执行配置可以在所有（丰富的）用户函数中访问，因此自定义配置将在所有函数中全局可用。</p>
<p>设置一个自定义的全局配置：</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">()</span>
<span class="n">conf</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;mykey&#34;</span><span class="o">,</span> <span class="s">&#34;myvalue&#34;</span><span class="o">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">getConfig</span><span class="o">.</span><span class="n">setGlobalJobParameters</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span>
</code></pre></div><p>请注意，你也可以传递一个扩展 <code>ExecutionConfig.GlobalJobParameters</code> 类的自定义类作为全局作业参数给执行配置。该接口允许实现 <code>Map&lt;String, String&gt; toMap()</code> 方法，该方法将在 web 前端显示来自配置的值。</p>
<p><strong>从全局配置中访问值</strong></p>
<p>全局工作参数中的对象在系统中的很多地方都可以访问。所有实现 RichFunction 接口的用户函数都可以通过运行时上下文访问。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kd">class</span> <span class="nc">Tokenizer</span> <span class="kd">extends</span> <span class="n">RichFlatMapFunction</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="o">{</span>

    <span class="kd">private</span> <span class="n">String</span> <span class="n">mykey</span><span class="o">;</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">open</span><span class="o">(</span><span class="n">Configuration</span> <span class="n">parameters</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
      <span class="kd">super</span><span class="o">.</span><span class="na">open</span><span class="o">(</span><span class="n">parameters</span><span class="o">);</span>
      <span class="n">ExecutionConfig</span><span class="o">.</span><span class="na">GlobalJobParameters</span> <span class="n">globalParams</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getExecutionConfig</span><span class="o">().</span><span class="na">getGlobalJobParameters</span><span class="o">();</span>
      <span class="n">Configuration</span> <span class="n">globConf</span> <span class="o">=</span> <span class="o">(</span><span class="n">Configuration</span><span class="o">)</span> <span class="n">globalParams</span><span class="o">;</span>
      <span class="n">mykey</span> <span class="o">=</span> <span class="n">globConf</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="s">&#34;mykey&#34;</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>
    <span class="o">}</span>
    <span class="c1">// ... more here ...
</span></code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/guide" term="guide" label="Guide" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Hadoop 的兼容性]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Hadoop Compatibility Beta</blockquote><h2 id="hadoop-兼容性测试版">Hadoop 兼容性测试版</h2>
<p>Flink 与 Apache Hadoop MapReduce 接口兼容，因此允许重用为 Hadoop MapReduce 实现的代码。</p>
<p>您可以:</p>
<ul>
<li>在 Flink 程序中使用 Hadoop 的可写<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html#supported-data-types">数据类型</a>。</li>
<li>使用任何 Hadoop InputFormat 作为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#data-sources">数据源</a>。</li>
<li>使用任何 Hadoop 输出格式作为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#data-sinks">数据接收器</a>。</li>
<li>将 Hadoop Mapper 用作 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#flatmap">FlatMapFunction</a>。</li>
<li>使用 Hadoop Reducer 作为 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#groupreduce-on-grouped-dataset">GroupReduceFunction</a>。</li>
</ul>
<p>本文档展示了如何将现有的 Hadoop MapReduce 代码与 Flink 一起使用。从 Hadoop 支持的文件系统读取代码，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/filesystems/index.html#hadoop-file-system-hdfs-and-its-other-implementations">连接到其他系统</a>指南。</p>
<h3 id="项目配置">项目配置</h3>
<p>对 Hadoop 输入/输出格式的支持是 flink-java 和 flink-scala Maven 模块的一部分，这些模块在编写 Flink 作业时总是需要的。这些代码位于 <code>org.apache.flink.api.java.hadoop</code> 和 <code>org.apache.flink.api.scala.hadoop</code> 中的 mapred 和 mapreduce API 的附加子包中。</p>
<p>对 Hadoop Mappers 和 Reducers 的支持包含在 <code>flink-hadoop-compatibility</code> Maven 模块中。这段代码位于 <code>org.apache.flink.hadoopcompatibility</code> 包中。</p>
<p>如果您想重用 Mappers 和 Reducers，请在 pom.xml 中添加以下依赖关系。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
	<span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
	<span class="nt">&lt;artifactId&gt;</span>flink-hadoop-compatibility_2.11<span class="nt">&lt;/artifactId&gt;</span>
	<span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>另请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html#add-hadoop-classpaths">如何配置 hadoop 依赖关系</a>。</p>
<h3 id="使用-hadoop-输入格式">使用 Hadoop 输入格式</h3>
<p>要使用 Flink 的 Hadoop InputFormats，必须先使用 HadoopInputs 实用程序类的 readHadoopFile 或 createHadoopInput 来包装格式。前者用于从 FileInputFormat 派生的输入格式，而后者必须用于通用的输入格式。通过使用 <code>ExecutionEnvironmen#createInput</code>，产生的 InputFormat 可以用来创建数据源。</p>
<p>生成的 DataSet 包含 2 个元组，其中第一个字段是键，第二个字段是从 Hadoop InputFormat 中检索的值。</p>
<p>下面的示例展示了如何使用 Hadoop 的 TextInputFormat。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">LongWritable</span>, <span class="kt">Text</span><span class="o">)]</span> <span class="k">=</span>
  <span class="n">env</span><span class="o">.</span><span class="n">createInput</span><span class="o">(</span><span class="nc">HadoopInputs</span><span class="o">.</span><span class="n">readHadoopFile</span><span class="o">(</span>
                    <span class="k">new</span> <span class="nc">TextInputFormat</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">LongWritable</span><span class="o">],</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">Text</span><span class="o">],</span> <span class="n">textPath</span><span class="o">))</span>

<span class="c1">// Do something with the data.
</span><span class="c1"></span><span class="o">[</span><span class="kt">...</span><span class="o">]</span>
</code></pre></div><h3 id="使用-hadoop-输出格式">使用 Hadoop 输出格式</h3>
<p>Flink 为 Hadoop OutputFormat 提供了一个兼容性封装器，它支持任何实现 org.apache.hadoop.mapred.OutputFormat 或扩展 org.apache.hadoop.mapreduce.OutputFormat 的类。OutputFormat 包装器希望它的输入数据是一个包含2个key和value的 DataSet。这些数据将由 Hadoop OutputFormat 处理。</p>
<p>下面的示例展示了如何使用 Hadoop 的 TextOutputFormat。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Obtain your result to emit.
</span><span class="c1"></span><span class="k">val</span> <span class="n">hadoopResult</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Text</span>, <span class="kt">IntWritable</span><span class="o">)]</span> <span class="k">=</span> <span class="o">[</span><span class="kt">...</span><span class="o">]</span>

<span class="k">val</span> <span class="n">hadoopOF</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HadoopOutputFormat</span><span class="o">[</span><span class="kt">Text</span>,<span class="kt">IntWritable</span><span class="o">](</span>
  <span class="k">new</span> <span class="nc">TextOutputFormat</span><span class="o">[</span><span class="kt">Text</span>, <span class="kt">IntWritable</span><span class="o">],</span>
  <span class="k">new</span> <span class="nc">JobConf</span><span class="o">)</span>

<span class="n">hadoopOF</span><span class="o">.</span><span class="n">getJobConf</span><span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&#34;mapred.textoutputformat.separator&#34;</span><span class="o">,</span> <span class="s">&#34; &#34;</span><span class="o">)</span>
<span class="nc">FileOutputFormat</span><span class="o">.</span><span class="n">setOutputPath</span><span class="o">(</span><span class="n">hadoopOF</span><span class="o">.</span><span class="n">getJobConf</span><span class="o">,</span> <span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">resultPath</span><span class="o">))</span>

<span class="n">hadoopResult</span><span class="o">.</span><span class="n">output</span><span class="o">(</span><span class="n">hadoopOF</span><span class="o">)</span>
</code></pre></div><h3 id="使用-hadoop-mappers-和-reducers">使用 Hadoop Mappers 和 Reducers</h3>
<p>Hadoop Mappers 在语义上等同于 Flink 的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#flatmap">FlatMapFunctions</a>，Hadoop Reducers 等同于 Flink 的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#groupreduce-on-grouped-dataset">GroupReduceFunctions</a>。Flink 为 Hadoop MapReduce 的 Mapper 和 Reducer 接口的实现提供了封装器，也就是说，你可以在常规的 Flink 程序中重用你的 Hadoop Mapper 和 Reducer。目前，只支持 Hadoop 的 mapred API（org.apache.hadoop.mapred）的 Mapper 和 Reduce 接口。</p>
<p>包装器将一个 <code>DataSet&lt;Tuple2&lt;KEYIN,VALUEIN&gt;</code> 作为输入，并产生一个 <code>DataSet&lt;Tuple2&lt;KEYOUT,VALUEOUT&gt;</code> 作为输出，其中 KEYIN 和 KEYOUT 是键，VALUEIN 和 VALUEOUT 是 Hadoop 函数处理的 Hadoop 键值对的值。对于 Reducers，Flink 提供了一个包装器，用于带（HadoopReduceCombineFunction）和不带 Combiner（HadoopReduceFunction）的 GroupReduceFunction。包装器接受一个可选的 JobConf 对象来配置 Hadoop Mapper 或 Reducer。</p>
<p>Flink 的函数包装器有:</p>
<ul>
<li>sorg.apache.flink.hadoopcompatibility.mapred.HadoopMapFunction,</li>
<li>sorg.apache.flink.hadoopcompatibility.mapred.HadoopReduceFunction, 和</li>
<li>sorg.apache.flink.hadoopcompatibility.mapred.HadoopReduceCombineFunction.</li>
</ul>
<p>并可作为常规的 Flink <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#flatmap">FlatMapFunctions</a> 或 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#groupreduce-on-grouped-dataset">GroupReduceFunctions</a> 使用。</p>
<p>下面的例子展示了如何使用 Hadoop Mapper 和 Reducer 函数:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="c1">// Obtain data to process somehow.
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="o">[...]</span>

<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">text</span>
  <span class="c1">// use Hadoop Mapper (Tokenizer) as MapFunction
</span><span class="c1"></span>  <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopMapFunction</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">Tokenizer</span><span class="o">()</span>
  <span class="o">))</span>
  <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="n">0</span><span class="o">)</span>
  <span class="c1">// use Hadoop Reducer (Counter) as Reduce- and CombineFunction
</span><span class="c1"></span>  <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopReduceCombineFunction</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">Counter</span><span class="o">(),</span> <span class="k">new</span> <span class="n">Counter</span><span class="o">()</span>
  <span class="o">));</span>
</code></pre></div><p>请注意：Reducer 包装器工作在 Flink 的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#transformations-on-grouped-dataset">groupBy()</a> 操作所定义的组上。它不考虑您在 JobConf 中设置的任何自定义分区器、排序或分组比较器。</p>
<h3 id="完整的-hadoop-wordcount-示例">完整的 Hadoop WordCount 示例</h3>
<p>下面的示例展示了使用 Hadoop 数据类型、Input-和 OutputFormats 以及 Mapper 和 Reducer 实现的完整 WordCount 实现。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

<span class="c1">// Set up the Hadoop TextInputFormat.
</span><span class="c1"></span><span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="n">Job</span><span class="o">.</span><span class="na">getInstance</span><span class="o">();</span>
<span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="n">hadoopIF</span> <span class="o">=</span>
  <span class="k">new</span> <span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">TextInputFormat</span><span class="o">(),</span> <span class="n">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">job</span>
  <span class="o">);</span>
<span class="n">TextInputFormat</span><span class="o">.</span><span class="na">addInputPath</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="n">inputPath</span><span class="o">));</span>

<span class="c1">// Read data using the Hadoop TextInputFormat.
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">createInput</span><span class="o">(</span><span class="n">hadoopIF</span><span class="o">);</span>

<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">text</span>
  <span class="c1">// use Hadoop Mapper (Tokenizer) as MapFunction
</span><span class="c1"></span>  <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopMapFunction</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">Tokenizer</span><span class="o">()</span>
  <span class="o">))</span>
  <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="n">0</span><span class="o">)</span>
  <span class="c1">// use Hadoop Reducer (Counter) as Reduce- and CombineFunction
</span><span class="c1"></span>  <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopReduceCombineFunction</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">Counter</span><span class="o">(),</span> <span class="k">new</span> <span class="n">Counter</span><span class="o">()</span>
  <span class="o">));</span>

<span class="c1">// Set up the Hadoop TextOutputFormat.
</span><span class="c1"></span><span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;</span> <span class="n">hadoopOF</span> <span class="o">=</span>
  <span class="k">new</span> <span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">TextOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(),</span> <span class="n">job</span>
  <span class="o">);</span>
<span class="n">hadoopOF</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">set</span><span class="o">(</span><span class="s">&#34;mapreduce.output.textoutputformat.separator&#34;</span><span class="o">,</span> <span class="s">&#34; &#34;</span><span class="o">);</span>
<span class="n">TextOutputFormat</span><span class="o">.</span><span class="na">setOutputPath</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="n">outputPath</span><span class="o">));</span>

<span class="c1">// Emit data using the Hadoop TextOutputFormat.
</span><span class="c1"></span><span class="n">result</span><span class="o">.</span><span class="na">output</span><span class="o">(</span><span class="n">hadoopOF</span><span class="o">);</span>

<span class="c1">// Execute Program
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="s">&#34;Hadoop WordCount&#34;</span><span class="o">);</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/hadoop_compatibility.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/hadoop_compatibility.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[批处理例子]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Batch Examples</blockquote><h2 id="batch-示例">Batch 示例</h2>
<p>下面的示例程序展示了 Flink 的不同应用，从简单的单词计数到图形算法。这些代码样本说明了 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">Flink 的 DataSet API</a> 的使用。</p>
<p>以下和更多例子的完整源代码可以在 Flink 源码库的 <a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-batch">flink-examples-batch</a> 模块中找到。</p>
<h3 id="运行一个例子">运行一个例子</h3>
<p>为了运行一个 Flink 实例，我们假设你有一个正在运行的 Flink 实例。导航中的 &ldquo;Quickstart&rdquo; 和 &ldquo;Setup&rdquo; 选项卡描述了启动 Flink 的各种方法。</p>
<p>最简单的方法是运行 <code>./bin/start-cluster.sh</code>，默认情况下，它用一个 JobManager 和一个 TaskManager 启动一个本地集群。</p>
<p>Flink 的每个二进制版本都包含一个例子目录，其中有本页每个例子的 jar 文件。</p>
<p>要运行 WordCount 示例，请发出以下命令。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">./bin/flink run ./examples/batch/WordCount.jar
</code></pre></div><p>其他的例子也可以用类似的方式启动。</p>
<p>请注意，许多例子在运行时没有传递任何参数，而是使用内置的数据。要使用真实数据运行 WordCount，你必须传递数据的路径。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">./bin/flink run ./examples/batch/WordCount.jar --input /path/to/some/text/data --output /path/to/result
</code></pre></div><p>请注意，非本地文件系统需要一个模式前缀，如 <code>hdfs://</code>。</p>
<h3 id="wordcount">WordCount</h3>
<p>WordCount 是大数据处理系统中的 &ldquo;Hello World&rdquo;。它计算文本集合中的单词频率。该算法分两步工作。首先，文本被分割成单个单词。第二，对单词进行分组和计数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// get input data
</span><span class="c1"></span><span class="k">val</span> <span class="n">text</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;/path/to/file&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">text</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">toLowerCase</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34;\\W+&#34;</span><span class="o">)</span> <span class="n">filter</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">nonEmpty</span> <span class="o">}</span> <span class="o">}</span>
  <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">}</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

<span class="n">counts</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">outputPath</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34; &#34;</span><span class="o">)</span>
</code></pre></div><p><a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/wordcount/WordCount.scala">WordCount 的例子</a>实现了上面描述的算法，输入参数：<code>--input &lt;path&gt; --output &lt;path&gt;</code>。作为测试数据，任何文本文件都可以。</p>
<h3 id="页面排名">页面排名</h3>
<p>PageRank 算法计算由链接定义的图中页面的&quot;重要性&quot;，这些链接从一个页面指向另一个页面。它是一种迭代图算法，这意味着它反复应用相同的计算。在每一次迭代中，每个页面将其当前的排名分布在所有的邻居上，并计算其新的排名，作为它从邻居那里得到的排名的累加和。PageRank 算法是由 Google 搜索引擎推广的，它利用网页的重要性来对搜索查询的结果进行排名。</p>
<p>在这个简单的例子中，PageRank 的实现方式是<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">批量迭代</a>和固定的迭代次数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// User-defined types
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">Link</span><span class="o">(</span><span class="n">sourceId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">targetId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span>
<span class="k">case</span> <span class="k">class</span> <span class="nc">Page</span><span class="o">(</span><span class="n">pageId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">rank</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>
<span class="k">case</span> <span class="k">class</span> <span class="nc">AdjacencyList</span><span class="o">(</span><span class="n">sourceId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">targetIds</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Long</span><span class="o">])</span>

<span class="c1">// set up execution environment
</span><span class="c1"></span><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// read the pages and initial ranks by parsing a CSV file
</span><span class="c1"></span><span class="k">val</span> <span class="n">pages</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[</span><span class="kt">Page</span><span class="o">](</span><span class="n">pagesInputPath</span><span class="o">)</span>

<span class="c1">// the links are encoded as an adjacency list: (page-id, Array(neighbor-ids))
</span><span class="c1"></span><span class="k">val</span> <span class="n">links</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[</span><span class="kt">Link</span><span class="o">](</span><span class="n">linksInputPath</span><span class="o">)</span>

<span class="c1">// assign initial ranks to pages
</span><span class="c1"></span><span class="k">val</span> <span class="n">pagesWithRanks</span> <span class="k">=</span> <span class="n">pages</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="nc">Page</span><span class="o">(</span><span class="n">p</span><span class="o">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">numPages</span><span class="o">))</span>

<span class="c1">// build adjacency list from link input
</span><span class="c1"></span><span class="k">val</span> <span class="n">adjacencyLists</span> <span class="k">=</span> <span class="n">links</span>
  <span class="c1">// initialize lists
</span><span class="c1"></span>  <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">e</span> <span class="k">=&gt;</span> <span class="nc">AdjacencyList</span><span class="o">(</span><span class="n">e</span><span class="o">.</span><span class="n">sourceId</span><span class="o">,</span> <span class="nc">Array</span><span class="o">(</span><span class="n">e</span><span class="o">.</span><span class="n">targetId</span><span class="o">)))</span>
  <span class="c1">// concatenate lists
</span><span class="c1"></span>  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;sourceId&#34;</span><span class="o">).</span><span class="n">reduce</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">l1</span><span class="o">,</span> <span class="n">l2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="nc">AdjacencyList</span><span class="o">(</span><span class="n">l1</span><span class="o">.</span><span class="n">sourceId</span><span class="o">,</span> <span class="n">l1</span><span class="o">.</span><span class="n">targetIds</span> <span class="o">++</span> <span class="n">l2</span><span class="o">.</span><span class="n">targetIds</span><span class="o">)</span>
  <span class="o">}</span>

<span class="c1">// start iteration
</span><span class="c1"></span><span class="k">val</span> <span class="n">finalRanks</span> <span class="k">=</span> <span class="n">pagesWithRanks</span><span class="o">.</span><span class="n">iterateWithTermination</span><span class="o">(</span><span class="n">maxIterations</span><span class="o">)</span> <span class="o">{</span>
  <span class="n">currentRanks</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">newRanks</span> <span class="k">=</span> <span class="n">currentRanks</span>
      <span class="c1">// distribute ranks to target pages
</span><span class="c1"></span>      <span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">adjacencyLists</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;pageId&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;sourceId&#34;</span><span class="o">)</span> <span class="o">{</span>
        <span class="o">(</span><span class="n">page</span><span class="o">,</span> <span class="n">adjacent</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Page</span><span class="o">])</span> <span class="k">=&gt;</span>
        <span class="k">for</span> <span class="o">(</span><span class="n">targetId</span> <span class="k">&lt;-</span> <span class="n">adjacent</span><span class="o">.</span><span class="n">targetIds</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">Page</span><span class="o">(</span><span class="n">targetId</span><span class="o">,</span> <span class="n">page</span><span class="o">.</span><span class="n">rank</span> <span class="o">/</span> <span class="n">adjacent</span><span class="o">.</span><span class="n">targetIds</span><span class="o">.</span><span class="n">length</span><span class="o">))</span>
        <span class="o">}</span>
      <span class="o">}</span>
      <span class="c1">// collect ranks and sum them up
</span><span class="c1"></span>      <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;pageId&#34;</span><span class="o">).</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">SUM</span><span class="o">,</span> <span class="s">&#34;rank&#34;</span><span class="o">)</span>
      <span class="c1">// apply dampening factor
</span><span class="c1"></span>      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">p</span> <span class="k">=&gt;</span>
        <span class="nc">Page</span><span class="o">(</span><span class="n">p</span><span class="o">.</span><span class="n">pageId</span><span class="o">,</span> <span class="o">(</span><span class="n">p</span><span class="o">.</span><span class="n">rank</span> <span class="o">*</span> <span class="nc">DAMPENING_FACTOR</span><span class="o">)</span> <span class="o">+</span> <span class="o">((</span><span class="mi">1</span> <span class="o">-</span> <span class="nc">DAMPENING_FACTOR</span><span class="o">)</span> <span class="o">/</span> <span class="n">numPages</span><span class="o">))</span>
      <span class="o">}</span>

    <span class="c1">// terminate if no rank update was significant
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">termination</span> <span class="k">=</span> <span class="n">currentRanks</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">newRanks</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;pageId&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;pageId&#34;</span><span class="o">)</span> <span class="o">{</span>
      <span class="o">(</span><span class="n">current</span><span class="o">,</span> <span class="n">next</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span> <span class="k">=&gt;</span>
        <span class="c1">// check for significant update
</span><span class="c1"></span>        <span class="k">if</span> <span class="o">(</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="o">(</span><span class="n">current</span><span class="o">.</span><span class="n">rank</span> <span class="o">-</span> <span class="n">next</span><span class="o">.</span><span class="n">rank</span><span class="o">)</span> <span class="o">&gt;</span> <span class="nc">EPSILON</span><span class="o">)</span> <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="o">(</span><span class="n">newRanks</span><span class="o">,</span> <span class="n">termination</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">finalRanks</span>

<span class="c1">// emit result
</span><span class="c1"></span><span class="n">result</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">outputPath</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34; &#34;</span><span class="o">)</span>
</code></pre></div><p><a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/graph/PageRankBasic.scala">PageRank 程序</a>实现了上述示例。它需要以下参数才能运行。<code>--pages &lt;path&gt; --links &lt;path&gt; --output &lt;path&gt; --numPages &lt;n&gt; --iterations &lt;n&gt;</code>。</p>
<p>输入文件是纯文本文件，必须按以下格式进行。</p>
<ul>
<li>页数用一个（长）ID 表示，用换行字符分隔。
<ul>
<li>例如 &ldquo;1/n2/n12/n42/n63/n&rdquo; 给出了 5 个 ID 为 1、2、12、42 和 63 的页面。</li>
</ul>
</li>
<li>链接用页面 ID 对表示，用空格分隔。链接用换行符分隔。
<ul>
<li>例如 &ldquo;1 2\n2 12\n1 12\n42 63\n&rdquo; 给出了四个(定向)链接(1)-&gt;(2)，(2)-&gt;(12)，(1)-&gt;(12)和(42)-&gt;(63)。</li>
</ul>
</li>
</ul>
<p>对于这个简单的实现，要求每个页面至少有一个入站链接和一个出站链接（一个页面可以指向自己）。</p>
<h3 id="连接的组件">连接的组件</h3>
<p>Connected Components 算法通过给同一连接部分中的所有顶点分配相同的组件 ID，来识别较大图中相互连接的部分。与 PageRank 类似，Connected Components 是一种迭代算法。在每一步中，每个顶点将其当前的组件 ID 传播给所有的邻居。如果一个顶点接受来自邻居的组件 ID，如果它小于自己的组件 ID。</p>
<p>本实现使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">增量迭代</a>。没有改变组件 ID 的顶点不参与下一步。这产生了更好的性能，因为后面的迭代通常只处理一些离群的顶点。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// set up execution environment
</span><span class="c1"></span><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// read vertex and edge data
</span><span class="c1">// assign the initial components (equal to the vertex id)
</span><span class="c1"></span><span class="k">val</span> <span class="n">vertices</span> <span class="k">=</span> <span class="n">getVerticesDataSet</span><span class="o">(</span><span class="n">env</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="n">id</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">id</span><span class="o">)</span> <span class="o">}</span>

<span class="c1">// undirected edges by emitting for each input edge the input edges itself and an inverted
</span><span class="c1">// version
</span><span class="c1"></span><span class="k">val</span> <span class="n">edges</span> <span class="k">=</span> <span class="n">getEdgesDataSet</span><span class="o">(</span><span class="n">env</span><span class="o">).</span><span class="n">flatMap</span> <span class="o">{</span> <span class="n">edge</span> <span class="k">=&gt;</span> <span class="nc">Seq</span><span class="o">(</span><span class="n">edge</span><span class="o">,</span> <span class="o">(</span><span class="n">edge</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">edge</span><span class="o">.</span><span class="n">_1</span><span class="o">))</span> <span class="o">}</span>

<span class="c1">// open a delta iteration
</span><span class="c1"></span><span class="k">val</span> <span class="n">verticesWithComponents</span> <span class="k">=</span> <span class="n">vertices</span><span class="o">.</span><span class="n">iterateDelta</span><span class="o">(</span><span class="n">vertices</span><span class="o">,</span> <span class="n">maxIterations</span><span class="o">,</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">s</span><span class="o">,</span> <span class="n">ws</span><span class="o">)</span> <span class="k">=&gt;</span>

    <span class="c1">// apply the step logic: join with the edges
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">allNeighbors</span> <span class="k">=</span> <span class="n">ws</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">edges</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">{</span> <span class="o">(</span><span class="n">vertex</span><span class="o">,</span> <span class="n">edge</span><span class="o">)</span> <span class="k">=&gt;</span>
      <span class="o">(</span><span class="n">edge</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">vertex</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="c1">// select the minimum neighbor
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">minNeighbors</span> <span class="k">=</span> <span class="n">allNeighbors</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">min</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

    <span class="c1">// update if the component of the candidate is smaller
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">updatedComponents</span> <span class="k">=</span> <span class="n">minNeighbors</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">s</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
      <span class="o">(</span><span class="n">newVertex</span><span class="o">,</span> <span class="n">oldVertex</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)])</span> <span class="k">=&gt;</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">newVertex</span><span class="o">.</span><span class="n">_2</span> <span class="o">&lt;</span> <span class="n">oldVertex</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">newVertex</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="c1">// delta and new workset are identical
</span><span class="c1"></span>    <span class="o">(</span><span class="n">updatedComponents</span><span class="o">,</span> <span class="n">updatedComponents</span><span class="o">)</span>
<span class="o">}</span>

<span class="n">verticesWithComponents</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">outputPath</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34; &#34;</span><span class="o">)</span>
</code></pre></div><p><a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/graph/ConnectedComponents.scala">ConnectedComponents 程序</a>实现了上面的例子。它需要以下参数才能运行: <code>--vertices &lt;path&gt; --edges &lt;path&gt; --output &lt;path&gt; --iterations &lt;n&gt;</code>。</p>
<p>输入文件是纯文本文件，必须按如下格式编写。</p>
<ul>
<li>顶点用 ID 表示，并用换行符隔开。
<ul>
<li>例如 &ldquo;1/n2/n12/n42/n63/n&rdquo; 给出了五个顶点，分别是(1)、(2)、(12)、(42)和(63)。</li>
</ul>
</li>
<li>边缘用一对顶点 ID 表示，这些顶点 ID 用空格字符分隔。边缘用换行符隔开。
<ul>
<li>例如，&ldquo;1 2/n2 12/n1 12/n42 63/n&rdquo; 给出了四个(非直接)联系(1)-(2)、(2)-(12)、(1)-(12)和(42)-(63)。</li>
</ul>
</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/examples.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/examples.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[数据集中的 zipping 元素]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Zipping Elements in a Dataset</blockquote><h2 id="zipping-数据集中的元素">Zipping 数据集中的元素</h2>
<p>在某些算法中，人们可能需要为数据集元素分配唯一的标识符。本文档介绍了如何将 <a href="https://github.com/apache/flink/blob/master//flink-java/src/main/java/org/apache/flink/api/java/utils/DataSetUtils.java">DataSetUtils</a> 用于该目的。</p>
<h3 id="使用密集索引进行-zip">使用密集索引进行 Zip</h3>
<p><code>zipWithIndex</code> 给元素分配连续的标签，接收一个数据集作为输入，并返回一个新的（唯一id，初始值）2-tuples的数据集。这个过程需要两次传递，先计数再给元素贴标签，而且由于计数的同步性，不能采用流水线方式。备选的 <code>zipWithUniqueId</code> 以流水线的方式工作，当唯一的标签已经足够时，首选 <code>zip</code>。例如，下面的代码。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala._</span>

<span class="k">val</span> <span class="n">env</span><span class="k">:</span> <span class="kt">ExecutionEnvironment</span> <span class="o">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="s">&#34;A&#34;</span><span class="o">,</span> <span class="s">&#34;B&#34;</span><span class="o">,</span> <span class="s">&#34;C&#34;</span><span class="o">,</span> <span class="s">&#34;D&#34;</span><span class="o">,</span> <span class="s">&#34;E&#34;</span><span class="o">,</span> <span class="s">&#34;F&#34;</span><span class="o">,</span> <span class="s">&#34;G&#34;</span><span class="o">,</span> <span class="s">&#34;H&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">zipWithIndex</span>

<span class="n">result</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">resultPath</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34;,&#34;</span><span class="o">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><p>可以得到元组: (0,G), (1,H), (2,A), (3,B), (4,C), (5,D), (6,E), (7,F)</p>
<h3 id="带有唯一标识符的-zip">带有唯一标识符的 Zip</h3>
<p>在许多情况下，人们可能不需要分配连续的标签，<code>zipWithUniqueId</code> 以流水线的方式工作，加快了标签分配过程。该方法接收一个数据集作为输入，并返回一个由（唯一id，初始值）2-tuples组成的新数据集。例如，下面的代码。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala._</span>

<span class="k">val</span> <span class="n">env</span><span class="k">:</span> <span class="kt">ExecutionEnvironment</span> <span class="o">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="s">&#34;A&#34;</span><span class="o">,</span> <span class="s">&#34;B&#34;</span><span class="o">,</span> <span class="s">&#34;C&#34;</span><span class="o">,</span> <span class="s">&#34;D&#34;</span><span class="o">,</span> <span class="s">&#34;E&#34;</span><span class="o">,</span> <span class="s">&#34;F&#34;</span><span class="o">,</span> <span class="s">&#34;G&#34;</span><span class="o">,</span> <span class="s">&#34;H&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">zipWithUniqueId</span>

<span class="n">result</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">resultPath</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34;,&#34;</span><span class="o">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><p>可以得到元组: (0,G), (1,A), (2,H), (3,B), (5,C), (7,D), (9,E), (11,F)</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/zip_elements_guide.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/zip_elements_guide.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[本地执行]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-local-execution/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Local Execution</blockquote><h2 id="本地执行">本地执行</h2>
<p>Flink 可以在一台机器上运行，甚至在一台 Java 虚拟机中运行。这使得用户可以在本地测试和调试 Flink 程序。本节将对本地执行机制进行概述。</p>
<p>本地环境和执行器允许你在本地 Java 虚拟机中运行 Flink 程序，或者作为现有程序的一部分在任何 JVM 中运行。大多数例子可以通过简单地点击 IDE 的&quot;运行&quot;按钮在本地启动。</p>
<p>Flink 中支持两种不同的本地执行。LocalExecutionEnvironment 是启动完整的 Flink 运行时，包括一个 JobManager 和一个 TaskManager。这些包括内存管理和所有在集群模式下执行的内部算法。</p>
<p>CollectionEnvironment 是在 Java 集合上执行 Flink 程序。这种模式不会启动完整的 Flink 运行时，所以执行的开销非常低，而且是轻量级的。例如，一个 <code>DataSet.map()</code> 转换将通过将 <code>map()</code> 函数应用于 Java 列表中的所有元素来执行。</p>
<h3 id="调试">调试</h3>
<p>如果你在本地运行 Flink 程序，你也可以像其他 Java 程序一样调试你的程序。你可以使用 <code>System.out.println()</code> 来写出一些内部变量，也可以使用调试器。可以在 <code>map()</code>、<code>reduce()</code> 和其他所有方法中设置断点。也请参考 Java API 文档中的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#debugging">调试部分</a>，了解 Java API 中的测试和本地调试工具的指南。</p>
<h3 id="maven-依赖">Maven 依赖</h3>
<p>如果你是在 Maven 项目中开发程序，你必须使用这个依赖关系添加 flink-clients 模块。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-clients_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><h3 id="本地环境">本地环境</h3>
<p>LocalEnvironment 是 Flink 程序本地执行的一个句柄。使用它可以在本地 JVM 中运行程序&ndash;独立或嵌入其他程序中。</p>
<p>本地环境是通过 <code>ExecutionEnvironment.createLocalEnvironment()</code> 方法实例化的。默认情况下，它将使用与你的机器有多少 CPU 核（硬件上下文）一样多的本地线程来执行。您也可以指定所需的并行度。本地环境可以配置为使用 <code>enableLogging()/disableLogging()</code> 将日志记录到控制台。</p>
<p>在大多数情况下，调用 <code>ExecutionEnvironment.getExecutionEnvironment()</code> 是更好的方法。当程序在本地（命令行接口之外）启动时，该方法会返回一个 <code>LocalEnvironment</code>，当程序被命令行接口调用时，该方法会返回一个预配置的集群执行环境。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
    <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">createLocalEnvironment</span><span class="o">();</span>

    <span class="n">DataSet</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">readTextFile</span><span class="o">(</span><span class="s">&#34;file:///path/to/file&#34;</span><span class="o">);</span>

    <span class="n">data</span>
        <span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="k">new</span> <span class="n">FilterFunction</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;()</span> <span class="o">{</span>
            <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">filter</span><span class="o">(</span><span class="n">String</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
                <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="na">startsWith</span><span class="o">(</span><span class="s">&#34;http://&#34;</span><span class="o">);</span>
            <span class="o">}</span>
        <span class="o">})</span>
        <span class="o">.</span><span class="na">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///path/to/result&#34;</span><span class="o">);</span>

    <span class="n">JobExecutionResult</span> <span class="n">res</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">();</span>
<span class="o">}</span>
</code></pre></div><p>在执行结束后返回的 JobExecutionResult 对象，包含了程序运行时间和累加器结果。</p>
<p>LocalEnvironment 还允许向 Flink 传递自定义配置值。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">Configuration</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
<span class="n">conf</span><span class="o">.</span><span class="na">setFloat</span><span class="o">(</span><span class="n">ConfigConstants</span><span class="o">.</span><span class="na">TASK_MANAGER_MEMORY_FRACTION_KEY</span><span class="o">,</span> <span class="n">0</span><span class="o">.</span><span class="na">5f</span><span class="o">);</span>
<span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">createLocalEnvironment</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span>
</code></pre></div><p>注意：本地执行环境不启动任何 Web 前端来监控执行。</p>
<h3 id="收集环境">收集环境</h3>
<p>使用 CollectionEnvironment 在 Java 集合上执行是一种执行 Flink 程序的低开销方法。这种模式的典型用例是自动测试、调试和代码重用。</p>
<p>用户可以使用为批处理而实现的算法，也可以用于交互性更强的情况。Flink 程序的一个稍微改变的变体可以用于 Java 应用服务器中处理传入的请求。</p>
<p>基于集合执行的骨架:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
    <span class="c1">// initialize a new Collection-based execution environment
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CollectionEnvironment</span><span class="o">();</span>

    <span class="n">DataSet</span><span class="o">&lt;</span><span class="n">User</span><span class="o">&gt;</span> <span class="n">users</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">fromCollection</span><span class="o">(</span> <span class="cm">/* get elements from a Java Collection */</span><span class="o">);</span>

    <span class="cm">/* Data Set transformations ... */</span>

    <span class="c1">// retrieve the resulting Tuple2 elements into a ArrayList.
</span><span class="c1"></span>    <span class="n">Collection</span><span class="o">&lt;...&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;...&gt;();</span>
    <span class="n">resultDataSet</span><span class="o">.</span><span class="na">output</span><span class="o">(</span><span class="k">new</span> <span class="n">LocalCollectionOutputFormat</span><span class="o">&lt;...&gt;(</span><span class="n">result</span><span class="o">));</span>

    <span class="c1">// kick off execution.
</span><span class="c1"></span>    <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">();</span>

    <span class="c1">// Do some work with the resulting ArrayList (=Collection).
</span><span class="c1"></span>    <span class="k">for</span><span class="o">(...</span> <span class="n">t</span> <span class="o">:</span> <span class="n">result</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">System</span><span class="o">.</span><span class="na">err</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&#34;Result = &#34;</span><span class="o">+</span><span class="n">t</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>flink-examples-batch 模块包含一个完整的例子，叫做 CollectionExecutionExample。</p>
<p>请注意，基于集合的 Flink 程序的执行只可能在小数据上执行，小数据适合 JVM 堆。在集合上的执行不是多线程的，只使用一个线程。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/local_execution.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/local_execution.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[迭代]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-iterations/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-iterations/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Iterations</blockquote><h2 id="迭代">迭代</h2>
<p>迭代算法出现在数据分析的许多领域，如机器学习或图形分析。为了实现大数据的承诺，从数据中提取有意义的信息，此类算法至关重要。随着人们对在非常大的数据集上运行这类算法的兴趣越来越大，就需要以大规模并行的方式执行迭代。</p>
<p>Flink 程序通过定义一个步骤函数并将其嵌入到一个特殊的迭代运算符中来实现迭代算法。这个运算符有两个变体。Iterate 和 Delta Iterate。这两个运算符都是在当前的迭代状态上反复调用步骤函数，直到达到某个终止条件。</p>
<p>在这里，我们提供了这两个操作符变体的背景，并概述了它们的用法。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">编程指南</a>解释了如何在 Scala 和 Java 中实现这些操作符。我们还通过 Flink 的图处理 API <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/gelly/index.html">Gelly</a> 支持以顶点为中心的迭代和集和应用迭代。</p>
<p>下表提供了这两种运算符的概述:</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">Iterate</th>
<th style="text-align:left">Delta Iterate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Iteration 输入</td>
<td style="text-align:left">Partial Solution</td>
<td style="text-align:left">Workset and Solution Set</td>
</tr>
<tr>
<td style="text-align:left">Step 函数</td>
<td style="text-align:left">Arbitrary Data Flows</td>
<td style="text-align:left">Arbitrary Data Flows</td>
</tr>
<tr>
<td style="text-align:left">State Update</td>
<td style="text-align:left">Next partial solution</td>
<td style="text-align:left">Next workset,Changes to solution set</td>
</tr>
<tr>
<td style="text-align:left">Iteration Result</td>
<td style="text-align:left">Last partial solution</td>
<td style="text-align:left">Solution set state after last iteration</td>
</tr>
<tr>
<td style="text-align:left">Termination</td>
<td style="text-align:left">Maximum number of iterations (default),Custom aggregator convergence</td>
<td style="text-align:left">Maximum number of iterations or empty workset (default),Custom aggregator convergence</td>
</tr>
</tbody>
</table>
<h2 id="iterate-operator">Iterate Operator</h2>
<p>迭代运算符涵盖了简单的迭代形式：在每一次迭代中，step 函数都会消耗整个输入（上一次迭代的结果，或初始数据集），并计算出下一个版本的部分解（如 <code>map</code>, <code>reduce</code>, <code>join</code> 等）。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/iterations_iterate_operator.png" alt="img"></p>
<ol>
<li>迭代输入。第一次迭代的初始输入，来自数据源或之前的运算符。</li>
<li>step 函数。步骤函数将在每次迭代中执行。它是一个任意的数据流，由 map、reduce、join 等运算符组成，取决于你手头的具体任务。</li>
<li>下一个部分解决方案。在每次迭代中，步骤函数的输出将被反馈到下一次迭代中。</li>
<li>迭代结果。上一次迭代的输出会被写入数据接收器，或者作为后续运算符的输入。</li>
</ol>
<p>有多个选项可以指定迭代的终止条件。</p>
<ul>
<li>最大迭代次数。没有任何进一步的条件，迭代将被执行这么多次。</li>
<li>自定义聚合器收敛。迭代允许指定自定义聚合器和收敛标准，比如对发出的记录数量进行加总（聚合器），如果这个数字为零就终止（收敛标准）。</li>
</ul>
<p>你也可以用伪代码来思考迭代操作符。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">IterationState</span> <span class="n">state</span> <span class="o">=</span> <span class="n">getInitialState</span><span class="o">();</span>

<span class="k">while</span> <span class="o">(!</span><span class="n">terminationCriterion</span><span class="o">())</span> <span class="o">{</span>
	<span class="n">state</span> <span class="o">=</span> <span class="n">step</span><span class="o">(</span><span class="n">state</span><span class="o">);</span>
<span class="o">}</span>

<span class="n">setFinalState</span><span class="o">(</span><span class="n">state</span><span class="o">);</span>
</code></pre></div><p>详情和代码示例请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">编程指南</a>。</p>
<h2 id="例子-数字递增">例子: 数字递增</h2>
<p>在下面的例子中，我们对一组数字进行迭代递增。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/iterations_iterate_operator_example.png" alt="img"></p>
<ol>
<li>迭代输入。初始输入是从数据源读取的，由5个单字段记录组成（整数1至5）。</li>
<li>step 函数。步进函数是一个单一的 map 运算符，它将整数字段从i递增到i+1。它将被应用于输入的每一条记录。</li>
<li>下一个部分解。step 函数的输出将是 map 运算符的输出，也就是整数递增的记录。</li>
<li>迭代结果。经过十次迭代，初始数字将被递增十倍，结果是整数11到15。</li>
</ol>
<pre><code>// 1st           2nd                       10th
map(1) -&gt; 2      map(2) -&gt; 3      ...      map(10) -&gt; 11
map(2) -&gt; 3      map(3) -&gt; 4      ...      map(11) -&gt; 12
map(3) -&gt; 4      map(4) -&gt; 5      ...      map(12) -&gt; 13
map(4) -&gt; 5      map(5) -&gt; 6      ...      map(13) -&gt; 14
map(5) -&gt; 6      map(6) -&gt; 7      ...      map(14) -&gt; 15
</code></pre><p>请注意，1、2和4可以是任意的数据流。</p>
<h2 id="增量迭代运算符">增量迭代运算符</h2>
<p>delta 迭代算子涵盖了增量迭代的情况。增量迭代有选择地修改其解的元素，并对解进行演化，而不是完全重新计算。</p>
<p>在适用的情况下，这将导致更高效的算法，因为在每次迭代中，并不是解集中的每个元素都会改变。这样就可以把注意力集中在解的热点部分，而对冷点部分不加处理。通常情况下，大部分解的冷却速度比较快，后面的迭代只对一小部分数据进行操作。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/iterations_delta_iterate_operator.png" alt="img"></p>
<ol>
<li>迭代输入。从数据源或以前的运算符中读取初始工作集和解决方案集，作为第一次迭代的输入。</li>
<li>step 函数。在每次迭代中，步骤函数将被执行。它是一个任意的数据流，由 map、reduce、join 等运算符组成，取决于你手头的具体任务。</li>
<li>下一个工作集/更新解决方案集。下一个工作集驱动迭代计算，并将反馈到下一个迭代中。此外，解决方案集将被更新并隐式转发（它不需要被重建）。这两个数据集都可以通过步长函数的不同运算符进行更新。</li>
<li>迭代结果。最后一次迭代后，解集被写入数据接收器，或作为下面运算符的输入。</li>
</ol>
<p>delta 迭代的默认终止条件由空工作集收敛准则和最大迭代次数指定。当产生的下一个工作集为空或达到最大迭代次数时，迭代将终止。也可以指定一个自定义的聚合器和收敛准则。</p>
<p>你也可以用伪代码来思考迭代操作符。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">IterationState</span> <span class="n">workset</span> <span class="o">=</span> <span class="n">getInitialState</span><span class="o">();</span>
<span class="n">IterationState</span> <span class="n">solution</span> <span class="o">=</span> <span class="n">getInitialSolution</span><span class="o">();</span>

<span class="k">while</span> <span class="o">(!</span><span class="n">terminationCriterion</span><span class="o">())</span> <span class="o">{</span>
	<span class="o">(</span><span class="n">delta</span><span class="o">,</span> <span class="n">workset</span><span class="o">)</span> <span class="o">=</span> <span class="n">step</span><span class="o">(</span><span class="n">workset</span><span class="o">,</span> <span class="n">solution</span><span class="o">);</span>

	<span class="n">solution</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="n">delta</span><span class="o">)</span>
<span class="o">}</span>

<span class="n">setFinalState</span><span class="o">(</span><span class="n">solution</span><span class="o">);</span>
</code></pre></div><p>详情和代码示例请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">编程指南</a>。</p>
<h2 id="例子-在图中传播最小值">例子: 在图中传播最小值</h2>
<p>在下面的例子中，每个顶点都有一个ID和一个着色。每个顶点将把它的顶点ID传播给邻近的顶点。目标是给子图中的每个顶点分配最小的ID。如果一个接收到的ID比当前的ID小，它就会改变成接收到ID的顶点的颜色。这在社区分析或连接组件计算中可以找到一个应用。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/iterations_delta_iterate_operator_example.png" alt="img"></p>
<p>初始输入被设定为工作集和解决方案集。在上图中，颜色直观地显示了解决方案集的演变。随着每次迭代，最小ID的颜色在各自的子图中蔓延。同时，每一次迭代，工作量（交换和比较顶点ID）都在减少。这对应于工作集的大小递减，在三次迭代后，工作集从所有七个顶点变为零，此时迭代终止。重要的观察是，下半子图在上半子图之前收敛，而delta迭代能够用工作集抽象捕捉到这一点。</p>
<p>在上子图中，ID 1（橙色）是最小ID。在第一次迭代中，它将被传播到顶点2，随后它的颜色将变为橙色。顶点3和4将收到ID 2（黄色）作为它们当前的最小ID，并改变为黄色。因为顶点1的颜色在第一次迭代中没有改变，所以在下一个工作集中可以跳过它。</p>
<p>在下层子图中，ID 5（青色）是最小ID。下层子图的所有顶点都会在第一次迭代中收到它。同样，我们可以在下一个工作集中跳过没有变化的顶点（顶点5）。</p>
<p>在第2次迭代中，工作集大小已经从7个元素减少到5个元素（顶点2、3、4、6和7）。这些都是迭代的一部分，并进一步传播它们当前的最小ID。在这次迭代之后，下半部分子图已经收敛了（图的冷部分），因为它在工作集中没有元素，而上半部分则需要对剩下的两个工作集元素（顶点3和4）进行进一步的迭代（图的热部分）。</p>
<p>当第3次迭代后工作集为空时，迭代终止。</p>
<h2 id="superstep-同步">Superstep 同步</h2>
<p>我们将迭代操作符的步骤函数的每次执行称为单次迭代。在并行设置中，步骤函数的多个实例在迭代状态的不同分区上并行评估。在许多设置中，在所有并行实例上对步骤函数的一次评估形成一个所谓的超级步骤，这也是同步的粒度。因此，一个迭代的所有并行任务都需要完成 superstep，才会初始化下一个 superstep。终止标准也将在 superstep 障碍处进行评估。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/iterations_supersteps.png" alt="img"></p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
</feed>
