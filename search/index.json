[{"categories":["rakulang"],"contents":"Raku 中的动态变量\n$*ARGFILES  $*ARGFILES Magic command-line input handle.\n argfiles.raku\n$*ARGFILES.raku.say; # IO::Handle.new(:path(Any),:chomp) # 按行读取 for $*ARGFILES.lines -\u0026gt; $line { say \u0026#34;$line\u0026#34;; } # 一次性读取 # say $*ARGFILES.slurp; USAGE\n$ raku argfiles.raku file1 file2 file3 ...  class IO::Handle - Raku Documentation Input/Output - Raku Documentation Raku文件操作 - Qiita  @*ARGS  @*ARGS - Arguments from the command line. 命令行中的参数。\n agrs.raku\nsay @*ARGS.WAHT; # (Array) say @*ARGS; # [a b c d e] say @*ARGS.raku; # [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;] USAGE\n$ raku args.raku a b c d e  class Array - Raku Documentation  $*IN  $*IN - 标准输入文件句柄, 等同于 stdin\n in.raku\nsay $*IN.raku; # IO::Handle.new(:path(IO::Special.new(what \u0026#34;\u0026lt;STDIN\u0026gt;\u0026#34;)),:chomp) say $*IN.path; # IO::Special.new(what \u0026#34;\u0026lt;STDIN\u0026gt;\u0026#34;) say $*IN.chomp; # True for $*IN.lines -\u0026gt; $line { say \u0026#34;$line\u0026#34;; } USAGE\n$ raku in.raku 人力 ... $ cat somefile.txt | raku in.raku $*OUT  $*OUT - 标准输出文件句柄, 等同于 stdout\n out.raku\nsay $*OUT.raku; # IO::Handle.new(:path(IO::Special.new(what \u0026#34;\u0026lt;STDOUT\u0026gt;\u0026#34;)),:chomp) say $*OUT.path; # IO::Special.new(what \u0026#34;\u0026lt;STDOUT\u0026gt;\u0026#34;) say $*OUT.chomp; # True $*OUT.say( q:to/新年快乐/ ); 祝你新年快乐 2016.01.23 让我再说一次 新年快乐 # 通常我们会在打印时省略 $*OUT # say \u0026#34;哈利路亚\u0026#34;; 最后一段代码中 // 中间的字符是分割符。这打印出:\n祝你新年快乐 2016.01.23 让我再说一次 USAGE\n$ raku out.raku $ raku out.raku \u0026gt; result.txt  role IO - Raku Documentation  $*ERR  $*ERR - 标准错误文件句柄, 等同于 stderr\n err.raku\nsay $*ERR.raku; # IO::Handle.new(:path(IO::Special.new(what \u0026#34;\u0026lt;STDERR\u0026gt;\u0026#34;)),:chomp) say $*ERR.path; # IO::Special.new(what \u0026#34;\u0026lt;STDERR\u0026gt;\u0026#34;) say $*ERR.chomp; # True $*ERR.say(\u0026#34;我错了\u0026#34;); # 平时可以使用 note # note \u0026#34;前方高能预警\u0026#34;; USAGE\n$ raku err.raku \u0026gt; /dev/null 我错了 $*REPO  $*REPO A variable holding information about modules installed/loaded\n repo.raku\nsay $*REPO; say $*REPO.raku; say $*REPO.id; say $*REPO.path-spec; say $*REPO.loaded; say $*REPO.repo-chain; $*TZ  $*TZ The system\u0026rsquo;s local timezone.\n tz.raku\nsay $*TZ; # 32400 say $*TZ.raku; # 32400 say $*TZ.WHAT; # (Int) $*CWD  $*CWD The Current Working Directory.\n cwd.raku\nsay $*CWD; # \u0026#34;/Users/kujira\u0026#34;.IO say $*CWD.path; # /Users/kujira say $*CWD.raku; # \u0026#34;/Users/kujira\u0026#34;.IO(:SPEC(IO::Spec::Unix),:CWD(\u0026#34;/Users/kujira\u0026#34;)) $*KERNEL  $*KERNEL Which kernel am I compiled for?\n kernel.raku\nsay $*KERNEL; # darwin (15.2.0) say $*KERNEL.release; # Darwin Kernel Version 15.2.0: Fri Nov 13 19:56:56 PST 2015; root:xnu-3248.20.55~2/RELEASE_X86_64 say $*KERNEL.name; # darwin say $*KERNEL.auth; # unknown say $*KERNEL.version; # v15.2.0 say $*KERNEL.signature; # (Blob) say $*KERNEL.desc; # (Str) say $*KERNEL.raku; # Kernel.new(release Str, name \u0026#34;darwin\u0026#34;, auth \u0026#34;unknown\u0026#34;, version Version.new(\u0026#39;15.2.0\u0026#39;), signature Blob, desc Str) say $*KERNEL.WHAT; # (Kernel) $*DISTRO  $*DISTRO Which OS distribution am I compiling under?\n distro.raku\nsay $*DISTRO; # macosx (10.11.2) say $*DISTRO.name; # macosx say $*DISTRO.is-win; # False say $*DISTRO.version; # v11.4 say $*DISTRO.path-sep; # : say $*DISTRO.auth; # Apple Computer, Inc. say $*DISTRO.desc; # unknown say $*DISTRO.release; # 20F71 say $*DISTRO.signature; # (Blob) say $*DISTRO.gist; # macosx (11.4) say $*DISTRO.Str; # macosx say $*DISTRO.raku; # Distro.new(release \u0026#34;20F71\u0026#34;, path-sep \u0026#34;:\u0026#34;, name \u0026#34;macos\u0026#34;, auth \u0026#34;Apple Inc.\u0026#34;, version v11.4, signature Blob, desc \u0026#34;unknown\u0026#34;)  Raku で windows かどうか判定したい - tokuhirom\u0026rsquo;s blog DISTRO.t  $*VM  $*VM Which virtual machine am I compiling under?\n vm.raku\nsay $*VM; # moar (2021.06) say $*VM.config; say $*VM.raku; $*RAKU  $*RAKU Which Raku am I compiled for?\n perl.raku\nsay $*RAKU; # Raku (6.d) say $*RAKU.compiler; # rakudo (2021.06) say $*RAKU.raku; # Raku.new(compiler Compiler.new(id \u0026#34;24F69C23F0D9F44910DEB9B097353B0DD30C7E96\u0026#34;, release \u0026#34;\u0026#34;, codename \u0026#34;\u0026#34;, name \u0026#34;rakudo\u0026#34;, auth \u0026#34;The Perl Foundation\u0026#34;, version v2021.06, signature Blob, desc Str), name \u0026#34;Raku\u0026#34;, auth \u0026#34;The Perl Foundation\u0026#34;, version v6.d, signature Blob, desc Str) $*PID  $*PID Process ID of the current process.\n pid.raku\nsay $*PID; # 35480 say $*PID.raku; # 35480 say $*PID.WHAT; # (Int) $*PROGRAM-NAME  $*PROGRAM-NAME Path to the current executable as it was entered on the command line, or C\u0026lt;-e\u0026gt; if perl was invoked with the -e flag.\n program-name.raku\nsay $*PROGRAM-NAME; say $*PROGRAM-NAME.raku; say $*PROGRAM-NAME.IO.basename; $*PROGRAM  $*PROGRAM Location (in the form of an CIO::Path object) of the Perl program being executed.\n program.raku\nsay $*PROGRAM; # \u0026#34;/Users/kujira/program.raku\u0026#34;.IO say $*PROGRAM.Str; # program.raku say $*PROGRAM.raku; # \u0026#34;program.raku\u0026#34;.IO(:SPEC(IO::Spec::Unix),:CWD(\u0026#34;/Users/kujira\u0026#34;)) say $*PROGRAM.SPEC; # (Unix) say $*PROGRAM.CWD; # /Users/kujira say $*PROGRAM.WHAT; # (Path) $*EXECUTABLE  $*EXECUTABLE Absolute path of the perl executable that is currently running.\n executable.raku\nsay $*EXECUTABLE; # \u0026#34;/usr/local/bin/raku\u0026#34;.IO say $*EXECUTABLE.Str; # /usr/local/bin/raku say $*EXECUTABLE.basename; # raku say $*EXECUTABLE.WHAT; # (Path) say $*EXECUTABLE.raku; # \u0026#34;/usr/local/bin/raku\u0026#34;.IO(:SPEC(IO::Spec::Unix)) say $*EXECUTABLE.SPEC; # (Unix) $*EXECUTABLE-NAME  $*EXECUTABLE-NAME The name of the perl executable that is currently running. (e.g. raku-p, raku-m, Niecza.exe) Favor $*EXECUTABLE because it is not guaranteed that the perl executable is in PATH.\n executable-name.raku\nsay $*EXECUTABLE-NAME; # raku say $*EXECUTABLE-NAME.WHAT; # (Str) $*USER  $*USER The user that is running the program. It is an object that evaluates to \u0026ldquo;username (uid)\u0026rdquo;. It will evaluate to the username only if treated as a string and the numeric user id if treated as a number.\n user.raku\nsay $*USER; # kujira (801) say +$*USER; # 801 say ~$*USER; # kujira say $*USER.raku; # IdName.new $*GROUP  $*GROUP The primary group of the user who is running the program. It is an object that evaluates to \u0026ldquo;groupname (gid)\u0026rdquo;. It will evaluate to the groupname only if treated as a string and the numeric group id if treated as a number.\n group.raku\nsay $*GROUP; # whale (0) say ~$*GROUP; # whale say +$*GROUP; # 0 say $*GROUP.raku; # IdName.new $*HOME  $*HOME An LIO::Path object representing the \u0026ldquo;home directory\u0026rdquo; of the user that is running the program. If the \u0026ldquo;home directory\u0026rdquo; cannot be determined it will be L\n home.raku\nsay $*HOME; # \u0026#34;/Users/kujira\u0026#34;.IO say $*HOME.CWD; # /Users/kujira say $*HOME.SPEC; # (Unix) say $*HOME.WHAT; # (Path) say $*HOME.raku; # \u0026#34;/Users/kujira\u0026#34;.IO(:SPEC(IO::Spec::Unix),:CWD(\u0026#34;/Users/kujira\u0026#34;))  class IO::Path - Raku Documentation  $*SPEC  $*SPEC The appropriate IO::Spec sub-class for the platform that the program is running on.\n spec.raku\nsay $*SPEC; # (Unix) say $*SPEC.raku; # IO::Spec::Unix say $*SPEC.path; # (/usr/local/Cellar/rakudo-star/2015.12/share/raku/site/bin /usr/local/sbin /usr/local/bin /usr/bin /bin /usr/sbin /sbin) say $*SPEC.tmpdir; # \u0026#34;/var/folders/9v/wr31l2zj78x1nw58jgljq_9w0000gn/T\u0026#34;.IO say $*SPEC.dir-sep; # / say $*SPEC.curdir; # . say $*SPEC.updir; # .. say $*SPEC.curupdir; # none(., ..) say $*SPEC.rootdir; # / say $*SPEC.devnull; # /dev/null  class IO::Spec - Raku Documentation class IO::Spec::QNX - Raku Documentation class IO::Spec::Unix - Raku Documentation class IO::Spec::Win32 - Raku Documentation class IO::Spec::Cygwin - Raku Documentation  http://qiita.com/B73W56H84/items/18053bf37de8bb2bb808#%E5%8F%82%E8%80%83%E3%81%A8%E6%B3%A8%E9%87%88\n","permalink":"https://ohmyweekly.github.io/notes/2016-02-29-dynamic-variables-in-raku/","tags":["dynamic variable"],"title":"Raku 中的动态变量"},{"categories":["rakulang"],"contents":"匿名 子例程、方法或子方法，当它们不能通过名字调用时，就被称为匿名的\n# named subroutine sub double($x) { 2 * $x }; # 匿名子例程,存储在一个具名的标量里 my $double = sub ($x) { 2 * $x }; 注意，匿名子例程仍然可以有名字\n# 使用 anon 关键字使子例程匿名 my $s = anon sub triple($x) { 3 * $x } say $s.name; # triple 副词 通常, 副词是函数的命名参数. 也有一些其它特殊语法形式允许副词出现在某些合适的地方:\nq:w\u0026#34;foo bar # \u0026#34;:w\u0026#34; is a Quotelike form modifier adverb m:g/a|b|c/ # \u0026#34;:g\u0026#34; is also 4 +\u0026gt; 5 :rotate # \u0026#34;:rotate\u0026#34; is an operator adverb @h{3}:exists # \u0026#34;:exists\u0026#34; is also, but is known as a subscript adverb 副词通常使用冒号对儿标记来表示, 因为这个原因, 冒号对儿标记法也以副词对儿形式著称:\n:a(4) # Same as \u0026#34;a\u0026#34; =\u0026gt; 4 Autothreading Autothreading 是这样的: 如果你传递一个 junction 给子例程, 该子例程期望的参数类型为 Any 或它的子类型。那么这个子例程调用会被执行多次, 每次使用一个不同的 junction 状态. 这些调用的结果被组合成一个跟原 junction 同类型的 junction。\nsub f($x) { 2 * $x }; if f(1|2|3) == 4 { say \u0026#39;success\u0026#39;; } 这里 f() 是含有一个参数的子例程，然而因为它没有显式的类型声明，它就被隐式的声明为 Any 型。 Junction 参数使 f(1|2|3) 调用在内部作为 f(1)|f(2)|f(3) 执行, 而结果是跟原 junction 同类型的 junction, 即 2|4|6。这种把一个 Junction 分成对多次函数调用的处理就叫做 autothreading。\nColon Pair 和 Colon List 冒号对儿是用于创建或 Pair 对象的便捷语法. 两种最常见的形式是:\n:a(4) # Same as \u0026#34;a\u0026#34; =\u0026gt; 4, same as Pair.new(:key\u0026lt;a\u0026gt;,:value(5)) :a\u0026lt;4\u0026gt; # Same as \u0026#34;a\u0026#34; =\u0026gt; \u0026#34;4\u0026#34;, same as Pair.new(:key\u0026lt;a\u0026gt;,:value\u0026lt;5\u0026gt;) 这也是人们熟知的副词对儿形式。注意, 当冒号后面括号前面的部分不是一个合法的标识符的时候, 会应用其它语义, 不是所有的副词对儿都创建 Pair 对象。 另外两个常见的形式是:\n:a # Same as :a(True) :!a # Same as :a(False) 一个 colon 列表是一个仅包含冒号对儿的列表, 不需要逗号, 甚至不需要空格:\n:a(4):c:!d:c # Same as a =\u0026gt; 4, c =\u0026gt; True, d =\u0026gt; False, c =\u0026gt; True Constraint 约束是给参数或 subset 类型添加的限制。通过单词 where 引入约束. 在下面的例子中, 约束用于确保, 当调用一个名为 abbreviate 的子例程, 其参数为一个长度小于 10 个字符的字符串时,会抛出一个错误:\nsub abbreviate (Str $thing where { .chars \u0026gt;= 10 }) { ... } 上例中的 Str 也是一个约束, 但是经常作为\u0026quot;类型约束\u0026quot;.\nInstance 类的实例在其它编程语言中也叫对象. 对象存储属性, 通常是 new 方法调用的返回值, 或者是对象字面量. 大部分类型的实例被定义为 True, 例如 defined($instance) 为 True。\nmy Str $str = \u0026#34;hello\u0026#34;; ## 这使用内建类型,例如 Str if defined($str) { say \u0026#34;Oh, yeah. I\u0026#39;m defined.\u0026#34;; } else { say \u0026#34;No. Something off? \u0026#34;; } ## if you wanted objects... class A { # nothing here for now. } my $an_instance = A.new; say $an_instance.defined.perl;# defined($an_instance) works too. 类拥有方法和属性的所有蓝图, 而类的实例把蓝图带到真实世界中。\nInvocant 在 Raku 中调用方法的对象叫做调用者. 在方法中它就是 self 引用的东西.\nsay \u0026#39;str\u0026#39;.uc; # \u0026#39;str\u0026#39; 是 方法 uc 的调用者 Literal 字面量是一块直接代表对象的代码, 通常指向对象自身.\nmy $x = 2; # the 2 is a literal say $x; # $x is not a literal, but a variable lvalue lvalue 或者左值是能出现在赋值操作符左侧的任何东西; 典型的左值有变量, 私有属性和 is rw 属性, 变量列表和左值子例程。\n左值的例子:\n   Declaration lvalue Comments     my $x; $x    my ($a, $b); ($a, $b)    has $!attribute; $!attribute Only inside classes   has $.attrib is rw; $.attrib    sub a is rw { $x }; a()     不是左值的例子:\n3 # literals constant x = 3; # constants has $.attrib; # attributes; you can only assign to $!attrib sub f { }; f(); # \u0026#34;normal\u0026#34; subs are not writable sub f($x) { $x = 3 }; # error - parameters are read-only by default Mainline mainline 是程序中不属于任何 block 的程序文本.\nsub f { # not in mainline, in sub f } f(); # in mainline again Slurpy 子例程或方法中的形参如果能接收任意数量的参数, 那这个形参就会被认为是 slurpy 的。它由参数名字前面的星号标出。\nsub sum (*@numbers) { return [+] @numbers; } Type Object 类型对象是一个代表类 /role/package/grammar/enum 的对象。它通常和类型名相同。\nclass A { }; say A; # A is the type object my $x = A.new(); # same here my $x = class { method greet() { say \u0026#34;hi\u0026#34;; } } # $x now holds a type object returned from the # anonymous class definition ","permalink":"https://ohmyweekly.github.io/notes/2016-03-26-terms-in-raku/","tags":["term"],"title":"Raku 中的术语"},{"categories":["rakulang"],"contents":"签名也是对象 class Signature {} 签名是代码对象参数列表的静态描述。即, 签名描述了你需要什么参数和多少参数传递给代码或函数以调用它们。\n传递参数给签名把包含在 Capture 中的参数绑定到了签名上。\n\u0026gt; sub a($a, $b) {}; \u0026gt; \u0026amp;a.signature.perl.say :($a, $b) \u0026gt; my $b = -\u0026gt; $a, $b {}; \u0026gt; $b.signature.perl.say :($a, $b) 签名是一个对象, 就像 Raku 中的任何其它东西一样。 任何 Callable 类型中都有签名, 并且它能使用 .signature 方法获取到。\nclass Signature { ... } 签名字面量 签名出现在子例程和方法名后面的圆括号中, 对于 blocks 签名出现在 -\u0026gt; 或 \u0026lt;-\u0026gt; 箭头后面, 或者作为变量声明符(例如 my )的输入, 或者以冒号开头作为单独的项。\nsub f($x) { } # ^^^^ sub f 的签名 method x() { } # ^^ 方法 x 的签名 my $s = sub (*@a) { } # ^^^^^ 匿名函数的签名 for \u0026lt;a b c\u0026gt; -\u0026gt; $x { } # ^^ Block 的签名 my ($a, @b) = 5, (6,7,8); # ^^^^^^^^ 变量声明符的签名 my $sig = :($a, $b); # ^^^^^^^^ 独立的签名对象 签名字面量可以用于定义回调或闭包的签名。\nsub f(\u0026amp;c:(Int)) {} sub will-work(Int) {} sub won\u0026#39;t-work(Str){} f(\u0026amp;will-work); f(\u0026amp;won\u0026#39;t-work); CATCH { default { put .^name, \u0026#39;: \u0026#39;, .Str } }; # OUTPUT: «X::TypeCheck::Binding::Parameter: Constraint type check failed in binding to parameter \u0026#39;\u0026amp;c\u0026#39;␤» f(-\u0026gt; Int { \u0026#39;this works too\u0026#39; } ); 支持签名和列表(List)间的智能匹配。\nmy $sig = :(Int $i, Str $s); say (10, \u0026#39;answer\u0026#39;) ~~ $sig; # OUTPUT: «True␤»  my $sub = sub ( Str $s, Int $i ) { return $s xx $i }; say $sub.signature ~~ :( Str, Int ); # OUTPUT: «True␤»  given $sig { when :(Str, Int) { say \u0026#39;mismatch\u0026#39; } when :($, $) { say \u0026#39;match\u0026#39; } default { say \u0026#39;no match\u0026#39; } } # OUTPUT: «match␤»  它匹配第二个 when 从句, 因为 :($, $) 表示带有两个标量, 匿名参数的 Signature, 它是 $sig 的更一版版本。\n当和散列智能匹配时, 签名被认为由散列的键组成。\nmy %h = left =\u0026gt; 1, right =\u0026gt; 2; say %h ~~ :(:$left, :$right); # OUTPUT # True 参数分隔符 签名由逗号分割的 0 个或多个形式参数组成。\nmy $sig = :($a, @b, %c) sub add ($a, $b) { $a + $b } 作为一个例外, 签名中的第一个参数的后面可以跟着一个冒号而非逗号来标记方法的调用者。调用者是用来调用方法的对象, 其通常绑定到 self。 通过在签名中指定调用者, 你可以更改它绑定到的变量名称。\nmethod ($a: @b, %c){} # 第一个参数是调用者 class Foo { method whoami ($me:) { \u0026#34;Well I\u0026#39;m class $me.^name(), of course!\u0026#34; } } say Foo.whoami; # Well I\u0026#39;m class Foo, of course! 类型约束 参数可以可选地拥有一个类型约束(默认为 Any)。这些能用于限制函数允许的输入。\nmy $sig = :(Int $a, Str $b) sub divisors (Int $n) { $_ if $n %% $_ for 1..$n } divisors 2.5; # !!! Calling \u0026#39;divisors\u0026#39; will never work with argument types (Rat) # ===SORRY!=== Error while compiling:  # Calling divisors(Rat) will never work with declared signature (Int $n)  匿名参数也行, 如果参数只需要它的类型约束的话。\nmy $sig = :($, @, %a) # 两个匿名参数和一个 \u0026#34;正常的(有名字的)\u0026#34;参数 $sig = :(Int, Positional) # 只有类型也行(两个参数) sub baz (Str) {\u0026#34;Got passed a Str\u0026#34;} 类型约束也可以是类型捕获(type captures)。\n除了这些名义上的类型之外, 额外的约束可以以代码块的形式加到参数上, 代码块必须返回一个真值以通过类型检测。\nsub f(Real $x where { $x \u0026gt; 0 }, Real $y where { $y \u0026gt;= $x }) { } where 子句中的代码有一些限制：任何会产生副作用的东西（例如, 打印输出, 从迭代器中拉取或递增状态变量）都不受支持, 如果使用了, 可能会产生令人惊讶的结果。此外, 在某些实现中, where 子句的代码可能会针对单个类型检查运行多次。\nwhere 子句不需要是代码块, where 子句右侧的任何内容都将用于和参数智能匹配。所以你也可以写：\nmulti factorial(Int $ where 0) { 1 } multi factorial(Int $x) { $x * factorial($x - 1) } 第一个还能简化为:\nmulti factorial(0) { 1 } 你可以直接把字面量用作类型并把值约束到匿名参数上。\n提示：注意不要在你有几个条件时不小心离开区块：\n-\u0026gt; $y where .so \u0026amp;\u0026amp; .name {}( sub one {} ); # WRONG!!  -\u0026gt; $y where { .so \u0026amp;\u0026amp; .name } {}( sub two {} ); # OK!  -\u0026gt; $y where .so \u0026amp; .name.so {}( sub three {} ); # Also good  第一个版本是错误的, 会发出一个关于 sub 对象强制转换为字符串的警告。原因是表达式相当于 ($y ~~ ($y.so \u0026amp;\u0026amp; $y.name)); 那就是“调用 .so, 如果它为 True, 调用 .name; 如果这也是 True, 则使用它的值进行smartmatching \u0026hellip;”。这是 (.so \u0026amp;\u0026amp; .name) 的结果将被智能匹配, 但我们要检查 .so 和 .name 是否为真值。这就是为什么明确的 Block 或者 Junction 是正确的版本。\n在签名中不是子签名(sub-signature)的一部分的所有先前的参数都可以在参数后面的 where 从句中访问。 因此, 最后一个参数的 where 从句可以访问不是子签名一部分的签名的所有参数。 对于子签名, 把 where 从句放在子签名中。\nsub foo($a, $b where * == $a ** 2) { say \u0026#34;$bis a square of $a\u0026#34; } foo 2, 4; # OUTPUT: «4 is a square of 2␤»»  # foo 2, 3;  # OUTPUT: «Constraint type check failed in binding to parameter \u0026#39;$b\u0026#39;…»  sub one-of-them(:$a, :$b, :$c where { $a.defined ^^ $b.defined ^^ $c.defined }) { $a // $b // $c }; say one-of-them(c=\u0026gt;42); # OUTPUT«42␤» 约束可选参数 可选参数也可以拥有约束。任何参数 where 从句都将被执行, 即使它是可选的, 而且不是由调用者提供。在这种情况下, 您可能必须防止 where 从句中的未定义值。\nsub f(Int $a, UInt $i? where { !$i.defined or $i \u0026gt; 5 } ) { ... } 约束吞噬参数 吞噬参数不能拥有类型约束。一个 where 从句连同一个 Junction可以达到同样的那个效果。\nsub f(*@a where { $_.all ~~ Int }) { say @a }; f(42); f(\u0026lt;a\u0026gt;); # OUTPUT«[42] Constraint type check failed for parameter \u0026#39;@a\u0026#39; in sub f at ...» 约束定义值和未定义值 通常, 类型约束只检查传递的值是否是正确的类型。\nsub limit-lines (Str $s, Int $limit) { my @lines = $s.lines; @lines[0 ..^ min @lines.elems, $limit].join(\u0026#34;\\n\u0026#34;) } say (limit-lines \u0026#34;a \\n b \\n c \\n d \\n\u0026#34;, 3).perl; # \u0026#34;a \\n b \\n c \u0026#34; say limit-lines Str, 3; # Uh-oh. Dies with \u0026#34;Cannot call \u0026#39;lines\u0026#39;;\u0026#34; CATCH { default { put .^name, \u0026#39;: \u0026#39;, .Str } }; # OUTPUT: «X::Multi::NoMatch: Cannot resolve caller lines(Str: ); none of these signatures match: # (Cool:D $: |c is raw) # (Str:D $: :$count!, *%_) # (Str:D $: $limit, *%_) # (Str:D $: *%_)» say limit-lines \u0026#34;a \\n b\u0026#34;, Int # Always returns the max number of lines 这样的情况, 我们其实只想处理定义了的字符串。要这样做, 我们使用 :D 类型约束。\nsub limit-lines (Str:D $s, Int $limit) { ... } say limit-lines Str, 3; CATCH { default { put .^name, .Str } }; # OUTPUT«X::AdHocParameter \u0026#39;$s\u0026#39; requires an instance of type Str, but a # type object was passed. Did you forget a .new?» # Dies with \u0026#34;参数 \u0026#39;$s\u0026#39; 需要一个实例, 但是函数 limit-lines 中却传递了一个类型对象。 如果传递一个诸如 Str 这样的类型对象进去, 那么就会报错。这样的失败方式比以前更好了, 因为失败的原因更清晰了。\n也有可能未定义的类型是子例程唯一有意义的接收值。这可以使用 :U 类型约束来约束它。例如, 我们可以把 \u0026amp;limit-lines 转换成 multi 函数以使用 :U 约束。\nmulti limit-lines (Str $s, Int:D $limit) { my @lines = $s.lines; @lines[0 ..^ min @lines.elems, $limit].join(\u0026#34;\\n\u0026#34;); } multi limit-lines (Str $s, Int:U $) {$s} # 如果传递给我一个未定义的类型对象, 就返回整个字符串 say limit-lines \u0026#34;a \\n b \\n c\u0026#34;, Int; # \u0026#34;a \\n b \\n c\u0026#34; 为了显式地标示常规的行为, 可以使用 :_, 但这不是必须的。 :(Num:_ $) 和 :(Num $) 相同。\n约束 Callables 的签名 要基于 block和子例程的签名约束 block 和子例程引用, 参数名要写在签名的后面。\nsub f(\u0026amp;c:(Int, Str)) { say c(10, \u0026#39;ten\u0026#39;) }; sub g(Int $i, Str $s) { $s ~ $i }; f(\u0026amp;g); # OUTPUT: ten10 约束返回类型 --\u0026gt; 标记后面跟着一个类型会强制在子例程执行成功时进行类型检测。返回类型箭头必须放在参数列表的后面。跟在签名声明后面的 returns 关键字有同样的功能。Nil 在类型检测中被认为是定义了的。这允许沿着调用链返回和传递 Failure。\nsub (--\u0026gt; Int) { my Int $i; $i}; sub (--\u0026gt; Int:D) { 1 }; sub () returns Int { 1 }; sub foo(--\u0026gt; Int) { 1 }; sub foo() returns Int { 1 }; # 同上 sub does-not-work(--\u0026gt; Int) { \u0026#34;\u0026#34; }; # throws X::TypeCheck::Return 如果类型约束是一个常量表达式, 那么它被用于子例程的返回值。那个子例程中的任何 return 语句必须是不含参数的。\nsub foo(--\u0026gt; 123) { return } Nil 和 Failure 总是被允许作为返回类型, 不管类型约束是什么。\nsub foo(--\u0026gt; Int) { Nil }; say foo.perl; # Nil 不支持类型捕获和强制类型。\n强制类型 要接受一个类型但是强制它自动地转为另一种类型, 使用接受的类型作为目标类型的参数。如果接受的类型是 Any, 那么它可以被省略。\nsub f(Int(Str) $want-int, Str() $want-str) { say $want-int.WHAT, $want-str.WHAT } f \u0026#39;10\u0026#39;, 10; # OUTPUT # (Int)(Str) 吞噬参数(或长度可变参数) 数组或散列参数可以通过前置一个星号(s)被标记为吞噬参数, 这意味着它可以被绑定给任意数量的参数(0 个或 多个)。\n它们被叫做吞噬参数, 因为它们吞完函数中的任何剩余参数, 就像有些人吞吃面条那样。\n$ = :($a, @b) # 正好两个参数, 而第二个参数必须是 Positional 的 $ = :($a, *@b) # 至少一个参数, @b 吞噬完任何剩余的参数 $ = :(*%h) # 没有位置参数, 除了任意数量的具名参数 sub one-arg (@) { } sub slurpy (*@) { } one-arg (5, 6, 7); # ok, same as one-arg((5, 6, 7)) slurpy (5, 6, 7); # ok slurpy 5, 6, 7 ; # ok # one-arg(5, 6, 7) ; # X::TypeCheck::Argument # one-arg 5, 6, 7 ; # X::TypeCheck::Argument sub named-names (*%named-args) { %named-args.keys }; say named-names :foo(42) :bar\u0026lt;baz\u0026gt;; # foo bar 注意位置参数不允许出现在吞噬参数的后面。\n:(*@args, $last); CATCH { when X::Parameter::WrongOrder { put .^name, \u0026#39;: \u0026#39;, .Str } } # OUTPUT«X::Parameter::WrongOrder: 不能把必要参数放在可变长度参数的后面» 带有一个星号的吞噬参数会通过消融一层或多层裸的可迭代对象来展平参数。带有两个星号的吞噬参数不会展平参数：\nsub a(*@a) { @a.join(\u0026#34;|\u0026#34;).say }; sub b(**@b) { @b.join(\u0026#34;|\u0026#34;).say }; a(1,[1,2],([3,4],5)); # 1|1|2|3|4|5 b(1,[1,2],([3,4],5)); # 1|1 2|3 4 5 通常, 吞噬参数会创建一个数组, 为每个 argument 创建一个标量容器, 并且把每个参数的值赋值给那些标量。如果在该过程中原参数也有一个中间的标量分量, 那么它在调用函数中是访问不到的。\n吞噬参数在和某些 traits and modifiers 组合使用时会有特殊行为, 像下面描述的那样。\nSingle Argument Rule Slurpy 单一参数规则允许根据上下文处理子程序、for-loop 和列表构造函数的参数。许多位置类型的方法可以像处理列表或参数一样处理单个参数。在 Signature 中使用 +@ 作为标志, 提供了语法糖, 使这项工作更容易一些。任何非位置类型的单一参数都会被提升为一个具有单一项的列表。\nsub f(+@a){ dd @a }; f(1); # OUTPUT«[1]␤» f(1, 2, 3); # OUTPUT«[1, 2, 3]␤» my @b = \u0026lt;a b c\u0026gt;; f @b; # OUTPUT«[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]␤» 类型捕获 类型捕获允许把类型约束的说明推迟到函数被调用时。它们允许签名和函数体中的类型都可以引用。\nsub f(::T $p1, T $p2, ::C) { # $p1 和 $p2 的类型都为 T, 但是我们还不知道具体类型是什么 # C 将会保存一个源于类型对象或值的类型 my C $closure = $p1 / $p2; return sub (T $p1) { $closure * $p1; } } # 第一个参数是 Int 类型, 所以第二个参数也是 # 我们从调用用于 \u0026amp;f 中的操作符导出第三个类型 my \u0026amp;s = f(10,2, Int.new / Int.new); say s(2); # 10 / 2 * 2 == 10 Positional vs. Named 参数可以是跟位置有关的或者是具名的。所有的参数都是 positional 的, 除了吞噬型散列参数和有前置冒号标记的参数:\n$ = :($a) # 位置参数 $ = :(:$a) # 名字为 a 的具名参数 $ = :(*@a) # 吞噬型位置参数 $ = :(*%h) # 吞噬型具名参数 在调用者这边, 位置参数的传递顺序和它们声明顺序相同。\nsub pos($x, $y) { \u0026#34;x = $xy = $y\u0026#34; }; pos(4, 5); # x = 4 y = 5 对于具名实参和具名形参, 只用名字用于将实参映射到形参上。\nsub named(:$x, :$y) { \u0026#34;x=$xy=$y\u0026#34; } named( y =\u0026gt; 5, x =\u0026gt; 4); 具名参数也可以和变量的名字不同:\nsub named(:official($private)) { \u0026#34;公务\u0026#34; if $private } named :official; 别名也是那样做的:\nsub ( :color(:colour($c)) ) { } # \u0026#39;color\u0026#39; 和 \u0026#39;colour\u0026#39; 都可以 sub ( :color(:$colour) ) { } # same API for the caller 带有具名参数的函数可以被动态地调用, 使用 |非关联化一个 Pair 来把它转换为一个具名参数。\nmulti f(:$named) { note \u0026amp;?ROUTINE.signature }; multi f(:$also-named) { note \u0026amp;?ROUTINE.signature }; for \u0026#39;named\u0026#39;, \u0026#39;also-named\u0026#39; -\u0026gt; $n { f(|($n =\u0026gt; rand)) # «(:$named)␤(:$also-named)␤» } my $pair = :named(1); f |$pair; # «(:$named)␤» 同样的语法也可以用于将散列转换为具名参数：\nmy %pairs = also-named =\u0026gt; 4; sub f(|c) {}; f |%pairs; # (:$also-named) 可选参数和强制参数 Positional 参数默认是强制的, 也可以用默认值或结尾的问号使参数成为可选的:\n$ = :(Str $id) # 必要参数 required parameter $ = :($base = 10) # 可选参数, 默认为 10 $ = :(Int $x?) # 可选参数, 默认为 Int 类型的对象 具名参数默认是可选的, 可以通过在参数末尾加上一个感叹号使它变成强制参数:\n$ = :(:%config) # 可选参数 $ = :(:$debug = False) # 可选参数, 默认为 False $ = :(:$name!) # 名为 name 的强制具名参数 默认值可以依靠之前的参数, 并且每次调用都会被重新计算。\n$ = :($goal, $accuracy = $goal / 100); $ = :(:$excludes = [\u0026#39;.\u0026#39;, \u0026#39;..\u0026#39;]); # a new Array for every call 解构参数 参数后面可以跟着一个由括号括起来的 sub-signature, 子签名会解构给定的参数。列表解构成它的元素:\nsub first (@array ($first, *@rest)) { $first } 或:\nsub first ([$first, *@]) { $first } 而散列解构成它的键值对儿(pairs):\nsub all-dimensions (% (:length(:$x), :width(:$y), :depth(:$z))) { $x andthen $y andthen $z andthen True } andthen 返回第一个未定义的值, 否则返回最后一个元素。短路操作符。andthen 左侧的结果被绑定给 $_ 用于右侧, 或者作为参数传递, 如果右侧是一个 block 或 pointy block 的话。\n一般地, 对象根据它的属性解构。通用的惯用法是在 for 循环中解包一个 Pair 的键和值:\nfor \u0026lt;Peter Paul Merry\u0026gt;.pairs -\u0026gt; (:key($index), :value($guest)) { ... } 但是, 将对象解包为属性只是默认行为。要使对象以不同方式解构, 请更改其 Capture 方法。\n子签名 要匹配复合参数, 请在圆括号中的参数名后面使用子签名。\nsub foo(|c(Int, Str)){ put \u0026#34;called with {c.perl}\u0026#34; } foo(42, \u0026#34;answer\u0026#34;); # OUTPUT«called with \\(42, \u0026#34;answer\u0026#34;)» 长名字 为了在多重分派中排除特定参数, 使用一个双分号来分割它们。\nmulti sub f(Int $i, Str $s;; :$b) { dd $i, $s, $b }; f(10, \u0026#39;answer\u0026#39;); # OUTPUT«10 \u0026#34;answer\u0026#34;Any $b = Any» 捕获参数 在参数前前置一个垂直的 | 会让参数变为 Capture, 并使用完所有剩下的位置参数和具名参数。\n这常用在 proto 定义中( 像 proto foo (|) {*} ) 来标示例程的 multi 定义可以拥有任何类型约束。\n如果绑定到变量参数, 则可以使用 slip 运算符 | 作为整体转发。\nsub a(Int $i, Str $s) { say $i.WHAT, $s.WHAT } sub b(|c) { say c.WHAT; a(|c) } b(42, \u0026#34;answer\u0026#34;); # OUTPUT«(Capture)␤(Int)(Str)␤» 参数特性和修饰符 默认地, 形式参数被绑定到它们的实参上并且被标记为只读。你可以使用 traits 特性更改参数的只读特性。\nis copy特性让参数被复制, 并允许在子例程内部修改参数的值。\nsub count-up ($x is copy) { $x = Inf if $x ~~ Whatever; .say for 1..$x; } is rw 特性让参数只绑定到变量上(或其它可写的容器)。 赋值给参数会改变调用一侧的变量的值。\nsub swap($x is rw, $y is rw) { ($x, $y) = ($y, $x); } 对于吞噬参数, is rw 由语言设计者保留做将来之用\n方法 params 方法 method params(Signature:D:) returns Positional 返回 Parameter 对象列表以组成签名。\narity 方法 method arity(Signature:D:) returns Int:D 返回所必须的最小数量的满足签名的位置参数\ncount 方法 method count(Signature:D:) returns Real:D 返回能被绑定给签名的最大数量的位置参数。如果有吞噬位置参数则返回 Inf。\nreturns 方法 签名返回的任意约束是:\n:($a, $b --\u0026gt; Int).returns # Int ACCEPTS 方法 multi method ACCEPTS(Signature:D: Capture $topic) multi method ACCEPTS(Signature:D: @topic) multi method ACCEPTS(Signature:D: %topic) multi method ACCEPTS(Signature:D: Signature $topic) 前三个方法会看参能否绑定给 capture, 例如, 如果带有那个 Signature 的函数能使用 $topic 调用:\n(1,2, :foo) ~~ :($a, $b, :foo($bar)) # true \u0026lt;a b c d\u0026gt; ~~ :(Int $a) # False 最后一个会为真如果 $topic 能接收的任何东西也能被 Signature 接收。\n:($a, $b) ~~ :($foo, $bar, $baz?) # True :(Int $n) ~~ :(Str) # False ","permalink":"https://ohmyweekly.github.io/notes/2016-04-12-signature-in-raku/","tags":["signature"],"title":"Raku 中的签名"},{"categories":["rakulang"],"contents":"用类来排序数组 有多列数据, 序号, 学校, 课程… 年份这几列。要如果学校和课程相同就根据年份倒序排列。\n先按学校排序, 再按课程排序, 然后按年份倒序排序。\n我定义一个类来进行多列数据的排序, 很方便：\nclass Course { has Int $.numb; has Str $.univ; has Str $.dis; has Int $.paper; has Int $.cited; has Int $.year; } my @headers = \u0026lt;numb univ dis paper cited year\u0026gt;; my @courses; for $=finish.lines -\u0026gt; $line { next if $line ~~ /^num/; my @words = $line.words; @words[0, 3,4,5] = @words[0,3,4,5]».Int; my %h = @headers Z=\u0026gt; @words; my $course = Course.new(|%h); @courses.push($course); } my @sorted = @courses.sort(*.univ).sort(*.dis).sort(-*.year); for @sorted { say join \u0026#34;\u0026#34;, .numb, .univ, .dis, .paper, .cited, .year; } =finishnum\tuniv\tdis\tpaper\tcited\tyear 1\tBeijing\tPhysics\t193\t4555\t2005 2\tBeijing\tPhysics\t197\t2799\t2006 3\tBeijing\tPhysics\t240\t2664\t2007 4\tBeijing\tPhysics\t200\t3191\t2008 5\tBeijing\tPhysics\t268\t2668\t2009 6\tBeijing\tPhysics\t249\t2300\t2010 7\tBeijing\tPhysics\t262\t2080\t2011 8\tBeijing\tPhysics\t230\t2371\t2012 9\tBeijing\tPhysics\t309\t1367\t2013 10\tBeijing\tPhysics\t284\t615\t2014 11\tBeijing\tChemistry\t143\t1650\t2005 12\tBeijing\tChemistry\t149\t2379\t2006 13\tBeijing\tChemistry\t190\t2566\t2007 14\tBeijing\tChemistry\t147\t1888\t2008 15\tBeijing\tChemistry\t184\t2146\t2009 16\tBeijing\tChemistry\t214\t2568\t2010 17\tBeijing\tChemistry\t238\t2874\t2011 18\tBeijing\tChemistry\t265\t2097\t2012 19\tBeijing\tChemistry\t251\t1303\t2013 20\tBeijing\tChemistry\t241\t656\t2014 斐波拉契数列（Fibonacci Sequence）  analytic  sub fibonacci (Int $n where 0..* --\u0026gt; Int) { constant phi = (1 + sqrt 5) / 2; return round( phi**($n+1) / sqrt 5); } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 这种方法计算前1000个斐波拉契数大约为0.746s（2015.7.Rakudo, 以下都是）。\n 迭代  sub fibonacci (Int $n) { state @sequence = 1,1; while @sequence.elems \u0026lt;= $n { @sequence.push( @sequence[*-2] + @sequence[*-1] ); } return @sequence[$n]; } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 这种遍历法大概需要 1 秒多。\n 递归  sub fibonacci (Int $n where 0..*) { if $n == 0 | 1 { return 1; } else { return fibonacci($n-1) + fibonacci($n-2); } } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 比较慢。第 20 个之后越来越慢。\n 递归 - 超运算符  sub fibonacci (Int $n where 0..*) { if $n == 0 | 1 { return 1; } else { return [+] ($n-1, $n-2)».\u0026amp;fibonacci; } } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 和上面差不多。\n 递归 - map  sub fibonacci (Int $n where 0..*) { if $n == 0 | 1 { return 1; } else { return [+] map \u0026amp;fibonacci, ($n-1, $n-2); } } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 同上。\n 递归 - multi  multi fibonacci (0) { 1 } multi fibonacci (1) { 1 } multi fibonacci (Int $n --\u0026gt; Int) { return fibonacci($n-1) + fibonacci($n-2); } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 比上一个稍快。\n 递归 - multi_cached  multi fibonacci (0) { 1 } multi fibonacci (1) { 1 } my %cached; multi fibonacci (Int $n --\u0026gt; Int) { return %cached{$n} //= fibonacci($n-1) + fibonacci($n-2); } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 花费 0.849s 执行完毕。\n 递归 - multi_cached_state  multi fibonacci (0) { 1 } multi fibonacci (1) { 1 } multi fibonacci (Int $n --\u0026gt; Int) { state %cached; return %cached{$n} //= fibonacci($n-1) + fibonacci($n-2); } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 花费了 0.885s。\n 递归 - multi_cached_trait  multi fibonacci (0) { 1 } multi fibonacci (1) { 1 } multi fibonacci (Int $n --\u0026gt; Int) is cached { fibonacci($n-1) + fibonacci($n-2); } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 目前报错。\n 序列  sub fibonacci (Int $n) { constant @sequence := 1, 1, *+* ... *; return @sequence[$n]; } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 花费了 1.6s 多。 其中 := 是惰性赋值。按需求值。\n 序列 - inline  sub fibonacci (Int $n) { (1, 1, *+* ... *)[$n]; } for 0..1000 -\u0026gt; $i { say $i.fmt(\u0026#39;%3d\u0026#39;), \u0026#39;: \u0026#39;, fibonacci($i); } 花费了 133.808s。\n排序  bogosort  #! /usr/bin/env raku sub bogosort ( *@list ) { until [!after] @list { @list .= pick(*); } return @list; } #my @data = 6, 7, 2, 1, 8, 9, 2; my @data = \u0026lt;p e r l s i x \u0026gt;; say \u0026#39;input = \u0026#39; ~ @data; say \u0026#39;output = \u0026#39; ~ bogosort @data; 输出：\ninput = p e r l s i x output = e i l p r s x [Finished in 1.326s]  mergesort  #! /usr/bin/env raku sub merge (@a, @b) { gather while @a \u0026amp;\u0026amp; @b { if @a[0] before @b[0] { take @a.shift } else { take @b.shift } }, @a, @b; } sub mergesort ( *@list ) { return @list if @list.elems \u0026lt;= 1; my $middle = @list.elems div 2; my @left = mergesort @list[ 0 ..^ $middle ]; my @right = mergesort @list[ $middle .. * ]; return merge(@left, @right); } # my @data = 6, 7, 2, 1, 8, 9, 5, 3, 4; my @data = \u0026lt;p e r l s i x \u0026gt;; say \u0026#34;input = { @data }\u0026#34;; say \u0026#34;output = { mergesort(@data) }\u0026#34;; 输出：\ninput = p e r l s i x output = e i l p r s x [Finished in 0.337s]  mergesort_functional  #! /usr/bin/env raku multi merge ([], @ys) { @ys } multi merge (@xs, []) { @xs } multi merge ([$x, *@xs], [$y, *@ys]) { $x before $y ?? ($x, merge @xs, [$y, @ys]) !! ($y, merge [$x, @xs], @ys) } multi mergesort ( [] ) { [] } multi mergesort ([$x]) { [$x] } multi mergesort (@xs) { merge mergesort( @xs[0 ..^ @xs.elems div 2] ), mergesort( @xs[@xs.elems div 2 .. * ] ) } my @data = 6, 7, 2, 1, 8, 9, 5, 3, 4; #my @data = \u0026lt; p e r l s i x \u0026gt;; say \u0026#34;input = { @data }\u0026#34;; say \u0026#34;output = { mergesort(@data) }\u0026#34;; 输出：\ninput = 6 7 2 1 8 9 5 3 4 output = 1 2 3 4 5 6 7 8 9 [Finished in 0.373s]  mergesort_functional_given  #! /usr/bin/env raku multi merge ([], @ys) { @ys } multi merge (@xs, []) { @xs } multi merge ([$x, *@xs], [$y, *@ys]) { $x before $y ?? ($x, merge @xs, [$y, @ys]) !! ($y, merge [$x, @xs], @ys) } multi mergesort ( [] ) { [] } multi mergesort ([$x]) { [$x] } multi mergesort (@xs) { given @xs.elems div 2 -\u0026gt; $middle { merge mergesort( @xs[ 0 ..^ $middle ] ), mergesort( @xs[ $middle .. * ] ) } } my @data = 6, 7, 2, 1, 8, 9, 5, 3, 4; #my @data = \u0026lt; p e r l s i x \u0026gt;; say \u0026#34;input = { @data }\u0026#34;; say \u0026#34;output = { mergesort(@data) }\u0026#34;; 输出：\ninput = 6 7 2 1 8 9 5 3 4 output = 1 2 3 4 5 6 7 8 9 [Finished in 0.375s]  mergesort_functional_interleaved  #! /usr/bin/env raku multi merge ([], @ys) { @ys } multi merge (@xs, []) { @xs } multi merge ([$x, *@xs], [$y, *@ys]) { $x before $y ?? ($x, merge @xs, [$y, @ys]) !! ($y, merge [$x, @xs], @ys) } multi mergesort ( [] ) { [] } multi mergesort ([$x]) { [$x] } multi mergesort (@xs) { merge mergesort( @xs[0,2...*] ), mergesort( @xs[1,3...*] ) } my @data = 6, 7, 2, 1, 8, 9, 5, 3, 4; #my @data = \u0026lt; p e r l s i x \u0026gt;; say \u0026#34;input = { @data }\u0026#34;; say \u0026#34;output = { mergesort(@data) }\u0026#34;; 输出：\ninput = 6 7 2 1 8 9 5 3 4 output = 1 2 3 4 5 6 7 8 9 [Finished in 0.34s]  quicksort  #! /usr/bin/env raku sub quicksort( *@list ) { return @list if @list.elems \u0026lt; 2; my $pivot = @list.shift; # my $pivot = @list.=pick(*).shift; my (@before, @after); for @list -\u0026gt; $elem { if $elem before $pivot { @before.push($elem); } else { @after.push($elem); } } return quicksort(@before), $pivot, quicksort(@after); } #my @data = 6, 7, 2, 1, 8, 9, 5, 3, 4; my @data = \u0026lt;p e r l s i x \u0026gt;; say \u0026#34;input = { @data }\u0026#34;; say \u0026#34;output = { quicksort(@data) }\u0026#34;; 输出：\ninput = p e r l s i x output = e i l p r s x [Finished in 0.246s]  quicksort_classify  #! /usr/bin/env raku multi quicksort( [] ) { }; multi quicksort( Mu ) { }; multi quicksort( [$x] ) { $x }; multi quicksort( [$pivot, *@xs] ) { given @xs.classify:{ $^elem before $pivot ?? \u0026#39;pre\u0026#39; !! \u0026#39;post\u0026#39;} { quicksort( .\u0026lt;pre\u0026gt; ), $pivot, quicksort( .\u0026lt;post\u0026gt; ); } } #my @data = 6, 7, 2, 1, 8, 9, 5, 3, 4; my @data = \u0026lt;p e r l s i x \u0026gt;; say \u0026#34;input = { @data }\u0026#34;; say \u0026#34;output = { quicksort(@data) }\u0026#34;; 输出：\ninput = p e r l s i x output = e i l p r s x [Finished in 0.303s]  quicksort_functional  #! /usr/bin/env raku multi quicksort( [] ) { }; multi quicksort( [$x] ) { $x }; multi quicksort( [$pivot, *@xs] ) { quicksort(@xs.grep: * before $pivot), $pivot, quicksort(@xs.grep: * !before $pivot); } #my @data = 6, 7, 2, 1, 8, 9, 5, 3, 4; my @data = \u0026lt;p e r l s i x \u0026gt;; say \u0026#34;input = { @data }\u0026#34;; say \u0026#34;output = { quicksort(@data) }\u0026#34;; 输出:\ninput = p e r l s i x output = e i l p r s x [Finished in 0.27s] pm 模块 首先把自定义的 Bank.pm 模块复制到 Raku 的 lib 目录下：\ncp Bank.pm ~/.rakudobrew/moar-nom/install/share/raku/site/lib Bank.pm\n#! /usr/bin/raku class Ident { subset Pattern of Str where / \\d**3 \u0026#39;-\u0026#39; \\d**3 \u0026#39;-\u0026#39; \\d**3 /; has Str $.name = \u0026#39;????\u0026#39;; has Str $.ID where Pattern = \u0026#39;000-000-000\u0026#39;; } role Taxable [:$THRESHOLD = 100_000] { constant GENERAL_TAX_RATE = 0.01; has %.tax_record; method tax_credits {...} method calculate_tax () { my $tax_payable = ($.balance min $THRESHOLD) * GENERAL_TAX_RATE - $.tax_credits; %!tax_record{now} = $tax_payable; return $tax_payable; } } class Account does Taxable { subset ID of Str where / \u0026lt;alpha\u0026gt;**4 \u0026lt;digit\u0026gt;**5 /; state ID $next_account_ID = \u0026#39;AAAA00001\u0026#39;; has Str $.name = die \u0026#39;Must provide account name\u0026#39;; has Numeric $.balance = 0; has ID $.ID = $next_account_ID++; method deposit(Numeric $amount where *\u0026gt;0) { $!balance += $amount; } method withdraw(Numeric $amount where *\u0026gt;0) { fail \u0026#34;Insufficient funds to withdraw $amount\u0026#34; if $.balance \u0026lt; $amount; $!balance -= $amount; } method description () { \u0026#34;$.ID($.name): balance=$.balance\u0026#34;; } method tax_credits { 0 } } class Bank { has Ident $!ident handles\u0026lt;name ID \u0026gt;; has Account %!accounts; submethod BUILD (|args) { $!ident .= new(|args); } method add_account(Account $account) { %!accounts{$account.ID} = $account; } method close_account(Str $ID) { return %!accounts{$ID} :delete // fail \u0026#34;No such account\u0026#34;; } multi method get_account(Account::ID $ID) { return %!accounts{$ID} // fail \u0026#34;No such account\u0026#34;; } multi method get_account(Any $name) { # return %!accounts.values.grep({.name ~~ $name}); self.for_each_account({.take if .name ~~ $name}); } method for_each_account (\u0026amp;action_on) { gather for %!accounts.values -\u0026gt; $account is rw { action_on($account); } } method collect_taxes () { self.for_each_account: { my $tax = .calculate_tax(); .withdraw($tax); take .ID =\u0026gt; $tax; } } method report () { say \u0026#34;[ {self.ID}[{self.name}] ]\u0026#34;; # Or: say \u0026#34;[ $.ID [$.name] ]\u0026#34;; self.for_each_account(*.description.say); say \u0026#39;\u0026#39;; } } class Account::Corporate is Account does Taxable[THRESHOLD =\u0026gt; 1_000_000] { has Str $.company_ID; method tax_credits { 5_000 } method description () { callsame() ~ \u0026#34;[$.company_ID]\u0026#34;; } } 该模块的功能是计算银行存款汇率等。下面使用这个模块：\n demo  #! /usr/bin/env raku use Bank; my Bank $bank .= new(:ID(\u0026#39;123-456-789\u0026#39;)); $bank.add_account: Account.new(:name(\u0026#39;Leslie Grace\u0026#39;) ); $bank.add_account: Account.new(:name(\u0026#39;Dana McKenna\u0026#39;), :balance(10_000)); $bank.add_account: Account.new(:name(\u0026#39;AstroDynamic\u0026#39;), :balance( 2e7)); $bank.add_account: Account.new(:name(\u0026#39;Jan van Quod\u0026#39;), :balance( 9_999)); $bank.add_account: Account.new(:name(\u0026#39;OmniCorp LLC\u0026#39;), :balance( 1e6)); $bank.report; $bank.get_account(\u0026#39;AAAA00003\u0026#39;).deposit(100); $bank.get_account(\u0026#39;Jan van Quod\u0026#39;)».deposit(2); $bank.get_account(/D.na/)».deposit(2); #$bank.get_account(*)».deposit(99); $bank.report; given $bank.close_account(\u0026#39;AAAA00005\u0026#39;) { say \u0026#34;Closed $^account.perl()\\n\u0026#34;; $bank.report; } $bank.close_account(\u0026#39;ZZZZ99999\u0026#39;); $bank.get_account(\u0026#39;AAAA00001\u0026#39;).withdraw(1001); $bank.report; 输出：\n[ 123-456-789 [????] ] AAAA00004 (Jan van Quod): balance=9999 AAAA00001 (Leslie Grace): balance=0 AAAA00005 (OmniCorp LLC): balance=1000000 AAAA00002 (Dana McKenna): balance=10000 AAAA00003 (AstroDynamic): balance=20000000 [ 123-456-789 [????] ] AAAA00004 (Jan van Quod): balance=10001 AAAA00001 (Leslie Grace): balance=0 AAAA00005 (OmniCorp LLC): balance=1000000 AAAA00002 (Dana McKenna): balance=10002 AAAA00003 (AstroDynamic): balance=20000102 Closed Account.new(name =\u0026gt; \u0026quot;OmniCorp LLC\u0026quot;, balance =\u0026gt; 1000000e0, ID =\u0026gt; \u0026quot;AAAA00005\u0026quot;, tax_record =\u0026gt; {}\u0026lt;\u0026gt;) [ 123-456-789 [????] ] AAAA00004 (Jan van Quod): balance=10001 AAAA00001 (Leslie Grace): balance=0 AAAA00002 (Dana McKenna): balance=10002 AAAA00003 (AstroDynamic): balance=20000102 No such account in method close_account at ~/.rakudobrew/moar-nom/install/share/raku/site/lib/Bank.pm:67 Actually thrown at: in block \u0026lt;unit\u0026gt; at ~/04.bank_demo.pl:26 [Finished in 0.768s]  demo_inheritance  #! /usr/bin/env raku sub show ($text) { say \u0026#39;\u0026#39;; say (\u0026#39;____/ \u0026#39; ~ $text ~ \u0026#39;\\_________________________________________________\u0026#39;).substr(0,50); } use Bank; my Bank $bank .= new(:ID(\u0026#39;123-456-789\u0026#39;), :name(\u0026#39;Bank of Evil\u0026#39;)); $bank.add_account: Account.new(:name(\u0026#39;Leslie Grace\u0026#39;), :balance( 1_000)); $bank.add_account: Account.new(:name(\u0026#39;Dana McKenna\u0026#39;), :balance(10_000)); $bank.add_account: Account.new(:name(\u0026#39;Jan van Quod\u0026#39;), :balance( 9_999)); $bank.add_account: Account::Corporate.new(:name(\u0026#39;AstroDynamic\u0026#39;), :balance(2e7) :company_ID(\u0026#39;ASDY\u0026#39;)); $bank.add_account: Account::Corporate.new(:name(\u0026#39;OmniCorp LLC\u0026#39;), :balance(1e6) :company_ID(\u0026#39;OMNI\u0026#39;)); show \u0026#39;Status\u0026#39;; $bank.report; show \u0026#39;Taxes collected\u0026#39;; .say for $bank.collect_taxes(); show \u0026#39;Status\u0026#39;; $bank.report; $bank.collect_taxes(); show \u0026#39;Tax records\u0026#39;; $bank.for_each_account({ say .name, \u0026#39;: \u0026#39;, .tax_record }); show \u0026#39;Culling acounts\u0026#39;; given $bank { .for_each_account: { .close_account($^account.ID).say if $^account.balance \u0026lt; 10_000; } } show \u0026#39;Status\u0026#39;; $bank.report; 输出：\n____/ Status \\____________________________________ [ 123-456-789 [Bank of Evil] ] AAAA00004 (AstroDynamic): balance=20000000 [ASDY] AAAA00001 (Leslie Grace): balance=1000 AAAA00005 (OmniCorp LLC): balance=1000000 [OMNI] AAAA00002 (Dana McKenna): balance=10000 AAAA00003 (Jan van Quod): balance=9999 ____/ Taxes collected \\___________________________ AAAA00004 =\u0026gt; 5000 AAAA00001 =\u0026gt; 10 AAAA00005 =\u0026gt; 5000 AAAA00002 =\u0026gt; 100 AAAA00003 =\u0026gt; 99.99 ____/ Status \\____________________________________ [ 123-456-789 [Bank of Evil] ] AAAA00004 (AstroDynamic): balance=19995000 [ASDY] AAAA00001 (Leslie Grace): balance=990 AAAA00005 (OmniCorp LLC): balance=995000 [OMNI] AAAA00002 (Dana McKenna): balance=9900 AAAA00003 (Jan van Quod): balance=9899.01 ____/ Tax records \\_______________________________ AstroDynamic: Instant:1440413601.447466 =\u0026gt; 5000, Instant:1440413601.463112 =\u0026gt; 5000 Leslie Grace: Instant:1440413601.450753 =\u0026gt; 10, Instant:1440413601.465823 =\u0026gt; 9.9 OmniCorp LLC: Instant:1440413601.452933 =\u0026gt; 5000, Instant:1440413601.468346 =\u0026gt; 4950 Dana McKenna: Instant:1440413601.454860 =\u0026gt; 100, Instant:1440413601.470238 =\u0026gt; 99 Jan van Quod: Instant:1440413601.457399 =\u0026gt; 99.99, Instant:1440413601.471551 =\u0026gt; 98.9901 ____/ Culling acounts \\___________________________ Account.new(name =\u0026gt; \u0026quot;Leslie Grace\u0026quot;, balance =\u0026gt; 980.1, ID =\u0026gt; \u0026quot;AAAA00001\u0026quot;, tax_record =\u0026gt; {\u0026quot;Instant:1440413601.450753\u0026quot; =\u0026gt; 10.0, \u0026quot;Instant:1440413601.465823\u0026quot; =\u0026gt; 9.9}\u0026lt;\u0026gt;) Account.new(name =\u0026gt; \u0026quot;Dana McKenna\u0026quot;, balance =\u0026gt; 9801.0, ID =\u0026gt; \u0026quot;AAAA00002\u0026quot;, tax_record =\u0026gt; {\u0026quot;Instant:1440413601.454860\u0026quot; =\u0026gt; 100.0, \u0026quot;Instant:1440413601.470238\u0026quot; =\u0026gt; 99.0}\u0026lt;\u0026gt;) Account.new(name =\u0026gt; \u0026quot;Jan van Quod\u0026quot;, balance =\u0026gt; 9800.0199, ID =\u0026gt; \u0026quot;AAAA00003\u0026quot;, tax_record =\u0026gt; {\u0026quot;Instant:1440413601.457399\u0026quot; =\u0026gt; 99.99, \u0026quot;Instant:1440413601.471551\u0026quot; =\u0026gt; 98.9901}\u0026lt;\u0026gt;) ____/ Status \\____________________________________ [ 123-456-789 [Bank of Evil] ] AAAA00004 (AstroDynamic): balance=19990000 [ASDY] AAAA00005 (OmniCorp LLC): balance=990050 [OMNI] [Finished in 0.817s]  demo_unary_dot  #! /usr/bin/env raku use Bank; my Bank $bank .= new(:ID(\u0026#39;123-456-789\u0026#39;)); given $bank { .add_account: Account.new(:name(\u0026#39;Leslie Grace\u0026#39;), :balance( 1_000)); .add_account: Account.new(:name(\u0026#39;Dana McKenna\u0026#39;), :balance(10_000)); .add_account: Account.new(:name(\u0026#39;AstroDynamic\u0026#39;), :balance( 2e7)); .add_account: Account.new(:name(\u0026#39;Jan van Quod\u0026#39;), :balance( 9_999)); .add_account: Account.new(:name(\u0026#39;OmniCorp LLC\u0026#39;), :balance( 1e6)); .report; .get_account(\u0026#39;AAAA00003\u0026#39;).deposit(100); .report; say .close_account(\u0026#39;AAAA00005\u0026#39;); .report; .get_account(\u0026#39;AAAA00001\u0026#39;).withdraw(1001); .report; } 输出：\n[ 123-456-789 [????] ] AAAA00004 (Jan van Quod): balance=9999 AAAA00001 (Leslie Grace): balance=1000 AAAA00005 (OmniCorp LLC): balance=1000000 AAAA00002 (Dana McKenna): balance=10000 AAAA00003 (AstroDynamic): balance=20000000 [ 123-456-789 [????] ] AAAA00004 (Jan van Quod): balance=9999 AAAA00001 (Leslie Grace): balance=1000 AAAA00005 (OmniCorp LLC): balance=1000000 AAAA00002 (Dana McKenna): balance=10000 AAAA00003 (AstroDynamic): balance=20000100 Account.new(name =\u0026gt; \u0026quot;OmniCorp LLC\u0026quot;, balance =\u0026gt; 1000000e0, ID =\u0026gt; \u0026quot;AAAA00005\u0026quot;, tax_record =\u0026gt; {}\u0026lt;\u0026gt;) [ 123-456-789 [????] ] AAAA00004 (Jan van Quod): balance=9999 AAAA00001 (Leslie Grace): balance=1000 AAAA00002 (Dana McKenna): balance=10000 AAAA00003 (AstroDynamic): balance=20000100 Insufficient funds to withdraw 1001 in method withdraw at ~/.rakudobrew/moar-nom/install/share/raku/site/lib/Bank.pm:43 in block \u0026lt;unit\u0026gt; at ~04.bank_demo_unary_dot.pl:22 [Finished in 0.738s] LZW LZW 算法:\n demo  #! /usr/bin/env raku sub compress(Str $uncompressed --\u0026gt; List) { # Build a look-up table of encoded representations # (each ASCII char represented by its equivalent codepoint) my %code_for = map { $^ASCII.chr =\u0026gt; $^ASCII }, ^256; # Loop and collect each encoding... gather { # Track which characters we\u0026#39;ve seen but not yet encoded my $already_seen = \u0026#34;\u0026#34;; # Walk through each single character... for $uncompressed.comb -\u0026gt; $next_char { # Now we\u0026#39;ve seen that next character as well my $now_seen = $already_seen ~ $next_char; # If new char sequence is known, keep looking if %code_for{$now_seen}:exists { $already_seen = $now_seen; } # Otherwise, we have an unknown sequence of chars else { # Emit encoding for what we\u0026#39;ve previously seen take %code_for{$already_seen}; # Add encoding for new unknown sequence to table %code_for{$now_seen} = %code_for.elems; # Restart the current sequence from this char $already_seen = $next_char; } } # Emit the encoding for the final sequence (if any) take %code_for{$already_seen} if $already_seen ne \u0026#34;\u0026#34;; } } # Convert to codepoints... my @codes = compress(\u0026#39;To be or not to be. That be the question, matey!\u0026#39;); say @codes; separator; # Emit as characters... say @codes».chr; separator; # Convert to a binary sequence... my $bits_per_code = @codes.max.log(2).ceiling(); my $format = \u0026#39;%0\u0026#39; ~ $bits_per_code ~ \u0026#39;b\u0026#39;; my $bits = @codes».fmt($format).join; say $bits; separator; say $bits.comb(/.**1..7/).map({:2($^bitpattern).chr}).join; separator; sub separator { say \u0026#39;_\u0026#39; x 50 } 输出：\n84 111 32 98 101 32 111 114 32 110 111 116 32 116 257 259 46 32 84 104 97 267 259 268 104 260 113 117 101 115 116 105 111 110 44 32 109 276 101 121 33 __________________________________________________ T o b e o r n o t t ā ă . T h a ċ ă Č h Ą q u e s t i o n , m Ĕ e y ! __________________________________________________ 001010100001101111000100000001100010001100101000100000001101111001110010000100000001101110001101111001110100000100000001110100100000001100000011000101110000100000001010100001101000001100001100001011100000011100001100001101000100000100001110001001110101001100101001110011001110100001101001001101111001101110000101100000100000001101101100010100001100101001111001000100001 __________________________________________________ \u0015\u0006x@1\u000cQo\u001cB\u0001\\7NAt@0\u0018\\\u0010 C aBp\u001c\u00184 CDu\u0019'\u0019h4Ms8,\u0006l(2O\u0011\u0001 __________________________________________________ [Finished in 0.429s]  LZW_functional  #! /usr/bin/env raku # To compress a string... sub compress(Str $uncompressed) { # Encode the character list via a dictionary, from the start encode( $uncompressed.comb, code =\u0026gt; hash(map {$^ASCII.chr =\u0026gt; $^ASCII}, ^256), seen =\u0026gt; \u0026#34;\u0026#34; ) } # Encode an empty list where nothing already seen as nothing multi encode([], :%code, :$seen where \u0026#34;\u0026#34;) {} # Encode an empty list where something already seen by look-up multi encode([], :%code, :$seen) { %code{$seen} } # Encode an list of one or more uncompressed characters... multi encode([$next, *@uncompressed], :%code, :$seen) { # If [already-seen plus next char] is a known sequence... %code{ $seen~$next }:exists # Then encode all of that together ?? encode(@uncompressed, :%code, seen =\u0026gt; $seen~$next) # Else emit encoding for the already-seen sequence !! ( %code{$seen}, # Plus the encoding for the rest of the string... encode( @uncompressed, # Add encoding for new sequence to table code =\u0026gt; %( %code, $seen~$next =\u0026gt; %code.elems ), # Continue encoding from next character seen =\u0026gt; $next ) ) } # Convert to codepoints... my @codes = compress(\u0026#39;To be or not to be. That be the question, matey!\u0026#39;); say @codes; separator; # Emit as characters... say @codes».chr; separator; # Convert to a binary sequence... my $bits_per_code = @codes.max.log(2).ceiling(); my $format = \u0026#39;%0\u0026#39; ~ $bits_per_code ~ \u0026#39;b\u0026#39;; my $bits = @codes».fmt($format).join; say $bits; separator; say $bits.comb(/.**1..7/).map({:2($^bitpattern).chr}).join; separator; sub separator { say \u0026#39;_\u0026#39; x 50 } 输出：\n84 111 32 98 101 32 111 114 32 110 111 116 32 116 257 259 46 32 84 104 97 267 259 268 104 260 113 117 101 115 116 105 111 110 44 32 109 276 101 121 33 __________________________________________________ T o b e o r n o t t ā ă . T h a ċ ă Č h Ą q u e s t i o n , m Ĕ e y ! __________________________________________________ 001010100001101111000100000001100010001100101000100000001101111001110010000100000001101110001101111001110100000100000001110100100000001100000011000101110000100000001010100001101000001100001100001011100000011100001100001101000100000100001110001001110101001100101001110011001110100001101001001101111001101110000101100000100000001101101100010100001100101001111001000100001 __________________________________________________ \u0015\u0006x@1\u000cQo\u001cB\u0001\\7NAt@0\u0018\\\u0010 C aBp\u001c\u00184 CDu\u0019'\u0019h4Ms8,\u0006l(2O\u0011\u0001 __________________________________________________ [Finished in 0.658s]  validation_concurrent  #! /usr/bin/env raku my @records = ( { :Name\u0026lt;Damian Conway\u0026gt;, :Age(42), :ID(\u0026#39;00012345\u0026#39;) }, { :Name\u0026lt;Leslie Duvall\u0026gt;, :Age(29), :ID(\u0026#39;668\u0026#39;) }, { :Name\u0026lt;Sam Georgious\u0026gt;, :Age(-2), :ID(\u0026#39;00000007\u0026#39;) }, ); sub normalize_data (Hash $record) { $record\u0026lt;Name\u0026gt; .= subst(/\u0026lt;lower\u0026gt;/,{$\u0026lt;lower\u0026gt;.uc}, :g); $record\u0026lt;Age\u0026gt; max= 18; $record\u0026lt;ID\u0026gt; .= fmt(\u0026#39;%08d\u0026#39;); } sub report ($outcome) { say \u0026#34;\\tInvalid record ($outcome)\u0026#34;; } sub invalid_name ($rec) { \u0026#34;Bad name: $rec\u0026#34; if $rec\u0026lt;Name\u0026gt; !~~ /\\S/; } sub invalid_age ($rec) { \u0026#34;Bad age: $rec\u0026#34; if $rec\u0026lt;Age\u0026gt; \u0026lt; 18; } sub invalid_ID ($rec) { \u0026#34;Bad ID: $rec\u0026#34; if $rec\u0026lt;ID\u0026gt; !~~ /^\\d**8$/; } say \u0026#39;Validating...\u0026#39;; my @invalidations = ( @records».\u0026amp;invalid_name, @records».\u0026amp;invalid_age, @records».\u0026amp;invalid_ID, ); @invalidations».\u0026amp;report; say \u0026#39;Normalizing...\u0026#39;; @records».\u0026amp;normalize_data; say \u0026#39;Revalidating...\u0026#39;; @invalidations = ( @records».\u0026amp;invalid_name, @records».\u0026amp;invalid_age, @records».\u0026amp;invalid_ID, ); @invalidations».\u0026amp;report; 输出：\nValidating... postcircumfix:\u0026lt;{ }\u0026gt; not defined for type Str  validation_imperative  #! /usr/bin/env raku my @records = ( { :Name\u0026lt;Damian Conway\u0026gt;, :Age(42), :ID(\u0026#39;00012345\u0026#39;) }, { :Name\u0026lt;Leslie Duvall\u0026gt;, :Age(29), :ID(\u0026#39;668\u0026#39;) }, { :Name\u0026lt;Sam Georgious\u0026gt;, :Age(-2), :ID(\u0026#39;00000007\u0026#39;) }, ); sub normalize_data (Hash $record) { $record\u0026lt;Name\u0026gt; .= subst(/\u0026lt;lower\u0026gt;/,{$\u0026lt;lower\u0026gt;.uc}, :g); $record\u0026lt;Age\u0026gt; max= 18; $record\u0026lt;ID\u0026gt; .= fmt(\u0026#39;%08d\u0026#39;); } sub report ($outcome) { say \u0026#34;\\tInvalid record ($outcome)\u0026#34;; } sub invalid_name ($rec) { \u0026#34;Bad name: $rec\u0026#34; if $rec\u0026lt;Name\u0026gt; !~~ /\\S/; } sub invalid_age ($rec) { \u0026#34;Bad age: $rec\u0026#34; if $rec\u0026lt;Age\u0026gt; \u0026lt; 18; } sub invalid_ID ($rec) { \u0026#34;Bad ID: $rec\u0026#34; if $rec\u0026lt;ID\u0026gt; !~~ /^\\d**8$/; } say \u0026#39;Validating...\u0026#39;; my @invalidations = gather for @records -\u0026gt; $record { take invalid_name($record); take invalid_age($record); take invalid_ID($record); } for @invalidations -\u0026gt; $errmsg { report( $errmsg ); } say \u0026#39;Normalizing...\u0026#39;; for @records -\u0026gt; $record { normalize_data($record); } say \u0026#39;Revalidating...\u0026#39;; @invalidations = gather for @records -\u0026gt; $record { take invalid_name($record); take invalid_age($record); take invalid_ID($record); } for @invalidations -\u0026gt; $errmsg { report( $errmsg ); } 输出：\nValidating... Invalid record (Bad ID: Name\tLeslie Duvall Age\t29 ID\t668) Invalid record (Bad age: Name\tSam Georgious Age\t-2 ID\t00000007) Normalizing... Revalidating...  validation_junctions  #! /usr/bin/env raku my @records = ( { :Name\u0026lt;Damian Conway\u0026gt;, :Age(42), :ID(\u0026#39;00012345\u0026#39;) }, { :Name\u0026lt;Leslie Duvall\u0026gt;, :Age(29), :ID(\u0026#39;668\u0026#39;) }, { :Name\u0026lt;Sam Georgious\u0026gt;, :Age(-2), :ID(\u0026#39;00000007\u0026#39;) }, ); sub normalize_data (Hash $record) { $record\u0026lt;Name\u0026gt; .= subst(/\u0026lt;lower\u0026gt;/,{$\u0026lt;lower\u0026gt;.uc}, :g); $record\u0026lt;Age\u0026gt; max= 18; $record\u0026lt;ID\u0026gt; .= fmt(\u0026#39;%08d\u0026#39;); } sub report ($outcome) { say \u0026#34;\\tInvalid record ($outcome)\u0026#34;; } sub invalid_name ($rec) { \u0026#34;Bad name: $rec\u0026#34; if $rec\u0026lt;Name\u0026gt; !~~ /\\S/; } sub invalid_age ($rec) { \u0026#34;Bad age: $rec\u0026#34; if $rec\u0026lt;Age\u0026gt; \u0026lt; 18; } sub invalid_ID ($rec) { \u0026#34;Bad ID: $rec\u0026#34; if $rec\u0026lt;ID\u0026gt; !~~ /^\\d**8$/; } my $invalid_record = \u0026amp;invalid_name | \u0026amp;invalid_age | \u0026amp;invalid_ID; say \u0026#39;Validating...\u0026#39;; report( $invalid_record(all @records) ); say \u0026#39;Normalizing...\u0026#39;; normalize_data(all @records); say \u0026#39;Revalidating...\u0026#39;; report( $invalid_record(all @records) ); 输出：\nValidating... Invalid record (Bad ID: Name\tLeslie Duvall Age\t29 ID\t668) Invalid record (Bad age: Name\tSam Georgious Age\t-2 ID\t00000007) Normalizing... Revalidating...  prime_demo  #! /usr/bin/env raku sub is_prime(Int $n) { return $n % all(2..$n.sqrt+1); } for 1..1001 -\u0026gt; $n { say \u0026#34;$nis prime\u0026#34; if is_prime($n); } 输出：\n1 is prime 3 is prime 5 is prime 7 is prime 11 is prime ... 991 is prime 997 is prime  统计  #! /usr/bin/env raku my @values = (1, 1, 3, 4, 4, 4, 4, 5, 5, 5, 7, 7, 12, 12, 1, 7, 7, 99); say \u0026#39;mean (a) = \u0026#39;, mean_a(@values); say \u0026#39;mean (g) = \u0026#39;, mean_g(@values); say \u0026#39;mode = \u0026#39;, mode(@values); say \u0026#39;median = \u0026#39;, median(@values); sub mean_a (*@list) { ([+] @list) / @list.elems; } sub mean_g (*@list) { ([*] @list) ** (1/@list.elems); } sub mode (*@list) { given @list.Bag { .pairs.grep({$^elem.value == .values.max})».key; } } sub median (*@list) { given @list.sort { .elems %% 2 ?? mean_a( .[*/2-1, */2] ) !! .[*/2]; } } 输出：\nmean (a) = 10.444444 mean (g) = 4.95872541158849 mode = 7 4 median = 5  stats_hybird  #! /usr/bin/env raku my @values = (1, 1, 3, 4, 4, 4, 4, 5, 5, 5, 7, 7, 12, 12, 1, 7, 7, 99); say \u0026#39;mean (a) = \u0026#39;, mean_a(@values); say \u0026#39;mean (g) = \u0026#39;, mean_g(@values); say \u0026#39;mode = \u0026#39;, mode(@values); say \u0026#39;median = \u0026#39;, median(@values); sub mean_a (*@list) { my $sum = [+] @list; return $sum / @list.elems; } sub mean_g (*@list) { my $product = [*] @list; return $product ** (1/@list.elems) } sub mode (*@list) { my $frequencies = @list.Bag; my $list_elems = $frequencies.pairs; my $max_freq = $frequencies.values.max; my @max_vals = $list_elems.grep({.value == $max_freq}); return @max_vals».key; } sub median (*@list) { my @sorted = @list.sort; return @sorted.elems %% 2 ?? mean_a(@sorted.[*/2, */2-1]) !! @sorted.[*/2] } 输出：\nmean (a) = 10.444444 mean (g) = 4.95872541158849 mode = 7 4 median = 5  stats_imperative  #! /usr/bin/env raku my @values = (1, 1, 3, 4, 4, 4, 4, 5, 5, 5, 7, 7, 12, 12, 1, 7, 7, 99); say \u0026#39;mean (a) = \u0026#39;, mean_a(@values); say \u0026#39;mean (g) = \u0026#39;, mean_g(@values); say \u0026#39;mode = \u0026#39;, mode(@values); say \u0026#39;median = \u0026#39;, median(@values); sub mean_a (*@list) { my $sum; for @list -\u0026gt; $elem { $sum += $elem; } return $sum / @list.elems; } sub mean_g (*@list) { my $product; for @list -\u0026gt; $elem { $product *= $elem; } return $product ** (1/@list.elems); } sub mode (*@list) { my %counts; %counts{$_}++ for @list; my $max = %counts.values.max; return %counts.grep({ .value == $max })».key; } sub median (*@list) { @list.=sort(); return @list.elems %% 2 ?? mean_a( @list[*/2, */2-1] ) !! @list[*/2]; } 输出：\nmean (a) = 10.444444 mean (g) = 4.95872541158849 mode = 4 7 median = 5  stats_mode_func  #! /usr/bin/env raku my @values = (1, 1, 3, 4, 4, 4, 4, 5, 5, 5, 7, 7, 12, 12, 1, 7, 7, 99); say \u0026#39;mean (a) = \u0026#39;, mean_a(@values); say \u0026#39;mean (g) = \u0026#39;, mean_g(@values); say \u0026#39;mode = \u0026#39;, mode(@values); say \u0026#39;median = \u0026#39;, median(@values); sub mean_a (*@list) { sub sum { [+] @list } return sum() / @list.elems; } sub mean_g (*@list) { sub product { [*] @list } return product() ** (1/@list.elems) } sub mode (*@list) { sub frequencies { @list.Bag } sub list_elems { frequencies.pairs } sub max_freq { frequencies.values.max } sub max_vals { list_elems.grep: {.value == max_freq} } return max_vals».keys; } sub median (*@list) { sub sorted { @list.sort } return sorted.elems %% 2 ?? mean_a(sorted.[*/2, */2-1]) !! sorted.[*/2] } 输出：\nmean (a) = 10.444444 mean (g) = 4.95872541158849 mode = 7 4 median = 5  stats_OO  #! /usr/bin/env raku class StatList is List { method mean_a () { sub sum { [+] self } return sum() / self.elems; } method mean_g () { sub product { [*] self } return product() ** (1/self.elems) } method median () { sub sorted { self.sort } return sorted.elems %% 2 ?? StatList.new(sorted.[*/2, */2-1]).mean_a() !! sorted.[*/2]; } method mode () { sub frequencies { self.Bag } sub list_elems { frequencies.pairs } sub max_freq { frequencies.values.max } sub max_vals { list_elems.grep: {.value == max_freq} } return max_vals».keys; } } my $list = StatList.new(1,3,5,8,8,11); say $list.mean_a; say $list.mean_g; say $list.median; say $list.mode; 输出：\n6 4.68393277169202 13 8  以上所有文件都可以在这儿 下载到 - a Raku introductory tutorial by Damian Conway\n ","permalink":"https://ohmyweekly.github.io/notes/2016-02-14-raku-examples/","tags":["example"],"title":"Raku 代码示例"},{"categories":["rakulang"],"contents":"在 grammar 中, 有两个 regex 的变体, rule 和 token。rule 默认不会回溯。rule 与 token 的一个重要区别就是, rule 这样的正则采取了 :sigspace 修饰符。 rule 实际上是\nregex :ratchet :sigspace { ... } 的简写。ratchet 这个单词的意思是: (防倒转的)棘齿, 意思它是不能回溯的! 而 :sigspace 表明正则中的空白是有意义的, 而 token 实际上是\nregex :ratchet { ... } 的简写。所以在 token 中, 若不是显式地写上 \\s、\\h、\\n 等空白符号, 其它情况下就好像空白隐身了一样, 虽然你写了, 但是编译器却视而不见。\ngrammar Token::Rule::Difference { # 下面三者等价 # rule TOP { [\\w+]+ % \u0026#39; \u0026#39; | [\\d+]+ % \u0026#39; \u0026#39; } 等价于 # rule TOP { | [\\w+]+ % \u0026#39; \u0026#39; | [\\d+]+ % \u0026#39; \u0026#39; } 等价于 rule TOP {|[\\w+]+%\u0026#39;\u0026#39;|[\\d+]+%\u0026#39;\u0026#39;} } # $=finish.lines 中的每一行末尾都没有换行符 for $=finish.lines -\u0026gt; $line { print($line); say Token::Rule::Difference.parse($line) } =finishtoken takes whitespace invisible unless with sigspace rule is a token without sigspace 2015 12 25 2016 01 07 说明在 rule 中, | 左右两边的空格会被忽略, 这通常是为了使格式对齐, 看起来不乱。另外 rule 中, 开头和末尾的空白也会被忽略。\n如果每一行都带有换行符呢？\nuse Grammar::Debugger; grammar Token::Rule::Difference { # token TOP { ^ [\u0026lt;line\u0026gt;\\n]+ $ } # token line { # | [\\w+]+ % \u0026#39; \u0026#39; # | [\\d+]+ % \u0026#39; \u0026#39; # } # 等价于 rule TOP {^\u0026lt;wrap\u0026gt;+$} token wrap {\u0026lt;line\u0026gt;} rule line {[\\w+]+%\u0026#39;\u0026#39;|[\\d+]+%\u0026lt;[-\\s:]\u0026gt;} } my $str = q:to/EOF/;token takes whitespace invisible unless with sigspace rule is a token without sigspace 2015-12-25 12:23 2016-01-07 13:45 EOF my $parse = Token::Rule::Difference.parse($str); say $parse; token vs. rule  When we use rule in place of token, any whitespace after anatom is turned into a non-capturing call to ws\n 这句话是说, 在 rule 中, 任何跟在原子(atom)后面的空白会变成非捕获的 ws 调用, 即 \u0026lt;.ws\u0026gt;,\nrule entry {\u0026lt;key\u0026gt;\u0026#39;=\u0026#39;\u0026lt;value\u0026gt;} 等价于:\ntoken entry {\u0026lt;key\u0026gt;\u0026lt;.ws\u0026gt;\u0026#39;=\u0026#39;\u0026lt;.ws\u0026gt;\u0026lt;value\u0026gt;\u0026lt;.ws\u0026gt;} # . 抑制了捕获 在 grammar 中, 我们继承了默认的 ws, 但是我们也可以提供自己的 ws:\ntoken ws {\\h*} # 匹配水平空白, 不包括换行 rule 中空白的使用:\nmy $str = \u0026#34;Swift is hard to learn\u0026#34;; my token word {\\w+} my rule line {\u0026lt;word\u0026gt;+%[\u0026#39;,\u0026#39;]} $str~~m:g/ \u0026lt;line\u0026gt; /; 逗号附近的方括号保证了 \u0026lt;.ws\u0026gt; 调用产生的空白作为分割符的一部分。这利用了 \u0026lt;.ws\u0026gt; 的一个特点：\n在两个 \\w 之间解释为 \\s+, 其它地方解释为 \\s*。\n","permalink":"https://ohmyweekly.github.io/notes/2016-03-11-difference-between-rules-and-tokens/","tags":["token","rule"],"title":"token 和 rule 的区别"},{"categories":["rakulang"],"contents":"Multi 程序相当整洁, 但对于我来说似乎并不完整。一些背景 — 人们可以这样计算阶乘:\nmulti fac(0) { 1 } multi fac(Int $n where 1..Inf) { $n * fac( $n-1 ) } say fac(4); # 24 现在假设我们要把我们的递归 multi-sub 作为一个回调传递会怎样呢？\ngiven \u0026amp;fac -\u0026gt; $some_fun { say \u0026#34;some_fun(4)=\u0026#34;, $some_fun(4) } 现在\u0026hellip; 定义一个匿名的 multi-sub 怎么样？\nmy $anon_fac = do { multi hidden_fac(0) { 1 } multi hidden_fac(Int $n where 1..Inf) { $n * fac( $n - 1 ) } \u0026amp;hidden_fac }; say $anon_fac(4); # 24 这也会有作用, 但是有点 hack 的味道, 并且我们的 multi-sub 并不是真正的匿名。它仅仅是被隐藏了。真正匿名的对象不会在任何作用域中安装, 而在这个例子中, \u0026ldquo;hidden_fac\u0026rdquo; 被安装在 \u0026ldquo;do\u0026rdquo; block 中的本地作用域中。\nRaku说明书没有排除匿名的 multi 程序, 而且事实上\nmy $anon_fac = anon multi sub(0) { 1 } 会报一个错误:\n Cannot use \u0026lsquo;anon\u0026rsquo; with individual multi candidates. Please declare an anon-scoped proto instead\n 不能对单独的 multi 候选者使用 anon。请声明一个 anon-scoped 的 proto 代替。\n让我们回到原先那个以 \u0026ldquo;multi fac(0) { 1 }\u0026rdquo; 开始的例子。当编译器看到它, 就会在同一个作用域中为我们创建一个\u0026quot;proto fac\u0026quot; 作为 multi 定义。proto 的作用就像一个分发器(dispatcher) — 从概念上讲, 当我们调用 fac(4) 的时候, 我们让 proto fac 为我们从 multi facs 中挑选一个出来以调用。\n我们可以提前显式地定义一个 proto, 而且我们甚至能通过指定它的所有程序都需要 Int 类型的参数来对默认的 \u0026ldquo;proto\u0026rdquo; 加以改良。\nproto fac_with_proto(Int) { * } multi fac_with_proto(0) { 1 } multi fac_with_proto(Int $n where 1..Inf) { $n * fac( $n - 1 ) } say fac_with_proto(4); # 24 因此, anon muiti sub 抛出的错误 — Please declare an anon-scoped proto instead — 正是告诉我们 \u0026ldquo;没有要安装到的作用域, 我不能为你获取一个 proto。 使用你自己的 anon proto, 并把这个程序附加给它\u0026rdquo;。\n好的, 花蝴蝶, 感谢你的提醒! 我试试\u0026hellip;\nmy $fac_proto = anon proto uninstalled-fac(Int) { * }; say $fac_proto.name; # uninstalled-fac 好极了! 现在所有我们要做的就是给那个 proto 添加 multis。\n$fac_proto 是一个 Sub 对象, 它有方法来告诉你候选者, 但是没有办法设置(set) 候选者。并且我找不到任何方式在创建时传递一个候选者列表。\n适当的修补  什么会让 proto/multi 干净并且正交是一种方式去\n 在编译时指定候选者 在运行时添加候选者  这有点像\nmy $future_fac = Proto( :dispatch( sub (Int) {*} ), :candidates( [sub (0) {1}] ), :mutable ); $future_fac.candidates.push( sub (Int $n where 1..Inf) { $n * fac( $n-1 ) } ); $future_fac(4); # 24 我假定了一个 Sub 的子类 Proto 以揭露 multi 程序的内部工作原理。这个构造函数会允许定义任何 proto 声明符所做的: 签名 \u0026amp; 默认程序和名字。 还有, 它会允许在初始的候选者列表中传递一个属性。\n最后, 那个对象自身会让候选者方法返回一个数组, 而不是一个不可变列表, 如果 Proto 是使用 mutable 属性创建的话。不指定 mutable 将意味着所有的 multis 需要在编译时添加, 而不允许在运行时添加。\nhttp://blogs.perl.org/users/yary/2016/02/apropos-proto-perl6c-multi-thoughts.html\n","permalink":"https://ohmyweekly.github.io/notes/2016-02-11-multi-thoughts/","tags":["proto"],"title":"正确地使用 proto"},{"categories":["rakulang"],"contents":"## A Mutable Grammar For Raku\nRules Rules 就像 perl5的 regexes, 并且更好。它们像子例程和方法那样申明, 并且还能调用其它 rules\n下面是一个解析 Raku 基本变量名的例子：\ngrammar Raku { # token alpha 是一个预定义好的 rule token identifier {\u0026lt;alpha\u0026gt;\\w+} # 匹配一个全限定名标识符 # [ ... ] 是非捕获组 token name {\u0026lt;identifier\u0026gt;[\u0026#39;::\u0026#39;\u0026lt;identifier\u0026gt;]*} # .. | .. 是分支. 最长匹配胜出. token sigil {\u0026#39;$\u0026#39;|\u0026#39;@\u0026#39;|\u0026#39;\u0026amp;\u0026#39;|\u0026#39;%\u0026#39;|\u0026#39;::\u0026#39;} # \u0026lt;rule\u0026gt; 调用命名 rule, 隐式地锚定在当前位置 token variable {\u0026lt;sigil\u0026gt;\u0026lt;name\u0026gt;} } Grammars Grammar 跟类很像, 含有 rules 而不是 methods。Grammars 是 rules 的集合并支持继承。\n如果要求 Raku 中变量的名字必须大写：\n# 我们继承原来那个 grammar grammar PERL6 is Raku { # ... 重写我们想改变的解析规则 token identifier {# Raku 中的字符类现在写作 \u0026lt;[ ... ]\u0026gt; \u0026lt;[A..Z]\u0026gt;\u0026lt;[A..Z0..9_]\u0026gt;*} } 现在我们只需告诉编译器使用 PERL6 这个 grammar 而非默认 grammar 。还记得类中的方法调用顺序吗？ 先从本类开始, 沿着继承树从下而上到父类。Grammar 与之类似。\n然而有一个缺陷。假设你想更改一个符号, 例如把 $ 更改 为 ¢（因为你没有足够的 $$$ 来买下所有的变量, 不是吗？）看起来很简单：\ngrammar LowBudgetRaku is Raku { # token 就像类中的方法一样, 继承后可以修改 token sigil {\u0026#39;¢\u0026#39;|\u0026#39;@\u0026#39;|\u0026#39;\u0026amp;\u0026#39;|\u0026#39;%\u0026#39;|\u0026#39;::\u0026#39;} } 新的 grammar 解析工作的很好, 但是那之后的所有东西肯定会失败。当编译器在解析树里看见 sigil 匹配时, 它得找出到底是哪一个 - 这意味着它必须要检查匹配文本的字面值, 而它并不知道怎么处理 ¢。\n所以, 我们需要更多的技能\u0026hellip;\nProto Regexes  proto regex 是一套有着相同名字的 regexes/rules, 当前的 Raku grammar 使用这个结构：\nproto token sigil {*} # ... token sigil:sym\u0026lt;$\u0026gt; {\u0026lt;sym\u0026gt;} token sigil:sym\u0026lt;@\u0026gt; {\u0026lt;sym\u0026gt;} token sigil:sym\u0026lt;%\u0026gt; {\u0026lt;sym\u0026gt;} token sigil:sym\u0026lt;\u0026amp;\u0026gt; {\u0026lt;sym\u0026gt;} token sigil:sym\u0026lt;::\u0026gt; {\u0026lt;sym\u0026gt;} 这创建了一个叫做 sigil 的组(proto), 组里面有使用 sym 标识符参数化的 5 个规则(rules)（它们属于这个组因为它们跟组的名字相同）。 第一个把 sym 设置为 $ 然后匹配这个符号(使用\u0026lt;sym\u0026gt;). 第二个匹配 @ 等等。现在如果调用规则 \u0026lt;sigil\u0026gt;, 你会得到一个含有上述所有 5 个规则的列表, 列表元素之间是或的关系。所以它依然跟正则 '$' | '@' | '%' | '\u0026amp;' | '::' 匹配相同的东西, 但是更容易扩展。\n如果你想添加一个新的符号, Grammar 中唯一要修改的就是添加另外一个 sigil 规则： grammar SigilRichP6 is Raku { token sigil:sym\u0026lt;ħ\u0026gt; {\u0026lt;sym\u0026gt;} # 物理学家会很爱你 } 回到原来那个例子, 你可以重写已存在的规则：\ngrammar LowBudgetRaku is Raku { token sigil:sym\u0026lt;$\u0026gt; {\u0026#39;¢\u0026#39;} } 现在这个 grammar 为标量使用了一个不同的符号, 但是它和原来的 grammar 有着相同的规则和相同的参数(sigil:sym\u0026lt;$\u0026gt;), 编译器仍然知道怎么处理它。\n","permalink":"https://ohmyweekly.github.io/notes/2015-11-16-a-mutable-grammar-for-raku/","tags":["grammar"],"title":"A Mutable Grammar for Raku"},{"categories":["rakulang"],"contents":"comb - 操作字符串的利器\ncomb 子例程 comb 子例程的定义为：\nmulti sub comb(Regex $matcher, Str(Cool) $input, $limit = *) returns List:D multi method comb(Regex $matcher, $limit = *) returns List:D 用法：\ncomb /PATTERN/, STRING, LIMIT? # 子例程形式 STRING.comb(/PATTERN/, LIMIT?) # 方法形式 返回调用者（方法形式）的所有（或者至多 $limit 个，如果提供了的话）匹配，或者返回第二个参数（sub 形式）与 Regex 相匹配的字符串列表。\nsay \u0026#34;6 or 12\u0026#34;.comb(/\\d+/).join(\u0026#34;, \u0026#34;); # 6, 12 Str 类中的 comb multi sub comb(Str:D $matcher, Str:D $input, $limit = Inf) multi sub comb(Regex:D $matcher, Str:D $input, $limit = Inf, Bool :$match) multi sub comb(Int:D $size, Str:D $input, $limit = Inf) multi method comb(Str:D $input:) multi method comb(Str:D $input: Str:D $matcher, $limit = Inf) multi method comb(Str:D $input: Regex:D $matcher, $limit = Inf, Bool :$match) multi method comb(Str:D $input: Int:D $size, $limit = Inf) 在 $input 中搜索 $matcher 并返回所有匹配（默认是 Str，或者是 Match 对象，如果 $match 为真的话）的一个列表。$limit 表示至多返回 $limit 个匹配。\n如果没有提供 $matcher(匹配器)， 那么会返回字符串中的所有字符的列表。等价于使用了 $matcher = rx/./。\n例子：\ncomb(/\\w/, \u0026#34;a;b;c\u0026#34;).perl; # (\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;).list comb(/\\N/, \u0026#34;a;b;c\u0026#34;).perl; # (\u0026#34;a\u0026#34;, \u0026#34;;\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;;\u0026#34;, \u0026#34;c\u0026#34;).list comb(/\\w/, \u0026#34;a;b;c\u0026#34;, 2).perl; # (\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;).list comb(/\\w\\;\\w/, \u0026#34;a;b;c\u0026#34;, 2).perl; # (\u0026#34;a;b\u0026#34;,).list \u0026#34;123abc456def\u0026#34;.comb(3) # (123 abc 456 def) \u0026#34;123abc456def\u0026#34;.comb(3,2); # (123 abc) 如果匹配器（matcher）是一个整数值，那么它被认为和 /. ** matcher/ 类似，但是这个快了 30 倍。\n","permalink":"https://ohmyweekly.github.io/notes/2016-02-03-comb-in-raku/","tags":["comb"],"title":"comb - 操作字符串的利器"},{"categories":["rakulang"],"contents":"你是一个刚成立的小公司里的一名软件工程师, 有天晚上你收到了一封来自 CEO 的电子邮件:\n 亲爱的工程师,\n​ 好新闻！看起来我们的网站越来越受欢迎。我们要变的有钱了! 每秒钟有成千上万的人在同时访问我们的网站, 而且还在快速增长。\n我们必须立即识别出谁的通信量最大。幸运的是我的朋友给我发送了一份巨大的 IP 地址和名字的列表。很酷不是吗？你能写一段程序接收我们大量的访问者, 把它和地址/名字列表相比, 并创建一些统计吗？我的意思是, 生成一个国家的名字列表。\n做好了的话我给你们开个披萨聚会。\n  邮件的附件文件包含了一个 IP 地址和名字的列表。写一个程序来统计下有多少 IP 访问了你的网站。\n 输入描述 输入来自两部分。第一个是一个文本文件, 包含 IP 地址范围。每行一项,使用两个空格分割 IP 和名字。\n第二个文件是一个 IP 地址的列表, 每行一个, 它们是必须被查询的IP。\nIP 输入样本 输入是有包含两个 IP 地址和一个跟 IP 范围关联的名字的大量行组成。\n123.45.17.8 123.45.123.45 University of Vestige 123.50.1.1 123.50.10.1 National Center for Pointlessness 188.0.0.3 200.0.0.250 Mayo Tarkington 200.0.0.251 200.0.0.255 Daubs Haywire Committee 200.0.1.1 200.255.255.255 Geopolitical Encyclopedia 222.222.222.222 233.233.233.233 SAP Rostov 250.1.2.3 250.4.5.6 Shavian Refillable Committee 123.45.100.0 123.60.32.1 United Adverbs 190.0.0.1 201.1.1.1 Shavian Refillable Committee 238.0.0.1 254.1.2.3 National Center for Pointlessness 注意: 这些 IP 范围不能保证是 IPv4 \u0026ldquo;子网\u0026rdquo;。这意味着它们可能不能精确地由基于前缀的 CIDR 块来表示。\n范围可以重叠。可能多余2层深。\n可可有多个范围关联同一个名字。\n查询输入样本 250.1.3.4 123.50.1.20 189.133.73.57 123.50.1.21 250.1.2.4 123.50.1.21 250.1.3.100 250.1.3.5 188.0.0.5 123.50.1.100 123.50.2.34 123.50.1.100 123.51.100.52 127.0.0.1 123.50.1.22 123.50.1.21 188.0.0.5 123.45.101.100 123.45.31.52 230.230.230.230 输出格式化 倒序输出访问次数。\n8 - National Center for Pointlessness 4 - Shavian Refillable Committee 3 - Mayo Tarkington 2 - University of Vestige 1 - SAP Rostov 1 - United Adverbs 1 - \u0026lt;unknown\u0026gt; 解释 这儿是一个输入 IP 和它的名字的映射:\nNational Center for Pointlessness 123.50.1.20 123.50.1.21 123.50.1.22 123.50.1.21 123.50.1.21 123.50.1.100 123.50.1.100 123.50.2.34 Shavian Refillable Committee 250.1.2.4 250.1.3.4 250.1.3.5 250.1.3.100 Mayo Tarkington 188.0.0.5 188.0.0.5 189.133.73.57 University of Vestige 123.45.101.100 123.45.31.52 SAP Rostov 230.230.230.230 United Adverbs 123.51.100.52 \u0026lt;unknown\u0026gt; 127.0.0.1 smls 的解决方法:\nsub ip-to-number ($ip) { do given $ip.split(\u0026#39;.\u0026#39;) { .[0] +\u0026lt; 24 + .[1] +\u0026lt; 16 + .[2] +\u0026lt; 8 + .[3] +\u0026lt; 0 } } class IntervalTree { has $.min; has $.max; has $!center = ($!min + $!max) div 2; has @!intervals; has IntervalTree $!left; has IntervalTree $!right; method new ($min, $max) { self.bless(:$min, :$max) } method insert (|c ($start, $end, $name)) { if $end \u0026lt; $!center and $!min \u0026lt; $!center - 1 { ($!left //= self.new($!min, $!center)).insert(|c) } elsif $start \u0026gt; $!center and $!max \u0026gt; $!center { ($!right //= self.new($!center, $!max)).insert(|c) } else { @!intervals.push: [$start, $end, $name, $end-$start] } } method prepare { @!intervals.=sort(*[3]); $!left .prepare if $!left; $!right.prepare if $!right; } method lookup ($n) { my $best = ($n \u0026lt; $!center ?? ($!left .lookup($n) if $!left) !! ($!right.lookup($n) if $!right)); $best ?? @!intervals.first({ return $best if .[3] \u0026gt; $best[3]; .[0] \u0026lt;= $n \u0026lt;= .[1] }) // $best !! @!intervals.first({ .[0] \u0026lt;= $n \u0026lt;= .[1] }) } } sub MAIN ($ip-file, $query-file) { my $index = IntervalTree.new(0, ip-to-number \u0026#39;255.255.255.255\u0026#39;); for $ip-file.IO.lines { my ($start, $end, $name) = .split(\u0026#39;\u0026#39;, 3); $index.insert(ip-to-number($start), ip-to-number($end), $name); } $index.prepare; for $query-file.IO.lines -\u0026gt; $ip { my $name = $index.lookup(ip-to-number $ip)[2]; say \u0026#34;$ip{$name // \u0026#39;\u0026lt;unknown\u0026gt;\u0026#39;}\u0026#34;; } } ","permalink":"https://ohmyweekly.github.io/notes/2015-11-20-guess-who/","tags":["rakulang"],"title":"Guess Who"},{"categories":["rakulang"],"contents":"my class List is Iterable does Positional { .. } List 以序列化的方式存储 items并且潜在是惰性的。\n默认列表和数组的索引从 0 开始。\n你可以给列表中的元素赋值如果它们是容器的话。使用数组以使列表中的每个元素存储在容器中。\nItems、Flattening 和 Sigils 在 Raku 中, 把 List 赋值给一个标量变量不会丢失信息。不同之处在于迭代通常会把标量中的列表当作单个元素。\nmy @a = 1, 2, 3; for @a { } # 三次迭代 my $s = @a; for $s { } # 一次迭代 for @a.item { } # 一次迭代 for $s.list { } # 三次迭代 Lists 通常会插值(展开)除非它们通过一个 item(scalar)容器访问:\nmy @a = 1, 2, 3; my @flat = @a, @a; # two elements my @nested = @a.item, @a.item; # two elements .item 通常能被写为 $( ... ), 而在数组变量上甚至写为 $@a。\nMethods elems multi sub elems($list) returns Int:D multi method elems(List:D:) returns Int:D 返回列表中元素的个数。\nend multi sub end($list) returns Int:D multi method end(List:D:) returns Int:D 返回列表中最后一个元素的索引\nkeys multi sub keys($list) returns List:D multi method keys(List:D:) returns List:D 返回一个索引列表( 例如 0..(@list.elems-1) )\nvalues multi sub values($list) returns List:D multi method values(List:D:) returns List:D 返回列表的一份拷贝。\nkv multi sub kv($list) returns List:D multi method kv(List:D:) returns List:D 返回索引和值的交替的列表。例如：\n\u0026lt;a b c\u0026gt;.kv 返回:\n0, \u0026#39;a\u0026#39;, 1, \u0026#39;b\u0026#39;, 2, \u0026#39;c\u0026#39; pairs multi sub pairs($list) returns List:D multi method pairs(List:D:) returns List:D 返回一个 pairs 的列表, 使用索引作为键, 列表值作为键值。\n\u0026lt;a b c\u0026gt;.pairs # 0 =\u0026gt; \u0026#39;a\u0026#39;, 1 =\u0026gt; \u0026#39;b\u0026#39;, 2 =\u0026gt; \u0026#39;c\u0026#39; join multi sub join($separator, *@list) returns Str:D multi method join(List:D: $separator) returns Str:D 把列表中元素当作字符串, 在元素之间插入 $separator 并把所有东西连接成单个字符串。\n例如:\njoin \u0026#39;, \u0026#39;, \u0026lt;a b c\u0026gt;; # \u0026#39;a, b, c\u0026#39; map multi sub map(\u0026amp;code, *@elems) returns List:D multi method map(List:D: \u0026amp;code) returns List:D 对每个元素调用 \u0026amp;code 并且把值收集到另外一个列表中并返回它。这个过程是惰性的。\u0026amp;code只在返回值被访问的时候调用。\n例子:\n(\u0026#39;hello\u0026#39;, 1, 22/7, 42, \u0026#39;world\u0026#39;).map: { .WHAT.raku }; # Str Int Rat Int Str map *.Str.chars, \u0026#39;hello\u0026#39;, 1, 22/7, 42, \u0026#39;world\u0026#39;; # 5 1 8 2 5 grep multi sub grep(Mu $matcher, *@elems) returns List:D multi method grep(List:D: Mu $matcher) returns List:D 返回一个使用 $matcher 智能匹配的惰性列表。元素是以出现在原列表中的顺序返回的。\n例子:\n(\u0026#39;hello\u0026#39;, 1, 22/7, 42, \u0026#39;world\u0026#39;).grep: Int; # 1 42 grep { .Str.chars \u0026gt; 3 }, \u0026#39;hello\u0026#39;, 1, 22/7, 42, \u0026#39;world\u0026#39;; # hello 3.142857 world first multi sub first(Mu $matcher, *@elems) multi method first(List:D: Mu $matcher) 返回列表中第一个匹配 $matcher 的元素, 当没有匹配值时, 失败。\n例子:\nsay (1, 22/7, 42).first: * \u0026gt; 5; # 42 say $f = (\u0026#39;hello\u0026#39;, 1, 22/7, 42, \u0026#39;world\u0026#39;).first: Complex; (\u0026#39;hello\u0026#39;, 1, 22/7, 42, \u0026#39;world\u0026#39;,1+2i).first: Complex; # 1+2i say $f.raku; # Failure.new(exception =\u0026gt; X::AdHoc.new(payload =\u0026gt; \u0026#34;No values matched\u0026#34;)) classify multi sub classify(\u0026amp;mapper, *@values) returns Hash:D multi method classify(List:D: \u0026amp;mapper) returns Hash:D 根据映射器把一列值转换成代表那些值的类别的散列; 散列的每个键代表着将要归入列表的一个或多个值的类别。比如字符个数, 元素多少, 键值就是根据 mapper 得到的这个类别下的元素, 它来自于原始列表：\n例子：\nsay classify { $_ %% 2 ?? \u0026#39;even\u0026#39; !! \u0026#39;odd\u0026#39; }, (1, 7, 6, 3, 2); # (\u0026#34;odd\u0026#34; =\u0026gt; [1, 7, 3], \u0026#34;even\u0026#34; =\u0026gt; [6, 2]).hash; say (\u0026#39;hello\u0026#39;, 1, 22/7, 42, \u0026#39;world\u0026#39;).classify: { .Str.chars }; # (\u0026#34;5\u0026#34; =\u0026gt; [\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;], \u0026#34;1\u0026#34; =\u0026gt; [1], \u0026#34;8\u0026#34; =\u0026gt; [22/7], \u0026#34;2\u0026#34; =\u0026gt; [42]).hash Bool multi method Bool(List:D:) returns Bool:D 如果列表至少含有一个元素则返回 True, 如果列表为空则返回 False。\nStr multi method Str(List:D:) returns Str:D 字符串化列表中的元素并使用空格把这些元素连接起来。( 和 .join(' ') 一样)。\nInt multi method Int(List:D:) return Int:D 返回列表中元素的数量(和 .elems 一样)\npick multi sub pick($count, *@list) returns List:D multi method pick(List:D: $count = 1) 从调用者身上随机返回 $count 个不重复的元素。 如果 * 作为 $count 传递进来或 $count 大于或等于列表的大小, 那么就以随机序列的方式返回列表中的所有元素。\n例子:\nsay \u0026lt;a b c d e\u0026gt;.pick; # b say \u0026lt;a b c d e\u0026gt;.pick: 3; # c a e say \u0026lt;a b c d e\u0026gt;.pick: *; # e d a b c roll multi sub roll($count, *@list) returns List:D multi method roll(List:D: $count = 1) 返回一个 $count 个元素的惰性列表, 每个元素都从列表中随机选择。每个随机选择都是独立的.\n如果给 $count 传递了 * 号, 则返回一个惰性的, 从原列表中随机选取元素的无限列表。\nsay \u0026lt;a b c d e\u0026gt;.roll; # b say \u0026lt;a b c d e\u0026gt;.roll: 3; # c c e say roll 8, \u0026lt;a b c d e\u0026gt;; # b a e d a e b c my $random_digits := (^10).roll(*);1; say $random_digits[^15]; # 3 8 7 6 0 1 3 2 0 8 8 5 8 0 5 eager multi method eager(List:D:) returns List:D 急切地计算列表中的所有元素, 并返回调用者。如果列表标示它是 \u0026ldquo;konw inifinite\u0026rdquo; 的, 急切求值可以停止在探测到的无限的点上。\nreverse multi sub reverse(*@list ) returns List:D multi method reverse(List:D:) returns List:D 以相反的顺序返回一个含有相同元素的列表。 注意 reverse 总是指反转列表中的元素, 如果你想反转字符串中的字符, 那么使用 flip。\nsay \u0026lt;hello world!\u0026gt;.reverse # world! hello say reverse ^10 # 9 8 7 6 5 4 3 2 1 0 rotate multi sub rotate(@list, Int:D $n = 1) returns List:D multi method rotate(List:D: Int:D $n = 1) returns List:D 以 $n 个元素旋转列表, 这把原列表分成两部分, 旋转中心就是在这两部分之间:\n\u0026lt;a b c d e\u0026gt;.rotate(2); # \u0026lt;c d e a b\u0026gt; \u0026lt;a b c d e\u0026gt;.rotate(-1); # \u0026lt;e a b c d\u0026gt; sort multi sub sort(*@elems) returns List:D multi sub sort(\u0026amp;by, *@elems) returns List:D multi method sort(List:D:) returns List:D multi method sort(List:D:, \u0026amp;by) returns List:D 列表排序, 最小的元素首先。默认使用 infix:\u0026lt;cmp\u0026gt; 排序列表中的元素。\n如果提供了 \u0026amp;by, 那么它接收两个参数, 它由列表元素对儿调用, 并且应该返回 Order::Increase, Order::Same 或 Order::Decrease。\n如果 \u0026amp;by 只接受一个参数, 那么列表元素是通过 by($a) cmp by($b) 来排序的。\u0026amp;by 的返回值被缓存起来, 以使每个列表元素只调用一次 \u0026amp;by。\nsay (3, -4, 7, -1, 2, 0).sort; # -4 -1 0 2 3 7 say (3, -4, 7, -1, 2, 0).sort: *.abs; # 0 -1 2 3 -4 7 say (3, -4, 7, -1, 2, 0).sort: { $^b leg $^a }; # 7 3 2 0 -4 -1 reduce multi sub reduce(\u0026amp;with, *@elems) multi method reduce(List:D: \u0026amp;with) 把 \u0026amp;with 应用到列表中的第一个和第二个值上, 然后把 \u0026amp;with 应用到那个计算的结果值和第三个值上, 以此类推。按照那种方式生成单个项。\n注意 reduce 是一个隐式的循环。\nsay (1, 2, 3).reduce: * - *; # -4 splice multi sub splice(@list, $start, $elems?, *@replacement) returns List:D multi method splice(List:D: $start, $elems?, *@replacement) returns List:D 从列表中删除从 $start 索引开始的 $elems 个元素, 返回删除的元素并用 @replacement 来代替它。如果省略了 $elems, 所有从 $index 开始的元素都被删除。\nmy @foo = \u0026lt;a b c d e f g\u0026gt;; say @foo.splice(2, 3, \u0026lt;M N O P\u0026gt;); # c d e say @foo; # a b M N O P f g pop multi sub pop(List:D ) multi method pop(List:D:) 从列表中移除并返回最后一项。如果列表为空则失败。\n\u0026gt; my @foo = \u0026lt;a b\u0026gt;; \u0026gt; @foo.pop; # b \u0026gt; pop @foo # a \u0026gt; pop @foo # Element popped from empty list push multi sub push(List:D, *@values) returns List:D multi method push(List:D: *@values) returns List:D 把 @values 添加到列表的末尾, 并返回修改后的列表。 如果列表是无限列表则失败。\nmy @foo = \u0026lt;a b c\u0026gt;; @foo.push: 1, 3 ... 11; say @foo; # a b c 1 3 5 7 9 11 shift multi sub shift(List:D ) multi method shift(List:D:) 从列表中移除并返回第一项元素。如果列表为空则失败。\nmy @foo = \u0026lt;a b\u0026gt;; say @foo.shift; # a say @foo.shift; # b say @foo.shift; # Element shifted from empty list unshift multi sub unshift(List:D, *@values) returns List:D multi method unshift(List:D: *@values) returns List:D 添加 @values 到列表的开头, 并返回修改后的列表。如果列表是无限列表则失败。\nmy @foo = \u0026lt;a b c\u0026gt;; @foo.unshift: 1, 3 ... 11; say @foo; # 1 3 5 7 9 11 a b c combinations multi method combinations (List:D: Int:D $of) returns List:D multi method combinations (List:D: Range:D $of = 0..*) returns List:D multi sub combinations ($n, $k) returns List:D Int 变体返回调用者列表所有的 $of-combinations 组合。例如:\nsay .join(\u0026#39;|\u0026#39;) for \u0026lt;a b c\u0026gt;.combinations(2); 打印\na|b a|c b|c 因为 \u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;, \u0026lsquo;c\u0026rsquo; 的所有 2-combinations 是 [\u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;], [\u0026lsquo;a\u0026rsquo;, \u0026lsquo;c\u0026rsquo;], [\u0026lsquo;b\u0026rsquo;, \u0026lsquo;c\u0026rsquo;].\nRange 变体把所有单独的组合组合到单个列表中, 所以:\nsay .join(\u0026#39;|\u0026#39;) for \u0026lt;a b c\u0026gt;.combinations(2..3); 打印：\na|b a|c b|c a|b|c 因为那是一个所有 2-和3-combinations 组合的列表。\n子例程 combinations($n, $k) 等价于 (^$n).combinations($k), 所以：\n.say for combinations(4, 2) 打印：\n0 1 0 2 0 3 1 2 1 3 2 3 permutations multi method permutations(List:D:) returns List:D multi sub permutations($n) returns List:D 返回列表所有可能的组合作为数组的列表。所以:\nsay .join(\u0026#39;|\u0026#39;) for \u0026lt;a b c\u0026gt;.permutations 打印：\na|b|c a|c|b b|a|c b|c|a c|a|b c|b|a permutations 把所有列表元素当作可区别的, 所以 (1, 1, 2).permutations 仍旧返回 6 个元素的列表, 即使只有 3 个不同的排列。\npermutations($n) 等价于 (^$n).permutations, 所以:\n.say for permutations 3; 打印：\n1 2 3 1 3 2 2 1 3 2 3 1 3 1 2 3 2 1 ","permalink":"https://ohmyweekly.github.io/notes/2015-06-10-list-in-raku/","tags":["rakulang"],"title":"List in Raku"},{"categories":["rakulang"],"contents":"收集 Raku 方面的博文如下:\n Raku 的 Grammar 学习总结 Raku 的正则表达式 Raku 的正则表达式 Perl 6 的列表 Raku 的字符串 Raku URB 解析 Raku 格式化帮助信息 Raku 解决 24 Game Raku 神的90亿名字整数版 Raku 计算矩阵占比 Raku 24游戏 Raku temporary Raku 括号匹配 Raku 上传文件 译 Raku Object Orientation 译 Native Calling Interface 译 Raku 5to6-nutshell Raku 括号的匹配 神的90亿名字整数版 Raku 正则替换 Raku 累加器工厂 Raku IEEE754 Raku usbip 自动 bind Raku 5to6-perlfunc Raku GetOption Raku Search Binary-Tree node Raku Getopt::Kinoko  ","permalink":"https://ohmyweekly.github.io/notes/2016-01-01-raku-blogs/","tags":["blog"],"title":"Raku Blogs"},{"categories":["rakulang"],"contents":"Grammars Grammars - 一组具名 regexes 组成正式的 grammar\nGrammars 是一个很强大的工具用于析构文本并通常返回数据结构。\n例如, Raku 是使用 Raku 风格 grammar 解析并执行的。\n对普通 Raku 使用者更实用的一个例子是 JSON::Tiny模块, 它能反序列化任何合法的 JSON 文件, 而反序列代码只有不到 100 行, 还能扩展。\nGrammars 允许你把 regexes 组织到一块儿, 就像类(class) 中组织方法那样。\n具名正则 (Named Regexes) 　grammars 的主要组成部分是 regexes。 而 Raku 的 regexes语法不在该文档的讨论范围, 具名正则(named regexes) 有它自己的特殊语法, 这跟子例程(subroutine) 的定义很像:\nmy regex number {\\d+[\\.\\d+]?} # 普通 regex 中空格被忽略, [] 是非捕获组 上面的代码使用 my 关键字指定了本地作用域的 regex, 因为具名正则(named regexes) 通常用在 grammars 里面。\n正则有名字了就方便我们在任何地方重用那个正则了:\nsay \u0026#34;32.51\u0026#34; ~~ \u0026amp;number; say \u0026#34;15 + 4.5\u0026#34; ~~ /\\s*\u0026#39;+\u0026#39;\\s*/ \u0026amp;number # my regex number { \\d+ [ \\. \\d+ ]? }  为什么用 \u0026amp;number, 对比具名子例程你就知道了:\n\u0026gt; sub number { say \u0026#34;i am a subroutine\u0026#34; } # 具名子例程 \u0026gt; \u0026amp;number # sub number () { #`(Sub|140651249646256) ... } \u0026amp;number 就是直接引用了具名的 regex 或 子例程。而在 / / 或 grammars 里面, 引用一个具名正则的语法也很特殊, 就是给名字包裹上 \u0026lt; \u0026gt;。\u0026lt;\u0026gt; 就像引号那样, 当用它引起某个具名正则后, 引用这个 \u0026lt;\u0026gt; 就会把该具名正则插入(带入)到整个正则之中, 就像字符串插值那样：\n# 具名正则的声明 my regex number {\\d+[\\.\\d+]?} my token ident {\\w+} my rule alpha {\u0026lt;[A..Za..z]\u0026gt;} # 1.0 通过 \u0026amp; 来引用 say so \u0026#34;12.34\u0026#34; ~~ \u0026amp;number; # true # 2.0 在正则构造 // 里使用 say so \u0026#34;12.88 + 0.12\u0026#34; ~~ /\\s*\u0026#39;+\u0026#39;\\s*/; # true # say so \u0026#34;12.88 + 0.12\u0026#34; ~~ / \\s* \u0026#39;+\u0026#39; \\s* /; # wrong, method \u0026#39;number\u0026#39; not found for invocant of class \u0026#39;Cursor\u0026#39; # 3.0 在 grammar 里面使用 grammar EquationParse { # 这里也不能给 number 起别名, 除非 number 是在 grammar 内部声明的 token TOP {\\s*\u0026#39;+\u0026#39;\\s*\\s*\u0026#39;=\u0026#39;\\s*} } # 等式解析 my $expr = EquationParse.parse(\u0026#34;12.88 + 0.12 = 13.00\u0026#34;); say $expr; 声明具名正则不是只有一个 regex 声明符, 实际上 , regex 声明符用的最少, 大多数时候, 都是使用 token 或 rule 声明符。token 和 rule 这两个都是 ratcheing (棘轮)的, 这意味着如果匹配失败, 那么匹配引擎就不会回并尝试匹配了。这通常会是你想要的, 但不适用于所有情况:\n 棘轮用于单向驱动, 防止逆转。\n my regex works-but-slow {.+q } # 可能会回溯 my token fails-but-fast {.+q } # 不回溯 my $s = \u0026#39;Tokens and rules won\\\u0026#39;t backtrack, which makes them fail quicker!\u0026#39;; say so $s ~~ \u0026amp;works-but-slow; # True say so $s ~~ \u0026amp;fails-but-fast; # False, .+ 得到了整个字符串但不回溯 token 和 rule 的唯一区别就是 rule 声明符会让正则中的 :sigspace 修饰符起效:\nmy token non-space-y {\u0026#39;once\u0026#39;\u0026#39;upon\u0026#39;\u0026#39;a\u0026#39;\u0026#39;time\u0026#39;} my rule space-y {\u0026#39;once\u0026#39;\u0026#39;upon\u0026#39;\u0026#39;a\u0026#39;\u0026#39;time\u0026#39;} say \u0026#39;onceuponatime\u0026#39; ~~ \u0026amp;non-space-y; say \u0026#39;once upon a time\u0026#39; ~~ \u0026amp;space-y; 创建 Grammar 　当使用 grammar 关键字而非 class 关键字声明来声明一个类时, 会自动得到以 Grammar 的父类。Grammars 应该只用于解析文本; 如果你想提取复杂的数据, 推荐 action object和 grammar 一块使用。\nProto regexes Grammars 由 rules，token 和 regexes 组成; 他们实际上是方法，因为 grammars 是类。这些方法可以共享一个共同的名称和功能，因此可以使用 proto。\n如果你有很多备选分支(alternations), 那么生成可读性好的代码或子类化(subclass)你的 grammar 可能会变得很困难。在下面的 Actions 类中, TOP 方法中的三元操作符并不理想, 并且当我们添加的操作越多, 它就变得越糟糕:\ngrammar Calculator { token TOP {[\u0026lt;add\u0026gt;|\u0026lt;sub\u0026gt;]} rule add {\u0026lt;num\u0026gt;\u0026#39;+\u0026#39;\u0026lt;num\u0026gt;} rule sub {\u0026lt;num\u0026gt;\u0026#39;-\u0026#39;\u0026lt;num\u0026gt;} token num {\\d+} } class Calculations { method TOP ($/) { make $\u0026lt;add\u0026gt; ?? $\u0026lt;add\u0026gt;.made !! $\u0026lt;sub\u0026gt;.made; } method add ($/) { make [+] $\u0026lt;num\u0026gt;; } method sub ($/) { make [-] $\u0026lt;num\u0026gt;; } } say Calculator.parse(\u0026#39;2 + 3\u0026#39;, actions =\u0026gt; Calculations).made; # OUTPUT: # 5 为了让事情变得更好, 我们可以在 tokens 身上使用看起来像 :sym\u0026lt;...\u0026gt; 那样的副词来使用正则表达式原型(protoregexes):\ngrammar Calculator { token TOP {\u0026lt;calc-op\u0026gt;} proto rule calc-op {*} rule calc-op:sym\u0026lt;add\u0026gt; {\u0026lt;num\u0026gt;\u0026#39;+\u0026#39;\u0026lt;num\u0026gt;} rule calc-op:sym\u0026lt;sub\u0026gt; {\u0026lt;num\u0026gt;\u0026#39;-\u0026#39;\u0026lt;num\u0026gt;} token num {\\d+} } class Calculations { method TOP ($/) { make $\u0026lt;calc-op\u0026gt;.made; } method calc-op:sym\u0026lt;add\u0026gt; ($/) { make [+] $\u0026lt;num\u0026gt;; } method calc-op:sym\u0026lt;sub\u0026gt; ($/) { make [-] $\u0026lt;num\u0026gt;; } } say Calculator.parse(\u0026#39;2 + 3\u0026#39;, actions =\u0026gt; Calculations).made; # OUTPUT: # 5 在这个 grammar 中, 备选分支(alternation)已经被 \u0026lt;calc-op\u0026gt; 替换掉了, 它实质上是我们将要创建的一组值的名字。我们通过使用 proto rule calc-op 定义了一个 rule 原型类型(prototype) 来达成。我们之前的每一个备选分支已经被新的 rule calc-op 替换掉了并且备选分支的名字被附加上了 :sym\u0026lt;\u0026gt; 副词。\n在 actions 类中, 我们现在摆脱了三目操作符, 仅仅只在 $\u0026lt;calc-op\u0026gt; 匹配对象上接收 .made 值。并且单独备选分支的 actions 现在和 grammar 遵守相同的命名模式: method calc-op:sym\u0026lt;add\u0026gt; 和 method calc-op:sym\u0026lt;sub\u0026gt;。\n当你子类化(subclass)那个 grammar 和 actions 类的时候才能看到这个方法的真正魅力。假设我们想为 calculator 增加一个乘法功能:\ngrammar BetterCalculator is Calculator { rule calc-op:sym\u0026lt;mult\u0026gt; {\u0026lt;num\u0026gt;\u0026#39;*\u0026#39;\u0026lt;num\u0026gt;} } class BetterCalculations is Calculations { method calc-op:sym\u0026lt;mult\u0026gt; ($/) { make [*] $\u0026lt;num\u0026gt; } } say BetterCalculator.parse(\u0026#39;2 * 3\u0026#39;, actions =\u0026gt; BetterCalculations).made; # OUTPUT: # 6 所有我们需要添加的就是为 calc-op 组添加额外的 rule 和 action, 感谢正则表达式原型(protoregexes), 所有的东西都能正常工作。\n特殊的 Tokens TOP grammar Foo { token TOP {\\d+} } TOP token 是默认的第一个尝试去匹配的 token , 当解析一个 grammar 的时候 - 那颗树的根。注意如果你正使用 .parse 方法进行解析, 那么 token TOP 被自动地锚定到字符串的开头和结尾(再看看 .subparse)。\n使用 rule TOP 或 regex TOP 也是可以接受的。\n在 .parse、.subparse 或 .parsefile Grammar 方法中使用 :rule 命名参数可以选择一个不同的 token 来进行首次匹配。\nws 当使用 rule 而非 token 时, 原子(atom)后面的任何空白(whitespace)被转换为一个对 ws 的非捕获调用。即:\nrule entry {\u0026lt;key\u0026gt;\u0026#39;=\u0026#39;\u0026lt;value\u0026gt;} 等价于:\ntoken entry {\u0026lt;key\u0026gt;\u0026lt;.ws\u0026gt;\u0026#39;=\u0026#39;\u0026lt;.ws\u0026gt;\u0026lt;value\u0026gt;\u0026lt;.ws\u0026gt;} # . = non-capturing 默认的 ws 匹配\u0026quot;空白\u0026quot;(whitespace), 例如空格序列(不管什么类型)、换行符、unspaces、或 heredocs。\n提供你自己的 ws token 是极好的:\ngrammar Foo { rule TOP {\\d\\d} }.parse: \u0026#34;4 \\n\\n 5\u0026#34;; # Succeeds grammar Bar { rule TOP {\\d\\d} token ws {\\h*} }.parse: \u0026#34;4 \\n\\n 5\u0026#34;; # Fails 上面的例子中, 在 Bar Gramamr 中重写了自己的 ws, 只匹配水平空白符, 所以 \\n\\n 匹配失败。\nsym \u0026lt;sym\u0026gt; token 可以在原型正则表达式(proto regex) 中使用，以匹配那个特定正则表达式的 :sym 副词的字符串值：\ngrammar Foo { token TOP {\u0026lt;letter\u0026gt;+} proto token letter {*} token letter:sym\u0026lt;P\u0026gt; {\u0026lt;sym\u0026gt;} token letter:sym\u0026lt;e\u0026gt; {\u0026lt;sym\u0026gt;} token letter:sym\u0026lt;r\u0026gt; {\u0026lt;sym\u0026gt;} token letter:sym\u0026lt;l\u0026gt; {\u0026lt;sym\u0026gt;} token letter:sym\u0026lt;*\u0026gt; {.} }.parse(\u0026#34;I ♥ Perl\u0026#34;, actions =\u0026gt; class { method TOP($/) { make $\u0026lt;letter\u0026gt;.grep(*.\u0026lt;sym\u0026gt;).join } }).made.say; # OUTPUT: «Perl␤» 当你已经将原型正则表达式与要匹配的字符串区分开来时，这很方便，因为使用 \u0026lt;sym\u0026gt; token 可防止重复这些字符串。\n总是成功断言 \u0026lt;?\u0026gt; is the always succeed assertion(总是匹配成功). 当它用作 grammar 中的 token 时, 它可以被用于触发一个 Action 类方法。在下面的 grammar 中, 我们查找阿拉伯数字并且使用 always succeed assertion 定义一个 succ token。\n在 action 类中, 我们使用对 succ 方法的调用来设置(在这个例子中, 我们在 @!numbers 中准备了一个新元素)。在 digit 方法中, 我们把阿拉伯数字转换为梵文数字并且把它添加到 @!numbers 数组的最后一个元素中。多亏了 succ, 最后一个元素总是当前正被解析的 digit 数字的数。\ngrammar Digifier { rule TOP {[\u0026lt;.succ\u0026gt;\u0026lt;digit\u0026gt;+]+} token succ {\u0026lt;?\u0026gt;} token digit {\u0026lt;[0..9]\u0026gt;} } class Devanagari { has @!numbers; method digit ($/) { @!numbers[*-1] ~= $/.ord.\u0026amp;[+](2358).chr } method succ ($) { @!numbers.push: \u0026#39;\u0026#39; } method TOP ($/) { make @!numbers[^(*-1)] } } say Digifier.parse(\u0026#39;255 435 777\u0026#39;, actions =\u0026gt; Devanagari.new).made; # OUTPUT: # (२५५ ४३५ ७७७) Grammar 中的方法 在 grammar 中使用 method 代替 rule 或 token 也是可以的, 只要它们返回一个 Cursor 类型:\ngrammar DigitMatcher { method TOP (:$full-unicode) { $full-unicode ?? self.num-full !! self.num-basic; } token num-full {\\d+} token num-basic {\u0026lt;[0..9]\u0026gt;+} } 上面的 grammar 会根据 parse 方法提供的参数尝试不同的匹配:\nsay +DigitMatcher.subparse: \u0026#39;12७१७९०९\u0026#39;, args =\u0026gt; \\(:full-unicode); # OUTPUT: # 12717909 say +DigitMatcher.subparse: \u0026#39;12७१७९०९\u0026#39;, args =\u0026gt; \\(:!full-unicode); # OUTPUT: # 12 Action Object 　一个成功的 grammar 匹配会给你一棵匹配对象(Match objects)的解析树, 匹配树(match tree)到达的越深, 则 grammar 中的分支越多, 那么在匹配树中航行以获取你真正感兴趣的东西就变的越来越困难。\n为了避免你在匹配树(match tree)中迷失, 你可以提供一个 action object。grammar 中每次解析成功一个具名规则(named rule)之后, 它就会尝试调用一个和该 grammar rule 同名的方法, 并传递给这个方法一个Match 对象作为位置参数。如果不存在这样的同名方法, 就跳过。\n这儿有一个例子来说明 grammar 和 action：\ngrammar TestGrammar { token TOP {^\\d+$} } class TestActions { method TOP($/) { $/.make(2 + $/); # 等价于 $/.make: 2 + $/ } } my $actions = TestActions.new; # 创建 Action 实例 my $match = TestGrammar.parse(\u0026#39;40\u0026#39;, :$actions); say $match; # ｢40｣ say $match.made; # 42 TestActions 的一个实例变量作为具名参数 actions 被传递给 parse 调用, 然后当 token TOP 匹配成功之后, 就会自动调用方法 TOP, 并传递匹配对象(match object) 作为方法的参数。\n为了让参数是匹配对象更清楚, 上面的例子使用 $/ 作为 action 方法的参数名, 尽管那仅仅是一个方便的约定, 跟内在无关。$match 也可以。(尽管使用 $/ 可以提供把 $ 作为 $/ 的缩写的优势。)\n下面是一个更有说服力的例子:\ngrammar KeyValuePairs { token TOP {[\u0026lt;pair\u0026gt;\\n+]*} token ws {\\h*} rule pair {\u0026lt;key=.identifier\u0026gt;\u0026#39;=\u0026#39;\u0026lt;value=.identifier\u0026gt;} token identifier {\\w+} } class KeyValuePairsActions { method pair ($/) { $/.make: $\u0026lt;key\u0026gt;.made =\u0026gt; $\u0026lt;value\u0026gt;.made } method identifier($/) { # 子例程 `make` 和在 $/ 上调用 .make 相同 make ~$/ } method TOP ($match) { # TOP 方法的参数可以使用任意变量名, 而不仅仅是 $/ $match.make: $match\u0026lt;pair\u0026gt;».made } } my $res = KeyValuePairs.parse(q:to/EOI/,:actions(KeyValuePairsActions)).made; second=b hits=42 perl=6 EOI for @$res -\u0026gt; $p { say \u0026#34;Key: $p.key()\\tValue: $p.value()\u0026#34;; } 这会输出:\nKey: second Value: b Key: hits Value: 42 Key: perl Value: 6 pair 这个 rule, 解析一对由等号分割的 pair, 并且给 identifier 这个 token 各自起了别名。对应的 action 方法构建了一个 Pair 对象, 并使用子匹配对象(sub match objects)的 .made 属性。这也暴露了一个事实: submatches 的 action 方法在那些调用正则/外部正则之前就被调用。所以 action 方法是按后续调用的。\n名为 TOP 的 action 方法仅仅把由 pair 这个 rule 的多重匹配组成的所有对象收集到一块, 然后以一个列表的方式返回。\n注意 KeyValuePairsActions 是作为一个类型对象(type object)传递给方法 parse的, 这是因为 action 方法中没有一个使用属性(属性只能通过实例来访问)。\n其它情况下, action 方法可能会在属性中保存状态。 那么这当然需要你传递一个实例给 parse 方法。\n注意, token ws 有点特殊: 当 :sigspace 开启的时候(就是我们使用 rule的时候), 我们覆写的 ws 会替换某些空白序列。这就是为什么 rule pair 中等号两边的空格解析没有问题并且闭合 } 之前的空白不会狼吞虎咽地吃下换行符, 因为换行符在 TOP token 已经占位置了, 并且 token 不会回溯。\n# ws 的内置定义 /\u0026lt;.ws\u0026gt;/ # match \u0026#34;whitespace\u0026#34;: # \\s+ if it\u0026#39;s between two \\w characters, # \\s* otherwise my token ws {\\h*} # 重写 ws 这个内置的 token \u0026gt;say so \u0026#34;\\n\u0026#34; ~~ \u0026amp;ws # True 所以 \u0026lt;.ws\u0026gt; 内置的定义是：如果空白在两个 \\w 单词字符之间, 则意思为 \\s+, 否则为 \\s*。 我们可以重写 ws 关于空白的定义, 重新定义我们需要的空白。比如把 ws 定义为 { \\h* } 就是所有水平空白符, 甚至可以将ws 定义为非空白字符。例如: token ws { 'x' }\n","permalink":"https://ohmyweekly.github.io/notes/2016-02-05-raku-grammars/","tags":["grammar"],"title":"Raku Grammars"},{"categories":["rakulang"],"contents":":my $foo 的作用域和用途 在 regex、token 或 rule 中, 定义像下面这样的变量是可能的:\ntoken directive {:my $foo = \u0026#34;in command\u0026#34;;\u0026lt;command\u0026gt;\u0026lt;subject\u0026gt;\u0026lt;value\u0026gt;?} 在中提到了一点有关该变量的东西, 我引用过来:\n 任何 grammar regex 实际上是一种方法, 并且你可以在这样一个子例程中使用一个冒号跟着任何作用域声明符来声明一个变量, 这些声明符包括 my, our, state 和 constant (作为类似的声明符, temp 和 let 也能被识别). 单个语句(直到结尾的分号或行末尾的闭括号为止) 被解析为普通的 Raku 代码:\n token prove-nondeterministic-parsing {:my $threshold = rand;\u0026#39;maybe\u0026#39;\\s+\u0026lt;it($threshold)\u0026gt;} 有谁能解释下这段代码的应用场景吗？\nwhat scope does :my $foo; have? :my $foo 在它所出现的 rule/token/regex 中拥有词法作用域(lexical scope)。你所得到的作用域要么很大要么很小:\ngrammar g { regex r1 {{ my $foo; ...}# `$foo` 在该 block 的结尾超出作用域。 ...{ say $foo; }# `$foo` 不在作用域中。 } } grammar i { my $foo; regex r1 {...} # 在 `r1` 内部, `$foo` 被识别出。 ... regex r999 {...} # 但是在 r999 中也是。 } 它的用途? 使用 :my $foo; 形式的变量声明以在 rule/token/regex 中声明本地作用域的变量, 如果没有进一步的声明, 那么这些变量能在 rule/token/regex 中的任何地方通过所声明的名字来引用。举个例子, 你可以看看 Rakudo 的 Grammar.nqp 源代码中的 token babble 中声明的 @extra_tweaks 变量的用法。\n使用 :my $*foo; 形式的变量声明来声明动态的词法变量。动态变量能够, 在没有进一步声明的情况下, 在闭合词法作用域和闭合动态作用域中通过它们声明的名字来引用。作为说明, 请查看 the declaration of @*nibbles in Rakudo\u0026rsquo;s Grammar module 和 its use in Rakudo\u0026rsquo;s Actions module 。\n一般的使用场景 在 regular expressions 中一般不使用 :… 风格的声明。:...; 结构通常用在特别复杂和庞大的 grammars 中。对于这些使用场景, 依靠 Raku 的正则表达式和闭包的一致性是合适的。正是这使得 rule/token/regex 级别的 :...; 变量声明变得正当。\nRegexes 和 closures 的一致性 很多 grammars 都是上下文有关的.\nRaku 使 regexes 和 closures 统一了:\nsay Regex.^mro; (Regex) (Method) (Routine) (Block) (Code) ... mro 是方法解析顺序, 这足以说明 regex 实际上是一种特殊类型的方法(就像方法是一种特殊类型的子例程一样)。\nRaku: is there a phaser that runs only when you fall out of a loop? #!/usr/bin/env raku ROLL: for 1..10 -\u0026gt; $r { given (1..6).roll { when 6 { say \u0026#34;Roll $r:you win!\u0026#34;; last ROLL; } default { say \u0026#34;Roll $r:sorry...\u0026#34;; } } LAST { say \u0026#34;You either won or lost - this runs either way\u0026#34;; } } 更优雅的写法:\nconstant N = 5; for flat (1..6).roll xx * Z 1..N -\u0026gt; $_, $n { print \u0026#34;roll $n:$_\u0026#34;; when 6 { put \u0026#34;(won)\u0026#34;; last; } default { put \u0026#34;(lost)\u0026#34;; } LAST { print \u0026#34;result: \u0026#34;; when 6 { put \u0026#34;winner :)\u0026#34; } default { put \u0026#34;loser :(\u0026#34; } } } 怎么从命令行传递一个复数给 sub MAIN? #!/usr/bin/env raku sub MAIN($x) { say \u0026#34;$xsquared is { $x*$x }\u0026#34;; } 我要在命令行中传递一个复数给 MAIN:\n % ./square i  Cannot convert string to number: base-10 number must begin with valid digits or '.' in '⏏i' (indicated by ⏏) in sub MAIN at ./square line 7 in block \u0026lt;unit\u0026gt; at ./square line 5 Actually thrown at: in sub MAIN at ./square line 7 in block \u0026lt;unit\u0026gt; at ./square line 5 当我把脚本变为:\n#!/usr/bin/env raku sub MAIN(Complex $x) { say \u0026#34;$xsquared is { $x*$x }\u0026#34;; } 它竟然彻底罢工了:\n% ./square i Usage: square \u0026lt;x\u0026gt; % ./square 1 Usage: square \u0026lt;x\u0026gt; 一种方法是使用 Coercive type declaration (强制类型声明), 从 Str 到 Complex:\nsub MAIN(Complex(Str) $x) { say \u0026#34;$x的平方为 { $x * $x }\u0026#34;; } 那么:\n% ./squared.pl 1 1+0i 的平方为 1+0i % ./squared.pl 1+2i 1+2i 的平方为 -3+4i 但是:\n$ ./test.pl6 2 Usage: ./test.p6 \u0026lt;x\u0026gt; 所以你真正需要的是把其它 Numeric 类型强转为 Complex 类型:\n#!/usr/bin/env raku sub MAIN ( Complex(Real) $x ) { say \u0026#34;$xsquared is { $x*$x }\u0026#34;; } 我使用 Real 而非 Numeric, 因为 Complex 已经涵盖了其它的了。\nBlessing a Hash into an object 为什么我写的这段代码不对呢？\nclass WordCount { has %!words; # Tried with both . and ! method new($string) { my %words; my @sentence = split(/\\s+/, $string); for @sentence -\u0026gt; $word { %words{$word}++; } return self.bless(:%words); } method sayCounts() { my @keys = keys(%!words); for @keys -\u0026gt; $key { say $key ~ \u0026#34;\u0026#34; ~ %!words{$key}; } } } sub MAIN { my $sentence = \u0026#34;the boy jumped over the dog\u0026#34;; my $wordCount = WordCount.new($sentence); $wordCount.sayCounts(); } Raku-ify:\nclass WordCount { has Int %.words is default(0); method new($string) { my Int %words; for $string.split(/\\s+/) -\u0026gt; $word { %words{$word}++; } self.bless(:%words) } method gist { %.words.map({.value ~ \u0026#34;\u0026#34; ~ .key}).join(\u0026#34;\\n\u0026#34;) } } my $word-count = WordCount.new(\u0026#39;the boy jumped over the dog\u0026#39;); say $word-count; 散列中的每一项都是一个 Pair:\nmy %w = a =\u0026gt; 1; %w.map({ say $_.^name }) # OUTPUT«Pair␤» 所以:\n%.words.map({.value ~ \u0026#34;\u0026#34; ~ .key}).join(\u0026#34;\\n\u0026#34;) 等价于:\n%.words.kv.map( -\u0026gt; $word, $count { \u0026#34;$word$count\u0026#34; } ).join(\u0026#34;\\n\u0026#34;) 你还可以使用 sub-signature(子签名)来解构 .map 提供的 Pair：\n%.words.map( -\u0026gt; (:key($word), :value($count)) { \u0026#34;$word$count\u0026#34; } ).join(\u0026#34;\\n\u0026#34;) 字符串处理  将每行从第二列到最后一列数值为0的且数目多于6个的行删除\n 数据：\nOG004240: 1 3 1 1 9 0 4 5 1 1 6 1 2 OG004241: 1 2 1 4 7 2 1 3 1 2 9 1 1 OG004242: 1 2 1 2 4 1 3 9 2 2 4 2 2 OG004243: 0 4 1 2 9 2 4 5 1 2 3 1 1 OG004244: 0 2 1 3 8 3 3 2 2 3 4 2 2 OG004245: 0 3 1 2 7 3 3 0 3 2 7 2 2 OG004246: 0 0 2 0 1 15 0 15 0 0 1 0 1 my @lines = \u0026#34;a.txt\u0026#34;.IO.lines; for @lines -\u0026gt; $line { my @words = $line.split(/\\s+/); say $line unless @words[1..*].grep(* eq 0).elems \u0026gt; 6; } 使用 rakudoc -f Str.split 查看 split 的帮助文档。\n合并相同行：\n文件一：\n1###ENSMMUP00000017866###-###27582-27683 1###ENSMMUP00000017866###-###27508-27576 1###ENSMMUP00000017866###-###27290-27503 1###ENSMMUP00000040736###-###199515-200498 1###ENSMMUP00000040736###-###198582-198818 1###ENSMMUP00000030409###+###395728-395934 1###ENSMMUP00000030409###+###403004-403148 想合并相同的，生成文件格式如下：\n1###ENSMMUP00000017866###-###27582-27683 27508-27576 27290-27503 1###ENSMMUP00000040736###-###199515-200498 198582-198818 1###ENSMMUP00000030409###+###395728-395934 403004-403148 一种方法如下：\nmy @lines = \u0026#34;a.txt\u0026#34;.IO.lines; my %hash; for @lines -\u0026gt; $line { $line.match(/^(.*?)(\\d+\u0026#39;-\u0026#39;\\d+)/); %hash{$0} ~= $1 ~ \u0026#34;\u0026#34;; } for %hash.kv -\u0026gt; $key, $value { say $key, $value; } 如下数据，想去掉第3列重复的行且保留的行要使第四列最小, 原始数据：\n326 0.00 0.00 ( 0 ) 63 0.00 2.43 ( 0.0082 ) 64 0.00 2.43 ( 0.0082 ) 120 0.00 2.43 ( 0 ) 340 0.00 4.03 ( 0 ) 99 0.00 9.14 ( 0.0229 ) 441 0.00 9.14 ( 0.0232 ) 142 0.00 10.77 ( 0.0569 ) 292 0.00 10.77 ( 0.0393 ) 266 0.00 10.77 ( 0.0233 ) 想要的结果：\n326 0.00 0.00 ( 0 ) 120 0.00 2.43 ( 0 ) 340 0.00 4.03 ( 0 ) 99 0.00 9.14 ( 0.0229 ) 266 0.00 10.77 ( 0.0233 ) 一种方法如下：\nmy @lines = \u0026#34;a.txt\u0026#34;.IO.lines; my %hash; for @lines -\u0026gt; $line { $line.match(/(\\d+\\.\\d+)\\s+\\(\\s+(\\S+)/); %hash{$0} ~= $1 ~ \u0026#34;\u0026#34;; } for @lines -\u0026gt; $line { $line.match(/(\\d+\\.\\d+)\\s+\\(\\s+(\\S+)/); for %hash.kv -\u0026gt; $key, $value { say $line if $0 ~~ $key \u0026amp;\u0026amp; $1 ~~ $value.words.min; } } 有 gene.txt 和 in.txt 两个文件, 文件内容如下:\ngene.txt:（2000多行)\nchr1 ABCA4 94458582 94586799 chr1 ACADM 76190031 76229363 chr16 BBS2 56518258 56554008 chr17 G6PC 41052813 41066450 chr17 GAA 78078244 78093271 in.txt:(5万多行)\n1 94505603 rs368951547 C T NA NA 1 94505604 rs61750126 A C 0.02066 NA 1 94505611 rs137853898 G A NA not-provided 1 94505620 rs370967816 T A NA NA 1 94505621 rs149503495 T C NA NA 1 94505627 rs374610040 A G NA NA 22 18901263 rs377148163 C A NA NA 22 18901290 rs381848 G A 0.07989 NA 22 18901322 rs62232347 C A NA NA 22 18901326 rs201353896 TCC T 0.05005 NA 22 18901327 rs10537001 CCT C 0.0528 NA 16 18901326 rs201353896 TCC T 0.05005 NA 17 18901327 rs10537001 CCT C 0.0528 NA gene.txt 和 in.txt 的第一列的数字部分相同，并且 In 的第二列在gene 的三四列范围之间，就输出 in.txt 中的那一行。\n解决方法：\nmy @lines = \u0026#34;a.txt\u0026#34;.IO.lines; my @inlines = \u0026#34;in.txt\u0026#34;.IO.lines; my %hash; for @lines -\u0026gt; $line { $line.match(/^chr(\\d+)\\s+(\\w+)\\s+(\\d+)\\s+(\\d+)/); %hash{$0~$1~$2~$3} = $0 ~ \u0026#34;\u0026#34; ~ $2 ~ \u0026#34;\u0026#34; ~ $3; } for @inlines -\u0026gt; $line { $line.match(/^(\\d+)\\s+(\\d+)/); for %hash.values -\u0026gt; $value { say $line if $0 ~~ $value.words[0] \u0026amp;\u0026amp; $1 \u0026lt;= $value.words[1].Num \u0026amp;\u0026amp; $1 \u0026lt;= $value.words[2].Num; } } 例如我现在数组中的值是 @project = ('NX11','NX12','NX13’)。\n另外一个数组是 @get = ('ss','ssfd','NX12','sed','NX11’)。\n现在把第一个数组中出现过的值，如果第二个数组中也有的话删除掉，然后保留第二个数组剩下的值。\n使用差集：\n@get (-) @project 有如下数据：\nPL -0.00 5.50 PL -0.25 3.50 PL -0.50 0.00 PL -0.75 4.50 -0.25 -0.00 1.00 -0.25 -0.25 4.50 -0.25 -0.50 1.00 -0.75 -0.75 1.00 -0.75 -1.00 0.00 -1.00 -0.25 3.50 -1.00 -0.50 0.00 -1.00 -1.25 3.40 -1.00 -1.75 4.00 将第一列值相同的行合并， 分使合并第二列和第三列：\n结果如下：\nPL -0.00 -0.25 -0.50 -0.75 PL 5.50 3.50 0.00 4.50 ... 面向对象 Fluent interface (流接口)\n在软件工程中，一个流接口（fluent Interface）是指实现一种实现面向对象的能提高代码可读性的API的方法。 在 Raku 中有很多种方法, 但是最简单的一种是声明属性为可读写并使用 given 关键字。类型注释是可选的。\nclass Employee { subset Salary of Real where * \u0026gt; 0; subset NonEmptyString of Str where * ~~ /\\S/; # 至少一个非空白符号 has NonEmptyString $.name is rw; has NonEmptyString $.surname is rw; has Salary $.salary is rw; method gist { return qq:to[END];Name: $.nameSurname: $.surnameSalary: $.salaryEND } } my $employee = Employee.new(); given $employee { .name = \u0026#39;Sally\u0026#39;; .surname = \u0026#39;Ride\u0026#39;; .salary = 200; } say $employee; # Output: # Name: Sally # Surname: Ride # Salary: 200  在 Raku 中怎样检查文件的时间戳属性  在 Raku 中怎样检查文件的时间戳属性？ 在 Perl 5 中是使用文件测试操作符 file test operators , 在 Raku 中是使用来自于 IO::FileTestable role 的方法 (e.g. .modified, .accessed and .changed) 。\n例如:\nmy $filename = \u0026#34;sample.txt\u0026#34;; my $seconds_since_epoch = $filename.IO.accessed; my $readable_timestamp = DateTime.new($filename.IO.accessed); say \u0026#34;File \u0026#39;$filename\u0026#39;was last accessed at \u0026#39;$readable_timestamp\u0026#39;, which is {$seconds_since_epoch.Num}seconds since the epoch\u0026#34;; 2、我正尝试生成包含 10 个随机随机序列的 FASTQ 文件，序列由随机品质分数构成。我原来是使用下面的代码，它工作良好:\nmy @seq = (rand_fa_seq() for ^10); my @qual = (rand_qual() for ^10); @seq.raku.say; @qual.raku.say; sub rand_fa_seq { return join(\u0026#34;\u0026#34;, roll(20,\u0026#34;ACGT\u0026#34;.comb)); } sub rand_qual { return join(\u0026#34;\u0026#34;, roll(20,\u0026#34;EFGHIJ\u0026#34;.comb)) } 等价于：\nsub rand-fa-seq($n = 20) { \u0026lt;A C G T\u0026gt;.roll($n).join } sub rand-qual($n = 20) { \u0026lt;E F G H I J\u0026gt;.roll($n).join } my @seq = rand-fa-seq() xx 10; my @qual = rand-qual() xx 10; 3、在 Raku 中使用 \u0026ldquo;列表解析\u0026rdquo; 生成非平方数列表\n在 Raku 中我怎样使用 \u0026ldquo;列表解析\u0026rdquo; 创建一组非平方数? 我在 Rosetta Code 那儿看到了如何打印一组非平方数的代码：\nsub nth_term (Int $n) { $n + round sqrt $n } say nth_term $_ for 1 .. 22; 目前为止我看到的最接近的东西是使用 for 关键字。 但是因为这实际上仅仅是一个内联（inline）循环，我认为这从技术上来讲并不是列表解析，尽管它看起来相似：\nmy @y = ($_**2 + 1 for 1 .. 10); 但是，我真正想知道是否有一种 “列表解析 “ 的方法来创建可在数学上描述的诸如非平方数的列表。这儿有一个我用来创建一组非平方数的方法（直到 30）：\nmy @non_squares = grep {sqrt($_) != floor(sqrt($_))}, 1 .. 30; 我怎样用列表解析来实现它呢？\n实际上， 你的例子 my @y = ($_**2 + 1 for 1 .. 10); 是 Raku 方式写成的列表解析。你还可以添加一个条件测试， 就像 Raku design document S04 中建议的那样：\n 为了轻松地书写列表解析， 循环语句修饰符允许包含单个条件语句修饰符：\n sub odd(Int $n) {return $n % 2} @evens = ($_ * 2 if .\u0026amp;odd for 0..100); 这个就是怎样写一个 Raku 列表解析的非平方数（直到 30）：\nmy @non_squares = ($_ if .sqrt != .sqrt.Int for 1 .. 30); 一丢丢解释：在每次 for 循环迭代中， 从 1 到 30 这个范围中的当前数字会被赋值给默认变量 $_（等价于 it）。没有调用者的方法调用会默认在 “it\u0026quot; 身上调用（例如 .sqrt 等价于 $_.sqrt）。 所以，对于 1到30中的每一个数字，它的平方根被检查以查看它是否有非整数平方根。 如果是真， 那它就被包含在列表中。\n4、Raku 中的 Print 函数和冒号\n我想知道在 Raku 中冒号与方法和函数调用有什么关系。\n我在 Raku spec test (S32-io) 中看到了这个(我添加了注释):\n$fh.print: \u0026#34;0123456789A\u0026#34;; # prints \u0026#39;0123456789A\u0026#39; to the file 据我所知，这等价于：\n$fh.print(\u0026#34;0123456789A\u0026#34;); # prints \u0026#39;0123456789A\u0026#39; to the file 这两种方式看起来都接收多个参数而且展平列表也没问题：\n$fh.print: \u0026#34;012\u0026#34;, \u0026#34;345\u0026#34;, \u0026#34;6789A\u0026#34;; # prints \u0026#39;0123456789A\u0026#39; to the file $fh.print(\u0026#34;012\u0026#34;, \u0026#34;345\u0026#34;, \u0026#34;6789A\u0026#34;); # prints \u0026#39;0123456789A\u0026#39; to the file my @a = \u0026lt;012 345 6789A\u0026gt;; $fh.print(@a); # prints \u0026#39;0123456789A\u0026#39; to the file $fh.print: @a; # prints \u0026#39;0123456789A\u0026#39; to the file 存在这两种语法一定有某种原因。 使用这种或另一种语法有某种理由吗？\n我还注意到，当作为方法使用时， 我们不得不使用带有 : 或 ()的 print：\n$fh.print(@a); # Works $fh.print: @a; # Works! $fh.print @a; # ERROR! 当使用带冒号的 print 函数时，还有一些有意思的行为。 在这种情况下, ： 和 () 不等价：\nprint @a; # Prints \u0026#39;0123456789A\u0026#39; (no newline, just like Perl 5) print(@a); # Ditto print: @a; # Prints \u0026#39;012 345 6789A\u0026#39; followed by a newline (at least in REPL) print @a, @a; # Error (Two terms in a row) print: @a, @a; # Prints \u0026#39;012 345 6789A 012 345 6789A\u0026#39; followed by a newline (in REPL) 然后我尝试在脚本文件中使用print。这对于打印到标准输出有效：\nprint @a; 然而， 这不会打印到标准输出：\nprint: @a, @a; 但是方法版本的工作良好：\n$fh.print: @a, @a; # Prints \u0026#39;0123456789A0123456789A\u0026#39; to the file 我感觉我已经理解了这个， 但是不能用语言表达出来。有人可以解释下使用 print 的这些变化吗。 还有， 这些行为会因为 Great List Refactor 而改变吗？\nAnswer：\n使用冒号代替圆括号的一个主要原因是通过移除一组圆括号，它能使代码更清晰。在其它方面它们真的一样。\n当你使用 print: @a ， 那你真正在做的就是在行上放置一个标签， 并让 @a 落进去（fall-through）。这在 REPL 中会调用带有值的 say 方法。\n如果你没有在方法调用中使用括号或冒号，， 那么方法会以无参数方式调用。\n你可以交换方法的顺序，还有调用者，如果你使用冒号的话。\nsay $*ERR: \u0026#39;hello world\u0026#39;; # $*ERR.say(\u0026#39;hello world\u0026#39;)    我刚刚确认了， 就像你说的， print: @a 就是 label: @a, label 可以是任何东西. – Christopher Bottoms Jun 26 at 14:12       换句话说，冒号能代替方法调用的圆括号，但不能代替子例程调用。 – Christopher Bottoms Jun 26 at 14:12    5、排序散列键值对儿\nmy %hash = two =\u0026gt; 2, three =\u0026gt; 3, one =\u0026gt; 1, ; for %hash.sort(*.key)».kv -\u0026gt; ($key, $value) { say \u0026#34;\u0026#39;$key\u0026#39;=\u0026gt; \u0026#39;$value\u0026#39;\u0026#34;; }  %hash.sort({.key})».kv 和上面的 sort 等价吗?\n为什么这个 sort 没有 hyper » 提示就不会工作?\n这个 sort方法返回一个 Pairs 的列表。\n因为在列表身上调用 .kv 会返回一个索引, Pair 列表, 这不是你想要的; 你不能单单在列表身上调用 .kv 。所以你必须通过在每个 Pair 身上调用 .kv 方法分别从列表中的 Pair 中取出键和值, 这正是 ».kv 所做的。\n你还可以使用 .map(*.kv) 代替。\n».kv 语法允许把工作展开到多个线程中执行, 如果那样做有意义的话。\n(当前的 Rakudo仅以半随机的顺序工， 以防止人们错误地使用该特性 )\n通过在签名中使用副词以提取属性， 这是另一种 loop 写法：\nfor %hash.sort -\u0026gt; (:$key, :$value) { say \u0026#34;\u0026#39;$key\u0026#39;=\u0026gt; \u0026#39;$value\u0026#39;\u0026#34;; } for %hash.sort -\u0026gt; $pair (:$key, :$value) { say $pair; say $key === $pair.key and $value === $pair.value; # True␤ } # :$key is short for :key($key) for %hash.sort -\u0026gt; (:key($k), :value($v)) { say \u0026#34;\u0026#39;$k\u0026#39;=\u0026gt; \u0026#39;$v\u0026#39;\u0026#34;; } 这对其它没有方法创建一组它们公用属性的对象有用：\nclass C { has $.a; has $.b; has $.c; has $!private-value } my $c = 5; my $obj = C.new(:a\u0026lt;A\u0026gt;,:b(1),:$c); given $obj -\u0026gt; ( :$a, :b($b), :$c) ) { say \u0026#34;$a$b$c\u0026#34;; # A 1 5 } # ignore $.a by using an unnamed scalar given $obj -\u0026gt; ( :a($), :$b, :$c ) { ... } # places any unspecified public attributes in %others given $obj -\u0026gt; ( :$a, :$b, *%others ) { .say for keys %others; # c␤ } # 忽略任何未指定的属性 # useful to allow subclasses to add more attributes # 或仅仅丢弃掉任何你不关心的值 given $obj -\u0026gt; ( :$a, :$b, *% ) { ... } # 失败，因为它没有处理公用的 c 属性 # in the sub-signature given $obj -\u0026gt; ( :$a, :$b ) { ... } 关于签名能做什么，那只是开始。\n所有下面的，在子例程和方法签名中都是被允许的，非强制性的， 对于这个例子杀伤力过大。这在 multi subs 和 multi methods 中对于限制可能的候选者真的很有用。\nfor \u0026#39;one\u0026#39; =\u0026gt; 1, 1/3 -\u0026gt; # Type is an alias to the object type ::Type Any $_ # Any is the default type requirement # the public attributes of the object ( ::A-Type Any :key( :numerator( $a ) ), ::B-Type Any :value( :denominator( $b ) ) where $b \u0026gt;= 1, ) { my Type $obj = $_; # new variable declared as having the same type my A-Type $new-a = $a; my B-Type $new-b = $b; # could have used $_.^name or .^name instead of Type.^name # so you don\u0026#39;t actually have to add the alias to the signature # to get the name of the arguments type say Type.^name, \u0026#39;\u0026#39;, $_; say \u0026#39;\u0026#39;, A-Type.^name, \u0026#39;\u0026#39;, $a; say \u0026#39;\u0026#39;, B-Type.^name, \u0026#39;\u0026#39;, $b; } Pair one =\u0026gt; 1 Str one Int 1 Rat 0.333333 Int 1 Int 3 至于使用 .sort({.key}), 恩, 那从根本上来说是同一个东西, 因为 sort 在那儿接受任何 Callable 我要指出, 你甚至不需要为 sort 提供参数, 因为它默认比你给它的东西智能。\nRaku 有很多创建和访问 Callable 东西的方式。所以任何下面一种都可以工作：\n*.key { .key } # { $_.key } -\u0026gt; $_ { .key } # basically what the previous line turns into { $^placeholder-var.key } sub ($_) { .key } \u0026amp;a-subroutine-reference # you would have to create the subroutine though 还有， 因为所有普通的操作符实际上都是子例程，你可以在需要 Callable 的其它地方使用它们：\n\u0026amp;infix:\u0026lt;+\u0026gt; # the subroutines responsible for the numeric addition operator \u0026amp;[+] # ditto \u0026amp;prefix:\u0026lt;++\u0026gt; \u0026amp;postfix:\u0026lt;++\u0026gt; # etc  和 ( ) 的区别  # 无法正常排序 my @s = [2443,5,33, 90, -9, 2, 764]; say @s.sort; # 2443 5 33 90 -9 2 764 say @s.WHAT; # (Array) say @s.raku; # [[2443, 5, 33, 90, -9, 2, 764]]\u0026lt;\u0026gt; # 正常排序 my $array = [2443,5,33, 90, -9, 2, 764]; say $array.sort; # -9 2 5 33 90 764 2443 say $array.WHAT; # (Array) say $array.raku; # [2443, 5, 33, 90, -9, 2, 764] my @s = (2443,5,33,90,-9,2,764); say @s.sort; # -9 2 5 33 90 764 2443 say $array.WHAT; # (Array) say @s.raku; # [2443, 5, 33, 90, -9, 2, 764]\u0026lt;\u0026gt; 可见, 使用 [ ] 和 ( ) 创建数组是不一样的.\nmy @s = [2443, 5, 33, 90, -9, 2, 764]; 这创建了一个数组, 并把该数组赋值给 @s[0], 所以 @s 只有一个元素, 所以对 @s 进行排序是没有意义的. 然而你可以使用:\n@s[0].sort.say 来实现你要求的排序。\n How do I chain to an inline block in Raku?  我想修改一个数组(我在这个例子中使用了 splice, 但是它也可能是修改数组的任何操作)并返回修改后的数组 - 和 slice 不一样, slice 返回的是从数组中抠出的项。我可以很容易地通过在数组中存储一个 block 来做到, 就像下面这样:\nmy $1 = -\u0026gt; $a { splice($a,1,3,[1,2,3]); $a }; say (^6).map( { $_ \u0026lt; 4 ?? 0 !! $_ } ).Array; # [0 0 0 0 4 5] say (^6).map( { $_ \u0026lt; 4 ?? 0 !! $_ } ).Array.$1; # [0 1 2 3 4 5] 我怎么把由 $1 代表的 block 内联到单个表达式中呢？ 下面的解决方法不正确:\nsay (^6).map( { $_ \u0026lt; 4 ?? 0 !! $_ } ).Array.(-\u0026gt; $a { splice($a,1,3,[1,2,3]); $a }) Invocant requires a type object of type Array, but an object instance was passed. Did you forget a \u0026#39;multi\u0026#39;? 解决方法是添加一个 \u0026amp; 符号:\nsay (^6).map( { $_ \u0026lt; 4 ?? 0 !! $_ } ).Array.\u0026amp;(-\u0026gt; $a { splice($a,1,3,[1,2,3]); $a }) # 输出 [0 1 2 3 4 5]  Getting a positional slice using a Range variable as a subscript  my @numbers = \u0026lt;4 8 16 16 23 42\u0026gt;; .say for @numbers[0..2]; # this works # 4 # 8 # 15 # but this doesn\u0026#39;t my $range = 0..2; .say for @numbers[$range]; # 16 最后的那个下标看起来好像把 $range 解释为 range 中元素的个数(3)。怎么回事?\n解决方法\n使用 @numbers[|$range] 把 range 对象展平到列表中。或者在 Range 对象上使用绑定来传递它们。\n# On Fri Jul 2016, gfldex wrote: my @numbers = \u0026lt;4 8 15 16 23 42\u0026gt;; my $range = 0..2; .say for @numbers[$range]; # OUTPUT«16» # expected: # OUTPUT«4\\n 8\\n 15» # 这是对的, 并且还跟 \u0026#34;Scalar container implies item\u0026#34; 规则有关. # Changing it would break things like the second evaluation here: my @x = 1..10; my @y := 1..3; @x[@y] # (2 3 4) @x[item @y] # 4 # 注意在签名中 range 可以被绑定给 @y, 而特殊的 Range 可以生成一个像 @x[$(@arr-param)] 的表达式 # 这在它的语义中是不可预期的。 # 同样, 绑定给 $range 也能提供预期的结果 my @numbers = \u0026lt;4 8 15 16 23 42\u0026gt;; my $range := 0..2; .say for @numbers[$range]; # OUTPUT«4␤8␤15␤» # 这也是预期的结果, 因为使用绑定就没有标量容器来强制被当成一个 item 了。 # So, all here is working as designed. 或者：\n.say for @numbers[@($range)] # 4 # 8 # 15 绑定到标量容器的符号输出一个东西, 可以达到你想要的选择包含：\n前置一个 @ 符号来得到单个东西的复数形式：numbers[@$range]; 或者以不同的形式来声明 ragne 变量, 以使它直接工作。 对于后者, 考虑下面的形式:\n# Bind the symbol `numbers` to the value 1..10: my \\numbers = [0,1,2,3,4,5,6,7,8,9,10]; # Bind the symbol `rangeA` to the value 1..10: my \\rangeA := 1..10; # Bind the symbol `rangeB` to the value 1..10: my \\rangeB = 1..10; # Bind the symbol `$rangeC` to the value 1..10: my $rangeC := 1..10; # Bind the symbol `$rangeD` to a Scalar container # and then store the value 1..10 in it:` my $rangeD = 1..10; # Bind the symbol `@rangeE` to the value 1..10: my @rangeE := 1..10; # Bind the symbol `@rangeF` to an Array container and then # store 1 thru 10 in the Scalar containers 1 thru 10 inside the Array my @rangeF = 1..10; say numbers[rangeA]; # (1 2 3 4 5 6 7 8 9 10) say numbers[rangeB]; # (1 2 3 4 5 6 7 8 9 10) say numbers[$rangeC]; # (1 2 3 4 5 6 7 8 9 10) say numbers[$rangeD]; # 10 say numbers[@rangeE]; # (1 2 3 4 5 6 7 8 9 10) say numbers[@rangeF]; # (1 2 3 4 5 6 7 8 9 10) 绑定到标量容器($rangeD)上的符号总是产生单个值。在 [...] 下标中单个值必须是数字。 对于 range, 被当作单个数字时, 产生的是 range 的长度。\n","permalink":"https://ohmyweekly.github.io/notes/2015-08-22-raku-weekly-notes/","tags":["notes"],"title":"Raku Weekly Notes"},{"categories":["rakulang"],"contents":"模式匹配 匹配单个字符串:\nmy $name = \u0026#34;twostraws\u0026#34;; given $name { when \u0026#34;bilbo\u0026#34; { say \u0026#34;Hello, Bilbo Baggins!\u0026#34;} when \u0026#34;twostraws\u0026#34; { say \u0026#34;Hello, Paul Hudson!\u0026#34; } default { say \u0026#34;身份验证失败\u0026#34; } } 匹配元组:\nmy $bool1 = 1; my $bool2 = 0; given ($bool1, $bool2) { when (0, 0) {say \u0026#34;0, 0\u0026#34;} when (0, 1) {say \u0026#34;0, 1\u0026#34;} when (1, 0) {say \u0026#34;1, 0\u0026#34;} when (1, 1) {say \u0026#34;1, 1\u0026#34;} } given (\u0026#34;15\u0026#34;, \u0026#34;example\u0026#34;, \u0026#34;3.14\u0026#34;) { say $_.WHAT; when ($, $, Str) { say \u0026#34;I got a String of $_[2]\u0026#34; } } given (4, 5) { when ( $, $) {say \u0026#34;Ok\u0026#34;} } given (\u0026#34;fly.mp3\u0026#34;, 34, \u0026#34;It\u0026#39;s funny\u0026#34;) { when (/.mp3$/, /4$/, *.chars \u0026gt; 4) { say \u0026#34;Perfact\u0026#34; } } given 5 { when 1..10 { say \u0026#34;1..10 contains 5\u0026#34; } } 同时检查名字和密码 my $name = \u0026#34;twostraws\u0026#34;; my $password = \u0026#34;fr0st1es\u0026#34;; given ($name, $password) { when (\u0026#34;bilbo\u0026#34;, \u0026#34;bagg1n5\u0026#34;) { say \u0026#34;Hello, Bilbo Baggins!\u0026#34; } when (\u0026#34;twostraws\u0026#34;, \u0026#34;fr0st1es\u0026#34;) { say \u0026#34;Hello, Paul Hudson!\u0026#34; } default { say \u0026#34;你是谁?\u0026#34; } } 使用单个元组 my $authentication = (\u0026#34;twostraws\u0026#34;, \u0026#34;fr0st1es\u0026#34;); given $authentication { when (\u0026#34;bilbo\u0026#34;, \u0026#34;bagg1n5\u0026#34;) { say \u0026#34;Hello, Bilbo Baggins!\u0026#34; } when (\u0026#34;twostraws\u0026#34;, \u0026#34;fr0st1es\u0026#34;) { say \u0026#34;Hello, Paul Hudson!\u0026#34; } default { say \u0026#34;你是谁?\u0026#34; } } 部分匹配 # 你只关心某些感兴趣的值，不关心其它值，使用 `*` 号或 `$` 来代表 \u0026#34;any value is fine\u0026#34; my $authentication = (\u0026#34;twostraws\u0026#34;, \u0026#34;fr0st1es\u0026#34;, \u0026#34;127.0.0.1\u0026#34;); given $authentication { when (\u0026#34;bilbo\u0026#34;, \u0026#34;bagg1n5\u0026#34;, *) { say \u0026#34;Hello, Bilbo Baggins!\u0026#34;} when (\u0026#34;twostraws\u0026#34;, \u0026#34;fr0st1es\u0026#34;, $) { say \u0026#34;Hello, Paul Hudson!\u0026#34; } default { say \u0026#34;Who are you?\u0026#34; } } 只匹配元组的一部分 # 但仍然想知道其它部分是什么 my $authentication = (\u0026#34;twostraws\u0026#34;, \u0026#34;fr0st1es\u0026#34;); given $authentication { when (\u0026#34;bilbo\u0026#34;, *) { say \u0026#34;Hello, Bilbo Baggins!\u0026#34; } when (\u0026#34;twostraws\u0026#34;, *) { say \u0026#34;Hello, Paul Hudson: your password was $_!\u0026#34; } default { say \u0026#34;Who are you?\u0026#34; } } 匹配计算型元组 sub fizzbuzz(Int $number) returns Str { given ($number % 3 == 0, $number % 5 == 0) { when (True, False) { return \u0026#34;Fizz\u0026#34; } when (False, True) { return \u0026#34;Buzz\u0026#34; } when (True, True) { return \u0026#34;FizzBuzz\u0026#34; } when (False, False) { return $number.Str} } } say fizzbuzz(15); 遍历元组 my $twostraws = (\u0026#34;twostraws\u0026#34;, \u0026#34;fr0st1es\u0026#34;); my $bilbo = (\u0026#34;bilbo\u0026#34;, \u0026#34;bagg1n5\u0026#34;); my $taylor = (\u0026#34;taylor\u0026#34;, \u0026#34;fr0st1es\u0026#34;); my @users = $twostraws, $bilbo, $taylor; for @users -\u0026gt; $user { say $user[0]; } 使用 when 匹配元组中的指定值 my $twostraws = (\u0026#34;twostraws\u0026#34;, \u0026#34;fr0st1es\u0026#34;); my $bilbo = (\u0026#34;bilbo\u0026#34;, \u0026#34;bagg1n5\u0026#34;); my $taylor = (\u0026#34;taylor\u0026#34;, \u0026#34;fr0st1es\u0026#34;); my @users = $twostraws, $bilbo, $taylor; say \u0026#34;User twostraws has the password fr0st1es\u0026#34; when (\u0026#34;twostraws\u0026#34;, \u0026#34;fr0st1es\u0026#34;) for @users; # 打印秘密为指定值的用户 say \u0026#34;User $_[0]has password \\\u0026#34;fr0st1es\\\u0026#34;\u0026#34; when (*, \u0026#34;fr0st1es\u0026#34;) for @users; 匹配范围 my $age = 36; given $age { when 0 ..^ 18 { say \u0026#34;你有活力有时间，但是没钱\u0026#34; } when 18 ..^ 70 { say \u0026#34;你有活力有钱，但是没时间\u0026#34; } default { say \u0026#34;你有时间和金钱，但是没活力\u0026#34;} } when 可以配合智能匹配操作符 ~~ 单独使用 my $age = 36; when $age ~~ 0 ..^ 18 { say \u0026#34;你有活力有时间，但是没钱\u0026#34; } when $age ~~ 18 ..^ 70 { say \u0026#34;你有活力有钱，但是没时间\u0026#34; } default { say \u0026#34;你有时间和金钱，但是没活力\u0026#34;} 使用 contains 方法 my $age = 36; when (0 ..^ 18).contains($age) { say \u0026#34;你有活力有时间，但是没钱\u0026#34; } when (18 ..^ 70).contains($age) { say \u0026#34;你有活力有钱，但是没时间\u0026#34; } default { say \u0026#34;你有时间和金钱，但是没活力\u0026#34;} 匹配元组中的范围 my $user = (\u0026#34;twostraws\u0026#34;, \u0026#34;fr0st1es\u0026#34;, 36); given $user { my $name = $user[0]; when ($name, *, 0 ..^ 18) { say \u0026#34;$name有活力有时间，但是没钱\u0026#34; } when ($name, *, 18 ..^ 70) { say \u0026#34;$name有活力有钱，但是没时间\u0026#34; } when ($name, *, *) { say \u0026#34;$name有时间和金钱,但是没活力\u0026#34; } } 枚举 enum WeatherType \u0026lt;Cloudy Sunny Windy\u0026gt;; my $today = WeatherType::Cloudy; given $today { when WeatherType::Cloudy { say \u0026#34;多云\u0026#34; } when WeatherType::Sunny { say \u0026#34;晴天\u0026#34; } when WeatherType::Windy { say \u0026#34;有风\u0026#34; } } # 使用 if 语句 if $today ~~ WeatherType::Cloudy { say \u0026#34;多云\u0026#34; } 关联值 enum WeatherType ( Cloudy =\u0026gt; 100, Sunny =\u0026gt; 50, Windy =\u0026gt; 30 ); my $today = WeatherType::Windy; given $today { when WeatherType::Cloudy { say 20*Cloudy } when WeatherType::Sunny { say 10*Sunny } when WeatherType::Windy { say 12*Windy } } when 从句 my @numbers = 1..10; .say when $_ % 2 == 1 for @numbers; my @celebrities = \u0026#34;Michael Jackson\u0026#34;, \u0026#34;Taylor Swift\u0026#34;, \u0026#34;MichaelCaine\u0026#34;, \u0026#34;Adele Adkins\u0026#34;, \u0026#34;Michael Jordan\u0026#34;; .say when /^Michael/ for @celebrities; # 使用正则表达式 .say when $_.chars \u0026gt; 12 for @celebrities; # 调用方法 .say when /^Michael/ and $_.chars \u0026gt;12 for @celebrities; # 复合条件 ","permalink":"https://ohmyweekly.github.io/notes/2016-02-06-given-when-pattern-match-in-raku/","tags":["given","when"],"title":"Raku 中的 given/when 模式匹配"},{"categories":["rakulang"],"contents":"文件存在、文件的时间戳、文件的修改时间等等\n批量插入文本 my @filenames = dir \u0026#39;.\u0026#39;, test =\u0026gt; any(/\\.md$/, /\\.markdown/); for @filenames -\u0026gt; $filePath { my $path = $filePath.path(); $path ~~ s/.md//; $path ~~ s/.markdown//; my $date = DateTime.new(now); my $head = qq:heredoc \u0026#39;EOT\u0026#39;;title: $path.IO.basename()date: $datetags: Raku categories: Raku --- \u0026lt;blockquote class=\u0026#34;blockquote-center\u0026#34;\u0026gt;这城市有太多风景都在提醒那过去！\u0026lt;/blockquote\u0026gt; [TOC] EOT my @content = slurp $filePath; spurt($filePath.path, \u0026#34;$head\\n@content[]\u0026#34;); } 在当前目录中查找所有以 .md (.markdown)结尾的文件（即markdown文件）, 并在文件最前面插入一段文本， 形如：\ntitle: Raku date: 2015-08-20T23:19:13Z tags: Raku categories: Raku --- \u0026lt;blockquote class=\u0026#34;blockquote-center\u0026#34;\u0026gt;我站在天桥上念你, 有点狼狈\u0026lt;/blockquote\u0026gt; 类 IO::Path 提供了 basename, path, parts, 等方法供使用, 具体用法请看文档:\nrakudoc IO::Path 一些例子：\nsay IO::Path.new(\u0026#34;/etc/passwd\u0026#34;).basename; # passwd say IO::Path.new(\u0026#34;docs/README.pod\u0026#34;).extension; # pod say IO::Path.new(\u0026#34;/etc/passwd\u0026#34;).dirname; # /etc say IO::Path::Win32.new(\u0026#34;C:\\\\Windows\\\\registry.ini\u0026#34;).volume; # C: say IO::Path.new(\u0026#34;/etc/passwd\u0026#34;).parts.perl # (\u0026#34;dirname\u0026#34; =\u0026gt; \u0026#34;/etc\u0026#34;, \u0026#34;volume\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, \u0026#34;basename\u0026#34; =\u0026gt; \u0026#34;passwd\u0026#34;).hash 例子:\n# To iterate over the contents of the current directory: for dir() -\u0026gt; $file { say $file; } # As before, but include even \u0026#39;.\u0026#39; and \u0026#39;..\u0026#39; which are filtered out by # the default :test matcher: for dir(test =\u0026gt; *) -\u0026gt; $file { say $file; } # To get the names of all .jpg and .jpeg files in ~/Downloads: my @jpegs = \u0026#34;%*ENV\u0026lt;HOME\u0026gt;/Downloads\u0026#34;.IO.dir(test =\u0026gt; /:i\u0026#39;.\u0026#39;jpe?g $/)».Str; # An example program that lists all files and directories recursively: sub MAIN($dir = \u0026#39;.\u0026#39;) { my @todo = $dir.IO; while @todo { for @todo.pop.dir -\u0026gt; $path { say $path.Str; @todo.push: $path if $path.d; } } } 文件测试操作符 # If you have a string - a path to something in the filesystem: if \u0026#34;path/to/file\u0026#34;.IO ~~ :e { say \u0026#39;file exists\u0026#39;; } my $file = \u0026#34;path/to/file\u0026#34;; if $file.IO ~~ :e { say \u0026#39;file exists\u0026#39;; } # Instead of the colonpair syntax, you can use method calls too: if \u0026#39;path/to/file\u0026#39;.IO.e { say \u0026#39;file exists\u0026#39;; } # If you already have an IO object in $file, either by creating one yourself, or # by getting it from another subroutine, such as dir, you can write this: my $file = \u0026#34;path/to/file\u0026#34;.IO; if $file ~~ :e { say \u0026#39;file exists\u0026#39;; } 文件时间戳 say \u0026#34;path/to/file\u0026#34;.IO.modified; # e.g. Instant:1424089165 say DateTime.new(\u0026#34;path/to/file\u0026#34;.IO.modified); # e.g. 2015-02-16T12:18:50Z my $modification_instant = \u0026#34;path/to/file\u0026#34;.IO.modified; my $modification_time = DateTime.new($modification_instant); say $modification_time; # e.g. 2015-02-16T12:18:50Z say \u0026#34;path/to/file\u0026#34;.IO.accessed; # e.g. Instant:1424353577 say DateTime.new(\u0026#34;path/to/file\u0026#34;.IO.accessed); # e.g. 2015-02-19T13:45:42Z my $access_instant = \u0026#34;path/to/file\u0026#34;.IO.accessed; my $access_time = DateTime.new($access_instant); say $access_time; # e.g. 2015-02-19T13:45:42Z say \u0026#34;path/to/file\u0026#34;.IO.changed; # e.g. Instant:1424089165 say DateTime.new(\u0026#34;path/to/file\u0026#34;.IO.changed); # e.g. 2015-02-16T12:18:50Z my $change_instant = \u0026#34;path/to/file\u0026#34;.IO.changed; my $change_time = DateTime.new($chnge_instant); say $change_time; # e.g. 2015-02-16T12:18:50Z ","permalink":"https://ohmyweekly.github.io/notes/2015-10-11-io-in-raku/","tags":["io"],"title":"Raku 中的 IO 操作"},{"categories":["rakulang"],"contents":" Raku 中的 .polymod 方法 - 把数字分解成分母\n 命名 .polymod 方法接受几个除数并把它的调用者分解成一份一份的:\nmy $seconds = 1 * 60*60*24 # days + 3 * 60*60 # hours + 4 * 60 # minutes + 5; # seconds say $seconds.polymod: 60, 60; say $seconds.polymod: 60, 60, 24; # OUTPUT: # (5 4 27) # (5 4 3 1) 这种情况下我们作为参数传递的除数是和时间相关的: 60(每分钟有多少秒)， 60(每小时有多少分钟)，和24(每天有多少小时)。从最小的单位开始， 我们一直前进到最大的单位。\n输出和输入的除数是相匹配的 - 从最小的单位到最大的单位： 5 秒，4 分钟，3 小时和 1 天。\n手工制作 不使用 .polymod 而使用一个循环来展示怎么之前的计算:\nmy $seconds = 2 * 60*60*24 # days + 3 * 60*60 # hours + 4 * 60 # minutes + 5; # seconds my @pieces; for 60, 60, 24 -\u0026gt; $divisor { @pieces.push: $seconds mod $divisor; $seconds div= $divisor } @pieces.push: $seconds; say @pieces; # OUTPUT: # [5 4 3 2] 超越无限 当除数是以惰性列表的形式传递给 .polymod 方法时，它会一直运行直到余数为零并不会遍历整个列表:\nsay 120.polymod: 10¹, 10², 10³, 10⁴, 10⁵; say 120.polymod: lazy 10¹, 10², 10³, 10⁴, 10⁵; say 120.polymod: 10¹, 10², 10³ … ∞; # OUTPUT: # (0 12 0 0 0 0) # (0 12) # (0 12) 在第一个调用中， 我们让一系列数字按 10 的幂增长。该调用的输出包含了 4 个尾部的零，因为 .polymod 方法计算了每个除数。在第二个调用中，我们使用 lazy 关键字显式地创建了一个惰性列表， 而现在我们在返回的列表中只有 2 个条目。\n第一个除数(10)结果余数为 0，这是返回列表中的第一个条目，对于下一个除数，整除把我们的 120 变成了 12。12 除以 100 的余数为 12， 它是返回列表中的第二个条目。 现在， 12 整除 100 为 0， 它终止了 .polymod 的执行并给了我们两个 条目的结果。\n在最后一个调用中，我们使用了省略号，它是一个序列操作符，用来创建一系列按 10 的幂增长的数字，但是这一次序列是无限的。因为它是惰性的，结果再一次只有 2 个元素。\nZip It, Lock It, Put It In The Pocket 单独的数字很好但是对于它们所代表的单位不够具有描述性。我们来使用 Zip 元操作符:\nmy @units = \u0026lt;ng μg mg g kg\u0026gt;; my @pieces = 42_666_555_444_333.polymod: 10³ xx ∞; say @pieces Z~ @units; # OUTPUT: # (333ng 444μg 555mg 666g 42kg) 快速命名 对于被调用者和除数，你不仅仅限于使用 Ints，也可以使用其它类型的数字。\nsay ⅔.polymod: ⅓; say 5.Rat.polymod: .3, .2; say 3.Rat.polymod: ⅔, ⅓; # OUTPUT: # (0 2) # (0.2 0 80) # (0.333333 0 12) say 5.Num.polymod: .3, .2; say 3.Num.polymod: ⅔, ⅓; # OUTPUT: # (0.2 0.199999999999999 79) # (0.333333333333333 2.22044604925031e-16 12) 使用 Number::Denominate 模块 use Number::Denominate; my $seconds = 1 * 60*60*24 # days + 3 * 60*60 # hours + 4 * 60 # minutes + 5; # seconds say denominate $seconds; say denominate $seconds, :set\u0026lt;weight\u0026gt;; # OUTPUT: # 1 day, 3 hours, 4 minutes, and 5 seconds # 97 kilograms and 445 grams 你还可以定义自己的单位:\nsay denominate 449, :units( foo =\u0026gt; 3, \u0026lt;bar boors\u0026gt; =\u0026gt; 32, \u0026#39;ber\u0026#39; ); # OUTPUT: # 4 foos, 2 boors, and 1 ber http://raku.party/post/Raku-.polymod-break-up-a-number-into-denominations\n","permalink":"https://ohmyweekly.github.io/notes/2015-09-17-polymod-method-in-raku/","tags":["polymod"],"title":"Raku 中的 polymod 方法"},{"categories":["rakulang"],"contents":"proto proto 意思为原型。proto 从形式上声明了 multi 候选者之间的共性。 proto 充当作能检查但不会修改参数的包裹。看看这个基本的例子:\nproto congratulate(Str $reason, Str $name, |) {*} multi congratulate($reason, $name) { say \u0026#34;Hooray for your $reason, $name\u0026#34;; } multi congratulate($reason, $name, Int $rank) { say \u0026#34;Hooray for your $reason, $name-- you got rank $rank!\u0026#34;; } congratulate(\u0026#39;being a cool number\u0026#39;, \u0026#39;Fred\u0026#39;); # OK congratulate(\u0026#39;being a cool number\u0026#39;, \u0026#39;Fred\u0026#39;, 42); # OK congratulate(\u0026#39;being a cool number\u0026#39;, 42); # Proto match error 所有的 multi congratulate 会遵守基本的签名, 这个签名中有两个字符串参数, 后面跟着可选的更多的参数。 | 是一个未命名的 Capture 形参, 这允许 multi 接收额外的参数。第三个 congratulate 调用在编译时失败, 因为第一行的 proto 的签名变成了所有三个 multi congratulate 的共同签名, 而 42 不匹配 Str。\nsay \u0026amp;congratulate.signature #-\u0026gt; (Str $reason, Str $name, | is raw) 你可以给 proto 一个函数体, 并且在你想执行 dispatch 的地方放上一个 {*}。\n# attempts to notify someone -- returns False if unsuccessful proto notify(Str $user,Str $msg) { my \\hour = DateTime.now.hour; if hour \u0026gt; 8 or hour \u0026lt; 22 { return {*}; } else { # we can\u0026#39;t notify someone when they might be sleeping return False; } } {*} 总是分派给带有参数的候选者。默认参数和类型强制转换会起作用单不会传递。\nproto mistake-proto(Str() $str, Int $number = 42) {*} multi mistake-proto($str,$number) { say $str.WHAT } mistake-proto(7,42); #-\u0026gt; (Int) -- coercions not passed on mistake-proto(\u0026#39;test\u0026#39;); #!\u0026gt; fails -- defaults not passed on ","permalink":"https://ohmyweekly.github.io/notes/2015-10-06-proto-in-raku/","tags":["proto"],"title":"Raku 中的 proto"},{"categories":["rakulang"],"contents":" Composition and mix-ins Sigils Typed data structures Traits  所以到底什么是 role 呢？ role 是零个或多个方法和属性的集合。\nrole 不像 class，它不能被实例化（如果你尝试了，会生成一个 class）。Raku 中 Classes 是可变的，而 roles 是不可变的。\n申明 Roles 就像申明 Class 一样 使用关键字 role 来引入 role, 在 role 中声明属性和方法就像在 Raku 的类中声明属性和方法那样。\nrole DebugLog { has @.log_lines; has $.log_size is rw = 100; method log_message($message) { @!log_lines.shift if @!log_lines.elems \u0026gt;= $!log_size; @!log_lines.push($message); } } Role Composition  使用 does trait 将 role 组合到 Class 中：  class WebCrawler does DebugLog { ... }  这会把方法和属性添加到 class WebCrawler 里面去。 结果就像它们起初被写到 class 中一样。  Mix-ins  允许 role 的功能被添加到每个对象的根上 不影响其它的类实例 role 中的方法总是覆盖对象中已经存在的方法  Mix-ins Example  假设我们想跟踪某个对象发生了什么 Mix in the DebugLog role  $acount does DebugLog;  然后, 我们可以输出被登记的行  $account.log_lines».say;  现在我们只需给 log_message 方法添加调用 我们可以使用 .? 操作符, 这会调用某个方法, 如果方法存在的话  class Account { method change_password($new) { self.?log_message( \u0026#34;changing password to $new\u0026#34;; ) ... } } Sigil = 接口协定  在 Raku 中, 符号表明接口协定 这个接口协定由 role 定义 你可以只把东西放在带有符号的变量中, 如果该变量遵守(does)了要求的 role 的话 例外: 带有 $ 的变量可以存储任何东西(如果没有使用类型约束的话)  @ = Positional  @ 符号表明它是一个 Positional role 保证会有一个方法后环缀让你能调用 This is that gets called when you do an index positionally into something  say @fact[1]; say @fact.postcircumfix:\u0026lt;[ ]\u0026gt;(1);  注意: 优化器(如果有的话)可能发出更轻量级的东西  % = Associative  % 表明它是一个关联型(Associative)的 role 要有一个方法后环缀 postcircumfix:\u0026lt;{}\u0026gt; 让你调用 This is that gets called when you do an index associatively into something  say %price\u0026lt;Cheese\u0026gt;; say %price.postcircumfix:\u0026lt;{ }\u0026gt;(\u0026#39;Cheese\u0026#39;); \u0026amp; = Callable  \u0026amp; 表明它是一个 Callable 的 role 东西要能被调用 这个 role 被诸如 Block、Sub、Method之类的东西遵守 要求实现后环缀 postcircumfix:\u0026lt;()\u0026gt;  使用带有 block 的 class 关键字引入一个类：\nclass Puppy { ... } \u0001或使用\nclass Puppy; ... 1; 把类相关的东西单独写进一个文件\nRole 也可以被初始化 role BarChart { has Int @.bar-values; has $.b is rw; method plot { say @.bar-values; } } my $chart = BarChart.new(bar-values =\u0026gt; [1,2,3], b =\u0026gt; \u0026#34;Camelia\u0026#34;); say $chart.b; say $chart.bar-values; $chart.b = \u0026#34;Rakudo\u0026#34;; say $chart.b; say BarChart.^methods; 如果你初始化了 role, 那么它就变为类了。\n","permalink":"https://ohmyweekly.github.io/notes/2015-07-15-roles-in-raku/","tags":["role"],"title":"Raku 中的 Role"},{"categories":["rakulang"],"contents":"Raku: S/// 操作符 By Zoffix Znet\n来自 Perl 5 背景的我, 第一次使用 Raku 的非破坏性替换操作符 S/// 的经历就像下面这样:\n进展会更好的。我不但会改善错误信息, 而且会解释当前的所有事情。\n智能匹配 我有问题的原因是因为, 看到外形相似的操作符, 我就简单地把 Perl 5 中的绑定操作符(=~)转换为 Raku 中的智能匹配操作符(~~) 还期望它能正常工作。事实上我是异想天开。S/// 操作符没有文档, 并且结合令人困惑的(那个时候)警告信息, 这就是我痛苦的根源：\nmy $orig = \u0026#39;meowmix\u0026#39;; my $new = $orig ~~ S/me/c/; say $new; # OUTPUT warning: # Smartmatch with S/// can never succeed 这个丑陋的警告说这儿的 ~~ 操作符是个错误的选择并且确实如此。~~ 操作符不是 Perl 5 的 =~ 操作符的等价物。~~ 智能操作符把它左边的东西起了个叫做 $_ 的别名, 然后 ~~ 计算它右侧的东西, 然后在右侧这个东西身上调用 .ACCEPTS($_) 方法。这就是所有的魔法。\n所以上面的例子实际上发生了:\n 我们到达 S/// 的时候, $orig 被起了个叫做 $_ 的别名。 S/// 非破坏性地在 $_ 身上执行了替换并返回那个结果字符串。这是智能匹配将要操作的东西。 智能匹配, 按照 Str 与 Str 相匹配的规则, 会根据替换是否发生来给出 True 或 False（令人困惑的是, True 意味着没发生）  结果一路下来, 我们并没有得到我们想要的：替换过的字符串。\n使用 Given 既然我们知道了 S/// 总是作用在 $_ 上并且返回替换后的结果, 很容易就想到几种方法把 $_ 设置为我们原来的字符串并把 S/// 的返回值收集回来, 我们来看几个例子：\nmy $orig = \u0026#39;meowmix\u0026#39;; my $new = S/me/c/ given $orig; say $orig; say $new; my @orig = \u0026lt;meow cow sow vow\u0026gt;; my @new = do for @orig { S/\\w+\u0026lt;?before\u0026#39;ow\u0026#39;\u0026gt;/w/ }; say @orig; say @new; # OUTPUT: # meowmix # cowmix # [meow cow sow vow] # [wow wow wow wow] 第一个作用在单个值上。我们使用后置形式的 given 块儿, 这让我们避免了花括号（你可以使用 with 代替 given 得到同样的结果）。given $orig 会给 $orig 起个叫做 $_ 的别名。从输出来看, 原字符串没有被更改。\n第二个例子作用在数组中的一堆字符串身上并且我们使用 do 关键字来执行常规的 for 循环(那种情况下, 它把循环变量别名给 $_ 了)并把结果赋值给 @new 数组。再次, 输出显示原来的数组并没有发生改变。\n副词 S/// 操作符 - 就像 s/// 操作符和某些方法一样 - 允许你使用正则表达式副词：\ngiven \u0026#39;Lörem Ipsum Dolor Sit Amet\u0026#39; { say S:g /m/g/; # Löreg Ipsug Dolor Sit Aget say S:i /l/b/; # börem Ipsum Dolor Sit Amet say S:ii /l/b/; # Börem Ipsum Dolor Sit Amet say S:mm /o/u/; # Lürem Ipsum Dolor Sit Amet say S:nth(2) /m /g/; # Lörem Ipsug Dolor Sit Amet say S:x(2) /m /g/; # Löreg Ipsug Dolor Sit Amet say S:ss/Ipsum Dolor/Gipsum\\nColor/; # Lörem Gipsum Color Sit Amet say S:g:ii:nth(2) /m/g/; # Lörem Ipsug Dolor Sit Amet } 如你所见, 它们以 :foo 的形式添加在操作符 S 这个部件的后面。你可以大大方方地使用空白符号并且几个副词可以同时使用。下面是它们的意义：\n :g —(长形式：:global)全局匹配：替换掉所有的出现 :i —不区分大小写的匹配 :ii —(长形式： :samecase) 保留大小写：不管用作替换字母的大小写, 使用原来被替换的字母的大小写 :mm —(长形式：:samemark) 保留重音符号：在上面的例子中, 字母 o 上的分音符号被保留并被应用到替换字母 u 上 :nth(n) —只替换第 n 次出现的 :x(n) —至多替换 n 次（助记符: \u0026lsquo;x\u0026rsquo; 作为及时） :ss —(长形式：samespace)保留空白类型：空白字符的类型被保留, 而不管替换字符串中使用的是什么空白字符。在上面的例子中, 我们使用换行作为替换, 但是原来的空白被保留了。  方法形式 S/// 操作符很好, 但是有时候有点笨拙。不要害怕, Raku 提供了 .subst 方法能满足你所有的替换需求并且消除你对 .subst/.substr 的困惑。下面来看例子：\nsay \u0026#39;meowmix\u0026#39;.subst: \u0026#39;me\u0026#39;, \u0026#39;c\u0026#39;; say \u0026#39;meowmix\u0026#39;.subst: /m./, \u0026#39;c\u0026#39;; # OUTPUT: # cowmix # cowmix 这个方法要么接收一个正则表达式要么接收一个普通的字符串作为它的第一个位置参数, 它是要在调用者里面(\u0026ldquo;meowmix\u0026rdquo;)查找的东西。第二个参数是替换字符串。\n通过简单地把它们列为具名 Bool 参数, 你也可以使用副词。在 S/// 形式中, 副词 :ss 和 :ii 分别表明 :s(使空白有意义) 的出现和 :i(不区分大小写的匹配) 的出现。在方法形式中, 你必须把这些副词应用到正则表达式自身身上：\ngiven \u0026#39;Lorem Ipsum Dolor Sit Amet\u0026#39; { say .subst: /:il/, \u0026#39;b\u0026#39;, :ii; say .subst: /:sIpsum Dolor/, \u0026#34;Gipsum\\nColor\u0026#34;, :ss; } # OUTPUT: # Borem Ipsum Dolor Sit Amet # Lorem Gipsum Color Sit Amet 方法形式的捕获 捕获对于替换操作来说不陌生, 所以我们来尝试捕获下方法调用形式的替换：\nsay \u0026#39;meowmix\u0026#39;.subst: /me (.+)/, \u0026#34;c$0\u0026#34;; # OUTPUT: # Use of Nil in string context in block \u0026lt;unit\u0026gt; at test.p6 line 1 # c 不是我们要找的。我们的替换字符串构建在达到 .subst 方法之前, 并且里面的 $0 变量实际上指向任何这个方法调用之前的东西, 而不是 .subst 正则表达式中的捕获。所以我们怎么来修正它呢？\n.subst 方法的第二个参数也可以接受一个 Callable。在它里面, 你可以使用 $0, $1, ... $n 变量, 直到你想要的编号, 并从捕获中得到正确的值：\nsay \u0026#39;meowmix\u0026#39;.subst: /me (.+)/, -\u0026gt; { \u0026#34;c$0\u0026#34; }; # OUTPUT: # cowmix 这里, 我们为我们的 Callable 使用了尖号块儿, 但是 WhateverCode 和子例程也有效。每次替换都会调用这个 Callable, 并且把 Match 对象作为第一个位置参数传递给 Callable, 如果你需要访问它的话。\n结论 S/// 操作符在 Raku 中是 s/// 操作符的战友, 它不是修改原来的字符串, 而是拷贝原来的字符串, 修改, 然后返回修改过的版本。这个操作符的使用方式跟 Perl 5 中的非破坏性替换操作符的使用方式不同。作为备选, 方法版本的 .subst 也能使用。 方法形式和操作符形式的替换都能接收一组副词以修改它们的行为, 来满足你的需求。\n","permalink":"https://ohmyweekly.github.io/notes/2016-01-28-the-s-operator-in-raku/","tags":["rakulang"],"title":"Raku 中的 S/// 操作符"},{"categories":["rakulang"],"contents":"描述 有 7 块板子放在一个\u0026quot;袋子\u0026quot;中, 随机从袋子中移除一个板子展示到玩家面前直到袋子变空。当袋子变空时, 它会被重新装填, 如果需要额外的板子, 则重复前面那个过程。\n输出 使用随机 bag 系统随机输出 50 块板子。\n板子如下:\n O I S Z L J T  输出样本  LJOZISTTLOSZIJOSTJZILLTZISJOOJSIZLTZISOJTLIOJLTSZO OTJZSILILTZJOSOSIZTJLITZOJLSLZISTOJZTSIOJLZOSILJTS ITJLZOSILJZSOTTJLOSIZIOLTZSJOLSJZITOZTLJISTLSZOIJO  在 Raku 中我会这样写 (smls):\nsay (|\u0026lt;O I S Z L J T\u0026gt;.pick(*) xx *).[^50].join; 注意:\n | 操作符把每次迭代的项展开进外部的列表中, 以使你不必在结果上显式地调用 .flat 方法 使用 \u0026lt; \u0026gt; 字符串列表字面量看起来比在字符串字面量上使用 .comb 方法更合适 xx 操作符每次都会重新计算它左侧的表达式  ","permalink":"https://ohmyweekly.github.io/notes/2015-05-19-an-example-of-slip/","tags":["slip"],"title":"Raku 中的 Slip"},{"categories":["rakulang"],"contents":"一个裸的 :D、:U、:T 或 :_ 是限制默认类型为定义、未定义、类型对象或任何对象的类型约束。所以:\nclass Con { method man(:U: :D $x) } 其签名等价于 (Con:U: Any:D $x)。\nCon:U 是调用者, 在调用者后面加上一个冒号。要标记一个显式的调用者, 在它后面放上一个冒号就好了:\nmethod doit ($x: $a, $b, $c) { ... } Abstract vs Concrete types 　对于任何有名字的类型, 某些其它子集类型可以自动地通过在类型的名字后面追加一个合适的状语来派生出来：\nInt:_ 允许定义或未定的 Int 值 Int:D 只允许有定义的(强制的)Int 值 Int:U 只允许未定义值(抽象或失败)Int 值 Int:T 允许Int只作为类型对象 即, 它们的意思有点像:\nInt:D Int:_ where DEFINITE($_) Int:U Int:_ where not(DEFINITE($_)) Int:T Int:U where none(Failure) where DEFINITE 是一个布尔宏, 它说正处理的对象是否有一个合法的强制表示。(查看下面的自省) .\n在 Raku 中, Int 通常假定意为 Int:_, 除了调用者, 其中默认为 Int:D。（默认的 new 方法有一个原型, 它的调用者是 :T, 所以所有的 new 方法都默认允许类型对象。）\n这些默认可以通过各种编译指令在词法作用域中更改。\nuse parameters :D; 会让非调用者的参数默认为 :D。\n作为对比,\nuse variables :D; 会对用于变量声明中的类型做同样的事情。 在这样的词法作用域中, 你可以使用 :_ 形式回到标准的行为。特别地, 因为调用者默认为定义的:\nuse invocant :_; 会让调用者允许任何类型的有定义的和未定义的调用者。\n","permalink":"https://ohmyweekly.github.io/notes/2015-11-05-smile-type-in-raku/","tags":["smile"],"title":"Raku 中的 Smile 类型"},{"categories":["rakulang"],"contents":"Raku 中的 . 和 ! twigil 是什么？ . twigil 的意思是\u0026quot;这是类公用的\u0026quot;, ! twigil 的意思是“这是类私有的”, 只能用在类的内部。\nclass ScoreKeeper { has @.options; has %!player-points; } 普通的符号表明了词法作用域或包作用域, 但是 twigils 就像是 sigils 的兄弟, 表明了不同的作用域, 它影响了变量的作用域。. 和 ! 是二级 sigils。\n属性存在于每个类的实例中, 在类的内部, 可以直接使用 ! 访问到实例的属性:\nclass Point { has $.x; has $.y; method Str() { \u0026#34;($!x, $!y)\u0026#34; } } 那么 . twigil 和 ! twigil 之间有什么关系呢？下面看一个例子:\nclass Point { has $.x; has $.y; method Str() { # 注意这次我们使用 . 而非 ! \u0026#34;\\$.x 等价于 self.x()\u0026#34;.say if $.x == self.x(); \u0026#34;\\$.y 等价于 self.y()\u0026#34;.say if $.y == self.y(); } method print() { say self.x(); # 调用实例的名为 x 的方法 say self.y(); # 调用实例的名为 y 的方法 } } my $point = Point.new(x =\u0026gt; 10, y =\u0026gt; 20); $point.Str; $point.print; 会输出:\n$.x 等价于 self.x() $.y 等价于 self.y() 10 20 注意到, 属性被声明为 $.x 和 $.y, 但是在类的内部仍旧能通过 $!x 和 $!y 来访问属性。这是因为在 Raku 中所有的属性都是私有的并且在类中可以通过 $!attribute-name 直接访问这些属性。Raku 可以为你自动生成存取方法。\n公共属性拥有 . twigil, 私有属性拥有 ! twigil。\nhas $.x 就是私有属性 has $!x 加上一个getter方法, 即 method x() { ... }。\nhas $.x is rw 就是私有属性 has $!x 加上一个getter/setter方法。\n. twigil 就是调用了与属性同名的方法 class SaySomething { method a() { say \u0026#34;a\u0026#34;; } method b() { $.a; } method c() { self.a(); } # 这证明了 $.a 的 . twigil 做了一次隐式的实例方法调用。 } SaySomething.b; # 打印 \u0026#34;a\u0026#34; SaySomething.c; # 打印 \u0026#34;a\u0026#34; . twigil 中自动生成的方法可以被子类重写 class Point { has $.x; has $.y; } class Circle is Point { has $!radius; # 重写父类中的 x() 方法和 y() 方法 method x() { \u0026#34;I am x point in a Circle\u0026#34;; } method y() { \u0026#34;I am y point in a Circle\u0026#34;; } } my $circle = Circle.new(radius =\u0026gt; 10); $circle.x().say; # I am x point in a Circle $circle.y.say; # I am y point in a Circle 如果不想子类重写父类中的方法, 那么在父类中声明属性的时候, 使用 ! twigil 替代 . twigil。\n","permalink":"https://ohmyweekly.github.io/notes/2015-12-11-twigils-in-raku/","tags":["rakulang"],"title":"Raku 中的 twigil"},{"categories":["rakulang"],"contents":"空格最少化 在数组或散列的开括号(即下标左边的那个括号)之前不允许有空格, 参数列表的圆开括号前面也是不能有空格的。即:\n@deadbeef[$x] # okay %monsters{\u0026#39;cookie\u0026#39;} # okay saymewant(\u0026#39;cookie\u0026#39;) # okay @a [$b] # WRONG %people {\u0026#39;john\u0026#39;} # WRONG mewant (\u0026#39;cookie\u0026#39;) # WRONG 这种限制的的几个副作用之一就是条件控制结构的周围不再需要圆括号了:\nif $value eq $target { print \u0026#34;Bullseye!\u0026#34;; } while $i \u0026lt; 10 { $i++ } 然而, 显式的使用 unspace 语法仍然能够让你对齐下标和后缀操作符:\n%squirrels{\u0026#39;fluffy\u0026#39;} = Squirrel.new; %monsters.{\u0026#39;cookie\u0026#39;} = Monster.new; %beatles\\.{\u0026#39;ringo\u0026#39;} = Beatle.new; %people\\ .{\u0026#39;john\u0026#39;} = Person.new; ","permalink":"https://ohmyweekly.github.io/notes/2015-09-18-unspace-in-raku/","tags":["unspace"],"title":"Raku 中的 unspace"},{"categories":["rakulang"],"contents":"自省和 Raku 的对象系统 Raku 是构建在元对象层上面的。那意味着有些对象(元对象)控制着各种面向对象结构(例如类、roles、方法、属性、枚举…)怎样去表现。\n要感受类的元对象, 这儿有一个同样的例子出现2次: 一次一种 Raku中的普通声明, 一次通过元模型来表达:\nclass A { method x() { say 42 } } A.x(); # 42 对应于:\nconstant A := Metamodel::ClassHOW.new_type( name =\u0026gt; \u0026#39;A\u0026#39; ); # class A { A.^add_method(\u0026#39;x\u0026#39;, my method x(A:) { say 42 }); # method x() .. . A.^compose; # } A.x(); # 42 (除了声明形式的运行在编译时, 后面这种形式不是)\n对象后面的元对象能使用 $obj.HOW 获取, 这儿的 HOW 代表着 Higher Order Workings(或者 HOW the *%@$ does this work?)。\n这儿, 带有 .^ 的调用是元对象的调用, 所以 A.^compose 是 A.HOW.compose(A) 的简写。调用者也被传递到参数列表中, 以使它能够支持原型类型风格的类型系统, 那儿只有一个元对象。\n就像上面的例子展示的那样, 所有的面向对象特性对使用者都是可获得的, 而不仅仅是编译器。实际上编译器就是使用元对象的这样的调用的。\n元对象(MetaObjects) 这些是内省的宏, 类似于方法调用。\n元对象通常以 ALLCAPS(全大写)命名, 并且避免使用你自己的带有全大写名字的方法被认为是一个好的风格。这会避免和可能出现在未来版本中的任何元对象发生冲突。注意, 如果你必须使用带有全大写名字的方法的话, 把你的这个方法名字用引号引起来来间接安全地调用:\n#| THIS IS A CLASS FOR SHOUTING THINGS class MY-CLASSES-ARE-ALL-CAPS { method WHY { \u0026#34;I DON\u0026#39;T KNOW\u0026#34; } } my $c = MY-CLASSES-ARE-ALL-CAPS.new; say $c.WHY # \u0026#34;THIS IS A CLASS FOR SHOUTING THINGS\u0026#34;? 显示这？你在逗我! say $c.\u0026#34;WHY\u0026#34;() # \u0026#34;I DON\u0026#39;T KNOW\u0026#34; WHAT 类型的类型对象。例如 42.WHAT 返回 Int 类型对象。\nWHICH 对象的同一值。这能用于哈希和同一比较, 并且这是 === 中缀操作符的实现方式。\n\u0026gt; \u0026quot;a\u0026quot;.WHICH Str|a WHO 支持对象的包\n\u0026gt; \u0026quot;a\u0026quot;.WHO Str WHERE 对象的内存地址。注意这在移动的/紧凑的垃圾回收实现中是不稳定的。 在稳定的同一指示器中使用 WHERE。\nHOW 元类对象(the metaclass object)：“Higher Order Workings”。\nWHY 附加的 Pod 值。\nDEFINITE 对象有一个有效的强制表现。\n对于实例返回 True, 对于类型对象返回 False。\n\u0026gt; 42.DEFINITE True \u0026gt; Int.DEFINITE False VAR 返回底层的 Scalar 对象, 如果有的话。\n元对象系统的结构 对于每个类型声明符关键字, 例如 class、role 、enum、module、package、grammar 或 subset, 就有一个独立的元类在 Matamodel:: 命名空间中。(Rakudo 在 Raku::Metamodel:: 命名空间中实现了它们, 然后把 Raku::Metamodel 映射到 Metamodel)。\n这些元类(meta classes)中的很多都共享公共的功能。例如 roles、grammars 和 classes(类)都能包括方法和属性, 还能遵守 roles。这个共享的功能是在 roles 中实现的, 它被组合进合适的元类中。例如 role Metamodel::RoleContainer实现了类型能处理 roles 和 Metamodel::ClassHOW 的功能, 它是在 class 关键字后面的元类, 遵守了这个 role。\nBootstrapping concerns 你可能想知道为什么 Metamodel::ClassHOW 可以是一个类, 当按照 Metamodel::ClassHOW 作为一个类被定义时, 或者 roles 负责 role 处理的怎么能是 roles。答案是通过魔法。\n开玩笑啦。自举是特别实现的。Rakudo 使用语言的对象系统来实现自举, 它恰好(几乎)就是 Raku 的一个子集: NQP, Not Quite Perl。 NQP 有原始的, class-like 叫做 konwhow 的性质, 它用于自举它自己的类和 roles 实现。konwhow 建立在NQP 提供的虚拟机的原始基础上。\n因为元对象是根据低级(low-level)类型引导的, 自省有时能返回低级(low-level)类型而非你期望的那个类型, 例如返回一个 NQP-level 的子例程而非普通的 Routine 对象, 或返回一个引导的属性而非Attribute。\n组合和静态推理 在 Raku中, 类型是在解析时被构造的, 所以在开始, 它必须是可变的。然而, 如果所有类型一直是可变的, 那么关于类型的所有推断会在任何类型的修改时变得无效。例如父类的列表因此类型检测的结果能在那个时候改变。\n所以为了获得这两个世界中最好的东西, 当类型从可变转为不可变时是好时机。这就叫做组合, 并且对于从句法构成上声明的类型, 它发生在类型声明被完全解析时(所以总是在闭合花括号被解析时)。\n如果你通过元对象系统直接创建类型, 你必须要在它们身上调用 .^compose, 在它们变得完全起作用之前。\n很多元类也使用组合时来计算一些诸如方法解析顺序这样的属性, 发布一个方法缓存, 和其它清扫任务。在它们被组合之后干预类型有时是可能的, 但通常是造成灾难的因素。 不要那样做。\n能力和责任 元对象协议提供了很多常规 Raku 代码故意限制了的能力, 例如调用类中不信任你的私有方法, 窥探私有属性, 和其它通常不能完成的东西。\n常规的 Raku 代码有很多就地的安全检测; 元模型中不是这样，它靠近底层的虚拟机, 违反和虚拟机的约定可以导致所有奇怪的行为, 而在正常代码中, 显而易见的会是 bugs。\n所以, 在写元类型的时候要格外小心和思考。\n能力、便利和陷阱 元对象协议被设计的强大到实现 Raku 的对象系统。这种能力间或花费了便利的代价。\n例如, 当你写了 my $x = 42 并在 $x上调用方法时, 大部分方法会在整数 42 上起作用, 而不是在存储 42 的标量容器上。这是 Raku中设立的一块便利。元对象协议中的大部分不能提供自动忽略标量容器的便利性, 因为它们也用于实现那些标量容器。 所以, 如果你写了 my $t = MyType; ... $t.^compose, 那么你正组合那个 $ 变量表明的标量, 而不是 MyType。\n结果就是你需要很详尽的理解 Raku 的底层以避免陷阱, 当使用 MOP 时, 并且不能期望得到和普通 Raku 代码提供的 \u0026ldquo;do what I mean\u0026rdquo; 的便利。\n","permalink":"https://ohmyweekly.github.io/notes/2015-07-16-metaobject-in-raku/","tags":["meta object"],"title":"Raku 中的元对象"},{"categories":["rakulang"],"contents":"Raku 单行程序 这本书在进行之中。我希望你能觉得它有趣，甚至可能有用！如果你想贡献反馈的话，那么很欢迎提问题还有新的或有提升的正则表达式。\n作者 戴维法瑞尔 PerlTricks.com\n版本 版本 0.01\n许可 FreeBSD\n贡献者  Alexander Moquin Bruce Gray Carl Mäsak David H. Adler FROGGS Helmut Wollmersdorfer japhb Larry Wall Matt Oates Moritz Lenz Mouq Salve J Nilsen Sam S Skids timotimo  致谢 启发于 Peteris Krumins 的 Perl 5 example 文件。他逐字逐句地写了一本关于 Perl 5 单行 的书。\nirc上有很好地 folks。\n内容  介绍 教程 文件间距 行号 计算 创建字符串和创建数组 文本转换和替换 文本分析 选择性的行打印 使用管道转换数据(进行中) WWW(进行中) 转换到 Windows  介绍 把 Perl 和其它语言区别开的一件事情是在单行代码中写小程序的能力，即人们所熟知的\u0026quot;单行\u0026quot;。在终端里直接键入一个程序比写一个废弃的脚本往往更快。并且单行程序也很强大；它们是羽翼丰满的程序，能够加载外部库，但是也能集成到终端中。你可以在单行程序中输入或输出数据。\n像 Perl 5 一样， Raku支持单行程序。还有就像 Raku 到处清理着 Perl 5 的毒瘤一样，Raku 的单行语法也更好了。它拥有更少的特殊变量和选项因此更容易记忆。这本书提供了很多有用的 Raku 单行例子，从找出文件中得重复行到运行一个 web 服务器，它几乎能做所有事情。尽管 Raku 拥有更少的特殊变量，但是由于它高级的面向对象的语法，Raku 中的大部分单行程序比等价的 Perl 5 单行程序更短。\n这本书可以以多种方式阅读。如果你是单行程序的新手，从教程开始。它带领你掌握单行程序的核心概念；不要担心，一旦你理解了它会很容易。如果你精通 Perl，Bash，或 Sed/Awk，你可以立马开始工作。随意跳过和浏览你所感兴趣的东西。如果有些代码你不理解，那么在终端中试试！这个仓库中包含的无处不在的文件是 example.txt，它会在很多单行程序中用到。\n使用单行编程仅仅是 Raku 擅长的一个范例。这样的代码小而美，但是同时你正学习的是一种生产力技能，记住你正在学的是一种新的编程语言。检查 raku.org 网站获取官方文档。\n教程 要开始单行程序，所有你要掌握的是 -e 选项。这告诉 Perl 把它后面所跟的东西作为一个程序去执行。例如：\nraku -e \u0026#39;say \u0026#34;Hello, World!\u0026#34;\u0026#39; 我们来一步步剖析这段代码。 raku 引用了 Raku 程序， -e 告诉 Raku 去执行，而 'say \u0026quot;Hello, World!\u0026quot;'是要执行的程序。每个程序都必须被包围在单引号中（除了在 Windows 上，查看 转换到 Windows）。要运行单行程序，就把它键入到终端中好了：\n\u0026gt; raku -e 'say \u0026quot;Hello, World!\u0026quot;' Hello, World! 如果你想加载一个文件，就把文件路径添加到程序代码的后面：\nraku -e \u0026#39;for (lines) {say $_}\u0026#39; /path/to/file.txt 这个程序打印出了 path/to/file.txt 的每一行。你可能知道 $_ 是默认变量，它在这儿是指正被循环的当前行。lines 是一个列表，当你传递一个文件路径给单行程序的时候会自动为你创建这个列表。现在我们来重写那个单行程序，一步一步。它们都是等价的：\nraku -e \u0026#39;for (lines) { say $_ }\u0026#39; /path/to/file.txt raku -e \u0026#39;for (lines) { $_.say }\u0026#39; /path/to/file.txt raku -e \u0026#39;for (lines) { .say }\u0026#39; /path/to/file.txt raku -e \u0026#39;.say for (lines)\u0026#39; /path/to/file.txt raku -e \u0026#39;.say for lines\u0026#39; /path/to/file.txt 就像 $_ 是默认变量一样，在默认变量身上调用的方法可以省略掉变量引用。它们变成了默认方法。所以 $_.say 变成 .say。回报给写单行程序的人的东西是 - 更少的键入！\n-n 选项改变了程序的行为：它为文件中的每一行执行一次代码。所以，大写并打印 path/to/file.txt 的每一行你会键入：\nraku -ne \u0026#39;.uc.say\u0026#39; /path/to/file.txt -p 选项就像 -n, 除了它会自动打印 $_ 之外。所以大写文件中的所有行的另外一种方法是：\nraku -pe \u0026#39;$_ = .uc\u0026#39; /path/to/file.txt 或者两个做同样事情的更短的版本：\nraku -pe \u0026#39;.=uc\u0026#39; /path/to/file.txt raku -pe .=uc /path/to/file.txt 在第二个例子中，我们可以完全移除周围的单引号。这种场景很少遇到，但是如果你的单行程序中没有空格并且没有符号或引号，那么你通常可以移除外部的引号。\n-n 和 -p 选项真的很有用。本书中也有很多使用它们的单行例子。\n最后一件你要知道的事情是怎么加载模块。 -M 开关代表着加载模块：\nraku -M URI::Encode -e \u0026#39;say encode_uri(\u0026#34;example.com/10 ways to crush it with Raku\u0026#34;)\u0026#39; -M URI::Encode 加载了 URI::Encode 模块，它导入了 encode_uri 子例程。 你可以多次使用 -M 来加载多个模块：\nraku -M URI::Encode -M URI -e \u0026#39;\u0026lt;your code here\u0026gt;\u0026#39; 如果你有一个还没有安装的本地模块呢？ 简单， 仅仅传递一个 -I 开关来包含那个目录好了：\nraku -I lib -M URI::Encode -e \u0026#39;\u0026lt;your code here\u0026gt;\u0026#39; 现在 Raku 会在 lib 目录中搜索 URI::Encode 模块，和标准的安装位置一样。\n要查看 Raku 命令行开关有哪些， 使用 -h 选项查看帮助：\nraku -h 这打印可获得的不错的统计。\n文件间距 Double space a file\nraku -pe \u0026#39;$_ ~= \u0026#34;\\n\u0026#34;\u0026#39; example.txt N-space a file (例如. 4倍空白)\nraku -pe \u0026#39;$_ ~= \u0026#34;\\n\u0026#34; x 4\u0026#39; example.txt 在每一行前面添加一个空行：\nraku -pe \u0026#39;say \u0026#34;\u0026#34;\u0026#39; example.txt 移除所有空行：\nraku -ne \u0026#39;.say if /\\S/\u0026#39; example.txt raku -ne \u0026#39;.say if .chars\u0026#39; example.txt 移除所有的连续空白行，只保留一行：\nraku -e \u0026#39;$*ARGFILES.slurp.subst(/\\n+/, \u0026#34;\\n\\n\u0026#34;, :g).say\u0026#39; example.txt 行号 给文件中的所有行编号：\nraku -ne \u0026#39;say \u0026#34;{++$} $_\u0026#34;\u0026#39; example.txt raku -ne \u0026#39;say $*ARGFILES.ins ~ \u0026#34; $_ \u0026#34;\u0026#39; example.txt 只给文件中得非空行编号：\nraku -pe \u0026#39;$_ = \u0026#34;{++$} $_\u0026#34; if /\\S/\u0026#39; example.txt 给所有行编号但是只打印非空行：\nraku -pe \u0026#39;$_ = $*ARGFILES.ins ~ \u0026#34; $_ \u0026#34; if /\\S/\u0026#39; example.txt 打印文件中行数的总数：\nraku -e \u0026#39;say lines.elems\u0026#39; example.txt raku -e \u0026#39;say lines.Int\u0026#39; example.txt raku -e \u0026#39;lines.Int.say\u0026#39; example.txt 打印出文件中非空行的总数：\nraku -e \u0026#39;lines.grep(/\\S/.elems.say)\u0026#39; example.txt 打印文件中空行的数量：\nraku -e \u0026#39;lines.grep(/^\\s*$/).elems.say\u0026#39; example.txt 计算 检查一个数是否是质数：\nraku -e \u0026#39;say \u0026#34;7 is prime\u0026#34; if 7.is-prime\u0026#39; 打印一行中所有字段的和：\nraku -ne \u0026#39;say [+] .split(\u0026#34;\\t\u0026#34;)\u0026#39; 打印所有行的所有字段的和：\nraku -e \u0026#39;say [+] lines.split(\u0026#34;\\t\u0026#34;)\u0026#39; 打乱行中的所有字段：\nraku -ne \u0026#39;.split(\u0026#34;\\t\u0026#34;).pick(*).join(\u0026#34;\\t\u0026#34;).say\u0026#39; 找出一行中最小的元素：\nraku -ne \u0026#39;.split(\u0026#34;\\t\u0026#34;).min.say\u0026#39; 找出所有行的最小的元素：\nraku -e \u0026#39;lines.split(\u0026#34;\\t\u0026#34;).min.say\u0026#39; 找出一行中最大的元素：\nraku -ne \u0026#39;.split(\u0026#34;\\t\u0026#34;).max.say\u0026#39; 找出所有行的最大的元素：\nraku -e \u0026#39;lines.split(\u0026#34;\\t\u0026#34;).max.say\u0026#39; 找出一行中得数值化最小元素：\nraku -ne \u0026#39;.split(\u0026#34;\\t\u0026#34;)».Numeric.min.say\u0026#39; 找出一行中得数值化最大元素：\nraku -ne \u0026#39;.split(\u0026#34;\\t\u0026#34;)».Numeric.max.say\u0026#39; 使用字段的绝对值替换每个字段：\nraku -ne \u0026#39;.split(\u0026#34;\\t\u0026#34;).map(*.abs).join(\u0026#34;\\t\u0026#34;)\u0026#39; 找出每行中字符的总数：\nraku -ne \u0026#39;.chars.say\u0026#39; example.txt 找出每行中单词的总数：\nraku -ne \u0026#39;.words.elems.say\u0026#39; example.txt 找出每行中由逗号分隔的元素的总数：\nraku -ne \u0026#39;.split(\u0026#34;,\u0026#34;).elems.say\u0026#39; example.txt 找出所有行的字段（单词）的总数：\nraku -e \u0026#39;say lines.split(\u0026#34;\\t\u0026#34;).elems\u0026#39; example.txt # fields raku -e \u0026#39;say lines.words.elems\u0026#39; example.txt # words 打印匹配某个模式的字段的总数：\nraku -e \u0026#39;say lines.split(\u0026#34;\\t\u0026#34;).comb(/pattern/).elems\u0026#39; example.txt # fields raku -e \u0026#39;say lines.words.comb(/pattern/).elems\u0026#39; example.txt # words 打印匹配某个模式的行的总数：\nraku -e \u0026#39;say lines.grep(/in/.elems)\u0026#39; example.txt 打印数字 PI 到 n 位小数点(例如. 10位)：\nraku -e \u0026#39;say pi.fmt(\u0026#34;%.10f\u0026#34;);\u0026#39; 打印数字 PI 到 15 位小数点：\nraku -e \u0026#39;say π\u0026#39; 打印数字 E 到 n 位小数点(例如. 10位)：\nraku -e \u0026#39;say e.fmt(\u0026#34;%.10f\u0026#34;);\u0026#39; 打印数字 E 到 15 位小数点：\nraku -e \u0026#39;say e\u0026#39; 打印 UNIX 时间 (seconds since Jan 1, 1970, 00:00:00 UTC)\nraku -e \u0026#39;say time\u0026#39; 打印 GMT (格林威治标准时间)和地方计算机时间：\nraku -MDateTime::TimeZone -e \u0026#39;say to-timezone(\u0026#34;GMT\u0026#34;,DateTime.now)\u0026#39; raku -e \u0026#39;say DateTime.now\u0026#39; 以 H:M:S 格式打印当地计算机时间：\nraku -e \u0026#39;say DateTime.now.map({$_.hour, $_.minute, $_.second.round}).join(\u0026#34;:\u0026#34;)\u0026#39; 打印昨天的日期：\nraku -e \u0026#39;say DateTime.now.earlier(:1day)\u0026#39; 打印日期： 14 个月, 9 天，和 7 秒之前\nraku -e \u0026#39;say DateTime.now.earlier(:14months).earlier(:9days).earlier(:7seconds)\u0026#39; 在标准输出前加上时间戳（GMT，地方时间）：\ntail -f logfile | raku -MDateTime::TimeZone -ne \u0026#39;say to-timezone(\u0026#34;GMT\u0026#34;,DateTime.now) ~ \u0026#34;\\t$_\u0026#34;\u0026#39; tail -f logfile | raku -ne \u0026#39;say DateTime.now ~ \u0026#34;\\t$_\u0026#34;\u0026#39; 计算 5 的阶乘：\nraku -e \u0026#39;say [*] 1..5\u0026#39; 计算最大公约数：\nraku -e \u0026#39;say [gcd] @list_of_numbers\u0026#39; 使用欧几里得算法计算数字 20 和 35 的最大公约数：\nraku -e \u0026#39;say (20, 35, *%* ... 0)[*-2]\u0026#39; 计算 20 和 35 的最小公倍数：\nraku -e \u0026#39;say 20 lcm 35\u0026#39; 使用欧几里得算法: n*m/gcd(n,m) 计算数字 20 和 35 的最小公倍数：\nraku -e \u0026#39;say 20 * 35 / (20 gcd 35)\u0026#39; 生成 10 个 5 到 15（不包括 15）之间的随机数：\nraku -e \u0026#39;.say for (5..^15).roll(10)\u0026#39; 找出并打印列表的全排列：\nraku -e \u0026#39;say .join for [1..5].permutations\u0026#39; 生成幂集\nraku -e \u0026#39;.say for \u0026lt;1 2 3\u0026gt;.combinations\u0026#39; 把 IP 地址转换为无符号整数：\nraku -e \u0026#39;say :256[\u0026#34;127.0.0.1\u0026#34;.comb(/\\d+/)]\u0026#39; raku -e \u0026#39;say +\u0026#34;:256[{q/127.0.0.1/.subst(:g,/\\./,q/,/)}]\u0026#34;\u0026#39; raku -e \u0026#39;say Buf.new(+«\u0026#34;127.0.0.1\u0026#34;.split(\u0026#34;.\u0026#34;)).unpack(\u0026#34;N\u0026#34;)\u0026#39; 把无符号整数转换为 IP 地址：\nraku -e \u0026#39;say join \u0026#34;.\u0026#34;, @(pack \u0026#34;N\u0026#34;, 2130706433)\u0026#39; raku -e \u0026#39;say join \u0026#34;.\u0026#34;, map { ((2130706433+\u0026gt;(8*$_))+\u0026amp;0xFF) }, (3...0)\u0026#39; 创建字符串和创建数组 生成并打印字母表：\nraku -e \u0026#39;.say for \u0026#34;a\u0026#34;..\u0026#34;z\u0026#34;\u0026#39; 生成并打印所有从 \u0026ldquo;a\u0026rdquo; 到 \u0026ldquo;zz\u0026rdquo; 的字符串：\nraku -e \u0026#39;.say for \u0026#34;a\u0026#34;..\u0026#34;zz\u0026#34;\u0026#39; 把整数转换为十六进制：\nraku -e \u0026#39;say 255.base(16)\u0026#39; raku -e \u0026#39;say sprintf(\u0026#34;%x\u0026#34;, 255)\u0026#39; 把整数打印为十六进制转换表：\nraku -e \u0026#39;say sprintf(\u0026#34;%3i =\u0026gt; %2x\u0026#34;, $_, $_) for 0..255\u0026#39; 把整数编码为百分数：\nraku -e \u0026#39;say sprintf(\u0026#34;%%%x\u0026#34;, 255)\u0026#39; 生成一个随机的 10 个 a-z 字符长度的字符串：\nraku -e \u0026#39;print roll 10, \u0026#34;a\u0026#34;..\u0026#34;z\u0026#34;\u0026#39; raku -e \u0026#39;print roll \u0026#34;a\u0026#34;..\u0026#34;z\u0026#34;: 10\u0026#39; 生成一个随机的 15 个 ASCII 字符长度的密码：\nraku -e \u0026#39;print roll 15, \u0026#34;0\u0026#34;..\u0026#34;z\u0026#34;\u0026#39; raku -e \u0026#39;print roll \u0026#34;0\u0026#34;..\u0026#34;z\u0026#34;: 15\u0026#39; 创建一个指定长度的字符串：\nraku -e \u0026#39;print \u0026#34;a\u0026#34; x 50\u0026#39; 生成并打印从 1 到 100 数字为偶数的数组：\nraku -e \u0026#39;(1..100).grep(* %% 2).say\u0026#39; 找出字符串的长度：\nraku -e \u0026#39;\u0026#34;storm in a teacup\u0026#34;.chars.say\u0026#39; 找出数组的元素个数：\nraku -e \u0026#39;my @letters = \u0026#34;a\u0026#34;..\u0026#34;z\u0026#34;; @letters.Int.say\u0026#39; 文本转换和替换 对文件进行 ROT 13 加密：\nraku -pe \u0026#39;tr/A..Za..z/N..ZA..Mn..za..m/\u0026#39; example.txt 对字符串进行 Base64 编码：\nraku -MMIME::Base64 -ne \u0026#39;print MIME::Base64.encode-str($_)\u0026#39; example.txt 对字符串进行 Base64 解码：\nraku -MMIME::Base64 -ne \u0026#39;print MIME::Base64.decode-str($_)\u0026#39; base64.txt 对字符串进行 URL 转义：\nraku -MURI::Encode -le \u0026#39;say uri_encode($string)\u0026#39; URL-unescape a string\nraku -MURI::Encode -le \u0026#39;say uri_decode($string)\u0026#39; HTML-encode a string\nraku -MHTML::Entity -e \u0026#39;print encode-entities($string)\u0026#39; HTML-decode a string\nraku -MHTML::Entity -e \u0026#39;print decode-entities($string)\u0026#39; 把所有文本转换为大写：\nraku -pe \u0026#39;.=uc\u0026#39; example.txt raku -ne \u0026#39;say .uc\u0026#39; example.txt 把所有文本转换为小写：\nraku -pe \u0026#39;.=lc\u0026#39; example.txt raku -ne \u0026#39;say .lc\u0026#39; example.txt 只把每行的第一个单词转换为大写：\nraku -ne \u0026#39;say s/(\\w+){}/{$0.uc}/\u0026#39; example.txt 颠倒字母的大小写：\nraku -pe \u0026#39;tr/a..zA..Z/A..Za..z/\u0026#39; example.txt raku -ne \u0026#39;say tr/a..zA..Z/A..Za..z/.after\u0026#39; example.txt 对每行进行驼峰式大小写：\nraku -ne \u0026#39;say .wordcase\u0026#39; example.txt 在每行的开头去掉前置空白（空格、tabs）：\nraku -ne \u0026#39;say .trim-leading\u0026#39; example.txt 从每行的末尾去掉结尾的空白（空格、tabs）：\nraku -ne \u0026#39;say .trim-trailing\u0026#39; example.txt 从每行中去除行首和行尾的空白：\nraku -ne \u0026#39;say .trim\u0026#39; example.txt 把 UNIX 换行符转换为 DOS/Windows 换行符：\nraku -ne \u0026#39;print .subst(/\\n/, \u0026#34;\\r\\n\u0026#34;)\u0026#39; example.txt 把 DOS/Windows 换行符转换为 UNIX 换行符：\nraku -ne \u0026#39;print .subst(/\\r\\n/, \u0026#34;\\n\u0026#34;)\u0026#39; example.txt 把每行中所有的 \u0026ldquo;ut\u0026rdquo; 实体用 \u0026ldquo;foo\u0026rdquo; 替换掉：\nraku -pe \u0026#39;s:g/ut/foo/\u0026#39; example.txt 把包含 \u0026ldquo;lorem\u0026rdquo; 的每行中所有的 \u0026ldquo;ut\u0026rdquo; 实体用 \u0026ldquo;foo\u0026rdquo; 替换掉：\nraku -pe \u0026#39;s:g/ut/foo/ if /Lorem/\u0026#39; example.txt 把文件转换为 JSON 格式：\nraku -M JSON::Tiny -e \u0026#39;say to-json(lines)\u0026#39; example.txt 从文件的每一行中随机挑选 5 个单词：\nraku -ne \u0026#39;say .words.pick(5)\u0026#39; example.txt 文本分析 Print n-grams of a string\nraku -e \u0026#39;my $n=2; say \u0026#34;banana\u0026#34;.comb.rotor($n,$n-1).map({[~] @$_})\u0026#39; 打印唯一的 n-grams ```bash raku -e \u0026#39;my $n=2; say \u0026#34;banana\u0026#34;.comb.rotor($n,$n-1).map({[~] @$_}).Set.sort\u0026#39; 打印 n-grams 的出现次数：\nraku -e \u0026#39;my $n=2; say \u0026#34;banana\u0026#34;.comb.rotor($n,$n-1).map({[~] @$_}).Bag.sort.join(\u0026#34;\\n\u0026#34;)\u0026#39; 打印单词的出现次数(1-grams)：\nraku -e \u0026#39;say lines[0].words.map({[~] @$_}).Bag.sort.join(\u0026#34;\\n\u0026#34;)\u0026#39; example.txt 基于一组 1-grams 打印 Dice 相似系数：\nraku -e \u0026#39;my $a=\u0026#34;banana\u0026#34;.comb;my $b=\u0026#34;anna\u0026#34;.comb;say ($a (\u0026amp;) $b)/($a.Set + $b.Set)\u0026#39; 基于 1-grams 打印卡得杰相似系数：\nraku -e \u0026#39;my $a=\u0026#34;banana\u0026#34;.comb;my $b=\u0026#34;anna\u0026#34;.comb;say ($a (\u0026amp;) $b) / ($a (|) $b)\u0026#39; 基于 1-grams 打印重叠系数：\nraku -e \u0026#39;my $a=\u0026#34;banana\u0026#34;.comb;my $b=\u0026#34;anna\u0026#34;.comb;say ($a (\u0026amp;) $b)/($a.Set.elems,$b.Set.elems).min\u0026#39; 基于 1-grams 打印类似的余弦：\nraku -e \u0026#39;my $a=\u0026#34;banana\u0026#34;.comb;my $b=\u0026#34;anna\u0026#34;.comb;say ($a (\u0026amp;) $b)/($a.Set.elems.sqrt*$b.Set.elems.sqrt)\u0026#39; # 上面的命令提示 Seq 已经被消费 raku -e \u0026#39;my $a=\u0026#34;banana\u0026#34;.comb;my $b=\u0026#34;anna\u0026#34;.comb;say ($a.cache (\u0026amp;) $b.cache)/($a.cache.Set.elems.sqrt*$b.cache.Set.elems.sqrt)\u0026#39; 创建字符串中字符的索引并打印出来：\nraku -e \u0026#39;say {}.push: %(\u0026#34;banana\u0026#34;.comb.pairs).invert\u0026#39; 创建一行中单词的所以并打印出来：\nraku -e \u0026#39;({}.push: %(lines[0].words.pairs).invert).sort.join(\u0026#34;\\n\u0026#34;).say\u0026#39; example.txt 选择性的行打印 打印文件的第一行（模仿 head -1）：\nraku -ne \u0026#39;.say;exit\u0026#39; example.txt raku -e \u0026#39;lines[0].say\u0026#39; example.txt raku -e \u0026#39;lines.shift.say\u0026#39; example.txt 打印文件的前 10 行（模仿 head -10）\nraku -pe \u0026#39;exit if ++$ \u0026gt; 10\u0026#39; example.txt raku -ne \u0026#39;.say if ++$ \u0026lt; 11\u0026#39; example.txt 打印文件的最后一行（模仿 tail -1）：\nraku -e \u0026#39;lines.pop.say\u0026#39; example.txt 打印文件的最后 5 行（模仿 tail -5）：\nraku -e \u0026#39;.say for lines[*-5..*]\u0026#39; example.txt 只打印包含元音的行：\nraku -ne \u0026#39;/\u0026lt;[aeiou]\u0026gt;/ \u0026amp;\u0026amp; .print\u0026#39; example.txt 打印包含所有元音的行：\nraku -ne \u0026#39;.say if .comb (\u0026gt;=) \u0026lt;a e i o u\u0026gt;\u0026#39; example.txt raku -ne \u0026#39;.say if .comb ⊇ \u0026lt;a e i o u\u0026gt;\u0026#39; example.txt 打印字符数大于或等于 80 的行：\nraku -ne \u0026#39;.print if .chars \u0026gt;= 80\u0026#39; example.txt raku -ne \u0026#39;.chars \u0026gt;= 80 \u0026amp;\u0026amp; .print\u0026#39; example.txt 只打印第二行：\nraku -ne \u0026#39;.print if ++$ == 2\u0026#39; example.txt 打印除了第二行的所有行：\nraku -pe \u0026#39;next if ++$ == 2\u0026#39; example.txt 打印第一行到第三行之间的所有行：\nraku -ne \u0026#39;.print if (1..3).any == ++$\u0026#39; example.txt 打印两个正则表达式之间（包含匹配那个正则表达式的行）的所有行：\nraku -ne \u0026#39;.print if /^Lorem/../laborum\\.$/\u0026#39; example.txt 打印最长的行的长度：\nraku -e \u0026#39;say lines.max.chars\u0026#39; example.txt raku -ne \u0026#39;state $l=0; $l = .chars if .chars \u0026gt; $l;END { $l.say }\u0026#39; example.txt 打印长度最长的行：\nraku -e \u0026#39;say lines.max\u0026#39; example.txt raku -e \u0026#39;my $l=\u0026#34;\u0026#34;; for (lines) {$l = $_ if .chars \u0026gt; $l.chars};END { $l.say }\u0026#39; example.txt 打印包含数字的所有行：\nraku -ne \u0026#39;.say if /\\d/\u0026#39; example.txt raku -e \u0026#39;.say for lines.grep(/\\d/)\u0026#39; example.txt raku -ne \u0026#39;/\\d/ \u0026amp;\u0026amp; .say\u0026#39; example.txt raku -pe \u0026#39;next if ! $_.match(/\\d/)\u0026#39; example.txt 打印只包含数字的所有行：\nraku -ne \u0026#39;.say if /^\\d+$/\u0026#39; example.txt raku -e \u0026#39;.say for lines.grep(/^\\d+$/)\u0026#39; example.txt raku -ne \u0026#39;/^\\d+$/ \u0026amp;\u0026amp; .say\u0026#39; example.txt raku -pe \u0026#39;next if ! $_.match(/^\\d+$/)\u0026#39; example.txt 打印每个奇数行：\nraku -ne \u0026#39;.say if ++$ % 2\u0026#39; example.txt 打印每个偶数行：\nraku -ne \u0026#39;.say if ! (++$ % 2)\u0026#39; example.txt 打印所有重复的行：\nraku -ne \u0026#39;state %l;.say if ++%l{$_}==2\u0026#39; example.txt 打印唯一的行：\nraku -ne \u0026#39;state %l;.say if ++%l{$_}==1\u0026#39; example.txt 打印每一行中的第一个字段（单词）（模仿 cut -f 1 -d ' \u0026lsquo;）\nraku -ne \u0026#39;.words[0].say\u0026#39; example.txt 使用管道转换数据 Raku程序直接集成到了命令行中。你可以使用 | 管道符号从单行程序中输出数据和输入数据到单行程序中。为了 从管道中输入数据， Raku 自动地把 STDIN 设置为 $*IN。就像对文件那样，从管道输入的数据在单行中也能使用 -n 来进行循环迭代。从单行程序中输出数据就使用 print 或 say 好了。\n在当前目录中对所有文件进行 JSON 编码：\nls | raku -M JSON::Tiny -e \u0026#39;say to-json(lines)\u0026#39; 打印文件中的大约 5% 的随机样本行：\nraku -ne \u0026#39;.say if 1.rand \u0026lt;= 0.05\u0026#39; /usr/share/dict/words 颜色转换， 从 HTML 到 RGB\necho \u0026#34;#ffff00\u0026#34; | raku -ne \u0026#39;.comb(/\\w\\w/).map({:16($_)}).say\u0026#39; 颜色转换， 从 RGB 到 HTML\necho \u0026#34;#ffff00\u0026#34; | raku -ne \u0026#39;.comb(/\\w\\w/).map({:16($_)}).say\u0026#39; WWW 下载一个页面：\nraku -M HTTP::UserAgent -e \u0026#39;say HTTP::UserAgent.new.get(\u0026#34;google.com\u0026#34;).content\u0026#39; 下载一个页面并剥离 HTML：\nwget -O - \u0026#34;http://raku.org\u0026#34; | raku -ne \u0026#39;s:g/\\\u0026lt;.+?\\\u0026gt;//.say\u0026#39; 下载一个页面并剥离并解码 HTML：\nwget -O - \u0026#34;http://raku.org\u0026#34; | raku -MHTML::Strip -ne \u0026#39;strip_html($_).say\u0026#39; 开启一个简单地 web 服务器：\nraku -M HTTP::Server::Simple -e \u0026#39;HTTP::Server::Simple.new.run\u0026#39; 转换到 Windows 一旦你知道了里面的门道之后那么在 Windows 上运行单行程序就是小草一碟。单行程序既可以在 cmd.exe 中运行，又可以在 Powershell 中运行。主要的规则是：用双引号替换掉外部的单引号，在单行程序的内部使用插值引用操作符 qq// 来把字符串括起来。对于非插值的引起，你可以使用单引号。我们来看几个例子。\n这儿有一个打印时间的单行程序：\nraku -e \u0026#39;say DateTime.now\u0026#39; 要在 Windows 上运行，我们仅仅用双引号替换掉单引号好了：\nraku -e \u0026#34;say DateTime.now\u0026#34; 这个单行程序给文件中每一行添加了一个换行符，使用了插值字符串：\nraku -pe \u0026#39;$_ ~= \u0026#34;\\n\u0026#34;\u0026#39; example.txt 在 Windows 上这应该写为：\nraku -pe \u0026#34;$_~= qq/\\n/\u0026#34; example.txt 这种情况下，我们想对换行符进行插值，并且不为该行字面地添加反斜线和字符\u0026quot;n\u0026quot;，所以我们必须使用 qq。但是你通常也可以像这样在单行程序中使用单引号：\nraku -e \u0026#39;say \u0026#34;Hello, World!\u0026#34;\u0026#39; 在 Windows 上这应该写为：\nraku -e \u0026#34;say \u0026#39;hello, World!\u0026#39;\u0026#34; 简单地输出重定向工作起来像基于 Unix 系统那样。 这个单行程序使用 \u0026gt; 把 ASCII 字符索引表打印到一个文件中：\nraku -e \u0026#34;say .chr ~ \u0026#39; \u0026#39; ~ $_for 0..255\u0026#34; \u0026gt; ascii_codes.txt 在使用 \u0026gt; 的时候，如果文件不存在就会创建一个。如果文件确实存在，它会被重写。你可能更想追加到文件，使用 \u0026gt;\u0026gt; 代替。\n","permalink":"https://ohmyweekly.github.io/notes/2015-08-25-one-liners-in-raku/","tags":["oneliner"],"title":"Raku 中的单行程序"},{"categories":["rakulang"],"contents":"变量名以一个叫做魔符 sigil 的特殊字符开头, 后面跟着一个可选的第二个叫做 twigil 的特殊字符, 然后是一个标识符.\nSigils    符号 类型约束 默认类型 Flattens Assignment     $ Mu (no type constraint) Any No item   \u0026amp; Callable Callable No item   @ Positional Array Yes list   % Associative Hash Yes list    例子:\nmy $square = 9 ** 2; my @array = 1, 2, 3; # Array variable with three elements my %hash = London =\u0026gt; \u0026#39;UK\u0026#39;, Berlin =\u0026gt; \u0026#39;Germany\u0026#39;; 默认类型可以使用 is 关键字设置。\nclass FailHash is Hash { has Bool $!final = False; multi method AT-KEY ( ::?CLASS:D: Str:D \\key ){ fail X::OutOfRange.new(:what(\u0026#34;Hash key\u0026#34;), :got(key), :range(self.keys)) if $!final \u0026amp;\u0026amp; !self.EXISTS-KEY(key); callsame } method finalize() { $!final = True } } my %h is FailHash = oranges =\u0026gt; \u0026#34;round\u0026#34;, bananas =\u0026gt; \u0026#34;bendy\u0026#34;; say %h\u0026lt;oranges\u0026gt;; # OUTPUT «round␤» %h.finalize; say %h\u0026lt;cherry\u0026gt;; CATCH { default { put .^name, \u0026#39;: \u0026#39;, .Str } } # OUTPUT «X::OutOfRange: Hash key out of range. Is: cherry, should be in (oranges bananas)» 不带符号的变量也是可行的, 查看 无符号变量.\n项和列表赋值 有两种类型的赋值, item 赋值和 list 赋值。两者都使用 = 号操作符。根据 = 号左边的语法来区别 = 是 item 赋值还是 list 赋值。\nItem 赋值把等号右侧的值放到左侧的变量(容器)中。\n例如, 数组变量(@符号)在列表赋值时清空数组自身, 然后把等号右侧的值都放进数组自身中. 跟 Item 赋值相比, 这意味着等号左侧的变量类型始终是 Array, 不管右侧是什么类型。\n赋值类型(item 或 list)取决于当前表达式或声明符看到的第一个上下文:\nmy $foo = 5; # item assignment say $foo.perl; # 5 my @bar = 7, 9; # list assignment say @bar.WHAT; # Array say @bar.perl; # [7, 9] (my $baz) = 11, 13; # list assignment say $baz.WHAT; # Parcel say $baz.perl; # (11, 13) 因此, 包含在列表赋值中的赋值行为依赖于表达式或包含表达式的声明符。 例如, 如果内部赋值是一个声明符(例如 my), 就使用 item 赋值, 它比逗号和列表赋值的优先级更高:\nmy @array; @array = my $num = 42, \u0026#34;str\u0026#34;; # item assignment: uses declarator say @array.perl; # [42, \u0026#34;str\u0026#34;] (an Array) say $num.perl; # 42 (a Num) 类似地, 如果内部赋值是一个用于声明符初始化的表达式, 则内部表达式的上下文决定赋值的类型:\nmy $num; my @array = $num = 42, \u0026#34;str\u0026#34;; # item assignment: uses expression say @array.perl; # [42, \u0026#34;str\u0026#34;] (an Array) say $num.perl; # 42 (a Num) my ( @foo, $bar ); @foo = ($bar) = 42, \u0026#34;str\u0026#34;; # list assignment: uses parens say @foo.perl; # [42, \u0026#34;str\u0026#34;] (an Array) say $bar.perl; # $(42, \u0026#34;str\u0026#34;) (a Parcel) 然而, 如果内部赋值既不是声明符又不是表达式, 而是更大的表达式的一部分, 更大的表达式的上下文决定赋值的类型:\nmy ( @array, $num ); @array = $num = 42, \u0026#34;str\u0026#34;; # list assignment say @array.perl; # [42, \u0026#34;str\u0026#34;] (an Array) say $num.perl; # [42, \u0026#34;str\u0026#34;] (an Array) 这是因为整个表达式是 @array = $num = 42, \u0026quot;str\u0026quot;, 而 $num = 42 不是单独的表达式。\n查看操作符获取关于优先级的更多详情。\n无符号变量 在 Raku 中创建不带符号的变量也是可能的:\nmy \\degrees = pi / 180; my \\θ = 15 * degrees; 然而, 这些无符号变量并不创建容器. 那意味着上面的 degrees 和 θ 实际上直接代表 Nums。 为了说明, 我们定义一个无符号变量后再赋值:\nθ = 3; # Dies with the error \u0026#34;Cannot modify an immutable Num\u0026#34; 无符号变量不强制上下文, 所以它们可被用于原样地传递某些东西:\nsub logged(\u0026amp;f, |args) { say(\u0026#39;Calling \u0026#39; ~ \u0026amp;f.name ~ \u0026#39;with arguments \u0026#39; ~ args.perl); my \\result = f(|args); # ^^^^^^^ not enforcing any context here say(\u0026amp;f.name ~ \u0026#39;returned \u0026#39; ~ result.perl); return |result; } Twigils Twigils 影响变量的作用域。请记住 twigils 对基本的魔符插值没有影响，那就是，如果 $a 内插，$^a, $*a, $=a, $?a, $.a, 等等也会内插. 它仅仅取决于 $。\n   Twigil Scope     * 动态的   ! 属性(类成员)   ? 编译时变量   . 方法(并非真正的变量)   \u0026lt; 匹配对象索引(并非真正的变量)   ^ 自我声明的形式位置参数   : 自我声明的形式命名参数   = Pod 变量   ~ 子语言    * Twigil 动态变量通过 caller 查找, 不是通过外部作用域。例如:\nmy $lexical = 1; my $*dynamic1 = 10; my $*dynamic2 = 100; sub say-all() { say \u0026#34;$lexical, $*dynamic1, $*dynamic2\u0026#34;; } # prints 1, 10, 100 say-all(); { my $lexical = 2; my $*dynamic1 = 11; $*dynamic2 = 101; # 注意,这儿没有使用 my 来声明 # prints 1, 11, 101 say-all(); } # prints 1, 10, 101 say-all(); 第一次调用 \u0026amp;say-all 时, 就像你期望的一样, 它打印 \u0026ldquo;1, 10, 100\u0026rdquo;。可是第二次它打印 \u0026ldquo;1, 11, 101\u0026rdquo;。 这是因为 $lexical 不是在调用者的作用域内被查找, 而是在 \u0026amp;say-all 被定义的作用域里被查找的。这两个动态作用域变量在调用者的作用域内被查找, 所以值为 11 和 101。第三次调用 \u0026amp;say-all 后, $*dynamic1 不再是 11 了。但是 $*dynamic2 仍然是 101。这源于我们在块中声明了一个新的动态变量 $*dynamic1 的事实并且没有像我们对待 $*dynamic2 那样把值赋值给旧的变量。\n动态变量与其他变量类型在引用一个未声明的动态变量上不同的是前者不是一个编译时错误，而是运行时 failure，这样一个动态变量可以在未定义时使用只要在把它用作任何其它东西的时候检查它是否定义过:\nsub foo() { $*FOO // \u0026#39;foo\u0026#39;; } say foo; # -\u0026gt; \u0026#39;foo\u0026#39; my $*FOO = \u0026#39;bar\u0026#39;; say foo; # -\u0026gt; \u0026#39;bar\u0026#39; ! Twigil 属性是变量, 存在于每个类的实例中. 通过 ! 符号它们可以从类的里面直接被访问到:\nclass Point { has $.x; has $.y; method Str() { \u0026#34;($!x, $!y)\u0026#34; } } 注意属性是怎样被声明为 $.x 和 $.y 的, 但是仍然能够通过 $!x 和 $!y 访问到属性. 这是因为 在 Raku 中, 所有的属性都是私有的, 并且在类中能使用 $!attribute-name 直接访问这些属性. Raku 能自动为你生成访问方法. 关于对象、类和它们的属性和方法的详情, 请查看面向对象.\n? Twigil 编译时\u0026quot;常量\u0026quot;, 可通过 ? twigil 访问. 编译器对它们很熟悉, 并且编译后不能被修改. 常用的一个例子如下:\nsay \u0026#34;$?FILE:$?LINE\u0026#34;; # prints \u0026#34;hello.pl: 23\u0026#34; if this is the 23 line of a # file named \u0026#34;hello.pl\u0026#34;. 关于这些特殊变量的列表请查看编译时变量。\n尽管不能在运行时改变它们, 用户可以(重新)定义这种常量.\nconstant $?TABSTOP = 4; # this causes leading tabs in a heredoc or in a POD # block\u0026#39;s virtual margin to be counted as 4 spaces. . Twigil . twigil 真的不是用于变量的. 实际上, 看下面的代码:\nclass Point { has $.x; has $.y; method Str() { \u0026#34;($.x, $.y)\u0026#34; # 注意我们这次使用 . 而不是 ! } } 对 self(自身)调用了方法 x 和方法 y, 这是自动为你生成的, 因为在你声明你的属性的时候, 你使用的是 . twigil 。 注意, 子类可能会覆盖那些方法. 如果你不想这个发生, 请使用 $!x 和 $!y 代替。\n. twigil 只是调用了一个方法也表明下面是可能的\nclass SaySomething { method a() { say \u0026#34;a\u0026#34;; } method b() { $.a; } } SaySomething.b; # prints \u0026#34;a\u0026#34; 关于对象、类和它们的属性和方法的详情, 请查看面向对象.\n\u0026lt; Twigil \u0026lt; twigil 是 $/\u0026lt;...\u0026gt; 的别名, 其中, $/ 是匹配变量。关于匹配变量的更多详情请查看 $/变量和类型匹配\n^ Twigil ^ twigil 为 block 块 或 子例程 声明了一个形式位置参数. 形如 $^variable 的变量是一种占位变量. 它们可用在裸代码块中来声明代码块的形式参数。看下面代码中的块:\nfor ^4 { say \u0026#34;$^secondsfollows $^first\u0026#34;; } 它打印出:\n1 follows 0 3 follows 2 有两个形式参数，就是 $first 和 $second. 注意, 尽管 $^second 在代码中出现的比 $^first 早, $^first 依然是代码块中的第一个形式参数. 这是因为占位符变量是以 Unicode 顺序排序的。\n子例程也能使用占位符变量, 但是只有在子例程没有显式的参数列表时才行. 这对普通的块也适用\nsub say-it { say $^a; } # valid sub say-it() { say $^a; } # invalid { say $^a; } # valid -\u0026gt; $x, $y, $x { say $^a; } # 非法, 已经有参数列表 $x,$y,$x 了 占位符变量语法上不能有类型限制. 也注意, 也不能使用单个大写字母的占位符变量, 如 $^A\n: Twigil : twigil 为块或子例程声明了一个形式命名参数。使用这种形式声明的变量也是占位符变量的一种类型。因此适用于使用 ^ twigil 声明的变量的东西在这儿也适用(除了它们不是位置的以外, 因此没有使用 Unicode 顺序排序)。所以这个:\nsay { $:add ?? $^a + $^b !! $^a - $^b }( 4, 5 ) :!add # OUTPUT: # -1 查看 ^获取关于占位符变量的更多细节。\n= Twigil = twigil 用于访问 Pod 变量。当前文件中的每个 Pod 块都能通过一个 Pod 对象访问到, 例如 $=data, $=SYNOPSIS 或 =UserBlock, 即：一个和想要的块同名的变量加上一个 = twigil。\n=beginFoo... =end Foo #after that, $=Foo gives you all Foo-Pod-blocks 您可以通过 $=pod访问 Pod 树，它包含所有作为分级数据结构的Pod结构。\n请注意，所有这些 $=someBlockName 都支持位置和关联角色。\n~ Twigil 注意: Slangs（俚语）在 Rakudo 中还没有被实现。 NYI = Not Yet Implemented。\n~ twigil 是指子语言（称为俚语）。下面是有用的：\n   变量名 说 明     $~MAIN the current main language (e.g. Perl statements)   $~Quote the current root of quoting language   $~Quasi the current root of quasiquoting language   $~Regex the current root of regex language   $~Trans the current root of transliteration language   $~P5Regex the current root of the Perl 5 regex language    你在你当前的词法作用域中扩充这些语言。\nuse MONKEY-TYPING; augment slang Regex { # derive from $~Regex and then modify $~Regex token backslash:std\u0026lt;\\Y\u0026gt; {YY }; } 变量声明符和作用域 通常, 使用 my 关键字创建一个新的变量就足够了:\nmy $amazing-variable = \u0026#34;World\u0026#34;; say \u0026#34;Hello $amazing-variable!\u0026#34;; # Hello World! 然而, 有很多声明符能在 Twigils 的能力之外改变作用域的细节。\n   声明符 作用     my 作为词法作用域名字的开头   our 作为包作用域名字的开头   has 作为属性名的开头   anon 作为私有名字的开头   state 作为词法作用域但是持久名字的开头   augment 给已存在的名字添加定义   supersede 替换已存在名字的定义    还有两个类似于声明符的前缀, 但是作用于预定义变量:\n   前缀 作用     temp 在作用域的最后恢复变量的值   let 如果 block 成功退出就恢复变量的值    my 声明符 使用 my 声明一个变量给变量一个词法作用域. 这意味着变量只在当前块中存在.例如:\n{ my $foo = \u0026#34;bar\u0026#34;; say $foo; # -\u0026gt; \u0026#34;bar\u0026#34; } say $foo; # !!! \u0026#34;Variable \u0026#39;$foo\u0026#39; is not declared\u0026#34; 它抛出异常,因为只要我们在同一个作用域内 $foo 才被定义. 此外, 词法作用域意味着变量能在新的作用域内被临时地重新定义:\nmy $location = \u0026#34;outside\u0026#34;; sub outer-location { # Not redefined: say $location; } outer-location; # -\u0026gt; \u0026#34;outside\u0026#34; sub in-building { my $location = \u0026#34;inside\u0026#34;; say $location; } in-building; # -\u0026gt; \u0026#34;inside\u0026#34; outer-location; # -\u0026gt; \u0026#34;outside\u0026#34; 如果变量被重新定义了, 任何引用外部变量的代码会继续引用外部变量. 所以, 在这儿, \u0026amp;outer-location 仍然打印外部的 $location:\nsub new-location { my $location = \u0026#34;nowhere\u0026#34; outer-location; } new-location; # -\u0026gt; \u0026#34;outside\u0026#34; 为了让 new-location() 能打印 nowwhere, 需要使用 * twigil 让 $location 变为动态变量. 对于子例程来说, my 是默认作用域, 所以 my sub x( ) { } 和 sub x( ) { } 是一样的.\nour 声明符 our 跟 my 的作用类似, 除了把别名引入到符号表之外:\nmodule M { our $Var; # $Var available here } # Available as $M::Var here. 声明一组变量 声明符 my 和 our 接收一组扩在圆括号中的变量作为参数来一次性声明多个变量。\nmy (@a, $s, %h); 这可以和解构赋值结合使用。任何对这样一个列表的赋值会取得左侧列表中提供的元素数量并且从右侧列表中把对应的值赋值给它们。没有得到赋值的元素会根据变量的类型得到一个未定义值。\nmy (Str $a, Str $b, Int $c) = \u0026lt;a b\u0026gt;; say [$a, $b, $c].perl; # OUTPUT«[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, Int]␤» 要把列表解构到一个单个的值中, 通过使用 ($var,) 创建一个带有一个值的列表字面值。当使用了一个变量声明符时只在单个变量周围提供一个圆括号就足够了。\nsub f { 1,2,3 }; my ($a) = f; say $a.perl; # OUTPUT«1␤» 要跳过列表中的元素, 使用匿名状态变量 $。\nmy ($,$a,$,%h) = (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, [1,2,3], {:1th}); say [$a, %h].perl; # OUTPUT«[\u0026#34;b\u0026#34;, {:th(1)}]␤» has 声明符 has 作用在类的实例或 role 的属性上, 还有类或 roles 的方法上。has 隐式作用于方法上, 所以 has method x() {} 和 method x() {} 做得是相同的事情。\n查看面向对象获取更多文档和例子。\nhas method x( ) { } 等价于:\nmethod x( ) { } anon 声明符 anon 声明符阻止符号本安装在词法作用域内, 还有方法表中, 和其它任何地方. 例如, 你可以使用 anon 声明一个知道自己名字的子例程, 但是仍然不会被安装到作用域内:\nmy %operations = half =\u0026gt; anon sub half($x) { $x / 2 }, square =\u0026gt; anon sub square($x) { $x * $x }, ; say %operations\u0026lt;square\u0026gt;.name; # square say %operations\u0026lt;square\u0026gt;(8); # 64 state 声明符 state 声明词法作用域变量, 就像 my 那样。然而, 初始化只发生一次, 就在正常执行流中第一次遇见初始化的时候。因此, state 变量会在闭合块或 程序的多次执行之间保留它们的值。\n因此, 下面这个子例程:\nsub a { state @x; state $l = \u0026#39;A\u0026#39;; @x.push($l++); }; say a for 1..6; 会持续增加 $l 并在每次被调用时把它追加到 @x 中, 所以它会打印出:\n[A] [A B] [A B C] [A B C D] [A B C D E] [A B C D E F] This works per \u0026ldquo;clone\u0026rdquo; of the containing code object, as in this example:\n({ state $i = 1; $i++.say; } xx 3).map: {$_(), $_()}; # says 1 then 2 thrice 注意，这不是一个线程安全的解构, 当同一个 block 的同一个克隆运行在多个线程中时。要知道方法只有每个类一个克隆，而不是每个对象。\n至于 my，声明多个状态变量必须放置在圆括号中, 而声明一个单一变量，圆括号可以省略。\n请注意，许多操作符都伴随着隐式绑定，什么会导致超距作用。使用 .clone 或强迫创建一个可以绑定的新容器。\nmy @a; sub f() { state $i; $i++; @a.push: \u0026#34;k$i\u0026#34; =\u0026gt; $i # \u0026lt;-- .clone goes here }; f for 1..3; dd @a; # «Array $var = $[:k1(3), :k2(3), :k3(3)]» 所有的状态变量都是线程间共享的。这个结果可能是你不希望得到的或危险的。\nsub code(){ state $i = 0; say ++$i; $i }; await start { loop { last if code() \u0026gt;= 5 } }, start { loop { last if code() \u0026gt;= 5 } }; # OUTPUT«1␤2␤3␤4␤4␤3␤5␤» # OUTPUT«2␤1␤3␤4␤5␤» # many other more or less odd variations can be produced $ 变量 和显式地声明命名状态变量一样, $ 能够用作不带显式状态声明的匿名状态变量。\nsay \u0026#34;1-a 2-b 3-c\u0026#34;.subst(:g, /\\d/, {\u0026lt;one two three\u0026gt;[$++]}); # OUTPUT«one-a two-b three-c␤» 更进一步, 状态变量不需要存在于子例程中。你可以, 举个例子, 在单行程序中使用 $ 在文件中编号行号。\nraku -ne \u0026#39;say ++$ ~ \u0026#34; $_\u0026#34;\u0026#39; example.txt 实际上词法范围内每个对 $ 的引用都是是一个单独的变量。\nraku -e \u0026#39;{ say ++$; say $++ } for ^5\u0026#39; # OUTPUT«1␤0␤2␤1␤3␤2␤4␤3␤5␤4␤» 如果在作用域内你需要多次引用 $ 的值, 那么它应该被拷贝到一个新的变量中。\nsub foo() { given ++$ { when 1 { say \u0026#34;one\u0026#34;; } when 2 { say \u0026#34;two\u0026#34;; } when 3 { say \u0026#34;three\u0026#34;; } default { say \u0026#34;many\u0026#34;; } } } foo() for ^3; # OUTPUT«one␤two␤three␤» @ 变量 和 $ 变量类似, 也有一个位置匿名状态变量 @。\nsub foo($x) { say (@).push($x); } foo($_) for ^3; # OUTPUT: # [0] # [0 1] # [0 1 2] 这里的 @ 是用圆括号括起来了以和名为 @.push 的类成员变量消除歧义。索引访问并不需要这种歧义，但你需要拷贝这个值，以便用它做任何有用的事情。\nsub foo($x) { my $v = @; $v[$x] = $x; say $v; } foo($_) for ^3; # OUTPUT: # [0] # [0 1] # [0 1 2] 就和 $ 一样, 作用域中的每次提及 @ 就引入了一个新的匿名数组。\n% 变量 最后, 还有一个关联匿名状态变量 %。\nsub foo($x) { say (%).push($x =\u0026gt; $x); } foo($_) for ^3; # OUTPUT: # 0 =\u0026gt; 0 # 0 =\u0026gt; 0, 1 =\u0026gt; 1 # 0 =\u0026gt; 0, 1 =\u0026gt; 1, 2 =\u0026gt; 2 关于歧义的同样警告适用。正如你可能期望，索引访问也有可能（使用复制以使之有用）。\nsub foo($x) { my $v = %; $v{$x} = $x; say $v; } foo($_) for ^3; # OUTPUT: # 0 =\u0026gt; 0 # 0 =\u0026gt; 0, 1 =\u0026gt; 1 # 0 =\u0026gt; 0, 1 =\u0026gt; 1, 2 =\u0026gt; 2 就像其它的匿名状态变量一样, 在给定作用域中每次提及 % 最终都会引入一个单独的变量。\naugment 声明符 使用 augment, 你可以给已经存在的类或 grammars 增加属性和方法.\n因为类通常使用 our 作用域, 因此是全局的, 这意味着修改全局状态, 这是强烈不鼓励的, 对于大部分情况, 有更好的方法.\n# don\u0026#39;t do this use MONKEY-TYPING; augment class Int { method is-answer { self == 42 } } say 42.is-answer; # True temp 前缀 像 my 一样, temp 在作用域的末尾恢复旧的变量值. 然而, temp 不创建新的变量.\nmy $in = 0; # temp will \u0026#34;entangle\u0026#34; the global variable with the call stack # that keeps the calls at the bottom in order. sub f(*@c) { (temp $in)++; \u0026#34;\u0026lt;f\u0026gt;\\n\u0026#34; ~ @c\u0026gt;\u0026gt;.indent($in).join(\u0026#34;\\n\u0026#34;) ~ (+@c ?? \u0026#34;\\n\u0026#34; !! \u0026#34;\u0026#34;) ~ \u0026#39;\u0026lt;/f\u0026gt;\u0026#39; }; sub g(*@c) { (temp $in)++; \u0026#34;\u0026lt;g\u0026gt;\\n\u0026#34; ~ @c\u0026gt;\u0026gt;.indent($in).join(\u0026#34;\\n\u0026#34;) ~ (+@c ?? \u0026#34;\\n\u0026#34; !! \u0026#34;\u0026#34;) ~ \u0026#34;\u0026lt;/g\u0026gt;\u0026#34; }; print g(g(f(g()), g(), f())); # OUTPUT: # \u0026lt;g\u0026gt; # \u0026lt;g\u0026gt; # \u0026lt;f\u0026gt; # \u0026lt;g\u0026gt; # \u0026lt;/g\u0026gt; # \u0026lt;/f\u0026gt; # \u0026lt;g\u0026gt; # \u0026lt;/g\u0026gt; # \u0026lt;f\u0026gt; # \u0026lt;/f\u0026gt; # \u0026lt;/g\u0026gt; # \u0026lt;/g\u0026gt; let 前缀 跟 temp 类似, 如果 block 没有成功退出则恢复之前的值。成功的退出意味着该 block 返回了一个定义过的值或一个列表。\nmy $answer = 42; { let $answer = 84; die if not Bool.pick; CATCH { default { say \u0026#34;it\u0026#39;s been reset :(\u0026#34; } } say \u0026#34;we made it 84 sticks!\u0026#34;; } say $answer; 在上面的例子中, 如果 Bool.pick 返回 true, 那么答案会保持为 84, 因为那个 block 返回了一个定义了的值(say 返回 true)。 否则那个 die 语句会让那个 block 不成功地退出, 把答案重新设置为 42。\n类型约束和初始化 变量可以有类型约束, 约束在声明符和变量名之间:\nmy Int $x = 42; $x = \u0026#39;a string\u0026#39;; # throws an X::TypeCheck::Assignment error CATCH { default { put .^name, \u0026#39;: \u0026#39;, .Str } } # OUTPUT: X::TypeCheck::Assignment: Type check failed in assignment to $x; expected Int but got Str (\u0026#34;a string\u0026#34;) 如果一个标量有类型约束但是没有初始值, 它会使用类型约束的类型对象来初始化.\nmy Int $x; say $x.^name; # Int say $x.defined; # False 没有显式类型约束的标量的类型为 Mu, 但是默认会是 Any 类型的对象.\n带有 @ 符号的变量会被初始化为空的数组; 带有 % 符号的变量会被初始化为空的散列.\n变量的默认值可以使用 is default 特性设置, 通过把 Nil 赋值给变量来重新应用默认值:\nmy Real $product is default(1); say $product; # 1 $produce *= 5; say $product; # 5 $product = Nil; say $product; # 1 默认的有定义的变量指令 为了强制所有的变量拥有一个有定义的约束, 使用 use variables :D 指令。这个指令是词法作用域的并且可以使用 use variables :_ 指令进行切换。\nuse variables :D; my Int $i; # OUTPUT«===SORRY!=== Error while compiling \u0026lt;tmp\u0026gt;␤Variable definition of type Int:D (implicit :D by pragma) requires an initializer ... my Int $i = 1; # that works { use variables :_; my Int $i; } # 在这个 block 中关掉它 请注意, 赋值 Nil 会把这个变量恢复为它的默认值。一个有定义的约束类型的默认值是类型名加上 :D(例如 Int:D)。That means a definedness contraint is no guarantee of definedness. 这只适用于变量初始化, 不适用于签名。\n特殊变量 Pre-defined lexical variables\n每个代码块中都有3个特别的变量:\n   变量 意义     $_ 特殊变量   $/ 正则匹配   $! 异常    $_ $_ 是特殊变量，在没有显式标识的代码块中，它是默认参数。所以诸如 for @array { ... } 和 given $var { ... } 之类的结构会将变量绑定给 $_.\nfor \u0026lt;a b c\u0026gt; { say $_ } # sets $_ to \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39; and \u0026#39;c\u0026#39; in turn say $_ for \u0026lt;a b c\u0026gt;; # same, even though it\u0026#39;s not a block given \u0026#39;a\u0026#39; { say $_ } # sets $_ to \u0026#39;a\u0026#39; say $_ given \u0026#39;a\u0026#39;; # same, 尽管这不是一个块 CATCH 块将 $_ 设置为捕获到的异常。 ~~ 智能匹配操作符。 对 $_ 调用一个方法可以省略特殊变量 $_ 的名字，从而写的更短：\n.say; # 与 $_.say 相同 m/regex/ 和 /regex/ 正则匹配 和 s/regex/subst/ 替换是作用于 $_ 上的.\nsay \u0026#34;Looking for strings with non-alphabetic characters...\u0026#34;; for \u0026lt;ab:c d$e fgh ij*\u0026gt; { .say if m/\u0026lt;!alpha\u0026gt;/; } 输出:\nLooking for strings with non-alphabetic characters... ab:c d$e ij* $/ $/ 是匹配变量。它存储着最近一次正则匹配的结果，通常包含 Match 类型的对象。\n\u0026#39;abc 12\u0026#39; ~~ /\\w+/; # 设置 $/ 为一个Match 对象 say $/.Str; # abc Grammar.parse 方法会把调用者的 $/ 设置为 Match object 的结果。看下面的代码:\nuse XML::Grammar; # panda install XML XML.Grammar.parse(\u0026#34;\u0026lt;p\u0026gt;some text\u0026lt;/p\u0026gt;\u0026#34;); say $/; # OUTPUT: # ｢\u0026lt;p\u0026gt;some text\u0026lt;/p\u0026gt;｣ # root =\u0026gt; ｢\u0026lt;p\u0026gt;some text\u0026lt;/p\u0026gt;｣ # name =\u0026gt; ｢p｣ # child =\u0026gt; ｢some text｣ # text =\u0026gt; ｢some text｣ # textnode =\u0026gt; ｢some text｣ # element =\u0026gt; ｢\u0026lt;p\u0026gt;some text\u0026lt;/p\u0026gt;｣ # name =\u0026gt; ｢p｣ # child =\u0026gt; ｢some text｣ # text =\u0026gt; ｢some text｣ # textnode =\u0026gt; ｢some text｣ 其他匹配变量是 $/ 元素的别名：\n$0 # same as $/[0] $1 # same as $/[1] $\u0026lt;named\u0026gt; # same as $/\u0026lt;named\u0026gt; 位置属性 如果正则中有捕获分组, $/ 中会有位置属性. 它们由圆括号组成.\n\u0026#39;abbbbbcdddddeffg\u0026#39; ~~ /a (b+)c (d+ef+)g /; say $/[0]; # ｢bbbbb｣ say $/[1]; # ｢dddddeff｣ 这些捕获分组也能使用 $0,$1,$2 等便捷形式取得:\nsay $0; # ｢bbbbb｣ say $1; # ｢dddddeff｣ 要获取所有的位置属性, 使用 $/.list, @$/,@( ) 中的任意一个都可以:\nsay @().join; # bbbbbdddddeff 命名属性 如果正则中有命名捕获分组, $/ 可以有命名属性, 或者正则调用了另一个正则:\n\u0026#39;I.... see?\u0026#39; ~~ /\\w+$\u0026lt;punctuation\u0026gt;=[\u0026lt;-[\\w\\s]\u0026gt;+]\\s*$\u0026lt;final-word\u0026gt;=[\\w+.]/; say $/\u0026lt;punctuation\u0026gt;; # ｢....｣ say $/\u0026lt;final-word\u0026gt;; # ｢see?｣ 这些命名捕获分组也能使用便捷形式的 $\u0026lt;named\u0026gt; 获取:\nsay $\u0026lt;punctuation\u0026gt;; # ｢....｣ say $\u0026lt;final-word\u0026gt;; # ｢see?｣ 要获取所有的命名属性, 使用 $/.hash, %$/, %()中的任何一个:\nsay %().join; # \u0026#34;punctuation ....final-word see?\u0026#34; $! 变量 $! 是错误变量. 如果 try block 或语句前缀捕获到异常, 那个异常就会被存储在 $! 中。如果没有捕获到异常, 那么 $! 会被设置为 Any 类型对象。 注意, CATCH 块不设置 $!。CATCH 在 block 中把 $_ 设置为捕获到的异常。\n编译时变量    Compile-time Variables 说明     $?FILE 所在文件   $?LINE 所在行   ::?CLASS 所在类   \u0026amp;?ROUTINE 所在子例程   \u0026amp;?BLOCK 所在块   %?LANG What is the current set of interwoven languages?   %?RESOURCES The files associated with the \u0026ldquo;Distribution\u0026rdquo; of the current compilation unit.    for \u0026#39;.\u0026#39; { .Str.say when !.IO.d; .IO.dir()\u0026gt;\u0026gt;.\u0026amp;?BLOCK when .IO.d # lets recurse a little! } 其它编译时变量：\n   Compile-time Variables 说明     $?PACKAGE 所在包   $?MODULE 所在模块   $?CLASS 所在类(as variable)   $?ROLE 所在角色(as variable)   $?GRAMMAR 所在 grammar   $?TABSTOP 在 heredoc 或 虚拟边距中 tab 有多少空格   $?USAGE 从 MAIN 程序的签名中生成的使用信息   $?ENC Str.encode/Buf.decode/various IO 方法的默认编码.    动态变量    Dynamic Variable 说明     $*ARGFILES 神奇的命令行输入句柄   @*ARGS 来自命令行的参数   $*IN 标准输入文件句柄, AKA stdin   $*OUT 标准输出文件句柄, AKA stdout   $*ERR 标准错误文件句柄, AKA stderr   %*ENV 环境变量   $*REPO 存储安装过的/加载了的模块信息的变量   $*TZ 系统的本地时区.   $*CWD 当前工作目录.   $*KERNEL 在哪个内核下运行   $*DISTRO 在哪个操作系统分发下运行   $*VM 在哪个虚拟机下运行   $*PERL 在哪个 Perl 下运行   $*PID 当前进程的进程 ID   $*PROGRAM-NAME 当前可执行程序的路径就像它通过命令行键入一样, 或 -e 如果 perl 引用了 -e 标记   $*PROGRAM 正被执行的 Perl 程序的位置( 以 IO::Path 对象的形式)   $*EXECUTABLE 当前运行的可执行 perl 的绝对路径   $*EXECUTABLE-NAME 当前运行的可执行 perl 程序的名字。(e.g. raku-p, raku-m, Niecza.exe)   $*USER 正在运行该程序的用户. 它是一个被求值为 \u0026ldquo;username (uid)\u0026rdquo; 的对象. 它只有在被当作字符串时才被求值为用户名, 如果被当作数字则被求值为数值化的用户 id。   $*GROUP 运行程序的用户的主要组. 它是被计算为 \u0026ldquo;groupname (gid)\u0026rdquo; 的对象.它只有在被当作字符串时才被求值为组名, 如果被当作数字则被求值为数值化的组 id。   $*HOME 代表当前运行程序的用户家目录的 IO::Path 对象。如果家目录不确定则为 Nil。   $*SPEC 程序运行的平台的合适的 IO::Spec 子类, 对于特定操作系统代码,使用智能匹配: say \u0026ldquo;We are on Windows!\u0026rdquo; if $*SPEC ~~ IO::Spec::Win32   $*TMPDIR 代表着 \u0026ldquo;系统临时目录\u0026rdquo; 的 IO::Path 对象   $*TOLERANCE 由 \u0026lt;=~=\u0026gt; 操作符使用并且任何依赖它的操作, 来决定两个值是否近似地相等, 默认为 1e-15。   $*THREAD 代表当前执行线程的 Thread 对象。   $*SCHEDULER 代表当前默认调度程序的 ThreadPoolScheduler 对象。    注意 $*SCHEDULER 的用法:\n对于当前的 Rakudo, 这个默认在方法 .hyper 和 .race 上采用最大 16 个线程。要更改线程的最大数量, 要么在运行 perl 之前设置环境变量 RAKUDO_MAX_THREADS 的值, 要么在使用 .hyper 或 .race 之前创建一个默认改变了的作用域的拷贝:\nmy $*SCHEDULER = ThreadPoolScheduler.new( max_threads =\u0026gt; 64 ); 这种行为在 spec 测试中没有被测试并且还会变化。\n","permalink":"https://ohmyweekly.github.io/notes/2015-08-25-variables-in-raku/","tags":["variable"],"title":"Raku 中的变量"},{"categories":["rakulang"],"contents":"除了 q 和 qq 之外，现在还有一种基本形式的 Q，它不会进行插值，除非显式地修改它那样做。所以，q 实际上是 Q:q 的简称，qq 实际上是 Q:qq 的简称。实际上所有的 quote-like 形式都派生自带有副词的 Q 形式：\nS02-literals/quoting.t lines 95–116 S02-literals/quoting.t lines 132–139\nq// Q :q // qq// Q :qq // rx// Q :regex // s/// Q :subst /// tr/// Q :trans /// 诸如 :regex 的副词通过转换到不同的解析器改变了语言的解析。这能完全改变任何之后的副词还有所引起的东西自身的解释。\nq:s//Q :q :scalar  rx:s//Q :regex:scalar// 就像 q[...] 拥有简写形式的 \u0026lsquo;\u0026hellip;\u0026rsquo;, 并且 qq[...] 拥有简写形式的 \u0026ldquo;\u0026hellip;\u0026rdquo; 一样，完整的 Q[...] 引用也有一种使用半角括号 ｢\u0026hellip;｣ 的短形式。\n引号上的副词 广义上的引号现在可以接收副词了：\n S02-literals/quoting.t lines 210–223 S02-literals/quoting.t lines 55–69 S02-literals/quoting.t lines 427–501  Short Long Meaning ===== ==== ======= :x :exec 作为命令执行并返回结果 :w :words 按单词分割结果(没有引号保护) :ww :quotewords 按单词分割结果 (带有引号保护) :v :val Evaluate word or words for value literals :q :single 插值 \\\\, \\q 和 \\' (or whatever) :qq :double 使用 :s, :a, :h, :f, :c, :b 进行插值 :s :scalar 插值 $ vars :a :array 插值 @ vars :h :hash 插值 % vars :f :function 插值 \u0026amp; 调用 :c :closure 插值 {...} 表达式 :b :backslash 插值 \\n, \\t, 等. (至少暗示了 :q ) :to :heredoc 把结果解析为 heredoc 终止符 :regex 解析为正则表达式 :subst 解析为置换 (substitution) :trans 解析为转换 (transliteration) :code Quasiquoting :p :path 返回一个 Path 对象 (查看 S16 获取更多选项) 通过在开头加入一个带有短形式的单个副词的 Q，q，或 qq，你可以省略掉第一个冒号，这产生了如下形式：\nqw /a b c; # P5-esque qw// meaning q:w Qc \u0026#39;...{$x}...; # Q:c//, interpolate only closures qqx/$cmd@args[] # equivalent to P5\u0026#39;s qx// (注意 qx// 不插值)\n如果你想进一步缩写，那么定义一个宏：\nmacro qx {\u0026#39;qq:x \u0026#39; } # equivalent to P5\u0026#39;s qx// macro qTO { \u0026#39;qq❌w:to \u0026#39; } # qq❌w:to// macro quote:\u0026lt;❰ ❱\u0026gt; ($text) { quasi { {{{$text}}}.quoteharder } } 所有大写的副词被保留用作用户定义的引号。所有在 Latin-1 上面的 Unicode 分隔符被保留用作用户定义的引号。 S02-literals/quoting.t lines 352–426\n关于上面我们现在有了一个推论，我们现在能说：\n%hash = qw:c/a b c d {@array}{%hash}; 或：\n%hash = qq:w/a b c d {@array}{%hash}; 把东西(items)插值到 qw 中。默认地，数组和散列在插值时只带有空格分隔符，所以之后的按空格分割仍旧能工作。（但是内置的 «\u0026hellip;» 引号自动进行了等价于 qq:ww:v/.../ 的插值）。 内置的 \u0026lt;...\u0026gt; 等价于 q:w:v/.../。\n","permalink":"https://ohmyweekly.github.io/notes/2015-08-24-quote-in-raku/","tags":["quote"],"title":"Raku 中的引号"},{"categories":["rakulang"],"contents":"Raku 支持\u0026quot;泛型, roles 和 多重分发\u0026quot;, 它们都是很好的特点, 并且已经在其它 advent calendar 中发布过了。\n但是今天我们要看的是 MOP。 \u0026ldquo;MOP\u0026rdquo; 代表着元对象协议(\u0026ldquo;Meta-Object Protocol\u0026rdquo;)。那意味着, 它们实际上是你能从用户那边改变的一部分, 而不是对象、类等定义语言的东西。\n实际上, 在 Raku中, 你可以为类型添加方法, 移除某个方法, 包裹方法, 使用更多能力增强类(OO::Actors 和 OO::Monitors 就是两个这样的例子), 或者你可以完全重定义它(并且, 例如, 使用 Ruby-like 的对象系统。这儿有个例子)。\n但是今天, 我们首先看一下第一部分: 自省。在类型创建完之后查看它的类型, 了解它, 并使用这些信息。\n我们将要创建的模块是基于 Sixcheck 模块(一个 QuickCheck-like 模块)的需求: 为某个类型生成一些随机数据, 然后把数据喂给我们正测试的函数, 并检查某些后置条件(post-condition)。\n所以, 我们先写出第一个版本:\nmy %special-cases{Mu} = (Int) =\u0026gt; -\u0026gt; { (1..50).pick }, (Str) =\u0026gt; -\u0026gt; { (\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;).pick(50).join(\u0026#39;\u0026#39;) }, ; sub generate-data(Mu:U \\t) { %special-cases{t} ?? %special-cases{t}() !! t.new; } generate-data(Int); 注意以下几点:\n 我们给 %special-cases 指定了键的类型。那是因为默认地, 键的类型为 Str。显然地, 我们不想让我们的类型字符串化。我们实际上做的是指定它们为\u0026quot;Mu\u0026quot;的子类(这在类型\u0026quot;食物链\u0026quot;的顶端)。 我们在 Int 和 Str 周围放上圆括号, 以避免字符串化。 我们在函数参数类型中使用了 :U。那意味着那个值必须是未定义的(undefined)。类型对象(就像 Int、Str 等等)是未定义的, 所以它能满足我们(你可能见过一个叫 Nil 的不同的未知值)。 类型对象实际上是对象, 就像其它任何对象一样。这就是为什么我们在类型对象上调用 .new 方法, 例如, 它和直接调用 Int.new 相同(那对一致性和 autovivification 很有用)。 我们为 Int 和 Str 提供了 fallback, 因为调用 Int.new 和 Str.new ( 0 和 \u0026quot;\u0026quot; )不会在我们创建的数据中给我们任何随机化。 Raku 在函数中自动返回最后一个表达式。所以不需要在那儿放上一个 return。  我们用代码生成数据, 公平且公正。但是我们需要生成更多那样简单的数据。\n我们至少需要支持带有属性的类: 我们想查看属性列表, 为它们的类型生成数据, 并把它们喂给构造器。\n我们要能够看到类的内部。用 Raku 的术语来说, 我们将要到达的是元对象协议(Meta-Object Protocol)。首先我们定义一个类:\nclass Article { has Str $.title; has Str $.content; has Int $.view-count; } # 我们可以这样手动创建一个实例 Article.new(title =\u0026gt; \u0026#34;Raku Advent, 第 19 天\u0026#34;, content =\u0026gt; \u0026#34;Magic!\u0026#34;, view-count =\u0026gt; 0 ); 但是我们不想亲手创建那个文章 (article)。我们想把那个 class Article 传递给我们的 generate-data 函数, 并返回一个 Article(里面带有随机数据)。让我们回到我们的 REPL\u0026hellip;\nsay Article.^attributes; # (Str $!title Str $!content Int $!view-count) say Article.^attributes[0].WHAT; # (Attribute) 如果你点击了 MOP 链接, 你不会对我们得到一个含有 3 个元素的数组感到惊讶。如果你仍旧对该语法感到惊讶, 那么 .^ 是元方法调用。意思是 a.^b 会被转换为 a.HOW.b(a)。\n如果我们想知道我们可以访问到什么, 我们问它就是了(移除了匿名的那些):\nAttribute.^methods.grep(*.name ne \u0026#39;\u0026lt;anon\u0026gt;\u0026#39;); # (compose apply_handles get_value set_value # container readonly package inlined WHY set_why Str gist) Attribute.^attributes # Method \u0026#39;gist\u0026#39; not found for invocant of class \u0026#39;BOOTSTRAPATTR\u0026#39; 哎吆… 看起来这有点太 meta 了。幸好, 我们能使用 Rakudo 的一个非常好的属性: 它的大部分都是用 Raku 写的! 要查看我们可以得到什么, 我们查看源代码就好了:\n# has Str $!name; ... # has Mu $!type; 我们得到了键的名字, 还有去生成值的类型。让我们看看\u0026hellip;\n\u0026gt; say Article.^attributes.map(*.name) ($!title $!content $!view-count) \u0026gt; say Article.^attributes.map(*.type) ((Str) (Str) (Int)) 天才! 看起来是正确的。(如果你想知道为什么我们得到 $!（私有的） twigils, 那是因为 $. 只意味着将会生成的一个 getter 方法)。属性本身仍然是私有的, 并且在类中是可访问的。\n现在, 我们唯一要做的事情就是创建一个循环\u0026hellip;\nmy %args; for Article.^attributes -\u0026gt; $attr { %args{$attr.name.substr(2)} = generate-data($attr.type); } say %args.perl; 这是一个将会打印什么的例子:\n{:content(\u0026#34;muenglhaxrvykfdjzopqbtwisc\u0026#34;), :title(\u0026#34;rfpjndgohmasuwkyzebixqtvcl\u0026#34;), :view-count(45)} 每次你运行你的代码你都会得到不同的结果(然而我不认为它会创建一篇值得阅读的文章…)。剩下唯一要做的就是把它们传递给 Article 的构造函数:\nsay Article.new(|%args); (前缀 | 允许我们把 %args 作为具名参数传递, 而不是单个位置参数)。再次, 你应该会打印这些东西:\nArticle.new(title =\u0026gt; \u0026#34;kyvphxqmejtuicrbsnfoldgzaw\u0026#34;, content =\u0026gt; \u0026#34;jqbtcyovxlngpwikdszfmeuahr\u0026#34;, view-count =\u0026gt; 26) 呀! 我们设法在不了解 Article 的情况下胡乱地(blindly)创建了一个 Article 实例。 我们的代码能够用于为任何期望传递它的类属性的构造函数生成数据。好了!\nPS: 留个作业! 移动到 generate-data 函数, 以至于我们能给 Article 添加一个 User $.author 属性, 并且构建好这个函数。祝你好运!\n","permalink":"https://ohmyweekly.github.io/notes/2015-07-25-introspection-in-raku/","tags":["introspection"],"title":"Raku 中的自省"},{"categories":["rakulang"],"contents":"学会了怎么创建类, 我们继续用它来构建我们的中心内容:\nclass Hammer { method hammer($stuff) { say \u0026#34;You hammer on $stuff. BAM BAM BAM!\u0026#34;; } } class Gavel { method hammer($stuff) { say \u0026#34;You hammer on $stuff. BAM BAM BAM!\u0026#34;; } } class Mallet { method hammer($stuff) { say \u0026#34;You hammer on $stuff. BAM BAM BAM!\u0026#34;; } } 但是注意到了吗？这三个方法包含了同样的方法, 在类中重复了。我们必须那样做如果我们想让每个 Hammar、Gavel 和 Mallet 有能力击打的话。（并且那是合理的）。但是遗憾的是我们不得不把那个方法一式三份。\n为什么遗憾？ 因为一方面在真实世界中, 方法并不是很彼此相似, 有一天你决定在 hammer 方法中更改某些东西, 并没有意识到这个方法在三个不同的地方\u0026hellip; 这导致了一堆痛苦和难受。\n所以我们的新玩具, 类, 展现出了一个问题。我们想在每个类中重用 hammer 方法。一个新的概念, role 来拯救我们来了:\nrole Hammering { method hammer($stuff) { say \u0026#34;You hammer on $stuff. BAM BAM BAM!\u0026#34;; } } 虽然类经常以一个合适的名词命名, 但是 roles 经常以一个分词命名, 例如 Hammering。这不是一个必须遵守的规则, 但是它是一个好的经验法则。现在类的定义变的简单了:\nclass Hammer does Hammering { } class Gavel does Hammering { } class Mallet does Hammering { } 是的, 我们喜欢那样。\n这发生了什么？ 我们在类上使用 does 是干什么用的？ role 定义中的所有方法都被拷贝到类定义中。因为它是一个拷贝操作, 所以我们可以使用尽可能多的类。\n所以, 我们做的是: 当我们想重用方法的时候把方法放进 roles 里面。\n但是好处不止这一点儿。至少有两个好处:\nmy $hammer = Hammer.new; # create a new hammer object say $hammer ~~ Hammer; # \u0026#34;Bool::True\u0026#34; -- yes, this we know say $hammer ~~ Hammering; # \u0026#34;Bool::True\u0026#34; -- ooh! 所以 $hammer 知道它遵守了(does) Hammering, 我们现在不仅知道了对象属于哪个类, 还知道了对象并入了什么 role。这很有用如果我们不确定我们处理的是什么类型的对象:\nif $unkown_object ~~ Hammering { $unknown_object.hammer(\u0026#34;砸在钉子上\u0026#34;); # will always work } 一个类能一次接收几个 roles 吗？ 是的, 它可以:\nrole Flying { method fly { say \u0026#34;Whooosh!\u0026#34;; } } class FlyingHammer does Hammering does Flying { } 让一个类像那样遵守几个 roles 引入了一个有意思的可能: 冲突, 当来自两个不同 roles 的两个同名方法尝试占领同一个类时。这时会发生什么？ 好吧, 至少有 3 种可能:\n   第一个 role 赢了。 它的方法住进了类中     最后一个 role 赢了。 它覆盖了之前的方法     编译失败。冲突必须被解决。    这种情况下选项 3 应该是正确答案。原因和之前相同: 因为类和工程越来越庞大, 程序员可能意识不到两个 role 之间在哪儿发生冲突。所以我们标记了它。\nrole Sleeping { method lie { say \u0026#34;水平躺下\u0026#34;; } } role Lying { method lie { say \u0026#34;说谎...\u0026#34;; } } class SleepingLiar does Sleeping does Lying { } # 冲突! 下一个问题, 那么: 当在类中有 role 冲突时, 我们怎么修复它？ 简单: 在类中自己定义一个同名的方法:\nclass SleepingLiar does Sleeping does Lying { method lie { say \u0026#34;Lying in my sleep....\u0026#34;; } } 如果你想从一个贴别的 role 中调用一个方法, 语法是这样的:\nclass SleepingLiar does Sleeping does Lying { method lie { self.Sleeping::lie; } } 这就是 roles。它们把可重用的行为混合进类中。\n","permalink":"https://ohmyweekly.github.io/notes/2015-11-16-roles-conflict-in-raku/","tags":["roles"],"title":"Raku 中的角色冲突"},{"categories":["rakulang"],"contents":"Raku 为面向对象编程(OOP)提供强大支持。尽管 Raku 允许程序员以多种范式进行编程, 但面向对象编程是该语言的核心。\nRaku 带有丰富的预定义类型, 可分为两类：常规类型和原生类型。所有你能存储到变量中的东西要么是一个原生的 value, 要么是一个对象。这包括字面值、类型（类型对象）、code 和容器。\n原生类型用于底层类型（例如 uint64）。尽管原生类型没有和对象同样的功能, 如果在其上调用方法, 它们也会自动装入普通对象。\n一切不是原生值的东西都是一个对象。对象确实允许继承和封装。\n使用对象 要在对象上调用方法, 请在对象名上添加一个点, 然后添加方法名称：\nsay \u0026#34;abc\u0026#34;.uc; # OUTPUT: «ABC␤»  这将在 \u0026ldquo;abc\u0026rdquo; 上调用 uc 方法, 这是一个 Str 类型的对象。要为方法提供参数, 请在方法后面的括号内添加参数。\nmy $formatted-text = \u0026#34;Fourscore and seven years ago...\u0026#34;.indent(8); say $formatted-text; # OUTPUT: « Fourscore and seven years ago...»  $formatted-text 现在包含上面的文本, 但缩进了8个空格。\n多个参数由逗号分隔：\nmy @words = \u0026#34;Abe\u0026#34;, \u0026#34;Lincoln\u0026#34;; @words.push(\u0026#34;said\u0026#34;, $formatted-text.comb(/\\w+/)); say @words; # OUTPUT: «[Abe Lincoln said (Fourscore and seven years ago)]␤»  类似地, 可以通过在方法后放置冒号并使用逗号分隔参数列表来指定多个参数：\nsay @words.join(\u0026#39;--\u0026#39;).subst: \u0026#39;years\u0026#39;, \u0026#39;DAYS\u0026#39;; # OUTPUT: «Abe--Lincoln--said--Fourscore and seven DAYS ago␤»  如果要在没有括号的情况下传递参数, 则必须在方法之后添加一个 :, 因此没有冒号或括号的方法调用明确地是没有参数列表的方法调用：\nsay 4.log: ; # OUTPUT: «1.38629436111989␤» ( natural logarithm of 4 )  say 4.log: +2; # OUTPUT: «2␤» ( base-2 logarithm of 4 )  say 4.log +2; # OUTPUT: «3.38629436111989␤» ( natural logarithm of 4, plus 2 ) 许多看起来不像方法调用的操作（例如, 智能匹配或将对象插值到字符串中）可能会导致方法调用。\n方法可以返回可变容器, 在这种情况下, 您可以分配方法调用的返回值。这是使用对象的可读写属性的方式：\n$*IN.nl-in = \u0026#34;\\r\\n\u0026#34;; 在这里, 我们在 $*IN 对象上调用 nl-in 方法, 不带参数, 并使用 = 运算符将其赋值给它返回的容器。\n所有对象都支持类Mu的方法, 它是层次结构的根。所有对象都来自 Mu。\n类型对象 类型本身是对象, 你可以使用类型的名字获取类型对象:\nmy $int-type-obj = Int; 你可以通过调用 WHAT 方法查看任何对象的 type object(它实际上是一个方法形式的宏):\nmy $int-type-obj = 1.WHAT; 使用 === 恒等运算符可以比较类型对象(Mu除外)的相等性：\nsub f(Int $x) { if $x.WHAT === Int { say \u0026#39;you passed an Int\u0026#39;; } else { say \u0026#39;you passed a subtype of Int\u0026#39;; } } 虽然, 在大多数情况下, .isa方法就足够了：\nsub f($x) { if $x.isa(Int) { ... } ... } 子类型可以使用 smartmatching 来检查：\nif $type ~~ Real { say \u0026#39;$type contains Real or a subtype thereof\u0026#39;; } 类 使用 class 关键字进行类的声明, 通常后跟类名：\nclass Journey { } 此声明会创建类型对象并将其安装在名字 Journey 下的当前包和当前词法作用域中。您还可以通过词法声明类\nmy class Journey { } 这限制了它们对当前词法范围的可见性, 如果该类是嵌套在模块或另一个类中的实现细节, 这可能很有用。\n属性 属性存在于每个类的实例中。属性中存储着对象的状态。在 Raku 中, 一切属性都是私有的. 它们一般使用 has 关键字和 ! twigil 进行声明。\nclass Journey { has $!origin; has $!destination; has @!travellers; has $!notes; } 虽然没有公共(甚至保护属性)属性, 不过有一种方式可以自动生成访问器方法: 使用 . twigil 代替 ! twigil 。(那个 . 应该让你想起了方法调用)。\nclass Journey { has $.origin; has $.destination; has @!travellers; has $.notes; } 这默认提供只读访问器, 为了允许更改属性, 要添加 is rw 特性:\nclass Journey { has $.origin; has $.destination; has @!travellers; has $.notes is rw; } 现在, Journey对象创建之后, 它的 .origin, .destination 和 .notes 都能从类的外部访问, 但只有 .notes 可以被修改。\n如果在没有某些属性（例如 origin 或 destination）的情况下实例化对象, 我们可能无法获得想要的结果。要防止这种情况, 请提供默认值或通过使用is required trait 来标记属性以确保在对象创建时设置属性。\nclass Journey { # error if origin is not provided  has $.origin is required; # set the destination to Orlando as default (unless that is the origin!)  has $.destination = self.origin eq \u0026#39;Orlando\u0026#39; ?? \u0026#39;Kampala\u0026#39; !! \u0026#39;Orlando\u0026#39;; has @!travelers; has $.notes is rw; } 因为类从 Mu 继承了一个默认的构造器, 并且我们也要求类为我们生成一些访问器方法.\n# 创建一个新的类的实例. my $vacation = Journey.new( origin =\u0026gt; \u0026#39;Sweden\u0026#39;, destination =\u0026gt; \u0026#39;Switzerland\u0026#39;, notes =\u0026gt; \u0026#39;Pack hiking gear!\u0026#39; ); # 使用访问器; 这打印出 Sweden. say $vacation.origin; # 使用 rw 存取器来更改属性的值. $vacation.notes = \u0026#39;Pack hiking gear and sunglasses!\u0026#39;; 请注意, 虽然默认构造函数可以初始化只读属性, 但它只会设置具有访问器方法的属性。也就是说, 即使您传递 travelers =\u0026gt; [\u0026quot;Alex\u0026quot;, \u0026quot;Betty\u0026quot;] 给默认构造函数, @!travelers 也不会初始化该属性。\n方法 使用 method 关键字在类的 body 中声明方法：\nclass Journey { has $.origin; has $.destination; has @!travellers; has $.notes is rw; method add_traveller($name) { if $name ne any(@!travellers) { push @!travellers, $name; } else { warn \u0026#34;$nameis already going on the journey!\u0026#34;; } } method describe() { \u0026#34;From $!originto $!destination\u0026#34; } } 方法可以有签名, 就像子例程一样。 属性可以在方法中使用, 并且可以始终与 ! twigil 一起使用, 即使属性是用 . twigil 声明的。 这是因为 . twigil 声明了 ! twigil 并生成一个访问方法。\n看看上面的代码, 在方法 describe 中使用 $!origin 和 $.origin 有一个微妙但重要的区别。$!origin 是一种廉价且明显的属性查找。$.origin 是一个方法调用, 因此可以在子类中重写。只在你想要允许覆盖时使用 $.origin。\n与子例程不同, 其他命名参数不会产生编译时或运行时错误。这允许通过重新分派来链接方法。\n你可以编写自己的访问器来覆盖任何或所有自动生成的访问器。\nmy $ⲧ = \u0026#34;\u0026#34; xx 4; # A tab-like thing  class Journey { has $.origin; has $.destination; has @.travelers; has Str $.notes is rw; multi method notes() { \u0026#34;$!notes\\n\u0026#34; }; multi method notes( Str $note ) { $!notes ~= \u0026#34;$note\\n$ⲧ\u0026#34; }; method Str { \u0026#34;⤷ $!origin\\n$ⲧ\u0026#34; ~ self.notes() ~ \u0026#34;$!destination⤶\\n\u0026#34; }; } my $trip = Journey.new( :origin\u0026lt;Here\u0026gt;, :destination\u0026lt;There\u0026gt;, travelers =\u0026gt; \u0026lt;þor Freya\u0026gt; ); $trip.notes(\u0026#34;First steps\u0026#34;); notes $trip: \u0026#34;Almost there\u0026#34;; print $trip; # OUTPUT:  #⤷ Here  # First steps  # Almost there  #  #There ⤶  声明的 multi 方法 notes 使用不同的签名进行读写, notes 覆盖了 $.notes 声明中隐含的自动生成的方法。\n请注意, 在 notes $trip: \u0026quot;Almost there\u0026quot; 中我们使用了间接调用语法, 它首先放置方法名称, 然后放置对象, 然后用冒号分隔参数：method invocant: arguments。只要感觉比经典的句点和括号更自然, 我们就可以使用这种语法。它的工作原理完全相同。\n可以在运行时使用 .\u0026quot;\u0026quot; 运算符解析方法名称。\nclass A { has $.b }; my $name = \u0026#39;b\u0026#39;; A.new.\u0026#34;$name\u0026#34;().say; # OUTPUT: «(Any)␤»  相对于先前的属性部分, 过去用于更新 $.notes 的语法在本节已改变。代替赋值：\n$vacation.notes = \u0026#39;Pack hiking gear and sunglasses!\u0026#39;; 我们现在做一个方法调用:\n$trip.notes(\u0026#34;First steps\u0026#34;); 覆盖默认的自动生成的访问器意味着它在返回赋值时提供可变容器不再可用。方法调用是将属性的更新添加到计算和逻辑的首选方法。许多现代语言可以通过使用 “setter” 方法重载赋值来更新属性。虽然 Raku 可以为此目的使用 Proxy 对象重载赋值运算符, 但是目前不鼓励使用复杂逻辑重载赋值运算符来设置属性作为弱面向对象设计。\nhas $!attribute method attribute() { ... } class A { has $.attr is rw; } 等价于:\nclass A { has $!attr; method attr() is rw { $!attr; } } 类和实例方法 方法的签名可以有一个调用者(invocant)作为第一个参数, 后跟一个冒号, 这允许该方法引用它被调用的对象。\nclass Foo { method greet($me: $person) { say \u0026#34;Hi, I am $me.^name(), nice to meet you, $person\u0026#34;; } } Foo.new.greet(\u0026#34;Bob\u0026#34;); # OUTPUT: «Hi, I am Foo, nice to meet you, Bob␤»  在方法签名中提供调用者还允许通过使用类型约束将方法定义为类方法或对象方法。::?CLASS 变量可用于在编译时提供类名, 与 :U（对于类方法）或 :D（对于实例方法）结合使用。\nclass Pizza { has $!radius = 42; has @.ingredients; # 类方法: construct from a list of ingredients  method from-ingredients(::?CLASS:U $pizza: @ingredients) { $pizza.new( ingredients =\u0026gt; @ingredients ); } # 实例方法  method get-radius(::?CLASS:D:) { $!radius } } my $p = Pizza.from-ingredients: \u0026lt;cheese pepperoni vegetables\u0026gt;; say $p.ingredients; # OUTPUT: «[cheese pepperoni vegetables]␤»  say $p.get-radius; # OUTPUT: «42␤»  say Pizza.get-radius; # This will fail.  CATCH { default { put .^name ~ \u0026#34;:\\n\u0026#34; ~ .Str } }; # OUTPUT: «X::Parameter::InvalidConcreteness:␤  # Invocant of method \u0026#39;get-radius\u0026#39; must be  # an object instance of type \u0026#39;Pizza\u0026#39;,  # not a type object of type \u0026#39;Pizza\u0026#39;.  # Did you forget a \u0026#39;.new\u0026#39;?»  通过使用multi声明符, 方法既可以是类方法也可以是对象方法：\nclass C { multi method f(::?CLASS:U:) { say \u0026#34;class method\u0026#34; } multi method f(::?CLASS:D:) { say \u0026#34;object method\u0026#34; } } C.f; # OUTPUT: «class method␤»  C.new.f; # OUTPUT: «object method␤»  self 在方法内部, 术语 self 可用并绑定到调用对象。self 可以用来调用调用者的其他方法, 包括构造函数：\nclass Box { has $.data; method make-new-box-from() { self.new: data =\u0026gt; $!data; } } self 也可以用在类方法或实例方法中, 但要注意尝试从另一个方法调用一种类型的方法：\nclass C { method g() { 42 } method f(::?CLASS:U:) { self.g } method d(::?CLASS:D:) { self.f } } C.f; # OUTPUT: «42␤»  C.new.d; # This will fail.  CATCH { default { put .^name ~ \u0026#34;:\\n\u0026#34; ~ .Str } }; # OUTPUT: «X::Parameter::InvalidConcreteness:␤  # Invocant of method \u0026#39;f\u0026#39; must be a type object of type \u0026#39;C\u0026#39;,  # not an object instance of type \u0026#39;C\u0026#39;. Did you forget a \u0026#39;multi\u0026#39;?»  self 也可以与属性一起使用, 只要它们具有访问器。self.a 将为声明为 has $.a 的属性调用访问器。然而, self.a 和 $.a 之间存在差异, 因为后者将项化; $.a 将等同于 self.a.item 或 $(self.a)。\nclass A { has $.x = (1, 2, 3); method b() { .say for self.x; .say for $.x } }; A.new.b; # OUTPUT: «1␤2␤3␤(1 2 3)␤»  方法调用的冒号语法仅支持使用方法调用 self, 而不支持快捷方式。\n请注意, 如果 Mu 中的相关方法 bless, CREATE 没有重载, self 将指向这些方法的类型对象。\n在另一方面, 在初始化的不同阶段, 在实例上调用子方法 BUILD 和 TWEAK。子类中同名的子方法尚未运行, 因此你不应该依赖这些方法中的潜在虚方法调用。\n私有方法 方法名前带有感叹号的方法不能从定义类之外的任何地方调用; 这些方法是私有的, 因为它们在声明它们的类之外是不可见的。使用感叹号而不是点号调用私有方法：\nclass FunMath { has $.value is required; method !do-subtraction( $num ) { if $num ~~ Str { return $!value + (-1 * $num.chars); } return $!value + (-1 * $num); } method minus( $minuend: $subtrahend ) { # invoking the private method on the explicit invocant  $minuend!do-subtraction($subtrahend); } } my $five = FunMath.new(value =\u0026gt; 5); say $five.minus(6); # OUTPUT: «-1␤»  say $five.do-subtraction(6); CATCH { default { put .^name ~ \u0026#34;:\\n\u0026#34; ~ .Str } } # OUTPUT: «X::Method::NotFound:  # No such method \u0026#39;do-subtraction\u0026#39; for invocant of type  # \u0026#39;FunMath\u0026#39;. Did you mean \u0026#39;!do-subtraction\u0026#39;?␤»  私有方法不能被子类继承。\n子方法 子方法是子类不继承的公共方法。该名称源于它们在语义上与子例程类似的事实。\n子方法对于对象构造和销毁任务以及特定于特定类型的任务非常有用, 因此子类型必须重写它们。\n例如, 默认方法 new 在继承链中的每个类上调用 submethod BUILD:\nclass Point2D { has $.x; has $.y; submethod BUILD(:$!x, :$!y) { say \u0026#34;Initializing Point2D\u0026#34;; } } class InvertiblePoint2D is Point2D { submethod BUILD() { say \u0026#34;Initializing InvertiblePoint2D\u0026#34;; } method invert { self.new(x =\u0026gt; - $.x, y =\u0026gt; - $.y); } } say InvertiblePoint2D.new(x =\u0026gt; 1, y =\u0026gt; 2); # OUTPUT: «Initializing Point2D␤»  # OUTPUT: «Initializing InvertiblePoint2D␤»  # OUTPUT: «InvertiblePoint2D.new(x =\u0026gt; 1, y =\u0026gt; 2)␤»  另请参见：Object_construction。\n继承 类可以有父类:\nclass Child is Parent1 is Parent2 { } 如果在子类上调用一个方法, 但是子类没有提供该方法, 则调用其中一个父类中同名方法(如果存在)。父类被查询的顺序就叫做方法解析顺序(MRO)。Raku 使用 C3 方法解析顺序。你可以通过调用类型的元类型方法得知这个类型的 MRO.\n如果一个类没有指定它的父类, 就默认使用 Any. 所有的类都直接或间接的派生于 Mu - 类型层级结构的根。\n所有对公共方法的调用都是 C++ 意义上的“虚拟”, 这意味着对象的实际类型决定了要调用的方法, 而不是声明的类型：\nclass Parent { method frob { say \u0026#34;the parent class frobs\u0026#34; } } class Child is Parent { method frob { say \u0026#34;the child\u0026#39;s somewhat more fancy frob is called\u0026#34; } } my Parent $test; $test = Child.new; $test.frob; # calls the frob method of Child rather than Parent  # OUTPUT: «the child\u0026#39;s somewhat more fancy frob is called␤»  对象构造 对象通常通过方法调用创建, 或者通过类型对象或者通过同类型的其它对象创建。\n类 Mu 提供了一个名为 new 的构造函数方法, 它接受命名参数并使用它们来初始化公共属性。\nclass Point { has $.x; has $.y; } my $p = Point.new( x =\u0026gt; 5, y =\u0026gt; 2); # ^^^ inherited from class Mu say \u0026#34;x: \u0026#34;, $p.x; say \u0026#34;y: \u0026#34;, $p.y; # OUTPUT: «x: 5␤»  # OUTPUT: «y: 2␤»  Mu.new 在调用者身上调用 bless 方法, 传递所有的命名参数。 bless 创建新的对象, 然后以反向方法解析顺序(即从Mu到大多数派生类)遍历所有子类, 并且在每个类中检查是否存在名为 BUILD 的方法。 如果该方法存在, 则使用该方法中的所有命名参数调用该 new 方法。如果不存在名为 BUILD 的方法, 这个类的公开属性就会用同名的命名参数进行初始化。 在任何一种情况下, 如果 BULID 方法和 默认构造函数 都没有对属性进行初始化, 则应用默认值。这意味着 BUILD 可以更改属性, 但它无权访问声明为其默认值的属性的内容; 这些只在 TWEAK（见下文）中可用, 它可以“看到”在类的声明中初始化的属性的内容。\n在 BUILD 被调用之后, 名为 TWEAK 的方法会被调用, 如果它们存在, 传递给 new 的所有命名参数也会传递给 TWEAK。请参阅下面的使用示例。\n由于 BUILD 和 TWEAK 子方法的默认行为, 派生于 Mu 的 new 构造函数的命名参数可直接对应的任何方法解析顺序类的公共属性, 或对应于任何 BUILD 或 TWEAK 子方法的任何命名参数。\n此对象构造方案对自定义构造函数有几个含义。首先, 自定义 BUILD 方法应始终是子方法, 否则它们会破坏子类中的属性初始化。其次, BUILD 子方法可用于在对象构造时运行自定义代码。它们还可用于为属性初始化创建别名：\nclass EncodedBuffer { has $.enc; has $.data; submethod BUILD(:encoding(:$enc), :$data) { $!enc := $enc; $!data := $data; } } my $b1 = EncodedBuffer.new( encoding =\u0026gt; \u0026#39;UTF-8\u0026#39;, data =\u0026gt; [64, 65] ); my $b2 = EncodedBuffer.new( enc =\u0026gt; \u0026#39;UTF-8\u0026#39;, data =\u0026gt; [64, 65] ); # both enc and encoding are allowed now  因为传递实参给子例程把实参绑定给了形参, 如果把属性用作形参, 则不需要单独的绑定步骤。 所以上面的例子可以写为:\nsubmethod BUILD(:encoding(:$!enc), :$!data) { # nothing to do here anymore, the signature binding # does all the work for us. } 但是, 当属性可能具有特殊类型要求（例如 :$!id 必须为正整数）时, 请谨慎使用此属性的自动绑定。请记住, 除非您专门处理此属性, 否则将分配默认值, 并且该默认值将为 Any, 这会导致类型错误。\n第三个含义是, 如果你想要一个接受位置参数的构造函数, 你必须编写自己的 new 方法：\nclass Point { has $.x; has $.y; method new($x, $y) { self.bless(:$x, :$y); } } 然而, 这不是最佳实践, 因为它使得从子类正确地初始化对象变得更难了。\n另外需要注意的是, 名字 new 在 Raku 中并不特别。它只是一个常见的约定, 在大多数Raku类中都非常彻底。你可以从在调用的任何方法调用 bless, 或者使用 CREATE 来摆弄低级别的工作。\nTWEAK 子方法允许你在对象构造后检查或修改属性：\nclass RectangleWithCachedArea { has ($.x1, $.x2, $.y1, $.y2); has $.area; submethod TWEAK() { $!area = abs( ($!x2 - $!x1) * ( $!y2 - $!y1) ); } } say RectangleWithCachedArea.new( x2 =\u0026gt; 5, x1 =\u0026gt; 1, y2 =\u0026gt; 1, y1 =\u0026gt; 0).area; # OUTPUT: «4␤»  对象克隆 克隆是使用所有对象上可用的clone方法完成的, 这些克隆方法可以浅克隆公共和私有属性。公共属性的新值可以作为命名参数提供。\nclass Foo { has $.foo = 42; has $.bar = 100; } my $o1 = Foo.new; my $o2 = $o1.clone: :bar(5000); say $o1; # Foo.new(foo =\u0026gt; 42, bar =\u0026gt; 100)  say $o2; # Foo.new(foo =\u0026gt; 42, bar =\u0026gt; 5000)  有关如何克隆非标量属性的详细信息, 请参阅文档以获取克隆, 以及实现自己的自定义克隆方法的示例。\n角色 角色是属性和方法的集合; 但是, 与类不同, 角色仅用于描述对象行为的一部分; 这就是为什么一般来说, 角色应该在类和对象中混合使用。通常, 类用于管理对象, 而角色用于管理对象内的行为和代码重用。\n角色使用关键字 role 放在所声明的角色名称前面。角色使用 does 关键字 mixed in, does 关键字放在角色名之前。\nconstant ⲧ = \u0026#34;\u0026#34; xx 4; # Just a ⲧab  role Notable { has Str $.notes is rw; multi method notes() { \u0026#34;$!notes\\n\u0026#34; }; multi method notes( Str $note ) { $!notes ~= \u0026#34;$note\\n\u0026#34; ~ ⲧ }; } class Journey does Notable { has $.origin; has $.destination; has @.travelers; method Str { \u0026#34;⤷ $!origin\\n\u0026#34; ~ ⲧ ~ self.notes() ~ \u0026#34;$!destination⤶\\n\u0026#34; }; } my $trip = Journey.new( :origin\u0026lt;Here\u0026gt;, :destination\u0026lt;There\u0026gt;, travelers =\u0026gt; \u0026lt;þor Freya\u0026gt; ); $trip.notes(\u0026#34;First steps\u0026#34;); notes $trip: \u0026#34;Almost there\u0026#34;; print $trip; # OUTPUT:  #⤷ Here  # First steps  # Almost there  #  #There ⤶  一旦编译器解析角色声明的结束大括号, 角色就是不可变的。\nrole Serializable { method serialize() { self.perl; # 很粗超的序列化 } method deserialization-code($buf) { EVAL $buf; # 反转 .perl 操作 } } class Point does Serializable { has $.x; has $.y; } my $p = Point.new(:x(1), :y(2)); my $serialized = $p.serialize; # 由 role 提供的方法 my $clone-of-p = Point.deserialization-code($serialized); say $clone-of-p.x; # 1 编译器一解析到 role 声明的闭合花括号, roles 就不可变了。\n应用角色 角色应用程序与类继承有很大不同。将角色应用于类时, 该角色的方法将复制到类中。如果将多个角色应用于同一个类, 则冲突（例如, 同名的属性或 non-multi 方法）会导致编译时错误, 这可以通过在类中提供同名的方法来解决。\n这比多重继承安全得多, 其中编译器从不检测冲突, 而是解析为方法解析顺序中较早出现的超类, 这可能不是程序员想要的。\n例如, 如果你已经发现了一种有效的方法来骑牛, 并试图把它作为一种新的流行交通形式推销, 你可能会有一个 Bull 类, 对于你在房子周围的所有公牛, 以及一个 Automobile 类, 你可以驾驶的东西。\nclass Bull { has Bool $.castrated = False; method steer { # Turn your bull into a steer  $!castrated = True; return self; } } class Automobile { has $.direction; method steer($!direction) { } } class Taurus is Bull is Automobile { } my $t = Taurus.new; say $t.steer; # OUTPUT: «Taurus.new(castrated =\u0026gt; Bool::True, direction =\u0026gt; Any)␤»  通过这种设置, 您的贫困客户将发现自己无法转动他们的金牛座, 您将无法生产更多的产品！在这种情况下, 使用角色可能更好：\nrole Bull-Like { has Bool $.castrated = False; method steer { # Turn your bull into a steer  $!castrated = True; return self; } } role Steerable { has Real $.direction; method steer(Real $d = 0) { $!direction += $d; } } class Taurus does Bull-Like does Steerable { } 这段代码会死于：\n===SORRY!=== Method 'steer' must be resolved by class Taurus because it exists in multiple roles (Steerable, Bull-Like) 这项检查可以为你省去很多麻烦：\nclass Taurus does Bull-Like does Steerable { method steer($direction?) { self.Steerable::steer($direction) } } 将角色应用于第二个角色时, 实际应用程序将延迟, 直到第二个角色应用于类, 此时两个角色都将应用于该类。从而\nrole R1 { # methods here  } role R2 does R1 { # methods here  } class C does R2 { } 产生相同类 C 为\nrole R1 { # methods here  } role R2 { # methods here  } class C does R1 does R2 { } Stubs 当角色包含 stubbed 方法时, 必须在将角色应用于类时提供同名方法的 non-stubbed 版本。这允许您创建充当抽象接口的角色。\nrole AbstractSerializable { method serialize() { ... } # literal ... here marks the  # method as a stub  } # the following is a compile time error, for example  # Method \u0026#39;serialize\u0026#39; must be implemented by Point because  # it\u0026#39;s required by a role  class APoint does AbstractSerializable { has $.x; has $.y; } # this works:  class SPoint does AbstractSerializable { has $.x; has $.y; method serialize() { \u0026#34;p($.x, $.y)\u0026#34; } } stubbed 方法的实现也可以由另一个角色提供。\n继承 角色不能从类继承, 但它们可以携带类, 导致任何具有该角色的类从承载的类继承。所以, 如果你写：\nrole A is Exception { } class X::Ouch does A { } X::Ouch.^parents.say # OUTPUT: «((Exception))␤» 然后 X::Ouch 将直接从 Exception 继承, 我们可以通过列出其父项来看到。\n由于它们不使用可能称为继承的东西, 因此角色不是类层次结构的一部分。使用 .^roles 元方法列出角色, 它使用 transitive 标记来包含所有级别或仅包含第一个级别。尽管如此, 仍然可以使用智能匹配或类型约束来测试类或实例, 以查看它是否起作用。\nrole F { } class G does F { } G.^roles.say; # OUTPUT: «((F))␤»  role Ur {} role Ar does Ur {} class Whim does Ar {}; Whim.^roles(:!transitive).say; # OUTPUT: «((Ar))␤»  say G ~~ F; # OUTPUT: «True␤»  multi a (F $a) { \u0026#34;F\u0026#34;.say } multi a ($a) { \u0026#34;not F\u0026#34;.say } a(G); # OUTPUT: «F␤»  主从秩序 role M { method f { say \u0026#34;I am in role M\u0026#34; } } class A { method f { say \u0026#34;I am in class A\u0026#34; } } class B is A does M { method f { say \u0026#34;I am in class B\u0026#34; } } class C is A does M { } B.new.f; # OUTPUT «I am in class B␤»  C.new.f; # OUTPUT «I am in role M␤»  请注意, multi-method 的每个候选项都是它自己的方法。在这种情况下, 以上仅适用于两个这样的候选项具有相同签名的情况。否则, 没有冲突, 候选者只是添加到 multi-method 中。\n自动角色双关 任何直接实例化角色或将其用作类型对象的尝试都将自动创建一个与角色同名的类, 从而可以透明地使用角色, 就好像它是一个类一样。\nrole Point { has $.x; has $.y; method abs { sqrt($.x * $.x + $.y * $.y) } method dimensions { 2 } } say Point.new(x =\u0026gt; 6, y =\u0026gt; 8).abs; # OUTPUT «10␤»  say Point.dimensions; # OUTPUT «2␤»  我们把这种自动创建的类叫双关, 并且将生成的类叫双关语。\n但是, Punning 不是由大多数元编程构造引起的, 因为它们有时用于直接使用角色。\n参数化角色 角色可以通过在方括号中给它们签名来参数化：\nrole BinaryTree[::Type] { has BinaryTree[Type] $.left; has BinaryTree[Type] $.right; has Type $.node; method visit-preorder(\u0026amp;cb) { cb $.node; for $.left, $.right -\u0026gt; $branch { $branch.visit-preorder(\u0026amp;cb) if defined $branch; } } method visit-postorder(\u0026amp;cb) { for $.left, $.right -\u0026gt; $branch { $branch.visit-postorder(\u0026amp;cb) if defined $branch; } cb $.node; } method new-from-list(::?CLASS:U: *@el) { my $middle-index = @el.elems div 2; my @left = @el[0 .. $middle-index - 1]; my $middle = @el[$middle-index]; my @right = @el[$middle-index + 1 .. *]; self.new( node =\u0026gt; $middle, left =\u0026gt; @left ?? self.new-from-list(@left) !! self, right =\u0026gt; @right ?? self.new-from-list(@right) !! self, ); } } my $t = BinaryTree[Int].new-from-list(4, 5, 6); $t.visit-preorder(\u0026amp;say); # OUTPUT: «5␤4␤6␤»  $t.visit-postorder(\u0026amp;say); # OUTPUT: «4␤6␤5␤»  这里, 签名只包含类型捕获, 但任何签名都可以：\nenum Severity \u0026lt;debug info warn error critical\u0026gt;; role Logging[$filehandle = $*ERR] { method log(Severity $sev, $message) { $filehandle.print(\u0026#34;[{uc $sev}] $message\\n\u0026#34;); } } Logging[$*OUT].log(debug, \u0026#39;here we go\u0026#39;); # OUTPUT: «[DEBUG] here we go␤»  您可以拥有多个同名角色, 但签名不同; 多重分派的正常规则适用于选择多个候选者。\n混合角色 角色可以混合到对象中。角色的给定属性和方法将添加到对象已有的方法和属性中。支持多个mixin和匿名角色。\nrole R { method Str() {\u0026#39;hidden!\u0026#39;} }; my $i = 2 but R; sub f(\\bound){ put bound }; f($i); # OUTPUT: «hidden!␤»  my @positional := \u0026lt;a b\u0026gt; but R; say @positional.^name; # OUTPUT: «List+{R}␤»  请注意, 对象混合了角色, 而不是对象的类或容器。因此, @-sigiled 容器将需要绑定以使角色坚持, 如示例中的 @positional 所示。某些运算符将返回一个新值, 从而有效地从结果中删除 mixin。这就是为什么使用 does 在变量声明中混合角色可能更为清晰：\nrole R {}; my @positional does R = \u0026lt;a b\u0026gt;; say @positional.^name; # OUTPUT: «Array+{R}␤»  运算符 infix:\u0026lt;but\u0026gt; 比列表构造函数窄。提供要混合的角色列表时, 请始终使用括号。\nrole R1 { method m {} } role R2 { method n {} } my $a = 1 but R1,R2; # R2 is in sink context, issues a WARNING  say $a.^name; # OUTPUT: «Int+{R1}␤»  my $all-roles = 1 but (R1,R2); say $all-roles.^name; # OUTPUT: «Int+{R1,R2}␤»  Mixins 可用于对象生命中的任何一点。\n# A counter for Table of Contents  role TOC-Counter { has Int @!counters is default(0); method Str() { @!counters.join: \u0026#39;.\u0026#39; } method inc($level) { @!counters[$level - 1]++; @!counters.splice($level); self } } my Num $toc-counter = NaN; # don\u0026#39;t do math with Not A Number  say $toc-counter; # OUTPUT: «NaN␤»  $toc-counter does TOC-Counter; # now we mix the role in  $toc-counter.inc(1).inc(2).inc(2).inc(1).inc(2).inc(2).inc(3).inc(3); put $toc-counter / 1; # OUTPUT: «NaN␤» (because that\u0026#39;s numerical context)  put $toc-counter; # OUTPUT: «2.2.2␤» (put will call TOC-Counter::Str)  角色可以是匿名的。\nmy %seen of Int is default(0 but role :: { method Str() {\u0026#39;NULL\u0026#39;} }); say %seen\u0026lt;not-there\u0026gt;; # OUTPUT: «NULL␤»  say %seen\u0026lt;not-there\u0026gt;.defined; # OUTPUT: «True␤» (0 may be False but is well defined)  say Int.new(%seen\u0026lt;not-there\u0026gt;); # OUTPUT: «0␤»  元对象编程和自省 Raku 有一个元对象系统, 这意味着对象、类、角色、Grammars、enums 等行为本身由其它对象控制; 这些对象叫做元对象(想想元操作符, 它操作的对象是普通操作符). 像普通对象一样, 元对象是类的实例, 这时我们称它们为元类。\n对每个对象或类, 你能通过调用 .HOW 方法获取元对象. 注意, 尽管这看起来像是一个方法调用, 但它更像宏。\n所以, 你能用元对象干些什么呢? 你可以通过比较元类的相等性来检查两个对象是否具有同样的元类:\nsay 1.HOW === 2.HOW; # True say 1.HOW === Int.HOW; # True say 1.HOW === Num.HOW; # False Raku 使用单词 HOW, Higher Order Workings, 来引用元对象系统。因此, 在 Rakudo 中不必对此吃惊, 控制类行为的元类的类名叫做 Raku::Metamodel::ClassHow。每个类都有一个 Raku::Metamodel::ClassHOW的实例。\n但是,理所当然的, 元模型为你做了很多。例如它允许你内省对象和类。元对象方法调用的约定是, 在元对象上调用方法, 并且传递感兴趣的对象作为对象的第一参数。所以, 要获取对象的类名, 你可以这样写:\nmy $object = 1; my $metaobject = 1.HOW; say $metaobject.name($object); # Int # or shorter: say 1.HOW.name(1); # Int 为了避免使用同一个对象两次, 有一个便捷写法:\nsay 1.^name; # Int # same as say 1.HOW.name(1); # Int 内省 内省就是在运行时获取对象或类的信息的过程。在 Raku 中, 所有的内省都会搜查原对象。标准的基于类对象的 ClassHow 提供了这些工具:\ncan 给定一个方法名, can 返回可用的方法名\nclass A { method x($a) {} }; class B is A { method x() {} }; say B.^can(\u0026#39;x\u0026#39;).elems; # 2 for B.^can(\u0026#39;x\u0026#39;) { say .arity; # 1, 2 } 在这个例子中, 类 B 中有两个名为 x 的方法可能可用(尽管一个正常的方法调用仅仅会直接调用安置在 B 中那个方法). B 中的那个方法有一个参数(例如, 它期望一个参数, 一个调用者(self)), 而 A 中的 x 方法期望 2 个参数( self 和 $a).\nmethods 返回类中可用公共方法的列表( 这包括父类和 roles 中的方法). 默认它会停在类 Cool, Any 或 Mu 那儿; 若真要获取所有的方法, 使用副词 :all.\nclass A { method x() { }; } say A.^methods(); # x say A.^methods(:all); # x infinite defined ... mro 按方法解析顺序返回类自身的列表和它们的父类. 当方法被调用时, 类和它的父类按那个顺序被访问.(仅仅是概念上; 实际上方法列表在类构建是就创建了).\nsay 1.^mro; # (Int) (Cool) (Any) (Mu) name 返回类的名字:\nsay \u0026#39;a string\u0026#39;.^name; # Str parents 返回一个父类的列表. 默认它会停在 Cool, Any 或者 Mu 那儿, 但你可以提供一个副词 :all来压制它. 使用副词 :tree 会返回一个嵌套列表.\nclass D { }; class C1 is D { }; class C2 is D { }; class B is C1 is C2 { }; class A is B { }; say A.^parents(:all).perl; # (B, C1, C2, D, Any, Mu) say A.^parents(:all, :tree).perl; # ([B, [C1, [D, [Any, [Mu]]]], [C2, [D, [Any, [Mu]]]]],) https://docs.raku.org/language/objects\n","permalink":"https://ohmyweekly.github.io/notes/2015-06-15-object-orientation-in-raku/","tags":["object"],"title":"Raku 中的面向对象"},{"categories":["rakulang"],"contents":"Raku 中的大部分句法结构能归类为项和操作符. 这儿你能找到各种不同类型的项的概览.\nLiterals Int 42 12_300_00 :16\u0026lt;DEAD_BEEF\u0026gt; #十六进制 Int 字面量由数字组成, 并且能在数字之间包含下划线. 使用 :radix\u0026lt;number\u0026gt; 冒号对儿形式能指定 10 进制外的其它进制.\nRat 有理数 12.34 1_200.345_678 Rat(有理数)字面量由一个点号分割的两部分整数组成. 注意尾部的点号是不允许的, 所以你必须写成 1.0 而非 1. ( 这个规则很重要, 因为有一个以点号开头的中缀操作符, 例如 .. 范围操作符 ).\nNum 浮点数 12.3e-32 3e8 Num(浮点数)字面量由 Rat 或 Int 字面量后面再跟着一个字母 e 和 一个指数(可能为负)组成. 3e8 使用 值 3* 10**8 构建了一个 Num.\nStr \u0026#39;a string\u0026#39;\u0026#39;I\\\u0026#39;m escaped!\u0026#39; \u0026#34;I don\u0026#39;t need to be\u0026#34; \u0026#34;\\\u0026#34;But I still can be,\\\u0026#34; he said.\u0026#34; q|Other delimiters can be used too! 字符串字面量常常使用 ' 或 \u0026quot; 创建, 然儿, 字符串在 Raku 中其实是一种强大的子语言.\nRegex /match some text / rx/slurp \\srest (.*)$/ 这两种会产生字面正则。\nPair a =\u0026gt; 1 \u0026#39;a\u0026#39; =\u0026gt; \u0026#39;b\u0026#39; :identifier :!identifier :identifier\u0026lt;value\u0026gt; :identifier\u0026lt;value1 value2\u0026gt; :identifier($value) :identifier[\u0026#39;val1\u0026#39;, \u0026#39;val2\u0026#39;] :identifier{key1 =\u0026gt; \u0026#39;val1\u0026#39;, key2 =\u0026gt; \u0026#39;value2\u0026#39;} :$item :@array :%hash :\u0026amp;callable Pair 对象的创建要么使用 infix:«=\u0026gt;» (它会自动括起左边, 如果左边是标识符的话), 要么使用各种冒号对儿形式. 那些总是以一个冒号开头的创建形式, 冒号后面要么跟着一个标识符, 要么跟着一个已经存在的变量(不带符号的变量名作为 pair 的键, 变量的值作为 pair 的键值).\n在标识符形式的冒号对儿中, 可选的值可以是任意环缀. 如果没有环缀, 那它的值就是 Bool::True. !:identifier 形式的值是 Bool::False.\n如果冒号对儿在参数列表中, 所有的冒号对儿都会作为命名参数, 但是 'quoted string' =\u0026gt; $value 除外.\nterm * * 会创建一个类型为 Whatever 的对象. 详情查看 Whatever.\nIdentifier terms Raku中有内建的标识符项, 列出如下. 此外, 使用该语法能添加新的标识符项.\nsub term:\u0026lt;fourty-two\u0026gt; { 42 }; say fourty-two 或者作为常量:\nconstant forty-two = 42; say fourty-two self 在方法中, self 指向方法的调用者( 例如, 方法被调用的对象). 如果把它用在没有意义的上下文中, 会抛出一个  X::Syntax::NoSelf 类型的编译时错误.\nnow 返回一个代表当前时间的实例对象.\nrand 返回一个范围为 0..^1 的伪随机浮点数.\npi 返回数值 pi, 例如, 圆的周长和半径之间的比率.\ne 返回欧拉数值.\ni 返回复数的虚部.\n","permalink":"https://ohmyweekly.github.io/notes/2015-11-21-terms-in-raku/","tags":["term"],"title":"Raku 中的项"},{"categories":["rakulang"],"contents":"第八章 函数式编程 在本章中，我们将看看一些有利于函数式编程的功能。\n函数是一等公民 函数/子例程是一等公民:\n 它们能作为参数传递 它们能从另外一个函数中返回 它们能被赋值给变量  map 函数是用来说明这个概念的极好例子。map 是高阶函数, 它接收另外一个函数作为参数。\n脚本\nmy @array = \u0026lt;1 2 3 4 5\u0026gt;; sub squared($x) { $x ** 2 } say map(\u0026amp;squared, @array); 输出\n(1 4 9 16 25) 解释\n我们定义了一个叫做 squared 的子例程, 它接收一个数字并返回该数字的二次幂。下一步, 我们使用 map 这个高阶函数并传递给它两个参数, 一个子例程和一个数组。结果是所有数组元素的平方组成的列表。\n注意当传递子例程作为参数时, 我们需要在子例程的名字前添加一个 \u0026amp; 符号。\n闭包 　在 Raku 中所有的代码对象都是闭包, 这意味着它们能从外部作用域(outer scope)引用词法变量(lexical variables)。\n匿名函数 　匿名函数也叫做 lambda。\n匿名函数没有绑定到标识符(匿名函数没有名字)。\n让我们使用匿名函数重写 map 那个例子。\nmy @array = \u0026lt;1 2 3 4 5\u0026gt;; say map(-\u0026gt; $x {$x ** 2}, @array); 我们没有声明子例程并把它作为参数传递给 map, 而是在里面直接定义了匿名函数。\n匿名函数 -\u0026gt; $x {$x ** 2} 没有句柄并且不能被调用。\n按照 Raku 的说法我们把这个标记叫做 pointy block。\npointy block 也能用于把函数赋值给变量:\nmy $squared = -\u0026gt; $x { $x ** 2 } say $squared(9); 链式调用 　在 Raku 中, 方法可以链接起来, 你不再需要把一个方法的结果作为参数传递给另外一个方法了。\n我们假设你有一个数组。你被要求返回该数组的唯一值, 并且按从大到小的顺序排序。\n你可能会通过写出近似于这样的代码来解决那个问题:\nmy @array = \u0026lt;7 8 9 0 1 2 4 3 5 6 7 8 9 \u0026gt;; my @final-array = reverse(sort(unique(@array))); say @final-array; 首先我们在 @array 身上调用 unique 函数, 然后我们把它的结果作为参数传递给 sort 函数, 再然后我们把结果传递给 reverse 函数。\n和上面的例子相比, Raku 允许链式方法。\n上面的例子可以像下面这样写, 利用方法链的优点:\nmy @array = \u0026lt;7 8 9 0 1 2 4 3 5 6 7 8 9 \u0026gt;; my @final-array = @array.unique.sort.reverse; say @final-array; 你已经看到链式方法看起来有多清爽啦。\nFeed 操作符 　feed 操作符, 在有些函数式编程语言中也叫管道, 然而它是链式方法的一个更好的可视化产出。\n向前流\nmy @array = \u0026lt;7 8 9 0 1 2 4 3 5 6\u0026gt;; @array ==\u0026gt; unique() ==\u0026gt; sort() ==\u0026gt; reverse() ==\u0026gt; my @final-array; say @final-array; 解释\n从 @array 开始, 然后返回一个唯一元素的列表 然后 排序它 然后 反转它 然后 把结果保存到 @final-array 中 就像你看到的那样, 方法的流向是自上而下的。\n向后流\nmy @array = \u0026lt;7 8 9 0 1 2 4 3 5 6\u0026gt;; my @final-array-v2 \u0026lt;== reverse() \u0026lt;== sort() \u0026lt;== unique() \u0026lt;== @array; say @final-array-v2; 解释\n向后流就像向前流一样, 但是是以反转的顺序写的。\n方法的流动方向是自下而上。\nHyper 操作符 　hyper 操作符 ». 会在列表的所有元素身上调用一个方法并返回所有结果的一个列表。\nmy @array = \u0026lt;0 1 2 3 4 5 6 7 8 9 10\u0026gt;; sub is-even($var) { $var %% 2 }; say @array».is-prime; say @array».\u0026amp;is-even; 使用 hyper 操作符我们能调用 Raku 中已经定义过的方法, 例如 is-prime 告诉我们一个数字是否是质数。\n此外我们能定义新的子例程并使用 hyper 操作符调用它们。但是这时我们必须在方法的名字前面加上 \u0026amp; 符号。例如 \u0026amp;is-even。\n这很实用因为它使我们不必写 for 循环就可以迭代每个值。\nJunctions 　junction 是值的逻辑叠加。\n在下面的例子中 1|2|3 是一个 junction。\nmy $var = 2; if $var == 1|2|3 { say \u0026#34;The variable is 1 or 2 or 3\u0026#34; } junctions 的使用常常触发自动线程化; 每个 junction 元素都执行该操作, 并且所有的结果被组合到一个新的 junction 中并返回。\nLazy Lists 　惰性列表是被惰性求值的列表。\n惰性求值延迟表达式的计算直到需要时, 并把结果存储到查询表中以避免重复计算。\n惰性列表的优点包括:\n 通过避免不必要的计算带来的性能提升 构建潜在的无限数据结构的能力 定义控制流的能力  我们使用中缀操作符 ... 来创建惰性列表。\n惰性列表拥有一个初始元素, 一个发生器和一个结束点。\n简单的惰性列表\nmy $lazylist = (1 ... 10); say $lazylist; 初始元素为 1 而结束点为 10。因为没有定义发生器所以默认的发生器为 successor(+1)。换句话说, 这个惰性列表可能返回(如果需要的话)下面的元素 (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)。\n无限惰性列表\nmy $lazylist = (1 ... Inf); say $lazylist; 该列表可能返回(如果需要的话) 1 到无穷大之间的任何整数, 换句话说, 可以返回任何整数。\n使用推断发生器创建惰性列表\nmy $lazylist = (0,2 ... 10); say $lazylist; 初始的元素是 0 和 2 而结束点是 10。虽然没有定义发生器, 但是使用了初始元素, Raku 会把生成器推断为 (+2)。\n这个惰性列表可能返回(如果需要的话)下面的元素 (0, 2, 4, 6, 8, 10)。\n使用定义的发生器创建惰性列表\nmy $lazylist = (0, { $_ + 3 } ... 12); say $lazylist; 在这个例子中, 我们在闭合 { } 中显式地定义了一个发生器。\n这个惰性列表可能返回(如果需要的话)下面的元素 (0, 3, 6, 9, 12)。\n 当使用显式的发生器时, 结束点必须是发生器能返回的一个值。\n如果在上面的例子中我们使用的结束点是 10 而非 12, 那么发生器就不会停止。发生器会跳过那个结束点。\n二选一, 你可以使用 0 ...^ * \u0026gt; 10 代替 0 ... 10。你可以把它读作: 从 0 直到第一个大于 10(不包括它)的值\n这不会使发生器停止\nmy $lazylist = (0, { $_ + 3 } ... 10); say $lazylist; 这会使发生器停止\nmy $lazylist = (0, { $_ + 3 } ...^ * \u0026gt; 10); say $lazylist;  第九章 类和对象 在上一章中我们学习了 Raku 中函数式编程的便利性。在这一章中我们将看看 Raku 中的面向对象编程。\n介绍 　面向对象编程是当今广泛使用的范式之一。对象是一组绑定在一起的变量和子例程。\n其中的变量叫做属性, 而子例程被叫做方法。属性定义对象的状态, 而方法定义对象的行为。\n类定义一组对象结构。\n为了理解它们之间的关系, 考虑下面的例子:\n   房间里有 4 个 people objects =\u0026gt; 4 people     这 4 个人是 humans class =\u0026gt; Human   它们有不同的名字,年纪,性别和国籍 attribute =\u0026gt; name,age,sex,nationality    按面向对象的说法, 对象是类的实例。\n考虑下面的脚本:\nclass Human { has $name; has $age; has $sex; has $nationality; } my $john = Human.new(name =\u0026gt; \u0026#39;John\u0026#39;, age =\u0026gt; 23, sex =\u0026gt; \u0026#39;M\u0026#39; nationality =\u0026gt; \u0026#39;American\u0026#39;) say $john;  class 关键字用于定义类。 has 关键字用于定义类的属性。 .new 方法被称之为构造函数。它创建了对象作为类的实例。  在上面的例子中, 新的变量 $john 保存了由 Human.new() 所定义的新 \u0026ldquo;Human\u0026rdquo; 实例。 传递给 .new() 方法的参数用于设置底层对象的属性。 类可以使用 my 来声明一个本地作用域:\nmy class Human { } 封装 　封装是一个面向对象的概念, 它把一组数据和方法捆绑在一块。 对象中的数据(属性)应该是私有的, 换句话说, 只能从对象内部访问它。 为了从对象外部访问对象的属性, 我们使用叫做存取器的方法。 下面两个脚本拥有同样的结果。\n直接访问变量:\nmy $var = 7; say $var; 封装\nmy $var = 7; sub sayvar { $var; } say sayvar; sayvar 是一个存取器。它让我们通过不直接访问这个变量来访问这个变量。 在 Raku 中使用 twigils 使得封装很便利。 Twigils 是第二 符号。它们存在于符号和属性名之间。 有两个 twigils 用在类中:\n ! 用于显式地声明属性是私有的 . 用于为属性自动生成存取器  默认地, 所有的属性都是私有的, 但是总是用 ! twigil 是一个好习惯。 为了和我说的相一致, 我们应该把上面的类重写成下面这样:\nclass Human { has $!name; has $!age; has $!sex; has $!nationality; } my $john = Human.new(name =\u0026gt; \u0026#39;John\u0026#39;, age =\u0026gt; 23, sex =\u0026gt; \u0026#39;M\u0026#39;, nationality =\u0026gt; \u0026#39;American\u0026#39;); say $john; 给脚本追加这样的的语句: say $john.age; 它会返回这样的错误: Method 'age' not found for invocant of class 'Human'。 原因是 $!age 是私有的并且只能用于对象内部。 尝试在对象外部访问它会返回一个错误。 现在用 has $.age 代替 $!age 并看看 say $john.age; 的结果是什么。\n具名参数 vs. 位置参数 　在 Raku 中, 所有的类继承了一个默认的 .new 构造函数。 通过为他提供参数, 它能用于创建对象。 只能提供具名参数给默认的构造函数。\n如果你考虑到上面的例子, 你会看到所有提供给 .new 方法的参数都是按名字定义的:\n name =\u0026gt; \u0026lsquo;John\u0026rsquo; age =\u0026gt; 23  假如我不想在每次创建新对象的时候为每个属性提供一个名字呢? 那么我需要创建另外一个接收位置参数的构造函数。\nclass Human { has $.name; has $.age; has $.sex; has $.nationality; # 重写默认构造函数的新构造函数 method new ($name, $age, $sex, $nationality) { self.bless(:$name, :$age, :$sex, :$nationality); } } my $john = Human.new(\u0026#39;John\u0026#39;, 23, \u0026#39;M\u0026#39;, \u0026#39;American\u0026#39;); say $john; 能接收位置参数的构造函数需要按上面那样定义。\n方法 　介绍 　方法是对象的子例程。 像子例程一样, 方法是一种打包一组功能的手段, 它们接收参数, 拥有签名并可以被定义为 multi。 方法是使用关键字 method 来定义的。\n正常情况下, 方法被要求在对象的属性身上执行一些动作。这强制了封装的概念。对象的属性只能在对象里面使用方法来操作。在对象外面, 只能和对象的方法交互, 并且不能访问它的属性。\nclass Human { has $.name; has $.age; has $.sex; has $.nationality; has $.eligible; method assess-eligibility { if self.age \u0026lt; 21 { $!eligible = \u0026#39;No\u0026#39; } else { $!eligible = \u0026#39;Yes\u0026#39; } } } my $john = Human.new(name =\u0026gt; \u0026#39;John\u0026#39;, age =\u0026gt; 23, sex =\u0026gt; \u0026#39;M\u0026#39;, nationality =\u0026gt; \u0026#39;American\u0026#39;); $john.assess-eligibility; say $john.eligible; 一旦方法定义在类中, 它们就能在对象身上使用点记号来调用: object.method 或像上面的例子那样: $john.assess-eligibility。 在方法的定义中, 如果我们需要引用对象本身以调用另一个方法, 则使用 self 关键字。 在方法的定义中, 如果我们需要引用属性, 则使用 !, 即使属性是使用 . 定义的。 理由是 . twigil 做的就是使用 ! 声明一个属性并自动创建存取器。\n在上面的例子中, if self.age \u0026lt; 21 和 if $!age \u0026lt; 21 会有同样的效果, 尽管它们从技术上来讲是不同的:\n self.age 调用了 .age 方法(存取器)  二选一, 还能写成 $.age\n $!age 是直接调用那个变量  私有方法 　正常的方法能从类的外面在对象身上调用。 私有方法是只能从类的内部调用的方法。\n一个可能的使用情况是一个方法调用另外一个执行特定动作的方法。连接外部世界的方法是公共的而被引用的那个方法应该保持私有。我们不想让用户直接调用它, 所以我们把它声明为私有的。\n私有方法的声明需要在方法的名字前使用 ! twigil。 私有方法是使用 ! 而非 . 调用的。\nmethod !iamprivate { # code goes in here } method iampublic { self!iamprivate; # do additional things } 类属性 　类属性是属于类自身而非类的对象的属性。 它们能在定义期间初始化。 类属性是使用 my 关键字而非 has 关键字声明的。 它们是在类自己身上而非它的对象身上调用的。\nclass Human { has $.name; my $.counter = 0; method new($name) { Human.counter++; self.bless(:$name); } } my $a = Human.new(\u0026#39;a\u0026#39;); my $b = Human.new(\u0026#39;b\u0026#39;); say Human.counter; 访问类型 　到现在为止我们看到的所以例子都使用存取器来从对象属性中获取信息。 假如我们需要修改属性的值呢? 我们需要使用下面的 is rw 关键字把它标记为 read/write。\nclass Human { has $.name; has $.age is rw; } my $john = Human.new(name =\u0026gt; \u0026#39;John\u0026#39;, age =\u0026gt; 21); say $john.age; $john.age = 23; say $john.age; 默认地, 所有属性都声明为只读, 但是你可以显式地使用 is readonly 来声明。\n继承 　介绍 　继承是面向对象编程的另一个概念。 当定义类的时候, 很快我们会意思到很多属性/方法在很多类中是共有的。 我们应该重复代码吗?\n不! 我们应该使用继承。 假设我们想定义两个类, 一个类是 Human, 一个类是 Employees。 Human 拥有两个属性: name 和 age。\nEmployees 拥有 4 个属性: name, age, company 和 salary。 尝试按下面定义类:\nclass Human { has $.name; has $.age; } class Employee { has $.name; has $.age; has $.company; has $.salary; } 虽然上面的代码技术上是正确的, 但是概念上差。 更好的写法是下面这样:\nclass Human { has $.name; has $.age; } class Employee is Human { has $.company; has $.salary; } is 关键字定义了继承。 按面向对象的说法, Employee 是 Human 的孩子, 而 Human 是 Employee 的父亲。\n所有的子类继承了父类的属性和方法, 所以没有必要重新它们。\n重写 　类从它们的父类中继承所有的属性和方法。 有些情况下, 我们需要让子类中的方法表现得和继承的方法不一样。 为了做到这, 我们在子类中重新定义方法。\n这个概念就叫做重写。\n在下面的例子中, introduce-yourself 方法被 Employee 类继承。\nclass Human { has $.name; has $.age; method introduce-yourself { say \u0026#39;Hi 我是人类, 我的名字是 \u0026#39; ~ self.name; } } class Employee is Human { has $.company; has $.salary; } my $john = Human.new(name =\u0026gt; \u0026#39;John\u0026#39;, age =\u0026gt; 23,); my $jane = Employee.new(name =\u0026gt; \u0026#39;Jane\u0026#39;, age =\u0026gt; 25, company =\u0026gt; \u0026#39;Acme\u0026#39;, salary =\u0026gt; 4000); $john.introduce-yourself; $jane.introduce-yourself; 重写工作如下:\nclass Human { has $.name; has $.age; method introduce-yourself { say \u0026#39;Hi 我是人类, 我的名字是 \u0026#39; ~ self.name; } } class Employee is Human { has $.company; has $.salary; method introduce-yourself { say \u0026#39;Hi 我是一名员工, 我的名字是 \u0026#39; ~ self.name ~ \u0026#39;我工作在: \u0026#39; ~ self.comapny; } } my $john = Human.new(name =\u0026gt;\u0026#39;John\u0026#39;,age =\u0026gt; 23,); my $jane = Employee.new(name =\u0026gt;\u0026#39;Jane\u0026#39;,age =\u0026gt; 25,company =\u0026gt; \u0026#39;Acme\u0026#39;,salary =\u0026gt; 4000); $john.introduce-yourself; $jane.introduce-yourself; 根据对象所属的类, 会调用正确的方法。\nSubmethods 　Submethods 是一种子类继承不到的方法。\n它们只能从所声明的类中访问。 它们使用 submethod 关键字定义。\n多重继承 　在 Raku 中允许多重继承。一个类可以继承自多个其它的类。\nclass bar-chart { has Int @.bar-values; method plot { say @.bar-values; } } class line-chart { has Int @.line-values; method plot { say @.line-values; } } class combo-chart is bar-chart is line-chart { } my $actual-sales = bar-chart.new(bar-values =\u0026gt; [10,9,11,8,7,10]); my $forecast-sales = line-chart.new(line-values =\u0026gt; [9,8,10,7,6,9]); my $actual-vs-forecast = combo-chart.new(bar-values =\u0026gt; [10,9,11,8,7,10], line-values =\u0026gt; [9,8,10,7,6,9]); say \u0026#34;实际的销售: \u0026#34;; $actual-sales.plot; say \u0026#34;预测的销售: \u0026#34;; $forecast-sales.plot; say \u0026#34;实际 vs 预测:\u0026#34;; $actual-vs-forecast.plot; 输出:\n实际的销售: [10 9 11 8 7 10] 预测的销售: [9 8 10 7 6 9] 实际 vs 预测: [10 9 11 8 7 10] 解释\ncombo-chart 类应该能持有两个序列, 一个是绘制条形图的实际值, 另一个是绘制折线图的预测值。\n这就是我们为什么把它定义为 line-chart 和 bar-chart 的孩子的原因。\n你应该注意到了, 在 combo-chart 身上调用 plot 方法并没有产生所要求的结果。它只绘制了一个序列。\n发生了什么事?\ncombo-chart 继承自 line-chart 和 bar-chart, 它们都有一个叫做 plot 的方法。当我们在 combo-chart 身上调用那个方法时, Raku 内部会尝试通过调用其所继承的方法之一来解决冲突。\n纠正\n为了表现得正确, 我们应该在 combo-chart 中重写 plot 方法。\nclass bar-chart { has Int @.bar-values; method plot { say @.bar-values; } } class line-chart { has Int @.line-values; method plot { say @.line-values; } } class combo-chart is bar-chart is line-chart { method plot { say @.bar-values; say @.line-values; } } my $actual-sales = bar-chart.new(bar-values =\u0026gt; [10,9,11,8,7,10]); my $forecast-sales = line-chart.new(line-values =\u0026gt; [9,8,10,7,6,9]); my $actual-vs-forecast = combo-chart.new(bar-values =\u0026gt; [10,9,11,8,7,10], line-values =\u0026gt; [9,8,10,7,6,9]); say \u0026#34;实际的销售: \u0026#34;; $actual-sales.plot; say \u0026#34;预测的销售: \u0026#34;; $forecast-sales.plot; say \u0026#34;实际 vs 预测:\u0026#34;; $actual-vs-forecast.plot; 输出:\n实际的销售: [10 9 11 8 7 10] 预测的销售: [9 8 10 7 6 9] 实际 vs 预测: [10 9 11 8 7 10] [9 8 10 7 6 9] Roles 　Roles 在它们是属性和方法的集合这个意义上和类有点类似。 Roles 使用关键字 role 声明, 而想实现该 role 的类可以使用 does 关键字。\n使用 roles 重写多重继承的例子\nrole bar-chart { has Int @.bar-values; method plot { say @.bar-values; } } role line-chart { has Int @.line-values; method plot { say @.line-values; } } class combo-chart does bar-chart does line-chart { method plot { say @.bar-values; say @.line-values; } } my $actual-sales = bar-chart.new(bar-values =\u0026gt; [10,9,11,8,7,10]); my $forecast-sales = line-chart.new(line-values =\u0026gt; [9,8,10,7,6,9]); my $actual-vs-forecast = combo-chart.new(bar-values =\u0026gt; [10,9,11,8,7,10], line-values =\u0026gt; [9,8,10,7,6,9]); say \u0026#34;实际的销售: \u0026#34;; $actual-sales.plot; say \u0026#34;预测的销售: \u0026#34;; $forecast-sales.plot; say \u0026#34;实际 vs 预测:\u0026#34;; $actual-vs-forecast.plot; 运行上面的脚本你会看到结果是一样的。\n现在你问问自己, 如果 roles 表现得像类的话那么它们的用途是什么呢?\n要回答你的问题, 修改第一个用于展示多重继承的脚本, 这个脚本中我们忘记重写 plot 方法了。\nrole bar-chart { has Int @.bar-values; method plot { say @.bar-values; } } role line-chart { has Int @.line-values; method plot { say @.line-values; } } class combo-chart does bar-chart does line-chart { } my $actual-sales = bar-chart.new(bar-values =\u0026gt; [10,9,11,8,7,10]); my $forecast-sales = line-chart.new(line-values =\u0026gt; [9,8,10,7,6,9]); my $actual-vs-forecast = combo-chart.new(bar-values =\u0026gt; [10,9,11,8,7,10], line-values =\u0026gt; [9,8,10,7,6,9]); say \u0026#34;Actual sales:\u0026#34;; $actual-sales.plot; say \u0026#34;Forecast sales:\u0026#34;; $forecast-sales.plot; say \u0026#34;Actual vs Forecast:\u0026#34;; $actual-vs-forecast.plot; 输出:\n===SORRY!=== Method 'plot' must be resolved by class combo-chart because it exists in multiple roles (line-chart, bar-chart) 解释\n如果多个 roles 被应用到同一个类中, 会出现冲突并抛出一个编译时错误。 这是比多重继承更安全的方法, 其中冲突不被认为是错误并且简单地在运行时解决。 Roles 会提醒你有冲突。\n内省 　内省是获取诸如对象的类型、属性或方法等对象属性的信息的过程。\nclass Human { has Str $.name; has Int $.age; method introduce-yourself { say \u0026#39;Hi i am a human being, my name is \u0026#39; ~ self.name; } } class Employee is Human { has Str $.company; has Int $.salary; method introduce-yourself { say \u0026#39;Hi i am a employee, my name is \u0026#39; ~ self.name ~ \u0026#39;and I work at: \u0026#39; ~ self.company; } } my $john = Human.new(name =\u0026gt;\u0026#39;John\u0026#39;,age =\u0026gt; 23,); my $jane = Employee.new(name =\u0026gt;\u0026#39;Jane\u0026#39;,age =\u0026gt; 25,company =\u0026gt; \u0026#39;Acme\u0026#39;,salary =\u0026gt; 4000); say $john.WHAT; say $jane.WHAT; say $john.^attributes; say $jane.^attributes; say $john.^methods; say $jane.^methods; say $jane.^parents; if $jane ~~ Human {say \u0026#39;Jane is a Human\u0026#39;}; 内省使用了:\n .WHAT 返回已经创建的对象所属的类。 .^attributes 返回一个包含该对象所有属性的列表。 .^mtethods 返回能在该对象身上调用的所有方法。 .^parents 返回该对象所属类的所有父类。 ~~ 叫做智能匹配操作符。如果对象是从它所进行比较的类或任何它继承的类创建的, 则计算为 True。  第十章 异常处理 捕获异常 　异常是当某些东西出错时发生在运行时的特殊行为。\n我们说异常被抛出。考虑下面这个运行正确的脚本:\nmy Str $name; $name = \u0026#34;Joanna\u0026#34;; say \u0026#34;Hello \u0026#34; ~ $name; say \u0026#34;How are you doing today?\u0026#34; 输出:\nHello Joanna How are you doing today? 现在让这个脚本抛出异常:\nmy Str $name; $name = 123; say \u0026#34;Hello \u0026#34; ~ $name; say \u0026#34;How are you doing today?\u0026#34; 输出:\nType check failed in assignment to $name; expected Str but got Int in block \u0026lt;unit\u0026gt; at exceptions.pl6:2 你应该看到当错误出现时(在这个例子中把数组赋值给字符串变量)程序会停止并且其它行的代码不会被执行, 即使它们是正确的。\n异常处理是捕获已经抛出的异常的过程以使脚本能继续工作。\nmy Str $name; try { $name = 123; say \u0026#34;Hello \u0026#34; ~ $name; CATCH { default { say \u0026#34;Can you tell us your name again, we couldn\u0026#39;t find it in the register.\u0026#34;; } } } say \u0026#34;How are you doing today?\u0026#34;; 输出:\nCan you tell us your name again, we couldn't find it in the register. How are you doing today? 异常处理是使用 try-catch block 完成的。\ntry { # code goes in here # 如果有东西出错, 脚本会进入到下面的 CATCH block 中 # 如果什么错误也没有, 那么 CATCH block 会被忽略 CATCH { default { # 只有抛出异常时, 这儿的代码才会被求值 } } } CATCH block 能像定义 given block 那样定义。这意味着我们能捕获并处理各种不同类型的异常。\ntry { #code goes in here #if anything goes wrong, the script will enter the below CATCH block #if nothing goes wrong the CATCH block will be ignored CATCH { when X::AdHoc { #do something if an exception of type X::AdHoc is thrown } when X::IO { #do something if an exception of type X::IO is thrown } when X::OS { #do something if an exception of type X::OS is thrown } default { #do something if an exception is thrown and doesn\u0026#39;t belong to the above types } } } 抛出异常 　和捕获异常相比, Raku 也允许你显式地抛出异常。 有两种类型的异常可以抛出:\n ad-hoc 异常 类型异常  ad-hoc\nmy Int $age = 21; die \u0026#34;Error !\u0026#34;; typed\nmy Int $age = 21; X::AdHoc.new(payload =\u0026gt; \u0026#39;Error !\u0026#39;).throw; 使用 die 子例程后面跟着异常消息来抛出 Ad-hoc 异常。\nTyped 异常是对象, 因此上面的例子中使用了 .new() 构造函数。\n所有类型化的异常都是从类 X 开始, 下面是一些例子:\n X::AdHoc 是最简单的异常类型 X::IO 跟 IO 错误有关。 X::OS 跟 OS 错误有关。 X::Str::Numeric 跟把字符串强制转换为数字有关。   查看异常类型和相关方法的完整列表请到 http://doc.raku.org/type.html 并导航到以 X 开头的类型。\n 第十一章 正则表达式 正则表达式, 或 regex 是一个用于模式匹配的字符序列。\n理解它最简单的一种方式是把它看作模式。\nif \u0026#39;enlightenment\u0026#39; ~~ m/light / { say \u0026#34;enlightenment contains the word light\u0026#34;; } 在这个例子中, 智能匹配操作符 ~~ 用于检查一个字符串(enlightenment)是否包含一个单词(light)。\n\u0026ldquo;Enlightenment\u0026rdquo; 与正则表达式 m/ light / 匹配。\nRegex 定义 　正则表达式可以按如下方式定义:\n /light/ m/light/ rx/light/  除非显式地指定, 否则空白是无关紧要的, m/light/ 和 m/ light / 是相同的。\n匹配字符 　字母数字字符和下划线 _ 在正则表达式中是按原样写出的。\n所有其它字符必须使用反斜线或用引号围起来以转义。\n反斜线\nif \u0026#39;Temperature: 13\u0026#39; ~~ m/\\:/ { say \u0026#34;The string provided contains a colon :\u0026#34;; } 单引号\nif \u0026#39;Age = 13\u0026#39; ~~ m/\u0026#39;=\u0026#39;/ { say \u0026#34;The string provided contains an equal character = \u0026#34;; } 双引号\nif \u0026#39;name@company.com\u0026#39; ~~ m/\u0026#34;@\u0026#34;/ { say \u0026#34;This is a valid email address because it contains an @ character\u0026#34;; } 匹配字符类 　就像之前章节看到的, 匹配字符类很方便。\n话虽这么说，更系统的方法是使用 Unicode 属性。\nUnicode 属性闭合在 \u0026lt;: \u0026gt; 中。\nif \u0026#34;John123\u0026#34; ~~ /\u0026lt;:N\u0026gt;/ { say \u0026#34;Contains a number\u0026#34;; } else { say \u0026#34;Doesn\u0026#39;t contain a number\u0026#34; } if \u0026#34;John-Doe\u0026#34; ~~ /\u0026lt;:Lu\u0026gt;/ { say \u0026#34;Contains an uppercase letter\u0026#34;; } else { say \u0026#34;Doesn\u0026#39;t contain an upper case letter\u0026#34; } if \u0026#34;John-Doe\u0026#34; ~~ /\u0026lt;:Pd\u0026gt;/ { say \u0026#34;Contains a dash\u0026#34;; } else { say \u0026#34;Doesn\u0026#39;t contain a dash\u0026#34; } 通配符 通配符也可以用在正则表达式中。\n点 . 意味着任何单个字符。\nif \u0026#39;abc\u0026#39; ~~ m/a.c / { say \u0026#34;Match\u0026#34;; } if \u0026#39;a2c\u0026#39; ~~ m/a.c / { say \u0026#34;Match\u0026#34;; } if \u0026#39;ac\u0026#39; ~~ m/a.c / { say \u0026#34;Match\u0026#34;; } else { say \u0026#34;No Match\u0026#34;; } 量词 　量词在字符后面用于指定我们期望匹配它前面的东西的次数。\n问号 ? 意思是 0 或 1 次。\nif \u0026#39;ac\u0026#39; ~~ m/a?c / { say \u0026#34;Match\u0026#34;; } else { say \u0026#34;No Match\u0026#34;; } if \u0026#39;c\u0026#39; ~~ m/a?c / { say \u0026#34;Match\u0026#34;; } else { say \u0026#34;No Match\u0026#34;; } 星号 * 意思是 0 或多次。\nif \u0026#39;az\u0026#39; ~~ m/a*z / { say \u0026#34;Match\u0026#34;; } else { say \u0026#34;No Match\u0026#34;; } if \u0026#39;aaz\u0026#39; ~~ m/a*z / { say \u0026#34;Match\u0026#34;; } else { say \u0026#34;No Match\u0026#34;; } if \u0026#39;aaaaaaaaaaz\u0026#39; ~~ m/a*z / { say \u0026#34;Match\u0026#34;; }else { say \u0026#34;No Match\u0026#34;; }if \u0026#39;z\u0026#39;~~m/ a*z / { say \u0026#34;Match\u0026#34;; }else { say \u0026#34;No Match\u0026#34;; }+ 意思是至少匹配 1 次。\nif \u0026#39;az\u0026#39; ~~ m/a+z / { say \u0026#34;Match\u0026#34;; } else { say \u0026#34;No Match\u0026#34;; } if \u0026#39;aaz\u0026#39; ~~ m/a+z / { say \u0026#34;Match\u0026#34;; } else { say \u0026#34;No Match\u0026#34;; } if \u0026#39;aaaaaaaaaaz\u0026#39; ~~ m/a+z / { say \u0026#34;Match\u0026#34;; } else { say \u0026#34;No Match\u0026#34;; } if \u0026#39;z\u0026#39; ~~ m/a+z / { say \u0026#34;Match\u0026#34;; }else { say \u0026#34;No Match\u0026#34;; }匹配结果 　当匹配字符串的正则表达式成功时, 匹配结果被存储在一个特殊的变量 $/ 中。\n脚本\nif \u0026#39;Rakudo is a Raku compiler\u0026#39; ~~ m/:sRaku/ { say \u0026#34;The match is: \u0026#34; ~ $/; say \u0026#34;The string before the match is: \u0026#34; ~ $/.prematch; say \u0026#34;The string after the match is: \u0026#34; ~ $/.postmatch; say \u0026#34;The matching string starts at position: \u0026#34; ~ $/.from; say \u0026#34;The matching string ends at position: \u0026#34; ~ $/.to; } 输出\nThe match is: Raku The string before the match is: Rakudo is a The string after the match is: compiler The matching string starts at position: 12 The matching string ends at position: 18 解释\n$/ 返回一个 Match Object(匹配 regex 的字符串)。\n下面的方法可以在 Match Object 身上调用:\n .prematch 返回匹配前面的字符串 .postmatch 返回匹配后面的字符串 .from 返回匹配的开始位置 .to 返回匹配的结束位置   默认地空白在 regex 中是无关紧要的。 如果我们想在 regex 中包含空白, 我们必须显式地这样做。 regex m/:s Raku/ 中的 :s 强制考虑空白并且不会被删除。 二选一, 我们能把 regex 写为 m/Perl\\s6/ 并使用 \\s 占位符。 如果 regex 中包含的空白不止一个, 使用 :s 比使用 \\s 更高效。\n 例子 　让我们检查一个邮件是否合法。\n我们假设一个合法的电子邮件地址的形式如下:\nfirst name [dot] last name [at] company [dot] (com/org/net)\n 这个例子中用于电子邮件检测的 regex 不是很准确。它的核心意图是用来解释 Raku 中的 regex 的功能的。 不要在生产中原样使用它。\n 脚本\nmy $email = \u0026#39;john.doe@raku.org\u0026#39;; my $regex = /\u0026lt;:L\u0026gt;+\\.\u0026lt;:L\u0026gt;+\\@\u0026lt;:L+:N\u0026gt;+\\.\u0026lt;:L\u0026gt;+/; if $email ~~ $regex { say $/ ~ \u0026#34;is a valid email\u0026#34;; } else { say \u0026#34;This is not a valid email\u0026#34;; } 输出\njohn.doe@raku.org is a valid email 解释\n \u0026lt;:L\u0026gt; 匹配单个字符 \u0026lt;:L\u0026gt;+ 匹配单个字符或更多字符 \\. 匹配单个点号字符 \\@ 匹配单个 [at] 符号 \u0026lt;:L+:N\u0026gt; 匹配一个字母和数字 \u0026lt;:L+:N\u0026gt;+ 匹配一个或多个字母和数字  其中的 regex 可以分解成如下:\n first name \u0026lt;:L\u0026gt;+ [dot] \\. last name \u0026lt;:L\u0026gt;+ [at] \\@ company name \u0026lt;:L+:N\u0026gt;+ [dot] \\. com/org/net \u0026lt;:L\u0026gt;+  可选地, 一个 regex 可以被分解成多个具名 regexes。\nmy $email = \u0026#39;john.doe@raku.org\u0026#39;; my regex many-letters {\u0026lt;:L\u0026gt;+}; my regex dot {\\.}; my regex at {\\@}; my regex many-letters-numbers {\u0026lt;:L+:N\u0026gt;+}; if $email ~~ /\u0026lt;many-letters\u0026gt;\u0026lt;dot\u0026gt;\u0026lt;many-letters\u0026gt;\u0026lt;at\u0026gt;\u0026lt;many-letters-numbers\u0026gt;\u0026lt;dot\u0026gt;\u0026lt;many-letters\u0026gt;/ { say $/ ~ \u0026#34;is a valid email\u0026#34;; } else { say \u0026#34;This is not a valid email\u0026#34;; } 具名 regex 是使用 my regex regex-name { regex definition } 定义的。\n具名 regex 可以使用 \u0026lt;regex-name\u0026gt; 来调用。\n 更多关于 regexes 的东西, 查看 http://doc.raku.org/language/regexes\n 第十二章 Raku 模块 Rakudo 自带了 Panda 这个模块安装工具。\n要安装指定的模块, 在终端中键入如下命令:\nzef install \u0026quot;module name\u0026quot;  Raku 的模块目录可以在 http://modules.raku.org/ 中找到。\n 使用模块 　MD5 是一个关于密码的散列函数，它产生一个128位的散列值。 MD5 有多种加密存储在数据库中的口令的应用程序。当新用户注册时，其证书并不存储为纯文本，而是散列。这样做的理由是，如果该数据库被破解，攻击者将不能够知道口令是什么。\n比方说，你需要一个生成密码的MD5哈希以存储在数据库中备用的脚本。\n幸运的是，Raku 已经有一个能实现MD5算法的模块。我们安装下:\nzef install Digest::MD5\n现在运行下面的脚本:\nuse Digest::MD5; my $password = \u0026#34;password123\u0026#34;; my $hashed-password = Digest::MD5.new.md5_hex($password); say $hashed-password; 为了运行创建哈希的 md5_hex() 函数, 我们需要加载需要的模块。use 关键字用于加载模块。\n第十三章 Unicode Unicode 是编码并表现文本的标准, 它满足了世界上的大部分系统。\nUTF-8 是能够以Unicode编码所有可能的字符或代码点的字符编码。\n字符的定义是通过:\n字素: 可见的表示\n代码点: 赋值给字符的数字\n使用 Unicode 　让我们看一下使用 Unicode 能输出什么\nsay \u0026#34;a\u0026#34;; say \u0026#34;\\x0061\u0026#34;; say \u0026#34;\\c[LATIN SMALL LETTER A]\u0026#34;; 上面 3 行展示了构建字符的不同方法:\n 直接写出字符(字素) 使用 \\x 和代码点 使用 \\c 和代码点名字  现在我们来输出笑脸\nsay \u0026#34;☺\u0026#34;; say \u0026#34;\\x263a\u0026#34;; say \u0026#34;\\c[WHITE SMILING FACE]\u0026#34;; 组合两个代码点的另外一个例子\nsay \u0026#34;á\u0026#34;; say \u0026#34;\\x00e1\u0026#34;; say \u0026#34;\\x0061\\x0301\u0026#34;; say \u0026#34;\\c[LATIN SMALL LETTER A WITH ACUTE]\u0026#34;; 字母 á 可以被写为:\n 使用它的唯一代码点 \\x00e1 或作为 a 和 重音符号 \\x0061\\x0301 代码点的组合  有些方法可以使用\nsay \u0026#34;á\u0026#34;.NFC; say \u0026#34;á\u0026#34;.NFD; say \u0026#34;á\u0026#34;.uniname; 输出\nNFC:0x\u0026lt;00e1\u0026gt; NFD:0x\u0026lt;0061 0301\u0026gt; LATIN SMALL LETTER A WITH ACUTE NFC 返回唯一的代码点。\nNFD 分解(decompose)那个字符并返回每部分的代码点。\nuniname 返回代码点的名字。\nUnicode 字符可以用作标识符:\nmy $Δ = 1; $Δ++; say $Δ; 第十四章 并行、并发和异步 在正常情况下, 程序中的所有任务都是相继地运行的。\n这可能不是个事儿除非你正尝试去做的东西需要耗费很多时间。\n很自然地说, Raku 拥有能让你并行地运行东西的功能。\n在这个阶段, 注意并行可以是下面两个东西之一是很重要的:\n 任务并行化: 两个(或更多)独立的表达式并行地运行。 数据并行化: 单个表达式并行地迭代列表中的元素。  让我们从后者开始。\n数据并行化　 　my @array = (0..50000); #Array population my @result = @array.map({ is-prime $_ }); #call is-prime for each array element say now - INIT now; #Output the time it took for the script to complete 考虑上面的例子 我们只做一个操作 @array.map({is-prime $_})。is-prime 子例程相继被每个数组元素所调用:\nis-prime @array[0]  然后是 is-prime @array[1] 然后是 is-prime @array[2] 等等。\n幸运的是, 我们能同时在多个数组元素身上调用 is-prime 函数:\nmy @array = (0..50000); #Array population my @result = @array.race.map({ is-prime $_ }); #call is-prime for each array element say now - INIT now; #Output the time it took to complete 注意表达式中使用的 race。这个方法会使数组元素能够并行地迭代。\n运行两个例子(使用和不使用 race)运行之后, 比较两个脚本运行结束所花费的时间。\n race 不会保存元素的顺序。如果你想那样做, 使用 hyper 代替。\nrace\nmy @array = (1..1000); my @result = @array.race.map( {$_ + 1} ); @result».say; hyper\nmy @array = (1..1000); my @result = @array.hyper.map( {$_ + 1} ); @result».say; 如果你俩个脚本都运行了, 你应该注意到一个排序了一个没有排序。\n 任务并行化 　my @array1 = (0..49999); my @array2 = (2..50001); my @result1 = @array1.map( {is-prime($_ + 1)} ); my @result2 = @array2.map( {is-prime($_ - 1)} ); say @result1 == @result2; say now - INIT now; 考虑上面的例子:\n 我们定义了 2 个数组 对每个数组应用不同的操作并保存结果 并检查两个结果是否相同  该脚本等到 @array1.map( {is-prime($_ +1)} ) 完成然后计算 @array1.map( {is-prime($_ +1)} )。\n应用到每个数组的俩个操作彼此间没有依赖。\n为什么不并行地执行呢?\nmy @array1 = (0..49999); my @array2 = (2..50001); my $promise1 = start @array1.map( {$_ + 1} ); my $promise2 = start @array2.map( {$_ - 1} ); my @result1 = await $promise1; my @result2 = await $promise2; say @result1 == @result2; say now - INIT now; 解释\nstart 方法计算它后面的代码并返回promise 类型的对象或promise。\n如果代码被正确地求值, 那么 promise 会被保留(kept)。\n如果代码抛出异常, 那么 promise 会被破坏(broken)。\nawait 方法等待一个 promise。\n如果那个 promise 是被保留的, await 会获取到返回值。\n如果那个 promise 是被破坏的, await 会获取到抛出异常。\n检查每个脚本完成所花费的时间。\n 并行总是添加线程开销。如果开销抵消不了运算速度的增长，那么该脚本会显得较慢。 这就是为什么，在很简单的脚本中使用 race，hyper，start 和 await 实际上可以使它们慢下来。\n 并发和异步 　 关于并发和异步编程的更多信息, 请查看 http://doc.raku.org/language/concurrency\n 第十五章 社区 很多讨论发生在 #raku IRC 频道中。你可以到 http://raku.org/community/irc 进行任何询问。\nhttp://pl6anet.org/ 是一个 Raku 博客聚合器。\n","permalink":"https://ohmyweekly.github.io/notes/2015-12-21-raku-intro/","tags":["intro"],"title":"Raku 入门"},{"categories":["rakulang"],"contents":"第二天: 用 MAIN 函数控制命令行交互 在 UNIX 环境下，很多脚本都是要从命令行里获取运行参数的。在 Raku 中实现这个相当简单。比如下面这样：\n$ cat add.pl sub MAIN ($x, $y) { say $x + $y } $ raku add.pl 3 4 7 $ raku add.pl too many arguments Usage: add.pl x y 只要定义一个带命名变量的 MAIN 函数，你就可以获得一个命令行分析器。然后命令行参数就被自动绑定到 $x 和 $y 上了。如果不匹配，还有温馨的 Usage 提示。\n当然，你可能更喜欢自己定制 Usage 信息。那么自己动手，编写 USAGE 函数好了：\n$ cat add2.pl sub MAIN($x, $y) { say $x + $y } sub USAGE () { say \u0026#34;Usage: add.pl \u0026lt;num1\u0026gt; \u0026lt;num2\u0026gt;\u0026#34;; } $ raku add2.pl too many arguments Usage: add.pl \u0026lt;num1\u0026gt; \u0026lt;num2\u0026gt; 更进一步的，你可以用 multi 指令声明多个 MAIN 函数以完成一种可替代的语法，或者根据某些常量做出不同反应，比如：\n$ cat calc #!/usr/bin/env raku multi MAIN(\u0026#39;add\u0026#39;, $x, $y) { say $x + $y } multi MAIN(\u0026#39;div\u0026#39;, $x, $y) { say $x / $y } multi MAIN(\u0026#39;mult\u0026#39;, $x, $y) { say $x * $y } $ ./calc add 3 5 8 $ ./calc mult 3 5 15 $ ./calc Usage: ./calc add x y or ./calc div x y or ./calc mult x y 还有命名参数对应不同的选项的情况：\n$ cat copy.pl sub MAIN($source, $target, Bool :$verbose) { say \u0026#34;Copying \u0026#39;$source\u0026#39; to \u0026#39;$target\u0026#39;\u0026#34; if $verbose; run \u0026#34;cp $source$target\u0026#34;; } $ raku copy.pl calc calc2 $ raku copy.pl --verbose calc calc2 Copying \u0026#39;calc\u0026#39; to \u0026#39;calc2\u0026#39; 这里申明变量 $verbose 类型为 Bool，也就是不接受赋值。如果没有这个类型约束的话，它是需要赋值的，就像下面这样：\n$ cat do-nothing.pl sub MAIN(:$how = \u0026#39;fast\u0026#39;) { say \u0026#34;Do nothing, but do it $how\u0026#34;; } $ raku do-nothing.pl Do nothing, but do it fast $ raku do-nothing.pl --how=well Do nothing, but do it well $ raku do-nothing.pl what? Usage: do-nothing.pl [--how=value-of-how] 总之，Raku 提供了内置的命令行解析功能和使用帮助说明，你只要声明好函数就行了。\n文件操作  目录  不再用 opendir 和其他神马滴，Raku 中有专门的 dir 函数，用来列出指定目录（默认是当前所在目录）下所有的文件。好了，直接贴代码：\ndir dir \u0026#39;t\u0026#39; # t 目录下的文件 dir 还有一个可选的命名参数 test，用来 grep 结果，这样：\ndir \u0026#39;src/core\u0026#39;, test =\u0026gt; any(/^C/, /^P/) 创建目录，还是 mkdir 函数没错啦。\n 文件  最简单的读取文件的办法，是直接使用 slurp 函数，这个函数以标量形式返回文件的内容，这样：\nslurp \u0026#39;VERSION\u0026#39; 当然原始的文件句柄方式还是有效的，这样：\nmy $fh = open \u0026#39;CREDITS\u0026#39; $fh.getc #读取一个字符 $fh.get #读取一行（译者注：这两看起来好有 C 语言的赶脚啊） $fh.close; $fh = open \u0026#39;new\u0026#39;, :w # 以可写方式打开 $fh.print(\u0026#39;foo\u0026#39;) $fh.say(\u0026#39;bar\u0026#39;) $fh.close; say slurp(\u0026#39;new\u0026#39;)  文件测试  如果要测试文件是否存在以及文件的具体类型，直接使用 ~~ 操作符就搞定了，还是用代码说话：\n\u0026#39;LICENSE\u0026#39;.IO ~~ :e # 文件(广义的)是否存在 \u0026#39;LICENSE\u0026#39;.IO ~~ :d # 那么他是目录么？ \u0026#39;LICENSE\u0026#39;.IO ~~ :f # 那么是文件么(狭义的)？  File::Find  如果这些个标准特性还不够，那模块就派上用场了。File::Tools 包里的 File::Find 模块可以递归你指定的目录找你要的东西然后列出来。这个模块应该是跟着 Rakudo Star 一起打包了，如果你只裸装了 Rakudo，那么用 neutro 命令安装也是挺方便的。\n额，还是要例子？好吧~很简单的一行 find(:dir, :type, :name(/foo/))，这就会在 t/dir1 目录下，寻找名字匹配 foo 的文件，然后以树的形式列出来~不过要注意的是：这命令的返回可不是文本标量，而是一个个包括他们的完整路径在内的对象，而且还提供文件本身以及文件所在目录的访问器！更多信息，直接看文档吧。\n1、创建新文件\nopen(\u0026#39;new\u0026#39;, :w).close 2、匿名文件句柄\ngiven open(\u0026#39;foo\u0026#39;, :w) { .say(\u0026#39;Hello, world!\u0026#39;); .close } 第四天 – 序列操作符 去年，有一个序列操作符的简要梳理：\nmy @even-numbers := 0, 2 ... *; # 算术序列 my @odd-numbers := 1, 3 ... *; my @powers-of-two := 1, 2, 4 ... *; # 几何序列 这些现在在 Rakudo 里面实现了：\nmy @powers-of-two := 1, 2, 4 ... *; @powers-of-two[^10]; # 1 2 4 8 16 32 64 128 256 512 我们需要削减这个无限列表让 Rakudo 不会花费无限长的时间来计算它。这种情况下，我使用 [^10], 这其实是说 \u0026ldquo;给我前 10 个元素\u0026rdquo;。注意，当你把一个惰性列表绑定到一个数组变量上时，被计算过的值是会被记忆的，这是一种快捷的缓存。\n序列操作符 ... 是一个生成惰性列表的强大工具。上面的例子仅仅暗示了它能做什么。给定一个数字，它就从这个数字开始往下计数（除非序列的终点是一个更小的数字，这种情况下，它会倒数。\n1 ... 10; # 1 2 3 4 5 6 7 8 9 10 5 ... 1; # 5 4 3 2 1 给定两个数字来开始一个序列，它会把这当作一个算术序列，把前两个元素的差异添加到最后一个生成的元素上来产生下一个元素。如果给定三个元素，它会检查它们是否代表一个算术序列的开始或者它是否是一个几何序列，然后继续这个序列。\n当然，很多有趣的序列既非算术序列也非几何序列，这时，你需要显式地提供一个 sub 来生成序列中的下一个数：\nmy @Fibonacci := 0, 1, -\u0026gt; $a, $b { $a + $b } ... *; @Fibonacci[^10]; # 0 1 1 2 3 5 8 13 21 34 上面的 -\u0026gt; $a, $b { $a + $b } 是一个 pointy block (或者是一个匿名函数)，它带有 2 个参数并返回它们的和。这个序列操作符计算出该 block 有多少个参数，然后从当前序列的末尾传递所需的参数来生成序列的下一个数字，以此类推，循环下去。\n或者也可以中断循环，目前为止，所有的例子都有一个星号 * 放在右边，它意味着“没有终止条件”。如果你反而在那里放上一个数字，这个列表就会终止在那个数字。\n1, 1.1 ... 2; # 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 1, 1.1 ... 2.01; # 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 (1, 1.1 ... 2.01)[^14] # (1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 Nil Nil Nil) 第一个列表很自然地终止了，但是第二个列表漏掉了终止数，它会循环下去。结果就是一个无限列表，所以我把它限制到前 14 个元素，以至于我们能明白它正在做什么。\n那些有做浮点数学背景的人可能会气急败坏地说反复增加 0.1 直到精确到 2 为止很危险。\n在 Raku 中，没有这个问题，因为它会在可能的地方使用有理数（例如.分数)。如果我想找出所有 10000 以下的斐波纳契数，要找到到何处停止的那个精确的数字是很大的问题。幸运的是，就像你能使用块来指定怎样生成序列中的下一个元素一样，你可以使用块来测试序列是否结束：\n0, 1, -\u0026gt; $a, $b { $a + $b } ... -\u0026gt; $a { $a \u0026gt; 10000 }; # 0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946 尖头块 -\u0026gt; $a { $a \u0026gt; 10000 } 创建了一个含有一个参数的块，并且当参数大于 10000 时返回真；这就是我们需要的测试。\n除了我们所期待的所有斐波那契数小于 10000。 我们生成的裴波纳契数有一个大于 10000 的，当传递一个块作为终止测试时，该序列操作符所有的元素直到那个块返回真为止，然后它返回最后一个元素，然后停止。但是有一种替代形式的操作符能做同样的事情：\n0, 1, -\u0026gt; $a, $b { $a + $b } ...^ -\u0026gt; $a { $a \u0026gt; 10000 }; # 0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 从 ... 转换为 ...^ 意味着结果列表不包含让终止测试返回真时的第一个元素。\n在 Raku 中这真是一种冗长的指定序列的方法。在这里我没有地方解释所谓的闭包，但是去年的文章已经说过它们。使用闭包，你可以将上一个序列写为：\n0, 1, * + * ...^ * \u0026gt; 10000; # 0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 这样写是否清晰完全取决于你，条条大路通罗马。并且，序列操作符的左侧可以是任何列表，甚至是惰性的。这意味着你可以很容易的使用一个终止块来得到已存在的惰性列表的有限的一部分：\nmy @Fibonacci := 0, 1, * + * ... *; @Fibonacci ...^ * \u0026gt; 10000; # 0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 say @Fibonacci[30]; # 832040 (我坚持最后的检查只是为了说明 @Fibonacci 在超过 10000 之后依然会继续。\n这才触及到序列能做什么的皮毛，更多的信息，查看细则中的 “List infix precedence”，然后下拉到序列操作符（尽管如此，注意这还没有全部实现！它是一个极其复杂的操作符。）\n我还要告诉你的是，序列操作符不局限于工作于数字，如果你显式地指定了你自己的生成器，你可以生成任何类型的序列。但是我喜欢将这保留到未来的圣诞节礼物。\n第五天 – 为什么 Raku 语法是你想要的 圣临月第五天，您或许有些失望没能看到 Raku 酷呆了的新玩法。这次会是直观的解释一些编程语言的运行原理。 作为样例，先说下面这两行吧：\nsay 6 / 3; say \u0026#39;Price: 15 Euro\u0026#39; ~~ /\\d+/; 嗯，两行代码的运行结果分别是 2 和 15。相信这对 Raku 程序员来说没什么可奇怪的。但你再细看看，两行都有斜杠 /，却为了完全不一样的目的，第一个是数值运算，第二 个是正则匹配。\nRaku 怎么知道一个 / 号意味着什么？这当然不是简单的看 / 号后面的文本来决定，因为正则表达式可以看起来跟普通代码一样。 答案是：Raku 会持续跟踪他的预期。Raku 预期中最重要的两样东西就是：词和操作符。\n一个词可以是像 23 或者 str 这样的文字。当解释器发现这样一个文字，然后后面就会是一个语句的结束（即分号;），或者一个操作符像 +/* 等等。过了这个操作符，解 释器又开始预期下一个词。\n其实这就是问题的答案了：当解释器预期为词的时候，斜线 / 就代表正则表达式的开始；当预期为操作符的时候，斜线 / 就代表数字的除法运算。\n这种做法造成了深远的后果。子函数运行可以不加括号，而在函数名后面，perl 预期一个词开端的参数列表。另一方面，类型名必须跟在操作符后面，所以，所有的类型名必 须在解析时就是已知的。\n这样，很多字符都可以重复使用在不同的语法环境下了。\n第六天 – X 和 Z 元操作符 Raku 中一个新的创意就是元操作符，这种元操作符和普通的操作符结合改变了普通操作符的行为。这种元操作符有很多，但这里我们只关注它们中的两个: X 和 Z。\nX 操作符你可能已经见过它作为中缀交叉操作符的普通角色。它将列表联合在一起，每个列表中各取一个元素：\nsay ((1, 2) X (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;)).raku; # ((1, \u0026#34;a\u0026#34;), (1, \u0026#34;b\u0026#34;), (2, \u0026#34;a\u0026#34;), (2, \u0026#34;b\u0026#34;)) 然而, 这个中缀操作符 infix:\u0026lt;X\u0026gt; 其实是将 X 操作符应用到列表连接操作符 infix:\u0026lt;,\u0026gt; 上的简便形式。事实上，你可以这样写：\nsay ((1, 2) X, (10, 11)).raku; # ((1, 10), (1, 11), (2, 10), (2, 11)) 如果你将 X 应用到不同的中缀操作符上面会发生什么？应用到 infix:\u0026lt;+\u0026gt; 上呢？\nsay ((1, 2) X+ (10, 11)).raku; # (11, 12, 12, 13).list 它做了什么？它不是从每个组合中挑出所有的元素列表，这个操作符将中缀操作符 + 应用到元素间，并且结果不是一个列表，而是一个数字，是那个组合中所有元素的和。\n这对任何中缀操作符都有效。看看字符串连接 infix:\u0026lt; ~ \u0026gt;：\nsay ((1, 2) X~ (10, 11)).raku; # (\u0026#34;110\u0026#34;, \u0026#34;111\u0026#34;, \u0026#34;210\u0026#34;, \u0026#34;211\u0026#34;) 或者也许数值相等操作符 infix:\u0026lt;==\u0026gt;\nsay ((1, 2) X== (1, 1)).raku # (Bool::True, Bool::True, Bool::False, Bool::False) 但是这篇文章也是关于 Z 元操作符的。我们期望你已经知道他是什么了。如果你遇见过中缀操作符 Z，它当然是 Z, 的便捷形式。\nsay ((1, 2) Z, (3, 4)).raku; # ((1, 3), (2, 4)) say ((1, 2) Z+ (3, 4)).raku; # (4, 6).list say ((1, 2) Z== (1, 1)).raku; # (Bool::True, Bool::False) Z, 然后, 依次操作每个列表的每个元素，同时操作每个列表中的第一个元素，然后同时操作第二对儿，然后第三对儿，不管有多少。当到达列表的结尾时停止。\nZ 也是惰性的,所以你可以将它用在两个无限列表上，它会尽可能多地生成你需要的结果。X 只能处理左边是无限列表的列表，否则它不会设法得到任何东西。\n有一个键和值得列表，你想得到一个散列？ 容易！\nmy %hash = @keys Z=\u0026gt; @values; 或者，也许你想并行遍历两个列表？\nfor @a Z @b -\u0026gt; $a, $b { ... } 或者三个?\nfor @a Z @b Z @c -\u0026gt; $a, $b, $c { ... } 或者你能从扔3次有10个面的骰子的所有数字组合中，得到所有可能的总数：\nmy @d10 = 1 ... 10; my @scores = (@d10 X+ @d10) X+ @d10; 如果你想看到一些在真实世界这些原操作符的用途，看看 Moritz Lenz’s 写的 Sudoku.pm 数独解算器。\n第七天 词法变量 编程总是件很难持续做下去的事情。串几行代码很容易，根据想法做一个原型也是轻松愉快的。但随着程序慢慢变大，维护时间慢慢变长，事情慢慢就棘手起来了……最后，如果不幸的话，我们就得被迫重构——不是因为早先的问题复杂，而是因为程序本身复杂了……在不断的调试中急白了头的程序员们，早就不记得到底要怎么扩展程序以完成目的了……\n所以我们回溯一下编程史，找找对着复杂性的办法。而答案就在那里，不来不去——限制长度。当你架构一个成百上千模块组成的大型程序的时候，你必须能够让这些组件通过表面上很小的设置进行交互——否则你就等着被乱七八糟的组合干死吧。\n在各层次的编程上，我们都可以看到这么一个原则。因为他就只关心这一件事情：分散注意，专一的做一件事情！BCNF 范式、monads (译者注：不知道这东东咋翻译)、单子，例程，类，角色，模块，包等。这些都是在督促和指导我们限制编程的长度。这样我们才不会输在组 合学上。而这方面最简单的例子，就是词法变量。\n{ my $var; # $var可见 } # $var不可见 哈哈，这就是今天要介绍的一个非常酷的功能了！非常有趣的说。\nPerl 从第一版开始，在这方面一直不太对。比如 Perl5 的默认变量作用域是包。而这就是全局变量的一种。我在某个代码块里定义了一个变量，其他地方居然也能看到……\n$ perl -v This is perl 5, version 12, subversion 1 (v5.12.1) $ perl -E \u0026#39;{ $var = 42 }; say $var\u0026#39; 42 $ perl -wE \u0026#39;{ my $var= 42 }; say $var\u0026#39; Name \u0026#34;main::var\u0026#34; used only once: possible typo at -e line 1. Use of uninitialized value $var in say at -e line 1. 在 Raku 里，词法变量变成了默认设置。在 Rakudo 上运行上面的代码，根本无法通过编译：\n$ raku -e \u0026#39;{ $var = 42 }; say $var\u0026#39; ===SORRY!=== Symbol \u0026#39;$var\u0026#39; not predeclared in \u0026lt;anonymous\u0026gt; $ raku -e \u0026#39;{ my $var = 42 }; say $var\u0026#39; ===SORRY!=== Symbol \u0026#39;$var\u0026#39; not predeclared in \u0026lt;anonymous\u0026gt; 好了，你可能说：“嗯，可以减少点打错字的可能了”。这当然没错，但是更重要的是：这让你认真坦诚的对待变量作用域。这对你控制代码复杂性很有利！\n我们可以说出很多很多解释来说明为啥 Perl5 这么做。比如 Perl5 已经建议大家 use warnings; use strict; ，比如 Perl5 承诺的向后兼容，嗯，很伟大的做法，而 Perl1 压根没打算用来写大型程序和管理带来的复杂性；比如全局变量在单行模式下的各种方便……\nRaku 内置的强制你从小处着手，帮你在系统扩容的时候，更苛责的关注架构基础。在变量方面，也就是在脚本和模块中，将词法变量作为默认设置。不过在 perl -e 执行的单行命令中，默认依然是全局变量。（ Rakudo 还没有实现这个，目前单行依然是词法变量，期待实现的那天~）\n继续。好像到这里你感觉词法变量的价值已经说完了？没有！正确设计的结果可是令人惊讶和奖金源源不断啊~考虑一下这个子程序：\nsub counter($start_value) { my $count = $start_value; return { $count++ }; } 这里返回的是一个代码块。所以每次我们调用 counter() 的时候，得到的都是一小片断开的代码。然后看看当我创建两片这样的代码后的结果：\nmy $c1 = counter(5); say $c1(); # 5 say $c1(); # 6 \u0026amp;nbsp_place_holder; my $c2 = counter(42); say $c2(); # 42 say $c1(); # 7 say $c2(); # 43 看到了吧， $c1 和 $c2 是完全分开的，他们相互独立互不影响。尽管他们都写成 $count 的样子，看起来真是差不多，但他们都有自己独立的存储单元——因为每次我们运行进入那个代码块的时候，就是一次重新开始。这个小代码块从运行中的计数器里返回，这些计数器里保留了存储单元的对应关系。（他“关闭”这个存储单元，保护它不被 GC 回收掉。这类代码块叫闭包）\n这个闭包看起来像是个轻量级的对象？gxgx，他们确实就是。闭包背后的原则，即规范对闭包值的访问方式，与面向对象背后的封装和信息的原则是一样的。他们都是尽力限制事情的程度，在事情变得糟糕的时候，帮忙减少其影响和损失。\n你可以用词法变量做些很有趣的事情，比如闭包；而包变量不行。词法变量最酷啦！吼吼~~\n第八天 - 不同东西用不同名字 Perl5 的新手们，总会很奇怪的说为啥自己没法倒装呢？Perl 里有内置的 reverse 命令，但好像压根不起作用啊：\n$ perl -E \u0026#34;say reverse \u0026#39;hello\u0026#39;\u0026#34; hello 当他们去问一些有经验的 perler 的时候，解决办法很快就有了。因为 reverse 有两种操作模式，分别工作在标量和列表环境下，用来倒装字符串和列表元素：\n$ perl -E \u0026#34;say scalar reverse \u0026#39;hello\u0026#39;\u0026#34; olleh 比较悲剧的是这个情况和大多数的 perl 语境是不一致的。比方说，绝大多数的操作符和函数由自己决定语境，并在这个语境下分析数据。好比 + 和 * 作用于数字，. 作用于字符串。所以说他们代表一个操作并且提供语境，而 reverse 却不是。\n在 Raku 里，我们从过去的错误里吸取教训以摆脱历史的窘境。所以我们把列表倒叙，字符串翻转，哈希反演分开成了三个操作：\n# 字符串翻转，改名叫flip $ raku -e \u0026#39;say flip \u0026#34;hello\u0026#34;\u0026#39; olleh # 列表倒叙 $ raku -e \u0026#39;say join \u0026#34;, \u0026#34;, reverse \u0026lt;ab cd ef\u0026gt;\u0026#39; ef, cd, ab # 哈希反转，叫 invert my %capitals = France =\u0026gt; \u0026#34;Paris\u0026#34;, UK =\u0026gt; \u0026#34;London\u0026#34;; say %capitals.invert.raku; (\u0026#34;Paris\u0026#34; =\u0026gt; \u0026#34;France\u0026#34;, \u0026#34;London\u0026#34; =\u0026gt; \u0026#34;UK\u0026#34;) 注意哈希的反演和其他两个不同。因为哈希的值不要求是唯一的，所以反演后，哈希结构可能会被改变，或者某些值被覆盖…… 如果必要的话，使用者可以自己决定返回哈希结构时的操作方式。比如下面就是一种无损的方式：\nmy %inverse; %inverse.push( %original.invert ); 这个方法会在键值对存在的情况下，把新值 push 在原有值的队尾变成一个数组：\nmy %h; %h.push(\u0026#39;foo\u0026#39; =\u0026gt; 1); # foo =\u0026gt; 1 %h.push(\u0026#39;foo\u0026#39; =\u0026gt; 2); # foo =\u0026gt; [1, 2] %h.push(\u0026#39;foo\u0026#39; =\u0026gt; 3); # foo =\u0026gt; [1, 2, 3] 这三个函数，都会强制转换他们的参数。也就是说，如果你传递一个列表给 flip，这个列表会被强制成字符串后再翻转返回。\n第十天 – Feed operators 使用Perl 5 编程一段时间的人可能遇到或写过下面这样相似的代码：\nmy @new = sort { ... } map { ... } grep { ... } @original; 在这个构造中，数据从 @original 数组流进 grep，然后按顺序，流进 map ，然后流进 sort，最后将结果赋值给 @new 数组。因为它们每个都将列表作为它们最终的参数，仅仅通过位置，数据从一个操作符向左流进下一个操作符。\nRaku, 从另一方面，通过引入流向操作符让数据从一个操作符流进另一个操作符，让这种思想更明确。上面的 Perl 5 代码能使用 Raku 重写：\nmy @new \u0026lt;== sort { ... } \u0026lt;== map { ... } \u0026lt;== grep { ... } \u0026lt;== @original; 注意条条大路通罗马，这个在 Raku 中更能体现。你也可以跟 Perl 5 的写法相同：\nmy @new = sort { ... }, map { ... }, grep { ... }, @original; 唯一不同的是额外的逗号。\n所以，我们从这些流向操作符得到了什么？通常，当我们阅读代码的时候，你是从左向右读的，在原来的 Perl 5 代码中，你可能从左到右阅读你的代码直到你发现正在处理的结构，其流向是从右向左的，然后你跳到末尾，按照从右往左的方式再读一遍。\n在 Raku 中，现在有一个突出的句法标记，告诉你数据向左流动的性质。\n这样写也可以：\n@original ==\u0026gt; grep { ... } ==\u0026gt; map { ... } ==\u0026gt; sort { ... } ==\u0026gt; my @new; 下面是一些使用流向操作符的例子：\nmy @random-nums = (1..100).pick(*); # 100个随机数 my @odds-squared \u0026lt;== sort \u0026lt;== map { $_ ** 2 } \u0026lt;== grep { $_ % 2 } \u0026lt;== @random-nums; say ~@odds-squared; my @a= (1..100).pick(*); @a ==\u0026gt; grep {$_ % 2} ==\u0026gt; map { $_ ** 2} ==\u0026gt; sort {$^a \u0026lt;=\u0026gt; $^b} ==\u0026gt; my @c; # 1 9 25 49 81 121 169 225 289 361 441 529 625 729 841 961 1089 1225 1369 1521 1681 1849 2025 2209 2401 2601 2809 3025 3249 3481 3721 3969 4225 4489 4761 5041 5329 5625 5929 6241 6561 6889 7225 7569 7921 8281 8649 9025 9409 9801 my @odds-squared \u0026lt;== sort {$^b \u0026lt;=\u0026gt; $^a} \u0026lt;== map { $_ ** 2 } \u0026lt;== grep { $_ % 2 } \u0026lt;== @random-nums # 降序排列 # 9801 9409 9025 8649 8281 7921 7569 7225 6889 6561 6241 5929 5625 5329 5041 4761 4489 4225 3969 3721 3481 3249 3025 2809 2601 2401 2209 2025 1849 1681 1521 1369 1225 1089 961 841 729 625 529 441 361 289 225 169 121 81 49 25 9 1 my @rakudo-people = \u0026lt;scott patrick carl moritz jonathan jerry stephen\u0026gt;; @rakudo-people ==\u0026gt; grep { /at/ } ==\u0026gt; map { .ucfirst } ==\u0026gt; my @who-it\u0026#39;s-at; say ~@who-it\u0026#39;s-at; # Patrick Jonathan [+](my @a) \u0026lt;== map {$_ **2} \u0026lt;== 1..10 # 385， 1 到 10 的平方和 [+]() \u0026lt;== map {$_ **2} \u0026lt;== 1..10 # 385 第十二天 – 智能匹配 还记得Raku Advent 序列操作符吗?因为最后一个参数它接受的是一个上限，这让序列的生成停止了，例如：\n1, 2, 4 ... 32; # 1 2 4 8 16 32 1, 2, 4 ... * \u0026gt; 10; # 1 2 4 8 16 1,2,4 ... * \u0026gt; 100; # 1 2 4 8 16 32 64 128 1,2,4 ...^ * \u0026gt; 100; # 1 2 4 8 16 32 64 你能看到，在第一种情况下，使用了数值相等。第二个更有意思：*\u0026gt;10 在内部被重写为一个闭包，像这样 -\u0026gt; $x { $x \u0026gt; 10 } (through currying).\n序列操作符做了一些不可思议的比较，根据匹配者的类型。这种比较就叫做智能匹配，并且是在 Raku 中重复出现的一个概念，例如：\n# after the \u0026#39;when\u0026#39; keyword: given $age { when 100 { say \u0026#34;congratulations!\u0026#34; } when * \u0026lt; 18 { say \u0026#34;You\u0026#39;re not of age yet\u0026#34; } } # after \u0026#39;where\u0026#39;: subset Even of Int where * %% 2; # 显式地使用智能匹配操作符: if $input ~~ m/^\\d+$/ { say \u0026#34;$inputis an integer\u0026#34;; } # arguments to grep(), first() etc.: my @even = @numbers.grep: Even; 在智能操作符 ~~ 的右侧，并且在 when 和 where 的后面，要匹配的值被设置为 主题变量 $_。\n下面是一些智能操作符的用法：\n$foo ~~ Str #它的类型是 Str吗? $foo ~~ 6 #它等于 6 吗? $foo ~~ \u0026#34;bar\u0026#34; #或者它是 \u0026#34;bar\u0026#34; 吗? $foo ~~ /\\w+\u0026#39;-\u0026#39;\\d+/ # 它匹配某个模式吗? $foo ~~ (15..25) # 它的值在 15 和 25 之间吗? $foo ~~ -\u0026gt; $x { say \u0026#39;ok\u0026#39; if 5 \u0026lt; $x \u0026lt; 25 } # 调用闭包 $foo ~~ [1, *, 1, *, 1, *] # 含有6个元素的数组，是否其所有的奇数元素的值都为 1? 智能匹配的全部表现可以在这找到：.\n智能匹配没有特殊的操作符，而大部分智能匹配的情况会返回 Bool 值，对正则进行匹配会返回一个 Match 对象\n你可能开始怀疑：一个正确的，内置的类型，我怎么将它用在我自己的类中？你需要为它写一个特别的 ACCEPTS 方法。假如我们有一个叫Point 的类：\nclass Point { has $.x; has $.y; method ACCEPTS(Positional $p2) { return $.x == $p2[0] and $.y == $p2[1] } } 一切都清楚了吗?让我们看看它是如何工作的:\nmy $a = Point.new(x =\u0026gt; 7, y =\u0026gt; 9); say [3, 5] ~~ $a; # Bool::False say (7, 9) ~~ $a; # Bool::True 现在能恰当地做到你想要的，甚至使用你自己的类。\n第 16 天 - Raku 里的时间 今天是圣诞月历的第 0x10 天，是时候学习一下 raku 里的时间了。S32::Temporal 简介在过去一年中有了大量的修改，今天我们就来介绍一下在 raku 实现中关于时间的一些基础知识。\ntime 和 now 是两个可以返回当前时间（至少是你的系统认为的当前时间）的词。简单的展示一下：\nsay time; say now; 1292460064 Instant:2010-12-16T00:41:4.873248Z 第一个明显的区别，前者返回的是 POSIX 格式的数值型的结果；而后者返回的是一个瞬间的对象。如果你想获取秒级以下小数点位或者说闰秒，请用 now ；如果不用，那用 time 获取 POSIX 格式就够了。随你的便。\n DateTime 和他的伙伴  大多数时候，你要的不是当前时间。这种时候，你需要的是 DateTime 。比如还是获取当前时间：\nmy $moment = DateTime.new(now); # 或者 DateTime.new(time) 你有两种方式来创建 DateTime 对象：\nmy $dw = DateTime.new(:year(1963), :month(11), :day(23), :hour(17), :minute(15)); 这是 UTC 时区，如果你要更改时区的话，再加上 :timezone 就好了。这个格式里，只有 :year 是必须的，其他的默认就是1月1号半夜0点0分。\n上面这种写法确实乏味，你可以采用 ISO8601 格式的输入，来创建一个 DateTime 对象：\nmy $dw = DateTime.new(\u0026#34;1963-11-23T17:15:00Z\u0026#34;); 其中 Z 表示 UTC ，想改变的话，把 Z 替换成 +hhmm 或者 -hhmm 就好了。hh 表示小时，mm 表示分钟。 此外，还有一个更简略的 Date 对象。只包括年月日的：\nmy $jfk = Date.new(\u0026#34;1963-11-22\u0026#34;); # 你也可以用:year 等的写法 引入 Date 对象，是吸取了 CPAN 上 DateTime 模块的教训：有时候你压根不关心什么时区啊闰秒啊的。Date 对象非常容易处理，比如它有内置的 .succ 和 .pred 方法，用来简单的递增和递减。\n$jfk++; # 肯尼迪遇刺后的第二天 最后…\n以上就是关于 Raku 里的时间的内容了，想了解更多细节，去看看规范吧；或者去社区里提问\n第十九天 - 假作真时真亦假 今天的圣临礼物是教大家怎么用混淆完成一个小邪恶滴目的，吼吼~看起来这个功能挺疯狂的，其实有时候蛮有用的。先看下面这个用 but 的例子：\nmy $value = 42 but role { method Bool { False } }; say $value; # 42 say ?$value; # False 你看，我们改变了 $value 的 .Bool 方法。他不影响程序里其他所有的整数，哪怕别的变量也是 42。一般情况下，对于 Int 型，.Bool 方法（通过?操作符）返回值依据是是否等于 0。但这次它永远都返回 false 了。 事实上，我们还可以写的更简单，因为 False 是一个枚举值：\nmy $value = 42 but False; 因为 False 是 Bool 值，所有它会自动重载 .Bool 方法。这是 Raku 的一种转换方法。其他的值，也会对应的重载。\n这样在有的时候，这个东西就比较有用了：在 Perl5 里，你用 system 调用 shell 的时候，得牢牢记住在 shell 里，返回 0 才是正常的：\nif ( system($cmd) == 0 ) { # 或者!system($cmd) # ... } 而在 Raku 中，对应的 run 命令返回的是上面说的这种重载过的 Int，当且仅当返回值是 0 的时候，它的 bool 变成了 True，这正是我们想要的额！\nif run($cmd) { #不需要否定了 # ... } 好了，现在进入最疯狂的部分 —— 我们可以重载布尔值的布尔方法：\nmy $value = True but False; say $value; # True say ?$value; # False 没错，Raku 允许你这样自己踢自己屁股~~虽然我也不知道除了捣乱外怎么会有人愿意这么做，但是我还是很高兴看到 Raku 保持这种微妙的跟踪和重载类型的心态。我可没有……\nDay 21 – transliteration and beyond 转换听起来像拉丁词根,意味着字母的变化。这就是 Str.trans 方法所做的。\nsay \u0026#34;GATTACA\u0026#34;.trans( \u0026#34;TCAG\u0026#34; =\u0026gt; \u0026#34;0123\u0026#34; ); # prints \u0026#34;3200212\\n\u0026#34; 使用过Perl5 的人马上意识到这就是 tr/tcag/0123/ .\n下面是一个例子，使用 ROT-13算法加密文本：\nsub rot13($text) { $text.trans( \u0026#34;A..Za..z\u0026#34; =\u0026gt; \u0026#34;N..ZA..Mn..za..m\u0026#34; ) } 当 .trans 方法看到那些 .. 区间时，它会在内部将那些字母展开 (所以 \u0026ldquo;n..z\u0026rdquo; 意思是 \u0026ldquo;nopqrstuvwxyz\u0026rdquo;). 因此,rot13子例程的最终效果是将ASCII字母表的特定部分映射到其他部分。\n在 Perl5 中，两个点跟一个破折号相同，但是在Raku 中我们让那两个点 .. 代表 范围的概念，在主程序中，在正则中，还有在这里，转换。\n要注意的是，.trans 方法是不会改变原来的字符串； 它不会噶边 $text, 而是返回一个新的值。这在 Raku 中也是一个通用的旋律。要改变原值，请使用 .=trans\n$kabbala.=trans(\u0026#34;A..Ia..i\u0026#34; =\u0026gt; \u0026#34;1..91..9\u0026#34;); (并且，它不仅仅适用于 .trans 方法，它对所有方法都适用。)\n.trans 方法包含了一个秘密武器：假如我们想转义一些HTML，即，根据下面这个表来替换东西：\n\u0026amp; =\u0026gt; \u0026amp;amp; \u0026lt; =\u0026gt; \u0026amp;lt; \u0026gt; =\u0026gt; \u0026amp;gt; 但是我们不想关心替换还要按顺序进行：\nfoo =\u0026gt; bar foolishness =\u0026gt; folly 在上面的例子中，如果前面的替换先发生，就不回有后面的替换出现了 - 这可能不是你想要的。通常，我们想在短的子串之前，尝试并匹配最长的子串。\n所以，这看起来我们需要一个最长记号的替换匹配，以避免因为偶然的重复替换而产生的无限循环。 那就是 Raku 的 .trans 方法所提供的。这就是它的秘密武器：嵌入两个数组而非字符串。对于 HTML 转义，我们所需要的就是：\nmy $escaped = $html.trans( [ \u0026#39;\u0026amp;\u0026#39;, \u0026#39;\u0026lt;\u0026#39;, \u0026#39;\u0026gt;\u0026#39; ] =\u0026gt; [ \u0026#39;\u0026amp;amp;\u0026#39;, \u0026#39;\u0026amp;lt;\u0026#39;, \u0026#39;\u0026amp;gt;\u0026#39; ] ); 替换的顺序问题和避免循环就不用我们关心了。\n第二十二天 - Meta-Object Protocol 你有没有想过用你最爱的编程语言写一个类——但是不是按部就班的写类定义，而是通过几行代码？有些语言提供了 API 来完成这个功能。这些 API 的后面，就是元对象协议( Meta-Object Protocol )，简称 MOP。\nRaku 就有 MOP，你可以自己创建类、角色、语法，添加方法和属性，并且内省类。比如我们可以调用 MOP 查看 Rakudo 是如何实现 Rat 类型（有理数）的。调用 MOP ，只要把一般的 . 换成 .^ 就可以了。\n$ raku \u0026gt; say join \u0026#39;, \u0026#39;, Rat.^attributes $!numerator, $!denominator \u0026gt; # 列出全部方法比较多，所以随机选几个 \u0026gt; say join \u0026#39;, \u0026#39;, Rat.^methods(:local).pick(5) unpolar, ceiling, reals, Str, round \u0026gt; say Rat.^methods(:local).grep(\u0026#39;log\u0026#39;).[0].signature.raku :(Numeric $x: Numeric $base = { ... };; *%_) 显示出来的这几行信息相信都是不言自明了。Rat 有两个属性，$!numerator 和 $!denominator；有很多方法，其中 log 方法可接受的第一个变量是数值型 invocant(译者注：不知道怎么翻译，反正就是对象本身的引用 $_[0] )，用冒号标记过；第二个变量参数是可选的，名字是 $base，它设有一个默认值，不过 Rakudo 不打算告诉你……\nRaku 的数据库接口代码里有一个很不错的使用实例。它有一个选项用来记录对象的调用，但是只是记录一部分特定角色（比如和连接管理或者数据检索有关的）。下面是 dbi 里的代码：\nsub log-calls($obj, Role $r) { my $wrapper = RoleHOW.new; for $r.^methods -\u0026gt; $m { $wrapper.^add_method($m.name, method (|$c) { # 打印日志信息，note() 函数输出到标准错误 note \u0026#34;\u0026gt;\u0026gt; $m\u0026#34;; nextsame; }); } $wrapper.^compose(); # does 操作符和 but 类似，不过只修改一个对象的拷贝 $obj does $wrapper; } role Greet { method greet($x) { say \u0026#34;hello, $x\u0026#34;; } } class SomeGreeter does Greet { method LOLGREET($x) { say \u0026#34;OH HAI \u0026#34;~ uc $x; } } my $o = log-calls(SomeGreeter.new, Greet); # 记录日志啦，因为由 Greet 角色提供了 $o.greet(\u0026#39;you\u0026#39;); # 没记录，因为没角色提供这个 $o.LOLGREET(\u0026#39;u\u0026#39;); 运行结果如下：\n\u0026gt;\u0026gt; greet hello, you OH HAI U 所以说，有了 MOP ，除了指定的语法，你还可以像普通接口一样访问类、角色、语法和属性。这给了面向对象更大的灵活性，可以轻松的内省和修改对象了。\n第23天 - 一些精彩的排序示例 继续我们的圣临礼物。\n排序是一个非常非常常见的编程任务。Raku 加强了它的 .sort 功能来帮助大家更好的排序。 最最正常的默认写法是这样的：\nmy @sorted = @unsorted.sort; # 或者这样 sort @unsorted; 和 Perl 5 一样，也是可以自定义函数的：\n# 数值比较 my @sorted = @unsorted.sort: { $^a \u0026lt;=\u0026gt; $^b }; # 或者用函数调用的形式 my @sorted = sort { $^a \u0026lt;=\u0026gt; $^b }, @unsorted; # 字符串比对 ( 跟Perl5的cmp一样 ) my @sorted = @unsorted.sort: { $^a leg $^b }; # 类型依赖比对 my @sorted = @unsorted.sort: { $^a cmp $^b }; 你也可以把 : 换成 () ，然后再跟上一些方法进行后续处理，比如：\nmy @topten = @scores.sort( { $^b \u0026lt;=\u0026gt; $^a } ).list.munch(10); 小提示： $a 和 $b 不再像在 Perl5 中那样有特殊含义了，在 sort 代码块里用别的命名变量 $var、位置变量 $^var 或者其他任何的都跟在其他代码段里一样。\n你可以直接在排序的时候直接就做好变换函数：\nmy @sorted = @unsorted.sort: { foo($^a) cmp foo($^b) }; 不过 foo() 会在重复执行，如果列表不大也就罢了，如果比较大的话……如果 foo() 还是个计算密集型的……你懂的！\n在这种情况下，Perl 5 里有个习惯就是使用施瓦茨( Schwartzian )变换。施瓦茨变换的做法就是 decorate-sort-undecorate，foo() 函数只用执行一次：\n@sorted = map { $_-\u0026gt;[0] } sort { $a-\u0026gt;[1] cmp $b-\u0026gt;[1] } map { [$_, foo($_)] } @unsorted; Raku 里，你一样可以使用施瓦茨变换，不过 Raku 内置了一些智能方法。如果你有一个函数，它接受的参数个数是 0 或 1，Raku 会自动的替你启用施瓦茨变换。\n现在让我们来看一些例子吧。\n 不区分大小写的排序  把每个元素都改成小写，然后把数组按照小写的次序排好返回。\nmy @sorted = @unsorted.sort: { .lc };  单词长度排序  把每个元素的单词按照从短到长排序。\nmy @sorted = @unsorted.sort: { .chars }; 或者从长到短:\nmy @sorted = @unsorted.sort: { -.chars };  多次排序比较  你可以在 sort 代码块里放多个比较函数，sort 会注意执行直到退出。比如在单词长度的基础上，再按照 ASCII 码的顺序排序。\n.say for @a.sort: { $^a.chars, $^a } ; 不过，在 Rakudo 里好像运行有点问题……它只会比较长度不会比较数值，也就是说，10 排在 2 的前面。（没关系，TMTONTDI）\nRaku 里的 sort 本身是稳定工作的，你可以重复使用。\n.say for @a.sort.sort: { $^a.chars }; 不过这样 sort 有两次调用，no fashion！所以你还可以这么写：\n.say for @a.sort: { $^a.chars \u0026lt;=\u0026gt; $^b.chars || $^a leg $^b }; 不过这下你有两个参数了，Raku 没法自动给你启动施瓦茨变换了。\n又或者，你可以加上一个给自然数排序的函数：\n.say for @a.sort: { $^a.chars.\u0026amp;naturally, $^a }; “给自然数排序？”我好像听到你们的哭声了，“哪里有？” 很高兴你们这么问，现在继续解决这个问题。\n 自然数排序  标准的词法排序是按照 ASCII 次序的。先是自然数，然后是大写字母，最后是小写字母。所以人们在排序的时候经常得到这样的结果：\n0 1 100 11 144th 2 21 210 3rd 33rd AND ARE An Bit Can and by car d1 d10 d2 完全正确，但是没用……尤其是对非程序员来说，更郁闷了就……真正的自然排序，应该是先按数学量级排自然数，然后才是大小写字母。比如上面那个例子，应该排成这样：\n0 1 2 3rd 11 21 33rd 100 144th 210 An AND and ARE Bit by Can car d1 d2 d10 所以，我们必须的在排序的时候加上一点转换了。我使用 .subst 方法，这是我们所熟悉的 s/// 操作符的面向对象形式。\n.subst(/(\\d+)/, -\u0026gt; $/ { 0 ~ $0.chars.chr ~ $0 }, :g) 第一部分，捕获一个连续的数字，然后由 -\u0026gt; $/ {} 构成一个尖号块，意思是：“传递匹配到 $/ 的数组到 {} 代码里”。然后代码里替换成用 0 按照数量级排序的顺序联结的字符串。这个 0 是以 ASCII 字符串出现，联结在原始字符串上的。最后 /g 表示全局替换。\n如果也不区分大小写，那么：\n.lc.subst(/(\\d+)/, -\u0026gt; $/ { 0 ~ $0.chars.chr ~ $0 }, :g) 改成子例程的方式：\nsub naturally ($a) { $a.lc.subst(/(\\d+)/, -\u0026gt; $/ { 0 ~ $0.chars.chr ~ $0 }, :g) } 看起来很不错了，不过还有点小问题，比如 THE 、 The 和 the 会按照他们在列表里的顺序返回，而不是我们预计的顺序。有个简单的解决办法，就是在转换过的元素的结尾，加上一个中断。所以最终结果是：\nsub naturally ($a) { $a.lc.subst(/(\\d+)/, -\u0026gt; $/ { 0 ~ $0.chars.chr ~ $0 }, :g) ~ \u0026#34;\\x0\u0026#34; ~ $a } 然后你看，这个子例程只有一个参数，所以我们还可以用上施瓦茨变换了：\n.say for \u0026lt;0 1 100 11 144th 2 21 210 3rd 33rd AND ARE An Bit Can and by car d1 d10 d2\u0026gt;.sort: { .\u0026amp;naturally }; 或者用来给 ip 排序：\nmy @ips = ((0..255).roll(4).join(\u0026#39;.\u0026#39;)for 0..99); .say for @ips.sort: { .\u0026amp;naturally }; 输出:\n4.108.172.65 5.149.121.70 10.24.201.53 11.10.90.219 12.83.84.206 12.124.106.41 12.162.149.98 14.203.88.93 16.18.0.178 17.68.226.104 21.201.181.225 23.61.166.202 以及目录排序啊等等各种数字与字母的混合体。\n最后，圣诞快乐，排序快乐，愿施瓦茨与你同在！\n","permalink":"https://ohmyweekly.github.io/notes/2015-10-05-raku-calendar-2010/","tags":["calendar"],"title":"Raku 圣诞月历(2010)"},{"categories":["rakulang"],"contents":"喜欢修复 Raku 编译器中的 bug? 这儿有一个great grammar bugglet: 当 „” 引号用在引起的用空白分割的单词列表构造器中时看起来好像不能工作:\nsay „hello world”; .say for qww\u0026lt;„hello world”\u0026gt;; .say for qww\u0026lt;\u0026#34;hello world\u0026#34;\u0026gt;; # OUTPUT: # hello world # „hello # world” # hello world ” 引号不应该出现在输出中并且在输出中我们应该只有 3 行输出; 这 3 行输出都是 hello world。看起来像是一个待修复的有趣的 bug! 我们进去看看。\n你怎样拼写它? 事实上这段代码没能正确解析表明这是一个 grammar bug。大部分的 grammar 住在 src/Raku/Grammar.nqp中, 但是在我们的手变脏之前, 让我们来解决我们应该查看什么。\n二进制 raku 有一个 --target 命令行参数来接收其中之一的编译步骤并且会导致那个步骤的输出被产生出来。那儿有哪些步骤? 根据你正使用的后端它们也会有所不同, 但是你可以仅仅运行 raku --stagestats -e '' 把它们都打印出来:\nzoffix@leliana:~$ raku --stagestats -e '' Stage start : 0.000 Stage parse : 0.077 Stage syntaxcheck: 0.000 Stage ast : 0.000 Stage optimize : 0.001 Stage mast : 0.004 Stage mbc : 0.000 Stage moar : 0.000 Grammars 是关于解析的, 所以我们会查询 parse 目标(target)。至于要执行的代码, 我们会仅仅给它有问题的那块; 即 qww\u0026lt;\u0026gt;:\nzoffix@leliana:~$ raku --target=parse -e 'qww\u0026lt;„hello world”\u0026gt;' - statementlist: qww\u0026lt;„hello world”\u0026gt; - statement: 1 matches - EXPR: qww\u0026lt;„hello world”\u0026gt; - value: qww\u0026lt;„hello world”\u0026gt; - quote: qww\u0026lt;„hello world”\u0026gt; - quibble: \u0026lt;„hello world”\u0026gt; - babble: - B: - nibble: „hello world” - quote_mod: ww - sym: ww 那很棒! 每一行前面都有能在 grammar 中找到的 token 的名字, 所以现在我们知道了在哪里查找问题。\n我们还知道基本的引号能正确地工作, 所以我们也倾倒出它们的解析步骤, 来看看这两个输出之间是否有什么不同:\nzoffix@leliana:~$ raku --target=parse -e 'qww\u0026lt;\u0026quot;hello world\u0026quot;\u0026gt;' - statementlist: qww\u0026lt;\u0026quot;hello world\u0026quot;\u0026gt; - statement: 1 matches - EXPR: qww\u0026lt;\u0026quot;hello world\u0026quot;\u0026gt; - value: qww\u0026lt;\u0026quot;hello world\u0026quot;\u0026gt; - quote: qww\u0026lt;\u0026quot;hello world\u0026quot;\u0026gt; - quibble: \u0026lt;\u0026quot;hello world\u0026quot;\u0026gt; - babble: - B: - nibble: \u0026quot;hello world\u0026quot; - quote_mod: ww - sym: ww 那么\u0026hellip; 好吧, 除了引号不同, 解析数完全一样。所以它看起来好像所有涉及的 tokens 都是相同的, 但是那些 tokens 所做的事情不同。\n我们不必检查输出中我们看到的每个 tokens。statementlist 和 statement 是匹配普通语句的 tokens, EXPR 是占位符解析器, value 是它正操作的值中的一个。我们会忽略上面那些, 留给我们的是下面这样一个可疑的列表:\n- quote: qww\u0026lt;„hello world”\u0026gt; - quibble: \u0026lt;„hello world”\u0026gt; - babble: - B: - nibble: „hello world” - quote_mod: ww - sym: ww 让我们开始质问它们。\n到兔子洞里去\u0026hellip; 你自己搞一份本地的 Rakudo 仓库, 如果你已经有了一份,那么打开 src/Raku/Grammar.nqp, 然后放松点。\n我们会从树的顶部到底部跟随我们的 tokens, 所以我们首先需要找到的是 token quote, rule quote, regex quote 或 method quote; 以那个顺序搜索, 因为第一项很可能就是正确的东西。\n这种情况下, 它是一个 token quote, 它是一个 proto regex。我们的代码使用了它的 q 版本并且你还可以认出靠近它的 qq 和 Q 版本:\ntoken quote:sym\u0026lt;q\u0026gt; {:my $qm;\u0026#39;q\u0026#39;[|\u0026lt;quote_mod\u0026gt;{}\u0026lt;.qok($/)\u0026gt;{ $qm := $\u0026lt;quote_mod\u0026gt;.Str }\u0026lt;quibble(%*LANG\u0026lt;Quote\u0026gt;, \u0026#39;q\u0026#39;, $qm)\u0026gt;|{}\u0026lt;.qok($/)\u0026gt;\u0026lt;quibble(%*LANG\u0026lt;Quote\u0026gt;, \u0026#39;q\u0026#39;)\u0026gt;]} token quote:sym\u0026lt;qq\u0026gt; {:my $qm;\u0026#39;qq\u0026#39;[|\u0026lt;quote_mod\u0026gt;{ $qm := $\u0026lt;quote_mod\u0026gt;.Str }\u0026lt;.qok($/)\u0026gt;\u0026lt;quibble(%*LANG\u0026lt;Quote\u0026gt;, \u0026#39;qq\u0026#39;, $qm)\u0026gt;|{}\u0026lt;.qok($/)\u0026gt;\u0026lt;quibble(%*LANG\u0026lt;Quote\u0026gt;, \u0026#39;qq\u0026#39;)\u0026gt;]} token quote:sym\u0026lt;Q\u0026gt; {:my $qm;\u0026#39;Q\u0026#39;[|\u0026lt;quote_mod\u0026gt;{ $qm := $\u0026lt;quote_mod\u0026gt;.Str }\u0026lt;.qok($/)\u0026gt;\u0026lt;quibble(%*LANG\u0026lt;Quote\u0026gt;, $qm)\u0026gt;|{}\u0026lt;.qok($/)\u0026gt;\u0026lt;quibble(%*LANG\u0026lt;Quote\u0026gt;)\u0026gt;]} 可以看到 qq 和 Q 的主体看起来像 q, 我们也来看看它们是否有我们要找的那个 bug:\nzoffix@leliana:~$ raku -e '.say for qqww\u0026lt;„hello world”\u0026gt;' „hello world” zoffix@leliana:~$ raku -e '.say for Qww\u0026lt;„hello world”\u0026gt;' „hello world 是的, 它们也存在, 所以 token quote 不可能是那个问题。我们来分解下 token quote:sym\u0026lt;q\u0026gt; 是做什么的, 来算出怎么进行到下一步; 它的备选之一没有被用在我们当前的代码中, 所以我会省略它:\ntoken quote:sym\u0026lt;q\u0026gt; {:my $qm;\u0026#39;q\u0026#39;[|\u0026lt;quote_mod\u0026gt;{}\u0026lt;.qok($/)\u0026gt;{ $qm := $\u0026lt;quote_mod\u0026gt;.Str }\u0026lt;quibble(%*LANG\u0026lt;Quote\u0026gt;, \u0026#39;q\u0026#39;, $qm)\u0026gt;|# (this branch omited) ]} 在第二行中, 我们创建了一个变量, 然后匹配字面值 q 然后是 quote_mod token。那个是我们的 --target=parse 输出中的一部分并且如果你像我们找出 quote token 那样找出它, 你会注意到它是一个 proto regex, 即, 在那种情况下, 匹配我们代码的 ww 块。后面跟着的空 {} 块我们可以忽略(那是一个 bug 的替代方法可能在你读到这儿时已经被修复了)。目前为止, 我们已经匹配了我们代码的 qww 块。\n再往前走, 我们遇见了对 qok token 的调用, 当前的 Match 对象作为其参数。\u0026lt;.qok\u0026gt; 中的点号表明这是一个非捕获 token 匹配, 这就是它为什么它没有在我们的 --target=parse 输出中出现的原因。我们定位到那个 token 并看看它是关于什么的:\ntoken qok($x) {»\u0026lt;![(]\u0026gt;[\u0026lt;?[:]\u0026gt;||\u0026lt;!{ my $n := ~$x; $*W.is_name([$n]) || $*W.is_name([\u0026#39;\u0026amp;\u0026#39; ~ $n]) }\u0026gt;][\\s*\u0026#39;#\u0026#39;\u0026lt;.panic: \u0026#34;# not allowed as delimiter\u0026#34;\u0026gt;]?\u0026lt;.ws\u0026gt;} 我的天呐! 这么多符号, 但是这个家伙很容易了: » 是一个右单词边界后面不能跟着一个开圆括号(\u0026lt;![(]\u0026gt;), 再跟着一个备选分支([]), 再跟着一个检查, 即我们不想尝试使用 # 号作为分割符([...]?), 最后跟着一个 \u0026lt;.ws\u0026gt; token 吞噬各种各样的空白。\n在备选分支中, 我们使用了首个token匹配的 || 备选分支(和最长token匹配 | 相反), 并且首个 token 向前查看一个冒号 \u0026lt;?[:]\u0026gt;。 如果失败了, 我们就字符串化那个给定的参数(~$x)并且之后在 World对象 身上调用 is_name 方法, 原样地传递带有前置 \u0026amp; 符号的字符串化的参数。传递的 ~$x 是目前为止我们的 token quote:sym\u0026lt;q\u0026gt; token 所匹配到的东西(并且那是字符串 qww)。is_name 方法仅仅检查那个给定的符号是否被定义还有根据那个返回值检查我们的 token 匹配会通过还是会失败。如果那个求值代码返回一个真值那么我们正在使用的 \u0026lt;!{ ... }\u0026gt; 结构就会失败。\n总而言之, 这个 token 所做的所有事情就是检查我们没有使用 # 作为分隔符并且没有尝试去调用一个方法或sub。房间的这个角落没有 bug 迹象。 让我们回到我们的 token quote:sym\u0026lt;q\u0026gt; 来查看下一步做什么:\ntoken quote:sym\u0026lt;q\u0026gt; {:my $qm;\u0026#39;q\u0026#39;[|\u0026lt;quote_mod\u0026gt;{}\u0026lt;.qok($/)\u0026gt;{ $qm := $\u0026lt;quote_mod\u0026gt;.Str }\u0026lt;quibble(%*LANG\u0026lt;Quote\u0026gt;, \u0026#39;q\u0026#39;, $qm)\u0026gt;|# (this branch omited) ]} 我们已经完成了 \u0026lt;.qok\u0026gt; 的检查, 所以下一步是 { $qm := $\u0026lt;quote_mod\u0026gt;.Str }, 那仅仅把匹配到 quote_mod token 的字符串值存到 $qm 变量中。在我们的例子中, 那个值就是字符串 ww。\n下面跟着的是另外一个 token, 它在我们的 --target=parse s输出中出现过:\n\u0026lt;quibble(%*LANG\u0026lt;Quote\u0026gt;, \u0026#39;q\u0026#39;, $qm)\u0026gt; 这里, 我们使用三个位置参数引用了那个 token: Quote language braid, 字符串 q 和 我们保存在变量 $qm 中的字符串 ww。我想知道它是做什么的。那是我们的下一站。全力以赴!\nNibble Quibble Babbling Nibbler 这里是完整的 token quibble 并且你马上可以发现我们不得不从开始往更深处挖掘, 因为第 5 行是另外一个 token 匹配:\ntoken quibble($l, *@base_tweaks) {:my $lang;:my $start;:my $stop;\u0026lt;babble($l, @base_tweaks)\u0026gt;{ my $B := $\u0026lt;babble\u0026gt;\u0026lt;B\u0026gt;.ast; $lang := $B[0]; $start := $B[1]; $stop := $B[2]; }$start\u0026lt;nibble($lang)\u0026gt;[$stop||{ $/.CURSOR.typed_panic( \u0026#39;X::Comp::AdHoc\u0026#39;, payload =\u0026gt; \u0026#34;Couldn\u0026#39;t find terminator $stop(corresponding $startwas at line { HLL::Compiler.lineof( $\u0026lt;babble\u0026gt;\u0026lt;B\u0026gt;.orig(), $\u0026lt;babble\u0026gt;\u0026lt;B\u0026gt;.from() ) })\u0026#34;, expected =\u0026gt; [$stop], ) } ] { nqp::can($lang, \u0026#39;herelang\u0026#39;) \u0026amp;\u0026amp; self.queue_heredoc( $*W.nibble_to_str( $/, $\u0026lt;nibble\u0026gt;.ast[1], -\u0026gt; { \u0026#34;Stopper \u0026#39;\u0026#34; ~ $\u0026lt;nibble\u0026gt; ~ \u0026#34;\u0026#39; too complex for heredoc\u0026#34; }), $lang.herelang, ) } } 我们定义了 3 个变量然后引用了 babble token, 这个 babble 引用了和 quibble token 所引用的同样的参数。我们来以和查找所有之前的 tokens 同样的方式查找它并窥探它的内核。为了简洁, 我移除了大约一半代码:那部分是处理副词的, 目前我们不能在我们的代码中使用它。\ntoken babble($l, @base_tweaks?) {:my @extra_tweaks;# \u0026lt;irrelevant portion redacted\u0026gt; $\u0026lt;B\u0026gt;=[\u0026lt;?before.\u0026gt;]{ # Work out the delimeters. my $c := $/.CURSOR; my @delims := $c.peek_delimiters($c.target, $c.pos); my $start := @delims[0]; my $stop := @delims[1]; # Get the language. my $lang := self.quote_lang($l, $start, $stop, @base_tweaks, @extra_tweaks); $\u0026lt;B\u0026gt;.\u0026#39;!make\u0026#39;([$lang, $start, $stop]); }} 我们通过把向前查看捕获到 $\u0026lt;B\u0026gt; 捕获中开始, 它用作更新当前的 Cursor 位置, 然后进入以执行那个代码块。我们把当前的 Cursor 存储在 $c 中, 然后在它身上调用 .peek_delimiters 方法。如果我们为了它在内置的 rakudo 目录中进行 grep, 我们会看到它被定义在 NQP中, 在 nqp/src/HLL/Grammar.nqp中, 但是在我们冲出去阅读它的代码之前, 注意它是怎样返回两个分隔符的。我们仅仅把它们打印出来好了?\nsrc/Raku/Grammar.nqp 的 .nqp 后缀名表明我们正处在 NQP 的地盘儿, 所以我们不要使用 NQP ops仅仅并且不是完全的 Raku 代码。通过把下面这一行代码添加到 @delim 被赋值给 $start 和 $stop 的地方, 我们能找出 .peek_delimiters 给我们的东西:\nnqp::say(\u0026#34;$sart$stop\u0026#34;); 编译!\n$ perl Configure.pl --gen-moar --gen-nqp --backends=moar \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make test \u0026amp;\u0026amp; make install 即使在编译期间, 通过吐出额外的东西, 我们的调试行已经给了我们所有那些分隔符是关于什么的启发。再次运行我们的有问题的代码:\n$ ./raku -e \u0026#39;.say for qww\u0026lt;„hello world”\u0026gt;;\u0026#39; \u0026lt; \u0026gt; hello world 打印出的分隔符是 qww 里的尖括号分隔符。我们对那些不感兴趣, 所以我们可以忽略 .peek_delimiters 并继续。再往上是 .quote_lang 方法。 它的名字里有一个\u0026quot;引号\u0026quot;而我们有一个关于引号的问题.. 听起来我们离真相越来越近了。我们来看看我们正传递给它的是什么参数:\n $1 — Quote language braid $start / $stop — 尖括号分隔符 @base_tweaks — 包含一个元素: 字符串 ww @extra_tweaks — 额外的副词, 这里我们没有, 所以这个数组是空的  定位到 method quote_lang; 它仍然在 src/Raku/Grammar.nqp文件中:\nmethod quote_lang($l, $start, $stop, @base_tweaks?, @extra_tweaks?) { sub lang_key() { # \u0026lt;body redacted\u0026gt; } sub con_lang() { # \u0026lt;body redacted\u0026gt; } # Get language from cache or derive it. my $key := lang_key(); nqp::existskey(%quote_lang_cache, $key) \u0026amp;\u0026amp; $key ne \u0026#39;NOCACHE\u0026#39; ?? %quote_lang_cache{$key} !! (%quote_lang_cache{$key} := con_lang()); } 我们有两个词法子例程 lang_key 和 con_lang, 在它们下面我们把 lang_key 的输出存储到 $key 中, 在 %quote_lang_cache 中这个 $key 被用在整个缓存 dance 中, 所以我们可以忽略掉 lang_key sub 并直接进入 con_lang, 它被调用以生成我们的 quote_lang 方法的返回值:\nsub con_lang() { my $lang := $l.\u0026#39;!cursor_init\u0026#39;(self.orig(), :p(self.pos()), :shared(self.\u0026#39;!shared\u0026#39;())); for @base_tweaks { $lang := $lang.\u0026#34;tweak_$_\u0026#34;(1); } for @extra_tweaks { my $t := $_[0]; if nqp::can($lang, \u0026#34;tweak_$t\u0026#34;) { $lang := $lang.\u0026#34;tweak_$t\u0026#34;($_[1]); } else { self.sorry(\u0026#34;Unrecognized adverb: :$t\u0026#34;); } } nqp::istype($stop,VMArray) || $start ne $stop ?? $lang.balanced($start, $stop) !! $lang.unbalanced($stop); } 在初始化 Cursor 位置之后, $lang 继续包含我们的 Quote 语言编织然后我们落进一个 for 循环来迭代 @base_tweaks, 对于里面的每一个元素, 我们都调用方法 tweak_$_, 给它传递一个真值 1。因为我们仅仅只有一个 base tweak, 这意味着我们正在Quote braid上调用方法 tweak_ww。我们来看看那个方法是关于什么的。\n因为 Quote braid 被定义在同一个文件中, 仅仅搜索 method tweak_ww 好了:\nmethod tweak_ww($v) { $v ?? self.add-postproc(\u0026#34;quotewords\u0026#34;).apply_tweak(ww) !! self } 很好。我们给它的 $v 为真, 所以我们调用了 .add-postproc 然后调用 .apply_tweak(ww)。看一下那个方法的上面和下面, 我们看到 .add-postproc 也用在其它不含 bug 的引号中, 所以我们忽略它并直接跳到 .apply_tweak:\nmethod apply_tweak($role) { my $target := nqp::can(self, \u0026#39;herelang\u0026#39;) ?? self.herelang !! self; $target.HOW.mixin($target, $role); self } 啊哈! 它的参数是一个 role 并且它把该 role 混进来我们的 Quote braid 中。我们来看看那个 role 是关于什么的(再一次, 仅仅在文件中搜索 role ww, 或者仅仅向上滚动一点):\nrole ww { token escape:sym\u0026lt;\u0026#39; \u0026#39;\u0026gt; {\u0026lt;?[\u0026#39;]\u0026gt;\u0026lt;quote=.LANG(\u0026#39;MAIN\u0026#39;,\u0026#39;quote\u0026#39;)\u0026gt;} token escape:sym\u0026lt;‘ ’\u0026gt; {\u0026lt;?[‘]\u0026gt;\u0026lt;quote=.LANG(\u0026#39;MAIN\u0026#39;,\u0026#39;quote\u0026#39;)\u0026gt;} token escape:sym\u0026lt;\u0026#34; \u0026#34;\u0026gt; {\u0026lt;?[\u0026#34;]\u0026gt;\u0026lt;quote=.LANG(\u0026#39;MAIN\u0026#39;,\u0026#39;quote\u0026#39;)\u0026gt;} token escape:sym\u0026lt;“ ”\u0026gt; {\u0026lt;?[“]\u0026gt;\u0026lt;quote=.LANG(\u0026#39;MAIN\u0026#39;,\u0026#39;quote\u0026#39;)\u0026gt;} token escape:sym\u0026lt;colonpair\u0026gt; {\u0026lt;?[:]\u0026gt;\u0026lt;!RESTRICTED\u0026gt;\u0026lt;colonpair=.LANG(\u0026#39;MAIN\u0026#39;,\u0026#39;colonpair\u0026#39;)\u0026gt;} token escape:sym\u0026lt;#\u0026gt; {\u0026lt;?[#]\u0026gt;\u0026lt;.LANG(\u0026#39;MAIN\u0026#39;, \u0026#39;comment\u0026#39;)\u0026gt;} } 奥, 我的天呐!引号! 如果这个地方不是我们修复 bug 的地方, 那么我就是一个芭蕾舞女演员。 我们找到它了!\n我们定位到的 role 把进了某些 tokens 混合进了我们正使用的 Quote braid 中来解析 qww 的内容。我们带有 bug 的 „” 引号组合明显不在那个列表中。我们来把它添加进去!\ntoken escape:sym\u0026lt;„ ”\u0026gt; {\u0026lt;?[„]\u0026gt;\u0026lt;quote=.LANG(\u0026#39;MAIN\u0026#39;,\u0026#39;quote\u0026#39;)\u0026gt;} 编译! 运行我们带有 bug 的代码:\n$ ./raku -e \u0026#39;.say for qww\u0026lt;foo „hello world” bar\u0026gt;\u0026#39; foo bar 悲催! 好吧, 我们确实为引号处理找到了正确的地方, 但是我们让问题变得更加糟糕了。发生了什么?\nQuotastic Inaction 我们新的 token 肯定解析了那个引号, 但是我们绝对没有给它添加 Actions 动作\u0026hellip; 好吧, 对它起作用。Action 类和 Grammars 相邻, 在 src/Raku/Actions.nqp 中。打开它并定位到匹配的方法那里; 比如 method escape:sym\u0026lt;“ ”\u0026gt;。\nmethod escape:sym\u0026lt;\u0026#39; \u0026#39;\u0026gt;($/) { make mark_ww_atom($\u0026lt;quote\u0026gt;.ast); } method escape:sym\u0026lt;\u0026#34; \u0026#34;\u0026gt;($/) { make mark_ww_atom($\u0026lt;quote\u0026gt;.ast); } method escape:sym\u0026lt;‘ ’\u0026gt;($/) { make mark_ww_atom($\u0026lt;quote\u0026gt;.ast); } method escape:sym\u0026lt;“ ”\u0026gt;($/) { make mark_ww_atom($\u0026lt;quote\u0026gt;.ast); } 并在列表中添加我们自己的版本:\nmethod escape:sym\u0026lt;„ ”\u0026gt;($/) { make mark_ww_atom($\u0026lt;quote\u0026gt;.ast); } 编译! 运行我们带有 bug 的代码:\n$ ./raku -e \u0026#39;.say for qww\u0026lt;foo „hello world” bar\u0026gt;\u0026#39; foo hello world bar 呼! 成功了! 不再有 bug 了。我们修复了那个 bug!\n但是, 等一下\u0026hellip;\n遗漏了, 但是没有忘记 看一下所有可能的奢华的引号的列表。尽管我们的 bug 报告中仅仅提到了 „” 引号对儿, 但是 ‚‘ 和 「」 都不在我们的 role ww tokens 中。远远不止的是, 某些左/右引号, 当它们交换位置后, 在引起字符串的时候也刚好能工作, 所以它们也应该在 qww 中起效。然而, 添加一整串额外的 tokens 和一整串其它的 actions 方法是相当不精彩的。有没有更好的方法?\n我们仔细看看我们的 tokens:\ntoken escape:sym\u0026lt;“ ”\u0026gt; {\u0026lt;?[“]\u0026gt;\u0026lt;quote=.LANG(\u0026#39;MAIN\u0026#39;,\u0026#39;quote\u0026#39;)\u0026gt;} sym\u0026lt;“ ”\u0026gt; 我们可以把它省略了 — 这里它的功能仅仅是作为一个名字。我们留下的是一个向前查看的 “ 引号还有 \u0026lt;quote=.LANG('MAIN','quote')\u0026gt;。所以我们可以向前查看所有的我们关心的开口引号并让 MAIN braid 接管所有的细节。\n所以, 让我们用这个单个 token 替换掉所有的引号处理 tokens:\ntoken escape:sym\u0026lt;\u0026#39;\u0026gt; {\u0026lt;?[\u0026#39; \u0026#34; ‘ ‚ ’ “ „ ” 「 ]\u0026gt;\u0026lt;quote=.LANG(\u0026#39;MAIN\u0026#39;,\u0026#39;quote\u0026#39;)\u0026gt;} 并且使用下面这个单个 action 替换掉所有的匹配 actions 方法:\nmethod escape:sym\u0026lt;\u0026#39;\u0026gt;($/) { make mark_ww_atom($\u0026lt;quote\u0026gt;.ast); } 编译! 运行我们的带有某些引号变体的代码:\n$ ./raku -e \u0026#39;.say for qww\u0026lt;„looks like” ‚we fixed‘ ｢this thing｣\u0026gt;\u0026#39; looks like we fixed this thing 精彩! 我们不仅让所有的引号都能正常工作, 还设法清理的存在的 tokens 和 actions 方法。现在所有我们需要做的就是对我们的修复做测试并且我们已经准备提交了。\n享用 bug 烤肉 Raku 官方测试套件 Roast 是在 Rakudo 内建目录中的 t/spec 中，如果它不存在, 仅仅运行 make spectest 就好了并且在它把 roast 仓库克隆到 t/spec 中后就中止它。我们需要找到在哪里插入我们的测试而 grep 是干那件事的好朋友:\nzoffix@VirtualBox:~/CPANPRC/rakudo/t/spec$ grep -R 'qww' . Binary file ./.git/objects/pack/pack-5bdee39f28283fef4b500859f5b288ea4eec20d7.pack matches ./S02-literals/allomorphic.t: my @wordlist = qqww[1 2/3 4.5 6e7 8+9i] Z (IntStr, RatStr, RatStr, NumStr, ComplexStr); ./S02-literals/allomorphic.t: isa-ok $val, Str, \u0026quot;'$val' from qqww[] is a Str\u0026quot;; ./S02-literals/allomorphic.t: nok $val.isa($wrong-type), \u0026quot;'$val' from qqww[] is not a $wrong-type.perl()\u0026quot;; ./S02-literals/allomorphic.t: my @wordlist = qqww:v[1 2/3 4.5 6e7 8+9i]; ./S02-literals/allomorphic.t: my @written = qqww:v[1 2/3 $num 6e7 8+9i ten]; ./S02-literals/allomorphic.t: is-deeply @angled, @written, \u0026quot;«...» is equivalent to qqww:v[...]\u0026quot;; ./S02-literals/quoting.t: is(qqww[$alpha $beta], \u0026lt;foo bar\u0026gt;, 'qqww'); ./S02-literals/quoting.t: for (\u0026lt;\u0026lt;$a b c\u0026gt;\u0026gt;, qqww{$a b c}, qqw{$a b c}).kv -\u0026gt; $i, $_ { ./S02-literals/quoting.t: is-deeply qww\u0026lt;a a ‘b b’ ‚b b’ ’b b‘ ’b b‘ ’b b’ ‚b b‘ ‚b b’ “b b” „b b” ./S02-literals/quoting.t: 'fancy quotes in qww work just like regular quotes'; ./integration/advent2014-day16.t: for flat qww/ foo bar 'first second' / Z @a -\u0026gt; $string, $result { 看起来 S02-literals/quoting.t 是它的一个好地方。打开那个文件, 在它的顶部, 通过我们添加的测试的数量来增加 plan 的数量 — 在这个例子中仅仅增加一条就好了。然后滚动到底部并创建一个 block 块, 前面添加一个注释, 并为我们正修复的 bug 报告引用那个 RT 标签数字。\n在文件里面, 我们使用 is-deeply 测试函数, 它使用 eqv 操作符语义来做测试。我们会给它一个带有完整引号串的 qww\u0026lt;\u0026gt; 行并告诉它我们所期望返回的项目列表。还要写下测试描述:\n# RT #128304 { is-deeply qww\u0026lt;a a ‘b b’ ‚b b’ ’b b‘ ’b b‘ ’b b’ ‚b b‘ ‚b b’ “b b” „b b” ”b b“ ”b b“ ”b b” „b b“ „b b” ｢b b｣ ｢b b｣\u0026gt;, (\u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, |(\u0026#39;b b\u0026#39; xx 16)), \u0026#39;fancy quotes in qww work just like regular quotes\u0026#39;; } 返回到 Rakudo checkout, 运行修改后的测试并保证它通过:\n$ make t/spec/S02-literals/quoting.t # \u0026lt;lots of output\u0026gt; All tests successful. Files=1, Tests=185, 3 wallclock secs ( 0.03 usr 0.01 sys + 2.76 cusr 0.11 csys = 2.91 CPU) Result: PASS 漂亮。提交测试 bug 修复好了并且把它们送走! 我们做到了!\n结论 当我们在修复 Raku 中的解析 bugs 的时候, 把程序减少到能重新产生那个 bug 的最小部分然后使用 --target=parse 命令行参数, 得到解析树的输出, 找到所匹配的那个 tokens。statementlist\n然后, 在 src/Raku/Grammar.nqp 中跟随这些 tokens, 它也继承自 NQP 的 src/HLL/Grammar.nqp 。 与位于 src/Raku/Actions.nqp 中的 actions 类协作, 跟随着代码找出正在做什么并期望找出问题出现在什么位置。\n修复它。测试它。发布它。\n充满了乐趣。\nhttp://raku.party/post/Perl-6-Core-Hacking-Grammatical-Babble\n","permalink":"https://ohmyweekly.github.io/notes/2016-01-01-raku-core-hacking-grammatical-babble/","tags":["grammar"],"title":"Raku 核心魔改: Grammar 的胡言乱语"},{"categories":["rakulang"],"contents":"对于 Raku 程序员, .rotor 是一个强大的列表操作工具。\n分段 最简单的, .rotor 接收一个整数 $number 并把列表分成多个子列表, 每个子列表含有 $number 个元素:\nsay \u0026lt;a b c d e f g h\u0026gt;.rotor: 3 # ((a b c) (d e f)) 我们有一个含有 8 个元素的列表, 我们在该列表上调用接收参数 3 的 .rotor 方法, 它返回 2 个列表, 每个列表中含有 3 个元素。不包括原列表中的最后 2 个元素, 因为它们没有组成一个完整的3个元素的列表。然而它们可以被包含进来, 使用 :partial 具名参数设置为 True:\nsay \u0026lt;a b c d e f g h\u0026gt;.rotor: 3, :partial # ((a b c) (d e f) (g h)) say \u0026lt;a b c d e f g h\u0026gt;.rotor: 3, :partial(True) # ((a b c) (d e f) (g h)) say \u0026lt;a b c d e f g h\u0026gt;.rotor: 3, :partial(False) # ((a b c) (d e f)) 下面应用一下我们刚刚学到的。把字符串分成列宽相等的几段:\n\u0026#34;foobarberboorboozebazmeow\u0026#34;.comb.rotor(10, :partial)».join».say; # foobarberb # oorboozeba # zmeow 分行然后每行前面添加 4 个空格:\n\u0026#34;foobarberboorboozebazmeow\u0026#34;.comb.rotor(10, :partial)».join».indent(4)».say; # foobarberb # oorboozeba # zmeow 但是这最好被写为:\n\u0026#34;foobarberboorboozebazmeow\u0026#34;.comb(10)».say 注意缝隙 假设你正在接受输入: 你得到一个单词, 它的法语翻译和它的西班牙语翻译, 等一堆单词。你只想输出特定语言, 所以我们需要在我们的列表中跳过某些项。 .rotor 来拯救来了!\n指定一对儿(Pair)整数作为 rotor 的参数会让每个列表中含有 $key 个元素, 每个列表之间有 $value 个空隙。看例子更简单一些:\nsay ^10 .rotor: 3 =\u0026gt; 1, :partial; # OUTPUT: ((0 1 2) (4 5 6) (8 9)) say ^10 .rotor: 2 =\u0026gt; 2, :partial; # OUTPUT: ((0 1) (4 5) (8 9)) 第一个例子我们把缝隙设置为 1, 第二个例子我们把缝隙增加为 2。\nenum \u0026lt;English French Spanish\u0026gt;; say join \u0026#34;\u0026#34;, \u0026lt;Good Bon Buenos morning matin días\u0026gt;[French..*].rotor: 1 =\u0026gt; 2; # OUTPUT: Bon matin 其中 [French..*] 意思为 [1..*], 例子中 French 被枚举化为整数 1。\n重叠 当我们让缝隙变为负数的时候, 分段的列表中就会有元素重叠:\nsay \u0026lt;a a b c c c d\u0026gt;.rotor: 2 =\u0026gt; -1 # OUTPUT: ((a a) (a b) (b c) (c c) (c c) (c d)) say \u0026lt;a a b c c c d\u0026gt;.rotor(2 =\u0026gt; -1).map: {$_[0] eq $_[1] ?? \u0026#34;same\u0026#34; !! \u0026#34;different\u0026#34;} # OUTPUT: (same different different same same different) 全力以赴 .rotor 不单单只能接受单个 Int 值或 Pair, 你可以指定额外的 Int 或 Pairs 位置参数来把列表分成不同尺寸大小的子列表, 列表之间的缝隙也不同。下面以一个日志文件为例:\nIP: 198.0.1.22 Login: suser Time: 1454017107 Resource: /report/accounting/x23gs Input: x=42,y=32 Output: success =================================================== IP: 198.0.1.23 Login: nanom Time: 1454027106 Resource: /report/systems/boot Input: mode=standard Output: success 每段之间有一行双划线。\n我们想这样输出: Header 里包含 IP, Login, Time, Resource; Operation 里包含 Resource, Input, Output。\nfor \u0026#39;report.txt\u0026#39;.IO.lines».indent(4).rotor( 4 =\u0026gt; -1, 3 =\u0026gt; 1 ) -\u0026gt; $head, $op { .say for \u0026#34;Header:\u0026#34;, |$head, \u0026#34;Operation:\u0026#34;, |$op, \u0026#39;\u0026#39;; } 输出:\nHeader: IP: 198.0.1.22 Login: suser Time: 1454017107 Resource: /report/accounting/x23gs Operation: Resource: /report/accounting/x23gs Input: x=42,y=32 Output: success Header: IP: 198.0.1.23 Login: nanom Time: 1454027106 Resource: /report/systems/boot Operation: Resource: /report/systems/boot Input: mode=standard Output: success 先是 4 个元素一块, 缝隙为 -1(有重叠), 然后是 3 个元素一块, 缝隙为 1。这就在每个分段的列表中包含了 Resource 字段。因为 $op 和 $head 是列表, 我们使用管道符号 | 来展平列表。\n记住, 你提供给 .rotor 方法的模式可以动态地生成! 这儿我们使用 sin 函数来生成:\nsay ^92 .rotor( (0.2, 0.4 ... 3).map: (10 * *.sin).Int # pattern we supply to .rotor ).join: \u0026#34;\\n\u0026#34;\u0026#39;输出:\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 再举个例子:\n我现在想要将同类的序列（字符串）进行合并，比如有这样一个文件：\n\u0026gt;seq-1A GACACAGTCACCCGAGCCT \u0026gt;seq-1B TCAATCAATACTGAAGCGA \u0026gt;seq-1C AAAACTAGTCGAGAAGAGAG \u0026gt;seq-1D CGTGGAAAACTCCAG \u0026gt;seq-2A TAAAAGGCGTTCATTGGATATTTC \u0026gt;seq-2B ACTGGCAGTGCATCC 我想要进行合并 得到这样的结果：\n\u0026gt;seq-1 GACACAGTCACCCGAGCCTTCAATCAATACTGAAGCGAAAAACTAGTCGAGAAGAGAGCGTGGAAAACTCCAG \u0026gt;seq-2 TAAAAGGCGTTCATTGGATATTTCACTGGCAGTGCATCC 使用 rotor 来实现:\nmy %re; for \u0026#39;input.txt\u0026#39;.IO.lines».rotor(2, :partial) -\u0026gt; $header, $data { my $key = $header; $key ~~ s/\u0026lt;upper\u0026gt;$//; %re{$key} ~= $data; } for %re.kv -\u0026gt; $key, $value { say \u0026#34;$key\\n$value\u0026#34;; } 官网的例子 method rotor(*@cycle, Bool() :$partial) rotor 返回一个 list, 这个 list 的元素也是 list, 其中每个子列表由调用者中的元素组成. 在最简单的情况下, @cycle 只包含一个整数, 这时调用者列表被分割为多个子列表, 每个子列表中的元素尽可能多的为那个整数指定的个数. 如果 :$partial 为 True, 不够分的最后那部分也会被包括进去, 即使它不满足长度的要求:\nsay (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor(3).join(\u0026#39;|\u0026#39;); # a b c|d e f say (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor(3, :partial).join(\u0026#39;|\u0026#39;); # a b c|d e f|g h 如果 @cycle 的元素是一个 /type/Pair, 则 Pair 的键指定了所返回子列表的长度(即每个子列表中含有的元素数), Pair 的键值指定两个列表之间的间隙; 负的间隙会产生重叠:\nsay (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor(2 =\u0026gt; 1).join(\u0026#39;|\u0026#39;); # a b|d e|g h say (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor(3 =\u0026gt; -1).join(\u0026#39;|\u0026#39;); # a b c|c d e|e f g my $pair = 2 =\u0026gt; 1; my $key = $pair.key; my $value = $pair.value; say (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor($key =\u0026gt; $value).join(\u0026#39;|\u0026#39;) # a b|d e|g h 如果 @cycle 的元素个数大于 1 时, rotor 会按 @cycle 中的元素依次循环调用者列表, 得到每个子列表:\nsay (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor(2, 3).join(\u0026#39;|\u0026#39;); # a b|c d e|f g say (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor(1 =\u0026gt; 1, 3).join(\u0026#39;|\u0026#39;); # a|c d e|f 组合多个循环周期和 :partial 也有效:\nsay (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor(1 =\u0026gt; 1, 3 =\u0026gt; -1, :partial).join(\u0026#39;|\u0026#39;); # a|c d e|e|g h 注意, 从 rotor 函数返回的一列列表们赋值给一个变量时会展开为一个数组:\nmy @maybe_lol = (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor(2 =\u0026gt; 1); @maybe_lol.raku.say; # [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;h\u0026#34;]\u0026lt;\u0026gt; 这可能不是你想要的, 因为 rotor 之后的输出看起来是这样的:\nsay (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor(2 =\u0026gt; 1).raku; # ((\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;), (\u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;), (\u0026#34;g\u0026#34;, \u0026#34;h\u0026#34;)) 要强制返回列表的列表, 使用绑定而非赋值:\nmy @really_lol := (\u0026#39;a\u0026#39;..\u0026#39;h\u0026#39;).rotor(2 =\u0026gt; 1); @really_lol.raku.say; # ((\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;), (\u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;), (\u0026#34;g\u0026#34;, \u0026#34;h\u0026#34;)) ","permalink":"https://ohmyweekly.github.io/notes/2015-07-14-the-king-of-list-manipulation/","tags":["rotor"],"title":"rotor: 列表操作之王"},{"categories":["rakulang"],"contents":"就像你想的那样, 在类的定义中可以声明和定义方法。你期望不高的甚至文档中都很少提及是用 my 关键字声明的免费浮点方法。现在为什么你想要:\nmy method foo(SomeClass:D:) { self } 明显的答案是元对象协议中的 add_method 方法, 在 Rakudo 里你能找到它：\nsrc/core/Bool.pm 32: Bool.^add_method(\u0026#39;pred\u0026#39;, my method pred() { Bool::False }); 33: Bool.^add_method(\u0026#39;succ\u0026#39;, my method succ() { Bool::True }); 35: Bool.^add_method(\u0026#39;enums\u0026#39;, my method enums() { self.^enum_values }); 这种方法还有另外一种更诡异的用法。你可能很想知道在链式方法调用中究竟发生了什么。我们可以扯开最上面的那个表达式并插入一个短的变量, 输出我们的调试, 并且继续链式调用。好的名字很重要并且把它们浪费在一个短变量上没有必要。\n\u0026lt;a b c\u0026gt;.\u0026amp;(my method ::(List:D) { dd self; self } ).say; # output # (\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;) # (a b c) 没有显式调用我们就不能没有名字, 因为 Raku 不允许我们这样做, 所以我们使用了空的作用域 :: 以使解析器高兴。使用一个合适的调用, 我们就不需要它了。还有, 那个匿名方法不是 List 中的一员。我们需要使用后缀 .\u0026amp; 来调用它。如果我们需要多次使用那个方法我们可以把它拉出来并给它一个名字。\nmy multi method debug(List:D:) { dd self; self }; \u0026lt;a b c\u0026gt;.\u0026amp;debug.say; # output (\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;) (a b c) 或者, 如果我们想允许回调的话, 我们可以把它作为默认参数赋值。\nsub f(@l, :\u0026amp;debug = my method (List:D:){self}) { @l.\u0026amp;debug.say }; f \u0026lt;a b c\u0026gt;, debug =\u0026gt; my method ::(List:D){dd self; self}; # output #(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;) # (a b c) 在 Raku 中基本上所有的东西都是类, 包括方法。 如果它是类它可以是一个对象并且我们能在我们喜欢的任何地方溜进去。\nhttps://gfldex.wordpress.com/2016/07/20/sneaky-methods/\n","permalink":"https://ohmyweekly.github.io/notes/2015-05-01-sneaky-methods/","tags":["method"],"title":"Sneaky Methods"},{"categories":["rakulang"],"contents":"Protocol 在 Swift 中是一组方法和属性的集合, 可用于代码复用。 Raku 中有与之类似的结构, 叫做 Role, 下面转换一个 Swift 的 Protocol 为 Raku 的 Role, 把部门人员的相关信息打印为一个表格:\nProtocol in Swift 　import UIKit func padding(amount: Int) -\u0026gt; String { var paddingString = \u0026#34;\u0026#34; for _ in 0..\u0026lt;amount { paddingString += \u0026#34; \u0026#34; } return paddingString } // 协议 protocol TabularDataSource { var numberOfRows: Int { get } var numberOfColumns: Int { get } func labelForRow(row: Int) -\u0026gt; String // 行标签 func labelForColumn(column: Int) -\u0026gt; String // 列标签 func itemForRow(row: Int, column: Int) -\u0026gt; Int // 表格中的单元格 } struct Person { let name: String let age: Int let yearsOfExperience: Int } // 让 **Department** 遵守 **TabularDataSource**协议 struct Department: TabularDataSource { let name: String var people = [Person]() init(name: String) { self.name = name } mutating func addPerson(person: Person) { people.append(person) } // 实现协议中声明的属性和方法 var numberOfRows: Int { return people.count } var numberOfColumns: Int { return 2 } func labelForRow(row: Int) -\u0026gt; String { return people[row].name } func labelForColumn(column: Int) -\u0026gt; String { switch column { case 0: return \u0026#34;Age\u0026#34; case 1: return \u0026#34;Years of Experence\u0026#34; default: fatalError(\u0026#34;Invalid column!\u0026#34;) } } func itemForRow(row: Int, column: Int) -\u0026gt; Int { let person = people[row] // 指定的行 switch column { case 0: return person.age case 1: return person.yearsOfExperience default:fatalError(\u0026#34;Invalid column!\u0026#34;) } } } var deparment = Department(name: \u0026#34;Engineering\u0026#34;) deparment.addPerson(Person(name: \u0026#34;Joe\u0026#34;, age: 30, yearsOfExperience: 6)) deparment.addPerson(Person(name: \u0026#34;Karen\u0026#34;, age: 40, yearsOfExperience: 18)) deparment.addPerson(Person(name: \u0026#34;Fred\u0026#34;, age: 50, yearsOfExperience: 20)) // 传入一个数据源 func printTable(dataSource: TabularDataSource) { let rowLabels = (0 ..\u0026lt; dataSource.numberOfRows).map { dataSource.labelForRow($0) } let columnLabels = (0 ..\u0026lt; dataSource.numberOfColumns).map { dataSource.labelForColumn($0) } // 创建一个数组存储每个行标签的宽度 let rowLabelWidths = rowLabels.map { $0.characters.count } // 限定行标签的最大长度 guard let maxRowLabelWidth = rowLabelWidths.maxElement() else { return } // 创建第一行, 包含列标题 var firstRow = padding(maxRowLabelWidth) + \u0026#34; |\u0026#34; // 跟踪每列的宽度 var columnWidths = [Int]() for columnLabel in columnLabels { let columnHeader = \u0026#34; \\(columnLabel) |\u0026#34; firstRow += columnHeader columnWidths.append(columnHeader.characters.count) } print(firstRow) for i in 0 ..\u0026lt; dataSource.numberOfRows { let paddingAmount = maxRowLabelWidth - rowLabelWidths[i] var out = rowLabels[i] + padding(paddingAmount) + \u0026#34; |\u0026#34; for j in 0 ..\u0026lt; dataSource.numberOfColumns { let item = dataSource.itemForRow(i, column: j) let itemString = \u0026#34; \\(item) |\u0026#34; let paddingAmount = columnWidths[j] - itemString.characters.count out += padding(paddingAmount) + itemString } print(out) } } printTable(deparment) 其中的计算属性在 Raku 中可以使用重写属性的方法来完成。\nRole in Raku sub padding(Int $amount) { my $paddingString = \u0026#34;\u0026#34;; $paddingString ~= \u0026#34;\u0026#34; for 0 ..^ $amount; return $paddingString; } # 声明一个接口, 只定义了方法和属性, 没有做实现 role TabularDataSource { has $.numberOfRows is rw; has $.numberOfColumns is rw; method labelForRow(Int $row) { ... } method labelForColumn(Int $column) { ... } method itemForRow(Int $row, Int $column) { ... } } class Person { has Str $.name; has Int $.age; has Int $.yearsOfExperience; } class Department does TabularDataSource { has $.name; has @.people; method new(Str $name) { self.bless(name =\u0026gt; $name); } # 实现接口中的方法 # 重写方法 has $.numberOfRows 其实是 has $!numberOfRows 加上 method numberOfRows() { ... } 方法。 method numberOfRows() { return @!people.elems; } method numberOfColumns() { return 2; } method addPerson(Person $person) is rw { @!people.append($person); } # 如果类遵守了某个 role 但是未实现其中的方法, 则会报错如下: # Method \u0026#39;labelForRow\u0026#39; must be implemented by Department because it is required by a role method labelForRow(Int $row) { return @!people[$row].name; } method labelForColumn(Int $column) { given $column { when 0 { return \u0026#34;Age\u0026#34; } when 1 { return \u0026#34;Years of Experence\u0026#34; } default { die(\u0026#34;Invalid column!\u0026#34;)} } } method itemForRow(Int $row, Int $column) { my $person = @!people[$row]; given $column { when 0 { return $person.age } when 1 { return $person.yearsOfExperience } default { die(\u0026#34;Invalid column\u0026#34;) } } } } my $department = Department.new(\u0026#34;Engineering\u0026#34;); $department.addPerson(Person.new(name =\u0026gt; \u0026#34;Joe\u0026#34;, age =\u0026gt; 30, yearsOfExperience =\u0026gt; 6)); $department.addPerson(Person.new(name =\u0026gt; \u0026#34;Karen\u0026#34;, age =\u0026gt; 40, yearsOfExperience =\u0026gt; 18)); $department.addPerson(Person.new(name =\u0026gt; \u0026#34;Fred\u0026#34;, age =\u0026gt; 50, yearsOfExperience =\u0026gt; 20)); sub printTable(TabularDataSource $dataSource) { my @rowLabels = (0 ..^ $dataSource.numberOfRows ).map: { $dataSource.labelForRow($_)}; my @columnLabels = (0 ..^ $dataSource.numberOfColumns).map: {$dataSource.labelForColumn($_)}; my @rowLabelWidths = @rowLabels.map: {$_.chars}; my $maxRowLabelWidth = @rowLabelWidths.max // return; my $firstRow = padding($maxRowLabelWidth) ~ \u0026#34;|\u0026#34;; my @columnWidths; for @columnLabels -\u0026gt; $columnLabel { my $columnHeader = \u0026#34;$columnLabel|\u0026#34;; $firstRow ~= $columnHeader; @columnWidths.append($columnHeader.chars); } say($firstRow); for 0 ..^ $dataSource.numberOfRows -\u0026gt; $i { my $paddingAmount = $maxRowLabelWidth - @rowLabelWidths[$i]; my $out = @rowLabels[$i] ~ padding($paddingAmount) ~ \u0026#34;|\u0026#34;; for 0 ..^ $dataSource.numberOfColumns -\u0026gt; $j { my $item = $dataSource.itemForRow($i, $j); my $itemString = \u0026#34;$item|\u0026#34;; my $paddingAmount = @columnWidths[$j] - $itemString.chars; $out ~= padding($paddingAmount) ~ $itemString; } say($out); } } printTable($department); role 中的 { ... } 是 yadayada 操作符, 起占位作用, 表示方法会在别处实现。类中的方法同样也可以这样声明。\n最后输出:\n | Age | Years of Experence | Joe | 30 | 6 | Karen | 40 | 18 | Fred | 50 | 20 | ","permalink":"https://ohmyweekly.github.io/notes/2016-01-21-protocol-in-swift-and-roles-in-raku/","tags":["roles"],"title":"Swift 中的 protocol 和 Raku 中的 roles 一例"},{"categories":["rakulang"],"contents":"下标副词 为了使切片下标返回除了值以外的其它东西，那么给下标(subscript)添加合适的副词。\n@array = \u0026lt;A B\u0026gt;; @array[0,1,2]; # returns \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, (Any) @array[0,1,2] :p; # returns 0 =\u0026gt; \u0026#39;A\u0026#39;, 1 =\u0026gt; \u0026#39;B\u0026#39; @array[0,1,2] :kv; # returns 0, \u0026#39;A\u0026#39;, 1, \u0026#39;B\u0026#39; @array[0,1,2] :k; # returns 0, 1 @array[0,1,2] :v; # returns \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39; %hash = (:a\u0026lt;A\u0026gt;, :b\u0026lt;B\u0026gt;); %hash\u0026lt;a b c\u0026gt;; # returns \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, (Any) %hash\u0026lt;a b c\u0026gt; :p; # returns a =\u0026gt; \u0026#39;A\u0026#39;, b =\u0026gt; \u0026#39;B\u0026#39; %hash\u0026lt;a b c\u0026gt; :kv; # returns \u0026#39;a\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;B\u0026#39; %hash\u0026lt;a b c\u0026gt; :k; # returns \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39; %hash\u0026lt;a b c\u0026gt; :v; # returns \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39; 如果副词为真，那么这些副词形式都会清除不存在的条目；如果为假的话，就会留下不存在的项，就像普通的切片那样。所以：\n@array[0,1,2] :!p; # returns 0 =\u0026gt; \u0026#39;A\u0026#39;, 1 =\u0026gt; \u0026#39;B\u0026#39;, 2 =\u0026gt; (Any) %hash\u0026lt;a b c\u0026gt; :!kv; # returns \u0026#39;a\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;c\u0026#39;, (Any) 同样地，\nmy ($a,$b,$c) = %hash\u0026lt;a b c\u0026gt; :delete; 删除那些条目并顺道返回它们。这种形式能够工作是因为下标是顶端的在前的操作符。如果某些其它的操作符的优先级比处于顶端的逗号操作符的优先级紧凑，那么你必须用括号括起它或强制为列表上下文：\n1 + (%hash{$x} :delete); $x = (%hash{$x} :delete); ($x) = %hash{$x} :delete; 只有在副词为真的时候元素才会被删除。而 :!delete 本质上是一个空操作；你可以基于传递的诸如 :delete($kill'em) 标记顺带有条件地删除条目。在任何一种情况下，被删除的值会被返回。\n你也可以执行存在性测试，要么测试单个条目是否存在，要么测试条目的连接是否存在：\nif %hash\u0026lt;foo\u0026gt; :exists {...} if %hash{any \u0026lt;a b c\u0026gt;} :exists {...} if %hash{all \u0026lt;a b c\u0026gt;} :exists {...} if %hash{one \u0026lt;a b c\u0026gt;} :exists {...} if %hash{none \u0026lt;a b c\u0026gt;} :exists {...} 把 :exists 副词和一组切片结果的布尔值列表结合起来使用，你也可以用类型的语义这样使用：\nif any %hash\u0026lt;a b c\u0026gt; :exists {...} if all %hash\u0026lt;a b c\u0026gt; :exists {...} if one %hash\u0026lt;a b c\u0026gt; :exists {...} if none %hash\u0026lt;a b c\u0026gt; :exists {...} 你可以使用 :!exists 来测试不存在。这特别便捷因为优先级规则让 !%hash\u0026lt;a\u0026gt; :exists 把 :exists 应用到前缀 ! 上。 %hash\u0026lt;a\u0026gt; :!exists 没有那个问题。\n组合下标副词 像调用中得具名参数那样，下标中处理多个副词是没有顺序之分的。有些组合有意义，例如：\n%a = %b{@keys-to-extract} :delete :p; # same as :p :delete 会把给定的键分片到另外一个散列中。而\n@actually-deleted = %h{@keys-to-extract} :delete :k; # same as :k :delete 会返回真正从散列中删除的键。\n只指定返回类型的副词，不能被组合，因为诸如 :kv :p、或 :v :k 就没有意义。\n下面的这些副词组合被看做是合法的：\n:delete :kv delete, return key/values of actually deleted keys :delete :!kv delete, return key/values of all keys attempted :delete :p delete, return pairs of actually deleted keys :delete :!p delete, return pairs of all keys attempted :delete :k delete, return actually deleted keys :delete :!k delete, return all keys attempted to delete :delete :v delete, return values of actually deleted keys :delete :!v delete, return values of all keys attempted :delete :exists delete, return Bools indicating keys existed :delete :!exists delete, return Bools indicating keys did not exist :delete :exists :kv delete, return list with key,True for key existed :delete :!exists :kv delete, return list with key,False for key existed :delete :exists :!kv delete, return list with key,Bool whether key existed :delete :!exists :!kv delete, return list with key,!Bool whether key existed :delete :exists :p delete, return pairs with key/True for key existed :delete :!exists :p delete, return pairs with key/False for key existed :delete :exists :!p delete, return pairs with key/Bool whether key existed :delete :!exists :!p delete, return pairs with key/!Bool whether key existed :exists :kv return pairs with key,True for key exists :!exists :kv return pairs with key,False for key exists :exists :!kv return pairs with key,Bool for key exists :!exists :!kv return pairs with key,!Bool for key exists :exists :p return pairs with key/True for key exists :!exists :p return pairs with key/False for key exists :exists :!p return pairs with key/Bool for key exists :!exists :!p return pairs with key/!Bool for key exists ","permalink":"https://ohmyweekly.github.io/notes/2015-05-18-subscripts-adverbs/","tags":["subscript","adverb"],"title":"下标副词"},{"categories":["rakulang"],"contents":"Into the Breach 假设我们 的日志文件中有一个时间戳 2016-02-06T14:36+02:00, 我们使用智能匹配:\nsay $logfile ~~ /2016-02-06T14:36+02:00/; 这会报错:\n Unrecognized regex metacharacter - (must be quoted to match literally)\n 在 Raku 的正则表达式中任何非字母数字字符(\u0026lsquo;a\u0026rsquo;..\u0026lsquo;z\u0026rsquo;, \u0026lsquo;A\u0026rsquo;..\u0026lsquo;Z\u0026rsquo;, 0..9)都必须用引号引起来:\nsay $logfile ~~ /2016\u0026#39;-\u0026#39;02\u0026#39;-\u0026#39;06T14\u0026#39;:\u0026#39;36\u0026#39;+\u0026#39;02\u0026#39;:\u0026#39;00/; 现在我们得到等价的奇怪的表达式:\n｢2016-01-29T13:25+01:00｣ 这仅仅告诉我们, ~~ 智能匹配操作符匹配了一些文本, 这就是它匹配到的文本。｢｣ 是日语引号标记, 故意和剩余的文本区分开来。\n在 Raku 中, 默认打印出带有明确标记的匹配对象, 它准确地告诉你匹配从哪里开始, 到哪里结束。\n归纳 我们想让该正则表达式更具普遍性, 例如匹配 2016 年的日志:\nsay $logfile ~~ /2015|2016\u0026#39;-\u0026#39;02\u0026#39;-\u0026#39;06T14\u0026#39;:\u0026#39;36\u0026#39;+\u0026#39;02\u0026#39;:\u0026#39;00/; 但是这还会匹配到我们不想要的东西, 例如 \u0026lsquo;/post/2015/02\u0026rsquo; 或者甚至 \u0026lsquo;/number/120153\u0026rsquo;。因为 |的优先级没有字符间的连接优先级高。所以:\nsay $logfile ~~ /[2015|2016]\u0026#39;-\u0026#39;02\u0026#39;-\u0026#39;06T14\u0026#39;:\u0026#39;36\u0026#39;+\u0026#39;02\u0026#39;:\u0026#39;00/; 问题解决, 但是我们想匹配 \u0026lsquo;[ 1997 | 1998 | 1999 | 2000\u0026hellip; 2015 ]\u0026rsquo; 这些呢？\nLearning Shorthnd 匹配4位数字的年份好了:\nsay $logfile ~~ /\\d\\d\\d\\d\u0026#39;-\u0026#39;02\u0026#39;-\u0026#39;06T14.../; 其它需要数字的地方也可以使用 \\d 这种便捷形式的数字:\nsay $logfile ~~ /\\d\\d\\d\\d\u0026#39;-\u0026#39;\\d\\d- \\d\\dT \\d\\d\u0026#39;:\u0026#39;\\d\\d\u0026#39;+\u0026#39;\\d\\d\u0026#39;:\u0026#39;\\d\\d/; '+' \u0026lt;digits\u0026gt; : \u0026lt;digits\u0026gt; 只会匹配 +01 和 +12 之间的时区, 还有其它在 -11 到 -01 之间的时区, 所以我们使用 | 来匹配 \u0026lsquo;+\u0026rsquo; 或 \u0026lsquo;-\u0026rsquo;, 像这样:\nsay $logfile ~~ /\\d\\d\\d\\d\u0026#39;-\u0026#39;\\d\\d- \\d\\dT \\d\\d\u0026#39;:\u0026#39;\\d\\d[\u0026#39;+\u0026#39;|\u0026#39;-\u0026#39;]\\d\\d\u0026#39;:\u0026#39;\\d\\d/; 基本正确了, 但是由于历史原因, 时区还能是一个字母 Z, 所以, 还有一处要修改:\nsay $logfile ~~ /\\d\\d\\d\\d\u0026#39;-\u0026#39;\\d\\d- \\d\\dT \\d\\d\u0026#39;:\u0026#39;\\d\\d[[\u0026#39;+\u0026#39;|\u0026#39;-\u0026#39;]\\d\\d\u0026#39;:\u0026#39;\\d\\d|Z ]/; 重构 但是那个 [ '+' ... Z ] 表达式太长了, 能重构就更好了。regex 对象来拯救我们了, 它帮助我们清理代码。\nregex 对象看起来很像匹配表达式, 除了它使用花括号来告诉从哪开始, 到哪结束:\nmy regex Timezone {Z |[\u0026#39;+\u0026#39;|\u0026#39;-\u0026#39;]\\d\\d\u0026#39;:\u0026#39;\\d\\d}; say $logfile ~~ /\\d\\d\\d\\d\u0026#39;-\u0026#39;\\d\\d\u0026#39;-\u0026#39;\\d\\dT \\d\\d\u0026#39;:\u0026#39;\\d\\d\u0026lt;Timezone\u0026gt;/; \u0026lt;..\u0026gt; 从外表上看把重构后的表达式和主文本分开了, 而让 Timezone 表达式分离意味着我们能在代码中的任何地方使用它了。事实上我们可以重构其它的正则:\nmy regex Date {\\d\\d\\d\\d\u0026#39;-\u0026#39;\\d\\d\u0026#39;-\u0026#39;\\d\\d}; my regex Time {\\d\\d\u0026#39;:\u0026#39;\\d\\d}; my regex Timezone {Z |[\u0026#39;+\u0026#39;|\u0026#39;-\u0026#39;]\\d\\d\u0026#39;:\u0026#39;\\d\\d}; say $logfile ~~ /\u0026lt;Date\u0026gt;T \u0026lt;Time\u0026gt;\u0026lt;Timezone\u0026gt;/; 让所有这些 \\d\\d 坐在一块儿有些碍眼, 所以我们再重构下:\nmy regex Integer {\\d+}; my regex Date {\u0026lt;Integer\u0026gt;\u0026#39;-\u0026#39;\u0026lt;Integer\u0026gt;\u0026#39;-\u0026#39;\u0026lt;Integer\u0026gt;}; my regex Time {\u0026lt;Integer\u0026gt;\u0026#39;:\u0026#39;\u0026lt;Integer\u0026gt;}; my regex Timezone {Z |[\u0026#39;+\u0026#39;|\u0026#39;-\u0026#39;]\u0026lt;Integer\u0026gt;\u0026#39;:\u0026#39;\u0026lt;Integer\u0026gt;}; say $logfile ~~ /\u0026lt;Date\u0026gt;T \u0026lt;Time\u0026gt;\u0026lt;Timezone\u0026gt;/; http://theperlfisher.blogspot.jp/2016/02/from-regular-expressions-to-grammars-pt.html\n","permalink":"https://ohmyweekly.github.io/notes/2015-04-27-from-regex-to-grammar-part-one/","tags":["regex","grammar"],"title":"从正则表达式到 Grammar(第一部分)"},{"categories":["rakulang"],"contents":"如果你是正则表达式新人(至少当它们用于 Raku 中时), 那我建议你从这个系列的第一部分开始。那些掌握了一定正则表达式的人可以跳过上周的文章。现在, 继续演示!\n上周轶事 我们开始开发一个接收诸如:\nvar a = 3; console.log(\u0026#34;Hey, did you konw a = \u0026#34; + a + \u0026#34;?\u0026#34;); Javascript 表达式的 Raku 编译器, 并把这段代码转换为 Rakudo 那样的编译器能运行的 Raku 代码。在我们开始之前, 想想转换后的 Raku 代码看起来是什么样的可能会是个好主意。如果你已经知道了 Perl 5, 那么你应该熟悉这样的代码。\nmy $a = 3; say \u0026#34;Hey, did you konw a = \u0026#34; ~ $a ~ \u0026#34;?\u0026#34;; 我们将需要确保我们的正则表达式捕获到了 Javascript 的要素。如果你还记得上一次, 我们使用这样一组正则表达式来捕获我们的文本:\nmy rule Number {\\d+}; my rule Variable {\\w+}; my rule String {\u0026#39;\u0026#34;\u0026#39;\u0026lt;-[\u0026#34; ]\u0026gt;+\u0026#39;\u0026#34;\u0026#39;}; my rule Assignment-Expression {var \u0026lt;Variable\u0026gt;\u0026#39;=\u0026#39;\u0026lt;Number\u0026gt;}; my rule Function-Call {console \u0026#39;.\u0026#39;log \u0026#39;(\u0026#39;\u0026lt;String\u0026gt;\u0026#39;+\u0026#39;\u0026lt;Variable\u0026gt;\u0026#39;+\u0026#39;\u0026lt;String\u0026gt;\u0026#39;)\u0026#39;}; say \u0026#39;var a = 3; console.log(\u0026#34;Hey, did you konw a = \u0026#34; + a + \u0026#34;?\u0026#34;);\u0026#39; ~~ rule { \u0026lt;Assignment-Expression\u0026gt; \u0026#39;;\u0026#39; \u0026lt;Function-Call\u0026gt; \u0026#39;;\u0026#39; }; 如果你把这段代码放到一个 Raku 源文件中并运行它, 那么它的输出第一次看起来可能会有点奇怪:\n｢var a = 3; console.log( \u0026quot;Hey, did you know a = \u0026quot; + a + \u0026quot;?\u0026quot; );｣ Assignment-Expression =\u0026gt; ｢var a = 3｣ Variable =\u0026gt; ｢a ｣ Number =\u0026gt; ｢3｣ Function-Call =\u0026gt; ｢console.log( \u0026quot;Hey, did you know a = \u0026quot; + a + \u0026quot;?\u0026quot; )｣ String =\u0026gt; ｢\u0026quot;Hey, did you know a = \u0026quot; ｣ Variable =\u0026gt; ｢a ｣ String =\u0026gt; ｢\u0026quot;?\u0026quot; ｣ 如果你愿意暂时忽略 「」 标记, 你会看到匹配被缩进了, 几乎像资源管理器窗口一样, \u0026lt;Assignment-Expression\u0026gt; 作为目录, Variable 和 Number 作为目录里面的文件。实际上, 那离真相不远了。当我看到这种结构时, 我发现使用一点添加的语法能帮助我们像这样来观察它:\n$/ =\u0026gt; ｢var a = 3; console.log( \u0026quot;Hey, did you know a = \u0026quot; + a + \u0026quot;?\u0026quot; );｣ \u0026lt;Assignment-Expression\u0026gt; =\u0026gt; ｢var a = 3｣ \u0026lt;Variable\u0026gt; =\u0026gt; ｢a ｣ \u0026lt;Number\u0026gt; =\u0026gt; ｢3｣ \u0026lt;Function-Call\u0026gt; =\u0026gt; ｢console.log( \u0026quot;Hey, did you know a = \u0026quot; + a + \u0026quot;?\u0026quot; )｣ \u0026lt;String\u0026gt; =\u0026gt; ｢\u0026quot;Hey, did you know a = \u0026quot; ｣ \u0026lt;Variable\u0026gt; =\u0026gt; ｢a ｣ \u0026lt;String\u0026gt; =\u0026gt; ｢\u0026quot;?\u0026quot; ｣ 这几乎让怎么打印出文本变得更容易, 并在我们的正则表达式中指出了一个小问题。我们来打印给变量 a 所赋的数字, 从这儿开始。第一行告诉我们目录的根, 或者匹配树是 $/。如果你在测试文件的末尾添加上 say $/; 并返回它, 那么你会看到整个表达式被打印出了 2 次。那一定意味着 $/ 就是整个匹配。\n每向下推进一层就是把 =\u0026gt; 箭头的左侧的东西添加到 $/ 的右边。把之前的 say 语句修改为 say $/\u0026lt;Assignment-Expression\u0026gt;;, 并看看输出发生了什么改变。它现在看起来应该像这样:\n｢var a = 3｣ Variable =\u0026gt; ｢a ｣ Number =\u0026gt; ｢3｣ 让我们把把标记(不可见)添加进来, 所以我们能知道到了哪里\u0026hellip;\n$/\u0026lt;Assignment-Expression\u0026gt; =\u0026gt; ｢var a = 3｣ \u0026lt;Variable\u0026gt; =\u0026gt; ｢a ｣ \u0026lt;Number\u0026gt; =\u0026gt; ｢3｣ 我们现在能看到我们的目标, 数字 3, 仅仅实在更下面的一层。和上次一样, 我们能够添加表达式左侧的东西, 所以我们就动手吧。\nsay $/\u0026lt;Assignment-Expression\u0026gt;\u0026lt;Number\u0026gt;; ｢3｣ 我们几乎得到我们想要的了。「」 挡道, 所以我们在这儿把值转换回数字。我把转换(cast)用引号扩起来, 因为它不是 C/C++ 程序员那样认为的\u0026quot;casting\u0026quot;。我们想做的大约等价于 sscanf(str,\u0026quot;%d\u0026quot;,\u0026amp;num), 但是在 Raku 中, 这个操作符更加简单:\nsay +$/\u0026lt;Assignment-Expression\u0026gt;\u0026lt;Number\u0026gt;; # 3 如果不深入更多细节, 那么 $/ 是一个里面藏着隐式数字、字符串和布尔值的对象。前面添加的 + 把隐藏在 $/ 对象中的数字显示出来了。\n从 Javascript 到 Perl 我们离从 Javascript 生成 Raku 代码不远了。让我们使用上面所学的开始我们的第一个语句, 赋值语句。\nsay \u0026#39;my $\u0026#39; ~ $/\u0026lt;Assignment-Expression\u0026gt;\u0026lt;Variable\u0026gt; ~ \u0026#39;= \u0026#39; ~ $/\u0026lt;Assignment-Expression\u0026gt;\u0026lt;Number\u0026gt; ~ \u0026#39;;\u0026#39;; my $a = 3; 我们仅仅使用了 7 行 Raku 就把代码从一种语言转换为另外一种语言。并且大部分的 Raku 代码都是可重用的, 因为字符串, 数字, 和 Javascript/C/Java 风格的变量名在大部分语言之间是通用的。\n上次, 我们学习了怎么使用正则表达式来创建匹配。这次我们学会了怎么使用我们说匹配到的东西, 还有怎么在 say 语句中找出我们想要的东西。 不可见的匹配标记相当有用, 我可能会写一个模块来把它们放回到匹配表达式中, 那应该不难。\n那个方案有一个问题, 如果我们看一下 \u0026lt;Function-Call\u0026gt; 匹配, 会很容易发现那个问题。\n$/\u0026lt;Function-Call\u0026gt; =\u0026gt; ｢console.log( \u0026#34;Hey, did you know a = \u0026#34; + a + \u0026#34;?\u0026#34; )｣ \u0026lt;String\u0026gt; =\u0026gt; ｢\u0026#34;Hey, did you know a = \u0026#34; ｣ \u0026lt;Variable\u0026gt; =\u0026gt; ｢a ｣ \u0026lt;String\u0026gt; =\u0026gt; ｢\u0026#34;?\u0026#34; ｣ 当我们写了 say $/\u0026lt;Function-Call\u0026gt;\u0026lt;String\u0026gt;; 时, 我们会获取哪个 \u0026lt;String\u0026gt;? 在你运行这段代码之前, 先猜测一下。会是第一个吗, 因为一旦匹配对象被创建, Raku 就不会把它替换掉? 会是最后一个吗, 因为最后一个\u0026quot;覆盖\u0026quot;了第一个? 编译器会仅仅\u0026quot;感到困惑\u0026quot;然后什么也不打印吗? 运行一下看看!\n它实际上以一个列表的形式把两个匹配都返回了, 所以你可以引用任何一个。 我们的不可见标记现在看起来长这样:\n$/\u0026lt;Function-Call\u0026gt; =\u0026gt; ｢console.log( \u0026quot;Hey, did you know a = \u0026quot; + a + \u0026quot;?\u0026quot; )｣ \u0026lt;String\u0026gt;[0] =\u0026gt; ｢\u0026quot;Hey, did you know a = \u0026quot; ｣ \u0026lt;Variable\u0026gt; =\u0026gt; ｢a ｣ \u0026lt;String\u0026gt;[1] =\u0026gt; ｢\u0026quot;?\u0026quot; ｣ 所以, 如果我们想打印第一个字符串, 我们可以写上 say $/\u0026lt;Function-Call\u0026gt;\u0026lt;String\u0026gt;[0]; 并得到含有时髦的日语标记的「\u0026ldquo;Hey, did you know a = \u0026quot; 」。幸运的是有一种便捷方式来避免那些日语标记, 就像数字 3 中的那样:\nsay ~$/\u0026lt;Function-Call\u0026gt;\u0026lt;String\u0026gt;[0]; \u0026#34;Hey, did you know a = \u0026#34; ~ 操作符使匹配字符串化, 就像 + 让返回的匹配数字化一样。所以你可能自己把最后一行写作:\nsay \u0026#39;say \u0026#39; ~ $/\u0026lt;Function-Call\u0026gt;\u0026lt;String\u0026gt;[0] ~ \u0026#39;~ \u0026#39; \u0026#39;$\u0026#39; ~ $/\u0026lt;Function-Call\u0026gt;\u0026lt;Variable\u0026gt; ~ \u0026#39;~ \u0026#39; $\u0026lt;Function-Call\u0026gt;\u0026lt;String\u0026gt;[1] ~ \u0026#39;;\u0026#39;; say \u0026#34;Hey, did you know a = \u0026#34; ~ $a ~ \u0026#34;?\u0026#34;; 我们已经把我们的两行 Javascript 代码编译成 Raku 代码了。\n重构 现在已经能工作了, 但是有很多重复。目前我们得到是:\nmy rule Variable {\\w+}; my rule String {\u0026#39;\u0026#34;\u0026#39;\u0026lt;-[\u0026#34; ]\u0026gt;+\u0026#39;\u0026#34;\u0026#39;}; my rule Assignment-Expression {var \u0026lt;Variable\u0026gt;\u0026#39;=\u0026#39;\u0026lt;Number\u0026gt;}; my rule Function-Call {console \u0026#39;.\u0026#39;log \u0026#39;(\u0026#39;\u0026lt;String\u0026gt;\u0026#39;+\u0026#39;\u0026lt;Variable\u0026gt;\u0026#39;+\u0026#39;\u0026lt;String\u0026gt;\u0026#39;)\u0026#39;}; \u0026#39;var a = 3; console.log( \u0026#34;Hey, did you know a = \u0026#34; + a + \u0026#34;?\u0026#34; );\u0026#39; ~~ rule { \u0026lt;Assignment-Expression\u0026gt; \u0026#39;;\u0026#39; \u0026lt;Function-Call\u0026gt; \u0026#39;;\u0026#39; } say \u0026#39;my $\u0026#39; ~ $/\u0026lt;Assignment-Expression\u0026gt;\u0026lt;Variable\u0026gt; ~ \u0026#39;= \u0026#39; ~ $/\u0026lt;Assignment-Expression\u0026gt;\u0026lt;Number\u0026gt; ~ \u0026#39;;\u0026#39;; say \u0026#39;say \u0026#39; ~ $/\u0026lt;Function-Call\u0026gt;\u0026lt;String\u0026gt;[0] ~ \u0026#39;~ $\u0026#39; ~ $/\u0026lt;Function-Call\u0026gt;\u0026lt;Variable\u0026gt; ~ \u0026#39;~ \u0026#39; ~ $/\u0026lt;Function-Call\u0026gt;\u0026lt;String\u0026gt;[1] ~ \u0026#39;;\u0026#39;; 那些 rules 看起来相当好, \u0026lt;String\u0026gt; 和 \u0026lt;Variable\u0026gt; 的重复也是不可避免的。 但是看看 say 语句, 你会看到 \u0026lt;Assignment-Expression\u0026gt; 和 \u0026lt;Function-Call\u0026gt; 重复了自身好几次。避免这种重复的一种方法是创建一个临时变量, 但是那可能会变得丑陋。\nmy $assignment-expression = $/\u0026lt;Assignment-Expression\u0026gt;; say \u0026#39;my $\u0026#39; ~ $assignment-expression\u0026lt;Variable\u0026gt; ~ \u0026#39;= \u0026#39; ~ $assignment-expression\u0026lt;Number\u0026gt; ~ \u0026#39;;\u0026#39; 相反, 我们利用 Raku 的子例程签名, 并且重用 $/ 变量名以使我们能重用上面所写的代码, 然后拿掉 \u0026lt;Assignment-Expression\u0026gt; 部分。 我会把子例程的名字命名为 rule 的名字, 只是为了直接了当。(你会在之后看到为什么这样做。)\nsub assignment-expression($/) { \u0026#39;my $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;= \u0026#39; ~ $/\u0026lt;Number\u0026gt; ~ \u0026#39;;\u0026#39; } say assignment-expression( $/\u0026lt;Assignment-Expression\u0026gt; ); 让我们对 \u0026lt;Function-Call\u0026gt; 也做同样的事情, 创建一个含有 $/ 子例程签名的同名函数。 它现在写在一行里面就很整洁了, 并且只重复  部分, 因为它不得不重复。\nsub function-call( $/ ) { \u0026#39;say \u0026#39; ~ $/\u0026lt;String\u0026gt;[0] ~ \u0026#39;~ \u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;~ \u0026#39; ~ $/\u0026lt;String\u0026gt;[1] ~ \u0026#39;;\u0026#39; } say function-call( $/\u0026lt;Function-Call\u0026gt; ); 对象化 一路上我做了相当多的选择, 让我们到达这里。这就是我们上次重构的地方:\nmy rule Number {\\d+}; my rule Variable {\\w+}; my rule String {\u0026#39;\u0026#34;\u0026#39;\u0026lt;-[\u0026#34; ]\u0026gt;+\u0026#39;\u0026#34;\u0026#39;}; my rule Assignment-Expression {var \u0026lt;Variable\u0026gt;\u0026#39;=\u0026#39;\u0026lt;Number\u0026gt;}; my rule Function-Call {console \u0026#39;.\u0026#39;log \u0026#39;(\u0026#39;\u0026lt;String\u0026gt;\u0026#39;+\u0026#39;\u0026lt;Variable\u0026gt;\u0026#39;+\u0026#39;\u0026lt;String\u0026gt;\u0026#39;)\u0026#39;}; \u0026#39;var a = 3; console.log( \u0026#34;Hey, did you know a = \u0026#34; + a + \u0026#34;?\u0026#34; );\u0026#39; ~~ rule { \u0026lt;Assignment-Expression\u0026gt; \u0026#39;;\u0026#39; \u0026lt;Function-Call\u0026gt; \u0026#39;;\u0026#39; } sub assignment-expression( $/ ) { \u0026#39;my $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;= \u0026#39; ~ $/\u0026lt;Number\u0026gt; ~ \u0026#39;;\u0026#39; } sub function-call( $/ ) { \u0026#39;say \u0026#39; ~ $/\u0026lt;String\u0026gt;[0] ~ \u0026#39;~ $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;~ \u0026#39; ~ $/\u0026lt;String\u0026gt;[1] ~ \u0026#39;;\u0026#39;; } say assignment-expression( $/\u0026lt;Assignment-Expression\u0026gt; ); say function-call( $/\u0026lt;Function-Call\u0026gt; ); 这就是我们的回报。我们先捡起最后那两个 say 语句。 我们还没有给顶层 rule 一个名字, 所以我们就叫它\u0026hellip; 好吧, 现在还是叫 \u0026lsquo;top\u0026rsquo; 吧。\nsub top( $/ ) { assignment-expression( $/ ) ~ function-call( $/ ) } 收回你的吐槽 我们暂时还没有对处于文件顶层的 rules 做太多处理, 所以让我们开始工作吧。 在 Raku 中, 就一般编程而言, 把你的代码打包复用是不错的注意。而 Raku 让我们使用 class 关键字将我们的程序打包, 我们拥有的那些 rules 从任何意义上来说实际上不是\u0026quot;代码\u0026rdquo;。而它们能够用于代码中, 并且我们确实使用了它们, 它们自身实际上并没有做出任何决定。\n所以我们不应该使用 class 关键字来把它们打包到一块。相反, 有另外一种便捷的类型用于把一堆正则表达式和 rules 打包到一块儿, 它叫做 grammar。\n它的语法就像声明一个「rule」 那样。\ngrammar Javascript { rule Number {\\d+}; rule Variable {\\w+}; rule String {\u0026#39;\u0026#34;\u0026#39;\u0026lt;-[\u0026#34; ]\u0026gt;+\u0026#39;\u0026#34;\u0026#39;}; rule Assignment-Expression {var \u0026lt;Variable\u0026gt;\u0026#39;=\u0026#39;\u0026lt;Number\u0026gt;}; rule Function-Call {console \u0026#39;.\u0026#39;log \u0026#39;(\u0026#39;\u0026lt;String\u0026gt;\u0026#39;+\u0026#39;\u0026lt;Variable\u0026gt;\u0026#39;+\u0026#39;\u0026lt;String\u0026gt;\u0026#39;)\u0026#39;}; rule TOP {\u0026lt;Assignment-Expression\u0026gt;\u0026#39;;\u0026#39;\u0026lt;Function-Call\u0026gt;\u0026#39;;\u0026#39;}; } 你会注意到, 我们给我们的顶层 rule 也起了个名字, 并且暂时把它叫做 「TOP」 吧。 如果你正在家独自玩耍, 你可能已经做出更改并想知道 「\u0026lsquo;var a = 3;\u0026hellip;\u0026rsquo; ~~ rule { \u0026hellip; }」 是怎么起作用的, 因为键入诸如 「\u0026lsquo;var a = 3;\u0026hellip;\u0026rsquo; ~~ JavaScript;」这样的东西可能不会那么有作用。\nGrammars 就像类一样, 在里面它们实际上是一块可能的代码。 它们本身不会工作, 它们必须从潜在的转换为动态的代码。我们可以像你在类中做的那样:\nmy $JavaScript = JavaScript.new; 现在我们拥有了一个可以工作的变量。 现在, 让我们来使用它。所有的 Grammar 类都有一个内置的 「parse()」 方法, 以使我们能得到 grammar 中的正则表达式。 我们来修改我们的匹配语句以利用 parse() 方法:\n$JavaScript.parse(\u0026#39;var a = 3; console.log( \u0026#34;Hey, did you know a = \u0026#34; + a + \u0026#34;?\u0026#34; );\u0026#39;); 我们的代码应该又能工作了。\n接收动作 现在我们已经把我们所有的匹配的东西打包到一个小型的类里面了, 如果我们能对那些子例程做同样的处理将会很棒。 我们在这儿试试, 把我们的子例程放到它们自己的命名空间中, 就像我们对 rule 做的那样。 我们必须从 「sub」 修改为 「method」, 而我们的 「top」 方法将会使用 「self.」 去调用其它方法。\nclass Actions { method assignment-expression( $/ ) { \u0026#39;my $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;= \u0026#39; ~ $/\u0026lt;Number\u0026gt; ~ \u0026#39;;\u0026#39; } method function-call( $/ ) { \u0026#39;say \u0026#39; ~ $/\u0026lt;String\u0026gt;[0] ~ \u0026#39;~ $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;~ \u0026#39; ~ $/\u0026lt;String\u0026gt;[1] ~ \u0026#39;;\u0026#39;; } method top( $/ ) { self.assignment-expression( $/\u0026lt;Assignment-Expression\u0026gt; ) ~ self.function-call( $/\u0026lt;Function-Call\u0026gt; ) } } 就像之前那样, 我们可以在一行里面创建 Actions 对象:\nmy $actions = Actions.new; 并且调用 top 几乎像我们之前做的那样:\nsay $actions.top( $/ ); 我们已经修改了很多东西了, 所以我们来看看到哪了。\ngrammar JavaScript { rule Number {\\d+}; rule Variable {\\w+}; rule String {\u0026#39;\u0026#34;\u0026#39;\u0026lt;-[\u0026#34; ]\u0026gt;+\u0026#39;\u0026#34;\u0026#39;}; rule Assignment-Expression {var \u0026lt;Variable\u0026gt;\u0026#39;=\u0026#39;\u0026lt;Number\u0026gt;}; rule Function-Call {console \u0026#39;.\u0026#39;log \u0026#39;(\u0026#39;\u0026lt;String\u0026gt;\u0026#39;+\u0026#39;\u0026lt;Variable\u0026gt;\u0026#39;+\u0026#39;\u0026lt;String\u0026gt;\u0026#39;)\u0026#39;}; rule TOP {\u0026lt;Assignment-Expression\u0026gt;\u0026#39;;\u0026#39;\u0026lt;Function-Call\u0026gt;\u0026#39;;\u0026#39;} } my $j = JavaScript.new; $j.parse(\u0026#39;var a = 3; console.log( \u0026#34;Hey, did you know a = \u0026#34; + a + \u0026#34;?\u0026#34; );\u0026#39;); class Actions { method assignment-expression( $/ ) { \u0026#39;my $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;= \u0026#39; ~ $/\u0026lt;Number\u0026gt; ~ \u0026#39;;\u0026#39; } method function-call( $/ ) { \u0026#39;say \u0026#39; ~ $/\u0026lt;String\u0026gt;[0] ~ \u0026#39;~ $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;~ \u0026#39; ~ $/\u0026lt;String\u0026gt;[1] ~ \u0026#39;;\u0026#39;; } method top( $/ ) { self.assignment-expression( $/\u0026lt;Assignment-Expression\u0026gt; ) ~ self.function-call( $/\u0026lt;Function-Call\u0026gt; ) } } my $actions = Actions.new; say $actions.top($/); 不用担心, 我们快要到了。既然我们有了一个单独的类来处理 Actions, 我们把方法重命名为 Grammar 中所匹配的 rule 的名字, 以使我们不会忘记它们是什么。\nclass Actions { method Assignment-Expression( $/ ) { \u0026#39;my $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;= \u0026#39; ~ $/\u0026lt;Number\u0026gt; ~ \u0026#39;;\u0026#39; } method Function-Call( $/ ) { \u0026#39;say \u0026#39; ~ $/\u0026lt;String\u0026gt;[0] ~ \u0026#39;~$\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;~ \u0026#39; ~ $/\u0026lt;String\u0026gt;[1] ~ \u0026#39;;\u0026#39;; } method TOP( $/ ) { self.Assignment-Expression( $/\u0026lt;Assignment-Expression\u0026gt; ) ~ self.Function-Call( $/\u0026lt;Function-Call\u0026gt; ) } } 更进一步, 我们还有最后一点魔法能够利用。 我们将把 $javascript 和 $actions 对象像这样组合在一块。\nsay $javascript.parse(\u0026#39;...\u0026#39;, :actions($actions) ); 「:actions(\u0026hellip;)」 给 「parse()」方法声明的可选参数。我们正告诉正则表达式引擎, 任何时候, 像 \u0026lt;Function-Call\u0026gt; 或 \u0026lt;TOP\u0026gt; 这样的 rule 匹配时, 我们会在我们的类中让它调用对应的同名方法。\n这几乎是按原样工作的, 但是如果你运行修改后的代码, 你会发现解析返回了原来的匹配对象, 带着日语引用标记。所以看起来好像我们又回到了原地。不完全是。\n继续, 我们在其中之一的方法中添加一个临时的 \u0026quot;say 'Hello';\u0026quot; 语句, 仅仅是为了确认它们正被调用。这是正则引擎正在工作并且可能正解析它所 going over 的一个重要证据。 你甚至可以使用某些我们上面已经学到的技巧然后写上 「say $/\u0026lt;Variable\u0026gt;;」 来查看匹配是否正像你想的那样运行。继续运行并玩玩, 做完的时候再回到这儿。\n混合信号(Mixed Signals) 正发生的是方法正被调用, 但是它们的输出被丢弃。我们来捕获输出然后使用 grammar 的最后一个特性, 抽象语法树。现在, 这可能会勾起坐在教室里看黑板上画出的盒子和线段的场景, 但是也没有那么糟糕了。我们已经看到了一个, 实际上 say() 的输出就是一个 AST。\n我们来看下其它语法树, 我们在后台创建的那个。在 \u0026ldquo;$javascript.parse(\u0026hellip;)\u0026rdquo; 调用的末尾添加上 「.ast」, 这会给我们展示我们自己创建的语法树。\n如果你这样做了, 你会看到它打印了(Any), 这通常等价于「匹配失败」, 单是我们从之前的测试中知道匹配没有失败。所以这儿发生了什么? 当我们的方法运行的时候, 它们返回输出, 但是 Raku 不知道怎么处理这些输出, 或者说它不知道把输出安装到它所创建的 AST 中的哪个位置。\n关键是一个叫做「make」的小东西。在方法的开头, 把这个添加到过去我们放置「say」的地方。\nclass Actions { method Assignment-Expression( $/ ) { make \u0026#39;my $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;= \u0026#39; ~ $/\u0026lt;Number\u0026gt; ~ \u0026#39;;\u0026#39; } method Function-Call( $/ ) { make \u0026#39;say \u0026#39; ~ $/\u0026lt;String\u0026gt;[0] ~ \u0026#39;~ $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;~ \u0026#39; ~ $/\u0026lt;String\u0026gt;[1] ~ \u0026#39;;\u0026#39; } method TOP( $/ ) { make $/\u0026lt;Assignment-Expression\u0026gt;.ast ~ $/\u0026lt;Function-Call\u0026gt;.ast } } 还有, 因为 Raku 为我们调用方法, 我们不需要自己来调用 self.Function-Call(...), 我们需要做的全部工作就是查看 Function-Call(...) 返回给我们的语法树。最终我们做到了。一个完整, 虽然微小的编译器。为了防止你在编辑时迷失, 这儿有一个最终的结果。\ngrammar JavaScript { rule Number {\\d+}; rule Variable { \\w+ }; rule String { \u0026#39;\u0026#34;\u0026#39; \u0026lt;-[ \u0026#34; ]\u0026gt;+ \u0026#39;\u0026#34;\u0026#39; }; rule Assignment-Expression { var \u0026lt;Variable\u0026gt; \u0026#39;=\u0026#39; \u0026lt;Number\u0026gt; }; rule Function-Call { console \u0026#39;.\u0026#39; log \u0026#39;(\u0026#39; \u0026lt;String\u0026gt; \u0026#39;+\u0026#39; \u0026lt;Variable\u0026gt; \u0026#39;+\u0026#39; \u0026lt;String\u0026gt; \u0026#39;)\u0026#39; }; rule TOP { \u0026lt;Assignment-Expression\u0026gt; \u0026#39;;\u0026#39; \u0026lt;Function-Call\u0026gt; \u0026#39;;\u0026#39; }} class Actions { method Assignment-Expression( $/ ) { make \u0026#39;my $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;= \u0026#39; ~ $/\u0026lt;Number\u0026gt; ~ \u0026#39;;\u0026#39; }method Function-Call($/ ){ make \u0026#39;say \u0026#39; ~ $/\u0026lt;String\u0026gt;[0] ~ \u0026#39;~ $\u0026#39; ~ $/\u0026lt;Variable\u0026gt; ~ \u0026#39;~ \u0026#39; ~ $/\u0026lt;String\u0026gt;[1] ~ \u0026#39;;\u0026#39;; }method TOP($/ ){ make $/\u0026lt;Assignment-Expression\u0026gt;.ast ~ $/\u0026lt;Function-Call\u0026gt;.ast }} my $j=JavaScript.new; my $a=Actions.new; say $j.parse(\u0026#39;var a = 3; console.log( \u0026#34;Hey, did you know a = \u0026#34; + a + \u0026#34;?\u0026#34; );\u0026#39;, :actions($a)).ast; 到哪里去 一个简单但整洁的更改是你可以扩展 Assignment-Expression 来既接收数字又接收字符串。上次我们谈论了 rules 中的轮试,所以这个提示应该足够让你开始了:\nrule Assignment-Expression {var \u0026lt;Variable\u0026gt;\u0026#39;=\u0026#39;(\u0026lt;Number\u0026gt;|\u0026lt;String\u0026gt;)} 你必须修改下 Assignment-Expression 方法以使它起作用。或者你可以狡猾一点然后发现( \u0026lt;Number\u0026gt; | \u0026lt;String\u0026gt; ) 可以转换为它自己的小的普通的 \u0026ldquo;Term\u0026rdquo; rule, \u0026ldquo;rule Term { | }\u0026rdquo;, 然后添加一个 action \u0026ldquo;method Term($/) { make $/or $/}\u0026rdquo; 而只在 Assignment-Expression 中修改一个东西。\nhttp://theperlfisher.blogspot.fr/2016/02/from-regular-expressions-to-grammars-pt_28.html\n","permalink":"https://ohmyweekly.github.io/notes/2015-10-29-from-regex-to-grammar/","tags":["regex","grammar"],"title":"从正则表达式到 Grammar(第二部分)"},{"categories":["rakulang"],"contents":"以指定音量随机播放音频文件：\n#!/usr/bin/env raku my %v; # hash to hold data my token filename {.+?\\.\\S\\S\\S}; # filenames end in .??? my token volume {\\d+}; # any digits for volume my regex extra {.+\\S}; # anything following that my $mixer = \u0026#39;mixer\u0026#39;; my $player = \u0026#39;mplayer -vf dsize=600:-2 -geometry +200-10 \u0026#39;; my $lockfile = \u0026#39;/tmp/myplayer\u0026#39;; $lockfile.IO.spurt( $*PID ); # store the process ID so other process can kill this one END { $lockfile.IO.unlink; } # remove the lockfile at end for $=finish.lines { # loop through the lines below \u0026#39;=begin finish\u0026#39; last if /STOP/; # stop at a STOP line if m/(\u0026lt;filename\u0026gt;)\\s+(\u0026lt;volume\u0026gt;)\\s*(\u0026lt;extra\u0026gt;?)/ { # use the regexes/tokens my ( $m, $v, $e ) = $/[0..2]; # get captured values from $/ if $m and $v { # if there\u0026#39;s a filename and volume %v{$m}\u0026lt;v\u0026gt; ~= $v; # store it in the hash %v{$m}\u0026lt;e\u0026gt; ~= $e // \u0026#39;\u0026#39;; # with any extra arguments }}} # lisp-y to save lines for %v.keys.pick(*) -\u0026gt; $m { # loop randomly through keys say \u0026#34;Playing $m\u0026#34;; print qqx{$mixer%v{$m}\u0026lt;v\u0026gt;}; # set the volume print qqx{$player%v{$m}\u0026lt;e\u0026gt;\u0026#34;$m\u0026#34; }; # play the file } =beginfinish300.avi 77 Crystal Skull Rifftrax.avi 77 -aid 2 Star Trek 5.avi 77 Star Trek 7.avi aeon-flux.avi 93 =end finish改进版：\n#! /usr/bin/env raku my @mixer = \u0026#39;mixer\u0026#39;; my @player = \u0026lt;m6player -vf dsize=600:-2 -geometry +200-10 \u0026gt;; my $lockfile = \u0026#39;/tmp/myplayer\u0026#39;; $lockfile.IO.spurt( $*PID ); END { $lockfile.IO.unlink; } my token filename {.+?\\.\\S\\S\\S}; my token volume {\\d+}; my regex extra {.*}; my %song-data; for $=finish.lines { last if /^ \\s*STOP \\s*$/; next unless m/ \\s* \u0026lt;filename\u0026gt; \\s+ \u0026lt;volume\u0026gt; \u0026lt;extra\u0026gt; /; # $\u0026lt;extra\u0026gt; is short for $/{\u0026#39;extra\u0026#39;} %song-data{~$\u0026lt;filename\u0026gt;}\u0026lt;v e\u0026gt; = +$\u0026lt;volume\u0026gt;, [$\u0026lt;extra\u0026gt;.words]; } # uses sub-signature unpacking for %song-data.pick(*) -\u0026gt; ( :key($m), :value($) (:$v,:@e)) { say \u0026#34;Playing $m\u0026#34;; print run( @mixer, $v, :out ).out.slurp-rest.indent(4); print run( @player, @e, $m, :out ).out.slurp-rest.indent(4); } =beginfinish300.avi 77 Crystal Skull Rifftrax.avi 77 -aid 2 Star Trek 5.avi 77 Star Trek 7.avi aeon-flux.avi 93 =end finish","permalink":"https://ohmyweekly.github.io/notes/2015-06-14-play-audio-at-given-volume/","tags":["regex","audio"],"title":"以指定音量随机播放音频文件"},{"categories":["rakulang"],"contents":" 解析[ ] 里面的数据：  use Grammar::Debugger; grammar Lines { token TOP {^\u0026lt;line\u0026gt;+$} token line {\\[ \u0026lt;student\u0026gt;+%\u0026lt;semicolon\u0026gt;\\] \\n# 换行 \\n 是最容易被忽略的地方 } token student {\u0026lt;myname\u0026gt;+%\u0026lt;comma\u0026gt;# 分隔符也可以是一个 subrule } token myname {\u0026lt;[A..Za..z-]\u0026gt;+# 字符类的写法 \u0026lt;[...]\u0026gt; } token comma {\u0026#39;,\u0026#39;\\s+# 逗号, 分号 不能裸露出现在 token 中 } token semicolon {\u0026#39;;\u0026#39;\\s+} } my $parse = Lines.parsefile(\u0026#39;test.txt\u0026#39;); say $parse; test.txt 的内容如下：\n[Lue, Fan] [Lou, Man-Li] [Tian, Mijie; Zhou, Lin; Zou, Xiao; Zheng, Qiaoji; Luo, Lingling; Jiang, Na; Lin, Dunmin] 下面的 Grammar 用于解析一个字符串, 由于 tokens 不能回溯, 所以当解析 $str 时使用了 Grammar 的继承, 重写了 university 这个 token:\nmy $string = \u0026#34;[Wang, Zhiguo; Zhao, Zhiguo] Hangzhou Normal Univ, Ctr Cognit \u0026amp; Brain Disorders, Hangzhou, Zhejiang, Peoples R China; [Wang, Zhiguo; Theeuwes, Jan] Vrije Univ Amsterdam, Dept Cognit Psychol, Amsterdam, Netherlands\u0026#34;; grammar University::Grammar { token TOP {^\u0026lt;university\u0026gt;$} token university {[\u0026lt;bracket\u0026gt;\u0026lt;info\u0026gt;]+%\u0026#39;; \u0026#39;} token bracket {\u0026#39;[\u0026#39;\u0026lt;studentname\u0026gt;\u0026#39;] \u0026#39;} token studentname {\u0026lt;stdname=.info\u0026gt;+%\u0026#39;; \u0026#39;} token info {\u0026lt;field\u0026gt;+%\u0026#39;, \u0026#39;} token field {\u0026lt;-[,\\]\\[;\\n]\u0026gt;+} } # grammar 像类一样可以继承, 里面的 token 可以被重写 grammar MyUniversity is University::Grammar { token university {\u0026lt;info\u0026gt;+%\u0026#39;; \u0026#39;} } my $str = \u0026#34;Zhejiang Univ, Coll Environm \u0026amp; Resources Sci, Dept Resource Sci, Hangzhou 310029, Peoples R China; La Trobe Univ, Dept Agr Sci, Bundoora, Vic 3083, Australia; Hangzhou Normal Coll, Fac Life Sci, Hangzhou, Peoples R China\u0026#34;; my $parsed = University::Grammar.parse($string); # my $parsed = MyUniversity.parse($str); for @($parsed\u0026lt;university\u0026gt;\u0026lt;info\u0026gt;) -\u0026gt; $f { say $f\u0026lt;field\u0026gt;[0]; } ","permalink":"https://ohmyweekly.github.io/notes/2015-07-11-an-example-of-parsing-text-in-grammar/","tags":["grammar"],"title":"使用 Grammar 解析文本: 一个例子"},{"categories":["rakulang"],"contents":"描述 下面的日期, 有些使用了 M D Y 格式, 有些使用了 Y M D 格式, 还使用了任意分隔符! 请把这些散乱的文本解析成合适的 ISO 8601 (YYYY-MM-DD) 格式化日期。\n假设只有以 4 个数字开头的日期使用 Y M D 格式, 其它的使用 M D Y 格式。\n输入样本 2/13/15 1-31-10 5 10 2015 2012 3 17 2001-01-01 2008/01/07 输出样本 2015-02-13 2010-01-31 2015-05-10 2012-03-17 2001-01-01 2008-01-07 扩展挑战 [中级] 使用 2014-12-24 作为相对日期的基准。\n当添加 days(天数) 时, 要考虑到每月会有不同的天数, 忽略闰年。\n当添加月和年时, 使用整个 units, 以至于:\none month before october 10 is september 10\none year after 2001-04-02 is 2002-04-02\none month after january 30 is february 28 (not march 1)\nSally\u0026rsquo;s inputs: tomorrow 2010-dec-7 OCT 23 1 week ago next Monday last sunDAY 1 year ago 1 month ago last week LAST MONTH 10 October 2010 an year ago 2 years from tomoRRow 1 month from 2016-01-31 4 DAYS FROM today 9 weeks from yesterday Sally\u0026rsquo;s expected outputs: 2014-12-25 2010-12-01 2014-10-23 2014-12-17 2014-12-29 2014-12-21 2013-12-24 2014-11-24 2014-12-15 2014-11-24 2010-10-10 2013-12-24 2016-12-25 2016-02-28 2014-12-28 2015-02-25 smls 大神给出了完整的 grammar：\nmy $today = Date.new(2014, 12, 24); grammar MessyDate { rule TOP {|\u0026lt;date\u0026gt;{ make $\u0026lt;date\u0026gt;.made }|:i\u0026lt;duration\u0026gt;ago { make $today.earlier: |$\u0026lt;duration\u0026gt;.made }|:i\u0026lt;duration\u0026gt;from \u0026lt;date\u0026gt;{ make $\u0026lt;date\u0026gt;.made.later: |$\u0026lt;duration\u0026gt;.made }} rule date {|[||\u0026lt;month\u0026gt;(\u0026lt;sep\u0026gt;?)\u0026lt;day\u0026gt;[$0\u0026lt;year\u0026gt;]?||\u0026lt;day\u0026gt;(\u0026lt;sep\u0026gt;?)\u0026lt;month\u0026gt;[$0\u0026lt;year\u0026gt;]?||\u0026lt;year\u0026gt;(\u0026lt;sep\u0026gt;?)\u0026lt;month\u0026gt;$0\u0026lt;day\u0026gt;]{ make Date.new: $\u0026lt;year\u0026gt;.made//$today.year, |$\u0026lt;month day\u0026gt;».made }|:itoday { make $today }|:iyesterday { make $today - 1 }|:itomorrow { make $today + 1 }|:ilast \u0026lt;weekday\u0026gt;{ make $today - ($today.day-of-week - $\u0026lt;weekday\u0026gt;.made) % 7 || 7 }|:inext \u0026lt;weekday\u0026gt;{ make $today + ($\u0026lt;weekday\u0026gt;.made - $today.day-of-week) % 7 || 7 }|:ilast \u0026lt;unit\u0026gt;{ make $today.earlier: |($\u0026lt;unit\u0026gt;.made =\u0026gt; 1) }|:inext \u0026lt;unit\u0026gt;{ make $today.later: |($\u0026lt;unit\u0026gt;.made =\u0026gt; 1) }} rule duration { \u0026lt;count\u0026gt; \u0026lt;unit\u0026gt; { make $\u0026lt;unit\u0026gt;.made =\u0026gt; $\u0026lt;count\u0026gt;.made }} token year { | \u0026lt;number(4)\u0026gt; { make +$\u0026lt;number\u0026gt; }|\u0026lt;number(2, 0..49)\u0026gt;{ make 2000 + $\u0026lt;number\u0026gt; }|\u0026lt;number(2, 50..*)\u0026gt;{ make 1900 + $\u0026lt;number\u0026gt; }} token month { | \u0026lt;number(1..2, 1..12)\u0026gt; { make +$\u0026lt;number\u0026gt; }|:iJan[uary]?{ make 1 }|:iFeb[ruary]?{ make 2 }|:iMar[ch]?{ make 3 }|:iApr[il]?{ make 4 }|:iMay { make 5 }|:iJun[e]?{ make 6 }|:iJul[y]?{ make 7 }|:iAug[ust]?{ make 8 }|:iSep[tember]?{ make 9 }|:iOct[ober]?{ make 10 }|:iNov[ember]?{ make 11 }|:iDec[ember]?{ make 12 }} token day { \u0026lt;number(1..2, 1..31)\u0026gt; { make +$\u0026lt;number\u0026gt; }} token weekday { | :i Mon[day]? { make 1 }|:iTue[sday]?{ make 2 }|:iWed[nesday]?{ make 3 }|:iThu[rsday]?{ make 4 }|:iFri[day]?{ make 5 }|:iSat[urday]?{ make 6 }|:iSun[day]?{ make 7 }} token sep { \u0026lt;[-/.\\h]\u0026gt; }token count { (\u0026lt;[0..9]\u0026gt;+) { make +$0 }|an?{ make 1 }} token unit { :i (day|week|month|year) s? {make $0.lc }} multi token number ($digits) {\u0026lt;[0..9]\u0026gt;**{$digits}} multi token number ($digits, $test) {(\u0026lt;[0..9]\u0026gt;**{$digits})\u0026lt;?{ +$0 ~~ $test }\u0026gt;} } for lines() { say MessyDate.parse($_).made // \u0026#34;failed to parse \u0026#39;$_\u0026#39;\u0026#34;; }  [...] 是非捕获分组 \u0026lt;[...]\u0026gt; 是 Raku 中的字符类 \u0026lt;number(4)\u0026gt; 是扩展的 \u0026lt;...\u0026gt; 语法, 实际上是方法调用  在 grammar 中, 有两个 regex 的变体, rule 和 token。rule 默认不会回溯。rule 与 token 的一个重要区别是, rule 这样的正则采取了 :sigspace 修饰符。 rule 实际上是\nregex :ratchet :sigspace { ... } 的简写。ratchet 这个单词的意思是: (防倒转的)棘齿, 意思它是不能回溯的! 而 :sigspace 表明正则中的空白是有意义的, 而 token 实际上是:\nregex :ratchet { ... } 的简写。 所以在 token 中, 若不是显式的写上 \\s、\\h、\\n 等空白符号, 其它情况下就好像空白隐身了一样, 虽然你写了, 但是编译器却视而不见。\n// 在左侧匹配失败时会在右侧提供一个默认值。\n\u0026lt;number(4)\u0026gt; 和 \u0026lt;number(2, 0..49)\u0026gt; 中使用了扩展了的 \u0026lt;...\u0026gt; 元语法。 标识符(例如左面的 number)后面的第一个字符决定了闭合尖括号之前剩余文本的处理。它的底层语义是函数或方法调用, 所以, 如果标识符后面的第一个字符是左圆括号, 那么它要么是方法调用, 要么是函数调用:\n\u0026lt;number(4)\u0026gt; 等价于 \u0026lt;number=\u0026amp;number(4)\u0026gt;\n\u0026lt;number(2, 0..49)\u0026gt; 等价于 \u0026lt;number=\u0026amp;number(2, 0..49)\u0026gt;\nmulti token number ($digits) {\u0026lt;[0..9]\u0026gt;**{$digits}} multi token number ($digits, $test) {(\u0026lt;[0..9]\u0026gt;**{$digits})\u0026lt;?{ +$0 ~~ $test }\u0026gt;} 在扩展的 \u0026lt;...\u0026gt; 语法中, 一个前置的 ?{ 或 !{ 标示着代码断言:\n(\u0026lt;[0..9]\u0026gt; ** {$digits}) \u0026lt;?{ +$0 ~~ $test }\u0026gt; 等价于：\n# + 强制后面的$0为数值上下文, 以匹配 $test 中的数字 (\u0026lt;[0..9]\u0026gt; ** {$digits}) { +$0 ~~ $test or fail } 上面的两句代码, 具名 regex, token, 或 rule 是一个子例程, 所以可以传递参数给具名 token。\n这从标准输入里读取散乱的日期并把对应的 ISO 日期写到标准输出。\n它能解析任务描述中的所有日期（包含扩展）, 还有 - 然而, 在它们中我得到 4 个不同的结果。请弄清它们是否是错误的, 并且为什么是错的:\n2010-dec-7 \u0026ndash;\u0026gt; 我得到 2010-12-07 而不是 2010-12-01\nlast week \u0026ndash;\u0026gt; 我得到 2014-12-17 而不是 2014-12-15\n1 month from 2016-01-31 \u0026ndash;\u0026gt; 我得到 2016-02-29 而不是 2016-02-28\n9 weeks from yesterday \u0026ndash;\u0026gt; 我得到 2015-02-24 而不是 2015-02-25\n有人在评论中问他 make/made 是类中的方法吗？\n是的, 它们是 Match 类的方法。\nMatch objects(注意 object 是复数) 每个 regex match(并且通过扩展, 每个 grammar token match)的结果被表示为一个 Match 对象。\n通过这个对象你能访问各种信息片段:\n 匹配到的字符串 关于输入字符串匹配的开始和结束位置 每个位置捕获和具名捕获的sub-matches 与这个匹配有关的 AST 片段, 如果有的话  AST 片段 在 token/rule 里面调用 make, 设置将会与当前匹配关联的 \u0026ldquo;AST 片段\u0026rdquo;。然后, 你可以通过在当前结果 Match 对象身上调用 .made 方法来获取那个关联数据。\n这正是自由形式的插槽, 允许你使用 Match 对象存储任何你想要的东西并在以后检索它, 尽管显而易见这意味着像我那样创建一个 AST。\n在 grammar 中创建 \u0026ldquo;AST\u0026rdquo; 在我的 grammar 中每个 token/rule 使用 .made 来取得它的 sub-rule 匹配构建的数据片段, 把它们组合成一个更大的数据片段, 这是为了让它的 parent rule 能检索。等等。\n我在每个 token/rule 里面使用这些语法简写来引用 sub-matches 的 Match 对象:\n $0 引用 sub-match（由一个 () 捕获组导致） 的第一个位置处的 Match 对象。 $\u0026lt;date\u0026gt; 引用一个名字为 date 的具名 sub-match 的 Match 对象(通过 \u0026lt;date\u0026gt; 递归引用名为 date 的 token 导致).  https://www.reddit.com/r/dailyprogrammer/comments/3wshp7/20151214_challenge_245_easy_date_dilemma/\n","permalink":"https://ohmyweekly.github.io/notes/2015-09-15-use-grammar-to-parse-date/","tags":["grammar"],"title":"使用 Grammar 解析日期"},{"categories":["rakulang"],"contents":"举个例子, 假设 person 有一个 age 属性. 我能写一个 multimethod, 让它接收一个 person 作为参数, 并返回这样的结果吗:\nreturn \u0026#34;child\u0026#34; if age \u0026lt; 16; return \u0026#34;adult\u0026#34; if 16 \u0026lt;= age \u0026lt; 66; return \u0026#34;senior\u0026#34; if age \u0026gt;= 66; class Person { has Int $.age; has Str $.name; } 这仅仅定义了一个拥有两个属性, 叫做 Person 的类. age 必须是 Int 型, name 必须是 Str 型. . 语法会生成一个只读访问器, 以使我们能从类的外部访问 getter 方法.\n现在我们来定义一个 age-group multi 来告诉一个 person 属于哪个 age-group:\nmulti age-group ($person where (*.age \u0026lt; 16) ) { \u0026#34;child\u0026#34; } multi age-group ($person where (*.age \u0026gt;= 66) ) { \u0026#34;senior\u0026#34;} multi age-group ($person) { \u0026#34;adult\u0026#34; } where从句给参数添加了一个约束, 这个约束告诉参数必须匹配这个参数右边的东西.这用于区别将要选取的 multi. where从句可以是一个 regex, 类型, 一个确切的值, 一个断言 block,或者一些其它东西.\n*.age \u0026lt; 16 部分可能看起来更让人迷惑. 星号是什么? 星号是一个特殊的值, 叫做 Whatever. 它通常在给定情况下满足你的需求. 在智能匹配中, 它总是匹配, 所以你可以在 given/when block 中将它用作默认值. 但是 Whatever 最有用的地方之一是创建匿名 block. 对于大部分操作符, 如果你在 Whatever 上执行它们, 它会产生一个匿名 block 并使用它们的参数执行操作符. 如果一个表达式中有多个 Whatever, 则生成的匿名 block 会有多个参数对应于相应的 Whatever 位置.\n例如, * + 1 产生一个 block,使参数的值加1. * + * 产生一个 block 使它的两个参数相加. 这个例子中, 我们调用 Whatever 的 age方法, 并询问它是否小于 16. 我们能用其它几种方式达到同样的效果, 但是更啰嗦:\nsub ($person) { $person.age \u0026lt; 16 } -\u0026gt; $person { $person.age \u0026lt; 16 } { .age \u0026lt; 16 } 但是对于像这种简单的操作, Whatever 通常比其它方式更易读也更简洁. 不幸的是, 在参数列表的 where 从句中, 你需要使用括号括起很多复杂的表达式, 包括 Whatever block.\n现在让我们在 Rakudo 的 REPL 中试试它吧:\n\u0026gt; age-group Person.new(:name\u0026lt;timmy\u0026gt;, :age(10)) child \u0026gt; age-group Person.new(:name\u0026lt;john\u0026gt;, :age(23)) adult \u0026gt; age-group Person.new(:name\u0026lt;ezekiel\u0026gt;, :age(89)) senior 目前为止, 很好. 但是如果我们意外地传递了一个 age 而不是 Person 给 age-group 呢?\n\u0026gt; age-group 15 Method \u0026#39;age\u0026#39; not found for invocant of class \u0026#39;Int\u0026#39; 我们能指定只有 Person 对于 age-group 是合法的:\nmulti age-group (Person $person where (*.age \u0026lt; 16)) { \u0026#34;child\u0026#34; } multi age-group (Person $person where (*.age \u0026gt;= 66)) { \u0026#34;senior\u0026#34; } multi age-group (Person $person) { \u0026#34;adult\u0026#34; } 这正确地处理了 Person 问题. 调用带有 age 参数的 age-group 会怎样呢?\n\u0026gt; age-group 15 No applicable candidates found to dispatch to for \u0026#39;age-group\u0026#39;. Available candidates are: :(Person $person where ({ ... })) :(Person $person where ({ ... })) :(Person $person) 看起来更好. 假如我们允许询问 age 所属的 age-group 呢?\n我们能重写 age-group 的 Person 变体, 接收 Int 类型的 age, 并写一个单个的 Person 变体来调用 age-group:\nmulti age-group(Int $age where (* \u0026lt; 16) ) { \u0026#34;child\u0026#34; } multi age-group(Int $age where (* \u0026gt;= 66) ) { \u0026#34;senior\u0026#34; } multi age-group(Int $age) { \u0026#34;adult\u0026#34; } multi age-group(Person $person) { age-group $person.age } 这对于每个 Person 例子都有效, 还有它们的 ages.\n现在,让我们使用 age-group 定义一个叫做 print-name 的 multi 来分发. 根据 age-group 分发最明显的方法是使用 where 从句.\nmulti print-name(Person $person where (age-group($person) eq \u0026#34;child\u0026#34;)) { \u0026#34;Little {$person.name}\u0026#34; } multi print-name(Person $person where (age-group($person) eq \u0026#34;adult\u0026#34;)) { $person.name } multi print-name(Person $person where (age-group($person) eq \u0026#34;senior\u0026#34;)){ \u0026#34;Old Man {$person.name}\u0026#34;} 双引号字符串中的 {$person.name} 将 block 的结果插值到字符串中.\n让我们再试试:\n\u0026gt; print-name Person.new(:name\u0026lt;timmy\u0026gt;, :age(10)) Little Timmy \u0026gt; print-name Person.new(:name\u0026lt;john\u0026gt;, :age(23)) John \u0026gt; print-name Person.new(:name\u0026lt;ezekiel\u0026gt;, :age(89)) Old Man Ezekiel 那很棒. 但是如果我们有更多的基于 person 的 age-group 的 multis 要分发呢? 难道我们真的每次都要写出 (Person $person where (age-group($person) eq \u0026quot;child\u0026quot;)) 这样的代码吗? 不, 我们不需要, 感谢 subset 类型.\nsubset Child of Person where *.age \u0026lt; 16; subset Adult of Person where -\u0026gt; $person { 16 \u0026lt;= $person.age \u0026lt; 66 }; subset Senior of Person where *.age \u0026gt;= 66; multi print-name(Child $person) { \u0026#34;Little {$person.name}\u0026#34; } multi print-name(Adult $person) { $person.name } multi print-name(Senior $person) { \u0026#34;Old Man {$person.name}\u0026#34; } 由于 Rakudo 在处理含有组合的链式比较操作符的 Whatever 时有一个 bug, 我们不得不为 Adult 写一个显式的 block.\n这个 bug 现已修复, 所以:\nsubset Adult of Person where -\u0026gt; $person { 16 \u0026lt;= $person.age \u0026lt; 66 }; 等价于:\nsubset Adult of Person where 16 \u0026lt;= *.age \u0026lt; 66; 这个新版本的 print-name 与之前旧版本产生同样的结果. 现在我们能从 Child/Adult/Senior 的角度重写 age-group :\nmulti age-group(Child) { \u0026#34;child\u0026#34; } multi age-group(Adult) { \u0026#34;adult\u0026#34; } multi age-group(Senior) { \u0026#34;senior\u0026#34; } multi age-group(Int $age) { age-group Person.new(:$age) } :$age 是 :age($age) 的简写方式.\n又一次, 我们有了产生想要的结果的更清晰的代码, 多亏了 multiple 分发和 subset 类型.\n","permalink":"https://ohmyweekly.github.io/notes/2016-01-04-an-example-of-raku-subset-and-multi/","tags":["rakulang"],"title":"使用 Raku 的 subsets 和 multi 辨别年龄"},{"categories":["rakulang"],"contents":"为自定义的类添加下标(subscripts) 假如你定义了一个类, 你想把类的实例用作散列那样, 可以索引其中的元素, 那么你需要让你得自定义遵守 Associative 接口, 并重写 AT-KEY、EXISTS-KEY、DELETE-KEY、push 等跟散列有关的方法:\nclass HTTPHeader { ... } class HTTPHeader does Associative { has %!fields handles \u0026lt;self.AT-KEY self.EXISTS-KEY self.DELETE-KEY self.push list kv keys values\u0026gt;; method Str { say self.hash.fmt; } multi method EXISTS-KEY ($key) { %!fields{normalize-key $key}:exists } multi method DELETE-KEY ($key) { %!fields{normalize-key $key}:delete } multi method push (*@_) { %!fields.push: @_ } sub normalize-key ($key) { $key.subst(/\\w+/, *.tc, :g) } # titileCase 驼峰式的键 method AT-KEY (::?CLASS:D: $key) is rw { my $element := %!fields{normalize-key $key}; Proxy.new( FETCH =\u0026gt; method () { $element }, STORE =\u0026gt; method ($value) { $element = do given $value».split(/\u0026#39;,\u0026#39;\\s+/).flat { when 1 { .[0] } # a single value is stored as a string default { .Array } # multiple values are stored as an array } } ); } } my $header = HTTPHeader.new; say $header.WHAT; #-\u0026gt; (HTTPHeader) \u0026#34;\u0026#34;.say; $header\u0026lt;Accept\u0026gt; = \u0026#34;text/plain\u0026#34;; $header{\u0026#39;Accept-\u0026#39; X~ \u0026lt;Charset Encoding Language\u0026gt;} = \u0026lt;utf-8 gzip en\u0026gt;; $header.push(\u0026#39;Accept-Language\u0026#39; =\u0026gt; \u0026#34;fr\u0026#34;); # like .push on a Hash say $header.hash.fmt; \u0026#34;\u0026#34;.say; # say $header.Str; # 同上 say $header\u0026lt;Accept-Language\u0026gt;.values; say $header\u0026lt;Accept-Charset\u0026gt;; 输出：\n(HTTPHeader) Accept\ttext/plain Accept-Charset\tutf-8 Accept-Encoding\tgzip Accept-Language\ten fr (en fr) utf-8 同样, 你也可以使用数组下标, 只要你重写相应地方法。\n关于为自定义的类添加下标这个问题, stackoverflow 上的回答是不需要在 handles 后面所跟的方法中添加 self。 他的解释如下:\n为自定义的类添加下标(subscripts) 在自定义类上实现关联式下标(associative subscripting)。\n通过代理实现 Raku 通过在实现了集合类型的对象身上调用良定义的方法来实现关联式下标和位置下标（对于内置类型）。通过在 %!fields 属性后面添加 handles 特性(trait), 你就把这些方法调用传递给了 %!fields - 它作为一个散列, 会知道怎么来处理那些方法。\n灵活的键  However, HTTP header field names are supposed to be case-insensitive (and preferred in camel-case). We can accommodate this by taking the *-KEY and push methods out of the handles list, and implementing them separately\u0026hellip;\n 把所有的键处理方法代理给内部的散列意味着你的键得到了散列那样的插值 - 意味着它们将是大小写无关的因为散列的键是大小写无关的。为了避免那, 你把所有跟键有关的方法从 handles 子句中拿出并自己实现那些方法。在例子中, 键在被索引到 %!fields 让键变成大小写无关之前先进行了键的「标准化」。\n灵活的值 例子中的最后一部分展示了当值存入到散列那样的容器中时你如何控制值的插值。到目前为止, 通过赋值给这个自定义容器的实例提供的值要么是一个字符串, 要么是一个字符串的数组。额外的控制是通过移除定义在灵活的键中的 AT-KEY 方法来达成的并提供一个 Proxy 对象来代替它。如果你给容器赋值, 那么代理人对象的 STORE 方法会被调用并且那个方法会扫描所提供的字符串值中的 \u0026quot;, \u0026quot;（注意空格是必要的）。如果找到会接收那个字符串值作为几个字符串值的说明书。\nclass HTTPHeader { ... } class HTTPHeader does Associative { has %!fields handles \u0026lt;list kv keys values\u0026gt;; method Str { say self.hash.fmt; } multi method EXISTS-KEY ($key) { %!fields{normalize-key $key}:exists } multi method DELETE-KEY ($key) { %!fields{normalize-key $key}:delete } multi method push (*@_) { %!fields.push: @_ } sub normalize-key ($key) { $key.subst(/\\w+/, *.tc, :g) } method AT-KEY (::?CLASS:D: $key) is rw { my $element := %!fields{normalize-key $key}; Proxy.new( FETCH =\u0026gt; method () { $element }, STORE =\u0026gt; method ($value) { $element = do given $value».split(/\u0026#39;,\u0026#39;\\s+/).flat { when 1 { .[0] } # a single value is stored as a string default { .Array } # multiple values are stored as an array } } ); } } my $header = HTTPHeader.new; say $header.WHAT; #-\u0026gt; (HTTPHeader) \u0026#34;\u0026#34;.say; $header\u0026lt;Accept\u0026gt; = \u0026#34;text/plain\u0026#34;; $header{\u0026#39;Accept-\u0026#39; X~ \u0026lt;Charset Encoding Language\u0026gt;} = \u0026lt;utf-8 gzip en\u0026gt;; $header.push(\u0026#39;Accept-Language\u0026#39; =\u0026gt; \u0026#34;fr\u0026#34;); # like .push on a Hash say $header.hash.fmt; \u0026#34;\u0026#34;.say; # say $header.Str; # 同上 say $header\u0026lt;Accept-Language\u0026gt;.values; say $header\u0026lt;Accept-Charset\u0026gt;; 输出：\n(HTTPHeader) Accept\ttext/plain Accept-Charset\tutf-8 Accept-Encoding\tgzip Accept-Language\ten fr (en fr) utf-8 同样, 你也可以使用数组下标, 只要你重写相应地方法。\n","permalink":"https://ohmyweekly.github.io/notes/2015-08-18-add-subscripts-for-custom-class-in-raku/","tags":["subscript"],"title":"在 Raku 中为自定义的类添加下标"},{"categories":["rakulang"],"contents":"Raku 中怎么为已存在的类添加方法 Int 类有一个方法叫做 is-prime, 我想为 Int 类型添加其它的方法。\nclass MyInt is Int { method is-even () returns Bool:D { return False if self % 2; return True; } } my $n = MyInt.new(138); say $n.is-even; 通过类的继承也是一种方法, 但是不是我想要的。Swift 中可以通过扩展来实现, Raku 中有一个 add_method 方法:\nmethod add_method(Metamodel::MethodContainer: $obj, $name, $code) 这会给元类(meta class)添加一个方法, 使用 $name 作为调用的方法名。这只会在类型被组合前使用。\nInt.^add_method( \u0026#39;is-even\u0026#39;, method () returns Bool:D { return False if self % 2; return True; } ); say 137.is-even; say Int.^methods; 如果我调用 Int.^methods 时, is-even 没有出现。但是上面的代码能被调用并起作用了。\nInt.^add_method(\u0026#39;fac\u0026#39;, method () returns Int:D { return 1 if self == 0; return 1 if self == 1; my $sum = 1; for 1 .. self {$sum *= $_}; return $sum; }); 1.fac # 1 5.fac # 120 method 后面的圆括号是方法的签名：\nInt.^add_method(\u0026#39;power\u0026#39;, method (Int $num) returns Int:D { return self ** $num; }); 2.power(3).say; # 8 2.power(10).say; # 1024 词法方法 我可以让方法不依附于任何类, 并且能在对象上调用该方法:\nmy \u0026amp;is-even = method (Int:D :) returns Bool:D { self %% 2 }; 这构建了一个 Callable（查看以下 \u0026amp;is-even.WHAT）。 在签名中, 我把它约束为一个定义了的 Int 类型的值(Int:D), 但是没有给它名字。我在类型约束后面添加冒号来说明第一个参数是调用者。现在我能把这个方法应用到任何我想要的对象上:\nsay 137.\u0026amp;is-even; say 138.\u0026amp;is-even; say \u0026#34;foo\u0026#34;.\u0026amp;is-even; # works, although inside is-even blow up ","permalink":"https://ohmyweekly.github.io/notes/2015-11-11-add-a-method-to-an-existing-class-in-raku/","tags":["rakulang"],"title":"在 Raku 中怎么为已存在的类添加方法"},{"categories":["rakulang"],"contents":"Proc::Async 允许我们异步地执行外部程序。\nmy $proc = Proc::Async.new(\u0026#34;curl\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;-o\u0026#34;, \u0026#34;index.html\u0026#34;, \u0026#34;http://www.cpan.org\u0026#34;); my $res = await $proc.start; 我们可以在 Proc::Async 中使用超时吗? Proc::Async 没有正式支持该功能，但是我们可以很容易地实现它。看一下这个：\nclass Proc::Async::Timeout is Proc::Async { has $.timeout is rw; method start($self: |) { return callsame unless $.timeout; my $killer = Promise.in($.timeout).then: { $self.kill }; my $promise = callsame; Promise.anyof($promise, $killer).then: { $promise.result }; } } my $proc = Proc::Async::Timeout.new(\u0026#34;perl\u0026#34;, \u0026#34;-E\u0026#34;, \u0026#34;sleep 5; warn \u0026#39;end\u0026#39;\u0026#34;); $proc.timeout = 1; my $res = await $proc.start; say \u0026#34;maybe timeout\u0026#34; if $res.signal; 你甚至可以这样做：\nmy $start = Proc::Async.^methods.first(*.name eq \u0026#34;start\u0026#34;); $start.wrap: method ($self: :$timeout, |c) { return callwith($self, |c) unless $timeout; my $killer = Promise.in($timeout).then: { $self.kill }; my $promise = callwith($self, |c); Promise.anyof($promise, $killer).then: { $promise.result }; }; say await Proc::Async.new(\u0026#34;perl\u0026#34;, \u0026#34;-E\u0026#34;, \u0026#39;sleep 3; say \u0026#34;end\u0026#34;\u0026#39;).start(timeout =\u0026gt; 2); 哇，哇！如果你发现了更酷的方法，请告诉我。\nhttp://ks0608.hatenablog.com/entry/2016/05/17/001826\n","permalink":"https://ohmyweekly.github.io/notes/2015-05-14-execute-an-external-program-with-timeout-in-raku/","tags":["proc"],"title":"在 Raku 中执行有超时的外部程序"},{"categories":["rakulang"],"contents":"How does one write custom accessor methods in Raku? 我有一个类:\nclass Wizard { has Int $.mana is rw; } 我可以这样做:\nmy Wizard $gandalf .= new; $gandalf.mana = 150; 我想在不放弃使用 $gandalf.mana = 150; 的情况下, 在 setter 里面做一些检查。换句话说, 我不想这样写: $gandalf.setMana(150)。 如果程序尝试设置一个负值的话, 就退出。\nclass Wizard { has Int $!mana; method mana() is rw { return Proxy.new: FETCH =\u0026gt; sub ($) { return $!mana }, STORE =\u0026gt; sub ($, $mana) { die \u0026#34;It\u0026#39;s over 9000!\u0026#34; if ($mana // 0) \u0026gt; 9000; $!mana = $mana; } } } my Wizard $gandalf .= new; say $gandalf.mana; $gandalf.mana = 150; say $gandalf.mana; Pxoxy 是一种拦截对存储进行读写调用, 并做一些非默认行为的方式。在解析像 $gandalf.mana = $gandalf.mana + 5 这种表达式的时候, Raku 会自动调用 FETCH 和 STORE 方法。\nclass Baby { # use . instead of ! for better `.raku` representation  has Int $.mana; # overwrite the method the attribute declaration added method mana () is rw { Proxy.new( FETCH =\u0026gt; -\u0026gt; $ { $!mana }, STORE =\u0026gt; -\u0026gt; $, Int $new { die \u0026#39;invalid mana\u0026#39; unless $new \u0026gt;= 0; # placeholder for a better error $!mana = $new; }) } } my Baby $baby .= new; $baby.mana = 9; say $baby.mana; $baby.mana = -9; ","permalink":"https://ohmyweekly.github.io/notes/2015-05-19-how-to-write-custom-accessor-methods-in-raku/","tags":["accessor"],"title":"在 Raku 中自定义存取器"},{"categories":["rakulang"],"contents":"扩展 Raku 中的类型 使用继承 class BetterInt is Int { method even { self %% 2 } } my BetterInt $x .= new: 42; say $x.even; $x .= new: 71; say $x.even; say $x + 42; # OUTPUT: # True # False # 113 my BetterInt $x 约束 $x 只能包含 BetterInt 或它的子类这种类型的对象。.= new: 42 等价于 = BetterInt.new: 42。 下面的子例程期望接收一个 Int 型的参数，但是你给它传递一个 BetterInt 类型的参数它会很高兴:\nsub foo(Int $x) { say \u0026#34;\\$x is $x\u0026#34;} my BetterInt $x .= new: 42; foo $x; # OUTPUT: # $x is 42 But\u0026hellip; But\u0026hellip; But\u0026hellip; 另外一个选择是掺合进一个角色(role)。but 中缀操作符创建对象的一份拷贝并为该对象添加一个方法:\nmy $x = 42 but role { method even { self %% 2 } }; say $x.even; # OUTPUT: # True 当然角色不一定是内联的。这儿有另外一个例子使用了一个预定义的角色并且还展示了我们的对象确实被拷贝了一份：\nrole Better { method better { \u0026#34;Yes, I am better\u0026#34; } } class Foo { has $.attr is rw } my $original = Foo.new: :attr\u0026lt;original\u0026gt;; my $copy = $original but Better; $copy.attr = \u0026#39;copy\u0026#39;; say $original.attr; # still \u0026#39;original\u0026#39; say $copy.attr; # this one is \u0026#39;copy\u0026#39; say $copy.better; say $original.better; # fatal error: can\u0026#39;t find method # OUTPUT: # original # copy # Yes, I am better # Method \u0026#39;better\u0026#39; not found for invocant of class \u0026#39;Foo\u0026#39; # in block \u0026lt;unit\u0026gt; at test.p6 line 18 这看起来挺不错的，但是对于我们原来的目标来说，这个方法还是相当弱的：\nmy $x = 42 but role { method even { self %% 2 } }; say $x.even; # True $x = 72; say $x.even; # No such method! 那个角色被混合进我们容器里面存储的对象中了；所以一旦我们在容器中放进了一个新的值，或高级点的东西，那么 .even 方法就不见了，除非我们再次把那个角色混合进来。\n子例程 你知道你可以把子例程当做方法用嘛？ 你接收那个对象作为子例程的第一个位置参数并且你甚至能继续使用链式方法调用，但是不能把那些链子分解成多行：\nsub even { $^a %% 2 }; say 42.\u0026amp;even.uc; # OUTPUT: # TRUE 这确实是为核心类型添加额外功能的一种得体方式。我们的子例程定义中的 $^a 引用第一个参数（我们在调用的那个对象）并且整个子例程也可以被写为：\nsub ($x) { $x %% 2 } 飞龙在天 不管Javaccript 的那些人们怎么跟你说, 然而扩充原生类型是危险的。因为你正影响程序的所有部分。甚至看不到你的扩充的模块也受到影响。\n现在我有权告诉你，我跟你说过，你工作的核电厂融化了，让我们看看一些代码：\n# Foo.pm6 unit module Foo; sub fob is export { say 42.even; } # Bar.pm6 unit module Bar; use MONKEY-TYPING; augment class Int { method even { self %% 2 } } # test.p6 use Foo; use Bar; say 72.even; fob; # OUTPUT: # True # True 所有的行为都发生在 Bar.pm6 中。首先，我们写了一行 use MONKEY-TYPING 声明，它告诉我们正在做一些危险的行为。然后我们在类 class Int 的前面使用了 augment 关键字以扩充这个已经存在的类。我们的扩充添加了一个叫 even 的方法以告诉我们那个 Int 是否是偶数。\n所有的整数都可以使用 even 方法了，这虽然达到了我们的要求但是有点危险。\n我邪恶了 我们来扩充 Cool 类型以涵盖所有的西文排版行长单位：\nuse MONKEY-TYPING; augment class Cool { method even { self %% 2 } } .say for 72.even, \u0026#39;72\u0026#39;.even, pi.even, ½.even; # OUTPUT: # Method \u0026#39;even\u0026#39; not found for invocant of class \u0026#39;Int\u0026#39; # in block \u0026lt;unit\u0026gt; at test.p6 line 8 糟糕，程序奔溃了！原因是在我们扩充 Cool 类型的时候，派生自 Cool 的所有类型已经成型了(composed)。所以为了让它能工作，我们必须使用 .^compose 元对象协议方法来重新构成它们：\nuse MONKEY-TYPING; augment class Cool { method even { self %% 2 } } .^compose for Int, Num, Rat, Str, IntStr, NumStr, RatStr; .say for 72.even, \u0026#39;72\u0026#39;.even, pi.even, ½.even; # OUTPUT: # True # True # False # False 它现在能工作了！Int, Num, Rat, Str, IntStr, NumStr, RatStr 类型拥有了 .even 方法(注意：这些不是继承自 Cool 的仅有的类型)! 这既邪恶又让人吃惊。\n结论 当扩充 Raku 的核心类型或其它任意类的功能时，你有几种选择。\n 使用 is Class 子类 使用 but Role 混合一个角色 使用 $objec.\u0026amp;sub 调用子例程作为方法使用 使用 augment（注意安全）  http://blogs.perl.org/users/zoffix_znet/2016/04/extra-typical-perl-6.html\n","permalink":"https://ohmyweekly.github.io/notes/2015-11-25-there-is-more-than-one-way-to-extend-it/","tags":["but","compose"],"title":"扩展 Raku 中的类型"},{"categories":["rakulang"],"contents":"找出两个文件中共有的行, 顺序无关紧要 在 Perl 5 里, 你可以这样:\n#!/usr/bin/env perl use 5.010; use warnings; use strict; my %filea = map { $_ =\u0026gt; 1 } do { open my $fa, \u0026#39;\u0026lt;\u0026#39;, \u0026#39;filea\u0026#39; or die $!; \u0026lt;$fa\u0026gt; }; my %fileb = map { $_ =\u0026gt; 1 } do { open my $fb, \u0026#39;\u0026lt;\u0026#39;, \u0026#39;fileb\u0026#39; or die $!; \u0026lt;$fb\u0026gt; }; for( keys %filea ){ print if $fileb{$_}; } 在 Raku 中就长这样:\n#!/usr/bin/env raku my @a := \u0026#34;filea\u0026#34;.IO.lines; my @b := \u0026#34;fileb\u0026#34;.IO.lines; .say for keys( @a ∩ @b ); 因为 Raku 中的\u0026quot;惰性列表”, 底层实现能把工作分割成不同的任务并行执行, 然后在需要结果的时候返回给它们。所以, 这种情况下, @a 和 @b 的填充可以同时运行.但是要点是, 如果你有耗费时间, 不彼此依赖的操作, 或者函数 A 要传递一个 item 列表给函数 B, 这些操作可能并行执行, 提高速度, 你不需要做任何线程相关的东西。非常赞!\n","permalink":"https://ohmyweekly.github.io/notes/2015-06-18-find-the-common-lines-in-two-files/","tags":["set"],"title":"找到两个文件中共有的行"},{"categories":["rakulang"],"contents":"在我们探索 Raku 签名的第一部分中, 我们了解了怎么使用 Raku 强大而灵活的类型系统来约束具名参数和位置参数是如何被传递给子例程和方法的。我们还涉及了怎么使用 slurp 签名来创建能接收任意具名和位置参数列表的可变函数。\nRaku 的签名系统提供了更多。在这篇文章中我们将验证其中更高级的特性, 它们让 Raku 的调用语义更强大。\nClass 约束 你可以使用签名来指定传递进函数中的参数的类型约束。你使用的类型可以是任意类名。\nsub foo( Numeric $foo, Str $bar) { say \u0026#34;my string is $barand my number is $foo\u0026#34; } 这个签名要求我们传递 Numeric 和 Str 类型的参数。但是因为 Raku 的内置类型实际上就是类(classes), 并且因为 Numeric 拥有几个子类型, 我们可以传递进任何数字类型, 它都能工作:\nfoo(42, \u0026#34;blah\u0026#34;); foo(42.99, \u0026#34;yoohoo\u0026#34;); foo(3+9i, \u0026#34;hellooooooo\u0026#34;); # etc 我们自己定义的类中签名的工作原理也一样。\nclass Foo { has $.prop is rw; } sub inspect-a-foo( Foo $my-foo ) { say \u0026#34;this foo\u0026#39;s property is \u0026#34; ~ $my-foo.prop; } my $f = Foo.new( prop =\u0026gt; 42 ); inspect-a-foo($f); # this foo\u0026#39;s property is 42 在上面的例子中, 子例程 inspect-a-foo 只会接收 Foo 类型的参数, 或者 Foo 的子类。\n带有 where block 的特异性 通过在签名中使用 where 子句, Raku 允许我们更进一步的限制子例程的参数。where 子句接收任何 code block, 这个 code block 必须返回一个 true 值以使类型约束通过。\nsub foo(Int $positive where { $positive \u0026gt; 0 } ) { say \u0026#34;我很确信 $positive是正的!\u0026#34; } sub bar( Foo $foo where { $foo.prop.isa( Int ) and $foo.prop \u0026gt; 40 } ) { say \u0026#34;这个 Foo 的属性是一个大于 40 的整数\u0026#34; } 可以指定多个 where 子句来约束多个参数。\nsub quadrant2( Real $x where { $x \u0026lt; 0 }, Real $y where { $y \u0026gt; 0 } ) { say \u0026#34;at the point ($x, $y)\u0026#34; } quadrant2( 1, 1 ); # Constraint type check failed for parameter \u0026#39;$x\u0026#39; quadrant2( -1, -1 ); # Constraint type check failed for parameter \u0026#39;$y\u0026#39; quadrant2( -1, 1 ); # at the point (-1, 1) 约束块儿(Constraint blocks)甚至不需要是 blocks。事实上, 任何 Callable 类都可以。因此, 你可以很容易地获得功能函数的约束检测, 它们能在多个不同的子例程之间循环利用。\nsub is-positive( Real $n ) { $n \u0026gt; 0 } sub is-negative( Real $n ) { $n \u0026lt; 0 } sub is-zero( Real $n ) { $n == 0 } sub quadrant1( Real $x where is-positive( $x ), Real $y where is-positive( $y ) ) { ... } sub quadrant2( Real $x where is-negative( $x ), Real $y where is-positive( $y ) ) { ... } sub quadrant3( Real $x where is-negative( $x ), Real $y where is-negative( $y ) ) { ... } sub quadrant4( Real $x where is-positive( $x ), Real $y where is-negative( $y ) ) { ... } sub x-axis( Real $x, Real $y where is-zero( $y ) ) { ... } sub y-axis( Real $x where is-zero( $x ), Real $y ) { ... } sub origin( Real $x where is-zero( $x ), Real $y where is-zero( $y ) ) { ... } Return Types 每个 Raku 子例程也能指定它自己的返回值类型作为签名的一部分。这可以使用 returns 关键字来显式地指定, 但是我更喜欢用快捷形式的 --\u0026gt; 操作符, 它在签名自身之内。下面声明的两个子例程是等价的:\nsub are-they-equal( Str $foo, Str $bar ) returns Bool { $foo eq $bar } sub are-they-equal( Str $foo, Str $bar --\u0026gt; Bool ) { $foo eq $bar } 毫无疑问地, 如果返回错误的类型 Raku 会抛出错误。\n自省 Raku 中子例程是一等对象。但是 Raku 带来了一大堆新的内省工具, 包含询问子例程的签名信息的能力。每个子例程的签名实际上就是 Signature 类的一个对象。我们能找出子例程的元数和返回值类型。我们甚至能够在签名中抓取一个 Parameter 对象的列表。\nsub are-they-equal( Str $foo, Str $bar ) returns Bool { $foo eq $bar } say \u0026amp;are-they-equal.signature.arity; # 2 say \u0026amp;are-they-equal.signature.returns; # (Bool) my @params = \u0026amp;are-they-equal.signature.params; say @params[0].name; # $foo say @params[0].type; # (Str) say @params[0].sigil; # $ 总之, Raku 的签名很好很强大。\n","permalink":"https://ohmyweekly.github.io/notes/2015-08-29-exploring-raku-signatures-part-two/","tags":["signature"],"title":"探索 Raku 中的签名(第二部分)"},{"categories":["rakulang"],"contents":"散列也是容器 假设我们想计算某个东西的出现次数, 我们通常的做法是弄一个 \u0026ldquo;seen-hash\u0026rdquo; 散列。有时候我们有一组待查询的键, 其中有些键可能不在我们所扫描的数据中。那是一种特殊情况, 但是 Raku 能够完美地解决, 因为散列也是容器, 因此我们能够拥有默认值。\nmy $words = \u0026lt;Hashes are containers too\u0026gt;.lc; constant alphabet = \u0026#39;a\u0026#39; .. \u0026#39;z\u0026#39;; my %seen of Int is default(0); %seen{$_}++ for $words.comb; put \u0026#34;$_:%seen{$_}\u0026#34; for alphabet; 输出结果:\na: 3 b: 0 c: 1 d: 0 e: 3 f: 0 g: 0 h: 2 i: 1 j: 0 k: 0 l: 0 m: 0 n: 2 o: 3 p: 0 q: 0 r: 2 s: 3 t: 2 u: 0 v: 0 w: 0 x: 0 y: 0 z: 0 $words 中没有出现的特殊字符由 is default(0) 处理了。\n默认值可以被精心设计。我们来弄一个在数值上下文中为默认值为 0 但是在字符串上下文中为默认值为 NULL 并且总是被定义的一个散列。\nmy %seen of Int is default(0 but role :: { method Str() {\u0026#39;NULL\u0026#39;} }); say %seen\u0026lt;not-there\u0026gt;, %seen\u0026lt;not-there\u0026gt;.defined, Int.new(%seen\u0026lt;not-there\u0026gt;); # OUTPUT # «NULLTrue0␤» ","permalink":"https://ohmyweekly.github.io/notes/2015-06-28-hash-is-also-containers/","tags":["container"],"title":"散列也是容器"},{"categories":["rakulang"],"contents":"使用链式函数调用 考虑最简单的一种情况, 不带小数点的数字:\n\u0026#34;1234567890\u0026#34;.comb.reverse.rotor(3,:partial).map(*.join(\u0026#39;\u0026#39;)).join(\u0026#39;,\u0026#39;).comb.reverse.join(\u0026#39;\u0026#39;) # 1,234,567,890 使用 \\\\ 转义空白, 使代码对齐:\n\u0026#34;1234567890\u0026#34;.comb\\ .reverse\\ .rotor(3,:partial)\\ .map(*.join(‘’))\\ .join(‘,’)\\ .comb\\ .reverse\\ .join(‘’)\\ .say; 使用正则表达式 使用 Grammar ","permalink":"https://ohmyweekly.github.io/notes/2015-12-14-add-separator/","tags":["rotor"],"title":"给数字添加千分位符"},{"categories":["rakulang"],"contents":"现在你可能已经习惯了 Raku 中到处出现的前缀 \u0026ldquo;meta\u0026rdquo;。Metaclasses, Metaobjects, Metaoperators, 还有迷一般的 Meta-Object 协议。听起来一点也不可怕, 你都见过了不是吗？今天, 在 Raku Advent Calendar 上, 我们将进行完全的 meta 化(full meta)。我们将拥有能解析 grammar 的 grammar, 然后生成将用于解析 grammar 的 grammar。\nGrammar 无疑是 Raku 的杀手级功能。我们拥有了正则表达式曾经没有的东西: 可读性、可组合性当然还有解析 Raku 自身的能力。— 如果这不能展示它的强大, 那我不知道什么能够!\n为预定义好的 grammar(例如以 Bachus-Naur 形式)写解析器总是有点无趣, 几乎和复制粘贴一样。如果你曾经坐下来重头开始写一个解析器(或者期间温习一遍那本优秀的\u0026quot;让我们构建一个编译器\u0026quot;图书), 你可能会意识到模式是如此相似:从你的 grammar 中拿出单个 rule, 为它写一个子例程, 让它调用(可能是递归的)其它类似的为其它 grmmars rules 定义的子例程, 清洗, 重复。现在我们有了Raku grammar! 在这个新世界中, 我们不必为每个 token 写上子例程来完成工作了。 现在我们写 grammar 类, 里面放上 tokens、rules、regexes 标志。在标志里写正则表达式(或代码)并引用(可能是递归的) Raku gramamr 中的其它标志。如果你曾经使用过这些东西, 你肯定会意识到 Raku 中的 grammar 是多么的方便。\n但是假如我们已经有了一个 grammar, 例如之前提到过的 BNF? 我们所做的就是小心地把已经存在的 grammar(实际上在我们头脑中解析它)重新键入到一个新的 Raku Grammar 中以代表同样的一个东西, 但是那确实有一个可作为可执行代码的优势。对大多数人来说, 那都不是事儿。我们不是普通人, 我们是程序员。我们拥有资源。它们会让这些 grammar 变得有意义。\n绝妙的是 Raku grammar 和语言的其它元素没什么两样。grammar 就像类那样也是头等公民, 可以内省, 扩展。实际上, 你可以查看编译器源代码自身, 你会注意到 grammar 就是一种特定种类的类。它们遵守和类一样的规则, 允许我们就地创建 grammar, 就地给 grammar 添加 tokens, 最终完结这个 gramamr 以拥有一个合适的能实例化的类对象。现在既然我们能解析 BNF grammar(因为它们就是普通的文本)并从代码中创建 Raku grammar, 让我们把这些片段放在一起并写点能手动把 BNF gramamr 转化为 Raku grammar 的东西。\n解析 BNF grammar 的 grammar grammar Grammar::BNF { token TOP {\\s*\u0026lt;rule\u0026gt;+\\s*} token rule {\u0026lt;opt-ws\u0026gt;\u0026#39;\u0026lt;\u0026#39;\u0026lt;rule-name\u0026gt;\u0026#39;\u0026gt;\u0026#39;\u0026lt;opt-ws\u0026gt;\u0026#39;::=\u0026#39;\u0026lt;opt-ws\u0026gt;\u0026lt;expression\u0026gt;\u0026lt;line-end\u0026gt;} token expression {\u0026lt;list-of-terms\u0026gt;+%[\\s*\u0026#39;|\u0026#39;\u0026lt;opt-ws\u0026gt;]} token term {\u0026lt;literal\u0026gt;|\u0026#39;\u0026lt;\u0026#39;\u0026lt;rule-name\u0026gt;\u0026#39;\u0026gt;\u0026#39;} token list-of-terms {\u0026lt;term\u0026gt;+%\u0026lt;opt-ws\u0026gt;} token rule-name {\u0026lt;-[\u0026gt;]\u0026gt;+} token opt-ws {\\h*} token line-end {[\u0026lt;opt-ws\u0026gt;\\n]+} token literal {\u0026#39;\u0026#34;\u0026#39;\u0026lt;-[\u0026#34;]\u0026gt;*\u0026#39;\u0026#34;\u0026#39;|\u0026#34;\u0026#39;\u0026#34;\u0026lt;-[\u0026#39;]\u0026gt;*\u0026#34;\u0026#39;\u0026#34;} ... } 最上层的 3 个 token 发生了有意思的事情。rule 是 BNF grammar 的核心构造块: 一个 \u0026lt;symbol\u0026gt; ::= \u0026lt;expression\u0026gt; 块儿, 后面跟着一个换行符。整个 grammar 就是一列 rule。每个表达式是一列项、或可能的和它们的备选分支。每个项要么是一个字面值, 或一个由尖括号包围的标志名。足够了! 那涵盖了解析部分。让我们看一下生成自身。我们的确有一种\u0026quot;为 grammar 中的每个 token 做某事\u0026quot;的机制, 以 Actions的形式, 让我们继续并使用它:\nmy class Actions { has $.name = \u0026#39;BNFGrammar\u0026#39;; method TOP($/) { my $grmr := Metamodel::GrammarHOW.new_type(:$.name); $grmr.^add_method(\u0026#39;TOP\u0026#39;, EVAL \u0026#39;token { \u0026lt;\u0026#39; ~ $\u0026lt;rule\u0026gt;[0].ast.key ~ \u0026#39;\u0026gt; }\u0026#39;); for $\u0026lt;rule\u0026gt;.map(*.ast) -\u0026gt; $rule { $grmr.^add_method($rule.key, $rule.value); } $grmr.^compose; make $grmr; } method expression($/) { make EVAL \u0026#39;token { \u0026#39; ~ ~$/ ~ \u0026#39;}\u0026#39;; } method rule($/) { make ~$\u0026lt;rule-name\u0026gt; =\u0026gt; $\u0026lt;expression\u0026gt;.ast; } } TOP 方法毫无疑问是最魔幻和最恐怖的, 所以擒贼先擒王, 其它小喽啰就无关紧要了。基本上, TOP 那儿发生了三件事:\n 1、我们创建了一个新的 grammar, 作为一个新的 Raku 类型 2、我们使用 ^add_method 方法为 grammar 添加 token 3、我们使用 ^compose 方法定型该 grammar  虽然 Raku 指定名为 TOP 的 token 是解析开始的地方, 在 BNF 中第一个 rule 总是开始点。为了彼此适应, 我们精巧地制作了一个假的 TOP token, 它正是调用了 BNF grammar 中指定的第一个 rule。不可避免地, 恐怖又令人失望的 EVAL 引起了我们的注意, 就像它说了\u0026quot;这儿发生了可怕的事情\u0026quot; 一样。它那样说并不是完全错误的, 但是因为我们没有其它程序化构建单独正则的方法, 我们不得不\u0010接受这点不适。\nTOP 之后我们继续为我们的 grammar 添加 BNF rule 的剩余部分, 这一次保留它们原来的名字, 然后 ^compose 整个东西, 最后让它(make)成为解析的结果: 一个做好的解析类。\n在 expression 方法中我们把解析过的 BNF 元素粘贴到一块以产生合法的 Raku 代码。这变得特别容易, 因为那俩个单独的标志带有空格, 使用管道符号来轮试备选分支, 并使用尖括号包围标志名。目前为止, 一个 rule 看起来像这样:\n\u0026lt;foo\u0026gt; ::= \u0026#39;bar\u0026#39; | \u0026lt;baz\u0026gt; 我们求值(EVAL)的 Raku 代码变为:\ntoken { \u0026#39;bar\u0026#39; | \u0026lt;baz\u0026gt; } 因为我们已经在我们代码的 grammar 部分检测我们解析的 BNF 是正确的, 没有什么能够阻止我们传递解析整个表达式字面值到我们的代码中并使用一个 token { } 来包裹它, 所以让我们继续。\n最后, 对于我们解析的每一个 BNF rule, 我们产生了一个很不错的 Pair, 所以我们的 TOP 方法很愉快地处理它们中的每个。\n看起来我们好像在这儿结束了, 但是仅仅是为了方便使用者, 让我们写一个更好的方法, 接收一个 BNF grammar, 并为我们生成一个准备好使用的类型对象。我们记得, grammar 就是类, 所以我们没有什么能阻止我们直接为我们的 gramamr 添加它:\ngrammar Grammar::BNF { ... method generate($source, :$name = \u0026#39;BNFGrammar\u0026#39;) { my $actions = Actions.new(:$name); my $ret = self.new.parse($source, :$actions).ast; return $ret.WHAT; } } 这儿看起来很不错! 在你开始往你自己的项目中复制粘贴所有这些之前, 记得 Grammar::BNF 是一个可在 Raku Module Ecosystem获得的 Raku 模块, 使用你喜欢的模块管理器安装。\n假设你确实花费时间查看了开头的 post, 你可能会记得我许诺过我们将有 grammar(第一条)来解析 grammar(第二条), 然后生成 grammar(第三条), 使用生成的 grammar 来解析 grammar(第四条)。目前为止， 我们已经看到过 BNF::Grammar grammar(那是第一条), 并解析一个 BNF grammar(那是第二条), 以类对象的形式来生成 Raku grammar(第三条)。 就这些。我们仍旧缺乏最后一部分, 使用整个东西来解析 grammar。 我们只完成了 75% 的 meta 化, 今天足够了。为什么现在停止? 为什么不拿一个 BNF grammar , 使用 Raku grammar 来解析 grammar, 使用 Raku BNF grammar 的结果来解析我们原来的 BNF Grammar? 那不是很好吗？是的, 那很好, 我们只是留了一个练习给你。\n","permalink":"https://ohmyweekly.github.io/notes/2015-04-15-grammar-generating-grammar/","tags":["grammar"],"title":"Grammar Generating Grammar"},{"categories":["rakulang"],"contents":"薛定谔欧文应该是喜欢 Raku 的, 因为他的著名的薛定谔的猫可以用 Raku 的 Junction表达:\nmy $cat = \u0026#39;dead\u0026#39; | \u0026#39;alive\u0026#39;; say \u0026#34;cat is both dead and alive\u0026#34; if $cat eq \u0026#39;dead\u0026#39; and $cat eq \u0026#39;alive\u0026#39;; # OUTPUT: # cat is both dead and alive 这里面发生了什么事情? 我会告诉你全部的!\nAnyone 游戏? 拿最简单的来说, Junction 允许你把一堆值当作单个值。例如, 你可以使用 any Junction 来测试一个变量是否等于所给定值中的任意一个:\nsay \u0026#39;it matches!\u0026#39; if \u0026#39;foo\u0026#39; eq \u0026#39;foo\u0026#39; | \u0026#39;bar\u0026#39; | \u0026#39;ber\u0026#39;; say \u0026#39;single-digit prime\u0026#39; if 5 == any ^9.grep: *.is-prime; my @values = ^100; say \u0026#34;it\u0026#39;s in there!\u0026#34; if 42 == @values.any; # OUTPUT: # it matches! # single-digit prime # it\u0026#39;s in there! 要从一堆值中创建一个 any Junction, 你可以使用 | 中缀操作符、调用 any 函数或者使用 .any 方法。上面的条件会返回 True 如果 Junction 中的任意一个(any) 值匹配所给定的值的话。事实上, 没有人能阻止你在两端都使用 Junction:\nmy @one = 1..10; my @two = 5..15; say \u0026#34;There\u0026#39;s overlap!\u0026#34; if @one.any == @two.any; # OUTPUT: # there\u0026#39;s overlap! 运算符会返回 True 如果 @one 中的任意一个值(any) 在数值上等于 @two 中的任意一个值(any)的话。这个语法糖很甜, 但是我们还可以做的更多。\nAll for One and Any for None any Junction 唯一一个你能获得的 Junction。你还可以选择 all、any、one 和 none。当在布尔上下文中时, 它们的意思就像下面这样; 构建 Junction 的函数/方法名和 Junction 自身的名字一样并且下面还列出了构建 Junction 的中缀操作符:\n all — 所有的值都被计算为 True(使用中缀 \u0026amp;) any — 至少其中的一个值被计算为 True(使用中缀 |) one — 正好其中有一个值被计算为 True(使用中缀 ^) none — 没有一个值被计算为 True(没有可用的中缀)  使用 all JUnction 时要特别注意:\nmy @values = 2, 3, 5; say \u0026#39;all primes\u0026#39; if @values.all ~~ *.is-prime; my @moar-values; say \u0026#39;also all primes\u0026#39; if @moar-values.all ~~ *.is-prime; 即使它没有值的时候也会返回 True, 这可能不是你想要的。在那些情况下, 你可以使用:\nmy @moar-values; say \u0026#39;also all primes\u0026#39; if @moar-values and @moar-values.all ~~ *.is-prime; Call Me, Baby 你可以把 Junction 用作并不期望 Junction 的子例程的参数。那么会发生什么呢? 对于每一个 Junctioned 的值, 那个子例程都会被调用一次, 并且返回值会是一个 Junction：\nsub caculate-things($n) { say \u0026#34;$nis prime\u0026#34; if $n.is-prime; say \u0026#34;$nis an even number\u0026#34; if $n %% 2; say \u0026#34;$nis pretty big\u0026#34; if $n \u0026gt; 1e6; $n²; } my @values = 1, 5, 42, 1e10.Int; say \u0026#39;EXACTLY ONE square is larger than 1e10\u0026#39; if 1e10 \u0026lt; calculate-things @values.one; # OUTPUT: # 5 is a prime # 42 is an even number # 10000000000 is an even number # 10000000000 is pretty big # EXACTLY ONE square is larger than 1e10 暴露的副作用可能有点太过神奇并且你可能不想在生产代码中看到它, 但是使用一个子例程来修改原来的 Junctioned 化的值是相当能接受的。执行一个数据库查询来获取\u0026quot;实际的\u0026quot;值并且在之后计算那个条件怎么样：\nuse DBIish; my $dbh = DBIish.connect: \u0026#39;SQLite\u0026#39;, :database\u0026lt;test.db\u0026gt;; sub lookup ($id) { given $dbh.prepare: \u0026#39;SELECT id, text FROM stuff WHERE id = ?\u0026#39; { .execute: $id; .allrows[0][1] // \u0026#39;\u0026#39;; } } my @ids = 3, 5, 10; say \u0026#39;yeah, it got it, bruh\u0026#39; if \u0026#39;meow\u0026#39; eq lookup @ids.any; # OUTPUT (the database has a row with id = 5 and text = \u0026#39;meow\u0026#39;): # yeah, it got it, bruh 我们一直在期盼你, 请坐 那个游戏变化了当你的子例程正好期望一个 Junction 作为参数的时候。\nsub do-stuff (Junction $n) { say \u0026#39;value is even\u0026#39; if $n %% 2; say \u0026#39;value is prime\u0026#39; if $n.is-prime; say \u0026#39;value is large\u0026#39; if $n \u0026gt; 1e10; } do-stuff (2, 3, 1e11.Int).one; say \u0026#39;---\u0026#39;; do-stuff (2, 3, 1e11.Int).any; # OUTPUT: # value is large # --- # value is even # value is prime # value is large 当我们提供了一个 one Junction 时, 只有正好满足给定值中的其中一个条件才会被触发。当我们提供一个 any Junction 时, 满足条件的任何一个给定值都会触发。\n但是! 你没有必要非等着世界为你分发 Junction。你自己制造一个怎么样呢, 还能在测试条件时节省代码:\nsub do-stuff (*@v) { my $n = @v.one; say \u0026#34;$nis even\u0026#34; if $n %% 2; say \u0026#34;$nis prime\u0026#34; if $n.is-prime; say \u0026#34;$nis large\u0026#34; if $n \u0026gt; 1e10; } do-stuff 2, 3, 1e11.Int; say \u0026#39;---\u0026#39;; do-stuff 42; # OUTPUT: # one(2, 3, 100000000000) is large # --- # one(42) is even 没有人想过将来吗? 还有一个小秘密: Junction 被设计为时自动线程化的(即 auto-threaded)。尽管在写这篇文章的时候它们只会使用仅仅一个线程, 你不应该依赖它们能以任何可预测的顺序被执行。自动线程化会在 2018 年的某个时间被实现, 所以保持关注\u0026hellip; 你不必做任何事情, 你的值得自动线程化的复杂 Junctioned 化的操作可能会在几年之内变得更快。\n结论 Raku 的 Junction 是值的叠加态, 它允许你测试多个值就像它们是一个值一样。除了提供非常短并且易读的语法, Junction 还允许你使用子例程变换叠加值或者使用副作用。\n你还可以生成显式操作 Junction 的子例程或者把提供的多个值转换成 Junction 以简化代码。\n最后, Junction 被设计为能使用所有你计算机所提供的可用能力并且在不久的将来会做成自动线程化。\nJunction 很精彩, 使用它们, 玩的开心!\n","permalink":"https://ohmyweekly.github.io/notes/2015-03-22-junctions-in-raku/","tags":["junction"],"title":"Junction in Raku"},{"categories":["rakulang"],"contents":"Capture 的定义:\nclass Capture does Positional does Associative { } Capture 是一个用于给 code 对象传递参数的容器。Captures 是签名的另一面 — Captures 在调用方定义实参, 而签名(Signatures) 在被调用方定义形式参数。\n当你调用 print $a, $b 时, $a, $b 这部分就是一个 Capture。$a, $b 在这儿是实参。\nCaptures 包含一个 list-like 部分的位置参数和一个 hash-like 部分的具名参数。对于具名参数, Captures 使用一种略微不同的语法而不是普通的 List。有两种简单的方法生成一个具名参数：\n 使用一个未引起的键命名一个形参, 后面跟着 =\u0026gt;, 然后跟着参数 使用以形参命名的冒号对儿字面量  say unique 1, -2, 2, 3, as =\u0026gt; { abs $_ }; # 1, -2, 3 # ... is the same thing as: say unique 1, -2, 2, 3, :as({ abs $_ }); # 1, -2, 3 # Be careful not to quote the name of a named parameter: say unique 1, -2, 2, 3, \u0026#39;as\u0026#39; =\u0026gt; { abs $_ }; # 1, -2, 2, 3, \u0026#34;as\u0026#34; =\u0026gt; { ... } 单个独立的 Capture 也能被生成, 存储, 然后供之后使用。 在项(term)那儿前置一个反斜线 \\ 能生成一个字面的 Capture。通常, 这个 term 会是一个 terms 的列表, 在这个列表里面, 任何 Pair 字面值会被放在 Capture 的具名部分, 而其它 terms 会被放在Capture 的位置(positional) 部分。\nmy $c = \\(42); # 带有一个 positional 部分的 Capture  $c = \\(1, 2, a =\u0026gt; \u0026#39;b\u0026#39;); # 带有两个 positional 部分和一个具名部分的 Capture 要使用这样的 Capture, 在函数调用里你可以在这个 Capture 前面使用 | , 那么它看起来会像这个 Capture 中的所有值都被作为实参直接传递这个函数了 — 具名参数作为具名参数传递, 而位置参数会作为位置参数传递。 只要你愿意, 你可以重用这个 Capture 任意次, 甚至使用不同的函数。\nmy $c = \\(4,2,3); reverse(|$c).say; # 3 2 4 sort(5,|$c).say; # 2 3 4 5 在签名( Signature) 里面, 可以在不含符号的形参那儿前置一个竖线 | 来创建一个 Capture。这会把余下的参数列表打包到那个形参中：\nsub f($a, |c) { say c; } f(1, 2, 3, a =\u0026gt; 4, b =\u0026gt; 5); # c is \\(2, 3, a =\u0026gt; 4, b =\u0026gt; 5) 请注意，Capture 仍然是列表，因为它们可能包含容器，而不只是值:\nmy $b = 1; my $c = \\(4,2,$b,3); sort(|$c).say; # 1 2 3 4 $b = 6; sort(|$c).say; # 2 3 4 6 Capture 方法(From Mu) 定义为:\nmethod Capture(Mu:D: --\u0026gt; Capture:D) 返回与调用者的公共属性相对应的命名参数的 Capture：\nclass Foo { has $.foo = 42; has $.bar = 70; method bar { \u0026#39;something else\u0026#39; } }.new.Capture.say; # OUTPUT: «\\(:bar(\u0026#34;something else\u0026#34;), :foo(42))␤»  Cpature 方法(from List) 定义为:\nmethod Capture(--\u0026gt; Capture:D) 返回一个 Capture，其中列表中的每个 Pair（如果有）已转换为命名参数（使用Pair string的键）。列表中的所有其他元素按照它们被发现的顺序转换为位置参数，即列表中的第一个非 Pair item 成为第一个位置参数，其获得索引 0，第二个非 pair item 成为第二个位置参数，索引为 1 等。\nmy $list = (7, 5, a =\u0026gt; 2, b =\u0026gt; 17); my $capture = $list.Capture; say $capture.keys; # OUTPUT: «(0 1 a b)␤»  my-sub(|$capture); # RESULT: «7, 5, 2, 17»  sub my-sub($first, $second, :$a, :$b) { say \u0026#34;$first, $second, $a, $b\u0026#34; } 一个更高级的例子是所返回的 Capture 与 签名 进行智能匹配。\nmy $list = (7, 5, a =\u0026gt; 2, b =\u0026gt; 17); say so $list.Capture ~~ :($ where * == 7,$,:$a,:$b); # OUTPUT: «True␤»  $list = (8, 5, a =\u0026gt; 2, b =\u0026gt; 17); say so $list.Capture ~~ :($ where * == 7,$,:$a,:$b); # OUTPUT: «False␤»  ","permalink":"https://ohmyweekly.github.io/notes/2015-04-01-capture-in-raku/","tags":["capture"],"title":"Raku 中的捕获"},{"categories":["rakulang"],"contents":"操作符 操作符优先级 在像 1 + 2 * 3 这样的表达式中, 2 * 3 被首先计算, 因为中缀操作符 * 的优先级比 + 的优先级高。下面的表中总结了 Perl 6 中 的优先级级别, 从最牢固到最松散：\nA\tLevel\tExamples N\tTerms\t42 3.14 \u0026quot;eek\u0026quot; qq[\u0026quot;foo\u0026quot;] $x :!verbose @$array L\t方法后缀\t.meth .+ .? .* .() .[] .{} .\u0026lt;\u0026gt; .«» .:: .= .^ .: N\t自增\t++ -- R\t求幂\t** L\tSymbolic unary\t! + - ~ ? | || +^ ~^ ?^ ^ L\t乘法\t* / % %% +\u0026amp; +\u0026lt; +\u0026gt; ~\u0026amp; ~\u0026lt; ~\u0026gt; ?\u0026amp; div mod gcd lcm L\t加法\t+ - +| +^ ~| ~^ ?| ?^ L\t重复\tx xx X\t连结 ~ X\tJunctive and\t\u0026amp; X\tJunctive or\t| ^ L\tNamed unary\ttemp let N\tStructural infix but does \u0026lt;=\u0026gt; leg cmp .. ..^ ^.. ^..^ C\tChaining infix\t!= == \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= eq ne lt le gt ge ~~ === eqv !eqv X\tTight and\t\u0026amp;\u0026amp; X\tTight or\t|| ^^ // min max R\tConditional\t?? !! ff fff R\tItem assignment\t= =\u0026gt; += -= **= xx= .= L\tLoose unary\tso not X\tComma operator\t, : X\tList infix\tZ minmax X X~ X* Xeqv ... R\tList prefix\tprint push say die map substr ... [+] [*] any Z= X\tLoose and\tand andthen X\tLoose or\tor xor orelse X\tSequencer\t\u0026lt;==, ==\u0026gt;, \u0026lt;\u0026lt;==, ==\u0026gt;\u0026gt; N\tTerminator\t; {...}, unless, extra ), ], } 下面使用的两处 ! 符号一般代表任何一对儿拥有相同优先级的操作符, 上表指定的二元操作符的结合性解释如下(其中 A 代表结合性, associativities )：\nA\tAssoc\tMeaning of $a ! $b ! $c L\tleft\t($a ! $b) ! $c R\tright\t$a ! ($b ! $c) N\tnon\tILLEGAL C\tchain\t($a ! $b) and ($b ! $c) X\tlist\tinfix:\u0026lt;!\u0026gt;($a; $b; $) 对于一元操作符, 这解释为:\nA\tAssoc\tMeaning of !$a! L\tleft\t(!$a)! R\tright\t!($a!) N\tnon\tILLEGAL 下面描述的操作符, 默认假定为 left 结合性。\n操作符种类 操作符能出现在相对于 term 的几个位置处：\n+term\tprefix (后缀) term1 + term2\tinfix (中缀) term++\tpostfix (后缀) (term)\tcircumfix (环缀) term1[term2]\tpostcircumfix (后环缀) 每个操作符也可以用作子例程。这样的子例程的名字由操作符的种类, 然后后跟一个冒号, 再加上一组引号结构, 引号结构中是组成操作符的符号(s):\ninfix:\u0026lt;+\u0026gt;(1, 2) # same as 1 + 2 circumfix:«( )»(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;) # same as (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;), 目前编译错误。 circumfix:\u0026lt;[ ]\u0026gt;(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;).raku.say # [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;] 作为一种特殊情况, listop(列表操作符)既能作为 term 又能作为前缀。子例程调用是最常见的列表操作符。其它情况包括元运算中缀操作符 [+]| 1, 2, 3 和 prefix 等 stub 操作符。\n定义自定义操作符在 /language/functions#Defining_Operators. 中有涉及。\nTerm 优先级 环缀 \u0026lt; \u0026gt; 引起单词的结构, 以空白隔开内容。如果单词看起来像数字字面量或 Pair 字面量, 那么它会被转为合适的数字。\nsay \u0026lt;a b c\u0026gt;[1] # b 环缀 ( ) 分组操作符。\n在参数列表中, 在参数周围放上圆括号防止了参数被解释为具名参数。\nmulti sub p(:$a!) { say \u0026#39;named\u0026#39; } multi sub p($a) { say \u0026#39;positional\u0026#39; } p a =\u0026gt; 1; # named p (a =\u0026gt; 1); # positional 环缀 { } Block 或 散列构造器。\n如果 {} 里面的内容看起来像一组 pairs 并且没有 $_ 或其它占位符参数, 就返回一个散列, 这个散列由逐项逐项的 Pair 组成。\n否则就返回一个 Block。\n注意, 这个结构没有重新解析内容; 而里面的内容总是被解析为一组句子（例如, 像一个 block）, 并且如果后面的分析表明它需要被解析成一个散列, 那么 block 就会被执行并强转为散列。\n环缀 [ ] 数组构造器。在列表上下文中返回一个不会展平的 item 化的数组。\n方法后缀优先级 后环缀 [ ] sub postcircumfix:\u0026lt;[ ]\u0026gt;(@container, **@index, :$k, :$v, :$kv, :$p, :$exists, :$delete) :$k 会创建一个 Pair, 它是散列中的一个条目。键是 k, 键值为 $kv。所以, $k 等价于 k =\u0026gt; $k\n访问 @container 中的一个或多个元素, 即数组索引操作：\nmy @alphabet = \u0026#39;a\u0026#39; .. \u0026#39;z\u0026#39;; say @alphabet[0]; #-\u0026gt; a say @alphabet[1]; #-\u0026gt; b say @alphabet[*-1]; #-\u0026gt; z say @alphabet[100]:exists; #-\u0026gt; False say @alphabet[15, 4, 17, 11].join; #-\u0026gt; perl say @alphabet[23 .. *].raku; #-\u0026gt; (\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;) @alphabet[1, 2] = \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;; say @alphabet[0..3].raku #-\u0026gt; (\u0026#34;a\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;d\u0026#34;) 查看 Subscripts 获取关于该操作符行为的更详细的解释, 还有怎么在自定义类型中实现对它的支持。\n后环缀 { } sub postcircumfix:\u0026lt;{ }\u0026gt;(%container, **@key, :$k, :$v, :$kv, :$p, :$exists, :$delete) 访问 %container 的一个或多个元素, 即散列索引操作：\nmy %color = kiwi =\u0026gt; \u0026#34;green\u0026#34;, banana =\u0026gt; \u0026#34;yellow\u0026#34;, cherry =\u0026gt; \u0026#34;red\u0026#34;; say %color{\u0026#34;banana\u0026#34;}; #-\u0026gt; yellow say %color{\u0026#34;cherry\u0026#34;, \u0026#34;kiwi\u0026#34;}.raku; #-\u0026gt; (\u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;) say %color{\u0026#34;strawberry\u0026#34;}:exists; #-\u0026gt; False %color{\u0026#34;banana\u0026#34;, \u0026#34;lime\u0026#34;} = \u0026#34;yellowish\u0026#34;, \u0026#34;green\u0026#34;; %color{\u0026#34;cherry\u0026#34;}:delete; say %color; #-\u0026gt; banana =\u0026gt; yellowish, kiwi =\u0026gt; green, lime =\u0026gt; green 查看后环缀 \u0026lt; \u0026gt; 和后环缀 « » 作为便捷形式, 查看 Subscripts 获取这个操作符行为的更详细解释, 还有怎么在自定义类型中实现对它的支持。\n后环缀 \u0026lt; \u0026gt; 后环缀 { } 的简写形式, 它会引起它的参数。\nmy %color = kiwi =\u0026gt; \u0026#34;green\u0026#34;, banana =\u0026gt; \u0026#34;yellow\u0026#34;, cherry =\u0026gt; \u0026#34;red\u0026#34;; say %color\u0026lt;banana\u0026gt;; #-\u0026gt; yellow say %color\u0026lt;cherry kiwi\u0026gt;.raku; #-\u0026gt; (\u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;) say %color\u0026lt;strawberry\u0026gt;:exists; #-\u0026gt; False 这不是一个真正的操作符, 它仅仅是一个在编译时把 \u0026lt; \u0026gt; 变成 {} 后环缀操作符的语法糖。\n后环缀 « » 后环缀 { } 的简写形式。它会引起它的参数, 并且 « » 中能进行变量插值。\nmy %color = kiwi =\u0026gt; \u0026#34;green\u0026#34;, banana =\u0026gt; \u0026#34;yellow\u0026#34;, cherry =\u0026gt; \u0026#34;red\u0026#34;; my $fruit = \u0026#34;kiwi\u0026#34;; say %color«cherry $fruit».raku; #-\u0026gt; (\u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;) 这不是一个真正的操作符, 它仅仅是一个在编译时把 « » 变成 {} 后环缀操作符的语法糖。\n后环缀 ( ) 调用操作符。把调用者当作 Callable 并引用它, 它使用圆括号之间的表达式作为参数。\n注意, 标识符后面直接跟着一对儿圆括号总是被解析为子例程调用。\n如果你想要你的对象响应该调用操作符, 你需要实现 CALL-ME 方法。\npostfix . 该操作符用于调用一个方法, $invocant.method。\n技术上讲, 这不是一个操作符, 而是编译器中特殊情况下的语法。\npostfix .= 可变的方法调用。$invocant.=method, 脱去语法糖后就是 $invocant = $invocant.method, 这与 op=. 类似。\n技术上讲, 这不是一个操作符, 而是编译器中特殊情况下的语法。\npostfix .^ 元方法调用。 $invocant.^method 在 $invocant 的元类身上调用方法。脱去语法糖后, 它就是 $invocant.HOW.method($invocant, ...)。查看 HOW 获取更多信息。\n技术上讲, 这不是一个操作符, 而是编译器中特殊情况下的语法。\npostfix .? 有可能被调用的方法调用。如果有名为 method 的方法, $invocant.?method 就在 $invocant 上调用 method 方法。否则它就返回 Nil。\n技术上讲, 这不是一个操作符, 而是编译器中特殊情况下的语法。\npostfix .+ $invocant.+method  从 $invocant 身上调用所有叫做 method 的方法。如果没有找到这个名字的方法, 就会死掉。\n技术上讲, 这不是一个操作符, 而是编译器中特殊情况下的语法。\npostfix .* $invocant.*method 从 $invocant 身上调用所有叫做 method 的方法。\n技术上讲, 这不是一个操作符, 而是编译器中特殊情况下的语法。\npostfix .postfix 大多数情况下, 可以在后缀或后环缀前面放上一个点：\n@a[1, 2, 3]; @a.[1, 2, 3]; # Same 这对于视觉清晰或简洁很有帮助。例如, 如果对象的属性是一个函数, 在属性名后面放置一对儿圆括号会变成方法调用的一部分。所以要么使用两对儿圆括号, 要么在圆括号前面放上一个点来阻止方法调用。\nclass Operation { has $.symbol; has \u0026amp;.function; } my $addition = Operation.new(:symbol\u0026lt;+\u0026gt;, :function{ $^a + $^b }); say $addition.function()(1, 2); # 3 或者:\nsay $addition.function.(1,2); # 3 然而, 如果后缀是一个标识符, 那么它会被解释为一个普通的方法调用。\n1.i # No such method 'i' for invocant of type 'Int' 技术上讲, 这不是一个操作符, 而是编译器中特殊情况下的语法。\npostfix .: 前缀能够像方法那样, 使用冒号对儿标记法来调用。例如：\nmy $a = 1; say ++$a; # 2 say $a.:\u0026lt;++\u0026gt;; # 3 技术上讲, 这不是一个操作符, 而是编译器中特殊情况下的语法。\npostfix .:: 一个类限定的方法调用, 用于调用一个定义在父类或 role 中的方法, 甚至在子类中重新定义了之后。\nclass Bar { method baz { 42 } } class Foo is Bar { method baz { \u0026#34;nope\u0026#34; } } say Foo.Bar::baz; # 42 自增优先级 prefix ++ multi sub prefix:\u0026lt;++\u0026gt;($x is rw) is assoc\u0026lt;none\u0026gt; 把它的参数增加 1, 并返回增加后的值。\nmy $x = 3; say ++$x; # 4 say $x; # 4 它的工作原理是在它的参数身上调用 succ 方法, 这可以让自定义类型自由地实现它们自己的增量语义。\nprefix \u0026ndash; multi sub prefix:\u0026lt;--\u0026gt;($x is rw) is assoc\u0026lt;none\u0026gt; 把它的参数减少 1, 并返回减少后的值。\nmy $x = 3; say --$x; # 2 say $x; # 2 它的工作原理是在它的参数身上调用 pred 方法, 这可以让自定义类型自由地实现它们自己的减量语义。\npostfix ++ multi sub postfix:\u0026lt;++\u0026gt;($x is rw) is assoc\u0026lt;none\u0026gt; 把它的参数增加 1, 并返回 unincremented 的那个值。\nmy $x = 3; say $x++; # 3 say $x; # 4 它的工作原理是在它的参数身上调用 succ 方法, 这可以让自定义类型自由地实现它们自己的增量语义。\n注意这并不一定返回它的参数。例如, 对于未定义的值, 它返回 0:\nmy $x; say $x++; # 0 say $x; # 1 postfix \u0026ndash; multi sub postfix:\u0026lt;--\u0026gt;($x is rw) is assoc\u0026lt;none\u0026gt; 把它的参数减少 1, 并返回 undecremented 的那个值。\nmy $x = 3; say $x--; # 3 say $x; # 2 它的工作原理是在它的参数身上调用 pred 方法, 这可以让自定义类型自由地实现它们自己的减量语义。\n注意这并不一定返回它的参数。例如, 对于未定义的值, 它返回 0:\nmy $x; say $x--; # 0 say $x; # -1 求幂优先级 infix ** multi sub infix:\u0026lt;**\u0026gt;(Any, Any) returns Numeric:D is assoc\u0026lt;right\u0026gt; 求幂操作符把它的两个参数都强制转为 Numeric , 然后计算, 右侧为幂。\n如果 ** 右边是一个非负整数, 并且左侧是任意精度类型(Int, FatRat), 那么计算不会损失精度。\n象形一元操作符的优先级 prefix ? multi sub prefix:\u0026lt;?\u0026gt;(Mu) returns Bool:D 布尔上下文操作符。\n通过在参数身上调用 Bool 方法强制它的参数为 Bool。注意, 这会使 Junctions 失效。\nprefix ! multi sub prefix:\u0026lt;!\u0026gt;(Mu) returns Bool:D 否定的布尔上下文操作符。\n通过在参数身上调用 Bool 方法强制它的参数为 Bool, 并返回结果的否定值。注意, 这会使 Junctions 失效。\nprefix + multi sub prefix:\u0026lt;+\u0026gt;(Any) returns Numeric:D Numeric 上下文操作符。\n通过在参数身上调用 Numeric 方法强制将参数转为 Numeric 类型。\nprefix - multi sub prefix:\u0026lt;-\u0026gt;(Any) returns Numeric:D 否定的 Numeric 上下文操作符。\n通过在参数身上调用 Numeric 方法强制将参数转为 Numeric 类型, 并返回结果的否定值。\nprefix ~ multi sub prefix:\u0026lt;-\u0026gt;(Any) returns Str:D 字符串上下文操作符。\n通过在参数身上调用 Str 方法强制把参数转为 Str 类型。\nprefix | 将 Capture, Enum, Pair, List, EnumMap 和 Hash 展平到参数列表中。\n（在 Rakudo 中, 这不是作为一个合适的操作符来实现的, 而是编译器中的一种特殊情况, 这意味着它只对参数列表有效, 而非在任意代码中都有效。）\nprefix +^ multi sub prefix:\u0026lt;+^\u0026gt;(Any) returns Int:D 整数按位取反。\n将参数强转为 Int 类型并对结果按位取反, 假设两者互补。\nprefix ?^ multi sub prefix:\u0026lt;?^\u0026gt;(Mu) returns Bool:D 布尔按位取反。\n将参数强转为 Bool, 然后按位反转, 这使它和 prefix:\u0026lt;!\u0026gt; 一样。\nprefix ^ multi sub prefix:\u0026lt;^\u0026gt;(Any) returns Range:D upto 操作符.\n强制把它的参数转为 Numeric, 生成一个从 0 直到（但是排除） 参数为止的范围。\nsay ^5; # 0..^5 for ^5 { } # 5 iterations 乘法优先级 infix * multi sub infix:\u0026lt;*\u0026gt;(Any, Any) returns Numeric:D 把两边的参数都强转为 Numeric 并把它们相乘。结果是一个更宽的类型。查看 Numeric 获取更详细信息。\ninfix / multi sub infix:\u0026lt;/\u0026gt;(Any, Any) returns Numeric:D 把两边的参数都强制为 Numeric, 并用左边除以右边的数。整数相除返回 Rat, 否则返回\u0026quot;更宽类型\u0026quot;的结果。\ninfix div multi sub infix:\u0026lt;div\u0026gt;(Int:D, Int:D) returns Int:D 整除。向下取整。\ninfix % multi sub infix:\u0026lt;%\u0026gt;($x, $y) return Numeric:D 模操作符。首先强制为 Numeric。\n通常, 下面的等式是成立的：\n$x % $y == $x - floor($x / $y) * $y infix %% multi sub infix:\u0026lt;%%\u0026gt;($a, $b) returns Bool:D 整除操作符, 如果 $a % $b == 0 则返回 True.\ninfix mod multi sub infix:\u0026lt;mod\u0026gt;(Int:D $a, Int:D $b) returns Int:D 整数取模操作符。返回整数取模操作的剩余部分。\ninfix +\u0026amp; multi sub infix:\u0026lt;+\u0026amp;\u0026gt;($a, $b) returns Int:D Numeric 按位 AND。把两个参数都强转为 Int 并执行按位 AND 操作, 假定两者是互补的。\ninfix +\u0026lt; multi sub infix:\u0026lt;\u0026lt;+\u0026lt; \u0026gt;\u0026gt;($a, $b) returns Int:D 向左移动整数个位。\ninfix +\u0026gt; multi sub infix:\u0026lt;\u0026lt;+\u0026gt; \u0026gt;\u0026gt;($a, $b) returns Int:D 向右移动整数个位。\ninfix gcd multi sub infix:\u0026lt;gcd\u0026gt;($a, $b) returns Int:D 强制两个参数都为 Int 并返回最大公分母（greatest common denominator）。\ninfix lcm multi sub infix:\u0026lt;lcm\u0026gt;($a, $b) returns Int:D 强制两个参数为 Int 并返回最小公倍数(least common multiple)\n加法优先级 infix + multi sub infix:\u0026lt;+\u0026gt;($a, $b) returns Numeric:D 强制两个参数为 Numeric 并把它们相加。\ninfix - multi sub infix:\u0026lt;-\u0026gt;($a, $b) returns Numeric:D 强制两个参数为 Numeric 并用第一个参数减去第二个参数。\ninfix +| multi sub infix:\u0026lt;+|\u0026gt;($a, $b) returns Int:D 强制两个参数为 Int 并执行按位 OR（包括 OR）\ninfix +^ multi sub infix:\u0026lt;+^\u0026gt;($a, $b) returns Int:D 强制两个参数为 Int 并执行按位 XOR（不包括 OR）\ninfix ?| multi sub infix:\u0026lt;?|\u0026gt;($a, $b) returns Bool:D 强制两个参数为 Bool 并执行逻辑 OR（不包括 OR）\n重复操作符优先级 infix x proto sub infix:\u0026lt;x\u0026gt;(Any, Any) returns Str:D multi sub infix:\u0026lt;x\u0026gt;(Any, Any) multi sub infix:\u0026lt;x\u0026gt;(Str:D, Int:D) 把 $a 强转为 Str , 把 $b 强转为 Int, 并重复字符串 $b 次。如果 $b \u0026lt;= 0 则返回空字符串。\nsay \u0026#39;ab\u0026#39; x 3; # ababab say 42 x 3; # 424242 infix xx multi sub infix:\u0026lt;xx\u0026gt;($a, $b) returns List:D 返回一组重复的 $a 并计算 $b 次（$b 被强转为 Int）。如果 $b \u0026lt;= 0 ,则返回一个空列表。\n每次重复都会计算左侧的值, 所以\n[1, 2] xx 5 返回 5 个不同的数组（但是每次都是相同的内容）并且\nrand xx 3 返回 3 个独立的伪随机数。右侧可以是 *, 这时会返回一个惰性的, 无限的列表。\n连结 infix ~ proto sub infix:\u0026lt;~\u0026gt;(Any, Any) returns Str:D multi sub infix:\u0026lt;~\u0026gt;(Any, Any) multi sub infix:\u0026lt;~\u0026gt;(Str:D, Str:D) 强制两个参数为 Str 并连结它们。\nsay \u0026#39;ab\u0026#39; ~ \u0026#39;c\u0026#39;; # abc Junctive AND (all) 优先级 infix \u0026amp; multi sub infix:\u0026lt;\u0026amp;\u0026gt;($a, $b) returns Junction:D is assoc\u0026lt;list\u0026gt; 用它的参数创建一个 all Junction。查看 Junctions 获取更多详情。\nJunctive OR (any) Precedence infix | multi sub infix:\u0026lt;|\u0026gt;($a, $b) returns Junction:D is assoc\u0026lt;list\u0026gt; 用它的参数创建一个 any Junction。查看 Junctions 获取更多详情。\ninfix ^ multi sub infix:\u0026lt;^\u0026gt;($a, $b) returns Junction:D is assoc\u0026lt;list\u0026gt; 用它的参数创建一个 one Junction。查看 Junctions 获取更多详情。\nNamed Unary Precedence prefix temp sub prefix:\u0026lt;temp\u0026gt;(Mu $a is rw) temporizes 传入的变量作为参数, 这意味着退出作用域后它被重置为旧值。（这和 Perl 5 中的 local 操作符类似, 除了 temp 不重置值之外。）\nprefix let sub prefix:\u0026lt;let\u0026gt;(Mu $a is rw) 假定重置：如果通过异常或 fail() 退出当前作用域, 旧值就会被恢复。\nNonchaining Binary Precedence infix does sub infix:\u0026lt;does\u0026gt;(Mu $obj, Mu $role) is assoc\u0026lt;none\u0026gt; 在运行时把 $role 混合进 $obj 中。要求 $obj 是可变的。\n参数 $role 不一定要求是一个 role, 它可以表现的像是一个 role, 例如枚举值。\ninfix but sub infix:\u0026lt;but\u0026gt;(Mu $obj, Mu $role) is assoc\u0026lt;none\u0026gt; 把 $role 混合进 $obj 并创建一个 $obj 的副本。因为 $obj 是不能修改的, 但是能使用 mixins 用于创建不可变值。\n参数 $role 不一定要求是一个 role, 它可以表现的像是一个 role, 例如枚举值。\ninfix cmp proto sub infix:\u0026lt;cmp\u0026gt;(Any, Any) returns Order:D is assoc\u0026lt;none\u0026gt; multi sub infix:\u0026lt;cmp\u0026gt;(Any, Any) multi sub infix:\u0026lt;cmp\u0026gt;(Real:D, Real:D) multi sub infix:\u0026lt;cmp\u0026gt;(Str:D, Str:D) multi sub infix:\u0026lt;cmp\u0026gt;(Enum:D, Enum:D) multi sub infix:\u0026lt;cmp\u0026gt;(Version:D, Version:D) 一般的, “智能的” 三路比较器。\n比较字符串时使用字符串语义, 比较数字时使用数字语义, 比较 Pair 对象时, 先比较键, 再比较值, 等等。\nif $a eqv $b, then $a cmp $b always returns Order::Same. say (a =\u0026gt; 3) cmp (a =\u0026gt; 4); # Less say 4 cmp 4.0; # Same say \u0026#39;b\u0026#39; cmp \u0026#39;a\u0026#39;; # More infix leg proto sub infix:\u0026lt;leg\u0026gt;($a, $b) returns Order:D is assoc\u0026lt;none\u0026gt; multi sub infix:\u0026lt;leg\u0026gt;(Any, Any) multi sub infix:\u0026lt;leg\u0026gt;(Str:D, Str:D) 字符串三路比较器。leg 是 less, equal 还有 greater 的简写形式？\n把两个参数都强转为 Str, 然后按照字母次序比较。\nsay \u0026#39;a\u0026#39; leg \u0026#39;b\u0026#39;; Less say \u0026#39;a\u0026#39; leg \u0026#39;a\u0026#39;; Same say \u0026#39;b\u0026#39; leg \u0026#39;a\u0026#39;; More infix \u0026lt;=\u0026gt; multi sub infix:«\u0026lt;=\u0026gt;»($a, $b) returns Order:D is assoc\u0026lt;none\u0026gt; Numeric 三路比较器。\n把两个参数强转为 Real, 并执行数值比较。\ninfix .. multi sub infix:\u0026lt;..\u0026gt;($a, $b) returns Range:D is assoc\u0026lt;none\u0026gt; 由参数创建一个 Range。\ninfix ..^ multi sub infix:\u0026lt;..^\u0026gt;($a, $b) returns Range:D is assoc\u0026lt;none\u0026gt; 由参数创建一个 Range, 不包含末端。\ninfix ^.. multi sub infix:\u0026lt;^..\u0026gt;($a, $b) returns Range:D is assoc\u0026lt;none\u0026gt; 由参数创建一个 Range, 不包含开始端点。\ninfix ^..^ multi sub infix:\u0026lt;^..^\u0026gt;($a, $b) returns Range:D is assoc\u0026lt;none\u0026gt; 由参数创建一个 Range, 不包含开端和末端。\nChaining Binary Precedence infix == proto sub infix:\u0026lt;==\u0026gt;($, $) returns Bool:D is assoc:\u0026lt;chain\u0026gt; multi sub infix:\u0026lt;==\u0026gt;(Any, Any) multi sub infix:\u0026lt;==\u0026gt;(Int:D, Int:D) multi sub infix:\u0026lt;==\u0026gt;(Num:D, Num:D) multi sub infix:\u0026lt;==\u0026gt;(Rational:D, Rational:D) multi sub infix:\u0026lt;==\u0026gt;(Real:D, Real:D) multi sub infix:\u0026lt;==\u0026gt;(Complex:D, Complex:D) multi sub infix:\u0026lt;==\u0026gt;(Numeric:D, Numeric:D) 强转两个参数为 Numeric（如果必要）, 并\u0008返回 True 如果它们相等。\ninfix != proto sub infix:\u0026lt;!=\u0026gt;(Mu, Mu) returns Bool:D is assoc\u0026lt;chain\u0026gt; 强转两个参数为 Numeric（如果必要）, 并\u0008返回 True 如果它们不相等。\ninfix \u0026lt; proto sub infix:«\u0026lt;»(Any, Any) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:«\u0026lt;»(Int:D, Int:D) multi sub infix:«\u0026lt;»(Num:D, Num:D) multi sub infix:«\u0026lt;»(Real:D, Real:D) 强转两个参数为 Real （如果必要）, 并返回 True 如果第一个参数小于第二个参数。\ninfix \u0026lt;= proto sub infix:«\u0026lt;=»(Any, Any) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:«\u0026lt;=»(Int:D, Int:D) multi sub infix:«\u0026lt;=»(Num:D, Num:D) multi sub infix:«\u0026lt;=»(Real:D, Real:D) 强转两个参数为 Real （如果必要）, 并返回 True 如果第一个参数小于第二个参数。\ninfix \u0026gt; proto sub infix:«\u0026gt;»(Any, Any) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:«\u0026gt;»(Int:D, Int:D) multi sub infix:«\u0026gt;»(Num:D, Num:D) multi sub infix:«\u0026gt;»(Real:D, Real:D) 强转两个参数为 Real （如果必要）, 并返回 True 如果第一个参数大于第二个参数。\ninfix \u0026gt;= proto sub infix:«\u0026gt;=»(Any, Any) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:«\u0026gt;=»(Int:D, Int:D) multi sub infix:«\u0026gt;=»(Num:D, Num:D) multi sub infix:«\u0026gt;=»(Real:D, Real:D) 强转两个参数为 Real （如果必要）, 并返回 True 如果第一个参数大于或等于第二个参数。\ninfix eq proto sub infix:\u0026lt;eq\u0026gt;(Any, Any) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:\u0026lt;eq\u0026gt;(Any, Any) multi sub infix:\u0026lt;eq\u0026gt;(Str:D, Str:D) 强转两个参数为 Str（如果必要）, 并返回 True 如果第一个参数等于第二个参数。\n助记法: equal\ninfix ne proto sub infix:\u0026lt;ne\u0026gt;(Mu, Mu) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:\u0026lt;ne\u0026gt;(Mu, Mu) multi sub infix:\u0026lt;ne\u0026gt;(Str:D, Str:D) 强转两个参数为 Str（如果必要）, 并返回 False 如果第一个参数等于第二个参数。\n助记法: not equal\ninfix gt proto sub infix:\u0026lt;gt\u0026gt;(Mu, Mu) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:\u0026lt;gt\u0026gt;(Mu, Mu) multi sub infix:\u0026lt;gt\u0026gt;(Str:D, Str:D) 强转两个参数为 Str（如果必要）, 并返回 True 如果第一个参数大于第二个参数。\n助记法: greater than\ninfix ge proto sub infix:\u0026lt;ge\u0026gt;(Mu, Mu) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:\u0026lt;ge\u0026gt;(Mu, Mu) multi sub infix:\u0026lt;ge\u0026gt;(Str:D, Str:D) 强转两个参数为 Str（如果必要）, 并返回 True 如果第一个参数大于第二个参数。\n助记法: greater or equal\ninfix lt proto sub infix:\u0026lt;lt\u0026gt;(Mu, Mu) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:\u0026lt;lt\u0026gt;(Mu, Mu) multi sub infix:\u0026lt;lt\u0026gt;(Str:D, Str:D) 强转两个参数为 Str（如果必要）, 并返回 True 如果第一个参数小于第二个参数。\n助记法: less than\ninfix le proto sub infix:\u0026lt;le\u0026gt;(Mu, Mu) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:\u0026lt;le\u0026gt;(Mu, Mu) multi sub infix:\u0026lt;le\u0026gt;(Str:D, Str:D) 强转两个参数为 Str（如果必要）, 并返回 True 如果第一个参数小于或等于第二个参数。\n助记法: less or equal\ninfix before proto sub infix:\u0026lt;before\u0026gt;(Any, Any) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:\u0026lt;before\u0026gt;(Any, Any) multi sub infix:\u0026lt;before\u0026gt;(Real:D, Real:D) multi sub infix:\u0026lt;before\u0026gt;(Str:D, Str:D) multi sub infix:\u0026lt;before\u0026gt;(Enum:D, Enum:D) multi sub infix:\u0026lt;before\u0026gt;(Version:D, Version:D) 一般的排序, 使用\u0008和 cmp 相同的语义。如果第一个参数小于第二个参数则返回 True。\ninfix after proto sub infix:\u0026lt;after\u0026gt;(Any, Any) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:\u0026lt;after\u0026gt;(Any, Any) multi sub infix:\u0026lt;after\u0026gt;(Real:D, Real:D) multi sub infix:\u0026lt;after\u0026gt;(Str:D, Str:D) multi sub infix:\u0026lt;after\u0026gt;(Enum:D, Enum:D) multi sub infix:\u0026lt;after\u0026gt;(Version:D, Version:D) 一般的排序, 使用\u0008和 cmp 相同的语义。如果第一个参数大于第二个参数则返回 True。\ninfix eqv proto sub infix:\u0026lt;eqv\u0026gt;(Any, Any) returns Bool:D is assoc\u0026lt;chain\u0026gt; proto sub infix:\u0026lt;eqv\u0026gt;(Any, Any) 等值操作符。如果两个参数在结构上相同就返回 True。例如, 相同类型（并且递归）包含相同的值。\nsay [1, 2, 3] eqv [1, 2, 3]; # True say Any eqv Any; # True say 1 eqv 2; # False say 1 eqv 1.0; # False 对于任意对象使用默认的 eqv 操作是不可能的。例如, eqv 不认为同一对象的两个实例在结构上是相等的：\nclass A { has $.a; } say A.new(a =\u0026gt; 5) eqv A.new(a =\u0026gt; 5); #=\u0026gt;False 要得到这个类的对象相等（eqv）语义, 需要实现一个合适的中缀 eqv 操作符：\nclass A { has $.a; } multi infix:\u0026lt;eqv\u0026gt;(A $l, A $r) { $l.a eqv $r.a } say A.new(a =\u0026gt; 5) eqv A.new(a =\u0026gt; 5); #=\u0026gt;True infix === proto sub infix:\u0026lt;===\u0026gt;(Any, Any) returns Bool:D is assoc\u0026lt;chain\u0026gt; proto sub infix:\u0026lt;===\u0026gt;(Any, Any) 值相等。如果两个参数都是同一个对象则返回 True。\nclass A { }; my $a = A.new; say $a === $a; # True say A.new === A.new; # False say A === A; # True 对于值的类型, === 表现的和 eqv 一样:\nsay \u0026#39;a\u0026#39; === \u0026#39;a\u0026#39;; # True say \u0026#39;a\u0026#39; === \u0026#39;b\u0026#39;; # False # different types say 1 === 1.0; # False === 使用 WHICH 方法来获取对象相等, 所以所有的值类型必须重写方法 WHICH。\ninfix =:= proto sub infix:\u0026lt;=:=\u0026gt;(Mu \\a, Mu \\b) returns Bool:D is assoc\u0026lt;chain\u0026gt; multi sub infix:\u0026lt;=:=\u0026gt;(Mu \\a, Mu \\b) 容器相等。返回 True 如果两个参数都绑定到同一个容器上。如果它返回 True, 那通常意味着修改一个参数也会同时修改另外一个。\nmy ($a, $b) = (1, 3); say $a =:= $b; # False $b = 2; say $a; # 1 $b := $a; say $a =:= $b; # True $a = 5; say $b; # 5 infix ~~ 智能匹配操作符。把左侧参数起别名为 $_ , 然后计算右侧的值, 并在它身上调用 .ACCEPTS($_) 。匹配的语义由右侧操作数的类型决定。\n这儿有一个内建智能匹配函数的摘要：\n右侧 比较语义 Mu:U\t类型检查 Str\t字符串相等 Numeric\t数值相等 Regex\t正则匹配 Callable 调用的布尔结果 Any:D\t对象相等 Tight AND Precedence infix \u0026amp;\u0026amp; 在布尔上下文中返回第一个求值为 False 的参数, 否则返回最后一个参数。\n注意这是短路操作符, 如果其中的一个参数计算为 false 值, 那么该参数右侧的值绝不会被计算。\nsub a { 1 } sub b { 0 } sub c { die \u0026#34;never called\u0026#34; }; say a() \u0026amp;\u0026amp; b() \u0026amp;\u0026amp; c(); # 0 Tight OR Precedence infix || 在布尔上下文中返回第一个求值为 True 的参数, 否则返回最后一个参数。\n注意这是短路操作符, 如果其中的一个参数计算为 true 值, 那么该参数右侧的值绝不会被计算。\nsub a { 0 } sub b { 1 } sub c { die \u0026#34;never called\u0026#34; }; say a() || b() || c(); # 1 infix ^^ 返回第一个值为 true 的参数如果只有一个的话, 否则返回 Nil。只要找到两个值为 true 的参数就发生短路。\nsay 0 ^^ 42; # 42 say 0 ^^ 42 ^^ 1 ^^ die 8; # (empty line) 注意, 这个操作符的语义可能不是你假想的那样： infix ^^ 翻到它找到的第一个 true 值, 找到第二个 true 值后永远地反转为 Nil 值, 不管还有多少 true 值。（换句话说, 它的语义是\u0026quot;找到一个真值\u0026quot;, 而不是布尔起奇偶校验语义）\ninfix // Defined-or 操作符。返回第一个定义了的操作数, 否则返回最后一个操作数。短路操作符。\nsay Any // 0 // 42; # 0 infix min 返回参数的最小值。语义由 cmp 语义决定。\n$foo min= 0 # read as: $foo decreases to 0 infix max 返回参数的最大值。\n$foo max= 0 # read as: $foo increases to 0 Conditional Operator Precedence infix ?? !! 三目操作符, 条件操作符。\n$condition ?? $true !! $false 计算并返回 $true 表达式, 如果 $condition 为真的话。否则计算并返回 $false 分支。\ninfix ff sub infix:\u0026lt;ff\u0026gt;(Mu $a, Mu $b) 触发器操作符。\n把两个参数都跟 $_ 进行比较（即, $_ ~~ $a 和 $_ ~~ $b）。求值为 False 直到左侧的智能匹配为真, 这时, 它求值为真, 直到右侧的智能匹配为真。\n实际上, 左边的参数是\u0026quot;开始”条件, 右侧的参数是”停止” 条件。这种结构一般用于收集只在特定区域的行。例如：\nmy $excerpt = q:to/END/;Here\u0026#39;s some unimportant text. =begin code This code block is what we\u0026#39;re after. We\u0026#39;ll use \u0026#39;ff\u0026#39; to get it. =end code More unimportant text. END my @codelines = gather for $excerpt.lines { take $_ if \u0026#34;=begin code\u0026#34; ff \u0026#34;=end code\u0026#34; } # this will print four lines, # starting with \u0026#34;=begin code\u0026#34; and ending with \u0026#34;=end code\u0026#34; say @codelines.join(\u0026#34;\\n\u0026#34;); 匹配开始条件之后, 操作符会继续将停止条件与 $_ 进行匹配, 如果成功就做相应地表现。在这个例子中, 只有第一个元素被打印了：\nfor \u0026lt;AB C D B E F\u0026gt; { say $_ if /A/ ff /B/; # prints only \u0026#34;AB\u0026#34; } 如果你想测试开始条件, 并且没有结束条件, * 能用作 “停止” 条件。\nfor \u0026lt;A B C D E\u0026gt; { say $_ if /C/ ff *; # prints C, D, and E } 对于 sed-like 版本, 在开始条件匹配成功之后, 它不会使用停止条件与 $_ 进行匹配。\n这个操作符不能被重载, 因为它被编译器特殊处理过。\ninfix ^ff sub infix:\u0026lt;^ff\u0026gt;(Mu $a, Mu $b) 像 ff 那样工作, 除了它不会在条目匹配开始条件时返回真。（包括匹配停止条件的条目）\n一个比较:\nmy @list = \u0026lt;A B C\u0026gt;; say $_ if /A/ ff /C/ for @list; # prints A, B, and C say $_ if /A/ ^ff /C/ for @list; # prints B and C 这个操作符不能被重载, 因为它被编译器特殊处理过。\ninfix ff^ sub infix:\u0026lt;ff^\u0026gt;(Mu $a, Mu $b) 像 ff 那样工作, 除了它不会在条目匹配停止条件时返回真。（包括第一次匹配开始条件的条目）\nmy @list = \u0026lt;A B C\u0026gt;; say $_ if /A/ ff /C/ for @list; # prints A, B, and C say $_ if /A/ ff^ /C/ for @list; # prints A and B 这个操作符不能被重载, 因为它被编译器特殊处理过。\ninfix ^ff^ sub infix:\u0026lt;^ff^\u0026gt;(Mu $a, Mu $b) 像 ff 那样工作, 除了它不会在条目匹配停止条件时返回真, 也不会在条目匹配开始时返回真。（或者两者）\nmy @list = \u0026lt;A B C\u0026gt;; say $_ if /A/ ff /C/ for @list; # prints A, B, and C say $_ if /A/ ^ff^ /C/ for @list; # prints B 这个操作符不能被重载, 因为它被编译器特殊处理过。\ninfix fff sub infix:\u0026lt;fff\u0026gt;(Mu $a, Mu $b) 执行 sed-like 那样的 flipflop 操作, 在其中, 它返回 False 直到左侧的参数与 $_ 智能匹配, 并且在那之后返回 True 直到右侧的参数和 $_ 智能匹配。\n像 ff 那样工作, 除了它每次调用只尝试一个参数之外。即, 如果 $_ 和左侧的参数智能匹配, fff 随后不会尝试将同一个 $_ 和右侧的参数进行匹配。\nfor \u0026lt;AB C D B E F\u0026gt; { say $_ if /A/ fff /B/; # Prints \u0026#34;AB\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, and \u0026#34;B\u0026#34; } 对于 non-sed-like 版本, 查看 ff.\n这个操作符不能被重载, 因为它被编译器特殊处理过。\ninfix ^fff sub infix:\u0026lt;^fff\u0026gt;(Mu $a, Mu $b) 像 fff 那样, 除了它对于左侧的匹配不返回真之外。\nmy @list = \u0026lt;A B C\u0026gt;; say $_ if /A/ fff /C/ for @list; # prints A, B, and C say $_ if /A/ ^fff /C/ for @list; # prints B and C 对于 non-sed 版本, 查看 ^ff。\n这个操作符不能被重载, 因为它被编译器特殊处理过。\ninfix fff^ sub infix:\u0026lt;fff^\u0026gt;(Mu $a, Mu $b) 像 fff 那样, 除了它对于右侧的匹配不返回真之外。\nmy @list = \u0026lt;A B C\u0026gt;; say $_ if /A/ fff /C/ for @list; # prints A, B, and C say $_ if /A/ fff^ /C/ for @list; # prints A and B 对于 non-sed 版本, 查看 ff^。\n这个操作符不能被重载, 因为它被编译器特殊处理过。\ninfix ^fff^ sub infix:\u0026lt;^fff^\u0026gt;(Mu $a, Mu $b) 像 fff 那样, 除了它对于左侧和右侧的匹配都不返回真之外。\nmy @list = \u0026lt;A B C\u0026gt;; say $_ if /A/ fff /C/ for @list; # prints A, B, and C say $_ if /A/ ^fff^ /C/ for @list; # prints B 对于 non-sed 版本, 查看 ^ff^.\n这个操作符不能被重载, 因为它被编译器特殊处理过。\nItem Assignment Precedence infix = sub infix:\u0026lt;=\u0026gt;(Mu $a is rw, Mu $b) Item 赋值。\n把 = 号右侧的值放入左侧的容器中。它真正的语义是由左侧的容器类型决定的。\n（注意 item 赋值和列表赋值的优先级级别不同, 并且等号左侧的语法决定了等号是被解析为 item 赋值还是列表赋值操作符）。\ninfix =\u0026gt; sub infix:«=\u0026gt;»($key, Mu $value) returns Pair:D Pair 构造器。\n使用左侧值作为键, 右侧值作为值, 构造一个 Pair 对象。\n注意 =\u0026gt; 操作符是语法上的特例, 在这个结构中, 它允许左侧是一个未被引起的标识符。\nmy $p = a =\u0026gt; 1; say $p.key; # a say $p.value; # 1 在参数列表中, 在 =\u0026gt; 左侧使用未被引起的标识符构建的 Pair 会被解释为一个具名参数。\n查看 Terms 语言文档了解更多创建 Pair 对象的方式。\nLoose Unary Precedence prefix not multi sub prefix:\u0026lt;not\u0026gt;(Mu $x) returns Bool:D 在布尔上下文中计算它的参数（因此使 Junctions 失效）, 并返回否定的结果。\nprefix so multi sub prefix:\u0026lt;so\u0026gt;(Mu $x) returns Bool:D 在布尔上下文中计算它的参数（因此使 Junctions 失效）, 并返回结果。\n逗号操作符优先级 infix : 就像中缀操作符 , 那样, : 用作参数分隔符, 并把它左侧的参数标记为调用者。\n那会把函数调用转为方法调用。\nsubstr(\u0026#39;abc\u0026#39;: 1); # same as \u0026#39;abc\u0026#39;.substr(1) Infix : 只允许出现在非方法调用的第一个参数后面。在其它位置它会是语法错误。\nList Infix Precedence infix Z sub infix:\u0026lt;Z\u0026gt;(**@lists) returns List:D is assoc\u0026lt;chain\u0026gt; Zip operator。\nZ 像一个拉链那样把列表插入进来, 只要第一个输入列表耗尽就停止：\nsay (1, 2 Z \u0026lt;a b c\u0026gt; Z \u0026lt;+ -\u0026gt;).raku; # ((1, \u0026#34;a\u0026#34;, \u0026#34;+\u0026#34;), (2, \u0026#34;b\u0026#34;, \u0026#34;-\u0026#34;)).list Z 操作符也作为元操作符存在：\nsay 100, 200 Z+ 42, 23; # 142, 223 say 1..3 Z~ \u0026lt;a b c\u0026gt; Z~ \u0026#39;x\u0026#39; xx 3; # 1ax 2bx 3cx infix X sub infix:\u0026lt;X\u0026gt;(**@lists) returns List:D is assoc\u0026lt;chain\u0026gt; 从所有列表创建一个外积。最右边的元素变化得最迅速。\n1..3 X \u0026lt;a b c\u0026gt; X 9 # produces (1, \u0026#39;a\u0026#39;, 9), (1, \u0026#39;b\u0026#39;, 9), (1, \u0026#39;c\u0026#39;, 9), # (2, \u0026#39;a\u0026#39;, 9), (2, \u0026#39;b\u0026#39;, 9), (2, \u0026#39;c\u0026#39;, 9), # (3, \u0026#39;a\u0026#39;, 9), (3, \u0026#39;b\u0026#39;, 9), (3, \u0026#39;c\u0026#39;, 9) X 操作符也可以作为元操作符：\n1..3 X~ \u0026lt;a b c\u0026gt; X~ 9 # produces \u0026#39;1a9\u0026#39;, \u0026#39;1b9\u0026#39;, \u0026#39;1c9\u0026#39;, \u0026#39;2a9\u0026#39;, \u0026#39;2b9\u0026#39;, \u0026#39;2c9\u0026#39;, \u0026#39;3a9\u0026#39;, \u0026#39;3b9\u0026#39;, \u0026#39;3c9\u0026#39; infix \u0026hellip; multi sub infix:\u0026lt;...\u0026gt;(**@) is assoc\u0026lt;list\u0026gt; multi sub infix:\u0026lt;...^\u0026gt;(**@) is assoc\u0026lt;list\u0026gt; 序列操作符是一个用于产生惰性列表的普通操作符。\n它可以有一个初始元素和一个生成器在 … 的\u0008左侧, 在右侧是一个端点。\n序列操作符会使用尽可能多的参数来调用生成器。参数会从初始元素和已生成元素中获取。\n默认的生成器是 *.succ 或 *.pred, 取决于末端怎么比较：\nsay 1 ... 4; # 1 2 3 4 say 4 ... 1; # 4 3 2 1 say \u0026#39;a\u0026#39; ... \u0026#39;e\u0026#39;; # a b c d e say \u0026#39;e\u0026#39; ... \u0026#39;a\u0026#39;; # e d c b a * (Whatever) 末端生成一个无限序列, 使用的是默认的生成器 *.succ。\nsay (1 ... *)[^5]; # 1 2 3 4 5 自定义生成器是在 … 操作符之前的最后一个参数。下面这个自定义生成器接收两个参数, 生成了斐波纳契数。\nsay (1, 1, -\u0026gt; $a, $b { $a + $b } ... *)[^8]; # 1 1 2 3 5 8 13 21 # same but shorter say (1, 1, *+* ... *)[^8]; # 1 1 2 3 5 8 13 21 当然自定义生成器也能只接收一个参数。\nsay 5, { $_ * 2 } ... 40; # 5 10 20 40 生成器的参数个数至少要和初始元素的个数一样多。\n如果没有生成器, 并且有不止一个初始元素, 所有的初始元素都是数值, 那么序列操作符会尝试推导出生成器。它知道数学和几何序列。\nsay 2, 4, 6 ... 12; # 2 4 6 8 10 12 say 1, 2, 4 ... 32; # 1 2 4 8 16 32 如果末端不是 *, 它会和每个生成的元素进行智能匹配, 当智能匹配成功的时候序列就被终止。对于 ... 操作符, 会包含最后一个元素, 对于 ...^ 操作符, 会排除最后的那个元素。\n这允许你这样写：\nsay 1, 1, *+* ...^ *\u0026gt;= 100; 来生成所有直到 100 但不包括 100 的斐波纳契数。\n... 操作符还会把初始值看作”已生成的元素”, 所以它们也会对末端进行检查：\nmy $end = 4; say 1, 2, 4, 8, 16 ... $end; # outputs 1 2 4 List Prefix Precedence infix = 列表赋值。 它真正的语义是由左侧的容器类型决定的。查看 Array 和 Hash 获取普通案例。\nitem 赋值和列表赋值的优先级级别不同, 并且等号左侧的语法决定了等号是被解析为 item 赋值还是列表赋值操作符。\ninfix := 绑定。而 $x = $y 是把 $y 中的值放到 $x 里面, $x := $y 会让 $x 和 $y 引用同一个值。\nmy $a = 42; my $b = $a; $b++; say $a; 这会输出 42, 因为 $a 和 $b 都包含了数字 42, 但是容器是不同的。\nmy $a = 42; my $b := $a; $b++; say $a; 这会打印 43, 因为 $b 和 $a 都代表着同一个对象。\ninfix ::= 只读绑定. 查看 infix :=.\nlistop \u0026hellip; 这是 yada, yada, yada 操作符 或 stub 操作符。如果它在子例程或类型中是唯一的语句, 它会把子例程或类型标记为 stub（这在预声明类型和组成 roles 上下文中是有意义的）\n如果 ... 语句被执行了, 它会调用 \u0026amp;fail , 伴随着默认的消息 stub 代码的执行。\nlistop !!! 如果它在子例程或类型中是唯一的语句, 它会把子例程或类型标记为 stub（这在预声明类型和组成 roles 上下文中是有意义的）\n如果 !!! 语句被执行了, 它会调用 \u0026amp;die , 伴随着默认的消息 stub 代码的执行。\nlistop ??? 如果它在子例程或类型中是唯一的语句, 它会把子例程或类型标记为 stub（这在预声明类型和组成 roles 上下文中是有意义的）\n如果 ??? 语句被执行了, 它会调用 \u0026amp;warn , 伴随着默认的消息 stub 代码的执行。\nLoose AND precedence infix and 和中缀操作符 \u0026amp;\u0026amp; 一样, 除了优先级更宽松。\n在布尔上下文中返回第一个求值为 False 的操作数, 否则返回最后一个操作数。短路操作符。\ninfix andthen 返回第一个未定义的参数, 否则返回最后一个参数。短路操作符。左侧的结果被绑定到 $_ 身上, 在右侧中使用, 或者作为参数被传递, 如果右侧是一个 block 或 pointy block.\nLoose OR Precedence infix or 和中缀操作符 || 一样, 除了优先级更宽松。\n在布尔上下文中返回第一个求值为 True 的参数, 否则返回最后一个参数。短路操作符。\ninfix orelse 和中缀操作符 // 一样, 除了优先级更宽松之外。\n返回第一个定义过的参数, 否则返回最后一个参数。短路操作符。\n","permalink":"https://ohmyweekly.github.io/notes/2015-03-16-operator-in-raku/","tags":["operator"],"title":"Raku 中的操作符"},{"categories":["rakulang"],"contents":"Modules Exporting and Selective Importing is export packages(包), subroutines(子例程), variables(变量), constants(常量) 和 enums(枚举) , 通过在它们的名字后面添加 is export 特性来导出。\nunit module MyModule; our $var is export = 3; sub foo is export { ... }; constant $FOO is export = \u0026#34;foobar\u0026#34;; enum FooBar is export \u0026lt;one two three\u0026gt;; # Packages like classes can be exported too class MyClass is export {}; # If a subpackage is in the namespace of the current package # it doesn\u0026#39;t need to be explicitly exported class MyModule::MyClass {}; 就像所有的 traits 一样, 如果应用到子例程(routine)上, \u0026ldquo;is export\u0026rdquo; 应该出现在参数列表的后面:\nsub foo (Str $string) is export {...} 你可以给 is export 传递命名参数以组织要导出的符号, 然后导入程序\u0001 (importer) 可以剔除和选择导入哪一个。有 3 个预先定义好的标签：ALL, DEFAULT, MANDATORY（强制的）。\n# lib/MyModule.pm unit module MyModule; sub bag is export { ... } sub pants is export(:MANDATORY) { ... } sub sunglasses is export(:day) { ... } sub torch is export(:night) { ... } sub underpants is export(:ALL) { ... } # main.pl use lib \u0026#39;lib\u0026#39;; use MyModule; #bag, pants use MyModule :DEFAULT; #the same use MyModule :day; #pants, sunglasses use MyModule :night; #pants, torch use MyModule :ALL; #bag, pants, sunglasses, torch, underpants UNIT::EXPORT::* 表象之下, 其实 is export 是把符号添加到 EXPORT 命名空间中的 UNIT 作用域包中。例如, is export(:FOO) 会把目标添加到 UNIT::EXPORT::FOO 包中。这正是 Raku 决定导入什么所做的。\nunit module MyModule; sub foo is export { ... } sub bar is export(:other) { ... } 等价于:\nunit module MyModule; my package EXPORT::DEFAULT { our sub foo { ... } } my package EXPORT::other { our sub bar { ... } } 多数时候, is export 足够用了, 但是当你想动态生成要导出的符号时, EXPORT 包就很有用了。例如：\n# lib/MyModule.pm unit module MuModule; my package EXPORT::DEFAULT { for \u0026lt;zero one two three four\u0026gt;.kv -\u0026gt; $number, $name { for \u0026lt;sqrt log\u0026gt; -\u0026gt; $func { OUR::{\u0026#39;\u0026amp;\u0026#39; ~ $func ~ \u0026#39;-of-\u0026#39; ~ $name } := sub { $number.\u0026#34;$func()\u0026#34; }; } } } # main.pl use MyModule; say sqrt-of-four; #-\u0026gt; 2 say log-of-zero; #-\u0026gt; -Inf EXPORT 你可以用一个 EXPORT 子例程导出任意符号。EXPORT 必须返回一个 Map, 在 map 里面键是符号名, 键值是想要的值。符号名应该包含(如果有的话)关联类型。\nclass MyModule::Class { ... } sub EXPORT { { \u0026#39;$var\u0026#39; =\u0026gt; \u0026#39;one\u0026#39;, \u0026#39;@array\u0026#39; =\u0026gt; \u0026lt;one two three\u0026gt;, \u0026#39;%hash\u0026#39; =\u0026gt; { one =\u0026gt; \u0026#39;two\u0026#39;, three =\u0026gt; \u0026#39;four\u0026#39;}, \u0026#39;\u0026amp;doit\u0026#39; =\u0026gt; sub { ... }, \u0026#39;ShortName\u0026#39; =\u0026gt; MyModule::class } } # main.pl use lib \u0026#39;lib\u0026#39;; use MyModule; say $var; say @array; say %hash; doit(); say ShortName.new; #-\u0026gt; MyModule::Class.new 注意, EXPORT 不能声明在包内, 因为目前的 rakudo(2015.09) 好像把 EXPORT 当作 compunit 的一部分而非包的一部分。\n虽然 UNIT::EXPORT 包处理传递给 use 的命名参数, 而 EXPORT sub 处理位置参数。如果你把位置参数传递给 use, 那么这些参数会被传递给 EXPORT。如果传递了位置参数, 那么 module 就不再需要导出默认符号了。你仍然可以伴随着你的位置参数, 通过显式地给 use 传递 :DEFAULT 参数来导入它们。\n# lib/MyModule class MyModule::Class {} sub EXPORT($short_name?) { { do $short_name =\u0026gt; MyModule::Class if $short_name } } sub always is export(:MANDATORY) { say \u0026#34;works\u0026#34; } #import with :ALL or :DEFAULT to get sub shy is export { say \u0026#34;you found me!\u0026#34; } # main.pl use lib \u0026#39;lib\u0026#39;; use MyModule \u0026#39;foo\u0026#39;; say foo.new(); #MyModule::Class.new always(); #OK - is imported shy(); #FAIL - won\u0026#39;t be imported 发布模块 如果你已经写了一个 Raku 模块, 你想把它分享到社区, 我们会很高兴地把它放到 Raku 模块文件夹清单中。Raku modules directory\n现在, 你需要使用 git 对你的模块进行版本控制。\n这需要你有一个 Github 帐号, 以使你的模块能被从它的 Github 仓库中分享出去。\n要分享你的模块, 按照下面说的做：\n  创建一个以你的模块命名的工程文件夹。例如, 如果你的模块是 Vortex::TotalPerspective, 那么就创建一个叫做 Vortex::TotalPerspective 的工程文件夹。这个工程目录的名字也会被用作 Github 仓库的名字。\n  让你的工程目录看起来像这样：\nVortex-TotalPerspective/ |-- lib | `-- Vortex | `-- TotalPerspective.pm |-- LICENSE |-- META.info |-- README.md `-- t `-- basic.t 如果你的工程包含能帮助主模块完成工作的其它模块, 它们应该被放到你的 lib 目录中像这样组织：\nlib `-- Vortex |-- TotalPerspective.pm `-- TotalPerspective |-- FairyCake.pm `-- Gargravarr.pm   README.md 文件是一个 markdown 格式的文件, 它稍后会被 Github 自动渲染成 HTML\n  关于 LICENSE 文件, 如果你没有其它选择, 就是用和 Rakudo Raku 一样的 LICENSE 把。仅仅把它的原始 license 复制/粘贴进你自己的 LICENSE 文件中。\n  如果你还没有任何测试, 现在你可以忽略 t 目录 和 basic.t 文件。关于如何写测试, 你可以看看其它模块是怎么使用 Test 的。它和 Perl'5 的 Test::More 很类似。\n  如果要文档化你的模块, 在你的模块中使用 Raku Pod 标记。欢迎给模块写文档, 并且为了浏览的方便, 一旦 Raku module directory(或其它网站) 开始把 Pod 文档渲染成 HTML, 写文档尤为重要。\n  让你的 META.info 文件看起来像这样:\n     { \u0026quot;name\u0026quot; : \u0026quot;Vortex::TotalPerspective\u0026quot;, \u0026quot;version\u0026quot; : \u0026quot;0.1.0\u0026quot;, \u0026quot;description\u0026quot; : \u0026quot;Wonderful simulation to get some perspective.\u0026quot;, \u0026quot;author\u0026quot; : \u0026quot;Your Name\u0026quot;, \u0026quot;provides\u0026quot; : { \u0026quot;Vortex::TotalPerspective\u0026quot; : \u0026quot;lib/Vortex/TotalPerspective.pm\u0026quot; }, \u0026quot;depends\u0026quot; : [ ], \u0026quot;source-url\u0026quot; : \u0026quot;git://github.com/you/Vortex-TotalPerspective.git\u0026quot; } 关于选择版本号的方案, 或许使用 \u0026ldquo;major.minor.patch\u0026rdquo; （查看 the spec on versioning 获取详细信息 ）。如果版本号现在对你或你的用户来说不重要, 你可以给版本那儿放上一颗星(*)。\n在 provides 一节, 包含进你的发布中提供的所有命名空间。\n 把你的工程放在 git 版本控制之下, 如果你还未这样做。 一旦你对你的工程满意了, 在 Github 上为它创建一个仓库。必要的话, 查看 Github\u0026rsquo;s help docs。 你的 Github 仓库的名字应该和你工程目录的名字一样。创建完 Githhub 仓库后, Github 会为你展示怎么配置你的本地仓库以获悉你的 Github 仓库。 把你的工程推送到 Github 在 IRC 频道找个人帮你展示怎么把你的模块添加到ecosystem, 或者让他们是否能替你添加。 pull 请求被接收之后, 等个把小时。如果你的模块没有出现在 http://modules.raku.org/, 请到 http://modules.raku.org/log/update.log 翻看log 日志文件, 以查找是否有错误。  就是这样啦！感谢为 Raku 社区做贡献！\n如果你想尝试安装你的模块, 使用熊猫 panda 安装工具, 这已经包含在 Rakudo Raku 中了:\nzef install Vortex::TotalPerspective 这会下载你的模块到它自己的工作目录(~/.zef), 在那儿创建 build, 并把模块安装到 ~/.raku\n","permalink":"https://ohmyweekly.github.io/notes/2015-03-23-modules-in-raku/","tags":["module"],"title":"Raku 中的模块"},{"categories":["rakulang"],"contents":"Set my $keywords = set \u0026lt;if for unless while\u0026gt;; # create a set sub has-keyword(*@words) { for @words -\u0026gt; $word { # 依次检查数组中的元素是否属于集合 $keywords return True if $word (elem) $keywords; } False; } say has-keyword \u0026#39;not\u0026#39;, \u0026#39;one\u0026#39;, \u0026#39;here\u0026#39;; # False say has-keyword \u0026#39;but\u0026#39;, \u0026#39;here\u0026#39;, \u0026#39;for\u0026#39;; # True Series Operator my @a=\u0026lt;A G C T\u0026gt;; my $x=@a; for 1 ... * -\u0026gt; $a { (( [X~] $x xx $a )).join(\u0026#39;,\u0026#39;).say; last if $a == 4; } 倒序的 range:\nfor 10 ... 0 { .say; } flip/plop my $file = open \u0026#39;flip_flop.txt\u0026#39;; for $file.lines -\u0026gt; $line { say $line if !($line ~~ m/^\\;/ ff $line ~~ m/^\\\u0026#34;/); } $line ~~ m/^\\;/ ff $line ~~ m/^\\\u0026quot;/ 过滤掉 ; 和 \u0026quot; 之间的内容, 再对它进行取反操作就是过滤后剩下的文本。\nflip_flop.txt 内容如下：\n; next is some lines to skip,include this line fuck fuck fuck dam dam dam mie mie mie \u0026quot; next is subject There is more than one way to do it -- Larry Wall We hope Raku is wrote by the hole Socfilia -- Larry Wall ; next is some lines to skip,include this line fuck fuck fuck dam dam dam mie mie mie \u0026quot; next is subject programming is hard,Let's go shopping -- Larry Wall Ruby is Another Raku -- Larry Wall 输出：\nThere is more than one way to do it -- Larry Wall We hope Raku is wrote by the hole Socfilia -- Larry Wall programming is hard,Let\u0026#39;s go shopping -- Larry Wall Ruby is Another Raku -- Larry Wall ff 操作符左右两侧的 ^ 表示排除:\nfor 1..20 {.say if $_==9 ff $_==16} say \u0026#39;-\u0026#39; x 10; for 1..20 {.say if $_==9 ^ff $_==16} say \u0026#39;-\u0026#39; x 10; for 1..20 {.say if $_==9 ff^ $_==16} say \u0026#39;-\u0026#39; x 10; for 1..20 {.say if $_==9 ^ff^ $_==16} 输出:\n9 10 11 12 13 14 15 16 ---------- 10 11 12 13 14 15 16 ---------- 9 10 11 12 13 14 15 ---------- 10 11 12 13 14 15 Grammars 解析 CSV:\n﻿grammar CSV { token TOP {[\u0026lt;line\u0026gt;\\n?]+} token line {^^# Beginning of a line \u0026lt;value\u0026gt;*%\\, # Any number of \u0026lt;value\u0026gt;s with commas in `between` them $$# End of a line } token value {[|\u0026lt;-[\u0026#34;,\\n]\u0026gt;# Anything not a double quote, comma or newline |\u0026lt;quoted-text\u0026gt;# Or some quoted text ]*# Any number of times } token quoted-text {\\\u0026#34; [|\u0026lt;-[\u0026#34;\\\\]\u0026gt;# Anything not a \u0026#34; or \\ |\u0026#39;\\\u0026#34;\u0026#39;# Or \\\u0026#34;, an escaped quotation mark ]*# Any number of times \\\u0026#34; } } # method parse($str, :$rule = \u0026#39;TOP\u0026#39;, :$actions) returns Match:D say \u0026#34;Valid CSV file!\u0026#34; if CSV.parse( q:to/EOCSV/ ); Year,Make,Model,Length 1997,Ford,E350,2.34 2000,Mercury,Cougar,2.38 EOCSV say CSV.parse( q:to/EOCSV/,\u0026#39;line\u0026#39;, :$actions ); Year,Make,Model,Length 1997,Ford,E350,2.34 2000,Mercury,Cougar,2.38 EOCSV 解析天气数据:\ngrammar StationDataParser { token TOP {^\u0026lt;keyval\u0026gt;+\u0026lt;observations\u0026gt;$} token keyval {$\u0026lt;key\u0026gt;=[\u0026lt;-[=]\u0026gt;+]\u0026#39;=\u0026#39;\\h*$\u0026lt;val\u0026gt;=[\\N+]\\n} token observations {\u0026#39;Obs:\u0026#39;\\h*\\n\u0026lt;observation\u0026gt;+} token observation {$\u0026lt;year\u0026gt;=[\\d+]\\h*\u0026lt;temp\u0026gt;+%%[\\h*]\\n} token temp {\u0026#39;-\u0026#39;?\\d+\\.\\d+} } class StationData { has $.name; has $.country; has @.data; submethod BUILD(:%info (:Name($!name), :Country($!country), *%), :@!data) { } } class StationDataActions { method TOP($/) { make StationData.new( info =\u0026gt; $\u0026lt;keyval\u0026gt;.map(*.ast).hash, data =\u0026gt; $\u0026lt;observations\u0026gt;.ast ); } method keyval($/) { make ~$\u0026lt;key\u0026gt; =\u0026gt; ~$\u0026lt;val\u0026gt;; } method observations($/) { make $\u0026lt;observation\u0026gt;.map(*.ast).grep(*.value.none \u0026lt;= -99); } method observation($/) { make +$\u0026lt;year\u0026gt; =\u0026gt; $\u0026lt;temp\u0026gt;.map(*.Num); } } say StationDataParser.parse( q:to/EOCSV/,:actions(StationDataActions)).ast Name= Jan Mayen Country= NORWAY Lat= 70.9 Long= 8.7 Height= 10 Start year= 1921 End year= 2009 Obs: 1921 -4.4 -7.1 -6.8 -4.3 -0.8 2.2 4.7 5.8 2.7 -2.0 -2.1 -4.0 1922 -0.9 -1.7 -6.2 -3.7 -1.6 2.9 4.8 6.3 2.7 -0.2 -3.8 -2.6 2008 -2.8 -2.7 -4.6 -1.8 1.1 3.3 6.1 6.9 5.8 1.2 -3.5 -0.8 2009 -2.3 -5.3 -3.2 -1.6 2.0 2.9 6.7 7.2 3.8 0.6 -0.3 -1.3 EOCSV Raku Examples  1、生成8位随机密码  my @char_set = (0..9, \u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;, \u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;,\u0026#39;~\u0026#39;,\u0026#39;!\u0026#39;,\u0026#39;@\u0026#39;,\u0026#39;#\u0026#39;,\u0026#39;$\u0026#39;,\u0026#39;%\u0026#39;,\u0026#39;^\u0026#39;,\u0026#39;\u0026amp;\u0026#39;,\u0026#39;*\u0026#39;); say @char_set.pick(8).join(\u0026#34;\u0026#34;) # 不重复的8位密码 say @char_set.roll(8).join(\u0026#34;\u0026#34;) # 可以重复  2、打印前5个数字  .say for 1..10[^5] .say for 1,2,3,4 ... [^10] # 这个会无限循环   3、排序\n  3.1 按数值排序\n  my %hash = \u0026#39;Perl\u0026#39; =\u0026gt; 100, \u0026#39;Python\u0026#39; =\u0026gt; 100, \u0026#39;Go\u0026#39; =\u0026gt; 100, \u0026#39;CMD\u0026#39; =\u0026gt; 20, \u0026#39;Php\u0026#39; =\u0026gt; 80, \u0026#39;Java\u0026#39; =\u0026gt; 85; %hash.values.sort; %hash.values.sort(-*);  3.2 按分数排序散列：  my %hash = \u0026#39;Perl\u0026#39; =\u0026gt; 80, \u0026#39;Python\u0026#39; =\u0026gt; 100, \u0026#39;Go\u0026#39; =\u0026gt; 95, \u0026#39;CMD\u0026#39; =\u0026gt; 20, \u0026#39;Php\u0026#39; =\u0026gt; 80, \u0026#39;Java\u0026#39; =\u0026gt; 85; for %hash.sort({-.value}).hash.keys -\u0026gt; $key { say $key, \u0026#34;\\t\u0026#34;, %hash{\u0026#34;$key\u0026#34;} } 输出:\nPython\t100 Go\t95 Java\t85 Perl\t80 Php\t80 CMD\t20 (\u0026#39;xx\u0026#39;..\u0026#39;zz\u0026#39;).classify(*.substr(1))\u0026lt;z\u0026gt;; # xz yz zz 加密：\nsub rot13 { $^s.trans(\u0026#39;a..z\u0026#39; =\u0026gt; \u0026#39;n..za..m\u0026#39;, \u0026#39;A..Z\u0026#39; =\u0026gt; \u0026#39;N..ZA..M\u0026#39;) }  4、求 1! + 2! + 3! + 4! +5! + 6! +7! +8! +9! +10!  multi sub postfix:\u0026lt;!\u0026gt;(Int $x){ [*] 1..$x } say [+] 1!,2!,3!,4!,5!,6!,7!,8!,9!,10! # 4037913  5、列出对象所有可用的方法  使用元对象协议， 即 对象名.^methods\n\u0026gt; \u0026#34;JZY\u0026#34;.^methods  BUILD Int Num chomp chop substr pred succ match ords lines samecase samespace tr im-leading trim-trailing trim words encode wordcase trans indent codes path WHIC H Bool Str Stringy DUMP ACCEPTS Numeric gist perl comb subst split\n  6、 匿名子例程  my $x = sub($a){ $a+2 };say $x($_) for 1..4 my $x = -\u0026gt; $a { $a+2 };say $x($_) for 1..4 my $x = * + 2;say $x($_) for 1..4  7、字符串翻转与分割  \u0026gt; 1223.flip 3221 \u0026gt; 'abcd'.flip dcba \u0026gt; 1234.comb 1 2 3 4 \u0026gt; 1234.comb(/./) 1 2 3 4 \u0026gt; 'abcd'.comb a b c d  8、有这么一个四位数 A，其个位数相加得到 B，将 B 乘以 B 的反转数后得到 A，请求出这个数字。  举例， 1458 就符合这个条件，1+4+5+8 ＝ 18， 18 ＊ 81 ＝1458\n请找出另一个符合上面条件的四位数。\n(^37).map: { my $r = $_ * .flip; 1000 \u0026lt; $r and $_ == [+] $r.comb and say $r } ^37 产生一个范围 0 .. ^37, 就是 0到36之前的数，在表达式中代表 B\nmy $b; for 1000..^10000 -\u0026gt; $i { $b=[+] $i.comb; say $i if $b*$b.flip == $i; } 输出:\n1458 1729  9、 大小写转换  \u0026gt; my $word= \u0026quot;I Love Raku\u0026quot; I Love Raku \u0026gt; $word.wordcase() I Love Raku \u0026gt; my $lowercase = \u0026quot;i love perl 6\u0026quot; i love perl 6 \u0026gt; $lowercase.wordcase() I Love Raku \u0026gt; $word.samecase('A') I LOVE PERL 6 \u0026gt; $word.samecase('a') i love perl 6 \u0026gt; $word.samecase('a').wordcase() I Love Raku  10、 多行文本  my $string = q:to/THE END/;Norway Oslo : 59.914289,10.738739 : 2 Bergen : 60.388533,5.331856 : 4 Ukraine Kiev : 50.456001,30.50384 : 3 Switzerland Wengen : 46.608265,7.922065 : 3 THE END say $string;  11、 超运算符与子例程  my @a = \u0026lt;1 2 3 4\u0026gt;; sub by2($n){ return 2*$n; } sub power2($n) { return $n ** 2; } my @b = @a».\u0026amp;by2».\u0026amp;power2; say @b; # 4 16 36 64 为什么是 \u0026amp;function 呢：\n the name of the by2 function is \u0026amp;by2, just as the name of the foo scalar is $foo and the name of the foo array is @foo\n  12、 如何在 Raku 中执行外部命令并捕获输出  \u0026gt; my $res = qqx{mkdir 123456} # 或使用 qx{ } \u0026gt; my $res = qx{mkdir 112233}  13、Does Raku support something equivalent to Perl5\u0026rsquo;s DATA and END sections?  =fooThis is a Pod block. A single line one. This Pod block\u0026#39;s name is \u0026#39;foo\u0026#39;.  =beginquxThis is another syntax for defining a Pod block. It allows for multi line content. This block\u0026#39;s name is \u0026#39;qux\u0026#39;. =end qux =dataA data block -- a Pod block with the name \u0026#39;data\u0026#39;.  # Data blocks are P6\u0026#39;s version of P5\u0026#39;s __DATA__. # But you can have multiple data blocks: =begindataAnother data block. This time a multi line one. =end data $=pod.grep(*.name eq \u0026#39;data\u0026#39;).map(*.contents[0].contents.say); say \u0026#39;-\u0026#39; x 45; for @$=pod { if .name eq \u0026#39;data\u0026#39; { say .contents[0].contents } }  14、生成含有26个英文字母和下划线的 junction  any(\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;,\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;,\u0026#39;_\u0026#39;);  15、判断一个字符是否在某个集合中  \u0026gt; so any(\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;,\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;) ∈ set(\u0026#34;12a34\u0026#34;.comb) \u0026ldquo;12a34\u0026rdquo;.comb 会把字符串分割为单个字符，返回一个字符数组。\n 16、生成 IP 地址范围  .say for \u0026#34;192.168.10.\u0026#34; «~» (0..255).list  17、 生成 OC 中的测试数组  .say for \u0026#34;@\u0026#34; «~» \u0026#39;\u0026#34;Perl\u0026#39; «~» (1..30).list «~» \u0026#39;\u0026#34;,\u0026#39; @\u0026quot;Perl\u0026quot;1\u0026quot;, @\u0026quot;Perl\u0026quot;2\u0026quot;, @\u0026quot;Perl\u0026quot;3\u0026quot;, @\u0026quot;Perl\u0026quot;4\u0026quot;, @\u0026quot;Perl\u0026quot;5\u0026quot;, …   18、我想以 AGCT 4 种字母为基础生成字符串。  比如希望长度为1，输出A,G,C,T。\n如果长度为2，输出 AA,AG,AC,AT,GA,GG,GC,GT,CA,CG,CC,CT,TA,TG,TC,TT。这样的结果。\n@a X~ \u0026#34;\u0026#34; # 长度为1 (@a X~ @a) # 长度为2 (@a X~ @a) X~ @a # 长度为3 @a X~ @a X~ @a X~ @a # 长度为4 \u0026gt; my @a=\u0026lt;A G C T\u0026gt; A G C T \u0026gt; my $x=@a A G C T \u0026gt; $x xx 2 A G C T A G C T \u0026gt; $x xx 3 A G C T A G C T A G C T \u0026gt; ($x xx 3).WHAT (List) \u0026gt; $x.WHAT (Array) \u0026gt; ([X~] $x xx 2).join(',') AA,AG,AC,AT,GA,GG,GC,GT,CA,CG,CC,CT,TA,TG,TC,TT 惰性操作符：\nmy @a=\u0026lt;A G C T\u0026gt;; my $x=@a; # 或者使用 $x = @(\u0026#39;A\u0026#39;,\u0026#39;G\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;T\u0026#39;) for 1 ...^ * -\u0026gt; $a {(([X~] $x xx $a)).join(\u0026#39;,\u0026#39;).say;last if $a==4;}; Best Of Raku  Command Line 命令行  # Perl 5 Raku print \u0026#34;bananas are good\\n\u0026#34;; say \u0026#34;bananas are good\u0026#34;; print \u0026#34;and I said: $quotes{\\\u0026#34;me\\\u0026#34;}\\n\u0026#34;; say \u0026#34;and I said: %quotes{\u0026#34;me\u0026#34;}.\u0026#34;; print \u0026#34;and I said: $quotes{\\\u0026#34;me\\\u0026#34;}\\n\u0026#34;; say \u0026#34;and I said: %quotes\u0026lt;me\u0026gt;.\u0026#34;; print \u0026#34;What is ... \u0026#34;; $result = prompt \u0026#34;What is ... \u0026#34;; chomp($result = \u0026lt;\u0026gt;);  File IO  # Perl 5 Raku $content = do { local $/; $content = slurp \u0026#34;poetry.txt\u0026#34;; open my $FH, \u0026#34;poetry.txt\u0026#34;; \u0026lt;$FH\u0026gt; }; chomp(@content = do { @content = lines \u0026#34;poetry.txt\u0026#34;; open my $FH, \u0026#34;poetry.txt\u0026#34;; \u0026lt;$FH\u0026gt; });  Automatic multithreading  Applying operations to junctions and arrays is now syntactically compact and readable. Raku will create threads where appropriate to use multiple processors, cores or hyperthreading for high level language SIMD concurrent processing.\n# Perl 5 Raku my $sum; my $sum = [+] @numbers; $sum += $_ for @numbers; for (0 .. $#factor1) { @product = @factor1 \u0026gt;\u0026gt;*\u0026lt;\u0026lt; @factor2; $product[$] = $factor1[$] * $factor2[$_]; } The Perl 5 code is a simplification, of course Raku \u0026ldquo;does the right thing\u0026rdquo; when the arrays have different lengths.\n 比较  Here are junctions, then chained comparison operators.\n# Perl 5 Raku if ($a == 3 or $a == 4 or $a == 7) {...} if $a = 3 | 4 | 7 {...} if (4 \u0026lt; $a and $a \u0026lt; 12) {...} if 4 \u0026lt; $a \u0026lt; 12 {...} if (4 \u0026lt; $a and $a \u0026lt;= 12) {...} if $a ~~ 4^..12 {...} $a = defined $b ? $b : $c; $a = $b // $c; The defined-OR operator eases lot of cases where Perl 5 newbies could fall into traps.\n Case 结构  # Perl 5 Raku given $a { if ($a == 2 or $a == 5) {...} }} when 2 | 5 {...} elsif ($a == 6 or $a == 7 or $a == 8 or $a == 9) {} when 6 .. 9 {...} elsif ($a =~ /g/) {...} when \u0026#39;g\u0026#39; {...} else {...} default {...} } That new construct (backported to 5.10) is clear to read, very versatile and when used in combination with junctions, becomes even clearer.\n 强大的循环  List iteration via for is now much more versatile.\n# Perl 5 Raku for my $i (0..15) {...} for ^16 -\u0026gt; $i {...} for (my $i=15; $i\u0026gt;1; $i-2) {...} for 15,*-2...1 -\u0026gt; $i {...} for my $key (keys %hash) { for %hash.kv -\u0026gt; $key, $value { print \u0026#34;$key=\u0026gt; $hash{$key}\\n\u0026#34;; ... say \u0026#34;$key=\u0026gt; $value\u0026#34;; ... for my $i (0..$#a) { for zip(@a; @b; @c) -\u0026gt; $a, $b, $c {...} my $a = @a[$i]; my $b = @b[$i]; my $c = @c[$i]; ...  子例程中的具名参数  # Perl 5 Raku sub routine { sub routine ($a, $b, *@rest) {...} my $a = shift; my $b = shift; my @rest = @_; }  Objects with auto generated new and getters and setters  Simple Object creation is now as easy as it gets.\n# Perl 5 Raku package Heart::Gold; class Heart::Gold { has $.speed; sub new { method stop { $.speed = 0 } bless {speed =\u0026gt; 0 }, shift; } } my Heart::Gold $hg1 .= new; sub speed { $hg1.speed = 100; my $self = shift; my $hg2 = $hg1.clone; my $speed = shift; if (defined $speed) { $self-\u0026gt;{speed} = $speed } else { $self-\u0026gt;{speed} } } sub stop { my $self = shift; $self-\u0026gt;{speed} = 0; } Raku Variable  Variable Types  Raku (as Perl 5) knows 3 basic types of variables: Scalars (single values), Arrays (ordered and indexed lists of several values) and Hashes (2 column table, with ID and associated value pairs). They can be easily distinguished, because in front of their name is a special character called sigil (latin for sign). It\u0026rsquo;s the $ (similar to S) for Scalars, @ (like an a) for Arrays and a % (kv pair icon) for a Hash. They are now invariant (not changing), which means for instance, an array vaiable starts always with an @, even if you just want a slice of the content.\n$scalar @array @array[1] # $array[1] in Perl 5 @array[1,2] # @array[1,2] in Perl 5 %hash %hash{\u0026#39;ba\u0026#39;} # $hash{\u0026#39;ba\u0026#39;} in Perl 5 %hash{\u0026#39;ba\u0026#39;,\u0026#39;da\u0026#39;,\u0026#39;bim\u0026#39;} # @hash{\u0026#39;ba\u0026#39;,\u0026#39;da\u0026#39;,\u0026#39;bim\u0026#39;} in Perl 5 The sigils also mark distinct namespaces, meaning: in one lexical scope you can have 3 different variables named $stuff, @stuff and %stuff. These sigils can also be used as an operator to enforce a context in which the following data will be seen.\nThe fourth namespace is for subroutines and similar, even if you don\u0026rsquo;t usually think of them as variables. It\u0026rsquo;s sigil \u0026amp; is used to refer to subroutines without calling them.\nAll special namespaces from Perl 5 (often marked with special syntax), like tokens (PACKAGE), formats, file or dir handles, or builtins are now regular variables or routines.\nBecause all variables contain objects, they have methods. In fact, all operators, including square or curly bracket subscripts, are just methods of an object with a fancy name.\nThe primary sigil can be followed by a secondary sigil, called a twigil, which indicates a special scope for that variable.\nScalar This type stores one value, usually a reference to something: a value of a data type, a code object, an object or a compound of values like a pair, junction, array, hash or capture. The scalar context is now called item context, hence the scalar instruction from Perl 5 was renamed to item.\n$CHAPTER = 3; # first comment! $bin = 0b11; # same value in binary format $pi = 3.14159_26535_89793; # the underscores just ease reading $float = 6.02e-23; # floating number in scientific notation $text = \u0026#39;Welcome all!\u0026#39;; # single quoted string # double quoted string, does eval $pi to it\u0026#39;s content $text = \u0026#34;What is $pi?\u0026#34;; $text = q:to\u0026#39;EOT\u0026#39;;# heredoc string handy for multiline text like HTML templates or email EOT $handle = open $file_name; # file handle # an object from a class with a nested namespace $object = Class::Name.new(); $condition = 3|5|7; # a junction, a logical conjunction of values $arrayref = [0,1,1,2,3,5,8,13,21]; # an array stored as a single item # a hash stored as a single item $hashref = {\u0026#39;audreyt\u0026#39; =\u0026gt; \u0026#39;pugs\u0026#39;, \u0026#39;pm\u0026#39; =\u0026gt; \u0026#39;pct\u0026#39;, \u0026#39;damian\u0026#39; =\u0026gt; \u0026#39;larrys evil henchman\u0026#39;}; # pointing to a callable $coderef = sub { do_something_completely_diffenent(@_) }; (For info on some of those terms: comment, binary format, the underscores ease reading, scientific notation, single-quoted string, double-quoted string, heredoc string, file handle, class, junction, list of values, hash, callable.)\nUnlike Perl 5, references are automatically dereferenced to a fitting context. So you could use these $arrayrefs and $hashrefs similarly to an array or hash, making $ the universal variable prefix, pretty much like in PHP. The primary difference is that $ prefixed lists are not flattened in lists.\nmy $a = (1, 2, 3); my @a = 1, 2, 3; for $a { } # just one iteration for @a { } # three iterations Scalar Methods my $chapter = 3; undefine $chapter; defined $a; # false, returns 0  Array  An array is an ordered and indexed list of scalars. If not specified otherwise, they can be changed, expanded and shortened anytime and used as a list, stack, queue and much more. As in Haskell, lists are processed lazily, which means: the compiler looks only at the part it currently needs. This way Raku can handle infinite lists or do computation on lists that have not been computed yet. The lazy command enforces this and the eager command forces all values to be computed.\nThe list context is forced with a @() operator or list() command. That\u0026rsquo;s not autoflattening like in Perl 5 (automatically convert a List of Lists into one List). If you still want that, say flat(). Or say lol() to explicitly prevent autoflattening.\n@primes = (2,3,5,7,11,13,17,19,23); # an array gets filled like in Perl 5 @primes = 2,3,5,7,11,13,17,19,23 ; # same thing, since unlike P5 round braces just do group @primes = \u0026lt;2 3 5 7 11 13 17 19 23\u0026gt;; # ditto, \u0026lt;\u0026gt; is the new qw() $primes = (2,3,5,7,11,13,17,19,23); # same array object just sits in $primes, $primes[0] is 2 $primes = item @primes; # same thing, more explicit $primes = 2,; # just 2, first element of the Parcel @primes = 2; # array with one element @primes = [2,3,5,7,11,13,17,19,23]; # array with one element (List of Lists - LoL) @dev = {\u0026#39;dan\u0026#39; =\u0026gt; \u0026#39;parrot\u0026#39;}; # array with one element (a Hash) @data = [1..5],[6..10],[11..15]; # Array of Arrays (LoL) @list = lol @data; # no change @list = flat @data; # returns 1..15  Array Slices  @primes # all values as list @primes.values # same thing @primes.keys # list of all indices \u0026#34;@primes[]\u0026#34; # insert all values in a string, uses [] to distinguish from mail adresses $prime = @primes[0]; # get the first prime $prime = @primes[*-1]; # get the last one @some = @primes[2..5]; # get several $cell = @data[1][2]; # get 8, third value of second value (list) $cell = @data[1;2]; # same thing, shorten syntax @numbers = @data[1]; # get a copy of the second subarray (6..10) @copy = @data; # shallow copy of the array  Array Methods  Some of the more important things you can do with lists. All the methods can also used like ops in \u0026ldquo;elems @array;\u0026rdquo;\n? @array; # boolean context, Bool::True if array has any value in it, even if it\u0026#39;s a 0 + @array; # numeric context, number of elements (like in Perl 5 scalar @a) ~ @array; # string context, you get content of all cells, stringified and joined, same as \u0026#34;@primes[]\u0026#34; @array.elems; # same as + @array @array.end; # number of the last element, equal to @array.elems-1 @array.cat; # same ~ @array @array.join(\u0026#39;\u0026#39;); # also same result, you can put another string as parameter that gets between all values @array.unshift; # prepend one value to the array @array.shift; # remove the first value and return it @array.push; # add one value on the end @array.pop; # remove one value from the end and return it @array.splice($pos,$n);# starting at $pos remove $n values and replace them with values that follow those two  parameters  @array.delete(@ind); # delete all cells with indices in @ind @array.exists(@ind); # Bool::True if all indices of @ind have a value (can be 0 or \u0026#39;\u0026#39;) @array.pick([$n]); # return $n (default is 1) randomly selected values, without duplication @array.roll([$n]); # return $n (default is 1) randomly selected values, duplication possible (like roll dice) @array.reverse; # all elements in reversed order # returns a list where $n times first item is taken to last # position if $n is positive, if negative the other way around @array.rotate($n); @array.sort($coderef); # returns a list sorted by a user-defined criteria, default is alphanumerical sorting @array.min; # numerical smallest value of that array @array.max; # numerical largest value of that array $a,$b= @array.minmax; # both at once, like in .sort, .min, or .max, a sorting algorithm can be provided @array.map($coderef); # high oder map function, runs $coderef with every value as $_ and returns the list or results @array.classify($cr); # kind of map, but creates a hash, where keys are the results of $cr and values are from @array @array.categorize($cr);# kind of classify, but closure can have no (Nil) or several results, so a key can have a list of values @array.grep({$_\u0026gt;1}); # high order grep, returns only these elements that pass a condition ($cr returns something positive) @array.first($coder); # kind of grep, return just the first matching value @array.zip; # join arrays by picking first element left successively from here and then there There is even a whole class of metaoperators that work upon lists.  Hash  In Raku a Hash is an unordered list of Pairs. A Pair is a single key =\u0026gt; value association and appears in many places of the language syntax. A hash allows lookup of values by key using {} or \u0026lt;\u0026gt; syntax.\n%dev = \u0026#39;pugs\u0026#39;=\u0026gt;\u0026#39;audreyt\u0026#39;, \u0026#39;pct\u0026#39;=\u0026gt;\u0026#39;pm\u0026#39;, \u0026#34;STD\u0026#34;=\u0026gt;\u0026#39;larry\u0026#39;; %dev = :rakudo(\u0026#39;jnthn\u0026#39;), :testsuite(\u0026#39;moritz\u0026#39;); # adverb (pair) syntax works as well %dev = (\u0026#39;audreyt\u0026#39;, \u0026#39;pugs\u0026#39;, \u0026#39;pm\u0026#39;, \u0026#39;pct\u0026#39;, \u0026#39;larry\u0026#39;, \u0026#34;STD\u0026#34;); # lists get autoconverted in hash context %compiler = Parrot =\u0026gt; {Rakudo =\u0026gt; \u0026#39;jnthn\u0026#39;}, SMOP =\u0026gt; {Mildew =\u0026gt; \u0026#39;ruoso\u0026#39;}; # hash of hashes (HoH)  Hash Slices  $value = %dev{\u0026#39;key\u0026#39;}; # just give me the value related to that key, like in P5 $value = %dev\u0026lt;pm\u0026gt;; # \u0026lt;\u0026gt; autoquotes like qw() in P5 $value = %dev\u0026lt;\u0026lt;$name\u0026gt;\u0026gt;; # same thing, just with eval @values = %dev{\u0026#39;key1\u0026#39;, \u0026#39;key2\u0026#39;}; @values = %dev\u0026lt;key1 key2\u0026gt;; @values = %dev\u0026lt;\u0026lt;key1 key2 $key3\u0026gt;\u0026gt;; %compiler\u0026lt;Parrot\u0026gt;\u0026lt;Rakudo\u0026gt;; # value in a HoH, returns \u0026#39;jnthn\u0026#39; %compiler\u0026lt;SMOP\u0026gt;; # returns the Pair: Mildew =\u0026gt; \u0026#39;ruoso\u0026#39; %dev {\u0026#39;audrey\u0026#39;}; # error, spaces between varname and braces (postcircumfix operator) are no longer allowed %dev\\ {\u0026#39;allison\u0026#39;}; # works, quote the space %dev .\u0026lt;dukeleto\u0026gt;; # error %dev\\ .{\u0026#39;patrick\u0026#39;}; # works too, \u0026#34;long dot style\u0026#34;, because it\u0026#39;s an object in truth  Hash Methods  ? %dev # bool context, true if hash has any pairs + %dev # numeric context, returns number of pairs(keys) ~ %dev # string context, nicely formatted 2 column table using \\t and \\n $table = %dev; # same as ~ %dev %dev.say; # stringified, but only $key and $value are separated by \\t @pairs = %dev; # list of all containing pairs %dev.pairs # same thing in all context %dev.elems # same as + %dev or + %dev.pairs %dev.keys # returns a list of all keys %dev.values # list of all values %dev.kv # flat list with key1, value1, key 2 ... %dev.invert # reverse all key =\u0026gt; value relations %dev.push (@pairs) # inserts a list of pairs, if a key is already present in %dev, both values gets added to an array  Callable  Internally subroutines, methods and alike are variables with the sigil \u0026amp; and stored in a fourth namespace. Unlike Perl 5, all subroutines can be overwritten or augmented with user defined routines. Of course scalars can also contain routines.\n\u0026amp;function = sub { ... }; # store subroutine in callable namespace function(); # call/run it $coderef = sub { ... }; # store it in a scalar $coderef($several, $parameter); # run that code  Data Types  In contrast to variable types (container types) every value has a type too. These are organized internally as classes or roles and can be categorized into 3 piles: the undefined, immutable, and the mutable types.\nYou can assign one of these types to scalar, array, or hash variables, which enforces the contents to be that type.\nmy Int $a; my Int @a; # array of Int  Pair  Pairs are new and their syntax is used nearly everywhere in the language where there is an association between a name and a value.\n$pair = \u0026#39;jakub\u0026#39; =\u0026gt; \u0026#39;helena\u0026#39;; # \u0026#34;=\u0026gt;\u0026#34; is the pair constructor $pair = :jakub(\u0026#39;helena\u0026#39;); # same in adverbial notation $pair = :jakub\u0026lt;helena\u0026gt;; # same using \u0026lt;\u0026gt;, the new qw() $pair.key # returns \u0026#39;jakub\u0026#39; $pair.value # returns \u0026#39;helena\u0026#39; $pair.isa(Pair) # Bool::True  Capture  Captures are also a new type, which holds the parameters a routine gets. Because Perl now knows both positional and named parameters, it is a mixture of a list and array.\n$cap = \\(@a,$s,%h,\u0026#39;a\u0026#39;=\u0026gt;3); # creating a capture, \u0026#34;\\\u0026#34; was free since there are no references anymore |$cap # flatten into argument list (without |, it will pass it as a single value) ||$cap # flatten into semicolon list (meant for variadic functions that take list of lists) One important difference between a capture and a compound structure of lists and hashes: While assignments with = will copy the complete content of the named variables, this is not so in the case of a capture. When I change sinthelastexample, thecontentofcap changes too, because when parameters to a routine are variables, they are also interpolated in the moment the routine is called, not when it\u0026rsquo;s defined.\n  Assignment and Binding\n  Assignment\n  As rightfully expected, assignments are done with the equal sign. But unlike Perl 5 you always get a copy of the right side data assigned to the left, no matter how nested the data structure was (lists of lists eg). You never get in Raku a reference with =. The only exception may be seen captures.\nmy @original = [1,2],[3,4]; my $copy = @original[0]; # $copy points to [1,2] @original[0][0] = \u0026#39;fresh stuff\u0026#39;; # $copy[0] holds still 1  Binding  Since every variable in Raku is a reference, programmers can use binding to get 2 variables that point to the same memory location.\n$original = 5; $original := $mirror; # normal binding, done on runtime $original ::= $mirror; # same thing, but done during compile time $original = 3; say $mirror; # prints 3 $original =:= $mirror # true, because they\u0026#39;re bound together $original === $mirror # also true, because content and type are equal  FP oriented  sub balanced($s) { .none \u0026lt; 0 and .[*-1] == 0 given [\\+] \u0026#39;\\\\\u0026#39; «leg« $s.comb; } my $n = prompt \u0026#34;Number of bracket pairs: \u0026#34;; my $s = \u0026lt;[ ]\u0026gt;.roll($n*2).join; say \u0026#34;$s{ balanced($s) ?? \u0026#34;is\u0026#34; !! \u0026#34;is not\u0026#34; }well-balanced\u0026#34;  String munging  sub balanced($_ is copy) { () while s:g/\u0026#39;[]\u0026#39;//; $_ eq \u0026#39;\u0026#39;; } my $n = prompt \u0026#34;Number of bracket pairs: \u0026#34;; my $s = \u0026lt;[ ]\u0026gt;.roll($n*2).join; say \u0026#34;$sis\u0026#34;, \u0026#39;not\u0026#39; xx not balanced($s)), \u0026#34;well-balanced\u0026#34;;  Parsing with a grammar  grammar BalBrack { token TOP {\u0026#39;[\u0026#39;\u0026lt;TOP\u0026gt;*\u0026#39;]\u0026#39;} } my $n = prompt \u0026#34;Number of bracket pairs: \u0026#34;; my $s = (\u0026#39;[\u0026#39; xx $n, \u0026#39;]\u0026#39; xx $n).pick(*).join; say \u0026#34;$s{ BalBrack.parse($s) ?? \u0026#34;is\u0026#34; !! \u0026#34;is not\u0026#34; }well-balanced\u0026#34;;  凯撒加密  实现一个凯撒加密， 编码和解码都要有\nkey 是一个 1 到 25 之间的整数\nmy @alpha = \u0026#39;A\u0026#39; .. \u0026#39;Z\u0026#39;; sub encrypt ( $key where 1..25, $plaintext ) { $plaintext.trans( @alpha Z=\u0026gt; @alpha.rotate($key) ); } sub decrypt ( $key where 1..25, $cyphertext ) { $cyphertext.trans( @alpha.rotate($key) Z=\u0026gt; @alpha ); } my $original = \u0026#39;THE FIVE BOXING WIZARDS JUMP QUICKLY\u0026#39;; my $en = encrypt( 13, $original ); my $de = decrypt( 13, $en ); .say for $original, $en, $de; say \u0026#39;OK\u0026#39; if $original eq all( map { .\u0026amp;decrypt(.\u0026amp;encrypt($original)) }, 1..25 ); Output: THE FIVE BOXING WIZARDS JUMP QUICKLY GUR SVIR OBKVAT JVMNEQF WHZC DHVPXYL THE FIVE BOXING WIZARDS JUMP QUICKLY OK   日期格式化  使用 \u0026ldquo;2007-11-10\u0026rdquo; 和 \u0026ldquo;Sunday, November 10, 2007\u0026rdquo; 日期格式显式当前日期:\nuse DateTime::Utils; my $dt = DateTime.now; say strftime(\u0026#39;%Y-%m-%d\u0026#39;, $dt); say strftime(\u0026#39;%A, %B %d, %Y\u0026#39;, $dt);  阶乘  n 的阶乘定义为 n*(n-1)*(n-2)…*1, 零的阶乘为1.\n定义一个函数返回一个数字的阶乘。\n 使用自定义后缀操作符  sub postfix:\u0026lt;!\u0026gt;($n where $n \u0026gt; 0) { [*] 2..$n } say 5!  [*]  my @a = 1, [\\*] 1..*; say @a[5]; 标量容器中存储的对象不会在 flattening 上下文中插值，即使那个对象是可迭代的。\nmy @a = 3,4,5; for 1, 2, @a { .say } # 5次迭代 输出:\n1 2 3 4 5 my $s = @a; for 1, 2, $s { ... } # 3次迭代 输出:\n1 2 3 4 5 这里，$s 和 @a 指向同一个数组对象，但是标量容器的出现阻止 $s 被展开到 for 循环中。\n.list 和 .flat 方法能被用于还原展开行为：\nfor 1, 2, $s.list { .say } # 5次遍历 for 1, 2, @($s) { .say } # 5次遍历，@()会强制为列表上下文 输出:\n1 2 3 4 5 相反，.item 方法和 $() 能用于防止插值：\nmy @b = 1,2,@a; # @b 有5个元素 my @c = 1,2,@a.item; # @c 有3个元素 my @c = 1,2,$(@a); # 同上 say +@c; # 3  Feed operators  feed 操作符是完全懒惰的，意味着在使用者要求任何元素之前不会执行任何操作。这就是\nmy @a \u0026lt;== grep { ... } \u0026lt;== map { ... } \u0026lt;== grep { ... } \u0026lt;== 1, 2, 3 是完全懒惰的。\n Grammars  文法是一种强大的工具, 用于拆解文本,并通常返回数据结构 例如, Raku 是使用 Raku 风格的文法解析和执行的. 对普通 Raku 用户来说,一个更实用的例子就是 JSON::Simple 模块, 这个模块能反序列化任何有效的 JSON 文件, 反序列化代码还写了不到 100 行, 简单,可扩展.\n词法允许你组织正则, 就像类允许你组织普通代码的方法一样.\n命名正则 命名正则有特殊的语法, 与子例程的定义类似:\nmy regex number {\\d+[\\.\\d+]?} 这个例子中, 我们必须使用 my 关键词指定这个正则是词法作用域的, 因为 命名正则 通常用在 词法中. 给正则命名后有利于在其他地方复用正则:\nsay \u0026#34;32.51\u0026#34; ~~ \u0026amp;number; say \u0026#34;15 + 4.5\u0026#34; ~~ /\u0026lt;number\u0026gt;\\s*\u0026#39;+\u0026#39;\\s*\u0026lt;number\u0026gt;/ 首先说下, 使用 regex/token/rule 定义了一个正则表达式后怎么去调用它。就像调用一个子例程那样, 使用 \u0026amp; 符号: \u0026amp; 后面跟正则表达式的名字, 即 \u0026amp;regex_name。\nregex 不是命名正则仅有的标识符 \u0026ndash; 实际上, 它用的不多. 大多数时候, 用的最多的是 token 和 rule 标识符. 它们都是不能回溯的, 这意味着正则引擎在匹配失败时不会备份和重试. 这通常是你想要的, 但不是对所有场合都合适:\nmy regex works-but-slow {.+q } my token fails-but-fast {.+q } # Tokens 不会沿原路返回, 这让它们更快地失败! my $s = \u0026#39;Tokens won\\\u0026#39;t backtrack, which makes them fail quicker!\u0026#39;; say so $s ~~ \u0026amp;works-but-slow; # True say so $s ~~ \u0026amp;fails-but-fast; # False, the entire string get taken by the .+ token 和 rule 标识符的不同之处在于 rule 标识符让 Regex 的 :sigspace 起作用了:\nmy token non-space-y {once upon a time } my rule space-y {once upon a time } say \u0026#39;onceuponatime\u0026#39; ~~ \u0026amp;non-space-y; say \u0026#39;once upon a time\u0026#39; ~~ \u0026amp;space-y; Action Classes 实际上, 命名正则甚至能接受额外的参数, 它使用的语法跟子例程参数列表的语法一样.\n​写一个程序打印从 1 到 100 的整数，但是对 3 的倍数打印 \u0026ldquo;Fizz\u0026rdquo;, 对 5 的倍数打印 \u0026ldquo;Buzz\u0026rdquo;, 对于即是 3 的倍数，又是 5 的倍数的打印 \u0026ldquo;FizzBuzz\u0026rdquo;.\nfor 1 .. 100 { when $_ %% (3 \u0026amp; 5) { say \u0026#39;FizzBuzz\u0026#39;; } when $_ %% 3 { say \u0026#39;Fizz\u0026#39;; } when $_ %% 5 { say \u0026#39;Buzz\u0026#39;; } default { .say; } } Or abusing multi subs:\nmulti sub fizzbuzz(Int $ where * %% 15) { \u0026#39;FizzBuzz\u0026#39; } multi sub fizzbuzz(Int $ where * %% 5) { \u0026#39;Buzz\u0026#39; } multi sub fizzbuzz(Int $ where * %% 3) { \u0026#39;Fizz\u0026#39; } multi sub fizzbuzz(Int $number ) { $number } (1 .. 100)».\u0026amp;fizzbuzz.join(\u0026#34;\\n\u0026#34;).say; Most concisely:\nsay \u0026#39;Fizz\u0026#39; x $_ %% 3 ~ \u0026#39;Buzz\u0026#39; x $_ %% 5 || $_ for 1 .. 100; And here\u0026rsquo;s an implementation that never checks for divisibility:\n.say for ((\u0026#39;\u0026#39; xx 2, \u0026#39;Fizz\u0026#39;) xx * Z~ (\u0026#39;\u0026#39; xx 4, \u0026#39;Buzz\u0026#39;) xx *) Z||1 .. 100; ","permalink":"https://ohmyweekly.github.io/notes/2015-03-16-features-in-raku/","tags":["rakulang"],"title":"Raku 中的特性(二)"},{"categories":["rakulang"],"contents":"一个日历 #!/usr/bin/env raku constant @months = \u0026lt;January February March April May June July August September October November December\u0026gt;; constant @days = \u0026lt;Su Mo Tu We Th Fr Sa\u0026gt;; sub center(Str $text, Int $width) { my $prefix = \u0026#39;\u0026#39; x ($width - $text.chars) div 2; my $suffix = \u0026#39;\u0026#39; x $width - $text.chars - $prefix.chars; return $prefix ~ $text ~ $suffix; } sub MAIN(:$year = Date.today.year, :$month = Date.today.month) { my $dt = Date.new(:year($year), :month($month), :day(1) ); my $ss = $dt.day-of-week % 7; my @slots = \u0026#39;\u0026#39;.fmt(\u0026#34;%2s\u0026#34;) xx $ss; my $days-in-month = $dt.days-in-month; for $ss ..^ $ss + $days-in-month { @slots[$_] = $dt.day.fmt(\u0026#34;%2d\u0026#34;); $dt++ } my $weekdays = @days.fmt(\u0026#34;%2s\u0026#34;).join: \u0026#34;\u0026#34;; say center(@months[$month-1] ~ \u0026#34;\u0026#34; ~ $year, $weekdays.chars); say $weekdays; for @slots.kv -\u0026gt; $k, $v { print \u0026#34;$v\u0026#34;; print \u0026#34;\\n\u0026#34; if ($k+1) %% 7 or $v == $days-in-month; } } Bags and Sets 过去几年, 我写了很多这种代码的变种：\nmy %words; for slurp.comb(/\\w+/).map(*.lc) -\u0026gt; $word { %words{$word}++; } (此外: slurp.comb(/\\w+/).map(*.lc) 从指定的标准输入或命令行读取文件, 遍历数据中的单词, 然后小写化该单词。 eg: raku slurp.pl score.txt)\nRaku 引入了两种新的组合类型来实现这种功能。 在这种情况下, 半路杀出个 KeyBag 代替了 hash:\nmy %words := KeyBag.new; for slurp.comb(/\\w+/).map(*.lc) -\u0026gt; $word { %words{$word}++; } 这种情况下, 为什么你会喜欢 KeyBag 多于散列呢, 难道是前者代码更多吗？很好, 如果你想要的是一个正整数值的散列的话, KeyBag 将更好地表达出你的意思。\n%words{\u0026#34;the\u0026#34;} = \u0026#34;green\u0026#34;; 未处理过的异常：不能解析数字：green\n然而 KeyBag 有几条锦囊妙计。首先, 四行代码初始化你的 KeyBag 不是很罗嗦, 但是 Raku 能让它全部写在一行也不会有问题：\nmy %words := KeyBag.new(slurp.comb(/\\w+/).map(*.lc)); KeyBag.new 尽力把放到它里面的东西变成 KeyBag 的内容。给出一个列表, 列表中的每个元素都会被添加到 KeyBag 中, 结果和之前的代码块是完全一样的。\n如果你不需要在创建bag后去修改它, 你可以使用 Bag 来代替 KeyBag。不同之处是 Bag 是不会改变的；如果 %words 是一个 Bag, 则 %words{$word}++ 是非法的。如果对你的程序来说, 不变没有问题的话, 那你可以让代码更紧凑。\nmy %words := bag slurp.comb(/\\w+/).map(*.lc); # 散列 %words 不会再变化 bag 是一个有用的子例程, 它只是对任何你给它的东西上调用 Bag.new 方法。（我不清楚为什么没有同样功能的 keybag 子例程）\nBag 和 KeyBag 有几个雕虫小技。它们都有它们自己的 .roll 和 .pick 方法, 以根据给定的值来权衡它们的结果：\n\u0026gt; my $bag = bag \u0026quot;red\u0026quot; =\u0026gt; 2, \u0026quot;blue\u0026quot; =\u0026gt; 10; \u0026gt; say $bag.roll(10); \u0026gt; say $bag.pick(*).join(\u0026quot; \u0026quot;); blue blue blue blue blue blue red blue red blue blue red blue blue red blue blue blue blue blue blue blue This wouldn’t be too hard to emulate using a normal Array, but this version would be: \u0026gt; $bag = bag \u0026quot;red\u0026quot; =\u0026gt; 20000000000000000001, \u0026quot;blue\u0026quot; =\u0026gt; 100000000000000000000; \u0026gt; say $bag.roll(10); \u0026gt; say $bag.pick(10).join(\u0026quot; \u0026quot;); blue blue blue blue red blue red blue blue blue blue blue blue red blue blue blue red blue blue sub MAIN($file1, $file2) { my $words1 = bag slurp($file1).comb(/\\w+/).map(*.lc); my $words2 = set slurp($file2).comb(/\\w+/).map(*.lc); my $unique = ($words1 (-) $words2); for $unique.list.sort({ -$words1{$_} })[^10] -\u0026gt; $word { say \u0026#34;$word:{ $words1{$word}}\u0026#34;; } } 传递两个文件名, 这使得 Bag 从第一个文件中获取单词, 让 Set 从第二个文件中获取单词, 然后使用 集合差 操作符 (-) 来计算只在第一个文件中含有的单词, 按那些单词出现的频率排序, 然后打印出前10 个单词。\n这是介绍 Set 的最好时机。就像你从上面猜到的一样, Set 跟 Bag 的作用很像。不同的地方在于, 它们都是散列, 而 Bag 是从Any到正整数的映射, Set 是从 Any 到 Bool::True 的映射。集合 Set 是不可改变的, 所以也有一个 可变的 KeySet。\n在 Set 和 Bag 之间, 我们有很丰富的操作符：\n操作符\tUnicode\t“Texas”\t结果类型 属于\t∈\t(elem)\tBool 不属于\t∉\t!(elem)\tBool 包含\t∋\t(cont)\tBool 不包含\t∌\t!(cont)\tBool 并集\t∪\t(|)\tSet 或 Bag 交集\t∩\t(\u0026amp;)\tSet 或 Bag 差集\t(-)\tSet 子集\t⊆\t(\u0026lt;=)\tBool 非子集\t⊈\t!(\u0026lt;=)\tBool 真子集\t⊂\t(\u0026lt;)\tBool 非真子集\t⊄\t!(\u0026lt;)\tBool 超级\t⊇\t(\u0026gt;=)\tBool 非超级\t⊉\t!(\u0026gt;=)\tBool 真超级\t⊃\t(\u0026gt;)\tBool 非真超级\t⊅\t!(\u0026gt;)\tBool bag multiplication\t⊍\t(.)\tBag bag addition\t⊎\t(+)\tBag set symmetric difference (^)\tSet 它们中的大多数都能不言自明。返回 Set 的操作符在做运算前会将它们的参数提升为 Set。返回 Bag 的操作符在做运算前会将它们的参数提升为 Bag。返回 Set 或 Bag 的操作符在做运算前会将它们的参数提升为 Bag, 如果它们中至少有一个是 Bag 或 KeyBag, 否则会转换为 Set；在任何一种情况下, 它们都返回提升后的类型。\n\u0026gt; my $a = bag \u0026lt;a a a b b c\u0026gt;; # bag(a(3), b(2), c) \u0026gt; my $b = bag \u0026lt;a b b b\u0026gt;; # bag(a, b(3)) \u0026gt; $a (|) $b; bag(\u0026quot;a\u0026quot; =\u0026gt; 3, \u0026quot;b\u0026quot; =\u0026gt; 3, \u0026quot;c\u0026quot; =\u0026gt; 1) \u0026gt; $a (\u0026amp;) $b; bag(\u0026quot;a\u0026quot; =\u0026gt; 1, \u0026quot;b\u0026quot; =\u0026gt; 2) \u0026gt; $a (+) $b; bag(\u0026quot;a\u0026quot; =\u0026gt; 4, \u0026quot;b\u0026quot; =\u0026gt; 5, \u0026quot;c\u0026quot; =\u0026gt; 1) \u0026gt; $a (.) $b; bag(\u0026quot;a\u0026quot; =\u0026gt; 3, \u0026quot;b\u0026quot; =\u0026gt; 6) 下面是作者放在 github上的 Demo\nA quick example of getting the 10 most common words in Hamlet which are not found in Much Ado About Nothing:\n\u0026gt; raku bin/most-common-unique.pl data/Hamlet.txt data/Much_Ado_About_Nothing.txt ham: 358 queen: 119 hamlet: 118 hor: 111 pol: 86 laer: 62 oph: 58 ros: 53 horatio: 48 clown: 47\n超棒的匿名函数 Raku 对函数有很好的支持。Raku 令人惊叹的把函数声明包起来, 让你可以用各种方法来定义一个函数又不丢失任何特性。你可以定义参数类型、可选参数、命名参数, 甚至在子句里也可以。如果我不知道更好的理由的话, 我可能都在怀疑这是不是在补偿 Perl5 里那个相当基本的参数处理（咳咳 , @_, 你懂的）。 除开这些, Raku 也允许你定义没有命名的函数。\nsub {say \u0026#34;lol, I\u0026#39;m so anonymous!\u0026#34; } 这有什么用？你不命名它, 就没法调用它啊, 对不？错！\n你可以保存这个函数到一个变量里。或者从另一个函数里 return 这个函数。或者传参给下一个函数。事实上, 当你不命名你的函数的时候, 你随后要运行什么代码就变得非常清晰了。就像一个可执行的 \u0026ldquo;todo\u0026rdquo; 列表一样。\n现在让我们说说匿名函数可以给我们做点什么。在 Raku 里它看起来会是什么样子呢？\n嗯, 就用最著名的排序来做例子吧。你可能想象 Raku 有一个 sort_lexicographically 函数和一个 sort_numberically 函数。不过其实没有。只有一个 sort 函数。当你需要具体用某种形式的排序时, 你就可以传递一个匿名函数给 sort 。\nmy @sorted_words = @words.sort({ ~$_ }); my @sorted_numbers = @numbers.sort({ +$_ }); （从技术上来说, 这是块, 不是函数。不过如果你不打算在里面使用 return 的话, 差异不大。） 当然你可以做的比这两个排序办法多多了。你可以通过鞋子大小排序, 或者最大地面速度, 或者自燃可能性的降序等等。因为你可以把任何逻辑作为一个参数传递进去。面向对象的教徒们对这种模式可非常自豪, 还专门命名为“依赖注入”。\n想想看, map、 grep 和 reduce 都很依赖这种函数传递。我们有时候把这种传递函数给函数的做法叫“高阶编程”, 好像这是某些高手的特权似的。但其实这是一个非常有用而且可以普通使用的技能。\n上面的示例都是在当前执行时就运行函数了。其实这里没什么限制。我们可以创建函数, 然后稍后再运行：\nsub make_surprise_for($name) { return sub { say \u0026#34;Sur-priiise, $name!\u0026#34; }; } my $reveal_surprise = make_surprise_for(\u0026#34;Finn\u0026#34;); # # 目前什么都没发生 # 等着 # 继续等着 # 等啊等啊等啊 $reveal_surprise(); # \u0026#34;Sur-priiise, Finn!\u0026#34; $reveal_surpirse 里的函数记住了 $name 变量值, 虽然原始函数是在很早之前传递进去的参数。棒极了！这个效果就叫在 $name 变量上闭合的匿名函数。不过这里可没什么技术 - 反正很棒就是了。\n事实上, 如果放在其他主要存储机制比如数组和散列旁边再看匿名函数本身, 这感觉是很自然的事情。所有这些都可以存储在变量里, 作为参数传递或者从函数里返回。一个匿名数组允许你保存序列给以后调用。一个匿名散列允许你存储映射给以后调用。一个匿名函数允许你存储计算或者行为给以后调用。\n本月晚些时候, 我会写篇介绍怎样通过 Raku 的动态域来创建漂亮的 DSL-y 接口。我们可以看到匿名函数在那里是怎么发挥作用的。\n第九天:最长标示匹配 Raku 正则表达式偏好尽可能的匹配最长的选择。\nsay \u0026#34;food and drink\u0026#34; ~~ /foo |food /; # food 这跟 Perl5 不一样。Perl5 更喜欢上面例子中的第一个选择, 结果匹配的是 \u0026ldquo;foo\u0026rdquo;。 如果你希望的话, 你依然可以按照优先匹配的原则运行, 这个原则隐藏在稍长选择操作符 || 背后：\nsay \u0026#34;food and drink\u0026#34; ~~ /foo ||food /; # foo \u0026hellip;就是这样。这就是最长标记匹配。 ☺ 短文完毕。\n“喂, 等等！”你听见你绝望而惊讶的大叫了, 满足你希望让每天的 Raku 圣临历走的慢一点的愿望。“为什么说最长标记匹配很重要？谁会在意这个？”\n我很高兴你这样问。事实证明, 最长标记匹配（简称 LTM ）在如何解析的时候和我们的直觉配合相当默契。如果你创造了一门语言, 你希望人们可以声明一个叫 forest_density 的变量而不用提及这个单词和循环里用的 for 语法冲突, LTM 可以做到。\n我喜欢“奇怪的一致性”这个说法 - 尤其当程序语言设计的共性让大家越来越雷同的时候。这里就是一种在类和语法之间的一致性。 Raku 基本上把这种一致性发挥到了极致。让我简单的阐述下我的意思。 现在我们习惯于写一个类, 总体来看, 类差不多是长这个样子的：\nclass { method method method } 奇怪的是, 语法有个非常类似的结构：\ngrammar { rule rule rule } (实际上关键词有 regex, token 和 rule, 不过当我们把他当作一个组来讨论的时候, 我们暂时统一叫做 rules)\n我们同样习惯于派生子类（class B is A）, 然后添加或者重写方法来产生一个新旧行为在一起的组合。Pelr6 提供了 multi methods , 它允许你添加相同名字的新方法, 而且不重写原有的, 它只尝试匹配所有的到新方法而已。这个调度是由一个（通常自动生成的） proto method 处理的。它负责调度给所有合格的候选者。\n这些是怎样用语法和角色运行起来的呢？额, 首先它从原有的里面派生出新的语法, 和派生子类一样。（事实上, 底层是 完全 相同的机制。语法不过是有个不同元类对象的类罢了。）新的角色也会重写原有的角色, 和你在方法上习惯的一样。\nS05 有个漂亮的解析信件的示例。然后派生出来解析正式信件的语法：\ngrammar Letter { rule text {} rule greet {[Hi|Hey|Yo]$=(\\S+?), $$} rule body {+?} # note: backtracks forwards via +? rule close {Later dude, $=(.+)} } grammar FormalLetter is Letter { rule greet {Dear $=(\\S+?), $$} rule close { Yours sincerely, $=(.+) }} 派生出来的 FormalLetter 重写了 greet 和 close, 但是没重写 body。\n但是这一切在 multi 方法下也能正常运行吗？我们是不是可以定义一种“原型角色”来允许我们在一个语法里用同样的名字有多种角色, 内容各不相同？比如, 我们可能希望用一个角色 term 来解析语言, 不过有很多不同的 terms：字符串、数字……而且数字可能是十进制、二进制、八进制、十六进制等……\nRaku 语法可以包含一个原型角色, 然后你可以定义、重定义同名角色随便多少次。显然让我们回到文章最开始的 / foo | food /。所有你起了相同名字的角色会编译成一个大的 alternation。\n不仅如此 - 调用其他角色的角色, 有些可能是原型角色, 这些也会全部扁平化到一个大的 LTM 轮流选择里。实践中, 这意味着一个 term 的所有可能会一次被全部尝试一遍, 机会平等。没哪个会因为自己是先定义的所以胜出, 只有最长匹配的那个选择才胜出。\n这个奇怪的一致性说明事实上, 在调用某个方式的时候, 最具体的方法胜出, 而且这个“最具体”必须加上引号。签名里参数描述类型越好, 方法就越具体。\n在分析某个角色的时候, 同样是最具体的角色胜出, 不过这里“最具体”必须成功解析才行。角色描述下一步进入的文本越详细, 角色就越具体。\n这就是奇怪的一致性。因为表面上方法和角色看起来就是完全不一样的怪兽。\n我们真心相信我们理解了派生语法的原理并且得到了一门新的语言。 LTM 就是最合适的因为它允许新旧角色通过一个公平和可预测的办法混杂在一起。角色不是因为他们定义的前后而胜出, 而是因为它能最好的解析文本。这才是挑选精英的办法。\n事实上, Raku 编译器自己就是这样工作的。它使用 Raku 语法解析你的程序, 这个语法是可以派生的……不管你在程序里什么时候声明了一个新操作符, 都会给你派生出一个新的语法。新操作符的解析就作为新角色加入到新语法里。然后把解析剩余程序的任务交给新的语法。你的新操作符会胜过那写相同但匹配更短的, 不过输给相同但匹配更长的。\n开开心心玩 Rakudo 和 Euler 项目 Raku 实现的领先者 Rakudo , 目前还不完美, 说起性能也尤其让人尴尬。然而先行者不会问“他快么？”, 而会问“他够快么？”, 甚至是“我怎样能帮他变得更快呢？”。\n为了说服你 Rakudo 已经能做到足够快了。我们准备尝试做一组 Euler 项目测试。其中很多涉及强行的数值计算, Rakudo 目前还不是很擅长。不过我们可没必要就此顿足：语言性能降低了, 程序员就要更心灵手巧了, 这正是乐趣所在啊。\n所有的代码都是在 Rakudo 2012.11 上测试通过的。\n先从一些简单的例子开始。\n想想斐波那契序列里数值不超过四百万的元素, 计算这些值的总和。办法超级简单：\nsay [+] grep * %% 2, (1, 2, *+* ...^ * \u0026gt; 4_000_000); 运行时间：0.4秒\n注意怎样使用操作符才能让代码即紧凑又保持可读性(当然这点大家肯定意见不一)。我们用了：\n 无论如何用 * 创建 lambda 函数 用序列操作符 ...^ 来建立斐波那契序列 用整除操作符 %% 来过滤元素 用 [+] 做 reduce 操作计算和  当然, 没人强制你这样疯狂的使用操作符 - 香草(vanilla)命令式的代码也没问题：\n 600851475143 的最大素因数是多少？\n 命令式的解决方案是这样的：\nsub largest-prime-factor($n is copy) { for 2, 3, *+2 ... * { while $n %% $_ { $n div= $_; return $_ if $_ \u0026gt; $n; } } } say largest-prime-factor(600_851_475_143); 运行时间：2.6秒\n注意用的 is copy, 因为 Raku 的绑定参数默认是只读的。还有用了整数除法 div, 而没用数值除法的 /。\n到目前为止都没有什么特别的, 我们继续:\nn从1到100, nCr的值, 不一定要求不同, 有多少大于一百万的？\n我们将使用 feed 操作符 ==\u0026gt; 来分解算法成计算的每一步：\n[1], -\u0026gt; @p { [0, @p Z+ @p, 0] } ... * # 生成杨辉三角 ==\u0026gt; (*[0..100])() # 生成0到100的n行 ==\u0026gt; map *.list # 平铺成一个列表 ==\u0026gt; grep * \u0026gt; 1_000_000 # 过滤超过1000000的数 ==\u0026gt; elems() # 计算个数 ==\u0026gt; say; # 输出结果 运行时间：5.2s\n注意使用了 Z 操作符和 + 来压缩 0,@p 和 @p,0 的两个列表。\n这个单行生成杨辉三角的写法是从 Rosetta 代码里偷过来的。那是另一个不错的项目, 如果你对 Raku 的片段练习很感兴趣的话。\n让我们做些更巧妙的。\n存在一个毕达哥拉斯三元数组让 a +b + c = 1000 。求 a、b、c 的值。\n暴力破解可以完成 (Polettix 的解决办法), 但是这个办法不够快（在我机器上花了11秒左右）。让我们用点代数知识把问题更简单的解决。\n先创建一个 (a, b, c) 组成的毕达哥拉斯三元数组:\na \u0026lt; b \u0026lt; c a² + b² = c² 要求 N = a + b +c 就要符合：\nb = N·(N - 2a) / 2·(N - a) c = N·(N - 2a) / 2·(N - a) + a²/(N - a) 这就自动符合了 b \u0026lt; c 的条件。 而 a \u0026lt; b 的条件则产生下面这个约束：\na \u0026lt; (1 - 1/√2)·N 我们就得到以下代码了：\nsub triplets(\\N) { for 1..Int((1 - sqrt(0.5)) * N) -\u0026gt; \\a { my \\u = N * (N - 2 * a); my \\v = 2 * (N - a); # 检查 b = u/v 是否是整数 # 如果是, 我们就找到了一个三元数组 if u %% v { my \\b = u div v; my \\c = N - a - b; take $(a, b, c); } } } say [*] .list for gather triplets(1000); 运行时间：0.5s\n注意 sigilless 变量 \\N, \\a …… 的声明, $(...) 是怎么用来把三元数组作为单独元素返回的, 用$_.list 的缩写 .list 来恢复其列表性。\n\u0026amp;triplets 子例程作为生成器, 并且使用 \u0026amp;take 切换到结果。相应的 \u0026amp;gather 用来划定生成器的(动态)作用域, 而且它也可以放进 \u0026amp;triplets, 这个可能返回一个惰性列表。\n我们同样可以使用流操作符改写成数据流驱动的风格：\nconstant N = 1000; 1..Int((1 - sqrt(0.5)) * N) ==\u0026gt; map -\u0026gt; \\a { [ a, N * (N - 2 * a), 2 * (N - a) ] } ==\u0026gt; grep -\u0026gt; [ \\a, \\u, \\v ] { u %% v } ==\u0026gt; map -\u0026gt; [ \\a, \\u, \\v ] { my \\b = u div v; my \\c = N - a - b; a * b * c } ==\u0026gt; say; 运行时间：0.5s\n注意我们是怎样用解压签名绑定 -\u0026gt; [...] 来解压传递过来的数组的。\n使用这种特殊的风格没有什么实质的好处：事实上还很容易影响到性能, 我们随后会看到一个这方面的例子。 写纯函数式算法是个超级好的路子。不过原则上这就意味着让那些足够先进的优化器乱来（想想自动向量化和线程）。不过 Rakudo 还没到这个复杂地步。\n但是如果我们没有聪明到可以找到这么牛叉的解决办法, 该怎么办呢？\n求第一个连续四个整数, 他们有四个不同的素因数。\n除了暴力破解, 我没找到任何更好的办法：\nconstant $N = 4; my $i = 0; for 2..* { $i = factors($_) == $N ?? $i + 1 !! 0; if $i == $N { say $_ - $N + 1; last; } } 这里, \u0026amp;fators 返回素因数的个数, 原始的实现差不多是这样的：\nsub factors($n is copy) { my $i = 0; for 2, 3, *+2 ...^ * \u0026gt; $n { if $n %% $_ { ++$i; repeat while $n %% $_ { $n div= $_ } } } return $i; } 运行时间：unknown (33s for N=3)\n注意 repeat while ...{...} 的用法, 这是 do {...} while(...); 的新写法。 我们可以加上点缓存来加速程序：\nBEGIN my %cache = 1 =\u0026gt; 0; multi factors($n where %cache) { %cache{$n} } multi factors($n) { for 2, 3, *+2 ...^ * \u0026gt; sqrt($n) { if $n %% $_ { my $r = $n; $r div= $_ while $r %% $_; return %cache{$n} = 1 + factors($r); } } return %cache{$n} = 1; } 运行时间：unknown (3.5s for N=3)\n注意用 BEGIN 来初始化缓存, 不管出现在源代码里哪个位置。还有用 multi 来启用对 \u0026amp;factors 的多样调度。where 子句可以根据参数的值进行动态调度。\n哪怕有缓存, 我们依然无法在一个合理的时间内回答上来原来的问题。现在我们怎么办？只能用点骗子手段了Zavolaj – Rakudo 版本的 NativeCall – 来在C语言里实现因式分解。\n事实证明这还不够好, 所以我们继续重构剩下的代码, 添加一些原型声明：\nuse NativeCall; sub factors(int $n) returns int is native(\u0026#39;./prob047-gerdr\u0026#39;) { * } my int $N = 4; my int $n = 2; my int $i = 0; while $i != $N { $i = factors($n) == $N ?? $i + 1 !! 0; $n = $n + 1; } say $n - $N; 运行时间：1m2s (0.8s for N=3)\n相比之下, 完全使用C语言实现这个算法, 运行时间在0.1秒之内。所以目前 Rakudo 还没法赢得任何一种速度测试。\n重复一下, 用三种办法做一件事：\n在 2 ≤ a ≤ 100 和 2 ≤ b ≤ 100 的情况下由 ab生成的序列里有多少不一样的元素？ 下面是一个很漂亮但很慢的解决办法, 可以用来验证其他办法是否正确：\nsay +(2..100 X=\u0026gt; 2..100).classify({ .key ** .value }); 运行时间：11s\n注意使用 X=\u0026gt; 来构造笛卡尔乘积。用对构造器 =\u0026gt; 防止序列被压扁而已。\n因为 Rakudo 支持大整数语义, 所以在计算像 100100 这种大数的时候没有精密度上的损失。\n不过我们并不真的在意幂的值, 不过用基数和指数来唯一标示幂。我们需要注意基数可能自己本身就是前面某次的幂值：\nconstant A = 100; constant B = 100; my (%powers, %count); # 找出那些是之前基数的幂的基数 # 分别存储基数和指数 for 2..Int(sqrt A) -\u0026gt; \\a { next if a ~~ %powers; %powers{a, a**2, a**3 ...^ * \u0026gt; A} = a X=\u0026gt; 1..*; } # 计算重复的个数 for %powers.values -\u0026gt; \\p { for 2..B -\u0026gt; \\e { # 上升到 \\e 的幂 # 根据之前的基数和对应指数分类 ++%count{p.key =\u0026gt; p.value * e} } } # 添加 +%count 作为一个需要保存的副本 say (A - 1) * (B - 1) + %count - [+] %count.values; 运行时间：0.9s\n注意用序列操作符 ...^ 推断集合序列, 只要提供至少三个元素, 列表赋值 %powers{...} = ... 就会无休止的进行下去。\n我们再次用数据驱动的函数式的风格重写一遍：\nsub cross(@a, @b) { @a X @b } sub dups(@a) { @a - @a.uniq } constant A = 100; constant B = 100; 2..Int(sqrt A) ==\u0026gt; map -\u0026gt; \\a { (a, a**2, a**3 ...^ * \u0026gt; A) Z=\u0026gt; (a X 1..*).tree } ==\u0026gt; reverse() ==\u0026gt; hash() ==\u0026gt; values() ==\u0026gt; cross(2..B) ==\u0026gt; map -\u0026gt; \\n, [\\r, \\e] { (r) =\u0026gt; e * n } ==\u0026gt; dups() ==\u0026gt; ((A - 1) * (B - 1) - *)() ==\u0026gt; say(); 运行时间：1.5s\n注意我们怎么用 \u0026amp;tree 来防止压扁的。我们可以像之前那样用 X=\u0026gt; 替代 X , 不过这会让通过 -\u0026gt; \\n, [\\r, \\e] 解构变得很复杂。\n和预想的一样, 这个写法没像命令式的那样执行出来。怎么才能正常运行呢？这算是我留给读者的作业吧。\n解析 IPv4 地址 Raku 的正则现在是一种子语言了, 很多语法没有变:\n/\\d+/ 捕获数字：\n/(\\d+)/ 现在 $0 存储着匹配到的数字, 而不是 Perl 5 中的 $1. 所有的特殊变量 $0,$1,$2 在 Raku 里就是 $/[0], $/[1], $/[2]. 在 Perl 5 中, $0 是脚本或程序的文件名, 但是这在 Raku 中变成了 $*EXECUTABLE_NAME。\n如果你对获得一个正则匹配的所有捕获组感兴趣, 你可以使用 @(), 它是 @($/) 的语法糖。 $/ 变量中的对象拥有许多关于最后一次匹配的有用信息。例如, $/.from 将给你匹配的起始字符串位置。 但是 $0 将使我们在这篇文章中走得足够远。我们用它来从一个字符串中提取单个特征。\n修饰符现在放在前面了:\n$_ = \u0026#39;1 23 456 78.9\u0026#39;; say .Str for m:g/(\\d+)/; # 1 23 456 78 9 匹配所有看起来像这样的东西很有用, 以至于它有一个专门的 .comb 方法：\n$str.comb(/\\d+/); 如果你对 .split 很熟悉, 你可以想到 .comb 就是它的表哥, 它匹配 .split 丢弃的东西 。 Perl 5 中匹配 IPv4 地址的正则如下:\n/(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/ 这在 Raku 中是无效的。首先, {} 块在 Raku 的 正则中是真正的代码块；它们包含 Raku 代码。第二, 在 Raku 中请使用 ** N..M (或 ** N..*) 代替 {N,M}\n在 Raku 中匹配1到3位数字的正则如下:\n/\\d**1..3/ 匹配 Ipv4地址：\n/(\\d**1..3)\\.(\\d**1..3)\\.(\\d**1..3)\\.(\\d**1..3)/ 那仍有点笨拙。在 Raku 的正则中, 你可以使用重复操作符 % , 下面是重复 (\\d ** 1..3) 这个正则 4次, 并使用 . 点号作为分隔符。\n/(\\d**1..3)**4%\u0026#39;.\u0026#39;/ % 操作符是一个量词修饰符, 所以它只跟在一个像 * 或 + 或 ** 的量词后面。上面的正则意思是 匹配 4 组数字, 在每组数字间插入一个直接量 点号 .。\n你也可能注意到 \\. 变成了 '.' , 它们是一样的。\n$_ = \u0026#34;Go 127.0.0.1, I said! He went to 173.194.32.32.\u0026#34;; say .Str for m:g/(\\d**1..3)**4%\u0026#39;.\u0026#39;/; # output: 127.0.0.1 173.194.32.32 或者我们可以使用 .comb:\n$_ = \u0026#34;Go 127.0.0.1, I said! He went to 173.194.32.32.\u0026#34;; my @ip4addrs = .comb(/(\\d**1..3)**4%\u0026#39;.\u0026#39;/); # 127.0.0.1 173.194.32.32 如果我们对单独的数字感兴趣：\n$_ = \u0026#34;Go 127.0.0.1, I said! He went to 173.194.32.32.\u0026#34;; say .list\u0026gt;\u0026gt;.Str.perl for m:g/(\\d**1..3)**4%\u0026#39;.\u0026#39;/; # output: (\u0026#34;127\u0026#34;, \u0026#34;0\u0026#34;, \u0026#34;0\u0026#34;, \u0026#34;1\u0026#34;) (\u0026#34;173\u0026#34;, \u0026#34;194\u0026#34;, \u0026#34;32\u0026#34;, \u0026#34;32\u0026#34;) 引号 在很多地方, Raku 都提供给你更合理的默认设置以便在大多数情况下让你的工作变得更简单有趣。引号也不例外。\n最常见的两种引号就是单引号和双引号。单引号最简单：让你引起一个字符串。唯一的“魔法”就是你可以用反斜杠转义一个单引号。而因为反斜杠的这个作用, 你可以用 \\\\ 来表示反斜杠本身了。不过其实这个做法也是没必要的, 反斜杠自己可以直接传递。下面是一组例子：\n\u0026gt; say 'Everybody loves Magical Trevor’; Everybody loves Magical Trevor \u0026gt; say 'Oh wow, it\\'s backslashed!’; Oh wow, it's backslashed! \u0026gt; say 'You can include a \\\\ like this’; You can include a \\ like this \u0026gt; say 'Nothing like \\n is available’; Nothing like \\n is available \u0026gt; say 'And a \\ on its own is no problem’; And a \\ on its own is no problem 双引号, 额, 从字面上看就知道了, 两倍自然更强大了。:-) 它支持反斜杠转义, 但更重要的是他支持内插。也就是说变量和闭包可以放进双引号里。大大的帮你节约使用连接操作符或者字符串格式定义等等的时间。\n下面是几个简单的例子：\n\u0026gt; say \u0026quot;Ooh look!\\nLine breaks!\u0026quot; Ooh look! Line breaks! \u0026gt; my $who = 'Ninochka'; say \u0026quot;Hello, dear $who\u0026quot; Hello, dear Ninochka \u0026gt; say \u0026quot;Hello, { prompt 'Enter your name: ' }!\u0026quot; Enter your name: _Jonathan_ Hello, Jonathan! 上面第二个例子展示了标量内插, 第三个则展示了闭包也可以插入双引号字符串里。闭包产生的值会被字符串化然后插入字符串中。那除了 $ 开头的呢？ 规则是这样的：所有的都可以插入, 但前提是它们被某些后环缀(译者注：postcircumfix)(也就是带下标或者扩的数组或者哈希, 可以做引用或者方法调用)允许。事实上你也可以把他们都存进标量里。\n\u0026gt; my @beer = \u0026lt;Chimay Hobgoblin Yeti\u0026gt;; Chimay Hobgoblin Yeti \u0026gt; say \u0026quot;First up, a @beer[0]\u0026quot; First up, a Chimay \u0026gt; say \u0026quot;Then @beer[1,2].join(' and ')!\u0026quot; Then Hobgoblin and Yeti! \u0026gt; say \u0026quot;Tu je \u0026amp;prompt('Ktore pivo chces? ')\u0026quot; Ktore pivo chces? _Starobrno_ Tu je Starobrno 这里你看到了一个数组元素的内插, 一个被调用了方法的数组切片的内插和一个函数调用的内插。后环缀规则意味着我们再也不会砸掉你口年的邮箱地址了(译者注：邮箱地址里有@号)。\n\u0026gt; say \u0026#34;Please spam me at blackhole@jnthn.net\u0026#34; Please spam me at blackhole@jnthn.net 选择你自己的分隔符 单/双引号对大多数情况下都很好用, 不过如果你想在字符串里使用这些引号的时候咋办？继续用反斜杠不是什么好主意。其实你可以自定义其他字符做为引号字符。Raku 替你选好了。q 和 qq 引号结构后面紧跟的字符就会被作为分隔符。如果这个字符有相对应的关闭符, 那么就自动查找这个（比如, 如果你用了一个开启花括号{, 那么字符串就会在闭合花括号}处结束。注意你还可以使用多字符开启符和闭合符（不过要求是相同字符重复组成的多字符））。另外, q 的语义等同于单引号, qq 的语义等同于双引号。\n\u0026gt; say q{C'est la vie} C'est la vie \u0026gt; say q{{Unmatched } and { are { OK } in { here}} Unmatched } and { are { OK } in { here \u0026gt; say qq!Lottery results: {(1..49).roll(6).sort}! Lottery results: 12 13 26 34 36 46 定界符(Heredoc) 所有的引号结构都允许你包含多行内容。不过, 还有更好的办法：定界文档。还是用 q 或者 qq 开始, 然后跟上 :to 副词来定义我们期望在文本最后某行匹配的字符。让我们通过下面这个感人的故事看看它是怎么工作的。\nprint q:to/THE END/ Once upon a time, there was a pub. The pub had lots of awesome beer. One day, a Perl workshop was held near to the pub. The hackers drank the pub dry. The pub owner could finally afford a vacation. THE END 脚本的输出如下：\nOnce upon a time, there was a pub. The pub had lots of awesome beer. One day, a Perl workshop was held near to the pub. The hackers drank the pub dry. The pub owner could finally afford a vacation. 注意输出文本并没有像源程序那样缩进。定界符会自动清楚缩进到终端的级别。如果我们用 qq , 我们也可以往定界符里插入东西。注意这些都是通过字符串的 ident 方法实现的, 但是如果你的字符串里没有内插, 我们会在编译期的时候调用 ident 作为一种优化手段。\n你同样可以有多个定界符, 包括调用定界符里的数据的方法也是可以的（注意下面的程序就调用了 lines 方法）。\nmy ($input, @searches) = q:to/INPUT/,q:to/SEARCHES/.lines; Once upon a time, there was a pub. The pub had lots of awesome beer. One day, a Perl workshop was held near to the pub. The hackers drank the pub dry. The pub owner could finally afford a vacation. INPUT beer masak vacation whisky SEARCHES for @searches -\u0026gt; $s { say $input ~~ /$s/ ?? \u0026#34;Found $s\u0026#34; !! \u0026#34;Didn\u0026#39;t find $s\u0026#34;; } 这个程序输出是：\nFound beer Didn't find masak Found vacation Didn't find whisky 自定义引号结构的引号副词 单/双引号的语义, 也是 q 和 qq 的语义, 已经可以解决绝大多数情况了。不过如果你有这么种情况：你要输出内插闭包而不是标量怎么办？这时候就要用上引号副词了。它们决定你是否开启引号特性。下面是例子：\n\u0026gt; say qq:!s\u0026quot;It costs $10 to {\u0026lt;eat nom\u0026gt;.pick} here.\u0026quot; It costs $10 to eat here. 这里我们使用了 qq 语义, 但是关闭里标量内插, 这意味着我们可以放心往里写价钱而不用担心他会试图解析成上一次正则匹配的第十一个捕获值。注意这里使用的标准的冒号对(colonpair)语法。如果你希望从一个最基础的引号结构开始, 然后自己手动的一个个打开选项, 那么你应该使用 Q 结构。\n\u0026gt; say Q{$*OS\\n\u0026amp;sin(3)} $*OS\\n\u0026amp;sin(3) \u0026gt; say Q:s{$*OS\\n\u0026amp;sin(3)} MSWin32\\n\u0026amp;sin(3) \u0026gt; say Q:s:b{$*OS\\n\u0026amp;sin(3)} MSWin32 \u0026amp;sin(3) \u0026gt; say Q:s:b:f{$*OS\\n\u0026amp;sin(3)} MSWin32 0.141120008059867 这里我们用了无特性引号结构, 然后打开附加特性, 地一个是标量内插, 然后是反斜杠转义, 然后函数内插。注意我们同样可以选择自己希望的任何分隔符。\n引号结构是一门语言 最后, 值得一提的是：当解析器进入引号结构的时候, 其实他是切换成解析另外一个语言了。当我们用副词构建引号结构的时候, 他只不过是把这些额外的角色混合进基础的引号语言里来开启额外的特性。好奇的童鞋可以看这里： Rakudo 怎么做到的。而当我们碰到闭包或者其他内插的时候, 解析器再临时切换回主语言。所以你可以这样写：\n\u0026gt; say \u0026#34;Hello, { prompt \u0026#34;Enter your name: \u0026#34; }!\u0026#34; Enter your name: Jonathan Hello, Jonathan! 解析器不会困惑于内插的闭包里又带有其他双引号字符串的问题。因为我们解析主语言, 然后切换到引号语言, 然后返回主语言, 然后重新再返回引号语言来解析这个程序里的字符串里的闭包里的字符串。这就是 Raku 解析器送给我们的圣诞节礼物, 俄罗斯套娃娃。\n","permalink":"https://ohmyweekly.github.io/notes/2015-04-15-raku-calendar-2012/","tags":["calendar"],"title":"Raku 圣诞月历 2012"},{"categories":["rakulang"],"contents":"Sneaking into a loop Zoffix 回答了一个关于 Perl 5 的 \u0026lt;\u0026gt; 操作符的问题。\nslurp.words.Bag.sort(-*.value).fmt(\u0026#34;%10s3d\\n\u0026#34;).say; slurp 会从 STDIN 中读取整个 \u0026ldquo;file\u0026rdquo; 并返回一个 Str。方法 Str::words 会按照某种 Unicode 意义的单词把该字符串分割成一个列表。把列表强转为 Bag 则创建一个计数 Hash, 它是如下表述的快捷方式。\nmy %h; %h{$_}++ for \u0026lt;peter paul marry\u0026gt;; dd %h; # OUTPUT«Hash %h = {:marry(1), :paul(1), :peter(1)}␤» 在关联数组上调用 .sort(-*.value) 会按照值的降序排序并返回一个排序后的 Pairs 列表。List::fmt 会调用 Pair::fmt, 它调用 fmt 方法, .key 作为其第二个参数, .value 也作为参数。say 会会使用一个空格连接各个元素并输出到标准输出。最后一步有一点错误因为除了第一行之外的每一行前面都会有一个额外的空格。\nslurp.words.Bag.sort(-*.value).fmt(\u0026#34;%10s=\u0026gt; %3d\u0026#34;).join(\u0026#34;\\n\u0026#34;).say; 手动连接字符串更好。这对于简短的单行程序来说有点多了。我们需要找到最长的单词并使用 .chars 来获取列宽。\nslurp 会在 $*IN 身上调用 .slurp-rest 方法。\n$*IN = \u0026lt;peter paul marry peter paul paul\u0026gt; but role { method slurp-rest { self.Str } }; 这是一种 hack 因为它会在任何形式的类型检测上失败并且它除了 slurp 之外不会对任何东西起作用。还有, 实际上我们从 $*IN 那里解绑 STDIN。不要在工作中使用这个奇淫技巧。\n现在我们能开心地吞噬并开始计数了。\nmy %counted-words = slurp.words.Bag; my $word-width = [max] %counted-words.keys».chars; 并且继续在链子断开的地方继续。\n%counted-words.sort(-*.value).fmt(\u0026#34;%{$word-width}s3d\u0026#34;).join(\u0026#34;\\n\u0026#34;).say; 问题解决了但是很丑陋。我们把一个单行程序拆开了。我们来修复 fmt 以使它再次完整。\n我们想要的是一个 fmt 方法, 它接收一个位置的(Positional), 一个 printf 风格的格式字符串和一个格式字符串中的 block per %*。还有, 我们可能需要在 self.fmt 前面放上一个分隔符。\nmy multi method fmt(Positional:D: $fmt-str, *@width where *.all ~~ Callable, :$separator = \u0026#34;\u0026#34;){ self.fmt( $fmt-str.subst(:g, \u0026#34;%*\u0026#34;, { my \u0026amp;width = @width[$++] // Failure.new(\u0026#34;missingh block\u0026#34;); \u0026#39;%\u0026#39; ~ (\u0026amp;width.count == 2 ?? width(self, $_) !! width(self)) }), $separator); } 表达式 *.all ~~ Callable 检查 slurp array中的所有元素是否实现了 CALL-ME(那是实际被执行的方法在你执行 foo()的时候)。\n然后我们在格式字符串上使用了 subst 来替换 %*, 替换是一个(闭包)块儿, 它每次匹配被调用一次。而且这儿我们有不错的惯用法。\nsay \u0026#34;1-a 2-b 3-c\u0026#34;.subst(:g, /\\d/, {\u0026lt;one two three\u0026gt;[$++]}); # one-a two-b three-c 匿名状态变量 $ 从 0 开始计数, 每次代码块执行时增 1。实际上我们在这儿做的就是移除一个循环并给 subst 偷偷加入一个额外的计数器和数组下标。或者可以说我们注册了一个迭代器到 subst 里面的循环中。有人可能会质疑 subst 应该接收一个 Seq 作为它的第二个位置参数, 它会让调用变得冗长。无论如何, 我们把洞补上了。\n在第 11 行, 我们从吞噬数组中拿出一个元素或者在没有元素时创建一个 Failure。我们把 block 存储在一个变量中因为我们想在第 12 行中内省。如果那个 block 接收两个位置参数,we feed the topic subst is calling the block with as a 2nd parameter to our stored block. 那碰巧是一个 Match 并且对于影响所匹配的东西可能有用。在我们这个例子中我们对 %* 进行匹配并且当前位置由 $++ 计数。做完那个之后我们得到了一个格式字符串, 它带有一个由用户提供的 fmt 版本的列宽参数。\n用户提供的块儿使用一组 Pairs 调用。我们不得不深入一层以得到更大的键。\n{[max] .values».keys».chars} 得到第一列的列宽。\nprint %counted-words.sort(-*.value).\u0026amp;fmt( \u0026#34;%*s3d\u0026#34;, {[max] .values».keys».chars}, separator =\u0026gt; \u0026#34;\\n\u0026#34;); 那个时髦的 .\u0026amp;fmt 调用是必须的因为我们免费的浮点方法不是 List 的方法。\n","permalink":"https://ohmyweekly.github.io/notes/2015-03-23-sneaking-into-a-loop/","tags":["rakulang"],"title":"Sneaking Into a Loop"},{"categories":["rakulang"],"contents":"Subset 子集 Subset 可用于声明某一类型的子集。\n输入一个密码, 要求密码必须满足如下条件：\n1、 至少 8 位 2、 必须包含大写字母、小写字母 和 数字\nsubset Length8 of Str where *.chars \u0026lt; 8; subset UpCase of Str where none(\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;) ∈ *.comb.Set; subset LowerCase of Str where none(\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;) ∈ *.comb.Set; subset IntNumber of Str where none(\u0026#39;0\u0026#39;..\u0026#39;9\u0026#39;) ∈ *.comb.Set; my $guess = prompt(\u0026#39;Enter your password:\u0026#39;); given $guess { when Length8 { say \u0026#39;密码长度必须为 8 位 以上\u0026#39;; proceed } when UpCase { say \u0026#39;密码必须包括大写字母\u0026#39;; proceed } when LowerCase { say \u0026#39;密码必须包含小写字母\u0026#39;; proceed } when IntNumber { say \u0026#39;密码必须包含数字\u0026#39;; } } 该程序具有可扩展性, 要增加一种密码验证, 只有添加一个 subset 就好了, 然后在 given/when 里面增加一个处理。\nproceed 相当于 continue, 不像 C 里面的 falling through, Raku 里面的 proceed 在继续执行下一个 when 语句时会计算 when 后面的条件。所以, 只要有 proceed, 则 proceed 后面的那个条件就会被执行。\n","permalink":"https://ohmyweekly.github.io/notes/2015-03-19-subset/","tags":["subset"],"title":"Subset"},{"categories":["rakulang"],"contents":"When 可以用在主题化($_)的语句中 Raku 里面有个特殊的变量叫 $_, 即主题化变量, the variable in question.\nfor (\u0026#39;Swift\u0026#39;, \u0026#39;PHP\u0026#39;, \u0026#39;Python\u0026#39;, \u0026#39;Perl\u0026#39;) -\u0026gt; $item { say $item when $item ~~ /^P/; } 输出:\nPHP Python Perl for (12, 24, 56, 42) { .say when *\u0026gt;40 } 输出:\n56 42 而 where 用于对类型进行约束:\nfor (\u0026#39;Swift\u0026#39;, \u0026#39;PHP\u0026#39;, \u0026#39;Python\u0026#39;, \u0026#39;Perl\u0026#39;, 42) -\u0026gt; $item where $item ~~ Str { say $item; } 输出:\nSwift PHP Python Perl Constraint type check failed for parameter '$item' ","permalink":"https://ohmyweekly.github.io/notes/2015-03-15-when-and-where/","tags":["where","when"],"title":"When and Where"},{"categories":["rakulang"],"contents":"Data munging in Raku 案例学习: 生成成绩报告单\nexample.txt STDOUT Peter\tB Celine\tA- Zsófia\tB+ João\tF Maryam\tB+ 秀英\tB- Finn\tD+ Aarav\tA Emma\tF Omar\tB 输出报告单：\nZsófia\u0026#39;s grade: B+ List of students with a failing grade: João, Emma Distribution of grades by letter: A: 2 students B: 5 students D: 1 student F: 2 students example.txt 是一个文本文件, 每行一个学生姓名和分数, 中间用空格分割。 我们希望我们的脚本能解析这样的文件并打印含有如下信息的报告：\n学生名为 “Zsófia” 的成绩\n所有不及格学生的名字 (i.e. worse than D-),\n根据字母( 不带 +/- ) 把成绩分组。得到学生成绩的分布。\n让我们一步步来, 添加 shebang 行:\n#!/usr/bin/env raku 在 Raku 中所有这些都为我们做好了。\n读取并解析输入:\nmy %grade = \u0026#34;grades.txt\u0026#34;.IO.lines.map: { m:s/^(\\w+)(\u0026lt;[A..F]\u0026gt;\u0026lt;[+-]\u0026gt;?)$/ or die \u0026#34;Can\u0026#39;t parse line \u0026#39;$_\u0026#39;\u0026#34;; ~$0 =\u0026gt; ~$1 }; 在 Raku 中, 对文件名字符串调用 .IO 方法会返回一个代表文件系统路径的对象, 我们可以继续在这个对象上调用 .lines 方法, 得到文件的所有行的一个惰性列表。“Lazy” 意味着它只会从磁盘中按需读取新行, 当我们使用 .map 方法遍历列表元素的时候, 这样能使用单个赋值操作就能优雅地初始化一个散列。\n我们不需要让文件句柄识别 Unicode, 也不用管文件句柄是否正确关闭, 这在 Raku 中都是默认发生的。\nmethod: ... 语法也可以写为 .method(...), 前者使 map 看起来更像一个 block 语句, 并减少了括号凌乱。 :s(\u0026ldquo;sigspace\u0026rdquo;) 正则修饰符使解析 token 间的空白更优雅。但 Raku 中的字符类比 Perl 5 复杂了一丢丢。 正则捕获结果变量($0, $1, …) 返回一个完整的 Match 对象 - 它为复杂使用场景增加了很多灵活性, 但是这里我们只想保留字符串, 所以使用 ~ 前置操作符字符串化了匹配对象。\n查看数据的特定项:\nsay \u0026#34;Zsófia\u0026#39;s grade: %grade\u0026lt;Zsófia\u0026gt;\u0026#34;; Raku 总是把散列中 { } 中的东西解析为表达式, 使用 \u0026lt; \u0026gt; 表示字面值。\n过滤数据:\nsay \u0026#34;List of students with a failing grade:\u0026#34;; say \u0026#34;\u0026#34; ~ %grade.grep(*.value ge \u0026#34;E\u0026#34;)».key.join(\u0026#34;, \u0026#34;); Raku 中允许我们按执行顺序把一些列方法写为链式操作。有一个重要区别：Raku 能让我们直接遍历散列的项, 散列中每一项都是一个 Pair 对象(Pair 对象能使用 .key 和 .value 方法)。\n* Whatever star 用于定义一个简单的回调, 而不用写一个花括号块。 ». hyper operator 用于对 .grep 返回的 Pairs 的每个 Pair 上调用 一次 .key 方法, 得出姓名列表\n从数据中创建频率分布:\nsay \u0026#34;Distribution of grades by letter:\u0026#34;; say \u0026#34;{.key}: {+.value}student{\u0026#34;s\u0026#34; if .value != 1}\u0026#34; for %grade.classify(*.value.comb[0]).sort(*.key); 计数和分组实在太常见了, Raku 提供了 .classify 方法。 classify 方法里需要指定要分组的项(这里是代表 %grade 条目的 Pair 对象), 这些项应该根据什么规则进行分组(这里是根据第一个字母的值, 它代表分数(没有 +/-)。\n这生成一个匿名的散列, 散列的值是匿名数组。\n%(\u0026#34;B\u0026#34; =\u0026gt; [\u0026#34;Peter\u0026#34; =\u0026gt; \u0026#34;B\u0026#34;, \u0026#34;Zsófia\u0026#34; =\u0026gt; \u0026#34;B+\u0026#34;, \u0026#34;Maryam\u0026#34; =\u0026gt; \u0026#34;B+\u0026#34;, \u0026#34;秀英\u0026#34; =\u0026gt; \u0026#34;B-\u0026#34;, \u0026#34;Omar\u0026#34; =\u0026gt; \u0026#34;B\u0026#34;], \u0026#34;A\u0026#34; =\u0026gt; [\u0026#34;Celine\u0026#34; =\u0026gt; \u0026#34;A-\u0026#34;, \u0026#34;Aarav\u0026#34; =\u0026gt; \u0026#34;A\u0026#34;], \u0026#34;F\u0026#34; =\u0026gt; [\u0026#34;João\u0026#34; =\u0026gt; \u0026#34;F\u0026#34;, \u0026#34;Emma\u0026#34; =\u0026gt; \u0026#34;F\u0026#34;], \u0026#34;D\u0026#34; =\u0026gt; [\u0026#34;Finn\u0026#34; =\u0026gt; \u0026#34;D+\u0026#34;]) 因为我们只对每组元素的个数感兴趣, 我们使用 + 前置操作符数字化每个值然后打印它, 在数组前面添加 + 符号会得到数组元素的个数。\n在 term 位置上一个单独的 .method 方法等价于 $_.method, 意思是对当前循环变量调用该方法。任意代码的返回值能使用花括号 {} 插值到字符串中。\nif 语句能被用作表达式 - 当条件为 false 时, 返回空列表, 然后被字符串化为空字符串。对字符串调用不带参数的 .comb 会生成该字符串的一个字符列表。\n","permalink":"https://ohmyweekly.github.io/notes/2014-12-19-raku-calendar/","tags":["calendar"],"title":"Raku 圣诞月历 - 2014"},{"categories":["rakulang"],"contents":"第一章 概要 略\n第二章 基础 假设有一场乒乓球比赛, 比赛结果以这种格式记录：\nPlayer1 Player2 | 3:2 这意味着选手1与选手2的比分为 3:2, 你需要一个脚本算出每位选手赢了几场比赛并且胜了几局。输入数据(存储在一个叫做 scores 的文件中)像下面这样：\nBeth Ana Charlie Dave Ana Dave | 3:0 Charlie Beth | 3:1 Ana Beth | 2:3 Dave Charlie | 3:0 Ana Charlie | 3:1 Beth Dave | 0:3 第一行是选手清单。随后每一行记录着比赛结果。\n这里使用 Raku 给出一种解决方案：\n#!/usr/bin/env raku my $file = open \u0026#39;scores\u0026#39;; my @names = $file.get.words ; # get 方法读入一行, 每调用一次 get, 读取一行 my %matches; # 赢得比赛次数 my %sets; # 赢得比赛局数 for $file.lines -\u0026gt; $line { # .lines 是惰性的 my ($pairing, $result) = $line.split(\u0026#39;| \u0026#39;); # 对剩下的每一行调用 split 操作 my ($p1, $p2) = $pairing.words; # 提取选手1和选手2的名字 my ($r1, $r2) = $result.split(\u0026#39;:\u0026#39;); # 提取比赛比分 %sets{$p1} += $r1; # 选手1赢得的比赛局数 %sets{$p2} += $r2; # 选手2赢得的比赛局数 if $r1 \u0026gt; $r2 { # 如果每场比赛中, 选手1赢的局数多于选手2, 则选手1赢得的比赛数+1, 反之选手2的+1 %matches{$p1}++; } else { %matches{$p2}++; } } my @sorted = @names.sort( { %sets{$_} } ).sort({ %matches{$_} } ).reverse; for @sorted -\u0026gt; $n { say \u0026#34;$nhas won %matches{$n}matches and %sets{$n}sets\u0026#34;; } 输出如下：\nAna has won 2 matches and 8 sets Dave has won 2 matches and 6 sets Charlie has won 1 matches and 4 sets Beth has won 1 matches and 4 sets 每个 Raku 程序应该以 #!/usr/bin/env raku 作为开始。\n在 Raku 中, 变量名以一个魔符打头, 这个魔符是一个非字母数字符号, 诸如 $, @, % 或者 \u0026amp;, 还有更少见的双冒号 ::。 内置函数 open 打开了一个名叫 scores 的文件, 并返回一个文件句柄, 即一个代表该文件的对象。赋值符号 = 将句柄赋值给左边的变量, 这意味着 $file 现在存储着该文件句柄。\nmy @names = $file.get.words; 上边这句的右侧对存储在 $file 中的文件句柄调用了 get 方法, get 方法从文件中读取并返回一行, 并去掉行的末尾。.words 也是一个方法, 用于从 get 方法返回的字符串上。.words 方法将它的组件 - 它操作的字符串, 分解成一组单词, 这里即意味着不含空格的字符串。它把单个字符串 \u0026lsquo;Beth Ana Charlie Dave\u0026rsquo; 转换成一组字符串 \u0026lsquo;Beth\u0026rsquo;, \u0026lsquo;Ana\u0026rsquo;, \u0026lsquo;Charlie\u0026rsquo;, \u0026lsquo;Dave\u0026rsquo;。最后, 这组字符串存储在数组 @names 中。\nmy %matches; my %sets; 在比分计数程序中, %matches 存储每位选手赢得的比赛数, %sets 存储每位选手赢得的比赛局数。\nfor $file.lines -\u0026gt; $line { ... } for 循环中 $file.lines 产生一组从文件 scores 读取的行, 从上次 $file.lines 离开的地方开始, 一直到文件末尾结束。 在第一次循环中, $line 会包含字符串 Ana Dave | 3:0; 在第二次循环中, $line 会包含 Charlie Beth | 3:1,以此类推。\nmy ($pairing, $result) = $line.split(\u0026#39;| \u0026#39;); split此处是一个方法, 字符串 \u0026lsquo;|\u0026rsquo; 是它的参数。\n第一次循环结束：\nVariable Contents $line 'Ana Dave | 3:0' $pairing 'Ana Dave' $result '3:0' $p1 'Ana' $p2 'Dave' $r1 '3' $r2 '0'  my @sorted = @names.sort({ %sets{$_} }).sort({ %matches{$_} }).reverse; 这一句是排序, 先按比赛局数多少排序, 再按赢得的比赛数排序, 然后反转。打印选手名字的时候以胜负次序排序, 代码必须使用选手的分数, 而非他们的名字来进行排序。sort 方法的参数是一个代码块, 用于将数组元素（选手的名字）转换成用于排序的数据。数组的元素通过变量 $_ 传递到代码块中。\n最简单的使用分数排序选手的方法应该是:\n@names.sort( { %matches{$_} } ) 这是通过使用赢得比赛的次数来进行排序。然而, Ana 和 Dave都赢了两场比赛。还需要比较谁赢的的比赛局数多, 才能决定比赛的排名。\n在双引号括起的字符串中, 标量和花括号中的变量能进行变量插值。\nmy $names = \u0026#39;things\u0026#39;; say \u0026#39;Do not call me $names\u0026#39;; # Do not call me $names say \u0026#34;Do not call me $names\u0026#34;; # Do not call me things 花括号中的数组进行插值后会变成用空格分隔的条目。花括号中的散列插值后每个散列键值对单独成为一行, 每行包含一个健, 随后是一个 tab 制表符, 然后是键值, 最后是一个新行符。\nsay \u0026#34;Math: { 1 + 2 }\u0026#34; # Math: 3 my @people = \u0026lt;Luke Matthew Mark\u0026gt;; say \u0026#34;The synoptics are: {@people}\u0026#34; # The synoptics are: Luke Matthew Mark say \u0026#34;{%sets}\u0026#34;; # From the table tennis tournament # Charlie 4 # Dave 6 # Ana 8 # Beth 4 当数组和散列变量直接出现在双引号字符串中(并且不在花括号 {} 里), 它们只在它们的名字后跟着一个 postcircumfix - 一对括号, 后面跟着语句时才会进行插值。在变量名和后置环缀之间进行方法调用也是可以的(例如 @flavours.sort())\nmy @flavours = \u0026lt;vanilla peach\u0026gt;; say \u0026#34;we have @flavours\u0026#34;; # we have @flavours, 这里没进行插值 say \u0026#34;we have @flavours[0]\u0026#34;; # we have vanilla, 后置环缀, 变量名字后面跟着一对儿括号 # so-called \u0026#34;Zen slice\u0026#34; say \u0026#34;we have @flavours[]\u0026#34;; # we have vanilla peach # 以后置环缀结尾的方法调用 say \u0026#34;we have @flavours.sort()\u0026#34;; # we have peach vanilla # 链式方法调用: say \u0026#34;we have @flavours.sort.join(\u0026#39;, \u0026#39;)\u0026#34;; # we have peach, vanilla 练习 例子中的第一行选手的名字是多余的, 你可以在参加比赛的选手中找出所有选手的名字！如果例子中的第一行被省略了, 你如何更改程序？提示：%hash.keys 返回散列 %hash 中的所有键。\n答案: 移除此行：\nmy @names = $file.get.words; 并且将\nmy @sorted = @names.sort({ %sets{$_} }).sort({ %matches{$_} }).reverse; 变成:\nmy @sorted = %sets.keys.sort({ %sets{$_} }).sort({ %matches{$_} }).reverse; 除了移除冗余, 你也可以用它来提醒我们, 如果一个选手没有在第一行的名字清单中被提到, 例如因为输入错误, 你该怎样修改你的程序？\n答案: 引入另外一个散列, 合法选手的名字作为键, 当读取选手名字的时候查找该散列：\nmy @names = $file.get.split(\u0026#39;\u0026#39;); my %legitimate-players; for @names -\u0026gt; $n { # -\u0026gt; 两侧要有空格 %legitimate-players{$n} = 1; } for $file.lines -\u0026gt; $line { my ($pairing, $result) = $line.split(\u0026#39;| \u0026#39;); my ($p1, $p2) = $pairing.split(\u0026#39;\u0026#39;); for $p1, $p2 -\u0026gt; $p { if !%legitimate-players{$p} { say \u0026#34;Warning: \u0026#39;$p\u0026#39;is not on our list!\u0026#34;; } } ... } 第三章 操作符 my @scores = \u0026#39;Ana\u0026#39; =\u0026gt; 8, \u0026#39;Dave\u0026#39; =\u0026gt; 6, \u0026#39;Charlie\u0026#39; =\u0026gt; 4, \u0026#39;Beth\u0026#39; =\u0026gt; 4; my $screen-width = 30; my $label-area-width = 1 + [max] @scores».key».chars; my $max-score = [max] @scores».value; my $unit = ($screen-width - $label-area-width) / $max-score; my $format = \u0026#39;%- \u0026#39; ~ $label-area-width ~ \u0026#34;s%s\\n\u0026#34;; for @scores { printf $format, .key, \u0026#39;X\u0026#39; x ($unit * .value); } 在这个例子中, 我们计算一下每位选手在竞标赛中赢得比赛的局数。\nmy @scores = \u0026#39;Ana\u0026#39; =\u0026gt; 8, \u0026#39;Dave\u0026#39; =\u0026gt; 6, \u0026#39;Charlie\u0026#39; =\u0026gt; 4, \u0026#39;Beth\u0026#39; =\u0026gt; 4; 这一句局包含了三个不同的操作符 = 和 =\u0026gt; 和 ,。以字符串连接操作符 ~ 为例, $string ~= \u0026quot;text\u0026quot; 等价于 $string = $string ~ \u0026quot;text\u0026quot;。\n=\u0026gt; 操作符(大键号)创建了一个键值对对象, 一个键值对存储着键和值；键在 =\u0026gt; 操作符的左侧, 值在右侧。这个操作符有一个特殊的特性：编译器会把 =\u0026gt; 操作符左侧的任何裸标识符解释为一个字符串。你也可以这样写：\nmy @scores = Ana =\u0026gt; 8, Dave =\u0026gt; 6, Charlie =\u0026gt; 4, Beth =\u0026gt; 4; 最后逗号操作符 , 构建了一个对象序列, 在该情况下, 所谓的对象就是键值对。\n这三个操作符都是中缀操作符, 这意味着它在两个条目之间。\n一个项前面可以有0个或多个前缀操作符, 所以你可以写比如 4 + -5。+ 号（一个中缀操作符）的后面, 编译器期望一个项, 为了将 - 号解释为项 5 的一个前缀。\nmy $label-area-width = 1 + [max] @scores».key».chars; » 是一个特殊的符号, 打印不出来可以用两个大于号 \u0026gt;\u0026gt; 代替。中缀操作符 max 返回两个值中的较大者, 所以 2 max 3 返回 3。方括号包裹着一个中缀操作符让 Raku 将该中缀操作符应用到列表中的元素之间。[max] 1,5,3,7 和 1 max 5 max 3 max 7 一样, 结果都为 7。\n同样地, [+] 用来计算列表元素的和, [*] 用来计算列表元素的积, [\u0026lt;=] 用来检查一个列表的值是否按递增排序。\n@scores».key».chars my @scores = Ana =\u0026gt; 8, Dave =\u0026gt; 6, Charlie =\u0026gt; 4, Beth =\u0026gt; 4; Ana 8 Dave 6 Charlie 4 Beth 4 @scores.key Method \u0026#39;key\u0026#39; not found for invocant of class \u0026#39;Array\u0026#39; @scores\u0026gt;\u0026gt;.key Ana Dave Charlie Beth 就像 @variable.method 在 @variable 上调用一个方法一样, @array».method 对 @array 中的每一项调用 method 方法, 并且返回一个返回值的列表。即 @scores\u0026gt;\u0026gt;.key 返回一个列表。\n@scores\u0026gt;\u0026gt;.key\u0026gt;\u0026gt;.chars # 每个名字含有几个字符 4 7 4 表达式 [max] @scores».key».chars 给出 (3,4,7,4) 中的最大值。它与下面的表达式相同：\n@scores[0].key.chars max @scores[1].key.chars max @scores[2].key.chars max ... @scores[0] \u0026#34;Ana\u0026#34; =\u0026gt; 8 @scores[0].key Ana my $format = \u0026#39;%- \u0026#39; ~ $label-area-width ~ \u0026#34;s%s\\n\u0026#34;; for @scores { printf $format, .key, \u0026#39;X\u0026#39; x ($unit * .value); } 定义一个格式, %- 表示左对齐, ~ 是字符串连接操作符 .for 循环中, @scores 中的每一项被绑定给特殊变量 $_, .key 是每项的键, 即名字, .value 是每项的键值, 即得分。小 x 是字符串重复操作符。\n关于优先级的的一句话 my @scores = \u0026#39;Ana\u0026#39; =\u0026gt; 8, \u0026#39;Dave\u0026#39; =\u0026gt; 6, \u0026#39;Charlie\u0026#39; =\u0026gt; 4, \u0026#39;Beth\u0026#39; =\u0026gt; 4; 等号右侧产生一个列表（因为逗号, 操作符）, 这个列表由对儿组成(因为 =\u0026gt;), 并且结果赋值给数组变量。 在 Perl5 中会这样解释:\n(my @scores = \u0026#39;Ana\u0026#39;) =\u0026gt; 8, \u0026#39;Dave\u0026#39; =\u0026gt; 6, \u0026#39;Charlie\u0026#39; =\u0026gt; 4, \u0026#39;Beth\u0026#39; =\u0026gt; 4; 以至于数组 @scores 中只有一个项, 表达式的其余部分被计算后丢弃。\n优先级规则控制着编译器如何解释这一行。Raku 的优先级规则申明 中缀操作符 =\u0026gt; 比 , 中缀操作符对于参数的绑定更紧, 而逗号操作符比等号赋值操作符绑定的更紧。\n实际上有两种不同优先级的赋值操作符。当赋值操作符右侧是一个标量时, 使用较紧优先级的项赋值操作符, 否则使用较松优先级的列表赋值操作符。(如同螺丝的松紧) 比较 $a = 1, $b = 2 和 @a = 1, 2, 前者是在一个列表中赋值给两个变量, 后者是将含有两个项的一个列表赋值给一个变量。\nsay 5 - 7 / 2; # 5 - 3.5 = 1.5 say (5 - 7) / 2; # (-2) / 2 = -1 Raku 中的优先级可以用圆括号改变, 但是如果圆括号直接跟在标识符的后面而不加空格的话, 则会被解释为参数列表。例如：\nsay(5 - 7) / 2; # -2 只打印出了 5-7 的值。\n优先级表\n   expression 名称     (), 42.5 (tightest precedence)   42.rand term   $x++ method calls and postcircumfixes   $x**2 autoincrement and autodecrement   ?$x, !$x exponentiation operator   +$x, ~$x boolean prefix   2*3, 7/5 prefix context operators   1+2, 7-5 multiplicative infix operators   $a x 3 additive infix operators   $x ~\u0026quot;.nn\u0026quot; replication operators   1\u0026amp;2 string concatenation   1 2   abs $x junctive OR   $x cmp 3 named unary prefix   $x == 3 non-chaining binary operators   $x \u0026amp;\u0026amp; $y chaining binary operators   $x    $x \u0026gt; 0 ?? 1 !! -1 tight OR infix   $x = 1 conditional operator   not $x item assignment   1, 2 loose unary prefix   1, 2 Z @a comma   @a = 1, 2 list infix   $x and say \u0026ldquo;Yes\u0026rdquo; list prefix, list assignment   $x or die \u0026ldquo;No\u0026rdquo; loose AND infix   ; loose OR infix    statement terminator    (loosest precedence)    比较和智能匹配 my @a = 1, 2, 3; my @b = 1, 2, 3; say @a === @a; # Bool::True say @a === @b; # Bool::False # these use identity for value say 3 === 3 # Bool::True say \u0026#39;a\u0026#39; === \u0026#39;a\u0026#39;; # Bool::True my $a = \u0026#39;a\u0026#39;; say $a === \u0026#39;a\u0026#39;; # Bool::True @b===@a; # False @a eqv @b; # True \u0026#39;2\u0026#39; eqv 2; # False 只有当两个对象有相同的类型和相同的结构时, eqv 操作符才返回 True。在前面定义的例子中, @a eqv @b 结果为 True, 因为 @a 和 @b 各自包含相同的值, 另一方面, \u0026lsquo;2\u0026rsquo; eqv 2 返回 \u0026lsquo;False\u0026rsquo; ,因为一个参数是字符串, 另一个是整数, 类型不相同。\n数字比较 使用 == 中缀操作符查看两个对象是否有相同的数字值。如果某个对象不是数字, Perl 会在比较之前尽力使其数字化。如果没有更好的方式将对象转换为数字, Perl 会使用默认的数字 0 。\nsay 1 == 1.0; # Bool::True say 1 == \u0026#39;1\u0026#39;; # Bool::True say 1 == \u0026#39;2\u0026#39;; # Bool::False say 3 == \u0026#39;3b\u0026#39;; # fails 跟数字比较相关的还有 \u0026lt;,\u0026lt;=,\u0026gt;,\u0026gt;=。如果两个对象的数字值不同, 使用 != 会返回 True 。\n如果你将数组或列表作为数字, 它会计算列表中项的个数。\nmy @colors = \u0026lt;red blue green\u0026gt;; if @colors == 3 { say \u0026#34;It\u0026#39;s true, @colorscontains 3 items\u0026#34;; } 字符串比较 Raku 中使用 eq 比较字符串, 必要时会将其参数转换为字符串。\nif $greeting eq \u0026#39;hello\u0026#39; { say \u0026#39;welcome\u0026#39;; } Table 3.2: Operators and Comparisons\n   数字比较 字符串比较 意思     == eq 等于   != ne 不等于   !== !eq 不等于   \u0026lt; lt 小于   \u0026lt;= le 小于或等于   \u0026gt; gt 大于   \u0026gt;= ge 大于或等于    例如, 'a' lt 'b' 为 true, 'a' lt 'aa' 也为 true。 != 是 !== 的便捷形式, 它实际是 ! 元操作符加在 中缀操作符 == 之前。同样地, ne 和 !eqs 是一样的。\n三路操作符\n三路操作符有两个操作数, 如果左侧较小, 返回 Order::Increase , 两侧相等则返回 Order::Same, 如果右侧较小则返回 Order::Decrease。对于数字使用三路操作符 \u0026lt;=\u0026gt;,对于字符串, 使用三路操作符 leg （取自 lesser, equal, greater）。中缀操作符 cmp 是一个对类型敏感的三路操作符, 它像 \u0026lt;=\u0026gt; 一样比较数字, 像 leg 一样比较字符串, 并且比较键值对儿时, 先比较键, 如果键相同再比较键值：\nsay 10 \u0026lt;=\u0026gt; 5; # +1 say 10 leg 5; # because \u0026#39;1\u0026#39; lt \u0026#39;5\u0026#39; say \u0026#39;ab\u0026#39; leg \u0026#39;a\u0026#39;; # +1, lexicographic comparison 三路操作符的典型用处就是用在排序中。列表中的 .sort 方法能使用一个含有两个值的块或一个函数, 比较它们, 并返回一个小于, 等于或大于 0 的值。 sort 方法根据该返回值进行排序：\nsay ~\u0026lt;abstract Concrete\u0026gt;.sort; # output: Concrete abstract say ~\u0026lt;abstract Concrete\u0026gt;.sort: -\u0026gt; $a, $b { uc($a) leg uc($b) }; # output: abstract Concrete 默认的, 比较是大小写敏感的, 通过比较它们的大写变形, 而不是比较它们的值, 这个例子使用了大小写敏感排序。\n智能匹配 使用 ~~ 做正确的事情。\nif $pints-drunk ~~ 8 { say \u0026#34;Go home, you\u0026#39;ve had enough!\u0026#34;; } if $country ~~ \u0026#39;Sweden\u0026#39; { say \u0026#34;Meatballs with lingonberries and potato moose, please.\u0026#34; } unless $group-size ~~ 2..4 { say \u0026#34;You must have between 2 and 4 people to book this tour.\u0026#34;; } 智能匹配总是根据 ~~ 右侧值的类型来决定使用哪种比较。上个例子中, 比较的是数字、字符串和范围。 智能匹配的工作方式 $answer ~~ 42 等价于 42.ACCPETS( $answer )。对 ~~ 操作符右侧的操作数调用 ACCEPTS 方法, 并将左操作数作为参数传入。\n第四章 子例程和签名 一个子例程就是一段执行特殊任务的代码片段。它可以对提供的数据(实参)操作, 并产生结果（返回值）。子例程的签名是它所含的参数和它产生的返回值的描述。从某一意义上来说, 第三章描述的操作符也是 Raku 用特殊方式解释的子例程。\n申明子例程 子例程申明由几部分组成。首先, sub 表明你在申明一个子例程, 然后是可选的子例程的名称和可选的签名。子例程的主体是一个用花括号扩起来的代码块。 默认的, 子例程是本地作用域的, 就像任何使用 my 申明的变量一样。这意味着, 一个子例程只能在它被申明的作用域内被调用。使用 our 来申明子例程可以使其在当前包中可见。\n{ our sub eat() { say \u0026#34;om nom nom\u0026#34;; } sub drink() { say \u0026#34;glug glug\u0026#34;; } } our \u0026amp;eat; # makes the package-scoped sub eat available in this lexical scope eat(); # om nom nom drink(); # 失败, can\u0026#39;t drink outside of the block our 也能让子例程从包或模块的外部是可见的：\nmodule EatAndDrink { our sub eat() { say \u0026#34;om nom nom\u0026#34;; } sub drink() { say \u0026#34;glug glug\u0026#34;; } } EatAndDrink::eat(); # om nom nom EatAndDrink::drink(); # fails, not declared with \u0026#34;our\u0026#34; 你也可以导出一个子例程, 让它在另外的作用域内可见。\n# in file Math/Trivial.pm module Math::Trivial { sub double($x) is export { return 2 * $x; } } 然后在其它程序或模块中你可以这样写:\nuse Math::Trivial; # imports sub double say double(21); # 21 is only half the truth Raku 的子例程都是对象。你可以将它们随意传递并存储在数据结构中。编程语言设计者常常将它们称之为 first-class 子例程；它们就像数组和散列一样作为语言的基础。\nFirst-class 子例程能帮助你解决复杂的问题。例如, 为了做出一个微型的ASCII艺术舞蹈图, 你可能要建立一个散列, 键是舞蹈动作的名称, 键值是匿名散列。假使使用者能键入一系列舞蹈动作（可能是站在舞蹈平台上或其它外部输入设备）。 你怎么保持一个变量清单中都是合法的行为, 允许使用者输入, 并限制输入是一系列安全的行为呢？\nmy %moves = hands-over-head =\u0026gt; sub { say \u0026#39;/o\\ \u0026#39; }, bird-arms =\u0026gt; sub { say \u0026#39;|/o\\| \u0026#39;}, left =\u0026gt; sub { say \u0026#39;\u0026gt;o \u0026#39; }, right =\u0026gt; sub { say \u0026#39;o\u0026lt; \u0026#39; }, arms-up =\u0026gt; sub { say \u0026#39;\\o/ \u0026#39; }; my @awesome-dance = \u0026lt;arms-up bird-arms right hands-over-head\u0026gt;; for @awesome-dance -\u0026gt; $move { %moves{$move}.(); # 在散列上调用方法 } outputs: \\o/ |/o\\| o\u0026lt; /o\\.  Adding Signatures 子例程的签名执行两个任务。首先, 它申明哪个调用者可能或必须将参数传递给子例程。第二, 它申明子例程中的变量被绑定到哪些参数上。这些变量叫做参数。Raku 的签名更深入, 它们允许你限制参数的类型, 值和参数的定义, 并准确匹配复杂数据结构的某一部分。此外, 它们也允许你显式地指定子例程返回值的类型。\n基础 签名最简单的形式是, 绑定到输入参数上的用逗号分隔的一列变量的名字。\nsub order-beer($type, $pints) { say ($pints == 1 ?? \u0026#39;A pint\u0026#39; !! \u0026#34;$pintspints\u0026#34;) ~ \u0026#34;of $type, please.\u0026#34; } order-beer(\u0026#39;Hobgoblin\u0026#39;, 1); # A pint of Hobgoblin, please. order-beer(\u0026#39;Zlatý Bažant\u0026#39;, 3); # 3 pints of Zlatý Bažant, please. 这里使用的关系绑定而非赋值就是签名。默认地, 在 Raku 中, 子例程中引用到传入参数的签名的变量是只读的。这意味着你不能从子例程内部修改它们。 如果只读绑定太受限制了, 你可以将 is rw (rw 是 read/write 的缩写) 特性应用到参数上以降低这种限制。这个特性说明参数是可读可写的, 这允许你从子例程内部修改参数。使用的时候必须小心, 因为它会修改传入的原始对象。如果你试图传入一个字面值, 一个常量, 或其它类型的不可变对象到一个有 is rw 特性的参数中, 绑定会在调用时失败并抛出异常:\nsub make-it-more-so($it is rw) { $it ~= substr($it, $it.chars - 1) x 5; } my $happy = \u0026#34;yay!\u0026#34;; make-it-more-so($happy); say $happy; # yay!!!!!! # 原始传入对象被修改了 make-it-more-so(\u0026#34;uh-oh\u0026#34;); # 失败, 不能修改一个常量 如果你想将参数的本地副本用在子例程内部而不改变调用者的变量, 使用 is copy 特性：\nsub say-it-one-higher($it is copy) { $it++; say $it; } my $unanswer = 41; say-it-one-higher($unanswer); # 42 say-it-one-higher(41); # 42 say $unanswer; # 41 在诸如 C/C++ 和 Scheme 等其它类型的编程语言中,这种广为人知的求值策略就是按值传递。当使用 is copy 特性时, 只有本地副本被赋值。其它任何传递给子例程的参数在调用者的作用域内保持不变。（一个不可变对象是当这个对象被创建后, 它的状态不会改变, 作为比较, 一个可变对象的状态在创建后是会被改变的）\n传递数组、散列和代码 一个变量的魔符表明它的本意用途。在签名中, 变量的魔符也起着限制传入的参数类型的作用。例如, @ 符号检查传入的对象行使位置角色（一个角色包含像数组和列表的类型）。如果传递的东西不能匹配这样的限制, 会引起调用失败：\nsub shout-them(@words) { for @words -\u0026gt; $w { print uc(\u0026#34;$w\u0026#34;); } } my @last_words = \u0026lt;do not want\u0026gt;; shout-them(@last_words); # DO NOT WANT shout-them(\u0026#39;help\u0026#39;); # 失败了, 字符串不是位置参数 类似地, % 符号表明调用者必须传递一个行使关系角色的对象；即允许通过 \u0026lt;...\u0026gt; 或 {...} 进行索引的东西。 \u0026amp; 符号要求调用者传递一个诸如匿名散列之类的行使能调用的角色的对象。在那种情况下, 你也可以不用 \u0026amp; 符号调用可调用的参数：\nsub do-it-lots(\u0026amp;it, $how-many-times) { for 1..$how-many-times { it(); } } do-it-lots(sub { say \u0026#34;Eating a stroopwafel\u0026#34; }, 10); # 此处是一个匿名子例程 标量使用 $ 符号, 并表明没有限制。什么都可以绑定在它上面, 即使它使用另外的符号绑定到一个对象上。\n插值、数组和散列 有时你想从数组中填充占位参数。你可以通过在数组前添加一个垂直竖条或管道字符 ( | ): eat(|@food), 而不是写作 eat(@food[0], @food[1], @food[2], ...) 等将它们吸进参数列表( | 像不像一个吸管, ^_^)。\n同样地, 你可以将散列插值进具名参数:\nsub order-shrimps($count, :$from) { say \u0026#34;I\u0026#39;d like $countpieces of shrimp from the $from, please\u0026#34;; } my %user-preferences = from =\u0026gt; \u0026#39;Northern Sea\u0026#39;; order-shrimps(3, |%user-preferences); # I\u0026#39;d like 3 pieces of shrimp from the Northern Sea, please 可选参数 为使参数可选, 要么给签名的参数赋值为默认值：\nsub order-steak($how = \u0026#39;medium\u0026#39;) { say \u0026#34;I\u0026#39;d like a steak, $how\u0026#34;; } order-steak(); order-steak(\u0026#39;well done\u0026#39;); 或者在参数名字的后面添加一个问号(?):\nsub order-burger($type, $side?) { say \u0026#34;I\u0026#39;d like a $typeburger\u0026#34; ~ ( defined($side) ?? \u0026#34;with a side of $side\u0026#34; !! \u0026#34;\u0026#34; ); } order-burger(\u0026#34;triple bacon\u0026#34;, \u0026#34;deep fried onion rings\u0026#34;); 如果没有参数被传递, 参数会被绑定成一个未定义的值。defined(...) 函数用来检查是否有值。\n强制参数 默认地, 位置参数是必不可少的。然而, 你可以通过在参数后面追加一个感叹号来显式地指定该参数是必须的：\nsub order-drink($size, $flavor!) { say \u0026#34;$size$flavor, coming right up!\u0026#34;; } order-drink(\u0026#39;Large\u0026#39;, \u0026#39;Mountain Dew\u0026#39;); # OK order-drink(\u0026#39;Small\u0026#39;); # Error 具名实参和形参  arguments 实参 parameters 形参  当一个子例程有很多参数时, 调用者很难记清传递参数的顺序。这种情况下, 通过名字传递参数往往更容易。这样, 参数出现的顺序就无关紧要了:\nsub order-beer($type, $pints) { say ($pints == 1 ?? \u0026#39;A pint\u0026#39; !! \u0026#34;$pintspints\u0026#34;) ~ \u0026#34;of $type, please.\u0026#34; } order-beer(type =\u0026gt; \u0026#39;Hobgoblin\u0026#39;, pints =\u0026gt; 1); # A pint of Hobgoblin, please. order-beer(pints =\u0026gt; 3, type =\u0026gt; \u0026#39;Zlatý Bažant\u0026#39;); # 3 pints of Zlatý Bažant, please. 你也可以指定参数只能按名字被传递（这意味着它不允许按位置传递）。这样的话, 在参数名字前加一个冒号：\nsub order-shrimps($count, :$from = \u0026#39;Northern Sea\u0026#39;) { say \u0026#34;I\u0026#39;d like $countpieces of shrimp from the $from, please\u0026#34;; } order-shrimps(6); # takes \u0026#39;Northern Sea\u0026#39; order-shrimps(4, from =\u0026gt; \u0026#39;Atlantic Ocean\u0026#39;); order-shrimps(22, \u0026#39;Mediterranean Sea\u0026#39;); # 不允许, :$from is named only 不像位置参数, 命名参数默认是可选的。在命名参数后面追加一个 ! 号使命名参数强制性存在。\nsub design-ice-cream-mixture($base = \u0026#39;Vanilla\u0026#39;, :$name!) { say \u0026#34;Creating a new recipe named $name!\u0026#34; } design-ice-cream-mixture(name =\u0026gt; \u0026#39;Plain\u0026#39;); design-ice-cream-mixture(base =\u0026gt; \u0026#39;Strawberry chip\u0026#39;); # 错误,没有指定 $name 重命名参数\n因为按名字传递实参给形参是合理的, 形参的名字应该应该作为子例程公共 API 的一部分被考虑在内. 小心地挑选它们吧! 有时候, 使用一个名字暴露形参而使用另外一个名字绑定到变量会很方便:\nsub announce-time(:dinner($supper) = \u0026#39;8pm\u0026#39;) { say \u0026#34;We eat dinner at $supper\u0026#34;; } announce-time(dinner =\u0026gt; \u0026#39;9pm\u0026#39;); # We eat dinner at 9pm 参数可以有多个名字, 如果你的用户有些是英国人, 有些是美国人, 你可能这样写：\nsub paint-rectangle( :$x = 0, :$y = 0, :$width = 100, :$height = 50, :color(:colour($c))) { # print a piece of SVG that represents a rectangle say qq[\u0026lt;rect x=\u0026#34;$x\u0026#34; y=\u0026#34;$y\u0026#34; width=\u0026#34;$width\u0026#34; height=\u0026#34;$height\u0026#34; \u0026gt;] } # both calls work the same paint-rectangle :color\u0026lt;Blue\u0026gt;; paint-rectangle :colour\u0026lt;Blue\u0026gt;; # of course you can still fill the other options paint-rectangle :width(30), :height(10), :colour\u0026lt;Blue\u0026gt;; 可选的命名参数语法\n命名变量通常是成对的（键值对）。写一个 Pairs 有多种方式。各种方法的不同之处就是清晰性, 因为每种选择提供不同的引述机制。下面的三种调用是一样的意思：\nannounce-time(dinner =\u0026gt; \u0026#39;9pm\u0026#39;); announce-time(:dinner(\u0026#39;9pm\u0026#39;)); announce-time(:dinner\u0026lt;9pm\u0026gt;); 如果传递的是布尔值, 你可以省略键值对的键值：\ntoggle-blender( :enabled); # enables the blender 开启果汁机 toggle-blender(:!enabled); # disables the blender 关闭果汁机 形如 :name 但不带值的命名参数有一个隐式的布尔真值 Bool::True。它的对立形式是 :!name , 其值是隐式的布尔假值 Bool::false。如果你使用变量创建了一个 Pair, 你可以将变量名作为 Pair 的键复用.\nmy $dinner = \u0026#39;9pm\u0026#39;; announce-dinner :$dinner; # same as dinner =\u0026gt; $dinner;  Pair forms and their meanings. Shorthand Long form Description :allowed allowed =\u0026gt; Bool::True Boolean flag :!allowed allowed =\u0026gt; Bool::False Boolean flag :bev\u0026lt;tea coffee\u0026gt; bev =\u0026gt; ('tea', 'coffee') List :times[1, 3] times =\u0026gt; [1, 3] Array :opts{ a =\u0026gt; 2 } opts =\u0026gt; { a =\u0026gt; 2 } Hash :$var var =\u0026gt; $var Scalar variable :@var var =\u0026gt; @var Array variable :%var var =\u0026gt; %var Hash variable :\u0026amp;var vaf =\u0026gt; \u0026amp;var Callable/ Subroutine variable  你可以使用在任何可以使用 Pair 对象的上下文使用表中的任意一种形式. 例如, 生成散列:\n# TODO: better example my $black = 12; my %color-popularities = :$black, :blue(8), red =\u0026gt; 18, :white\u0026lt;0\u0026gt;; # 与此相同： # my %color-popularities = # black =\u0026gt; 12, # blue =\u0026gt; 8, # red =\u0026gt; 18, # white =\u0026gt; 0; 最后, 通过位置而非名字传递一个已存在的 Pair 对象到子例程中, 要么把它放在圆括号中 ( 就像 (:$thing) ), 或者使用 =\u0026gt; 操作符引起左侧的字符串: \u0026quot;thing\u0026quot; =\u0026gt; $thing。\n参数的顺序\n当位置参数和命名参数都出现在签名中时, 所有的位置参数都要出现在命名参数之前：\nsub mix(@ingredients, :$name) { ... } # OK sub notmix(:$name, @ingredients) { ... } # Error 必须的位置参数要在可选的位置参数之前。然而, 命名参数没有这种限制。\nsub copy-machine($amount, $size = \u0026#39;A4\u0026#39;, :$color!, :$quality) { ... } # OK sub fax-machine($amount = 1, $number) { ... } # Error Slurpy 参数 有时候, 你会希望让子例程接受任何数量的参数, 并且将所有这些参数收集到一个数组中。为了达到这个目的, 给签名添加一个数组参数, 就是在数组前添加一个 * 号前缀：\nsub shout-them(*@words) { for @words -\u0026gt; $w { print uc(\u0026#34;$w\u0026#34;); } } # 现在你可以传递项 shout-them(\u0026#39;go\u0026#39;); # GO shout-them(\u0026#39;go\u0026#39;, \u0026#39;home\u0026#39;); # GO HOME 除了集合所有的值之外, slurpy 参数会展平任何它接收到的数组, 最后你只会得到一个展平的列表, 因此：\nmy @words = (\u0026#39;go\u0026#39;, \u0026#39;home\u0026#39;); shout-them(@words); 会导致 *@words 参数有两个字符串元素, 而非只有单个数组元素。\n你可以选择将某些参数捕获到位置参数中, 并让其它参数被吸进数组参数里。这种情况下, slupy 应该放到最后。相似地, *%hash slurps 所有剩下的未绑定的命名参数到散列 %hash 中。Slurpy 数组和散列允许你传递所有的位置参数和命名参数到另一个子例程中。\nsub debug-wrapper(\u0026amp;code, *@positional, *%named) { warn \u0026#34;Calling \u0026#39;\u0026amp;code.name()\u0026#39; with arguments \u0026#34; ~ \u0026#34;@positional.perl(), %named.perl()\\n\u0026#34;; code(|@positional, |%named); warn \u0026#34;... back from \u0026#39;\u0026amp;code.name()\u0026#39;\\n\u0026#34;; } debug-wrapper(\u0026amp;order-shrimps, 4, from =\u0026gt; \u0026#39;Atlantic Ocean\u0026#39;); 返回值 子例程也能返回值。之前本章中的 ASCII 艺术舞蹈例子会更简单当每个子例程返回一个新字符串：\nmy %moves = hands-over-head =\u0026gt; sub { return \u0026#39;/o\\ \u0026#39; }, bird-arms =\u0026gt; sub { return \u0026#39;|/o\\| \u0026#39; }, left =\u0026gt; sub { return \u0026#39;\u0026gt;o \u0026#39; }, right =\u0026gt; sub { return \u0026#39;o\u0026lt; \u0026#39; }, arms-up =\u0026gt; sub { return \u0026#39;\\o/ \u0026#39; }; my @awesome-dance = \u0026lt;arms-up bird-arms right hands-over-head\u0026gt;; for @awesome-dance -\u0026gt; $move { print %moves{$move}.(); } print \u0026#34;\\n\u0026#34;; 子例程也能返回多个值（译者注：那不就是返回一个列表嘛）：\nsub menu { if rand \u0026lt; 0.5 { return (\u0026#39;fish\u0026#39;, \u0026#39;white wine\u0026#39;) } else { return (\u0026#39;steak\u0026#39;, \u0026#39;red wine\u0026#39;); } } my ($food, $beverage) = menu(); 如果你把 return 语句排除在外, 则在子例程内部运行的最后一个语句产生的值被返回。这意味着前一个例子可以简化为：\nsub menu { if rand \u0026lt; 0.5 { \u0026#39;fish\u0026#39;, \u0026#39;white wine\u0026#39; } else { \u0026#39;steak\u0026#39;, \u0026#39;red wine\u0026#39;; } } my ($food, $beverage) = menu(); 记得：当子例程中的控制流极其复杂时, 添加一个显式的 return 会让代码更清晰, 所以 return 还是加上的好。 return 另外的副作用就是执行后立即退出子例程：\nsub create-world(*%characteristics) { my $world = World.new(%characteristics); return $world if %characteristics\u0026lt;temporary\u0026gt;; save-world($world); } \u0026hellip;并且你最好别放错你的新单词 $word 如果它是临时的。因为这是你要获取的仅有的一个。\n返回值的类型 像其它现代语言一样, Raku 允许你显式地指定子例程返回值的类型。这允许你限制从子例程中返回的值的类型。使用 returns 特性可以做到这样：\nsub double-up($i) returns Int { return $i * 2; } my Int $ultimate-answer = double-up(21); # 42 当然, 使用这个 returns 特性是可选的\nWorking With Types 很多子例程不能完整意义上使用任意参数工作, 但是要求参数支持确定的方法或有其它属性。这种情况下, 限制参数类型就有意义了, 诸如传递不正确值作为参数, 当调用子例程时, 这会引起 Raku 发出错误, 或者甚至在编译时, 如果编译器足够聪明来捕捉错误。\n基本类型 最简单的限制子例程接收可能的值的方法是在参数前写上类型名。例如, 一个子例程对其参数执行数值计算, 这要求它的参数类型是 Numeric：\nsub mean(Numeric $a, Numeric $b) { return ($a + $b) / 2; } say mean 2.5, 1.5; say mean \u0026#39;some\u0026#39;, \u0026#39;strings\u0026#39;; 产生输出：\nNominal type check failed for parameter \u0026#39;$a\u0026#39;; expected Numeric but got Str instead nominal 类型是一个人实际类型的名字, 这里是 Numeric。 如果多个参数有类型限制, 每个参数必须填充它绑定的参数限制的类型\n添加限制 有时, 类型的名字不足以描述参数的要求。这种情况下, 你可能使用 where 代码块添加一个额外的限制：\nsub circle-radius-from-area(Real $area where { $area \u0026gt;= 0 }) { ($area / pi).sqrt } say circle-radius-from-area(3); # OK say circle-radius-from-area(-3); # Error 因为这种计算只对非负面积值有意义, 该子例程的参数包含了一个限制, 对于非负值它会返回真。如果这个限制返回一个假的值, 类型检查会失败, 当有些东西调用该子例程时。\nwhere 之后的代码块是可选的。Raku 通过通过智能匹配 where 后面的参数来执行检查。 例如, 它可能接受在某一确定范围中的参数：\nsub set-volume(Numeric $volume where 0..11) { say \u0026#34;Turning it up to $volume\u0026#34;; } 或者你可以将参数限制为散列的键：\nmy %in-stock = \u0026#39;Staropramen\u0026#39; =\u0026gt; 8, \u0026#39;Mori\u0026#39; =\u0026gt; 5, \u0026#39;La Trappe\u0026#39; =\u0026gt; 9; sub order-beer(Str $name where %in-stock) { say \u0026#34;Here\u0026#39;s your $name\u0026#34;; %in-stock{$name}--; if %in-stock{$name} == 0 { say \u0026#34;OH NO! That was the last $name, folks! :\u0026#39;(\u0026#34;; %in-stock.delete($name); } } 抽象参数和具体参数 下面检测变量是否定义。在 Raku 中这样写:\nsub foo(Int:D $arg) { # Do something } 留意附加在参数类型后面的 :D 笑脸。这个动词表明给定的参数必须被绑定到一个具体的对象上。如果不是的话, 会抛出一个运行时异常。这就是为什么它那么高兴！作为对比, 动词 :U 用于表明该参数需要一个未定义的或抽象的对象。此外, 动词 :_ 允许定义或未定义的值。实际上, 使用 :_ 有点多余。\n最后, 动词 :T 能用于表明参数只能是类型对象, 例如\nsub say-foobar(Int:T $arg) { say \u0026#39;FOOBAR!\u0026#39;; } say-foobar(Int); # FOOBAR! ","permalink":"https://ohmyweekly.github.io/notes/2015-03-14-using-raku/","tags":["rakulang"],"title":"Using Raku"},{"categories":["rakulang"],"contents":"让 Raku 与 Perl 相当不同的一点是，Raku 避免了魔法。有几个地方人们可以说，\u0026ldquo;它神奇地发生了\u0026rdquo;。但仔细看一下，通常会发现行为背后有相当好解释的机制。这就像看魔术师的把戏：我们总是知道有解释，而且它们肯定是符合逻辑的。\n因此，我有个小把戏给你。看一下代码，告诉我：你在这里看到了多少个角色？\nrole R[::T] { has T $.a; } class C does R[Int] { } 直观的答案当然是 1，这也是事实。但这里的部分技巧是术语的替换：在使用\u0026quot;角色\u0026quot;一词的地方，更准确的术语应该是\u0026quot;角色类型对象\u0026quot;。现在，试着猜出正确答案。而且，要确定的是，它不止一个。\nRaku 魔术是如何不神奇的 Raku 的最大优点之一，随着时间的推移，我越来越学会重视，就是它做任何事情都要保持逻辑性。有时这并不意味着要有直觉。有些行为一开始甚至可能使初学者感到困惑。但是，在解释时，逻辑通常是相当有说服力的。Raku 提供的一套广泛的内省工具，通常对理解它有很大帮助。在这篇文章中，我将尝试以\u0026quot;魔术师\u0026quot;的方式演示如何使用其中的一些工具来将一只兔子变成许多只。\n我还将在很大程度上依赖于 Rakudo 对 Raku 的实现，它是基于 NQP 的，这使得在某些情况下相当容易看到 Raku 语法的幕后。顺便说一下，这也是 Raku 中的魔法量处于可忽略不计的水平的另一个原因。你们当中有多少人，我的读者，曾经研究过 Perl 或任何你最喜欢的语言的来源？如果我为自己回答，那么就是一个词：从未。尽管 C 语言是我多年来的首选语言。但现在我会坚持要求你在你的主目录下的某个地方做 git clone https://github.com/rakudo/rakudo.git，所有其他项目都放在那里。然后，只要你遇到问题，答案很可能就在 Rakudo 项目的 src/Perl6/Metamodel 目录下的一个文件里。\n四位一体 我不得不谷歌一下这个词。从第一部《黑客帝国》电影开始，\u0026ldquo;三位一体\u0026quot;对我来说就很熟悉了，但对这一行中的其他词却不熟悉。是的，这个词就是这个棘手问题的答案。Raku 角色是四位一体。这篇文章将一步一步地告诉你为什么。\n在这一点上，我想提醒的是，反省和乐库元模型的一般知识将是非常有益的。一些信息可以在本周期的前几篇文章中找到，一些可以在Raku文档中找到。\n第1步: 多重性 让我们从最简单的自省开始。\n⇒ raku -e \u0026#39;role R[::T] { }; say R.WHAT\u0026#39; (R) 不要和 \u0026lsquo;⇒\u0026rsquo; 字符混淆，这只是我最喜欢的命令行提示。\n注意，我们只用方括号来声明角色，而不是对它调用方法。还要注意的是，这个角色报告自己只是 R；同样，没有涉及方括号。\n接下来，你可能已经知道，在 Raku 中，同一个角色有可能有不同的变体。\n⇒ raku -e \u0026#39;role R[::T] { }; role R { }; say R.WHAT\u0026#39; (R) 我们有两个声明，但仍然只使用 R 来调用 WHAT。\n让我们换个角度，看看这个角色是如何实现的。\n⇒ raku -e \u0026#39;role R[::T] { }; say R.HOW.^name\u0026#39; Perl6::Metamodel::ParametricRoleGroupHOW 注意名字中的 Group 部分。新手可能会对这个词感到困惑，只要他们只使用一个角色的变体。但是当他们到了本节第二个例子的时候，事情就开始变得比较清楚了。让我再把它们变得更加混乱。\nrole R[::T] { method foo { 42 } } role R { } say R.^lookup(\u0026#39;foo\u0026#39;); 你希望这段代码能输出什么？根据文档，在一个类上这样做会得到相当可预测的结果。\nclass Foo { method foo { } }; say Foo.^lookup(\u0026#34;foo\u0026#34;); # foo 现在，忘记这个经验吧。因为对于上面的例子中的 R，我们会得到 (Mu)，意思是没有找到方法！而对于 R，我们会得到 (Mu)。\n在这一点上，我想退一步说。如果你读了 Raku 的文档或书，做了关于角色和参数化的部分，有一个细节可能会让你觉得相当熟悉。如果这也是我要指出的\u0026quot;东西\u0026rdquo;，那么你就不会错了：参数化是关于参数的；有了参数，就有了签名！\u0026quot;。现在这段代码一定是完全有意义的。\nrole R[Int:D $a, Str:D $b] { ... } 角色声明中被方括号括起来的部分是一个签名，这有另一层意思，我将在后面再谈。\n不幸的是，我在写这篇文章的时候有点超时，它应该在涉及到几个更基本的主题之后再完成。出于这个原因，我为下面的一点离题而道歉。\n多重分派 人们可以在 Raku 文档中找到这一部分。另一节阐述了其语法和功能。但我想简单地谈谈这个功能的内部实现。让我们从一个基本的声明开始。\nproto foo(|) {*} multi foo(Int:D $i) {} multi foo(Str:D $s) {} say \u0026amp;foo.raku; # proto sub foo (|) {*} 正如你所看到的，raku 方法只报告了 proto。另外，如果我们对 \u0026amp;foo 调用 is_dispatcher 方法，它将返回 True。好的，但是这两个 multi 在哪里，当我们调用 foo(\u0026quot;bar\u0026quot;) 时会发生什么？用两句话来说，Raku 首先会找到 proto 方法。如果它通过检查 is_dispatcher 的返回值识别出它是这样的，那么它就会通过调用 \u0026amp;foo.candidates 来获取已知的候选列表。\nsay \u0026amp;foo.candidates.map(*.raku).join(\u0026#34;\\n\u0026#34;); # multi sub foo (Int:D $i) { #`(Sub|140560053018928) ... } # multi sub foo (Str:D $s) { #`(Sub|140560053019072) ... } 然后，它试图将提供的参数与每个候选者的签名绑定。如果绑定成功，则调用该候选程序（如果没有找到，则抛出一个异常）。\n显然，在现实生活中，事情要复杂得多，但我们还不需要知道这些\u0026hellip;\u0026hellip;\n回到多重角色的问题上 有时我对不能在文章的纯文本中递归到一个子主题感到奇怪。就把这一节的标题看成是上一节的 return 语句\u0026hellip;\u0026hellip;啊，算了！不说了。\n好吧，我的观点是什么，就是要讲述多重调度的故事？当我们看到 R.HOW 在类名中报告了一个 Group，就可以和多重调度实现中的 proto 相提并论了。事实上，我们调用 HOW 方法的类型对象 R 是一个伞状的实体，在其共同的名字下代表了角色的所有变体。而且，当我们把 R[Int] 应用于一个类时，实际发生的过程是一种多重分派，Raku 试图把方括号中的参数与角色候选者的签名相匹配。类似于我们如何列出 \u0026amp;foo 的候选者，我们也可以列出 R 的候选者。\n``raku say R.^candidates.map(*.^name).join(\u0026quot;, \u0026ldquo;); # R, R\n 唯一不同的是，这次我们使用了一个元方法 `.^candidates`。 在这一点上，还有一个谜团没有被揭开。还记得使用 `.^lookup` 的那个例子吗？为什么它不能找到这个方法？ `Perl6::Metamodel::ParametricRoleGroupHOW` 所支持的类型对象并不是一个我们可以实际使用的角色。它既没有方法也没有属性。然而，在某些情况下，我们可能希望它假装是一个成熟的角色。为了做到这一点，它选择了一个候选角色作为默认角色，然后在其上重新分配外部请求。当有一个没有签名的候选者时（如我们的 `role R {}`），它就成为隐式默认。否则，第一个声明的有签名的候选者就会成为默认的候选者。 回到我们的例子，`R.^lookup('foo')` 失败了，因为 `role R {}` 没有声明一个有这个名字的方法。 ### 第二步: 候选者 直奔主题，让我们对候选者本身进行自省。 ```raku say R.^candidates.map({ .HOW.^name }).join(\u0026quot;, \u0026quot;); 这看起来一定很熟悉，只是我们加入了.HOW的调用。下面是我们用它得到的东西。\nPerl6::Metamodel::ParametricRoleHOW, Perl6::Metamodel::ParametricRoleHOW 它看起来也很熟悉，除了\u0026hellip;\u0026hellip;是的，在类名称中没有 Group，我想欢迎我们的第二种角色! 实际上，我们已经知道了。如果我像这样挥舞着我的手，让我的观众分心，用\u0026hellip;\n哎呀，最后一句话应该是落在另一个窗口里的! 对于你，我的观众，我还有一行代码。\nrole R {}.HOW.^name.say; # Perl6::Metamodel::ParametricRoleHOW BTW，这是一个很好的例子，说明无处不在的 Raku 概念，即所有东西都是一个对象。甚至声明也是；而且，只是为了好玩。\n{ say \u0026#34;foo\u0026#34; }.^name.say; # Block 但是我分心了\u0026hellip;\n所以，这里真正重要的是，当我们声明一个角色时，Raku 为我们创建一个 Perl6::Metamodel::ParametricRoleHOW 类的实例。每个声明都有一个独特的类的实例支持，它负责持有角色类型对象的每个细节。例如，要想知道它是否可以被参数化，可以这样做。\nsub is_parameterized(Mu \\r --\u0026gt; Bool) { ? r.^signatured } say is_parameterized(role R[::T] {}); # True say is_parameterized(role R {}); # False 请注意，由于 signatured 是在 NQP 中实现的方法，它不知道高级类型，并返回0或1。有时情况会变得更糟糕。我上面提到的查找元方法实际上返回 nqp::null()，这是一种 VM 级的对象。它决不能出现在 Raku 上。因此语言把它变成了 Mu，这是最基本的 Raku 类。\n关于 Perl6::Metamodel::ParametricRoleHOW，在这一点上没有什么可说的。但我们稍后会回到它上面去。\n第三步: 不确定性 为了更接近我们的第三种角色，我们从下面这个片段开始。\nrole R1[::T Stringy, ::V] { method foo { \u0026#34;stringy\u0026#34; } } role R1[::T Numeric, ::V] { method foo { \u0026#34;numeric\u0026#34; } } my \\r = role R2[::T] does R1[Int, T] { } 让我们自省一下 R1。\n# We know there is only one role,  # hence .head for prettier output say r.^roles.map({ .^name ~ \u0026#34;of \u0026#34; ~ .HOW.^name }).head; # R1[Int,T] of Perl6::Metamodel::CurriedRoleHOW 输出显示了两个明显的变化。首先，角色名称现在报告了它的参数。第二，元对象现在属于 Perl6::Metamodel::CurriedRoleHOW 类。这是 Rakudo 在幕后做的另一种\u0026quot;魔法\u0026rdquo;，我将在本节中披露。\n在上面的例子中，R2 声明最引人注目的特点是什么？事实上，在它消耗 R1 的地方，我们只知道第一个参数，而第二个参数仍然是一个通用参数。为了表示这种情况，我们对角色的了解是不完整的，Rakudo 使用了柯里化的角色。\n从起源的角度来看，柯里化角色与前两种角色的关键区别在于，没有办法声明一个角色。柯里化只能是一个组的参数化的结果。而且，实际上，我很清楚，正式的组在 Raku 语法中并没有表示。但是只要它作为第一个角色声明的结果出现，我们就可以说它是由它产生的。而柯里化则完全由参数化产生。\n也许有些令人惊讶，但在所有参数都为编译器所熟知的情况下，也可以发现柯里化的存在。\nsay R2[Str].HOW.^name; # Perl6::Metamodel::CurriedRoleHOW 部分原因是，当我们使用这样的角色时，我们所需要的也许是一些内省、类型检查或任何其他不需要具体对象的操作。比如说。\nsub foo(R1[Int, Str] $a) {...} 我们在这里需要的是 foo 参数，以通过对 R1[Int, Str] 的类型检查。因为柯里化会帮我们完成这个工作，所以 Rakudo 在这里使用它。\nsay \u0026amp;foo.signature.params[0].type.HOW.^name; # Perl6::Metamodel::CurriedRoleHOW 这是因为:\nsay R2[Str] ~~ R1[Int, Str]; # True say R2[Int] ~~ R1[Int, Str]; # False 但还有一个主要原因。它将在下一节中披露。\n第4步: 具体性 任何角色的命运都是被一个类所吞噬。(BTW，这里的双关语也不例外。)现在是时候考虑这最后阶段了。\nrole R1[::T] { } role R2[::T] { } role R3 { } class C does R1[Int] does R2[Str] does R3 { } 通过反省类，我们会遇到所有的老朋友。\nsay C.^roles .map({ .^name ~ \u0026#34;of \u0026#34; ~ .HOW.^name }) .join(\u0026#34;\\n\u0026#34;); # R3 of Perl6::Metamodel::ParametricRoleGroupHOW # R2[Str] of Perl6::Metamodel::CurriedRoleHOW # R1[Int] of Perl6::Metamodel::CurriedRoleHOW 有趣的是，我们在这里发现了不同种类的角色的混合。其原因是浮在上面的：与 R3 相反，另外两个角色是参数化的。\n但由于我喜欢迷惑听众，所以我要告诉你：这些其实不是这个类所建立的角色！这是另一个操作。\n当然，这是另一种操作。完整的短语必须是使用这个。\u0026ldquo;不是直接使用的角色\u0026rdquo;。\n当我们尝试另一种方法时，情况就会大不相同。\nsay C.^mro(:roles) .map({ .^name ~ \u0026#34;of \u0026#34; ~ .HOW.^name }) .join(\u0026#34;\\n\u0026#34;); # C of Perl6::Metamodel::ClassHOW # R3 of Perl6::Metamodel::ConcreteRoleHOW # R2 of Perl6::Metamodel::ConcreteRoleHOW # R1 of Perl6::Metamodel::ConcreteRoleHOW .^roles 和 .^mro 的区别在于，前者为我们提供了用于声明类的内容；而后者则为我们提供了实际构建的内容。\n正如 HOW 类的名字所暗示的，我们现在处理的是角色的具体表现。换句话说，这是一种所有细节都知道的角色，它们是为这个特定的类专门设计的。这里强调的是目的：这个过程被称为特化；而 specialize 是实现它的元模型方法的名称。\n我还想提醒你一下上一节的最后一句话。为什么每当人们使用 R[Int] 或类似形式的角色参数化时，他们都要处理一个柯里化的角色，原因是完全的特化需要角色被消耗的类。稍后我将说明原因。\n我们现在可以退一步，概述一下角色的生命周期。\n 一个 Perl6::Metamodel::ParametricRoleGroupHOW 被创建。 一个 Perl6::Metamodel::ParametricRoleHOW 被创建并添加到该组。 一个类被声明并 does 这个角色。编译器尝试对角色进行参数化，如果需要参数化，Perl6::Metamodel::CurriedRoleHOW 就会被创建；否则就会使用原来的 Perl6::Metamodel::ParametricRoleHOW。 参数化的结果被添加到类的角色列表中。 当类被组成时，所有在上一步中添加的角色都被赋予了各自的参数和类的类型对象。在这一点上，我们得到了由 Perl6::Metamodel::ConcreteRoleHOW 支持的角色类型对象，或者换句话说，角色具体化。 这些具体化被添加到类中。 具体化是通过将其属性和方法迁移到类的类型对象中来应用的。  值得注意的是，具体化被保留为独立的实体，与它们所产生的角色分开。这就是我们在上面通过使用 .^roles 和 .^mro 进行内省观察到的。它们也可以使用 .^concretizations 元模型方法来访问。\nsay C.^concretizations .map({ .^name ~ \u0026#34;of \u0026#34; ~ .HOW.^name }) .join(\u0026#34;\\n\u0026#34;); # R3 of Perl6::Metamodel::ConcreteRoleHOW # R2 of Perl6::Metamodel::ConcreteRoleHOW # R1 of Perl6::Metamodel::ConcreteRoleHOW 在这一点上，有两个相当大的主题仍然故意不清楚：一个角色候选人是如何被选择的？ 以及特化是做什么的？第一个问题我也许可以或多或少地完整地介绍一下。第二个问题对本文来说太复杂了，但有几个关键点绝对值得一提。\n第1a步: 选择 震惊一个无辜的读者在媒体中非常流行。虽然我勉强算是个记者，但只要我把这篇文章称为文章\u0026ndash;我有什么资格打破规则？所以，请坐好，握紧你的大脑。\n我们开始了\u0026hellip;\u0026hellip;准备好了没有\u0026hellip;\u0026hellip;真相即将揭晓！\u0026quot;。\n角色是一种例程。\n很好，开始了。我说了！我一直想说的!\n说真的，正如经常发现的关于点击率的新闻一样，这并不完全是真的，但有一点是真的。我想让你考虑一个例子。\nrole R { say \u0026#34;inside the role\u0026#34;; } module Foo { say \u0026#34;inside Foo\u0026#34;; } class C { say \u0026#34;inside the class\u0026#34;; } # inside Foo # inside the class 我们只看到两行输出，让我们了解到类的声明与模块的行为是一样的。但不是角色。让我们在这个例子中再加一行。\nR.^candidates[0].^body_block.(C); # inside the role 为什么是这样，为什么我把 C 作为一个参数传递，我将在下面关于特化的部分尝试回答。\n现在我建议对主体块进行内省，但首先要在上述片段中增加一个角色的变体。\nmy \\r = role R[::T, ::V Numeric] { } say r.^body_block.raku; # multi sub (::$?CLASS ::::?CLASS Mu $, ::T Mu $, ::V Numeric $) { #`(Sub|94052949943024) ... } 现在有印象了吗？sub 前面的 multi 这个词说明了一切，我现在的工作已经减少到所需的最小的措辞。\n当编译器建立一个角色组时，它也创建了一个多调度例程。在内部，它被称为选择器。在每一个新添加的参数化角色中，它的主体块（实际上是一个 multi sub）都会被抽取出来，并作为一个多重调度候选程序添加到选择器中。现在，当人们在他们的代码中写下像 R[Int, Str] 这样的东西时，编译器会做一个类似于选择多重调度例程候选者的过程。基于进程提供的主体块候选，它选择了该块所属的角色。\n所以，现在当我们提到角色签名时，一定会有更多的意义。因为它是一个签名，作为一个事实。如果我把一个角色声明 role R[::T, ::V] {} 以某种更适合人类程序员阅读的方式重新表述，它可能看起来像。\n 声明一个候选角色 R，其主体为块sub (::T, ::V) {...}。\n 很好，我们现在已经解决了这个问题。但是，我细心的读者，是不是有什么事情困扰着你？上面的\u0026quot;类似\u0026quot;一词是不是意味着某种\u0026hellip;\u0026hellip;呃\u0026hellip;\u0026hellip;惊喜？好吧，不幸的是，候选的选择并不遵循完整的多重调度协议，因为它缺乏对命名参数的支持。这是由于类型参数化的低级实现的限制。这意味着下面的两个声明被认为是相同的。\nrole R[Int, Bool :$foo] {...} role R[Int, Str:D :$bar] {...} 希望当新的分派机制到达 Raku 时，情况会有所改变，但我在此不做任何承诺。\n与此同时，你仍然可以使用这些名字，只是不要依靠它们来唯一地识别你的角色候选人。\n一个黑魔法塞恩斯 这其实与选择候选者无关，但我不能忍受不给你看一些棘手的东西。此外，在许多小说和童话故事中，黑魔法是一种可以让你实现目标的东西，但有一个附加的价格标签。有时这个标签是相当血腥的，但这不是我的情况。实际上，我的目标和代价是一样的：我想用不同的东西来吸引你。\n这就是要施的法术。\nuse nqp; my \\r = role R[::T, ::V Numeric] { } class C { } my \\tenv = r.^body_block().(C, Str, Int); my \\ctx = nqp::atpos(tenv, 1); my \\iter = nqp::iterator(ctx); while iter { my \\elem = nqp::shift(iter); say nqp::iterkey_s(elem), \u0026#34;=\u0026gt; \u0026#34;, nqp::iterval(elem); } 只要主体块是一个例程，显然我们可以自己调用它。为了理解剩下的几行和所有使用的 nqp:: ops，人们需要参考 NQP ops 文档。\n总之，\u0026ldquo;咒语\u0026quot;产生的输出可能看起来像这样。\n::?CLASS =\u0026gt; (C) $?ROLE =\u0026gt; (R) T =\u0026gt; (Str) $?CONCRETIZATION =\u0026gt; (Mu) $?PACKAGE =\u0026gt; (R) ::?PACKAGE =\u0026gt; (R) V =\u0026gt; (Int) ::?ROLE =\u0026gt; (R) $?CLASS =\u0026gt; (C) $_ =\u0026gt; (Mu) 用两个字来说，角色体块返回一个包含两个元素的数组。第二个元素是符号名称到其具体值的映射。也就是说，在 =\u0026gt; 箭头左侧的键中，你可以很容易地从角色签名中发现我们的 T 和 V 类型捕获；以及编译器常量，如 ::?CLASS 和其他。\n总的来说，代码返回的东西被称为内部类型环境，并被用于另一种广泛采用的机制，即泛型实例化。但这个话题肯定远远超出了本文的目的。这里确实值得一提的是，所有包含在环境中的符号实际上都是角色体词法。例如，如果我们把我的 FOO = 42 加入到主体中，那么上面的输出就会有以下一行加入到其中。\nFOO =\u0026gt; 42 另外，看着这些符号，你现在甚至可以更好地理解为什么角色的特化需要一个类来消耗它。你下次在做类似的事情时可能会考虑到这一点。\nmethod foo(::?CLASS:D: |) {...} 最后我想指出的一点是 $?CONCRETIZATION 符号，它还没有被记录下来。它只在角色主体和角色方法中可用，并且当它可用时被绑定到角色的具体化中。这个符号主要是用于自省的目的。\n步骤4a: 特化 所以，我们有一个候选者。我们知道具体的参数。我们知道消耗它的类。因此，我们确实知道了一切，可以进行特化，并得到具体化，最终将这个角色纳入消费它的类中。\n正如我在上面已经提到的，特化是一个相当复杂的过程。在 Rakudo 元模型的实现中，它分布在几个源文件中，并涉及到一些其他的内部机制，如通用实例化，我在上面也暗示过。我最好不要深究其中的细节，而是专注于主要的阶段。那些真正好奇的人可以从 Rakudo 编译器源文件 src/Perl6/Metamodel/ParametricRoleHOW.nqp 中的方法特化开始。\n特化一个新的角色，首先要创建一个 Perl6::Metamodel::ConcreteRoleHOW 的新实例和相应的具体角色类型对象。然后调用 body block 来获得一个类型环境结构。我将重点介绍一下这个。像往常一样，我们先举一个例子。\nrole R { say \u0026#34;inside the role, class is \u0026#34;, ::?CLASS.^name; say \u0026#34;class is composed? \u0026#34;, ::?CLASS.^is_composed ?? \u0026#34;yes\u0026#34; !! \u0026#34;no\u0026#34;; } class C1 does R { } class C2 does R { } # inside the role, class is C1 # class is composed? no # inside the role, class is C2 # class is composed? no 我们在这里观察到的是，角色主体已经被调用了两次，它知道它所应用的类，而且这个类还没有被组成（我在另一篇文章中介绍了一些关于类生命周期的信息）。另外，正如我已经提到的，具体化在这一点上存在。\nrole R { say $?CONCRETIZATION.^name; # R } 但它还是空的。\nsay $?CONCRETIZATION.^attributes.elems; # 0 say $?CONCRETIZATION.^methods.elems; # 0 而且，很明显，没有组成。\nsay $?CONCRETIZATION.^is_composed ?? \u0026#34;yes\u0026#34; !! \u0026#34;no\u0026#34;; # no 所有这些使得角色体成为一个好地方，可以在角色被实际消费时做需要做的事情。\n现在，有了所有必要的信息，元模型通过实例化原始参数化或柯里化角色的属性和方法，并将它们安装到新创建的具体化中来最终完成专业化。例如这个片段的例子。\nrole R[::T] { has T $.attr } class C R[Str] { } 如果我们转储原始角色和具体化的属性，我们可能会看到类似于下面的输出。\nrole attr: (Attribute|94613946040184 T $!attr) concretization attr: (Attribute|94613946043184 Str $!attr) 当用属性和方法完成时，任何被消耗的角色都会被实例化和具体化。例如，对于这个声明。\nrole R1[::T, ::V] does R2[::T] { ... } R2 的具体化将被添加到 R1 的具体化中，然后 R2 的具体化将被添加到 R1 的具体化中。\n最后，如果有任何父类添加到角色中，它们也会被实例化和添加。\n当所有上述准备工作完成后，我们的具体化就被组成了。它现在已经准备好被添加到它的消费类中。\n故事就这样结束了。\n偿还债务 知道很久以前的承诺最终得到了兑现，真的让人松了一口气。不幸的是，为了涵盖这个主题，我已经跳过了其他一些更基本的主题。例如，让读者更好地了解多重调度、类型对象的组成，以及 Rakudo、NQP 和后端虚拟机是如何相互作用的，这对读者是有益的。如果我写了足够多的文章，并考虑把这些材料编成一本书，那么由这段文字组成的章节将被放在离书的开头更远的地方。\n不管怎么说，我已经尽了最大的努力，远离那些还未被提及的概念，希望你在这里找到有用的信息。\n原文链接: https://vrurg.github.io/arfb-publication/07-roles-or-when-one-is-many/\n","permalink":"https://ohmyweekly.github.io/notes/2021-06-22-roles-or-when-one-is-many/","tags":["Raku","Role"],"title":"Roles or When One Is Many"},{"categories":["rakulang"],"contents":"Self-describing code Junctions  Distributive  %hash{any(@keys)} 等价于:\nany(%hash{@keys})  Boolean     类型 操作符 True if \u0026hellip;     any | 至少一个值为真   all \u0026amp; 所有值都为真   one ^ 只有一个值为真   none  值都不为真    Junctions 通常出现在布尔上下文中。例如, 在下面的例子中, $value 和几个值进行相等性比较。很容易写出这样的代码:\nif $value == 1 || $value == 2 || $value == 5 使用 any Junction 会简洁不少:\nif $value == any(1, 2, 5) if $value == (1,2,4).any 惯用法是使用 | 操作符号来进行多值比较:\nif $value == 1|2|5 找出数组中满足条件的第一个元素, 我们首先想到的可能是, 使用 for 循环迭代数组, 找出满足条件的元素就立即退出循环:\nmy $result = False; for @values -\u0026gt; $value { if $value \u0026gt; 42 { $result = True; last; } } if $result { ... } 改用 Junction 后等价于:\nif any(@values) \u0026gt; 42 { ... } 还可以在 Junction 上调用方法或运算符:\nif one(@values).is-prime { ... } if all(@values) %% 3 { ... } Named arguments(命名参数) Colonpair(冒号对儿) 通常用于命名参数中:\n:foo(42) foo =\u0026gt; 42 sub bar(:$foo) { say $foo; } bar(:foo(42)); 有几种特殊形式的冒号对儿:\n:foo :foo(True) :!foo :foo(False) :foo\u0026lt;bar\u0026gt; :foo(\u0026#34;bar\u0026#34;) :foo 与 foo =\u0026gt; True 相同, :!foo 等价于 foo =\u0026gt; False。:foo\u0026lt;bar\u0026gt; 使用了一组尖括号引起了值, 值在尖括号中不进行插值。\n来看一个命名参数例子, 下面的代码遍历 @dirs 中的目录, 找出后缀名为 txt 且不为空的文件:\nfor find(@dirs, :file, :ext\u0026lt;txt\u0026gt;, :!empty) -\u0026gt; $file { ... } 冒号对儿的值可以是变量, 但是在圆括号中再写一遍变量名就显得啰嗦:\nfoo(:bar($bar), :baz($baz), :quz($quz)) 因此, 冒号对儿提供了一种简写形式, 如果冒号后面紧跟着 $、@、% 和 \u0026amp; 等符号, 那么冒号对儿的值就是 $sth、@sth、%sth 和 \u0026amp;sth:\nfoo(:$bar, :$baz, :$quz) 这种简写形式消除了命名参数的重复。\nPointy blocks(尖号块儿) All blocks are Callable, 即所有的块儿都是可调用的。\n for blocks  -\u0026gt; $elem { ...} 就是尖号块儿:\nfor @array -\u0026gt; $elem { ... } for 循环依次把 @array 中的每个元素赋值给尖号块儿中的 $elem 变量, 然后执行尖号块儿的主体。\n Ordering  如果 foo 例程有返回值且不为假, 则赋值给 $value, 然后执行块儿的主体:\nif my $value = foo() { ... } 但是上面的语句在 Raku 中是不合法的, 要使用尖号块儿的方式:\nif foo() -\u0026gt; $value { ... } Signatures(签名) for 循环可以一次迭代两个(或多个)元素。尖号块儿相当于匿名函数, 其中的 $first, $second 就是尖号块儿的签名。\nfor @array -\u0026gt; $first, $second { ... } 下面的智能匹配中, 变量 $1 和 $2 有些多余:\nif / (\\S+)\\s+(.*)/ { my $name = $1; my $value = $2; ... }通过尖号块儿, 把匹配结果直接赋值给 $name 和 $value, 节省了两个变量名:\nif / (\\S+)\\s+(.*)/ -\u0026gt; ($name, $value) { ... } for 尖号块儿和 if 尖号块儿的结构类似, 语法上非常整齐:\nfor expression() -\u0026gt; $value { ... } if expression() -\u0026gt; $value { ... } Whatever code 如果 grep 的过滤条件中有多个变量, 那么使用尖号块儿这种匿名函数比较合适:\n@numbers.grep(-\u0026gt; $n { $n \u0026gt; 2 }); 如果过滤条件中只有一个变量, 那么形式更短的 Whatever code 更符合惯用法:\n@numbers.grep(* \u0026gt; 2); Meta-operators  Reduction Zip Corss Hyper  Reduction meta operators\n fold/reduce an infix operator Respects associativity  reduce 运算符可以用于求和:\nmy $sum = reduce { $^a + $^b }, @list; my $sum = [+] @list [+] # sum [*] # product [~] # join [===] # all equal [\u0026lt;\u0026gt;] # ascending order [||] # first true value, if any  Zip 元运算符用于连接列表:\n(1, 2, 3) Z+ (30, 20 ,10) # (21, 22, 13) -\u0026gt; ($a, $b) 解构 Zip 后的元素:\nfor @a Z @b -\u0026gt; ($a, $b) { ... } Z=\u0026gt; 运算符通常用于从两个列表中制作哈希:\n%hash = @keys Z=\u0026gt; @values Zip 元运算符可以写成链式的:\n@a Z @b Z @c [Z] @list-of-lists Cross 是交叉运算符。使用两层 for 循环也可以实现交叉运算符的功能, 就是代码稍长:\ngather for 3, 9 -\u0026gt; $i { for 2, 10 -\u0026gt; $j { take $i + $j } } 而使用交叉运算符, 一行代码搞定:\n3, 9 X+ 2, 10 添加前缀也很简单:\n\u0026#34;prefix-\u0026#34; X~ @list Hyper 运算符可以把任何运算符(中缀、前缀和后缀等)应用到列表上:\n@list».abs @list.map(*.abs) !«@list@list.map(!*)@list»[1] @list.map(*[1]) 欧几里得距离:\n@a Z- @b Squared(平方)\n(@a Z- @b)»² Summed(求和)\n[+] (@a Z- @b)»² Square root(求平方根)\nsqrt [+] (@a Z- @b)»² Smartmatch(智能匹配) “Is the value part of this set”\n@list.grep(* \u0026gt; 2) @list.grep({ $_ eq \u0026#34;foo\u0026#34; }) @list.grep(Int) @list.grep({ .isa(Innt) }) @list.grep(/foo .*bar/) @list.grep({ .match(/foo .*bar/) }) @list.grep(2..4) @list.grep({ 2 \u0026lt;= $_ \u0026lt;= 4 }) @list.grep(:is-prime) @list.grep({ .is-prime }) combine junctions(结合 Junctions):\n@list.grep(:is-prime \u0026amp; /22/) @list.grep({ .is-prime \u0026amp;\u0026amp; .matches(/22/) }) @list.grep(none(/22/)) @list.grep({ !.matches(/22/) }) 最后, 还是查找文件的例子:\nfind(@dirs, :ext(\u0026#39;rakumo\u0026#39;|\u0026#39;pm6\u0026#39;), :size(* \u0026gt; 1024), :depth(3..5), :contains(/raku/) ); 原文链接: https://www.youtube.com/watch?v=elalwvfmYgk\n","permalink":"https://ohmyweekly.github.io/notes/2021-06-17-raku-syntax-i-miss-in-other-languages/","tags":["Raku"],"title":"Raku Syntax I Miss in Other Languages"},{"categories":["Raku"],"contents":"Raku 的 Roast 仓库是一个测试套件, 我们可以从这个仓库中学习到很多 Raku 的知识。\n元运算符  cross  dd \u0026lt;a b\u0026gt; X \u0026lt;c d\u0026gt; # Output: ((\u0026#34;a\u0026#34;, \u0026#34;c\u0026#34;), (\u0026#34;a\u0026#34;, \u0026#34;d\u0026#34;), (\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;), (\u0026#34;b\u0026#34;, \u0026#34;d\u0026#34;)).Seq dd 1 X 1 X 1 X 1 # Output: ((1, 1, 1, 1),).Seq dd 1, 2, 3 X** 2, 4 # Output: 1, 1, 4, 16, 9, 81).Seq my @result = gather { for 1..3 X \u0026#39;a\u0026#39;..\u0026#39;b\u0026#39; -\u0026gt; ($n, $a) { take \u0026#34;$n|$a\u0026#34; } } dd @result; # Output: Array @result = [\u0026#34;1|a\u0026#34;, \u0026#34;1|b\u0026#34;, \u0026#34;2|a\u0026#34;, \u0026#34;2|b\u0026#34;, \u0026#34;3|a\u0026#34;, \u0026#34;3|b\u0026#34;] my @result = gather for (1..3 X \u0026#39;A\u0026#39;..\u0026#39;B\u0026#39;) -\u0026gt; $na { take $na.join(\u0026#39;:\u0026#39;); } dd @result; # Output: Array @result = [\u0026#34;1:A\u0026#34;, \u0026#34;1:B\u0026#34;, \u0026#34;2:A\u0026#34;, \u0026#34;2:B\u0026#34;, \u0026#34;3:A\u0026#34;, \u0026#34;3:B\u0026#34;] dd \u0026lt;a b\u0026gt; X, \u0026lt;c d\u0026gt;; # Output: ((\u0026#34;a\u0026#34;, \u0026#34;c\u0026#34;), (\u0026#34;a\u0026#34;, \u0026#34;d\u0026#34;), (\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;), (\u0026#34;b\u0026#34;, \u0026#34;d\u0026#34;)).Seq dd \u0026lt;a b\u0026gt; X~ \u0026lt;1 2\u0026gt;; # Output: (\u0026#34;a1\u0026#34;, \u0026#34;a2\u0026#34;, \u0026#34;b1\u0026#34;, \u0026#34;b2\u0026#34;).Seq dd \u0026lt;a b\u0026gt; X, 1,2 X, \u0026lt;x y\u0026gt;; # Output: ((\u0026#34;a\u0026#34;, 1, \u0026#34;x\u0026#34;), (\u0026#34;a\u0026#34;, 1, \u0026#34;y\u0026#34;), (\u0026#34;a\u0026#34;, 2, \u0026#34;x\u0026#34;), (\u0026#34;a\u0026#34;, 2, \u0026#34;y\u0026#34;), (\u0026#34;b\u0026#34;, 1, \u0026#34;x\u0026#34;), (\u0026#34;b\u0026#34;, 1, \u0026#34;y\u0026#34;), (\u0026#34;b\u0026#34;, 2, \u0026#34;x\u0026#34;), (\u0026#34;b\u0026#34;, 2, \u0026#34;y\u0026#34;)).Seq dd 1,2 X* 3,4; # Output: (3, 4, 6, 8).Seq dd 1,2 Xcmp 3,2,0; # Output: (Order::Less, Order::Less, Order::More, Order::Less, Order::Same, Order::More).Seq dd 1 X* 3,4; # Output: (3, 4).Seq dd 1, 2 X* 3; # Output: (3, 6).Seq dd 1 X* 3; # Output: (3,).Seq # $[] does not flatten dd $[1,2] X~ \u0026lt;a b\u0026gt; # Output: (\u0026#34;1 2a\u0026#34;, \u0026#34;1 2b\u0026#34;).Seq  zip  dd \u0026lt;a b\u0026gt; Z \u0026lt;c d\u0026gt;; # Output: ((\u0026#34;a\u0026#34;, \u0026#34;c\u0026#34;), (\u0026#34;b\u0026#34;, \u0026#34;d\u0026#34;)).Seq dd 1, 2, 3 Z** 2, 4; # Output: (1, 16).Seq dd \u0026lt;a b\u0026gt; Z~ \u0026lt;1 2\u0026gt;; # Output: (\u0026#34;a1\u0026#34;, \u0026#34;b2\u0026#34;).Seq dd 1,2 Z* 3,4; # Output: (3, 8).Seq dd 1,2 Zcmp 3,2,0; # Output: (Order::Less, Order::Same).Seq dd (1..* Z** 1..*).[^5]; # Output: (1, 4, 27, 256, 3125) dd (1..* Z+ (3, 2 ... *)).[^5]; # Output: (4, 4, 4, 4, 4) dd 1 Z* 3,4; # Output: (3,).Seq dd 1, 2 Z* 3; # Output: (3,).Seq dd 1 Z* 3; # Output: (3,).Seq dd \u0026lt;a b c d\u0026gt; Z \u0026#39;x\u0026#39;, \u0026#39;z\u0026#39;, *; # Output: ((\u0026#34;a\u0026#34;, \u0026#34;x\u0026#34;), (\u0026#34;b\u0026#34;, \u0026#34;z\u0026#34;), (\u0026#34;c\u0026#34;, \u0026#34;z\u0026#34;), (\u0026#34;d\u0026#34;, \u0026#34;z\u0026#34;)).Seq dd 1, 2, 3, * Z 10, 20, 30, 40, 50; # Output: ((1, 10), (2, 20), (3, 30), (3, 40), (3, 50)).Seq dd (2, 10, * Z 3, 4, 5, *).[^5]; # Output: ((2, 3), (10, 4), (10, 5), (10, 5), (10, 5)) dd \u0026lt;a b c d\u0026gt; Z~ \u0026#39;x\u0026#39;, \u0026#39;z\u0026#39;, *; # Output: (\u0026#34;ax\u0026#34;, \u0026#34;bz\u0026#34;, \u0026#34;cz\u0026#34;, \u0026#34;dz\u0026#34;).Seq dd 1, 2, 3, * Z+ 10, 20, 30, 40, 50; # Output: (11, 22, 33, 43, 53).Seq dd (2, 10, * Z* 3, 4, 5, *).[^5]; # Output: (6, 40, 50, 50, 50) dd [Z](1,2,3;4,5,6;7,8,9); # Output: ((1, 4, 7), (2, 5, 8), (3, 6, 9)).Seq dd [Z\u0026lt;](1,2,3;4,5,6;7,8,9); (Bool::True, Bool::True, Bool::True).Seq  hyper  my $a := (1,2,3); my $b := (2,4,6); # 以下表达式都输出 (3, 6, 9) dd $a \u0026gt;\u0026gt;+\u0026lt;\u0026lt; $b; dd $a »+« $b; dd $a \u0026gt;\u0026gt;+\u0026gt;\u0026gt; $b; dd $a »+» $b; dd $a \u0026lt;\u0026lt;+\u0026gt;\u0026gt; $b; dd $a «+» $b; dd $a \u0026lt;\u0026lt;+\u0026lt;\u0026lt; $b; dd $a «+« $b; dd $a \u0026gt;\u0026gt;[\u0026amp;infix:\u0026lt;+\u0026gt;]\u0026lt;\u0026lt; $b; dd $a »[\u0026amp;infix:\u0026lt;+\u0026gt;]« $b; dd $a \u0026gt;\u0026gt;[\u0026amp;infix:\u0026lt;+\u0026gt;]\u0026gt;\u0026gt; $b; dd $a »[\u0026amp;infix:\u0026lt;+\u0026gt;]» $b; dd $a \u0026lt;\u0026lt;[\u0026amp;infix:\u0026lt;+\u0026gt;]\u0026gt;\u0026gt; $b; dd $a «[\u0026amp;infix:\u0026lt;+\u0026gt;]» $b; dd $a \u0026lt;\u0026lt;[\u0026amp;infix:\u0026lt;+\u0026gt;]\u0026lt;\u0026lt; $b; dd $a «[\u0026amp;infix:\u0026lt;+\u0026gt;]« $b; »*« 运算符的优先级比 »+« 运算符的优先级高:\ndd (1, 2, 3) »+« (10, 20, 30) »*« (2, 3, 4); # Output: (21, 62, 123) unary postfix(一元后缀运算符)\nmy @r = (1, 2, 3); @r»++; dd @r; # Output: Array @r = [2, 3, 4] unary prefix(一元前缀运算符)\nmy @r; @r = -« (3, 2, 1); dd @r; # Output: Array @r = [-3, -2, -1] dimension upgrade(升维), auto dimension upgrade on rhs/lhs ASCII notation\nmy @r = (1, 2, 3) \u0026gt;\u0026gt;+\u0026gt;\u0026gt; 1; dd @r; # Output: Array @r = [2, 3, 4] my @r = 2 \u0026lt;\u0026lt;*\u0026lt;\u0026lt; (10, 20, 30); dd @r; # Output: Array @r = [20, 40, 60] both-dwim and non-dwim sanity:\ndd (1,2,3) \u0026lt;\u0026lt;~\u0026gt;\u0026gt; \u0026lt;A B C D E\u0026gt;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;2B\u0026#34;, \u0026#34;3C\u0026#34;, \u0026#34;1D\u0026#34;, \u0026#34;2E\u0026#34;) extension(扩展)\ndd (1,2,3,4) \u0026gt;\u0026gt;~\u0026gt;\u0026gt; \u0026lt;A B C D E\u0026gt;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;2B\u0026#34;, \u0026#34;3C\u0026#34;, \u0026#34;4D\u0026#34;) dd (1,2,3,4,5) \u0026lt;\u0026lt;~\u0026lt;\u0026lt; \u0026lt;A B C D\u0026gt;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;2B\u0026#34;, \u0026#34;3C\u0026#34;, \u0026#34;4D\u0026#34;) dd (1,2,3,4) \u0026gt;\u0026gt;~\u0026gt;\u0026gt; \u0026lt;A B C\u0026gt;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;2B\u0026#34;, \u0026#34;3C\u0026#34;, \u0026#34;4A\u0026#34;) dd (1,2,3) \u0026lt;\u0026lt;~\u0026lt;\u0026lt; \u0026lt;A B C D\u0026gt;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;2B\u0026#34;, \u0026#34;3C\u0026#34;, \u0026#34;1D\u0026#34;) dd (1,2,3,4) \u0026gt;\u0026gt;~\u0026gt;\u0026gt; \u0026lt;A B\u0026gt;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;2B\u0026#34;, \u0026#34;3A\u0026#34;, \u0026#34;4B\u0026#34;) dd (1,2) \u0026lt;\u0026lt;~\u0026lt;\u0026lt; \u0026lt;A B C D\u0026gt;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;2B\u0026#34;, \u0026#34;1C\u0026#34;, \u0026#34;2D\u0026#34;) dd (1,2,3,4) \u0026gt;\u0026gt;~\u0026gt;\u0026gt; \u0026lt;A\u0026gt;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;2A\u0026#34;, \u0026#34;3A\u0026#34;, \u0026#34;4A\u0026#34;) dd (1,) \u0026lt;\u0026lt;~\u0026lt;\u0026lt; \u0026lt;A B C D\u0026gt;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;1B\u0026#34;, \u0026#34;1C\u0026#34;, \u0026#34;1D\u0026#34;) dd (1,2,3,4) \u0026gt;\u0026gt;~\u0026gt;\u0026gt; \u0026#39;A\u0026#39;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;2A\u0026#34;, \u0026#34;3A\u0026#34;, \u0026#34;4A\u0026#34;) dd 1 \u0026lt;\u0026lt;~\u0026lt;\u0026lt; \u0026lt;A B C D\u0026gt;; # Output: (\u0026#34;1A\u0026#34;, \u0026#34;1B\u0026#34;, \u0026#34;1C\u0026#34;, \u0026#34;1D\u0026#34;) 枚举  匿名枚举  匿名枚举返回一个 Map:\nmy $e = enum \u0026lt;ook! ook. ook? \u0026gt;; # Map.new((ook! =\u0026gt; 0, ook. =\u0026gt; 1, ook? =\u0026gt; 2)) say $e.keys.elems; # 3 say $e\u0026lt;ook!\u0026gt;; # 0 say $e\u0026lt;ook.\u0026gt;; # 1 say $e\u0026lt;ook?\u0026gt;; # 2  say so $e ~~ Map; # True say $e.keys; # (ook? ook! ook.) anon enum \u0026lt;un\u0026gt; 等价于 enum :: \u0026lt;un\u0026gt;:\nanon enum \u0026lt;un\u0026gt;; # Map.new((un =\u0026gt; 0)) say +un; # 0 my %e = enum :: \u0026lt;foo bar baz \u0026gt;; # {bar =\u0026gt; 1, baz =\u0026gt; 2, foo =\u0026gt; 0} say %e\u0026lt;bar\u0026gt;; # 1 say baz; # baz say +baz; # 2  枚举作为角色  枚举可以用在 but 或 does 中, 用以命令一个想要的属性:\nenum Maybe \u0026lt;No Yes Dunno\u0026gt;; class Bar { } class Foo does Maybe {} my $x = Foo.new(Maybe =\u0026gt; No); say $x.No; # True say $x.Yes; # False say $x.Dunno; # False my $y = Bar.new() does Maybe(Yes); say $y.Yes; # True say $y.No; # False say $y.Dunno; # False my $z = Bar.new() but Maybe(Dunno); say $z.No; say $z.Yes; say $z.Dunno; 属性  代理  官方文档关于 handles 的介绍很粗糙, 看完依然不知道怎么使用。\n","permalink":"https://ohmyweekly.github.io/notes/2021-06-05-learn-raku-from-roast/","tags":["Raku","Raku roast"],"title":"Learn Raku From Roast"},{"categories":["rustlang"],"contents":"Rust 的标准库 Trait 之旅 Table of Contents\n Intro Trait Basics  Trait Items  Self Functions Methods Associated Types Generic Parameters Generic Types vs Associated Types   Scope Derive Macros Default Impls Generic Blanket Impls Subtraits \u0026amp; Supertraits Trait Objects Marker Traits Auto Traits Unsafe Traits   Auto Traits  Send \u0026amp; Sync Sized   General Traits  Default Clone Copy Any   Formatting Traits  Display \u0026amp; ToString Debug   Operator Traits  Comparison Traits  PartialEq \u0026amp; Eq Hash PartialOrd \u0026amp; Ord   Arithmetic Traits  Add \u0026amp; AddAssign   Closure Traits  FnOnce, FnMut, \u0026amp; Fn   Other Traits  Deref \u0026amp; DerefMut Index \u0026amp; IndexMut Drop     Conversion Traits  From \u0026amp; Into   Error Handling  Error   Conversion Traits Continued  TryFrom \u0026amp; TryInto FromStr AsRef \u0026amp; AsMut Borrow \u0026amp; BorrowMut ToOwned   Iteration Traits  Iterator IntoIterator FromIterator   I/O Traits  Read \u0026amp; Write   Conclusion Discuss Notifications Further Reading  Intro 你有没有想过，这两者之间有什么区别?\n Deref\u0026lt;Target = T\u0026gt;, AsRef\u0026lt;T\u0026gt; 和 Borrow\u0026lt;T\u0026gt;? Clone, Copy 和 ToOwned? From\u0026lt;T\u0026gt; 和 Into\u0026lt;T\u0026gt;? TryFrom\u0026lt;\u0026amp;str\u0026gt; 和 FromStr? FnOnce, FnMut, Fn 和 fn?  或者曾经问过自己这样的问题:\n 在我的 trait 中, 我什么时候使用关联类型, 什么时候使用泛型类型? 什么是泛型的 blanket 实现? subtrait 和 supertrait 是如何工作的? 为什么这个 trait 没有任何方法?  那么这篇文章就是为你准备的! 它回答了以上所有的问题以及更多的问题。我们将一起对 Rust 标准库中所有最流行、最常用的 trait 进行快速飞越之旅!\n你可以按顺序逐节阅读本文，也可以跳转到你最感兴趣的 trait，因为每个 trait 部分都有一个链接列表，链接到 “先决知识” 部分，你应该阅读这些链接，以便有足够的背景来理解当前部分的解释。\nTrait 基础 我们将只涉及足够的基础知识，以便文章的其余部分可以精简，而不必在不同的 trait 中重新出现时重复相同的概念解释。\nTrait 项 Trait 项是指作为 trait 声明一部分的任何项。\nSelf Self 总是指实现类型。\ntraitTrait{// always returns i32 fn returns_num()-\u0026gt; i32;// returns implementing type fn returns_self()-\u0026gt; Self;}struct SomeType;struct OtherType;implTraitforSomeType{fn returns_num()-\u0026gt; i32 {5}// Self == SomeType fn returns_self()-\u0026gt; Self{SomeType}}implTraitforOtherType{fn returns_num()-\u0026gt; i32 {6}// Self == OtherType fn returns_self()-\u0026gt; Self{OtherType}}函数 Trait 函数是任何第一个参数不使用 self 关键字的函数。\ntraitDefault{// function fn default()-\u0026gt; Self;}Trait 函数可以通过 trait 或实现类型按照命名空间的方式来调用。\nfn main(){letzero: i32 =Default::default();letzero=i32::default();}方法 Trait 方法是指第一个参数使用 self 关键字并且类型为 Self、\u0026amp;Self、\u0026amp;mut Self 的任何函数。前面的类型也可以用 Box、Rc、Arc 或 Pin 来包装。\ntraitTrait{// methods fn takes_self(self);fn takes_immut_self(\u0026amp;self);fn takes_mut_self(\u0026amp;mutself);// above methods desugared fn takes_self(self: Self);fn takes_immut_self(self: \u0026amp;Self);fn takes_mut_self(self: \u0026amp;mutSelf);}// example from standard library traitToString{fn to_string(\u0026amp;self)-\u0026gt; String;}可以使用实现类型上的点运算符来调用方法。\nfn main(){letfive=5.to_string();}然而，与函数类似，它们也可以通过 trait 或实现类型按照命名空间的方式来调用。\nfn main(){letfive=ToString::to_string(\u0026amp;5);letfive=i32::to_string(\u0026amp;5);}关联类型 Trait 可以有关联类型。当我们需要在函数签名中使用 Self 以外的其他类型，但又希望类型由实现者选择，而不是在 trait 声明中硬编码时，这很有用。\ntraitTrait{type AssociatedType;fn func(arg: Self::AssociatedType);}struct SomeType;struct OtherType;// any type implementing Trait can // choose the type of AssociatedType implTraitforSomeType{type AssociatedType=i8;// chooses i8 fn func(arg: Self::AssociatedType){}}implTraitforOtherType{type AssociatedType=u8;// chooses u8 fn func(arg: Self::AssociatedType){}}fn main(){SomeType::func(-1_i8);// can only call func with i8 on SomeType OtherType::func(1_u8);// can only call func with u8 on OtherType }泛型参数 “泛型参数” 泛指泛型类型参数、泛型 lifetime 参数和泛型常量参数。由于这些说起来都很拗口，所以人们通常把它们缩写为 \u0026ldquo;generic types\u0026rdquo;, \u0026ldquo;lifetimes\u0026rdquo; 和 \u0026ldquo;generic consts\u0026rdquo;。由于 generic consts 没有在我们将要涉及的任何标准库 trait 中使用，所以它们不在本文的范围之内。\n我们可以使用参数来泛型化一个 trait 声明。\n// trait declaration generalized with lifetime \u0026amp; type parameters traitTrait\u0026lt;\u0026#39;a,T\u0026gt;{// signature uses generic type fn func1(arg: T);// signature uses lifetime fn func2(arg: \u0026amp;\u0026#39;ai32);// signature uses generic type \u0026amp; lifetime fn func3(arg: \u0026amp;\u0026#39;aT);}struct SomeType;impl\u0026lt;\u0026#39;a\u0026gt;Trait\u0026lt;\u0026#39;a,i8\u0026gt;forSomeType{fn func1(arg: i8){}fn func2(arg: \u0026amp;\u0026#39;ai32){}fn func3(arg: \u0026amp;\u0026#39;ai8){}}impl\u0026lt;\u0026#39;b\u0026gt;Trait\u0026lt;\u0026#39;b,u8\u0026gt;forSomeType{fn func1(arg: u8){}fn func2(arg: \u0026amp;\u0026#39;bi32){}fn func3(arg: \u0026amp;\u0026#39;bu8){}}可以为泛型类型提供默认值。最常用的默认值是 Self，但任何类型都可以。\n// make T = Self by default traitTrait\u0026lt;T=Self\u0026gt;{fn func(t: T){}}// any type can be used as the default traitTrait2\u0026lt;T=i32\u0026gt;{fn func2(t: T){}}struct SomeType;// omitting the generic type will // cause the impl to use the default // value, which is Self here implTraitforSomeType{fn func(t: SomeType){}}// default value here is i32 implTrait2forSomeType{fn func2(t: i32){}}// the default is overridable as we\u0026#39;d expect implTrait\u0026lt;String\u0026gt;forSomeType{fn func(t: String){}}// overridable here too implTrait2\u0026lt;String\u0026gt;forSomeType{fn func2(t: String){}}除了对 trait 进行参数化外，还可以对单个函数和方法进行参数化。\ntraitTrait{fn func\u0026lt;\u0026#39;a,T\u0026gt;(t: \u0026amp;\u0026#39;aT);}尖括号(\u0026lt; \u0026gt;)中的类型要么是泛型(T), 要么是一个具体类型(例如 i32)。所以, 看到尖括号时, 要意识到这个东西应该是泛型的。\n泛型类型 vs 关联类型 泛型类型和关联类型都将决定权交给了实现者，让他们决定在 trait 的函数和方法中应该使用哪种具体类型，所以本节试图解释什么时候使用一种类型而不是另一种类型。\n一般的经验法则是\n 当每个类型只能有一个 trait 的实现时，使用关联类型。 当每个类型可以有许多可能的 trait 的实现时，使用泛型类型。  假设我们想定义一个名为 Add 的 trait，它允许我们将值加在一起。下面是一个初始设计和只使用关联类型的实现。\ntraitAdd{type Rhs;type Output;fn add(self,rhs: Self::Rhs)-\u0026gt; Self::Output;}struct Point{x: i32,y: i32,}implAddforPoint{type Rhs=Point;type Output=Point;fn add(self,rhs: Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}fn main(){letp1=Point{x: 1,y: 1};letp2=Point{x: 2,y: 2};letp3=p1.add(p2);assert_eq!(p3.x,3);assert_eq!(p3.y,3);}比方说，我们想给 Point 添加和 i32 相加的能力，其中 i32 将和 x 和 y 成员相加。\ntraitAdd{type Rhs;type Output;fn add(self,rhs: Self::Rhs)-\u0026gt; Self::Output;}struct Point{x: i32,y: i32,}implAddforPoint{type Rhs=Point;type Output=Point;fn add(self,rhs: Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}implAddforPoint{// ❌ type Rhs=i32;type Output=Point;fn add(self,rhs: i32)-\u0026gt; Point{Point{x: self.x+rhs,y: self.y+rhs,}}}fn main(){letp1=Point{x: 1,y: 1};letp2=Point{x: 2,y: 2};letp3=p1.add(p2);assert_eq!(p3.x,3);assert_eq!(p3.y,3);letp1=Point{x: 1,y: 1};letint2=2;letp3=p1.add(int2);// ❌ assert_eq!(p3.x,3);assert_eq!(p3.y,3);}这抛出:\nerror[E0119]: conflicting implementations of trait `Add` for type `Point`:\r--\u0026gt; src/main.rs:23:1\r|\r12 | impl Add for Point {\r| ------------------ first implementation here\r...\r23 | impl Add for Point {\r| ^^^^^^^^^^^^^^^^^^ conflicting implementation for `Point`\r由于 Add trait 没有任何泛型类型的参数化，我们只能对每个类型进行一次实现，这意味着我们只能为 Rhs 和 Output 选择一次类型！为了允许 Point 和 Point 相加,以及 i32 和 Point 相加，我们必须将 Rhs 从关联类型重构为泛型类型，这将允许我们用不同的类型参数为 Rhs 多次实现 Point trait。\ntraitAdd\u0026lt;Rhs\u0026gt;{type Output;fn add(self,rhs: Rhs)-\u0026gt; Self::Output;}struct Point{x: i32,y: i32,}implAdd\u0026lt;Point\u0026gt;forPoint{type Output=Self;fn add(self,rhs: Point)-\u0026gt; Self::Output{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}implAdd\u0026lt;i32\u0026gt;forPoint{// ✅ type Output=Self;fn add(self,rhs: i32)-\u0026gt; Self::Output{Point{x: self.x+rhs,y: self.y+rhs,}}}fn main(){letp1=Point{x: 1,y: 1};letp2=Point{x: 2,y: 2};letp3=p1.add(p2);assert_eq!(p3.x,3);assert_eq!(p3.y,3);letp1=Point{x: 1,y: 1};letint2=2;letp3=p1.add(int2);// ✅ assert_eq!(p3.x,3);assert_eq!(p3.y,3);}比方说，我们添加了一个名为 Line 的新类型，它包含两个 Point，现在在我们的程序中，将两个 Point 相加应该产生一个 Line 而不是 Point。考虑到 Add trait 当前的设计，这是不可能的，因为 Output 是一个关联类型，但是我们可以通过将 Output 从关联类型重构为泛型类型来满足这些新的要求。\ntraitAdd\u0026lt;Rhs,Output\u0026gt;{fn add(self,rhs: Rhs)-\u0026gt; Output;}struct Point{x: i32,y: i32,}implAdd\u0026lt;Point,Point\u0026gt;forPoint{fn add(self,rhs: Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}implAdd\u0026lt;i32,Point\u0026gt;forPoint{fn add(self,rhs: i32)-\u0026gt; Point{Point{x: self.x+rhs,y: self.y+rhs,}}}struct Line{start: Point,end: Point,}implAdd\u0026lt;Point,Line\u0026gt;forPoint{// ✅ fn add(self,rhs: Point)-\u0026gt; Line{Line{start: self,end: rhs,}}}fn main(){letp1=Point{x: 1,y: 1};letp2=Point{x: 2,y: 2};letp3: Point=p1.add(p2);assert!(p3.x==3\u0026amp;\u0026amp;p3.y==3);letp1=Point{x: 1,y: 1};letint2=2;letp3=p1.add(int2);assert!(p3.x==3\u0026amp;\u0026amp;p3.y==3);letp1=Point{x: 1,y: 1};letp2=Point{x: 2,y: 2};letl: Line=p1.add(p2);// ✅ assert!(l.start.x==1\u0026amp;\u0026amp;l.start.y==1\u0026amp;\u0026amp;l.end.x==2\u0026amp;\u0026amp;l.end.y==2)}那么，上面的 Add trait 哪种最好呢？这真的取决于你的程序的要求! 合适的就是最好的。\n作用域 Trait 项不能使用，除非该 trait 在作用域内。大多数 Rustaceans 在第一次尝试写一个用 I/O 做任何事情的程序时，都会艰难地学会这一点，因为 Read 和 Write trait 不在标准库的预加载中。\nusestd::fs::File;usestd::io;fn main()-\u0026gt; Result\u0026lt;(),io::Error\u0026gt;{letmutfile=File::open(\u0026#34;Cargo.toml\u0026#34;)?;letmutbuffer=String::new();file.read_to_string(\u0026amp;mutbuffer)?;// ❌ read_to_string not found in File Ok(())}read_to_string(buf: \u0026amp;mut String) 由 std::io::Read trait 声明，并由 std::fs::File 结构体实现，但为了调用它，std::io::Read 必须在作用域内。\nusestd::fs::File;usestd::io;usestd::io::Read;// ✅ fn main()-\u0026gt; Result\u0026lt;(),io::Error\u0026gt;{letmutfile=File::open(\u0026#34;Cargo.toml\u0026#34;)?;letmutbuffer=String::new();file.read_to_string(\u0026amp;mutbuffer)?;// ✅ Ok(())}标准库中的 prelude 是标准库中的一个模块，即 std::prelude::v1，它在每个其他模块的顶部被自动导入，即 use std::prelude::v1::*。因此，下面的 trait 总是在作用域内，我们永远不需要显式导入它们，因为它们是 prelude 的一部分。\n AsMut AsRef Clone Copy Default Drop Eq Fn FnMut FnOnce From Into ToOwned IntoIterator Iterator PartialEq PartialOrd Send Sized Sync ToString Ord  派生宏 标准库导出了一些派生宏，如果一个类型的所有成员都实现了某个 trait, 我们可以使用这些宏来快速方便地在这个类型上实现该 trait。这些派生宏以它们所实现的 trait 命名。\n Clone Copy Debug Default Eq Hash Ord PartialEq PartialOrd  使用示例:\n// macro derives Copy \u0026amp; Clone impl for SomeType #[derive(Copy, Clone)]struct SomeType;注意：派生宏只是过程宏，可以做任何事情，没有硬性规定一定要实现一个 trait，也没有规定只有在类型的所有成员都实现一个 trait 的情况下才能工作，这些只是标准库中派生宏所遵循的约定。\n默认实现 Trait 可以为其函数和方法提供默认的实现。\ntraitTrait{fn method(\u0026amp;self){println!(\u0026#34;default impl\u0026#34;);}}struct SomeType;struct OtherType;// use default impl for Trait::method implTraitforSomeType{}implTraitforOtherType{// use our own impl for Trait::method fn method(\u0026amp;self){println!(\u0026#34;OtherType impl\u0026#34;);}}fn main(){SomeType.method();// prints \u0026#34;default impl\u0026#34; OtherType.method();// prints \u0026#34;OtherType impl\u0026#34; }如果一些 trait 方法可以只用其他 trait 方法来实现，这就特别方便。\ntraitGreet{fn greet(\u0026amp;self,name: \u0026amp;str)-\u0026gt; String;fn greet_loudly(\u0026amp;self,name: \u0026amp;str)-\u0026gt; String {self.greet(name)+\u0026#34;!\u0026#34;}}struct Hello;struct Hola;implGreetforHello{fn greet(\u0026amp;self,name: \u0026amp;str)-\u0026gt; String {format!(\u0026#34;Hello {}\u0026#34;,name)}// use default impl for greet_loudly }implGreetforHola{fn greet(\u0026amp;self,name: \u0026amp;str)-\u0026gt; String {format!(\u0026#34;Hola {}\u0026#34;,name)}// override default impl fn greet_loudly(\u0026amp;self,name: \u0026amp;str)-\u0026gt; String {letmutgreeting=self.greet(name);greeting.insert_str(0,\u0026#34;¡\u0026#34;);greeting+\u0026#34;!\u0026#34;}}fn main(){println!(\u0026#34;{}\u0026#34;,Hello.greet(\u0026#34;John\u0026#34;));// prints \u0026#34;Hello John\u0026#34; println!(\u0026#34;{}\u0026#34;,Hello.greet_loudly(\u0026#34;John\u0026#34;));// prints \u0026#34;Hello John!\u0026#34; println!(\u0026#34;{}\u0026#34;,Hola.greet(\u0026#34;John\u0026#34;));// prints \u0026#34;Hola John\u0026#34; println!(\u0026#34;{}\u0026#34;,Hola.greet_loudly(\u0026#34;John\u0026#34;));// prints \u0026#34;¡Hola John!\u0026#34; }标准库中的许多 trait 为它们的许多方法提供了默认的实现。\nGeneric Blanket Impls 通用全面实现是在泛型类型而不是具体类型上的实现。为了解释为什么以及如何使用，让我们从为数字类型编写一个 is_even 方法开始。\ntraitEven{fn is_even(self)-\u0026gt; bool;}implEvenfori8{fn is_even(self)-\u0026gt; bool {self%2_i8==0_i8}}implEvenforu8{fn is_even(self)-\u0026gt; bool {self%2_u8==0_u8}}implEvenfori16{fn is_even(self)-\u0026gt; bool {self%2_i16==0_i16}}// etc #[test]// ✅ fn test_is_even(){assert!(2_i8.is_even());assert!(4_u8.is_even());assert!(6_i16.is_even());// etc }显然，这是很啰嗦的。而且，我们所有的实现几乎都是一样的。此外，如果 Rust 决定在未来添加更多的数字类型，我们必须记得回到这段代码，并用新的数字类型更新它。我们可以使用一个通用的全面实现来解决所有这些问题。\nusestd::fmt::Debug;usestd::convert::TryInto;usestd::ops::Rem;traitEven{fn is_even(self)-\u0026gt; bool;}// generic blanket impl impl\u0026lt;T\u0026gt;EvenforTwhereT: Rem\u0026lt;Output=T\u0026gt;+PartialEq\u0026lt;T\u0026gt;+Sized,u8: TryInto\u0026lt;T\u0026gt;,\u0026lt;u8asTryInto\u0026lt;T\u0026gt;\u0026gt;::Error: Debug,{fn is_even(self)-\u0026gt; bool {// these unwraps will never panic self%2.try_into().unwrap()==0.try_into().unwrap()}}#[test]// ✅ fn test_is_even(){assert!(2_i8.is_even());assert!(4_u8.is_even());assert!(6_i16.is_even());// etc }Unlike default impls, which provide an impl, generic blanket impls provide the impl, so they are not overridable. 与默认实现不同，默认的实现提供了一个实现，而通用的全面实现提供了特定的实现，所以它们是不可覆盖的。\nusestd::fmt::Debug;usestd::convert::TryInto;usestd::ops::Rem;traitEven{fn is_even(self)-\u0026gt; bool;}impl\u0026lt;T\u0026gt;EvenforTwhereT: Rem\u0026lt;Output=T\u0026gt;+PartialEq\u0026lt;T\u0026gt;+Sized,u8: TryInto\u0026lt;T\u0026gt;,\u0026lt;u8asTryInto\u0026lt;T\u0026gt;\u0026gt;::Error: Debug,{fn is_even(self)-\u0026gt; bool {self%2.try_into().unwrap()==0.try_into().unwrap()}}implEvenforu8{// ❌ fn is_even(self)-\u0026gt; bool {self%2_u8==0_u8}}这抛出:\nerror[E0119]: conflicting implementations of trait `Even` for type `u8`:\r--\u0026gt; src/lib.rs:22:1\r|\r10 | / impl\u0026lt;T\u0026gt; Even for T\r11 | | where\r12 | | T: Rem\u0026lt;Output = T\u0026gt; + PartialEq\u0026lt;T\u0026gt; + Sized,\r13 | | u8: TryInto\u0026lt;T\u0026gt;,\r... |\r19 | | }\r20 | | }\r| |_- first implementation here\r21 | 22 | impl Even for u8 {\r| ^^^^^^^^^^^^^^^^ conflicting implementation for `u8`\rThese impls overlap, hence they conflict, hence Rust rejects the code to ensure trait coherence. Trait coherence is the property that there exists at most one impl of a trait for any given type. The rules Rust uses to enforce trait coherence, the implications of those rules, and workarounds for the implications are outside the scope of this article.\n这些实现重叠了，因此它们冲突，因此 Rust 拒绝了确保 trait 一致性的代码。Trait 一致性是指任何给定类型的 trait 最多存在一个实现的属性。Rust 用来强制执行 trait 一致性的规则，这些规则的含义，以及含义的变通方法都不在本文的范围内。\nSubtraits \u0026amp; Supertraits \u0026ldquo;subtrait\u0026rdquo; 中的 \u0026ldquo;sub\u0026rdquo; 指的是子集，\u0026ldquo;supertrait\u0026rdquo; 中的 \u0026ldquo;super\u0026rdquo; 指的是超集。如果我们有一个这样的 trait 声明。\ntraitSubtrait: Supertrait{}All of the types which impl Subtrait are a subset of all the types which impl Supertrait, or to put it in opposite but equivalent terms: all the types which impl Supertrait are a superset of all the types which impl Subtrait.\nAlso, the above is just syntax sugar for: 所有实现 Subtrait 的类型都是所有实现 Supertrait 的类型的子集，或者用相反但等价的词语来表达：所有实现 Supertrait 的类型都是所有实现 Subtrait 的类型的超集。\n另外，上面的代码只是下面这段代码的语法糖:\ntraitSubtraitwhereSelf: Supertrait{}这是一个微妙而又重要的区别，要理解的是，约束是在 Self 上的，即实现 Subtrait 的类型，而不是在 Subtrait 本身。后者是没有任何意义的，因为 trait 约束只能应用于具体的类型，这些类型可以实现 trait。Trait 不能实现其他 trait。\ntraitSupertrait{fn method(\u0026amp;self){println!(\u0026#34;in supertrait\u0026#34;);}}traitSubtrait: Supertrait{// this looks like it might impl or // override Supertrait::method but it // does not fn method(\u0026amp;self){println!(\u0026#34;in subtrait\u0026#34;)}}struct SomeType;// adds Supertrait::method to SomeType implSupertraitforSomeType{}// adds Subtrait::method to SomeType implSubtraitforSomeType{}// both methods exist on SomeType simultaneously // neither overriding or shadowing the other fn main(){SomeType.method();// ❌ ambiguous method call // must disambiguate using fully-qualified syntax \u0026lt;SomeTypeasSupertrait\u0026gt;::method(\u0026amp;st);// ✅ prints \u0026#34;in supertrait\u0026#34; \u0026lt;SomeTypeasSubtrait\u0026gt;::method(\u0026amp;st);// ✅ prints \u0026#34;in subtrait\u0026#34; }Furthermore, there are no rules for how a type must impl both a subtrait and a supertrait. It can use the methods from either in the impl of the other. 此外，没有规定一个类型必须同时实现一个 subtrait 和一个 supertrait。它可以在另一个类型的实现中使用其中一个类型的方法。\ntraitSupertrait{fn super_method(\u0026amp;mutself);}traitSubtrait: Supertrait{fn sub_method(\u0026amp;mutself);}struct CallSuperFromSub;implSupertraitforCallSuperFromSub{fn super_method(\u0026amp;mutself){println!(\u0026#34;in super\u0026#34;);}}implSubtraitforCallSuperFromSub{fn sub_method(\u0026amp;mutself){println!(\u0026#34;in sub\u0026#34;);self.super_method();}}struct CallSubFromSuper;implSupertraitforCallSubFromSuper{fn super_method(\u0026amp;mutself){println!(\u0026#34;in super\u0026#34;);self.sub_method();}}implSubtraitforCallSubFromSuper{fn sub_method(\u0026amp;mutself){println!(\u0026#34;in sub\u0026#34;);}}struct CallEachOther(bool);implSupertraitforCallEachOther{fn super_method(\u0026amp;mutself){println!(\u0026#34;in super\u0026#34;);ifself.0{self.0=false;self.sub_method();}}}implSubtraitforCallEachOther{fn sub_method(\u0026amp;mutself){println!(\u0026#34;in sub\u0026#34;);ifself.0{self.0=false;self.super_method();}}}fn main(){CallSuperFromSub.super_method();// prints \u0026#34;in super\u0026#34; CallSuperFromSub.sub_method();// prints \u0026#34;in sub\u0026#34;, \u0026#34;in super\u0026#34; CallSubFromSuper.super_method();// prints \u0026#34;in super\u0026#34;, \u0026#34;in sub\u0026#34; CallSubFromSuper.sub_method();// prints \u0026#34;in sub\u0026#34; CallEachOther(true).super_method();// prints \u0026#34;in super\u0026#34;, \u0026#34;in sub\u0026#34; CallEachOther(true).sub_method();// prints \u0026#34;in sub\u0026#34;, \u0026#34;in super\u0026#34; }Hopefully the examples above show that the relationship between subtraits and supertraits can be complex. Before introducing a mental model that neatly encapsulates all of that complexity let\u0026rsquo;s quickly review and establish the mental model we use for understanding trait bounds on generic types: 希望上面的例子能表明，subtrait 和 supertrait 之间的关系可能很复杂。在介绍一个能整齐地概括所有这些复杂性的心理模型之前，让我们快速回顾并建立我们用于理解泛型上的 trait 约束的心理模型。\nfn function\u0026lt;T: Clone\u0026gt;(t: T){// impl }Without knowing anything about the impl of this function we could reasonably guess that t.clone() gets called at some point because when a generic type is bounded by a trait that strongly implies it has a dependency on the trait. The mental model for understanding the relationship between generic types and their trait bounds is a simple and intuitive one: generic types depend on their trait bounds.\nNow let\u0026rsquo;s look the trait declaration for Copy: 在不了解这个函数的实现的情况下，我们可以合理地猜测 t.clone() 在某些时候会被调用，因为当一个泛型被一个 trait 约束时，强烈地意味着它对 trait 有依赖性。理解泛型与其 trait 约束之间关系的心理模型是一个简单而直观的模型：泛型 “依赖” 其 trait 约束。\n现在让我们看看 Copy 的 trait 声明。\ntraitCopy: Clone {}上面的语法看起来非常类似于在泛型类型上应用 trait 约束的语法，然而 Copy 根本不依赖于 Clone。我们前面开发的心理模型在这里并不能帮助我们。在我看来，理解 subtrait 和 supertrait 之间关系的最简单、最优雅的心理模型是：subtrait “精炼” 其 supertrait。\n“精炼” 这个词故意保持有些模糊，因为它在不同的语境中可以有不同的含义。\n subtrait 可能会使它的 supertrait 的方法更加特化，速度更快，使用更少的内存，例如：Copy: Clone subtrait 可以对 supertrait 的方法的实现做出额外的保证，例如 Eq: PartialEq, Ord: PartialOrd, ExactSizeIterator: Iterator subtrait 可能使 supertrait 的方法更灵活或更容易调用，例如 FnMut: FnOnce, `Fn: FnMut subtrait 可以扩展一个 supertrait，并添加新的方法，例如 DoubleEndedIterator: Iterator, ExactSizeIterator: Iterator  Trait 对象 泛型给了我们编译时的多态性，而 trait 对象给了我们运行时的多态性。我们可以使用 trait 对象来允许函数在运行时动态地返回不同的类型。\nfn example(condition: bool,vec: Vec\u0026lt;i32\u0026gt;)-\u0026gt; Box\u0026lt;dynIterator\u0026lt;Item=i32\u0026gt;\u0026gt;{letiter=vec.into_iter();ifcondition{// Has type: // Box\u0026lt;Map\u0026lt;IntoIter\u0026lt;i32\u0026gt;, Fn(i32) -\u0026gt; i32\u0026gt;\u0026gt; // But is cast to: // Box\u0026lt;dyn Iterator\u0026lt;Item = i32\u0026gt;\u0026gt; Box::new(iter.map(|n|n*2))}else{// Has type: // Box\u0026lt;Filter\u0026lt;IntoIter\u0026lt;i32\u0026gt;, Fn(\u0026amp;i32) -\u0026gt; bool\u0026gt;\u0026gt; // But is cast to: // Box\u0026lt;dyn Iterator\u0026lt;Item = i32\u0026gt;\u0026gt; Box::new(iter.filter(|\u0026amp;n|n\u0026gt;=2))}}Trait 对象还允许我们在集合中存储异构类型。\nusestd::f64::consts::PI;struct Circle{radius: f64,}struct Square{side: f64 }traitShape{fn area(\u0026amp;self)-\u0026gt; f64;}implShapeforCircle{fn area(\u0026amp;self)-\u0026gt; f64 {PI*self.radius*self.radius}}implShapeforSquare{fn area(\u0026amp;self)-\u0026gt; f64 {self.side*self.side}}fn get_total_area(shapes: Vec\u0026lt;Box\u0026lt;dynShape\u0026gt;\u0026gt;)-\u0026gt; f64 {shapes.into_iter().map(|s|s.area()).sum()}fn example(){letshapes: Vec\u0026lt;Box\u0026lt;dynShape\u0026gt;\u0026gt;=vec![Box::new(Circle{radius: 1.0}),// Box\u0026lt;Circle\u0026gt; cast to Box\u0026lt;dyn Shape\u0026gt; Box::new(Square{side: 1.0}),// Box\u0026lt;Square\u0026gt; cast to Box\u0026lt;dyn Shape\u0026gt; ];assert_eq!(PI+1.0,get_total_area(shapes));// ✅ }Trait 对象是不确定大小的，所以它们必须总是在指针后面。我们可以根据类型中是否存在 dyn 关键字来区分具体类型和 trait 对象。\nstruct Struct;traitTrait{}// regular struct \u0026amp;StructBox\u0026lt;Struct\u0026gt;Rc\u0026lt;Struct\u0026gt;Arc\u0026lt;Struct\u0026gt;// trait objects \u0026amp;dynTraitBox\u0026lt;dynTrait\u0026gt;Rc\u0026lt;dynTrait\u0026gt;Arc\u0026lt;dynTrait\u0026gt;并非所有的 trait 都可以转换为 trait 对象。如果一个 trait 满足这些要求，它就是对象安全的。\n trait 不需要 Self: Sized。 所有 trait 的方法都是对象安全的。  如果 trait 方法满足这些要求，它就是对象安全的。\n 方法需要 Self: Sized 或 该方法只在接收器位置使用 Self 类型。  理解为什么要求是这样的，与本文其他部分无关，但如果你仍然好奇，在 Sizedness in Rust 中会有介绍。\nMarker Traits 标记 trait 是没有 trait 项的 trait。它们的工作是将实现类型 “标记” 为具有某些属性，否则不可能用类型系统来表示。\n// Impling PartialEq for a type promises // that equality for the type has these properties: // - symmetry: a == b implies b == a, and // - transitivity: a == b \u0026amp;\u0026amp; b == c implies a == c // But DOES NOT promise this property: // - reflexivity: a == a traitPartialEq{fn eq(\u0026amp;self,other: \u0026amp;Self)-\u0026gt; bool;}// Eq has no trait items! The eq method is already // declared by PartialEq, but \u0026#34;impling\u0026#34; Eq // for a type promises this additional equality property: // - reflexivity: a == a traitEq: PartialEq {}// f64 impls PartialEq but not Eq because NaN != NaN // i32 impls PartialEq \u0026amp; Eq because there\u0026#39;s no NaNs :) Auto Traits 自动 trait 是指如果一个类型的所有成员都实现了这个 trait，那么这个 trait 就会被自动实现。“成员” 的含义取决于类型，例如：结构体的字段、枚举的变体、数组的元素、元组的项等等。\n所有的自动 trait 都是标记 trait，但不是所有的标记 trait 都是自动 trait。自动 trait 必须是标记 trait，这样编译器就可以为它们提供一个自动的缺省实现，如果它们有任何 trait 项，那就不可能了。\n自动 trait 的例子。\n// implemented for types which are safe to send between threads unsafeautotraitSend{}// implemented for types whose references are safe to send between threads unsafeautotraitSync{}Unsafe Traits Trait 可以被标记为不安全，以表明实现该 trait 可能需要不安全的代码。Send 和 Sync 都被标记为 unsafe，因为如果它们没有被自动实现，就意味着它一定包含一些非 Send 或非 Sync 成员，如果我们想手动标记类型为 Send 和 Sync，我们作为实现者必须格外小心，以确保没有数据竞争。\n// SomeType is not Send or Sync struct SomeType{not_send_or_sync: *const(),}// but if we\u0026#39;re confident that our impl doesn\u0026#39;t have any data // races we can explicitly mark it as Send and Sync using unsafe unsafeimplSendforSomeType{}unsafeimplSyncforSomeType{}Auto Traits Send \u0026amp; Sync 预备知识\n Marker Traits Auto Traits Unsafe Traits  unsafeautotraitSend{}unsafeautotraitSync{}如果一个类型是 Send，意味着在线程之间发送是安全的。如果一个类型是 Sync，这意味着在线程之间共享它的引用是安全的。更准确地说，如果且仅当 \u0026amp;T 是 Send 时，一些类型 T 是 Sync。\n几乎所有类型都是 Send 和 Sync 的。唯一值得注意的 Send 异常是 Rc，唯一值得注意的 Sync 异常是 Rc、Cell 和 RefCell。如果我们需要一个 Rc 的 Send 版本，我们可以使用 Arc。如果我们需要 Cell 或 RefCell 的 Sync 版本，我们可以 Mutex 或 RwLock。虽然如果我们使用 Mutex 或 RwLock 只是包裹一个原语类型，通常最好使用标准库提供的原子原语类型，如 AtomicBool、AtomicI32、AtomicUsize 等。\n几乎所有的类型都是 Sync，这可能会让一些人感到惊讶，但是是的，即使对于没有任何内部同步的类型也是如此。这要归功于 Rust 严格的借用规则。\n我们可以将同一数据的许多不可变的引用传递给许多线程，而且我们保证不会出现数据竞争，因为只要有任何不可变的引用存在， Rust 就会静态地保证底层数据不能被修改。\nusecrossbeam::thread;fn main(){letmutgreeting=String::from(\u0026#34;Hello\u0026#34;);letgreeting_ref=\u0026amp;greeting;thread::scope(|scoped_thread|{// spawn 3 threads fornin1..=3{// greeting_ref copied into every thread scoped_thread.spawn(move|_|{println!(\u0026#34;{} {}\u0026#34;,greeting_ref,n);// prints \u0026#34;Hello {n}\u0026#34; });}// line below could cause UB or data races but compiler rejects it greeting+=\u0026#34; world\u0026#34;;// ❌ cannot mutate greeting while immutable refs exist });// can mutate greeting after every thread has joined greeting+=\u0026#34; world\u0026#34;;// ✅ println!(\u0026#34;{}\u0026#34;,greeting);// prints \u0026#34;Hello world\u0026#34; }同样，我们可以将单个可变引用传递给一些数据到一个线程，我们可以保证不会出现数据竞争，因为 Rust 静态地保证了别名的可变引用不能存在，底层数据不能通过现有的单个可变引用以外的任何东西进行修改。\nusecrossbeam::thread;fn main(){letmutgreeting=String::from(\u0026#34;Hello\u0026#34;);letgreeting_ref=\u0026amp;mutgreeting;thread::scope(|scoped_thread|{// greeting_ref moved into thread scoped_thread.spawn(move|_|{*greeting_ref+=\u0026#34; world\u0026#34;;println!(\u0026#34;{}\u0026#34;,greeting_ref);// prints \u0026#34;Hello world\u0026#34; });// line below could cause UB or data races but compiler rejects it greeting+=\u0026#34;!!!\u0026#34;;// ❌ cannot mutate greeting while mutable refs exist });// can mutate greeting after the thread has joined greeting+=\u0026#34;!!!\u0026#34;;// ✅ println!(\u0026#34;{}\u0026#34;,greeting);// prints \u0026#34;Hello world!!!\u0026#34; }这就是为什么大多数类型都是 Sync 而不需要任何显式同步。如果我们需要在多个线程中同时修改一些数据 T，编译器不会让我们这样做，直到我们将数据包裹在 Arc\u0026lt;Mutex\u0026lt;T\u0026gt;\u0026gt; 或 Arc\u0026lt;RwLock\u0026lt;T\u0026gt;\u0026gt; 中，所以编译器强制要求在需要时使用显式同步。\nSized 预备知识\n Marker Traits Auto Traits  如果一个类型是 Sized 的，这意味着它的字节大小在编译时是已知的，并且可以将该类型的实例放在栈上。\n类型的大小和它的含义是一个微妙而又巨大的话题，它影响到语言的很多不同方面。它是如此重要，以至于我写了整整一篇文章，叫做 Sizedness in Rust，我强烈推荐任何想深入了解类型大小的人阅读。我总结一下与本文相关的几个关键内容。\n 所有的泛型类型都会得到一个隐式的 Sized 约束。  fn func\u0026lt;T\u0026gt;(t: \u0026amp;T){}// example above desugared fn func\u0026lt;T: Sized\u0026gt;(t: \u0026amp;T){}由于所有泛型类型都有一个隐式的 Sized 约束，如果我们想退出这个隐式约束，我们需要使用特殊的 \u0026ldquo;放宽约束\u0026rdquo; 语法 ?Sized，它目前只存在于 Sized trait。  // now T can be unsized fn func\u0026lt;T: ?Sized\u0026gt;(t: \u0026amp;T){}所有的 trait 都有一个隐式的 ?Sized 约束。  traitTrait{}// example above desugared traitTrait: ?Sized{}这是为了让 trait 对象可以实现 trait。同样，所有的琐碎细节都在 Sizedness in Rust 中。\nGeneral traits Default 预备知识\n Self Functions Derive Macros  traitDefault{fn default()-\u0026gt; Self;}可以构建 Default 类型的默认值。\nstruct Color{r: u8,g: u8,b: u8,}implDefaultforColor{// default color is black fn default()-\u0026gt; Self{Color{r: 0,g: 0,b: 0,}}}这对快速建立原型很有用，但在任何情况下，我们只需要一个类型的实例，而且对它是什么并不挑剔。\nfn main(){// just give me some color! letcolor=Color::default();}这也是一个我们可能想明确地暴露给我们的函数用户的选项。\nstruct Canvas;enum Shape{Circle,Rectangle,}implCanvas{// let user optionally pass a color fn paint(\u0026amp;mutself,shape: Shape,color: Option\u0026lt;Color\u0026gt;){// if no color is passed use the default color letcolor=color.unwrap_or_default();// etc }}Default 在我们需要构造泛型类型的泛型语境中也很有用。\nfn guarantee_length\u0026lt;T: Default\u0026gt;(mutvec: Vec\u0026lt;T\u0026gt;,min_len: usize)-\u0026gt; Vec\u0026lt;T\u0026gt;{for_in0..min_len.saturating_sub(vec.len()){vec.push(T::default());}vec}我们可以利用 Default 类型的另一种方式是使用 Rust 的结构体更新语法对结构体进行部分初始化。我们可以为 Color 设置一个 new 构造函数，将每个成员作为一个参数。\nimplColor{fn new(r: u8,g: u8,b: u8)-\u0026gt; Self{Color{r,g,b,}}}然而，我们也可以使用方便的构造函数，每个构造函数只接受一个特定的结构体成员，其他结构体成员则使用默认值。\nimplColor{fn red(r: u8)-\u0026gt; Self{Color{r,..Color::default()}}fn green(g: u8)-\u0026gt; Self{Color{g,..Color::default()}}fn blue(b: u8)-\u0026gt; Self{Color{b,..Color::default()}}}还有一个 Default 的派生宏，所以我们可以像这样编写 Color。\n// default color is still black // because u8::default() == 0 #[derive(Default)]struct Color{r: u8,g: u8,b: u8 }Clone 预备知识\n Self Methods Default Impls Derive Macros  traitClone{fn clone(\u0026amp;self)-\u0026gt; Self;// provided default impls fn clone_from(\u0026amp;mutself,source: \u0026amp;Self);}我们可以将 Clone 类型的不可变引用转换为自有值(owned values)，即 \u0026amp;T -\u0026gt; T。Clone 没有对这种转换的效率做出承诺，所以它可能是缓慢和昂贵的。为了快速地在一个类型上实现 Clone，我们可以使用派生宏。\n#[derive(Clone)]struct SomeType{cloneable_member1: CloneableType1,cloneable_member2: CloneableType2,// etc }// macro generates impl below implCloneforSomeType{fn clone(\u0026amp;self)-\u0026gt; Self{SomeType{cloneable_member1: self.cloneable_member1.clone(),cloneable_member2: self.cloneable_member2.clone(),// etc }}}Clone 也可以在泛型上下文中构建一个类型的实例。下面是上一节中的一个例子，除了使用 Clone 而不是 Default。\nfn guarantee_length\u0026lt;T: Clone\u0026gt;(mutvec: Vec\u0026lt;T\u0026gt;,min_len: usize,fill_with: \u0026amp;T)-\u0026gt; Vec\u0026lt;T\u0026gt;{for_in0..min_len.saturating_sub(vec.len()){vec.push(fill_with.clone());}vec}人们也经常使用克隆作为逃避的方法，以避免与借用检查器打交道。管理带有引用的结构体可能很有挑战性，但我们可以通过克隆将引用变成自有值(owned values)。\n// oof, we gotta worry about lifetimes 😟 struct SomeStruct\u0026lt;\u0026#39;a\u0026gt;{data: \u0026amp;\u0026#39;aVec\u0026lt;u8\u0026gt;,}// now we\u0026#39;re on easy street 😎 struct SomeStruct{data: Vec\u0026lt;u8\u0026gt;,}如果我们正在开发的程序的性能不是最重要的，那么我们就不需要为克隆数据而烦恼。Rust 是一种低级别的语言，暴露了很多低级别的细节，所以很容易被过早的优化所吸引，而不是真正解决手头的问题。对于许多程序来说，最好的优先顺序通常是首先建立正确性，其次是优雅性，第三是性能，只有在对程序进行剖析并确定了性能瓶颈之后才关注性能。这是很好的一般性建议，如果它不适用于你的特定程序，你就会知道。\nCopy 预备知识\n Marker Traits Subtraits \u0026amp; Supertraits Derive Macros  traitCopy: Clone {}我们复制 Copy 类型，例如：T -\u0026gt; T。Copy 承诺复制操作将是一个简单的按位(bitwise)拷贝，所以它将是非常快速和高效的。我们不能自己实现 Copy，只有编译器可以提供一个实现，但是我们可以通过使用 Copy 派生宏，以及 Clone 派生宏来告诉编译器这样做，因为 Copy 是 Clone 的一个子 trait。\n#[derive(Copy, Clone)]struct SomeType;Copy 完善了(refine) Clone。Clone 可能是缓慢和昂贵的，但 Copy 保证是快速和便宜的，所以 Copy 只是一个快速 Clone。如果一个类型实现了 Copy，这就使得 Clone 的实现变得微不足道了。\n// this is what the derive macro generates impl\u0026lt;T: Copy\u0026gt;CloneforT{// the clone method becomes just a copy fn clone(\u0026amp;self)-\u0026gt; Self{*self}}当一个类型被移动时，实现该类型的 Copy 会改变其行为。默认情况下，所有类型都有“移动语义”，但是一旦一个类型实现了 `Copy'，它就会得到“复制语义”。为了解释这两者之间的区别，我们来看看这些简单的场景。\n// a \u0026#34;move\u0026#34;, src: !Copy letdest=src;// a \u0026#34;copy\u0026#34;, src: Copy letdest=src;在这两种情况下，dest = src 对 src 的内容进行简单的按位复制，并将结果移动到 dest 中，唯一的区别是，在“移动”的情况下，借用检查器使 src 变量无效，并确保它以后不会被用于其他地方，而在“复制”的情况下，src 仍然有效并可使用。\n一言以蔽之。拷贝就“是”移动。移动就“是”拷贝。唯一的区别是借用检查器对它们的处理方式。\n关于移动的一个更具体的例子，假设 src 是一个 Vec\u0026lt;i32\u0026gt;，其内容是这样的。\n{data: *mut[i32],length: usize,capacity: usize }当我们写下 dest = src 时，我们的结果是：\nsrc={data: *mut[i32],length: usize,capacity: usize }dest={data: *mut[i32],length: usize,capacity: usize }这个时候，src 和 dest 都有对相同数据的别名可变引用，这是一个大忌，所以借用检查器使 src 变量无效，这样它就不能再被使用而不会产生编译错误。\n对于一个更具体的拷贝例子，假设 src 是一个 Option\u0026lt;i32\u0026gt;，它的内容是这样的：\n{is_valid: bool,data: i32 }现在，当我们写下 dest = src 时，我们的结果是：\nsrc={is_valid: bool,data: i32 }dest={is_valid: bool,data: i32 }这些都是可以同时使用的! 因此 Option\u0026lt;i32\u0026gt; 是可以 Copy 的。\n虽然 Copy 可以是一个自动 trait，但 Rust 语言的设计者决定让类型显式地选择复制语义，而不是在类型符合条件时默默地继承复制语义，因为后者会导致令人惊讶的混乱行为，经常导致错误。\nAny 预备知识\n Self Generic Blanket Impls Subtraits \u0026amp; Supertraits Trait Objects  traitAny: \u0026#39;static{fn type_id(\u0026amp;self)-\u0026gt; TypeId;}Rust 的多态性风格是参数化的，但如果我们想使用类似于动态类型语言的多态性风格，那么我们可以使用 Any trait 来模仿。我们不需要为我们的类型手动实现这个 trait，因为下面这个泛型覆盖实现已经覆盖了。\nimpl\u0026lt;T: \u0026#39;static+?Sized\u0026gt;AnyforT{fn type_id(\u0026amp;self)-\u0026gt; TypeId{TypeId::of::\u0026lt;T\u0026gt;()}}我们从 dyn Any 中得到 T 的方法是通过使用 downcast_ref::\u0026lt;T\u0026gt;() 和 downcast_mut::\u0026lt;T\u0026gt;() 方法。\nusestd::any::Any;#[derive(Default)]struct Point{x: i32,y: i32,}implPoint{fn inc(\u0026amp;mutself){self.x+=1;self.y+=1;}}fn map_any(mutany: Box\u0026lt;dynAny\u0026gt;)-\u0026gt; Box\u0026lt;dynAny\u0026gt;{ifletSome(num)=any.downcast_mut::\u0026lt;i32\u0026gt;(){*num+=1;}elseifletSome(string)=any.downcast_mut::\u0026lt;String\u0026gt;(){*string+=\u0026#34;!\u0026#34;;}elseifletSome(point)=any.downcast_mut::\u0026lt;Point\u0026gt;(){point.inc();}any}fn main(){letmutvec: Vec\u0026lt;Box\u0026lt;dynAny\u0026gt;\u0026gt;=vec![Box::new(0),Box::new(String::from(\u0026#34;a\u0026#34;)),Box::new(Point::default()),];// vec = [0, \u0026#34;a\u0026#34;, Point { x: 0, y: 0 }] vec=vec.into_iter().map(map_any).collect();// vec = [1, \u0026#34;a!\u0026#34;, Point { x: 1, y: 1 }] }这个 trait 很少需要使用，因为在大多数情况下，参数化多态性要优于临时多态性，后者也可以用枚举来模拟，因为枚举的类型更安全，需要的迂回更少。例如，我们可以把上面的例子写成这样。\n#[derive(Default)]struct Point{x: i32,y: i32,}implPoint{fn inc(\u0026amp;mutself){self.x+=1;self.y+=1;}}enum Stuff{Integer(i32),String(String),Point(Point),}fn map_stuff(mutstuff: Stuff)-\u0026gt; Stuff{match\u0026amp;mutstuff{Stuff::Integer(num)=\u0026gt;*num+=1,Stuff::String(string)=\u0026gt;*string+=\u0026#34;!\u0026#34;,Stuff::Point(point)=\u0026gt;point.inc(),}stuff}fn main(){letmutvec=vec![Stuff::Integer(0),Stuff::String(String::from(\u0026#34;a\u0026#34;)),Stuff::Point(Point::default()),];// vec = [0, \u0026#34;a\u0026#34;, Point { x: 0, y: 0 }] vec=vec.into_iter().map(map_stuff).collect();// vec = [1, \u0026#34;a!\u0026#34;, Point { x: 1, y: 1 }] }尽管 Any 很少被需要，但有时使用起来还是很方便的，我们将在后面的“错误处理”部分看到。\nFormatting Traits 我们可以使用 std::fmt 中的格式化宏将类型序列化为字符串，其中最著名的是 println!。我们可以将格式化参数传递给格式 str 中使用的 {} 占位符，然后用来选择使用哪个 trait 实现来序列化占位符的参数。\n   Trait Placeholder Description     Display {} display representation   Debug {:?} debug representation   Octal {:o} octal representation   LowerHex {:x} lowercase hex representation   UpperHex {:X} uppercase hex representation   Pointer {:p} memory address   Binary {:b} binary representation   LowerExp {:e} lowercase exponential representation   UpperExp {:E} uppercase exponential representation    Display \u0026amp; ToString 预备知识\n Self Methods Generic Blanket Impls  traitDisplay{fn fmt(\u0026amp;self,f: \u0026amp;mutFormatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; Result;}Display 类型可以被序列化为 String，这对程序的终端用户很友好。例如，给 Point 实现 Display:\nusestd::fmt;#[derive(Default)]struct Point{x: i32,y: i32,}implfmt::DisplayforPoint{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{write!(f,\u0026#34;({}, {})\u0026#34;,self.x,self.y)}}fn main(){println!(\u0026#34;origin: {}\u0026#34;,Point::default());// prints \u0026#34;origin: (0, 0)\u0026#34; // get Point\u0026#39;s Display representation as a String letstringified_point=format!(\u0026#34;{}\u0026#34;,Point::default());assert_eq!(\u0026#34;(0, 0)\u0026#34;,stringified_point);// ✅ }除了使用 format! 宏来获得一个类型的显示表示为 String 之外，我们还可以使用 ToString trait。\ntraitToString{fn to_string(\u0026amp;self)-\u0026gt; String;}我们没有必要自己去实现这个 trait。事实上，我们不能这样做，因为下面这个泛型覆盖实现，对于任何实现 Display 的类型，都自动实现 ToString。\nimpl\u0026lt;T: Display+?Sized\u0026gt;ToStringforT;将 ToString 与 Point 一起使用。\n#[test]// ✅ fn display_point(){letorigin=Point::default();assert_eq!(format!(\u0026#34;{}\u0026#34;,origin),\u0026#34;(0, 0)\u0026#34;);}#[test]// ✅ fn point_to_string(){letorigin=Point::default();assert_eq!(origin.to_string(),\u0026#34;(0, 0)\u0026#34;);}#[test]// ✅ fn display_equals_to_string(){letorigin=Point::default();assert_eq!(format!(\u0026#34;{}\u0026#34;,origin),origin.to_string());}Debug 预备知识\n Self Methods Derive Macros Display \u0026amp; ToString  traitDebug{fn fmt(\u0026amp;self,f: \u0026amp;mutFormatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; Result;}Debug 与 Display 有相同的签名。唯一的区别是，当我们使用 {:?} 格式符时，Debug 实现被调用。`Debug' 可以被派生。\nusestd::fmt;#[derive(Debug)]struct Point{x: i32,y: i32,}// derive macro generates impl below implfmt::DebugforPoint{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{f.debug_struct(\u0026#34;Point\u0026#34;).field(\u0026#34;x\u0026#34;,\u0026amp;self.x).field(\u0026#34;y\u0026#34;,\u0026amp;self.y).finish()}}为一个类型实现 Debug 也允许它在 dbg! 宏中使用，这比 println! 更有利于临时应急的打印日志。它的一些优点如下:\n dbg! 打印到 stderr 而不是 stdout，所以调试日志很容易与我们程序的实际 stdout 输出分开。 dbg! 打印传递给它的表达式，以及表达式所评估的值。 dbg! 拥有其参数的所有权，并返回这些参数，所以你可以在表达式中使用它。  fn some_condition()-\u0026gt; bool {true}// no logging fn example(){ifsome_condition(){// some code }}// println! logging fn example_println(){// 🤦 letresult=some_condition();println!(\u0026#34;{}\u0026#34;,result);// just prints \u0026#34;true\u0026#34; ifresult{// some code }}// dbg! logging fn example_dbg(){// 😍 ifdbg!(some_condition()){// prints \u0026#34;[src/main.rs:22] some_condition() = true\u0026#34; // some code }}唯一的缺点是，dbg! 在发布版本中不会被自动剥离，所以如果我们不想在最终的可执行文件中使用它，就必须从我们的代码中手动删除它。\nOperator Traits Rust 中的所有运算符都与 trait 相关。如果我们想为我们的类型实现运算符，就必须实现相关的 trait。\n   Trait(s) Category Operator(s) Description     Eq, PartialEq comparison == equality   Ord, PartialOrd comparison \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;= comparison   Add arithmetic + addition   AddAssign arithmetic += addition assignment   BitAnd arithmetic \u0026amp; bitwise AND   BitAndAssign arithmetic \u0026amp;= bitwise assignment   BitXor arithmetic ^ bitwise XOR   BitXorAssign arithmetic ^= bitwise XOR assignment   Div arithmetic / division   DivAssign arithmetic /= division assignment   Mul arithmetic * multiplication   MulAssign arithmetic *= multiplication assignment   Neg arithmetic - unary negation   Not arithmetic ! unary logical negation   Rem arithmetic % remainder   RemAssign arithmetic %= remainder assignment   Shl arithmetic \u0026lt;\u0026lt; left shift   ShlAssign arithmetic \u0026lt;\u0026lt;= left shift assignment   Shr arithmetic \u0026gt;\u0026gt; right shift   ShrAssign arithmetic \u0026gt;\u0026gt;= right shift assignment   Sub arithmetic - subtraction   SubAssign arithmetic -= subtraction assignment   Fn closure (...args) immutable closure invocation   FnMut closure (...args) mutable closure invocation   FnOnce closure (...args) one-time closure invocation   Deref other * immutable dereference   DerefMut other * mutable derenence   Drop other - type destructor   Index other [] immutable index   IndexMut other [] mutable index   RangeBounds other .. range    Comparison Traits    Trait(s) Category Operator(s) Description     Eq, PartialEq comparison == equality   Ord, PartialOrd comparison \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;= comparison    PartialEq \u0026amp; Eq 预备知识\n Self Methods Generic Parameters Default Impls Generic Blanket Impls Marker Traits Subtraits \u0026amp; Supertraits Sized  traitPartialEq\u0026lt;Rhs=Self\u0026gt;whereRhs: ?Sized,{fn eq(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;// provided default impls fn ne(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;}PartialEq\u0026lt;Rhs\u0026gt; 类型可以使用 == 运算符检查是否与 Rhs 类型相等。\n所有的 PartialEq\u0026lt;Rhs\u0026gt; 实现必须确保相等是对称的和传递的。这意味着对于所有的 a, b, 和 c:\n a == b 意味着 b == a (对称性) a == b \u0026amp;\u0026amp; b == c 意味着 a == c (传递性)  默认情况下 Rhs = Self，因为我们几乎总是想把一个类型的实例相互比较，而不是与不同类型的实例比较。这也自动保证了我们的实现是对称的和传递的。\nstruct Point{x: i32,y: i32 }// Rhs == Self == Point implPartialEqforPoint{// impl automatically symmetric \u0026amp; transitive fn eq(\u0026amp;self,other: \u0026amp;Point)-\u0026gt; bool {self.x==other.x\u0026amp;\u0026amp;self.y==other.y}}如果一个类型的所有成员都实现了 `PartialEq'，那么它可以被派生。\n#[derive(PartialEq)]struct Point{x: i32,y: i32 }#[derive(PartialEq)]enum Suit{Spade,Heart,Club,Diamond,}一旦为我们的类型实现了 PartialEq，我们也可以免费得到我们类型的引用之间的相等性比较，这要感谢这些泛型覆盖实现。\n// this impl only gives us: Point == Point #[derive(PartialEq)]struct Point{x: i32,y: i32 }// all of the generic blanket impls below // are provided by the standard library // this impl gives us: \u0026amp;Point == \u0026amp;Point impl\u0026lt;A,B\u0026gt;PartialEq\u0026lt;\u0026amp;\u0026#39;_B\u0026gt;for\u0026amp;\u0026#39;_AwhereA: PartialEq\u0026lt;B\u0026gt;+?Sized,B: ?Sized;// this impl gives us: \u0026amp;mut Point == \u0026amp;Point impl\u0026lt;A,B\u0026gt;PartialEq\u0026lt;\u0026amp;\u0026#39;_B\u0026gt;for\u0026amp;\u0026#39;_mutAwhereA: PartialEq\u0026lt;B\u0026gt;+?Sized,B: ?Sized;// this impl gives us: \u0026amp;Point == \u0026amp;mut Point impl\u0026lt;A,B\u0026gt;PartialEq\u0026lt;\u0026amp;\u0026#39;_mutB\u0026gt;for\u0026amp;\u0026#39;_AwhereA: PartialEq\u0026lt;B\u0026gt;+?Sized,B: ?Sized;// this impl gives us: \u0026amp;mut Point == \u0026amp;mut Point impl\u0026lt;A,B\u0026gt;PartialEq\u0026lt;\u0026amp;\u0026#39;_mutB\u0026gt;for\u0026amp;\u0026#39;_mutAwhereA: PartialEq\u0026lt;B\u0026gt;+?Sized,B: ?Sized;由于这个 trait 是泛型化的，我们可以定义不同类型之间的相等性。标准库利用这一点，允许检查许多类似字符串的类型，如String、\u0026amp;str、PathBuf、\u0026amp;Path、OsString、\u0026amp;OsStr 等之间的相等性。\n一般来说，我们只应该在不同类型之间实现相等性关系，如果它们实现同一种数据，并且类型之间的唯一区别是它们如何表示数据或如何允许与数据进行交互。\n这里有一个可爱但糟糕的例子，说明有人可能会被诱惑实现 PartialEq 来检查不符合上述标准的不同类型之间的相等。\n#[derive(PartialEq)]enum Suit{Spade,Club,Heart,Diamond,}#[derive(PartialEq)]enum Rank{Ace,Two,Three,Four,Five,Six,Seven,Eight,Nine,Ten,Jack,Queen,King,}#[derive(PartialEq)]struct Card{suit: Suit,rank: Rank,}// check equality of Card\u0026#39;s suit implPartialEq\u0026lt;Suit\u0026gt;forCard{fn eq(\u0026amp;self,other: \u0026amp;Suit)-\u0026gt; bool {self.suit==*other}}// check equality of Card\u0026#39;s rank implPartialEq\u0026lt;Rank\u0026gt;forCard{fn eq(\u0026amp;self,other: \u0026amp;Rank)-\u0026gt; bool {self.rank==*other}}fn main(){letAceOfSpades=Card{suit: Suit::Spade,rank: Rank::Ace,};assert!(AceOfSpades==Suit::Spade);// ✅ assert!(AceOfSpades==Rank::Ace);// ✅ }这很有效，而且有点道理。一张黑桃A的牌既是A又是黑桃，如果我们要写一个处理扑克牌的库，那么我们想让它简单方便地单独检查一张牌的花色和等级是合理的。然而，我们还缺少一些东西：对称性。 我们可以 Card == Suit 和 Card == Rank，但我们不能 Suit == Card 或 Rank == Card，所以让我们解决这个问题。\n// check equality of Card\u0026#39;s suit implPartialEq\u0026lt;Suit\u0026gt;forCard{fn eq(\u0026amp;self,other: \u0026amp;Suit)-\u0026gt; bool {self.suit==*other}}// added for symmetry implPartialEq\u0026lt;Card\u0026gt;forSuit{fn eq(\u0026amp;self,other: \u0026amp;Card)-\u0026gt; bool {*self==other.suit}}// check equality of Card\u0026#39;s rank implPartialEq\u0026lt;Rank\u0026gt;forCard{fn eq(\u0026amp;self,other: \u0026amp;Rank)-\u0026gt; bool {self.rank==*other}}// added for symmetry implPartialEq\u0026lt;Card\u0026gt;forRank{fn eq(\u0026amp;self,other: \u0026amp;Card)-\u0026gt; bool {*self==other.rank}}我们有对称性! 太好了。增加对称性只是打破了传递性！这是不可能的。哎呀。现在可以这样了：\nfn main(){// Ace of Spades leta=Card{suit: Suit::Spade,rank: Rank::Ace,};letb=Suit::Spade;// King of Spades letc=Card{suit: Suit::Spade,rank: Rank::King,};assert!(a==b\u0026amp;\u0026amp;b==c);// ✅ assert!(a==c);// ❌ }实现 PartialEq 以检查不同类型之间的相等关系的一个好例子是一个处理距离的程序，它使用不同的类型来代表不同的测量单位。\n#[derive(PartialEq)]struct Foot(u32);#[derive(PartialEq)]struct Yard(u32);#[derive(PartialEq)]struct Mile(u32);implPartialEq\u0026lt;Mile\u0026gt;forFoot{fn eq(\u0026amp;self,other: \u0026amp;Mile)-\u0026gt; bool {self.0==other.0*5280}}implPartialEq\u0026lt;Foot\u0026gt;forMile{fn eq(\u0026amp;self,other: \u0026amp;Foot)-\u0026gt; bool {self.0*5280==other.0}}implPartialEq\u0026lt;Mile\u0026gt;forYard{fn eq(\u0026amp;self,other: \u0026amp;Mile)-\u0026gt; bool {self.0==other.0*1760}}implPartialEq\u0026lt;Yard\u0026gt;forMile{fn eq(\u0026amp;self,other: \u0026amp;Yard)-\u0026gt; bool {self.0*1760==other.0}}implPartialEq\u0026lt;Foot\u0026gt;forYard{fn eq(\u0026amp;self,other: \u0026amp;Foot)-\u0026gt; bool {self.0*3==other.0}}implPartialEq\u0026lt;Yard\u0026gt;forFoot{fn eq(\u0026amp;self,other: \u0026amp;Yard)-\u0026gt; bool {self.0==other.0*3}}fn main(){leta=Foot(5280);letb=Yard(1760);letc=Mile(1);// symmetry assert!(a==b\u0026amp;\u0026amp;b==a);// ✅ assert!(b==c\u0026amp;\u0026amp;c==b);// ✅ assert!(a==c\u0026amp;\u0026amp;c==a);// ✅ // transitivity assert!(a==b\u0026amp;\u0026amp;b==c\u0026amp;\u0026amp;a==c);// ✅ assert!(c==b\u0026amp;\u0026amp;b==a\u0026amp;\u0026amp;c==a);// ✅ }Eq 是一个标记 trait，是 PartialEq\u0026lt;Self\u0026gt; 的子 trait。\ntraitEq: PartialEq\u0026lt;Self\u0026gt;{}如果我们为一个类型实现 Eq，在 PartialEq 所要求的对称性和传递性的基础上，我们还保证了自反性，即 对所有 a, a == a。在这个意义上，Eq 完善了 PartialEq，因为它代表了一个更严格的相等性版本。如果一个类型的所有成员都是Eq 的，那么 Eq 实现就可以为该类型派生。\n浮点类型是 PartialEq 的，但不是 Eq 的，因为 NaN != NaN。几乎所有其他的 PartialEq 类型都是 Eq，当然，除非它们包含浮点。\n一旦一个类型实现了 PartialEq 和 Debug，我们就可以在 assert_eq! 宏中使用它。我们也可以比较 PartialEq 类型的集合。\n#[derive(PartialEq, Debug)]struct Point{x: i32,y: i32,}fn example_assert(p1: Point,p2: Point){assert_eq!(p1,p2);}fn example_compare_collections\u0026lt;T: PartialEq\u0026gt;(vec1: Vec\u0026lt;T\u0026gt;,vec2: Vec\u0026lt;T\u0026gt;){// if T: PartialEq this now works! ifvec1==vec2{// some code }else{// other code }}Hash 预备知识\n Self Methods Generic Parameters Default Impls Derive Macros PartialEq \u0026amp; Eq  traitHash{fn hash\u0026lt;H: Hasher\u0026gt;(\u0026amp;self,state: \u0026amp;mutH);// provided default impls fn hash_slice\u0026lt;H: Hasher\u0026gt;(data: \u0026amp;[Self],state: \u0026amp;mutH);}这个 trait 与任何运算符无关，但谈论它的最好时机是在 PartialEq \u0026amp; Eq 之后，所以它在这里。Hash 类型可以使用 Hasher 进行散列。\nusestd:#️⃣:Hasher;usestd:#️⃣:Hash;struct Point{x: i32,y: i32,}implHashforPoint{fn hash\u0026lt;H: Hasher\u0026gt;(\u0026amp;self,hasher: \u0026amp;mutH){hasher.write_i32(self.x);hasher.write_i32(self.y);}}有一个派生宏，它生成的实现与上述相同。\n#[derive(Hash)]struct Point{x: i32,y: i32,}如果一个类型同时实现了 Hash 和 Eq，这些实现必须相互一致，即对于所有的 a 和 b，如果 a == b，那么 a.hash() == b.hash()。所以我们应该总是使用派生宏来实现两者，或者手动实现两者，但不能混合使用，否则就有可能破坏上述不变性。\n为一个类型实现 Eq 和 Hash 的主要好处是，它允许我们将该类型作为键存储在 HashMap 和 HashSet 中。\nusestd::collections::HashSet;// now our type can be stored // in HashSets and HashMaps! #[derive(PartialEq, Eq, Hash)]struct Point{x: i32,y: i32,}fn example_hashset(){letmutpoints=HashSet::new();points.insert(Point{x: 0,y: 0});// ✅ }PartialOrd \u0026amp; Ord 预备知识\n Self Methods Generic Parameters Default Impls Subtraits \u0026amp; Supertraits Derive Macros Sized PartialEq \u0026amp; Eq  enum Ordering{Less,Equal,Greater,}traitPartialOrd\u0026lt;Rhs=Self\u0026gt;: PartialEq\u0026lt;Rhs\u0026gt;whereRhs: ?Sized,{fn partial_cmp(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; Option\u0026lt;Ordering\u0026gt;;// provided default impls fn lt(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;fn le(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;fn gt(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;fn ge(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;}PartialOrd\u0026lt;Rhs\u0026gt; 类型可以使用 \u0026lt;, \u0026lt;=, \u0026gt;, 和 \u0026gt;= 运算符与 Rhs 类型进行比较。\n所有的 PartialOrd 实现必须确保比较是不对称的和传递的。这意味着对于所有的 a, b, 和 c:\n a \u0026lt; b 意味着 !(a \u0026gt; b) (不对称性) a \u0026lt; b \u0026amp;\u0026amp; b \u0026lt; c 意味着 a \u0026lt; c (传递性)  PartialOrd 是 PartialEq 的一个子 trait，它们的实现必须总是相互一致。\nfn must_always_agree\u0026lt;T: PartialOrd +PartialEq\u0026gt;(t1: T,t2: T){assert_eq!(t1.partial_cmp(\u0026amp;t2)==Some(Ordering::Equal),t1==t2);}PartialOrd 是对 PartialEq 的细化，当比较 PartialEq 类型时，我们可以检查它们是否相等，但当比较 PartialOrd 类型时，我们可以检查它们是否相等，如果它们不相等，我们可以检查它们是否不相等，因为第一项小于或大于第二项。\n默认情况下 Rhs = Self，因为我们几乎总是想把一个类型的实例相互比较，而不是和不同类型的实例比较。这也自动保证了我们的实现是对称的和传递的。\nusestd::cmp::Ordering;#[derive(PartialEq, PartialOrd)]struct Point{x: i32,y: i32 }// Rhs == Self == Point implPartialOrdforPoint{// impl automatically symmetric \u0026amp; transitive fn partial_cmp(\u0026amp;self,other: \u0026amp;Point)-\u0026gt; Option\u0026lt;Ordering\u0026gt;{Some(matchself.x.cmp(\u0026amp;other.x){Ordering::Equal=\u0026gt;self.y.cmp(\u0026amp;other.y),ordering=\u0026gt;ordering,})}}如果一个类型的所有成员都实现了 PartialOrd，那么它可以被派生。\n#[derive(PartialEq, PartialOrd)]struct Point{x: i32,y: i32,}#[derive(PartialEq, PartialOrd)]enum Stoplight{Red,Yellow,Green,}PartialOrd 派生宏基于其成员的字母顺序对类型进行排序。\n// generates PartialOrd impl which orders // Points based on x member first and // y member second because that\u0026#39;s the order // they appear in the source code #[derive(PartialOrd, PartialEq)]struct Point{x: i32,y: i32,}// generates DIFFERENT PartialOrd impl // which orders Points based on y member // first and x member second #[derive(PartialOrd, PartialEq)]struct Point{y: i32,x: i32,}Ord is a subtrait of Eq and PartialOrd\u0026lt;Self\u0026gt;: Ord 是 Eq 和 PartialOrd\u0026lt;Self\u0026gt; 的子 trait。\ntraitOrd: Eq +PartialOrd\u0026lt;Self\u0026gt;{fn cmp(\u0026amp;self,other: \u0026amp;Self)-\u0026gt; Ordering;// provided default impls fn max(self,other: Self)-\u0026gt; Self;fn min(self,other: Self)-\u0026gt; Self;fn clamp(self,min: Self,max: Self)-\u0026gt; Self;}如果我们为一个类型实现 Ord，在 PartialOrd 所要求的不对称性和传递性的基础上，我们还保证不对称性是完全的，即对于任何给定的 a 和 b，a == b 或 a \u0026gt; b 中只有一个是真的。在这个意义上，Ord 完善了 Eq 和 PartialOrd，因为它代表了一个更严格的比较版本。如果一个类型实现了 Ord，我们就可以用这个实现来实现 PartialOrd、PartialEq 和 Eq。\nusestd::cmp::Ordering;// of course we can use the derive macros here #[derive(Ord, PartialOrd, Eq, PartialEq)]struct Point{x: i32,y: i32,}// note: as with PartialOrd, the Ord derive macro // orders a type based on the lexicographical order // of its members // but here\u0026#39;s the impls if we wrote them out by hand implOrdforPoint{fn cmp(\u0026amp;self,other: \u0026amp;Self)-\u0026gt; Ordering{matchself.x.cmp(\u0026amp;self.y){Ordering::Equal=\u0026gt;self.y.cmp(\u0026amp;self.y),ordering=\u0026gt;ordering,}}}implPartialOrdforPoint{fn partial_cmp(\u0026amp;self,other: \u0026amp;Self)-\u0026gt; Option\u0026lt;Ordering\u0026gt;{Some(self.cmp(other))}}implPartialEqforPoint{fn eq(\u0026amp;self,other: \u0026amp;Self)-\u0026gt; bool {self.cmp(other)==Ordering::Equal}}implEqforPoint{}浮点数实现了 PartialOrd，但不是 Ord，因为 NaN \u0026lt; 0 == false 和 NaN \u0026gt;= 0 == false 同时为真。几乎所有其他的 PartialOrd 类型都是 Ord，当然，除非它们包含浮点数。\n一旦一个类型被认为是 Ord 的，我们就可以将其存储在 BTreeMap 和 BTreeSet 中，并且可以使用 sort() 方法对其进行排序，以及对数组、Vec 和 VecDeque 等任何类型的切片进行解引用。\nusestd::collections::BTreeSet;// now our type can be stored // in BTreeSets and BTreeMaps! #[derive(Ord, PartialOrd, PartialEq, Eq)]struct Point{x: i32,y: i32,}fn example_btreeset(){letmutpoints=BTreeSet::new();points.insert(Point{x: 0,y: 0});// ✅ }// we can also .sort() Ord types in collections! fn example_sort\u0026lt;T: Ord\u0026gt;(mutsortable: Vec\u0026lt;T\u0026gt;)-\u0026gt; Vec\u0026lt;T\u0026gt;{sortable.sort();sortable}Arithmetic Traits    Trait(s) Category Operator(s) Description     Add arithmetic + addition   AddAssign arithmetic += addition assignment   BitAnd arithmetic \u0026amp; bitwise AND   BitAndAssign arithmetic \u0026amp;= bitwise assignment   BitXor arithmetic ^ bitwise XOR   BitXorAssign arithmetic ^= bitwise XOR assignment   Div arithmetic / division   DivAssign arithmetic /= division assignment   Mul arithmetic * multiplication   MulAssign arithmetic *= multiplication assignment   Neg arithmetic - unary negation   Not arithmetic ! unary logical negation   Rem arithmetic % remainder   RemAssign arithmetic %= remainder assignment   Shl arithmetic \u0026lt;\u0026lt; left shift   ShlAssign arithmetic \u0026lt;\u0026lt;= left shift assignment   Shr arithmetic \u0026gt;\u0026gt; right shift   ShrAssign arithmetic \u0026gt;\u0026gt;= right shift assignment   Sub arithmetic - subtraction   SubAssign arithmetic -= subtraction assignment    仔细研究所有这些将是非常多余的。反正大多数只适用于数字类型。我们只讨论 Add 和 AddAssign，因为 + 操作符通常被重载来做其他事情，如向集合添加项目或将事物串联起来，这样我们就能覆盖最有趣的地方，而不会重复。\nAdd \u0026amp; AddAssign 预备知识\n Self Methods Associated Types Generic Parameters Generic Types vs Associated Types Derive Macros  traitAdd\u0026lt;Rhs=Self\u0026gt;{type Output;fn add(self,rhs: Rhs)-\u0026gt; Self::Output;}Add\u0026lt;Rhs, Output = T\u0026gt; 类型可以和 Rhs 类型相加，并将产生 T 作为输出。\n例子 Add\u0026lt;Point, Output = Point\u0026gt; 是针对 Point 实现的。\n#[derive(Clone, Copy)]struct Point{x: i32,y: i32,}implAddforPoint{type Output=Point;fn add(self,rhs: Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}fn main(){letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letp3=p1+p2;assert_eq!(p3.x,p1.x+p2.x);// ✅ assert_eq!(p3.y,p1.y+p2.y);// ✅ }但是如果我们只有对 Point 的引用呢？那我们还能让它们相加吗？让我们试试。\nfn main(){letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letp3=\u0026amp;p1+\u0026amp;p2;// ❌ }不幸的是没有。编译器会抛出异常：\nerror[E0369]: cannot add `\u0026amp;Point` to `\u0026amp;Point`\r--\u0026gt; src/main.rs:50:25\r|\r50 | let p3: Point = \u0026amp;p1 + \u0026amp;p2;\r| --- ^ --- \u0026amp;Point\r| |\r| \u0026amp;Point\r|\r= note: an implementation of `std::ops::Add` might be missing for `\u0026amp;Point`\r在 Rust 的类型系统中，对于某些类型 T 来说，T、\u0026amp;T 和 \u0026amp;mut T 都被视为唯一的不同类型，这意味着我们必须为它们分别提供 trait 实现。让我们为 \u0026amp;Point 定义一个 Add 实现：\nimplAddfor\u0026amp;Point{type Output=Point;fn add(self,rhs: \u0026amp;Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}fn main(){letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letp3=\u0026amp;p1+\u0026amp;p2;// ✅ assert_eq!(p3.x,p1.x+p2.x);// ✅ assert_eq!(p3.y,p1.y+p2.y);// ✅ }然而，有些事情还是感觉不大对劲。我们有两个独立的 Add 实现，分别用于 Point 和 \u0026amp;Point，它们目前做的是同样的事情，但不能保证将来也会这样做。例如，我们决定当我们把两个 Point 相加时，我们想创建一个包含这两个 Point 的 Line，而不是创建一个新的 Point，我们会像这样更新我们的 Add 程序：\nusestd::ops::Add;#[derive(Copy, Clone)]struct Point{x: i32,y: i32,}#[derive(Copy, Clone)]struct Line{start: Point,end: Point,}// we updated this impl implAddforPoint{type Output=Line;fn add(self,rhs: Point)-\u0026gt; Line{Line{start: self,end: rhs,}}}// but forgot to update this impl, uh oh! implAddfor\u0026amp;Point{type Output=Point;fn add(self,rhs: \u0026amp;Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}fn main(){letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letline: Line=p1+p2;// ✅ letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letline: Line=\u0026amp;p1+\u0026amp;p2;// ❌ expected Line, found Point }我们目前对 \u0026amp;Point 的 Add 实现造成了不必要的维护负担，我们希望这个实现与 Point 的实现相匹配，而不必在每次改变 Point 的实现时都要手动更新。我们希望尽可能地保持我们的代码是 DRY（Don\u0026rsquo;t Repeat Yourself）。幸运的是这是可以实现的。\n// updated, DRY impl implAddfor\u0026amp;Point{type Output=\u0026lt;PointasAdd\u0026gt;::Output;fn add(self,rhs: \u0026amp;Point)-\u0026gt; Self::Output{Point::add(*self,*rhs)}}fn main(){letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letline: Line=p1+p2;// ✅ letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letline: Line=\u0026amp;p1+\u0026amp;p2;// ✅ }AddAssign\u0026lt;Rhs\u0026gt; 类型允许我们相加并分配 Rhs 类型给它们。Trait 声明如下：\ntraitAddAssign\u0026lt;Rhs=Self\u0026gt;{fn add_assign(\u0026amp;mutself,rhs: Rhs);}为 Point 和 \u0026amp;Point 类型的实现的例子如下：\nusestd::ops::AddAssign;#[derive(Copy, Clone)]struct Point{x: i32,y: i32 }implAddAssignforPoint{fn add_assign(\u0026amp;mutself,rhs: Point){self.x+=rhs.x;self.y+=rhs.y;}}implAddAssign\u0026lt;\u0026amp;Point\u0026gt;forPoint{fn add_assign(\u0026amp;mutself,rhs: \u0026amp;Point){Point::add_assign(self,*rhs);}}fn main(){letmutp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};p1+=\u0026amp;p2;p1+=p2;assert!(p1.x==7\u0026amp;\u0026amp;p1.y==10);}闭包 Traits    Trait(s) Category Operator(s) Description     Fn closure (...args) immutable closure invocation   FnMut closure (...args) mutable closure invocation   FnOnce closure (...args) one-time closure invocation    FnOnce, FnMut, \u0026amp; Fn 预备知识\n Self Methods Associated Types Generic Parameters Generic Types vs Associated Types Subtraits \u0026amp; Supertraits  traitFnOnce\u0026lt;Args\u0026gt;{type Output;fn call_once(self,args: Args)-\u0026gt; Self::Output;}traitFnMut\u0026lt;Args\u0026gt;: FnOnce\u0026lt;Args\u0026gt;{fn call_mut(\u0026amp;mutself,args: Args)-\u0026gt; Self::Output;}traitFn\u0026lt;Args\u0026gt;: FnMut\u0026lt;Args\u0026gt;{fn call(\u0026amp;self,args: Args)-\u0026gt; Self::Output;}虽然这些 trait 存在，但在稳定的 Rust 中，我们不可能为自己的类型实现这些特性。我们唯一能创建的实现这些 trait 的类型是闭包。根据闭包从其环境中捕获的内容，决定了它是实现了 FnOnce、FnMut 还是 Fn。\nFnOnce 闭包只能被调用一次，因为它在执行中会消耗一些值。\nfn main(){letrange=0..10;letget_range_count=||range.count();assert_eq!(get_range_count(),10);// ✅ get_range_count();// ❌ }迭代器上的 .count() 方法会消耗迭代器，所以它只能被调用一次。因此，我们的闭包只能被调用一次。这就是为什么当我们试图第二次调用它时，会出现这个错误。\nerror[E0382]: use of moved value: `get_range_count`\r--\u0026gt; src/main.rs:5:5\r|\r4 | assert_eq!(get_range_count(), 10);\r| ----------------- `get_range_count` moved due to this call\r5 | get_range_count();\r| ^^^^^^^^^^^^^^^ value used here after move\r|\rnote: closure cannot be invoked more than once because it moves the variable `range` out of its environment\r--\u0026gt; src/main.rs:3:30\r|\r3 | let get_range_count = || range.count();\r| ^^^^^\rnote: this value implements `FnOnce`, which causes it to be moved when called\r--\u0026gt; src/main.rs:4:16\r|\r4 | assert_eq!(get_range_count(), 10);\r| ^^^^^^^^^^^^^^^\rFnMut 闭包可以被多次调用，也可以改变它从环境中捕获的变量。我们可以说 FnMut 闭包是执行副作用的，或者说是有状态的。下面是一个闭包的例子，它通过跟踪到目前为止看到的最小值，从迭代器中过滤出所有非升序的值。\nfn main(){letnums=vec![0,4,2,8,10,7,15,18,13];letmutmin=i32::MIN;letascending=nums.into_iter().filter(|\u0026amp;n|{ifn\u0026lt;=min{false}else{min=n;true}}).collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;();assert_eq!(vec![0,4,8,10,15,18],ascending);// ✅ }FnMut 完善了 FnOnce，即 FnOnce 需要取得其参数的所有权，只能调用一次，但 FnMut 只需要取得可变的引用，可以多次调用。FnMut 可以在任何可以使用 FnOnce 的地方使用。\nFn 闭包可以被多次调用，并且不改变它从环境中捕获的任何变量。我们可以说 Fn 闭包没有副作用或无状态。下面是一个闭包的例子，它过滤掉了所有小于它从环境中捕获的迭代器中的某个栈变量的值。\nfn main(){letnums=vec![0,4,2,8,10,7,15,18,13];letmin=9;letgreater_than_9=nums.into_iter().filter(|\u0026amp;n|n\u0026gt;min).collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;();assert_eq!(vec![10,15,18,13],greater_than_9);// ✅ }Fn 细化了 FnMut，即 FnMut 需要可变的引用并可多次调用，但 Fn 只需要不可变的引用并可多次调用。Fn 可以用在任何可以使用 FnMut 的地方，包括可以使用 FnOnce 的地方。\n如果一个闭包没有从它的环境中捕获任何东西，那么从技术上讲，它不是一个闭包，而只是一个匿名声明的内联函数，并且可以作为一个普通的函数指针被转换、使用和传递，也就是 fn。函数指针可以在任何可以使用 Fn 的地方使用，这包括可以使用 FnMut 和 FnOnce 的地方。\nfn add_one(x: i32)-\u0026gt; i32 {x+1}fn main(){letmutfn_ptr: fn(i32)-\u0026gt; i32 =add_one;assert_eq!(fn_ptr(1),2);// ✅ // capture-less closure cast to fn pointer fn_ptr=|x|x+1;// same as add_one assert_eq!(fn_ptr(1),2);// ✅ }传递普通函数指针以代替闭包的例子:\nfn main(){letnums=vec![-1,1,-2,2,-3,3];letabsolutes: Vec\u0026lt;i32\u0026gt;=nums.into_iter().map(i32::abs).collect();assert_eq!(vec![1,1,2,2,3,3],absolutes);// ✅ }其他 Trait    Trait(s) Category Operator(s) Description     Deref other * immutable dereference   DerefMut other * mutable derenence   Drop other - type destructor   Index other [] immutable index   IndexMut other [] mutable index   RangeBounds other .. range    Deref \u0026amp; DerefMut 预备知识\n Self Methods Associated Types Subtraits \u0026amp; Supertraits Sized  traitDeref{type Target: ?Sized;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Self::Target;}traitDerefMut: Deref{fn deref_mut(\u0026amp;mutself)-\u0026gt; \u0026amp;mutSelf::Target;}可以使用解引用操作符 * 把实现了 Deref\u0026lt;Target = T\u0026gt; trait 的类型解引用为 T 类型。这对于智能指针类型（如 Box 和 Rc）有明显的用途。然而，我们很少看到在 Rust 代码中显式地使用解引用操作符，这是因为 Rust 的一个叫做 “强制解引用”(deref coercion)的特性。\n当类型作为函数参数传递、从函数返回或作为方法调用的一部分使用时，Rust 会自动地对类型进行解引用。这就是为什么我们可以将 \u0026amp;String 和 \u0026amp;Vec\u0026lt;T\u0026gt; 传递给期望 \u0026amp;str 和 \u0026amp;[T] 的函数的原因，因为 String 实现了 Deref\u0026lt;Target = str\u0026gt;, Vec\u0026lt;T\u0026gt; 实现了 Deref\u0026lt;Target = [T]\u0026gt;。\nDeref 和 DerefMut 只应为智能指针类型而实现。人们试图误用和滥用这些 trait 的最常见方式是试图将某种 OOP 式的数据继承塞进 Rust 中。这是行不通的。Rust 不是面向对象的。让我们来看看几种不同的情况，在哪些情况下，如何以及为什么它不起作用。让我们从这个例子开始:\nusestd::ops::Deref;struct Human{health_points: u32,}enum Weapon{Spear,Axe,Sword,}// a Soldier is just a Human with a Weapon struct Soldier{human: Human,weapon: Weapon,}implDerefforSoldier{type Target=Human;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.human}}enum Mount{Horse,Donkey,Cow,}// a Knight is just a Soldier with a Mount struct Knight{soldier: Soldier,mount: Mount,}implDerefforKnight{type Target=Soldier;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Soldier{\u0026amp;self.soldier}}enum Spell{MagicMissile,FireBolt,ThornWhip,}// a Mage is just a Human who can cast Spells struct Mage{human: Human,spells: Vec\u0026lt;Spell\u0026gt;,}implDerefforMage{type Target=Human;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.human}}enum Staff{Wooden,Metallic,Plastic,}// a Wizard is just a Mage with a Staff struct Wizard{mage: Mage,staff: Staff,}implDerefforWizard{type Target=Mage;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Mage{\u0026amp;self.mage}}fn borrows_human(human: \u0026amp;Human){}fn borrows_soldier(soldier: \u0026amp;Soldier){}fn borrows_knight(knight: \u0026amp;Knight){}fn borrows_mage(mage: \u0026amp;Mage){}fn borrows_wizard(wizard: \u0026amp;Wizard){}fn example(human: Human,soldier: Soldier,knight: Knight,mage: Mage,wizard: Wizard){// all types can be used as Humans borrows_human(\u0026amp;human);borrows_human(\u0026amp;soldier);borrows_human(\u0026amp;knight);borrows_human(\u0026amp;mage);borrows_human(\u0026amp;wizard);// Knights can be used as Soldiers borrows_soldier(\u0026amp;soldier);borrows_soldier(\u0026amp;knight);// Wizards can be used as Mages borrows_mage(\u0026amp;mage);borrows_mage(\u0026amp;wizard);// Knights \u0026amp; Wizards passed as themselves borrows_knight(\u0026amp;knight);borrows_wizard(\u0026amp;wizard);}因此，乍一看，上面的内容看起来很不错！然而，在仔细研究后，很快就发现了问题。首先，强制解引用只对引用起作用，所以当我们真正想要传递所有权时，它就不起作用了。\nfn takes_human(human: Human){}fn example(human: Human,soldier: Soldier,knight: Knight,mage: Mage,wizard: Wizard){// all types CANNOT be used as Humans takes_human(human);takes_human(soldier);// ❌ takes_human(knight);// ❌ takes_human(mage);// ❌ takes_human(wizard);// ❌ }此外，强制解引用在泛型上下文中不起作用。比方说，我们只在人类身上实现一些 trait：\ntraitRest{fn rest(\u0026amp;self);}implRestforHuman{fn rest(\u0026amp;self){}}fn take_rest\u0026lt;T: Rest\u0026gt;(rester: \u0026amp;T){rester.rest()}fn example(human: Human,soldier: Soldier,knight: Knight,mage: Mage,wizard: Wizard){// all types CANNOT be used as Rest types, only Human take_rest(\u0026amp;human);take_rest(\u0026amp;soldier);// ❌ take_rest(\u0026amp;knight);// ❌ take_rest(\u0026amp;mage);// ❌ take_rest(\u0026amp;wizard);// ❌ }另外，尽管强制解引用在很多地方都能工作，但并不是所有地方都能工作。它对操作数不起作用，尽管操作符只是方法调用的语法糖。比方说，我们想让 Mage(法师) 用 += 运算符来学习 Spell(咒语)，这很可爱。\nimplDerefMutforWizard{fn deref_mut(\u0026amp;mutself)-\u0026gt; \u0026amp;mutMage{\u0026amp;mutself.mage}}implAddAssign\u0026lt;Spell\u0026gt;forMage{fn add_assign(\u0026amp;mutself,spell: Spell){self.spells.push(spell);}}fn example(mutmage: Mage,mutwizard: Wizard,spell: Spell){mage+=spell;wizard+=spell;// ❌ wizard not coerced to mage here wizard.add_assign(spell);// oof, we have to call it like this 🤦 }在 OOP 式数据继承的语言中，方法中 self 的值总是等于调用该方法的类型，但在 Rust 中，self 的值总是等于实现该方法的类型。\nstruct Human{profession: \u0026amp;\u0026#39;staticstr,health_points: u32,}implHuman{// self will always be a Human here, even if we call it on a Soldier fn state_profession(\u0026amp;self){println!(\u0026#34;I\u0026#39;m a {}!\u0026#34;,self.profession);}}struct Soldier{profession: \u0026amp;\u0026#39;staticstr,human: Human,weapon: Weapon,}fn example(soldier: \u0026amp;Soldier){assert_eq!(\u0026#34;servant\u0026#34;,soldier.human.profession);assert_eq!(\u0026#34;spearman\u0026#34;,soldier.profession);soldier.human.state_profession();// prints \u0026#34;I\u0026#39;m a servant!\u0026#34; soldier.state_profession();// still prints \u0026#34;I\u0026#39;m a servant!\u0026#34; 🤦 }当在一个新类型上实现 Deref 或 DerefMut 时，上述的问题尤其严重。假设我们想创建一个 SortedVec 类型，它只是一个 Vec，但它总是按排序顺序排列。下面是我们如何做的。\nstruct SortedVec\u0026lt;T: Ord\u0026gt;(Vec\u0026lt;T\u0026gt;);impl\u0026lt;T: Ord\u0026gt;SortedVec\u0026lt;T\u0026gt;{fn new(mutvec: Vec\u0026lt;T\u0026gt;)-\u0026gt; Self{vec.sort();SortedVec(vec)}fn push(\u0026amp;mutself,t: T){self.0.push(t);self.0.sort();}}很明显，我们不能在这里实现 DerefMut\u0026lt;Target = Vec\u0026lt;T\u0026gt;\u0026gt;，否则任何使用 SortedVec 的人都可以轻易地破坏排序的顺序。然而，实现 Deref\u0026lt;Target = Vec\u0026lt;T\u0026gt;\u0026gt; 肯定是安全的，对吗？试着在下面的程序中发现这个错误。\nusestd::ops::Deref;struct SortedVec\u0026lt;T: Ord\u0026gt;(Vec\u0026lt;T\u0026gt;);impl\u0026lt;T: Ord\u0026gt;SortedVec\u0026lt;T\u0026gt;{fn new(mutvec: Vec\u0026lt;T\u0026gt;)-\u0026gt; Self{vec.sort();SortedVec(vec)}fn push(\u0026amp;mutself,t: T){self.0.push(t);self.0.sort();}}impl\u0026lt;T: Ord\u0026gt;DerefforSortedVec\u0026lt;T\u0026gt;{type Target=Vec\u0026lt;T\u0026gt;;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Vec\u0026lt;T\u0026gt;{\u0026amp;self.0}}fn main(){letsorted=SortedVec::new(vec![2,8,6,3]);sorted.push(1);letsortedClone=sorted.clone();sortedClone.push(4);}我们从未为 SortedVec 实现 Clone，所以当我们调用 .clone() 方法时，编译器使用强制解引用来解决对 Vec 的方法调用，所以它返回一个 Vec 而不是 SortedVec!\nfn main(){letsorted: SortedVec\u0026lt;i32\u0026gt;=SortedVec::new(vec![2,8,6,3]);sorted.push(1);// still sorted // calling clone on SortedVec actually returns a Vec 🤦 letsortedClone: Vec\u0026lt;i32\u0026gt;=sorted.clone();sortedClone.push(4);// sortedClone no longer sorted 💀 }总之，以上这些限制、约束或麻烦都不是 Rust 的缺点，因为 Rust 一开始就没有被设计成一种 OO 语言或支持任何 OOP 模式。\n本节的主要启示是，不要试图用 Deref 和 DerefMut 实现来表现可爱或聪明。它们实际上只适合于智能指针类型，目前只能在标准库中实现，因为智能指针类型目前需要不稳定的特性和编译器的魔法才能工作。如果我们想要类似于 Deref 和 DerefMut 的功能和行为，那么我们实际上可能要找的是 AsRef 和 AsMut，我们将在后面讨论。\nIndex \u0026amp; IndexMut 预备知识\n Self Methods Associated Types Generic Parameters Generic Types vs Associated Types Subtraits \u0026amp; Supertraits Sized  traitIndex\u0026lt;Idx: ?Sized\u0026gt;{type Output: ?Sized;fn index(\u0026amp;self,index: Idx)-\u0026gt; \u0026amp;Self::Output;}traitIndexMut\u0026lt;Idx\u0026gt;: Index\u0026lt;Idx\u0026gt;whereIdx: ?Sized{fn index_mut(\u0026amp;mutself,index: Idx)-\u0026gt; \u0026amp;mutSelf::Output;}我们可以将 [] 索引到有 T 值的 Index\u0026lt;T, Output = U\u0026gt; 类型，索引操作将返回 \u0026amp;U 值。对于语法糖，编译器会在任何从索引操作返回的值前面自动插入一个解引用运算符 *。\nfn main(){// Vec\u0026lt;i32\u0026gt; impls Index\u0026lt;usize, Output = i32\u0026gt; so // indexing Vec\u0026lt;i32\u0026gt; should produce \u0026amp;i32s and yet... letvec=vec![1,2,3,4,5];letnum_ref: \u0026amp;i32 =vec[0];// ❌ expected \u0026amp;i32 found i32 // above line actually desugars to letnum_ref: \u0026amp;i32 =*vec[0];// ❌ expected \u0026amp;i32 found i32 // both of these alternatives work letnum: i32 =vec[0];// ✅ letnum_ref=\u0026amp;vec[0];// ✅ }一开始有点让人困惑，因为看起来 Index trait 并不遵循它自己的方法签名，但实际上这只是有问题的语法糖。\n因为 Idx 是一个泛型类型，Index trait 可以为一个给定的类型实现很多次，在 Vec\u0026lt;T\u0026gt; 的情况下，我们不仅可以使用 usize 对其进行索引，我们还可以使用 Range\u0026lt;usize\u0026gt; 对其进行索引，以获得切片。\nfn main(){letvec=vec![1,2,3,4,5];assert_eq!(\u0026amp;vec[..],\u0026amp;[1,2,3,4,5]);// ✅ assert_eq!(\u0026amp;vec[1..],\u0026amp;[2,3,4,5]);// ✅ assert_eq!(\u0026amp;vec[..4],\u0026amp;[1,2,3,4]);// ✅ assert_eq!(\u0026amp;vec[1..4],\u0026amp;[2,3,4]);// ✅ }为了展示我们如何实现 Index，这里有一个有趣的例子，展示了我们如何使用一个新类型和 Index trait 来实现 Vec 的包装索引和负索引。\nusestd::ops::Index;struct WrappingIndex\u0026lt;T\u0026gt;(Vec\u0026lt;T\u0026gt;);impl\u0026lt;T\u0026gt;Index\u0026lt;usize\u0026gt;forWrappingIndex\u0026lt;T\u0026gt;{type Output=T;fn index(\u0026amp;self,index: usize)-\u0026gt; \u0026amp;T{\u0026amp;self.0[index%self.0.len()]}}impl\u0026lt;T\u0026gt;Index\u0026lt;i128\u0026gt;forWrappingIndex\u0026lt;T\u0026gt;{type Output=T;fn index(\u0026amp;self,index: i128)-\u0026gt; \u0026amp;T{letself_len=self.0.len()asi128;letidx=(((index%self_len)+self_len)%self_len)asusize;\u0026amp;self.0[idx]}}#[test]// ✅ fn indexes(){letwrapping_vec=WrappingIndex(vec![1,2,3]);assert_eq!(1,wrapping_vec[0_usize]);assert_eq!(2,wrapping_vec[1_usize]);assert_eq!(3,wrapping_vec[2_usize]);}#[test]// ✅ fn wrapping_indexes(){letwrapping_vec=WrappingIndex(vec![1,2,3]);assert_eq!(1,wrapping_vec[3_usize]);assert_eq!(2,wrapping_vec[4_usize]);assert_eq!(3,wrapping_vec[5_usize]);}#[test]// ✅ fn neg_indexes(){letwrapping_vec=WrappingIndex(vec![1,2,3]);assert_eq!(1,wrapping_vec[-3_i128]);assert_eq!(2,wrapping_vec[-2_i128]);assert_eq!(3,wrapping_vec[-1_i128]);}#[test]// ✅ fn wrapping_neg_indexes(){letwrapping_vec=WrappingIndex(vec![1,2,3]);assert_eq!(1,wrapping_vec[-6_i128]);assert_eq!(2,wrapping_vec[-5_i128]);assert_eq!(3,wrapping_vec[-4_i128]);}没有要求 Idx 类型必须是数字类型或 Range，它可以是一个枚举! 下面是一个例子，使用篮球位置索引到一个篮球队，以检索该队的球员。\nusestd::ops::Index;enum BasketballPosition{PointGuard,ShootingGuard,Center,PowerForward,SmallForward,}struct BasketballPlayer{name: \u0026amp;\u0026#39;staticstr,position: BasketballPosition,}struct BasketballTeam{point_guard: BasketballPlayer,shooting_guard: BasketballPlayer,center: BasketballPlayer,power_forward: BasketballPlayer,small_forward: BasketballPlayer,}implIndex\u0026lt;BasketballPosition\u0026gt;forBasketballTeam{type Output=BasketballPlayer;fn index(\u0026amp;self,position: BasketballPosition)-\u0026gt; \u0026amp;BasketballPlayer{matchposition{BasketballPosition::PointGuard=\u0026gt;\u0026amp;self.point_guard,BasketballPosition::ShootingGuard=\u0026gt;\u0026amp;self.shooting_guard,BasketballPosition::Center=\u0026gt;\u0026amp;self.center,BasketballPosition::PowerForward=\u0026gt;\u0026amp;self.power_forward,BasketballPosition::SmallForward=\u0026gt;\u0026amp;self.small_forward,}}}Drop 预备知识\n Self Methods  traitDrop{fn drop(\u0026amp;mutself);}如果一个类型实现了 Drop，那么当它超出作用域但在被销毁之前，drop 将在该类型上调用。我们很少需要为我们的类型实现这个功能，但是一个很好的例子是，如果一个类型持有一些外部资源，当类型被销毁时，这些资源需要被清理掉。\n在标准库中有一个 BufWriter 类型，允许我们对 Write 类型进行缓冲写入。然而，如果 BufWriter 在其缓冲区的内容被刷新到底层的 Write 类型之前就被销毁了呢？幸好这是不可能的! BufWriter 实现了 Drop trait，所以每当它离开作用域时，flush 总是被调用。\nimpl\u0026lt;W: Write\u0026gt;DropforBufWriter\u0026lt;W\u0026gt;{fn drop(\u0026amp;mutself){self.flush_buf();}}另外，Rust 中的 Mutex 没有 unlock() 方法，因为它们不需要这些方法。在一个 Mutex 上调用 lock() 会返回一个 MutexGuard，当 Mutex 超出作用域时，由于它的 Drop 实现，它会自动解锁。\nimpl\u0026lt;T: ?Sized\u0026gt;DropforMutexGuard\u0026lt;\u0026#39;_,T\u0026gt;{fn drop(\u0026amp;mutself){unsafe{self.lock.inner.raw_unlock();}}}一般来说，如果你在某些资源上实现一个抽象，在使用后需要清理，那么这就是使用 Drop trait 的一个很好的理由。\n转换 Traits From \u0026amp; Into 预备知识\n Self Functions Methods Generic Parameters Generic Blanket Impls  traitFrom\u0026lt;T\u0026gt;{fn from(T)-\u0026gt; Self;}From\u0026lt;T\u0026gt; 类型允许我们将 T 转换为 Self。\ntraitInto\u0026lt;T\u0026gt;{fn into(self)-\u0026gt; T;}Into\u0026lt;T\u0026gt; 类型允许我们将 Self 转换为 T。\n这些 trait 是一个硬币的两个不同侧面。我们只能为我们的类型实现 From\u0026lt;T\u0026gt;，因为 Into\u0026lt;T\u0026gt; 的实现是由下面这个泛型覆盖实现自动提供的。\nimpl\u0026lt;T,U\u0026gt;Into\u0026lt;U\u0026gt;forTwhereU: From\u0026lt;T\u0026gt;,{fn into(self)-\u0026gt; U{U::from(self)}}这两个 trait 存在的原因是，它允许我们以稍微不同的方式在泛型类型上编写 trait 约束(trait bound)。\nfn function\u0026lt;T\u0026gt;(t: T)where// these bounds are equivalent T: From\u0026lt;i32\u0026gt;,i32: Into\u0026lt;T\u0026gt;{// these examples are equivalent letexample: T=T::from(0);letexample: T=0.into();}关于何时使用这两种方法并没有硬性规定，所以在每种情况下都要选择最合理的方法。现在让我们看看一些关于 Point 的例子。\nstruct Point{x: i32,y: i32,}implFrom\u0026lt;(i32,i32)\u0026gt;forPoint{fn from((x,y): (i32,i32))-\u0026gt; Self{Point{x,y}}}implFrom\u0026lt;[i32;2]\u0026gt;forPoint{fn from([x,y]: [i32;2])-\u0026gt; Self{Point{x,y}}}fn example(){// using From letorigin=Point::from((0,0));letorigin=Point::from([0,0]);// using Into letorigin: Point=(0,0).into();letorigin: Point=[0,0].into();}实现不是对称的，所以如果我们想把 Point 转换成元组和数组，我们也必须显示地地添加这些。\nstruct Point{x: i32,y: i32,}implFrom\u0026lt;(i32,i32)\u0026gt;forPoint{fn from((x,y): (i32,i32))-\u0026gt; Self{Point{x,y}}}implFrom\u0026lt;Point\u0026gt;for(i32,i32){fn from(Point{x,y}: Point)-\u0026gt; Self{(x,y)}}implFrom\u0026lt;[i32;2]\u0026gt;forPoint{fn from([x,y]: [i32;2])-\u0026gt; Self{Point{x,y}}}implFrom\u0026lt;Point\u0026gt;for[i32;2]{fn from(Point{x,y}: Point)-\u0026gt; Self{[x,y]}}fn example(){// from (i32, i32) into Point letpoint=Point::from((0,0));letpoint: Point=(0,0).into();// from Point into (i32, i32) lettuple=\u0026lt;(i32,i32)\u0026gt;::from(point);lettuple: (i32,i32)=point.into();// from [i32; 2] into Point letpoint=Point::from([0,0]);letpoint: Point=[0,0].into();// from Point into [i32; 2] letarray=\u0026lt;[i32;2]\u0026gt;::from(point);letarray: [i32;2]=point.into();}From\u0026lt;T\u0026gt; 的一个普遍用途是缩减模板代码。假设我们在程序中加入一个包含三个 Point 的 Triangle 类型，这里有许多方法可以构建它。\nstruct Point{x: i32,y: i32,}implPoint{fn new(x: i32,y: i32)-\u0026gt; Point{Point{x,y}}}implFrom\u0026lt;(i32,i32)\u0026gt;forPoint{fn from((x,y): (i32,i32))-\u0026gt; Point{Point{x,y}}}struct Triangle{p1: Point,p2: Point,p3: Point,}implTriangle{fn new(p1: Point,p2: Point,p3: Point)-\u0026gt; Triangle{Triangle{p1,p2,p3}}}impl\u0026lt;P\u0026gt;From\u0026lt;[P;3]\u0026gt;forTrianglewhereP: Into\u0026lt;Point\u0026gt;{fn from([p1,p2,p3]: [P;3])-\u0026gt; Triangle{Triangle{p1: p1.into(),p2: p2.into(),p3: p3.into(),}}}fn example(){// manual construction lettriangle=Triangle{p1: Point{x: 0,y: 0,},p2: Point{x: 1,y: 1,},p3: Point{x: 2,y: 2,},};// using Point::new lettriangle=Triangle{p1: Point::new(0,0),p2: Point::new(1,1),p3: Point::new(2,2),};// using From\u0026lt;(i32, i32)\u0026gt; for Point lettriangle=Triangle{p1: (0,0).into(),p2: (1,1).into(),p3: (2,2).into(),};// using Triangle::new + From\u0026lt;(i32, i32)\u0026gt; for Point lettriangle=Triangle::new((0,0).into(),(1,1).into(),(2,2).into(),);// using From\u0026lt;[Into\u0026lt;Point\u0026gt;; 3]\u0026gt; for Triangle lettriangle: Triangle=[(0,0),(1,1),(2,2),].into();}对于何时、如何或为什么我们应该为我们的类型实现 From\u0026lt;T\u0026gt;，没有任何规则，所以这取决于我们对每种情况的最佳判断。\nInto\u0026lt;T\u0026gt; 的一个流行用法是使需要自有值(owned values)的函数在接受自有值或借用值时具有通用性。\nstruct Person{name: String,}implPerson{// accepts: // - String fn new1(name: String)-\u0026gt; Person{Person{name}}// accepts: // - String // - \u0026amp;String // - \u0026amp;str // - Box\u0026lt;str\u0026gt; // - Cow\u0026lt;\u0026#39;_, str\u0026gt; // - char // since all of the above types can be converted into String fn new2\u0026lt;N: Into\u0026lt;String\u0026gt;\u0026gt;(name: N)-\u0026gt; Person{Person{name: name.into()}}}错误处理 谈论错误处理和 Error trait 的最佳时机是在讨论完 Display、Debug、Any 和 From 之后，在讨论 TryFrom 之前，因此错误处理部分与转换 trait 部分尴尬地一分为二。\nError 预备知识\n Self Methods Default Impls Generic Blanket Impls Subtraits \u0026amp; Supertraits Trait Objects Display \u0026amp; ToString Debug Any From \u0026amp; Into  traitError: Debug+Display{// provided default impls fn source(\u0026amp;self)-\u0026gt; Option\u0026lt;\u0026amp;(dynError+\u0026#39;static)\u0026gt;;fn backtrace(\u0026amp;self)-\u0026gt; Option\u0026lt;\u0026amp;Backtrace\u0026gt;;fn description(\u0026amp;self)-\u0026gt; \u0026amp;str;fn cause(\u0026amp;self)-\u0026gt; Option\u0026lt;\u0026amp;dynError\u0026gt;;}在 Rust 中，错误被返回，而不是被抛出。我们来看看一些例子。\n由于整数类型除以0会引起 panic，如果我们想让我们的程序更安全、更明确，我们可以实现一个 safe_div 函数，返回一个 Result，就像这样。\nusestd::fmt;usestd::error;#[derive(Debug, PartialEq)]struct DivByZero;implfmt::DisplayforDivByZero{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{write!(f,\u0026#34;division by zero error\u0026#34;)}}implerror::ErrorforDivByZero{}fn safe_div(numerator: i32,denominator: i32)-\u0026gt; Result\u0026lt;i32,DivByZero\u0026gt;{ifdenominator==0{returnErr(DivByZero);}Ok(numerator/denominator)}#[test]// ✅ fn test_safe_div(){assert_eq!(safe_div(8,2),Ok(4));assert_eq!(safe_div(5,0),Err(DivByZero));}由于错误是返回的，而不是抛出的，所以必须显式处理，如果当前函数不能处理一个错误，它应该将其传播给调用者。传播错误的最习惯的方法是使用 ? 操作符，它只是现在被废弃的 try! 宏的语法糖，它只是做这个。\nmacro_rules!try{($expr:expr)=\u0026gt;{match$expr{// if Ok just unwrap the value Ok(val)=\u0026gt;val,// if Err map the err value using From and return Err(err)=\u0026gt;{returnErr(From::from(err));}}};}如果我们想写一个将文件读成 String 的函数，我们可以这样写，用 ? 将 io::Error 传播到它们可能出现的任何地方。\nusestd::io::Read;usestd::path::Path;usestd::io;usestd::fs::File;fn read_file_to_string(path: \u0026amp;Path)-\u0026gt; Result\u0026lt;String,io::Error\u0026gt;{letmutfile=File::open(path)?;// ⬆️ io::Error letmutcontents=String::new();file.read_to_string(\u0026amp;mutcontents)?;// ⬆️ io::Error Ok(contents)}但是，假设我们正在读取的文件实际上是一个数字列表，我们想把它们加在一起，我们会像这样更新我们的函数。\nusestd::io::Read;usestd::path::Path;usestd::io;usestd::fs::File;fn sum_file(path: \u0026amp;Path)-\u0026gt; Result\u0026lt;i32,/* What to put here? */\u0026gt;{letmutfile=File::open(path)?;// ⬆️ io::Error letmutcontents=String::new();file.read_to_string(\u0026amp;mutcontents)?;// ⬆️ io::Error letmutsum=0;forlineincontents.lines(){sum+=line.parse::\u0026lt;i32\u0026gt;()?;// ⬆️ ParseIntError }Ok(sum)}但是现在我们的 Result 的错误类型是什么？它可以返回一个 io::Error 或者一个 ParseIntError。我们将看一下解决这个问题的三种方法，从临时应急的方法开始，最后是最稳健的方法。\n第一种方法是认识到所有实现了 Error 的类型也实现了 Display，所以我们可以将所有的错误映射到 String，并使用 String 作为我们的错误类型。\nusestd::fs::File;usestd::io;usestd::io::Read;usestd::path::Path;fn sum_file(path: \u0026amp;Path)-\u0026gt; Result\u0026lt;i32,String\u0026gt;{letmutfile=File::open(path).map_err(|e|e.to_string())?;// ⬆️ io::Error -\u0026gt; String letmutcontents=String::new();file.read_to_string(\u0026amp;mutcontents).map_err(|e|e.to_string())?;// ⬆️ io::Error -\u0026gt; String letmutsum=0;forlineincontents.lines(){sum+=line.parse::\u0026lt;i32\u0026gt;().map_err(|e|e.to_string())?;// ⬆️ ParseIntError -\u0026gt; String }Ok(sum)}对每个错误进行字符串化处理的明显缺点是，我们丢弃了类型信息，这使得调用者更难处理错误。\n上述方法的一个非显而易见的好处是我们可以定制字符串以提供更多的特定环境信息。例如，ParseIntError 通常字符串化为 \u0026ldquo;invalid digit found in string\u0026rdquo;，这是非常模糊的，没有提到无效的字符串是什么或者它试图解析成什么整数类型。如果我们要调试这个问题，这个错误信息几乎是无用的。然而，我们可以通过自己提供所有与上下文相关的信息来使其明显改善。\nsum+=line.parse::\u0026lt;i32\u0026gt;().map_err(|_|format!(\u0026#34;failed to parse {} into i32\u0026#34;,line))?;第二种方法是利用标准库中的这种泛型覆盖实现。\nimpl\u0026lt;E: error::Error\u0026gt;From\u0026lt;E\u0026gt;forBox\u0026lt;dynerror::Error\u0026gt;;这意味着任何 Error 类型都可以通过 ? 运算符隐式地转换为 Box\u0026lt;dyn error::Error\u0026gt;，所以我们可以在任何产生错误的函数的 Result 返回类型中把错误类型设置为 Box\u0026lt;dyn error::Error\u0026gt;，? 运算符将为我们完成其余的工作。\nusestd::fs::File;usestd::io::Read;usestd::path::Path;usestd::error;fn sum_file(path: \u0026amp;Path)-\u0026gt; Result\u0026lt;i32,Box\u0026lt;dynerror::Error\u0026gt;\u0026gt;{letmutfile=File::open(path)?;// ⬆️ io::Error -\u0026gt; Box\u0026lt;dyn error::Error\u0026gt; letmutcontents=String::new();file.read_to_string(\u0026amp;mutcontents)?;// ⬆️ io::Error -\u0026gt; Box\u0026lt;dyn error::Error\u0026gt; letmutsum=0;forlineincontents.lines(){sum+=line.parse::\u0026lt;i32\u0026gt;()?;// ⬆️ ParseIntError -\u0026gt; Box\u0026lt;dyn error::Error\u0026gt; }Ok(sum)}虽然更加简洁，但这似乎与之前的方法有相同的缺点，即丢弃了类型信息。这大部分是真的，但是如果调用者知道我们函数的实现细节，他们仍然可以使用 error::Error 上的 downcast_ref() 方法来处理不同的错误类型，这和它在 dyn Any 类型上的作用是一样的。\nfn handle_sum_file_errors(path: \u0026amp;Path){matchsum_file(path){Ok(sum)=\u0026gt;println!(\u0026#34;the sum is {}\u0026#34;,sum),Err(err)=\u0026gt;{ifletSome(e)=err.downcast_ref::\u0026lt;io::Error\u0026gt;(){// handle io::Error }elseifletSome(e)=err.downcast_ref::\u0026lt;ParseIntError\u0026gt;(){// handle ParseIntError }else{// we know sum_file can only return one of the // above errors so this branch is unreachable unreachable!();}}}}第三种方法是最稳健和类型安全的方法，可以聚合这些不同的错误，是使用一个枚举建立我们自己的自定义错误类型。\nusestd::num::ParseIntError;usestd::fs::File;usestd::io;usestd::io::Read;usestd::path::Path;usestd::error;usestd::fmt;#[derive(Debug)]enum SumFileError{Io(io::Error),Parse(ParseIntError),}implFrom\u0026lt;io::Error\u0026gt;forSumFileError{fn from(err: io::Error)-\u0026gt; Self{SumFileError::Io(err)}}implFrom\u0026lt;ParseIntError\u0026gt;forSumFileError{fn from(err: ParseIntError)-\u0026gt; Self{SumFileError::Parse(err)}}implfmt::DisplayforSumFileError{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{matchself{SumFileError::Io(err)=\u0026gt;write!(f,\u0026#34;sum file error: {}\u0026#34;,err),SumFileError::Parse(err)=\u0026gt;write!(f,\u0026#34;sum file error: {}\u0026#34;,err),}}}implerror::ErrorforSumFileError{// the default impl for this method always returns None // but we can now override it to make it way more useful! fn source(\u0026amp;self)-\u0026gt; Option\u0026lt;\u0026amp;(dynerror::Error+\u0026#39;static)\u0026gt;{Some(matchself{SumFileError::Io(err)=\u0026gt;err,SumFileError::Parse(err)=\u0026gt;err,})}}fn sum_file(path: \u0026amp;Path)-\u0026gt; Result\u0026lt;i32,SumFileError\u0026gt;{letmutfile=File::open(path)?;// ⬆️ io::Error -\u0026gt; SumFileError letmutcontents=String::new();file.read_to_string(\u0026amp;mutcontents)?;// ⬆️ io::Error -\u0026gt; SumFileError letmutsum=0;forlineincontents.lines(){sum+=line.parse::\u0026lt;i32\u0026gt;()?;// ⬆️ ParseIntError -\u0026gt; SumFileError }Ok(sum)}fn handle_sum_file_errors(path: \u0026amp;Path){matchsum_file(path){Ok(sum)=\u0026gt;println!(\u0026#34;the sum is {}\u0026#34;,sum),Err(SumFileError::Io(err))=\u0026gt;{// handle io::Error },Err(SumFileError::Parse(err))=\u0026gt;{// handle ParseIntError },}}Conversion Traits Continued TryFrom \u0026amp; TryInto 预备知识\n Self Functions Methods Associated Types Generic Parameters Generic Types vs Associated Types Generic Blanket Impls From \u0026amp; Into Error  TryFrom 和 TryInto 是 From 和 Into 的不可靠版本。\ntraitTryFrom\u0026lt;T\u0026gt;{type Error;fn try_from(value: T)-\u0026gt; Result\u0026lt;Self,Self::Error\u0026gt;;}traitTryInto\u0026lt;T\u0026gt;{type Error;fn try_into(self)-\u0026gt; Result\u0026lt;T,Self::Error\u0026gt;;}与 Into 类似，我们不能自己实现 TryInto，因为它的实现是由下面这个泛型覆盖实现提供的。\nimpl\u0026lt;T,U\u0026gt;TryInto\u0026lt;U\u0026gt;forTwhereU: TryFrom\u0026lt;T\u0026gt;,{type Error=U::Error;fn try_into(self)-\u0026gt; Result\u0026lt;U,U::Error\u0026gt;{U::try_from(self)}}Let\u0026rsquo;s say that in the context of our program it doesn\u0026rsquo;t make sense for Points to have x and y values that are less than -1000 or greater than 1000. This is how we\u0026rsquo;d rewrite our earlier From impls using TryFrom to signal to the users of our type that this conversion can now fail: 假设在我们的程序中，Point 的 x 和 y 的值小于 -1000 或大于 1000 是不合理的。这就是我们如何使用 TryFrom 重写我们先前的 From 实现，向我们类型的用户发出信号，这个转换现在可以失败。\nusestd::convert::TryFrom;usestd::error;usestd::fmt;struct Point{x: i32,y: i32,}#[derive(Debug)]struct OutOfBounds;implfmt::DisplayforOutOfBounds{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{write!(f,\u0026#34;out of bounds\u0026#34;)}}implerror::ErrorforOutOfBounds{}// now fallible implTryFrom\u0026lt;(i32,i32)\u0026gt;forPoint{type Error=OutOfBounds;fn try_from((x,y): (i32,i32))-\u0026gt; Result\u0026lt;Point,OutOfBounds\u0026gt;{ifx.abs()\u0026gt;1000||y.abs()\u0026gt;1000{returnErr(OutOfBounds);}Ok(Point{x,y})}}// still infallible implFrom\u0026lt;Point\u0026gt;for(i32,i32){fn from(Point{x,y}: Point)-\u0026gt; Self{(x,y)}}And here\u0026rsquo;s the refactored TryFrom\u0026lt;[TryInto\u0026lt;Point\u0026gt;; 3]\u0026gt; impl for Triangle: 这里是重构后的 TryFrom\u0026lt;[TryInto\u0026lt;Point\u0026gt;; 3]\u0026gt; 实现，用于 Triangle。\nusestd::convert::{TryFrom,TryInto};usestd::error;usestd::fmt;struct Point{x: i32,y: i32,}#[derive(Debug)]struct OutOfBounds;implfmt::DisplayforOutOfBounds{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{write!(f,\u0026#34;out of bounds\u0026#34;)}}implerror::ErrorforOutOfBounds{}implTryFrom\u0026lt;(i32,i32)\u0026gt;forPoint{type Error=OutOfBounds;fn try_from((x,y): (i32,i32))-\u0026gt; Result\u0026lt;Self,Self::Error\u0026gt;{ifx.abs()\u0026gt;1000||y.abs()\u0026gt;1000{returnErr(OutOfBounds);}Ok(Point{x,y})}}struct Triangle{p1: Point,p2: Point,p3: Point,}impl\u0026lt;P\u0026gt;TryFrom\u0026lt;[P;3]\u0026gt;forTrianglewhereP: TryInto\u0026lt;Point\u0026gt;,{type Error=P::Error;fn try_from([p1,p2,p3]: [P;3])-\u0026gt; Result\u0026lt;Self,Self::Error\u0026gt;{Ok(Triangle{p1: p1.try_into()?,p2: p2.try_into()?,p3: p3.try_into()?,})}}fn example()-\u0026gt; Result\u0026lt;Triangle,OutOfBounds\u0026gt;{lett: Triangle=[(0,0),(1,1),(2,2)].try_into()?;Ok(t)}FromStr 预备知识\n Self Functions Associated Types Error TryFrom \u0026amp; TryInto  traitFromStr{type Err;fn from_str(s: \u0026amp;str)-\u0026gt; Result\u0026lt;Self,Self::Err\u0026gt;;}FromStr types allow performing a fallible conversion from \u0026amp;str into Self. The idiomatic way to use FromStr is to call the .parse() method on \u0026amp;strs: FromStr 类型允许执行从 \u0026amp;str 到 Self 的错误转换。使用 FromStr 的习惯方法是对 \u0026amp;str 调用 .parse() 方法。\nusestd::str::FromStr;fn example\u0026lt;T: FromStr\u0026gt;(s: \u0026amp;\u0026#39;staticstr){// these are all equivalent lett: Result\u0026lt;T,_\u0026gt;=FromStr::from_str(s);lett=T::from_str(s);lett: Result\u0026lt;T,_\u0026gt;=s.parse();lett=s.parse::\u0026lt;T\u0026gt;();// most idiomatic }Point 实现的例子:\nusestd::error;usestd::fmt;usestd::iter::Enumerate;usestd::num::ParseIntError;usestd::str::{Chars,FromStr};#[derive(Debug, Eq, PartialEq)]struct Point{x: i32,y: i32,}implPoint{fn new(x: i32,y: i32)-\u0026gt; Self{Point{x,y}}}#[derive(Debug, PartialEq)]struct ParsePointError;implfmt::DisplayforParsePointError{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{write!(f,\u0026#34;failed to parse point\u0026#34;)}}implFrom\u0026lt;ParseIntError\u0026gt;forParsePointError{fn from(_e: ParseIntError)-\u0026gt; Self{ParsePointError}}implerror::ErrorforParsePointError{}implFromStrforPoint{type Err =ParsePointError;fn from_str(s: \u0026amp;str)-\u0026gt; Result\u0026lt;Self,Self::Err\u0026gt;{letis_num=|(_,c): \u0026amp;(usize,char)|matches!(c,\u0026#39;0\u0026#39;..=\u0026#39;9\u0026#39;|\u0026#39;-\u0026#39;);letisnt_num=|t: \u0026amp;(_,_)|!is_num(t);letget_num=|char_idxs: \u0026amp;mutEnumerate\u0026lt;Chars\u0026lt;\u0026#39;_\u0026gt;\u0026gt;|-\u0026gt; Result\u0026lt;(usize,usize),ParsePointError\u0026gt;{let(start,_)=char_idxs.skip_while(isnt_num).next().ok_or(ParsePointError)?;let(end,_)=char_idxs.skip_while(is_num).next().ok_or(ParsePointError)?;Ok((start,end))};letmutchar_idxs=s.chars().enumerate();let(x_start,x_end)=get_num(\u0026amp;mutchar_idxs)?;let(y_start,y_end)=get_num(\u0026amp;mutchar_idxs)?;letx=s[x_start..x_end].parse::\u0026lt;i32\u0026gt;()?;lety=s[y_start..y_end].parse::\u0026lt;i32\u0026gt;()?;Ok(Point{x,y})}}#[test]// ✅ fn pos_x_y(){letp=\u0026#34;(4, 5)\u0026#34;.parse::\u0026lt;Point\u0026gt;();assert_eq!(p,Ok(Point::new(4,5)));}#[test]// ✅ fn neg_x_y(){letp=\u0026#34;(-6, -2)\u0026#34;.parse::\u0026lt;Point\u0026gt;();assert_eq!(p,Ok(Point::new(-6,-2)));}#[test]// ✅ fn not_a_point(){letp=\u0026#34;not a point\u0026#34;.parse::\u0026lt;Point\u0026gt;();assert_eq!(p,Err(ParsePointError));}FromStr has the same signature as TryFrom\u0026lt;\u0026amp;str\u0026gt;. It doesn\u0026rsquo;t matter which one we impl for a type first as long as we forward the impl to the other one. Here\u0026rsquo;s a TryFrom\u0026lt;\u0026amp;str\u0026gt; impl for Point assuming it already has a FromStr impl: FromStr 与 TryFrom\u0026lt;\u0026amp;str\u0026gt; 的签名相同。只要我们把实现转发给另一个类型，哪一个实现并不重要。下面是一个针对 Point 的 TryFrom\u0026lt;\u0026amp;str\u0026gt; 实现，假设它已经有一个 FromStr 实现。\nimplTryFrom\u0026lt;\u0026amp;str\u0026gt;forPoint{type Error=\u0026lt;PointasFromStr\u0026gt;::Err;fn try_from(s: \u0026amp;str)-\u0026gt; Result\u0026lt;Point,Self::Error\u0026gt;{\u0026lt;PointasFromStr\u0026gt;::from_str(s)}}AsRef \u0026amp; AsMut 预备知识\n Self Methods Sized Generic Parameters Sized Deref \u0026amp; DerefMut  traitAsRef\u0026lt;T: ?Sized\u0026gt;{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;T;}traitAsMut\u0026lt;T: ?Sized\u0026gt;{fn as_mut(\u0026amp;mutself)-\u0026gt; \u0026amp;mutT;}?Sized 说明 T 类型是大小不确定的。As 作为介词, 表明发生了类型转换。\nAsRef is for cheap reference to reference conversions. However, one of the most common ways it\u0026rsquo;s used is to make functions generic over whether they take ownership or not:\nAsRef 是用于廉价的引用到引用的转换。然而，它最常见的使用方式之一是使函数在是否拥有所有权上通用。\n// accepts: // - \u0026amp;str // - \u0026amp;String fn takes_str(s: \u0026amp;str){// use \u0026amp;str }// accepts: // - \u0026amp;str // - \u0026amp;String // - String fn takes_asref_str\u0026lt;S: AsRef\u0026lt;str\u0026gt;\u0026gt;(s: S){lets: \u0026amp;str =s.as_ref();// use \u0026amp;str }fn example(slice: \u0026amp;str,borrow: \u0026amp;String,owned: String){takes_str(slice);takes_str(borrow);takes_str(owned);// ❌ takes_asref_str(slice);takes_asref_str(borrow);takes_asref_str(owned);// ✅ }The other most common use-case is returning a reference to inner private data wrapped by a type which protects some invariant. A good example from the standard library is String which is just a wrapper around Vec\u0026lt;u8\u0026gt;:\n另一个最常见的用例是返回一个对内部私有数据的引用，该数据由一个保护某些不变性的类型包裹。标准库中的一个很好的例子是 String，它只是 Vec\u0026lt;u8\u0026gt; 的一个包装器。\nstruct String {vec: Vec\u0026lt;u8\u0026gt;,}This inner Vec cannot be made public because if it was people could mutate any byte and break the String\u0026rsquo;s valid UTF-8 encoding. However, it\u0026rsquo;s safe to expose an immutable read-only reference to the inner byte array, hence this impl: 这个内部的 Vec 不能被公开，因为如果它被公开，人们可以改变任何字节并破坏 String 的有效 UTF-8 编码。然而，公开内部字节数组的不可变的只读引用是安全的，因此有了这个实现:\nimplAsRef\u0026lt;[u8]\u0026gt;forString;Generally, it often only makes sense to impl AsRef for a type if it wraps some other type to either provide additional functionality around the inner type or protect some invariant on the inner type.\nLet\u0026rsquo;s examine a example of bad AsRef impls: 一般来说，只有当一个类型包装了其他类型，为内部类型提供了额外的功能，或者保护了内部类型的某些不变性时，为其实现 AsRef 才有意义。\n让我们来看看一个不好的 AsRef 实现的例子。\nstruct User{name: String,age: u32,}implAsRef\u0026lt;String\u0026gt;forUser{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;String {\u0026amp;self.name}}implAsRef\u0026lt;u32\u0026gt;forUser{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;u32 {\u0026amp;self.age}}This works and kinda makes sense at first, but quickly falls apart if we add more members to User: 这在一开始是可行的，而且有点道理，但如果我们给 User 增加更多的成员，很快就会崩溃。\nstruct User{name: String,email: String,age: u32,height: u32,}implAsRef\u0026lt;String\u0026gt;forUser{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;String {// uh, do we return name or email here? }}implAsRef\u0026lt;u32\u0026gt;forUser{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;u32 {// uh, do we return age or height here? }}A User is composed of Strings and u32s but it\u0026rsquo;s not really the same thing as a String or a u32. Even if we had much more specific types: User 是由 String 和 u32 组成的，但它和 String 或 u32 并不是真正的一回事。即使我们有更具体的类型。\nstruct User{name: Name,email: Email,age: Age,height: Height,}It wouldn\u0026rsquo;t make much sense to impl AsRef for any of those because AsRef is for cheap reference to reference conversions between semantically equivalent things, and Name, Email, Age, and Height by themselves are not the same thing as a User.\nA good example where we would impl AsRef would be if we introduced a new type Moderator that just wrapped a User and added some moderation specific privileges: 实现 AsRef 对这些都没有意义，因为 AsRef 是用来在语义上等同的事物之间进行廉价的引用转换，而 Name、Email、Age 和 Height 本身就和 User 不是一回事。\n一个很好的例子是，如果我们引入一个新的类型 Moderator，它只是包裹了一个 User，并增加了一些特定的管理权限，我们就会使用 AsRef。\nstruct User{name: String,age: u32,}// unfortunately the standard library cannot provide // a generic blanket impl to save us from this boilerplate implAsRef\u0026lt;User\u0026gt;forUser{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;User{self}}enum Privilege{BanUsers,EditPosts,DeletePosts,}// although Moderators have some special // privileges they are still regular Users // and should be able to do all the same stuff struct Moderator{user: User,privileges: Vec\u0026lt;Privilege\u0026gt;}implAsRef\u0026lt;Moderator\u0026gt;forModerator{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Moderator{self}}implAsRef\u0026lt;User\u0026gt;forModerator{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;User{\u0026amp;self.user}}// this should be callable with Users // and Moderators (who are also Users) fn create_post\u0026lt;U: AsRef\u0026lt;User\u0026gt;\u0026gt;(u: U){letuser=u.as_ref();// etc }fn example(user: User,moderator: Moderator){create_post(\u0026amp;user);create_post(\u0026amp;moderator);// ✅ }This works because Moderators are just Users. Here\u0026rsquo;s the example from the Deref section except using AsRef instead: 这样做是因为 Moderator 就是 User。下面是 Deref 部分的例子，只是用 AsRef 代替。\nusestd::convert::AsRef;struct Human{health_points: u32,}implAsRef\u0026lt;Human\u0026gt;forHuman{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Human{self}}enum Weapon{Spear,Axe,Sword,}// a Soldier is just a Human with a Weapon struct Soldier{human: Human,weapon: Weapon,}implAsRef\u0026lt;Soldier\u0026gt;forSoldier{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Soldier{self}}implAsRef\u0026lt;Human\u0026gt;forSoldier{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.human}}enum Mount{Horse,Donkey,Cow,}// a Knight is just a Soldier with a Mount struct Knight{soldier: Soldier,mount: Mount,}implAsRef\u0026lt;Knight\u0026gt;forKnight{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Knight{self}}implAsRef\u0026lt;Soldier\u0026gt;forKnight{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Soldier{\u0026amp;self.soldier}}implAsRef\u0026lt;Human\u0026gt;forKnight{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.soldier.human}}enum Spell{MagicMissile,FireBolt,ThornWhip,}// a Mage is just a Human who can cast Spells struct Mage{human: Human,spells: Vec\u0026lt;Spell\u0026gt;,}implAsRef\u0026lt;Mage\u0026gt;forMage{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Mage{self}}implAsRef\u0026lt;Human\u0026gt;forMage{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.human}}enum Staff{Wooden,Metallic,Plastic,}// a Wizard is just a Mage with a Staff struct Wizard{mage: Mage,staff: Staff,}implAsRef\u0026lt;Wizard\u0026gt;forWizard{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Wizard{self}}implAsRef\u0026lt;Mage\u0026gt;forWizard{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Mage{\u0026amp;self.mage}}implAsRef\u0026lt;Human\u0026gt;forWizard{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.mage.human}}fn borrows_human\u0026lt;H: AsRef\u0026lt;Human\u0026gt;\u0026gt;(human: H){}fn borrows_soldier\u0026lt;S: AsRef\u0026lt;Soldier\u0026gt;\u0026gt;(soldier: S){}fn borrows_knight\u0026lt;K: AsRef\u0026lt;Knight\u0026gt;\u0026gt;(knight: K){}fn borrows_mage\u0026lt;M: AsRef\u0026lt;Mage\u0026gt;\u0026gt;(mage: M){}fn borrows_wizard\u0026lt;W: AsRef\u0026lt;Wizard\u0026gt;\u0026gt;(wizard: W){}fn example(human: Human,soldier: Soldier,knight: Knight,mage: Mage,wizard: Wizard){// all types can be used as Humans borrows_human(\u0026amp;human);borrows_human(\u0026amp;soldier);borrows_human(\u0026amp;knight);borrows_human(\u0026amp;mage);borrows_human(\u0026amp;wizard);// Knights can be used as Soldiers borrows_soldier(\u0026amp;soldier);borrows_soldier(\u0026amp;knight);// Wizards can be used as Mages borrows_mage(\u0026amp;mage);borrows_mage(\u0026amp;wizard);// Knights \u0026amp; Wizards passed as themselves borrows_knight(\u0026amp;knight);borrows_wizard(\u0026amp;wizard);}Deref didn\u0026rsquo;t work in the prior version of the example above because deref coercion is an implicit conversion between types which leaves room for people to mistakenly formulate the wrong ideas and expectations for how it will behave. AsRef works above because it makes the conversion between types explicit and there\u0026rsquo;s no room leftover to develop any wrong ideas or expectations. Deref 在上述例子的先前版本中不起作用，因为 deref coercion 是一种隐式类型转换，为人们错误地制定错误的想法和期望留下了空间。AsRef 在上面起作用，因为它使类型间的转换变得明确，没有余地来发展任何错误的想法或期望。\nBorrow \u0026amp; BorrowMut 预备知识\n Self Methods Generic Parameters Subtraits \u0026amp; Supertraits Sized AsRef \u0026amp; AsMut PartialEq \u0026amp; Eq Hash PartialOrd \u0026amp; Ord  traitBorrow\u0026lt;Borrowed\u0026gt;whereBorrowed: ?Sized,{fn borrow(\u0026amp;self)-\u0026gt; \u0026amp;Borrowed;}traitBorrowMut\u0026lt;Borrowed\u0026gt;: Borrow\u0026lt;Borrowed\u0026gt;whereBorrowed: ?Sized,{fn borrow_mut(\u0026amp;mutself)-\u0026gt; \u0026amp;mutBorrowed;}These traits were invented to solve the very specific problem of looking up String keys in HashSets, HashMaps, BTreeSets, and BTreeMaps using \u0026amp;str values.\nWe can view Borrow\u0026lt;T\u0026gt; and BorrowMut\u0026lt;T\u0026gt; as stricter versions of AsRef\u0026lt;T\u0026gt; and AsMut\u0026lt;T\u0026gt;, where the returned reference \u0026amp;T has equivalent Eq, Hash, and Ord impls to Self. This is more easily explained with a commented example: 这些 trait 的发明是为了解决在 HashSet, HashMap, BTreeSet, 和 BTreeMap 中使用 \u0026amp;str 值查找 String 键的特殊问题。\n我们可以把 Borrow\u0026lt;T\u0026gt; 和 BorrowMut\u0026lt;T\u0026gt; 看作是 AsRef\u0026lt;T\u0026gt; 和 AsMut\u0026lt;T\u0026gt; 的更严格的版本，其中返回的引用 \u0026amp;T 与 Self 的 Eq、Hash 和 Ord 等值。用一个注释的例子可以更容易地解释这个问题。\nusestd::borrow::Borrow;usestd:#️⃣:Hasher;usestd::collections::hash_map::DefaultHasher;usestd:#️⃣:Hash;fn get_hash\u0026lt;T: Hash\u0026gt;(t: T)-\u0026gt; u64 {letmuthasher=DefaultHasher::new();t.hash(\u0026amp;muthasher);hasher.finish()}fn asref_example\u0026lt;Owned,Ref\u0026gt;(owned1: Owned,owned2: Owned)whereOwned: Eq +Ord+Hash+AsRef\u0026lt;Ref\u0026gt;,Ref: Eq +Ord+Hash{letref1: \u0026amp;Ref=owned1.as_ref();letref2: \u0026amp;Ref=owned2.as_ref();// refs aren\u0026#39;t required to be equal if owned types are equal assert_eq!(owned1==owned2,ref1==ref2);// ❌ letowned1_hash=get_hash(\u0026amp;owned1);letowned2_hash=get_hash(\u0026amp;owned2);letref1_hash=get_hash(\u0026amp;ref1);letref2_hash=get_hash(\u0026amp;ref2);// ref hashes aren\u0026#39;t required to be equal if owned type hashes are equal assert_eq!(owned1_hash==owned2_hash,ref1_hash==ref2_hash);// ❌ // ref comparisons aren\u0026#39;t required to match owned type comparisons assert_eq!(owned1.cmp(\u0026amp;owned2),ref1.cmp(\u0026amp;ref2));// ❌ }fn borrow_example\u0026lt;Owned,Borrowed\u0026gt;(owned1: Owned,owned2: Owned)whereOwned: Eq +Ord+Hash+Borrow\u0026lt;Borrowed\u0026gt;,Borrowed: Eq +Ord+Hash{letborrow1: \u0026amp;Borrowed=owned1.borrow();letborrow2: \u0026amp;Borrowed=owned2.borrow();// borrows are required to be equal if owned types are equal assert_eq!(owned1==owned2,borrow1==borrow2);// ✅ letowned1_hash=get_hash(\u0026amp;owned1);letowned2_hash=get_hash(\u0026amp;owned2);letborrow1_hash=get_hash(\u0026amp;borrow1);letborrow2_hash=get_hash(\u0026amp;borrow2);// borrow hashes are required to be equal if owned type hashes are equal assert_eq!(owned1_hash==owned2_hash,borrow1_hash==borrow2_hash);// ✅ // borrow comparisons are required to match owned type comparisons assert_eq!(owned1.cmp(\u0026amp;owned2),borrow1.cmp(\u0026amp;borrow2));// ✅ }It\u0026rsquo;s good to be aware of these traits and understand why they exist since it helps demystify some of the methods on HashSet, HashMap, BTreeSet, and BTreeMap but it\u0026rsquo;s very rare that we would ever need to impl these traits for any of our types because it\u0026rsquo;s very rare that we would ever need create a pair of types where one is the \u0026ldquo;borrowed\u0026rdquo; version of the other in the first place. If we have some T then \u0026amp;T will get the job done 99.99% of the time, and T: Borrow\u0026lt;T\u0026gt; is already implemented for all T because of a generic blanket impl, so we don\u0026rsquo;t need to manually impl it and we don\u0026rsquo;t need to create some U such that T: Borrow\u0026lt;U\u0026gt;.\n知道这些 trait 并理解它们存在的原因是很好的，因为这有助于解开 HashSet、HashMap、BTreeSet 和 BTreeMap 上的一些方法，但是我们很少需要为我们的任何类型实现这些 trait，因为我们很少需要创建一对类型，其中一个是另一个的 \u0026ldquo;借用\u0026rdquo; 版本。如果我们有一些 T，那么 T 在 99.99% 的情况下都能完成工作，而且 T: Borrow\u0026lt;T\u0026gt; 已经为所有的 T 实现了，因为有一个通用的一揽子实现，所以我们不需要手动实现它，我们也不需要创建一些 U，使 T: Borrow\u0026lt;U\u0026gt;。\nToOwned 预备知识\n Self Methods Default Impls Clone Borrow \u0026amp; BorrowMut  traitToOwned{type Owned: Borrow\u0026lt;Self\u0026gt;;fn to_owned(\u0026amp;self)-\u0026gt; Self::Owned;// provided default impls fn clone_into(\u0026amp;self,target: \u0026amp;mutSelf::Owned);}ToOwned is a more generic version of Clone. Clone allows us to take a \u0026amp;T and turn it into an T but ToOwned allows us to take a \u0026amp;Borrowed and turn it into a Owned where Owned: Borrow\u0026lt;Borrowed\u0026gt;.\nIn other words, we can\u0026rsquo;t \u0026ldquo;clone\u0026rdquo; a \u0026amp;str into a String, or a \u0026amp;Path into a PathBuf, or an \u0026amp;OsStr into an OsString, since the clone method signature doesn\u0026rsquo;t support this kind of cross-type cloning, and that\u0026rsquo;s what ToOwned was made for.\nFor similar reasons as Borrow and BorrowMut, it\u0026rsquo;s good to be aware of this trait and understand why it exists but it\u0026rsquo;s very rare we\u0026rsquo;ll ever need to impl it for any of our types.\nToOwned 是 Clone 的一个更通用的版本。Clone 允许我们把一个 \u0026amp;T 变成一个 T，但 ToOwned 允许我们把一个 \u0026amp;Borrowed 变成一个 Owned，其中 Owned: Borrow\u0026lt;Borrowed\u0026gt;。\n换句话说，我们不能把一个 \u0026amp;str 克隆成一个 String，或者把一个 \u0026amp;Path 克隆成一个 PathBuf，或者把一个 \u0026amp;OsStr 克隆成一个 OsString，因为 clone 方法签名不支持这种跨类型克隆，而这正是 ToOwned 的用途。\n出于与 Borrow 和 BorrowMut 类似的原因，知道这个 trait 并理解它存在的原因是很好的，但我们很少需要为我们的任何类型实现这个 trait。\nIteration Traits Iterator 预备知识\n Self Methods Associated Types Default Impls  traitIterator{type Item;fn next(\u0026amp;mutself)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;;// provided default impls fn size_hint(\u0026amp;self)-\u0026gt; (usize,Option\u0026lt;usize\u0026gt;);fn count(self)-\u0026gt; usize;fn last(self)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;;fn advance_by(\u0026amp;mutself,n: usize)-\u0026gt; Result\u0026lt;(),usize\u0026gt;;fn nth(\u0026amp;mutself,n: usize)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;;fn step_by(self,step: usize)-\u0026gt; StepBy\u0026lt;Self\u0026gt;;fn chain\u0026lt;U\u0026gt;(self,other: U)-\u0026gt; Chain\u0026lt;Self,\u0026lt;UasIntoIterator\u0026gt;::IntoIter\u0026gt;whereU: IntoIterator\u0026lt;Item=Self::Item\u0026gt;;fn zip\u0026lt;U\u0026gt;(self,other: U)-\u0026gt; Zip\u0026lt;Self,\u0026lt;UasIntoIterator\u0026gt;::IntoIter\u0026gt;whereU: IntoIterator;fn map\u0026lt;B,F\u0026gt;(self,f: F)-\u0026gt; Map\u0026lt;Self,F\u0026gt;whereF: FnMut(Self::Item)-\u0026gt; B;fn for_each\u0026lt;F\u0026gt;(self,f: F)whereF: FnMut(Self::Item);fn filter\u0026lt;P\u0026gt;(self,predicate: P)-\u0026gt; Filter\u0026lt;Self,P\u0026gt;whereP: FnMut(\u0026amp;Self::Item)-\u0026gt; bool;fn filter_map\u0026lt;B,F\u0026gt;(self,f: F)-\u0026gt; FilterMap\u0026lt;Self,F\u0026gt;whereF: FnMut(Self::Item)-\u0026gt; Option\u0026lt;B\u0026gt;;fn enumerate(self)-\u0026gt; Enumerate\u0026lt;Self\u0026gt;;fn peekable(self)-\u0026gt; Peekable\u0026lt;Self\u0026gt;;fn skip_while\u0026lt;P\u0026gt;(self,predicate: P)-\u0026gt; SkipWhile\u0026lt;Self,P\u0026gt;whereP: FnMut(\u0026amp;Self::Item)-\u0026gt; bool;fn take_while\u0026lt;P\u0026gt;(self,predicate: P)-\u0026gt; TakeWhile\u0026lt;Self,P\u0026gt;whereP: FnMut(\u0026amp;Self::Item)-\u0026gt; bool;fn map_while\u0026lt;B,P\u0026gt;(self,predicate: P)-\u0026gt; MapWhile\u0026lt;Self,P\u0026gt;whereP: FnMut(Self::Item)-\u0026gt; Option\u0026lt;B\u0026gt;;fn skip(self,n: usize)-\u0026gt; Skip\u0026lt;Self\u0026gt;;fn take(self,n: usize)-\u0026gt; Take\u0026lt;Self\u0026gt;;fn scan\u0026lt;St,B,F\u0026gt;(self,initial_state: St,f: F)-\u0026gt; Scan\u0026lt;Self,St,F\u0026gt;whereF: FnMut(\u0026amp;mutSt,Self::Item)-\u0026gt; Option\u0026lt;B\u0026gt;;fn flat_map\u0026lt;U,F\u0026gt;(self,f: F)-\u0026gt; FlatMap\u0026lt;Self,U,F\u0026gt;whereF: FnMut(Self::Item)-\u0026gt; U,U: IntoIterator;fn flatten(self)-\u0026gt; Flatten\u0026lt;Self\u0026gt;whereSelf::Item: IntoIterator;fn fuse(self)-\u0026gt; Fuse\u0026lt;Self\u0026gt;;fn inspect\u0026lt;F\u0026gt;(self,f: F)-\u0026gt; Inspect\u0026lt;Self,F\u0026gt;whereF: FnMut(\u0026amp;Self::Item);fn by_ref(\u0026amp;mutself)-\u0026gt; \u0026amp;mutSelf;fn collect\u0026lt;B\u0026gt;(self)-\u0026gt; BwhereB: FromIterator\u0026lt;Self::Item\u0026gt;;fn partition\u0026lt;B,F\u0026gt;(self,f: F)-\u0026gt; (B,B)whereF: FnMut(\u0026amp;Self::Item)-\u0026gt; bool,B: Default +Extend\u0026lt;Self::Item\u0026gt;;fn partition_in_place\u0026lt;\u0026#39;a,T,P\u0026gt;(self,predicate: P)-\u0026gt; usize whereSelf: DoubleEndedIterator\u0026lt;Item=\u0026amp;\u0026#39;amutT\u0026gt;,T: \u0026#39;a,P: FnMut(\u0026amp;T)-\u0026gt; bool;fn is_partitioned\u0026lt;P\u0026gt;(self,predicate: P)-\u0026gt; bool whereP: FnMut(Self::Item)-\u0026gt; bool;fn try_fold\u0026lt;B,F,R\u0026gt;(\u0026amp;mutself,init: B,f: F)-\u0026gt; RwhereF: FnMut(B,Self::Item)-\u0026gt; R,R: Try\u0026lt;Ok=B\u0026gt;;fn try_for_each\u0026lt;F,R\u0026gt;(\u0026amp;mutself,f: F)-\u0026gt; RwhereF: FnMut(Self::Item)-\u0026gt; R,R: Try\u0026lt;Ok=()\u0026gt;;fn fold\u0026lt;B,F\u0026gt;(self,init: B,f: F)-\u0026gt; BwhereF: FnMut(B,Self::Item)-\u0026gt; B;fn fold_first\u0026lt;F\u0026gt;(self,f: F)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereF: FnMut(Self::Item,Self::Item)-\u0026gt; Self::Item;fn all\u0026lt;F\u0026gt;(\u0026amp;mutself,f: F)-\u0026gt; bool whereF: FnMut(Self::Item)-\u0026gt; bool;fn any\u0026lt;F\u0026gt;(\u0026amp;mutself,f: F)-\u0026gt; bool whereF: FnMut(Self::Item)-\u0026gt; bool;fn find\u0026lt;P\u0026gt;(\u0026amp;mutself,predicate: P)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereP: FnMut(\u0026amp;Self::Item)-\u0026gt; bool;fn find_map\u0026lt;B,F\u0026gt;(\u0026amp;mutself,f: F)-\u0026gt; Option\u0026lt;B\u0026gt;whereF: FnMut(Self::Item)-\u0026gt; Option\u0026lt;B\u0026gt;;fn try_find\u0026lt;F,R\u0026gt;(\u0026amp;mutself,f: F)-\u0026gt; Result\u0026lt;Option\u0026lt;Self::Item\u0026gt;,\u0026lt;RasTry\u0026gt;::Error\u0026gt;whereF: FnMut(\u0026amp;Self::Item)-\u0026gt; R,R: Try\u0026lt;Ok=bool\u0026gt;;fn position\u0026lt;P\u0026gt;(\u0026amp;mutself,predicate: P)-\u0026gt; Option\u0026lt;usize\u0026gt;whereP: FnMut(Self::Item)-\u0026gt; bool;fn rposition\u0026lt;P\u0026gt;(\u0026amp;mutself,predicate: P)-\u0026gt; Option\u0026lt;usize\u0026gt;whereSelf: ExactSizeIterator +DoubleEndedIterator,P: FnMut(Self::Item)-\u0026gt; bool;fn max(self)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereSelf::Item: Ord;fn min(self)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereSelf::Item: Ord;fn max_by_key\u0026lt;B,F\u0026gt;(self,f: F)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereF: FnMut(\u0026amp;Self::Item)-\u0026gt; B,B: Ord;fn max_by\u0026lt;F\u0026gt;(self,compare: F)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereF: FnMut(\u0026amp;Self::Item,\u0026amp;Self::Item)-\u0026gt; Ordering;fn min_by_key\u0026lt;B,F\u0026gt;(self,f: F)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereF: FnMut(\u0026amp;Self::Item)-\u0026gt; B,B: Ord;fn min_by\u0026lt;F\u0026gt;(self,compare: F)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereF: FnMut(\u0026amp;Self::Item,\u0026amp;Self::Item)-\u0026gt; Ordering;fn rev(self)-\u0026gt; Rev\u0026lt;Self\u0026gt;whereSelf: DoubleEndedIterator;fn unzip\u0026lt;A,B,FromA,FromB\u0026gt;(self)-\u0026gt; (FromA,FromB)whereSelf: Iterator\u0026lt;Item=(A,B)\u0026gt;,FromA: Default +Extend\u0026lt;A\u0026gt;,FromB: Default +Extend\u0026lt;B\u0026gt;;fn copied\u0026lt;\u0026#39;a,T\u0026gt;(self)-\u0026gt; Copied\u0026lt;Self\u0026gt;whereSelf: Iterator\u0026lt;Item=\u0026amp;\u0026#39;aT\u0026gt;,T: \u0026#39;a+Copy;fn cloned\u0026lt;\u0026#39;a,T\u0026gt;(self)-\u0026gt; Cloned\u0026lt;Self\u0026gt;whereSelf: Iterator\u0026lt;Item=\u0026amp;\u0026#39;aT\u0026gt;,T: \u0026#39;a+Clone;fn cycle(self)-\u0026gt; Cycle\u0026lt;Self\u0026gt;whereSelf: Clone;fn sum\u0026lt;S\u0026gt;(self)-\u0026gt; SwhereS: Sum\u0026lt;Self::Item\u0026gt;;fn product\u0026lt;P\u0026gt;(self)-\u0026gt; PwhereP: Product\u0026lt;Self::Item\u0026gt;;fn cmp\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; OrderingwhereI: IntoIterator\u0026lt;Item=Self::Item\u0026gt;,Self::Item: Ord;fn cmp_by\u0026lt;I,F\u0026gt;(self,other: I,cmp: F)-\u0026gt; OrderingwhereF: FnMut(Self::Item,\u0026lt;IasIntoIterator\u0026gt;::Item)-\u0026gt; Ordering,I: IntoIterator;fn partial_cmp\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; Option\u0026lt;Ordering\u0026gt;whereI: IntoIterator,Self::Item: PartialOrd\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn partial_cmp_by\u0026lt;I,F\u0026gt;(self,other: I,partial_cmp: F)-\u0026gt; Option\u0026lt;Ordering\u0026gt;whereF: FnMut(Self::Item,\u0026lt;IasIntoIterator\u0026gt;::Item)-\u0026gt; Option\u0026lt;Ordering\u0026gt;,I: IntoIterator;fn eq\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialEq\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn eq_by\u0026lt;I,F\u0026gt;(self,other: I,eq: F)-\u0026gt; bool whereF: FnMut(Self::Item,\u0026lt;IasIntoIterator\u0026gt;::Item)-\u0026gt; bool,I: IntoIterator;fn ne\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialEq\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn lt\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialOrd\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn le\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialOrd\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn gt\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialOrd\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn ge\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialOrd\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn is_sorted(self)-\u0026gt; bool whereSelf::Item: PartialOrd\u0026lt;Self::Item\u0026gt;;fn is_sorted_by\u0026lt;F\u0026gt;(self,compare: F)-\u0026gt; bool whereF: FnMut(\u0026amp;Self::Item,\u0026amp;Self::Item)-\u0026gt; Option\u0026lt;Ordering\u0026gt;;fn is_sorted_by_key\u0026lt;F,K\u0026gt;(self,f: F)-\u0026gt; bool whereF: FnMut(Self::Item)-\u0026gt; K,K: PartialOrd\u0026lt;K\u0026gt;;}Iterator\u0026lt;Item = T\u0026gt; 类型可以被迭代，并会产生 T 类型。没有 IteratorMut trait。每个 Iterator 实现可以通过 Item 关联类型指定它是返回不可变引用、可变引用还是拥有其值。\n   Vec\u0026lt;T\u0026gt; 方法 返回     .iter() Iterator\u0026lt;Item = \u0026amp;T\u0026gt;   .iter_mut() Iterator\u0026lt;Item = \u0026amp;mut T\u0026gt;   .into_iter() Iterator\u0026lt;Item = T\u0026gt;    对于初学者来说，有些东西不是很明显，但中级 Rustaceans 认为是理所当然的，那就是大多数类型都不是他们本身的迭代器。如果一个类型是可迭代的，我们几乎总是实现一些自定义的迭代器类型来迭代它，而不是试图让它自己迭代。\nstruct MyType{items: Vec\u0026lt;String\u0026gt;}implMyType{fn iter(\u0026amp;self)-\u0026gt; implIterator\u0026lt;Item=\u0026amp;String\u0026gt;{MyTypeIterator{index: 0,items: \u0026amp;self.items}}}struct MyTypeIterator\u0026lt;\u0026#39;a\u0026gt;{index: usize,items: \u0026amp;\u0026#39;aVec\u0026lt;String\u0026gt;}impl\u0026lt;\u0026#39;a\u0026gt;IteratorforMyTypeIterator\u0026lt;\u0026#39;a\u0026gt;{type Item=\u0026amp;\u0026#39;aString;fn next(\u0026amp;mutself)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;{ifself.index\u0026gt;=self.items.len(){None}else{letitem=\u0026amp;self.items[self.index];self.index+=1;Some(item)}}}为了便于教学，上面的例子展示了如何从头开始实现一个 Iterator，但在这种情况下，习惯性的解决方案将只是遵从 Vec 的 iter 方法。\nstruct MyType{items: Vec\u0026lt;String\u0026gt;}implMyType{fn iter(\u0026amp;self)-\u0026gt; implIterator\u0026lt;Item=\u0026amp;String\u0026gt;{self.items.iter()}}另外，这也是一个很好的通用全面实现，要注意。\nimpl\u0026lt;I: Iterator +?Sized\u0026gt;Iteratorfor\u0026amp;mutI;它说任何对迭代器的可变引用也是迭代器，这一点很有用，因为它允许我们使用 self 接收器，就像使用 \u0026amp;mut self 接收器一样。了解这一点很有用，因为它允许我们使用迭代器方法与 self 接收器，就像它们有 \u0026amp;mut self 接收器一样。\n举个例子，想象一下，我们有一个函数，它可以处理一个超过三个项的迭代器，但是函数的第一步是取出迭代器的前三项，并在迭代剩下的项之前分别处理它们，下面是一个初学者可能尝试写这个函数的方法。\nfn example\u0026lt;I: Iterator\u0026lt;Item=i32\u0026gt;\u0026gt;(mutiter: I){letfirst3: Vec\u0026lt;i32\u0026gt;=iter.take(3).collect();foriteminiter{// ❌ iter consumed in line above // process remaining items }}嗯，这很烦人。take 方法有一个 self 接收器，所以我们似乎不能在不消耗整个迭代器的情况下调用它。下面是上面代码的一个天真的重构。\nfn example\u0026lt;I: Iterator\u0026lt;Item=i32\u0026gt;\u0026gt;(mutiter: I){letfirst3: Vec\u0026lt;i32\u0026gt;=vec![iter.next().unwrap(),iter.next().unwrap(),iter.next().unwrap(),];foriteminiter{// ✅ // process remaining items }}这还算好的。然而，惯用\u0008的重构其实是这样的。\nfn example\u0026lt;I: Iterator\u0026lt;Item=i32\u0026gt;\u0026gt;(mutiter: I){letfirst3: Vec\u0026lt;i32\u0026gt;=iter.by_ref().take(3).collect();foriteminiter{// ✅ // process remaining items }}不太容易发现。但无论如何，现在我们知道了。\n另外，对于什么可以是迭代器，什么不能是迭代器，并没有什么规则或约定。如果类型是 Iterator，那么它就是一个迭代器。标准库中的一些创造性的例子如下。\nusestd::sync::mpsc::channel;usestd::thread;fn paths_can_be_iterated(path: \u0026amp;Path){forpartinpath{// iterate over parts of a path }}fn receivers_can_be_iterated(){let(send,recv)=channel();thread::spawn(move||{send.send(1).unwrap();send.send(2).unwrap();send.send(3).unwrap();});forreceivedinrecv{// iterate over received values }}IntoIterator 预备知识\n Self Methods Associated Types Iterator  traitIntoIteratorwhere\u0026lt;Self::IntoIterasIterator\u0026gt;::Item==Self::Item,{type Item;type IntoIter: Iterator;fn into_iter(self)-\u0026gt; Self::IntoIter;}IntoIterator 类型可以被转换为迭代器，因此得名。当一个类型在 for-in 循环中使用时，会调用 into_iter 方法。\n// vec = Vec\u0026lt;T\u0026gt; forvinvec{}// v = T // above line desugared forvinvec.into_iter(){}不仅 Vec 实现了 IntoIterator，如果我们想分别迭代不可变引用或可变引用而不是拥有其值，\u0026amp;Vec 和 \u0026amp;mut Vec 也分别实现了 IntoIterator。\n// vec = Vec\u0026lt;T\u0026gt; forvin\u0026amp;vec{}// v = \u0026amp;T // above example desugared forvin(\u0026amp;vec).into_iter(){}// vec = Vec\u0026lt;T\u0026gt; forvin\u0026amp;mutvec{}// v = \u0026amp;mut T // above example desugared forvin(\u0026amp;mutvec).into_iter(){}FromIterator 预备知识\n Self Functions Generic Parameters Iterator IntoIterator  traitFromIterator\u0026lt;A\u0026gt;{fn from_iter\u0026lt;T\u0026gt;(iter: T)-\u0026gt; SelfwhereT: IntoIterator\u0026lt;Item=A\u0026gt;;}FromIterator 类型可以从迭代器中创建，因此也被称为 FromIterator。FromIterator 最常见和最习惯的用法是调用 Iterator 上的 collect 方法。\nfn collect\u0026lt;B\u0026gt;(self)-\u0026gt; BwhereB: FromIterator\u0026lt;Self::Item\u0026gt;;将 Iterator\u0026lt;Item = char\u0026gt; 收集成 String 的例子如下:\nfn filter_letters(string: \u0026amp;str)-\u0026gt; String {string.chars().filter(|c|c.is_alphabetic()).collect()}标准库中的所有集合都实现了 IntoIterator 和 FromIterator，这样可以更容易在它们之间进行转换。\nusestd::collections::{BTreeSet,HashMap,HashSet,LinkedList};// String -\u0026gt; HashSet\u0026lt;char\u0026gt; fn unique_chars(string: \u0026amp;str)-\u0026gt; HashSet\u0026lt;char\u0026gt;{string.chars().collect()}// Vec\u0026lt;T\u0026gt; -\u0026gt; BTreeSet\u0026lt;T\u0026gt; fn ordered_unique_items\u0026lt;T: Ord\u0026gt;(vec: Vec\u0026lt;T\u0026gt;)-\u0026gt; BTreeSet\u0026lt;T\u0026gt;{vec.into_iter().collect()}// HashMap\u0026lt;K, V\u0026gt; -\u0026gt; LinkedList\u0026lt;(K, V)\u0026gt; fn entry_list\u0026lt;K,V\u0026gt;(map: HashMap\u0026lt;K,V\u0026gt;)-\u0026gt; LinkedList\u0026lt;(K,V)\u0026gt;{map.into_iter().collect()}// and countless more possible examples I/O Traits Read \u0026amp; Write 预备知识\n Self Methods Scope Generic Blanket Impls  traitRead{fn read(\u0026amp;mutself,buf: \u0026amp;mut[u8])-\u0026gt; Result\u0026lt;usize\u0026gt;;// provided default impls fn read_vectored(\u0026amp;mutself,bufs: \u0026amp;mut[IoSliceMut\u0026lt;\u0026#39;_\u0026gt;])-\u0026gt; Result\u0026lt;usize\u0026gt;;fn is_read_vectored(\u0026amp;self)-\u0026gt; bool;unsafefn initializer(\u0026amp;self)-\u0026gt; Initializer;fn read_to_end(\u0026amp;mutself,buf: \u0026amp;mutVec\u0026lt;u8\u0026gt;)-\u0026gt; Result\u0026lt;usize\u0026gt;;fn read_to_string(\u0026amp;mutself,buf: \u0026amp;mutString)-\u0026gt; Result\u0026lt;usize\u0026gt;;fn read_exact(\u0026amp;mutself,buf: \u0026amp;mut[u8])-\u0026gt; Result\u0026lt;()\u0026gt;;fn by_ref(\u0026amp;mutself)-\u0026gt; \u0026amp;mutSelfwhereSelf: Sized;fn bytes(self)-\u0026gt; Bytes\u0026lt;Self\u0026gt;whereSelf: Sized;fn chain\u0026lt;R: Read\u0026gt;(self,next: R)-\u0026gt; Chain\u0026lt;Self,R\u0026gt;whereSelf: Sized;fn take(self,limit: u64)-\u0026gt; Take\u0026lt;Self\u0026gt;whereSelf: Sized;}traitWrite{fn write(\u0026amp;mutself,buf: \u0026amp;[u8])-\u0026gt; Result\u0026lt;usize\u0026gt;;fn flush(\u0026amp;mutself)-\u0026gt; Result\u0026lt;()\u0026gt;;// provided default impls fn write_vectored(\u0026amp;mutself,bufs: \u0026amp;[IoSlice\u0026lt;\u0026#39;_\u0026gt;])-\u0026gt; Result\u0026lt;usize\u0026gt;;fn is_write_vectored(\u0026amp;self)-\u0026gt; bool;fn write_all(\u0026amp;mutself,buf: \u0026amp;[u8])-\u0026gt; Result\u0026lt;()\u0026gt;;fn write_all_vectored(\u0026amp;mutself,bufs: \u0026amp;mut[IoSlice\u0026lt;\u0026#39;_\u0026gt;])-\u0026gt; Result\u0026lt;()\u0026gt;;fn write_fmt(\u0026amp;mutself,fmt: Arguments\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; Result\u0026lt;()\u0026gt;;fn by_ref(\u0026amp;mutself)-\u0026gt; \u0026amp;mutSelfwhereSelf: Sized;}通用的全面实现值得了解。\nimpl\u0026lt;R: Read+?Sized\u0026gt;Readfor\u0026amp;mutR;impl\u0026lt;W: Write+?Sized\u0026gt;Writefor\u0026amp;mutW;这说明任何对 Read 类型的可变引用也是 Read，对 Write 也是如此。了解这一点很有用，因为它允许我们使用任何带有 self 接收器的方法，就像它有一个 \u0026amp;mut self 接收器一样。我们已经在 Iterator trait 部分介绍了如何做到这一点以及为什么它很有用，所以我不打算在这里再次重复。\n我想指出，\u0026amp;[u8] 实现了 Read，Vec\u0026lt;u8\u0026gt; 实现了 Write，所以我们可以很容易地使用 String 对我们的文件处理函数进行单元测试，这些函数很容易转换为 \u0026amp;[u8] 和 Vec\u0026lt;u8\u0026gt;。\nusestd::path::Path;usestd::fs::File;usestd::io::Read;usestd::io::Write;usestd::io;// function we want to test fn uppercase\u0026lt;R: Read,W: Write\u0026gt;(mutread: R,mutwrite: W)-\u0026gt; Result\u0026lt;(),io::Error\u0026gt;{letmutbuffer=String::new();read.read_to_string(\u0026amp;mutbuffer)?;letuppercase=buffer.to_uppercase();write.write_all(uppercase.as_bytes())?;write.flush()?;Ok(())}// in actual program we\u0026#39;d pass Files fn example(in_path: \u0026amp;Path,out_path: \u0026amp;Path)-\u0026gt; Result\u0026lt;(),io::Error\u0026gt;{letin_file=File::open(in_path)?;letout_file=File::open(out_path)?;uppercase(in_file,out_file)}// however in unit tests we can use Strings! #[test]// ✅ fn example_test(){letin_file: String =\u0026#34;i am screaming\u0026#34;.into();letmutout_file: Vec\u0026lt;u8\u0026gt;=Vec::new();uppercase(in_file.as_bytes(),\u0026amp;mutout_file).unwrap();letout_result=String::from_utf8(out_file).unwrap();assert_eq!(out_result,\u0026#34;I AM SCREAMING\u0026#34;);}结论 我们一起学到了很多东西! 事实上，太多了。它现在是我们的了:\nArtist credit: The Jenkins Comic\n讨论 在这里讨论这篇文章\n Github learnrust subreddit official Rust users forum Twitter lobste.rs rust subreddit  通知 当下一篇博文发布时，会收到通知\n Following pretzelhammer on Twitter or Watching this repo\u0026rsquo;s releases (click Watch -\u0026gt; click Custom -\u0026gt; select Releases -\u0026gt; click Apply)  更多阅读  Sizedness in Rust Common Rust Lifetime Misconceptions Learning Rust in 2020 Learn Assembly with Entirely Too Many Brainfuck Compilers  ","permalink":"https://ohmyweekly.github.io/notes/2021-05-19-a-tour-of-rusts-standard-library-traits/","tags":["Rust"],"title":"Rust 标准库中的 Trait 之旅"},{"categories":["rakulang"],"contents":"我最近写了一篇关于新的 MoarVM 调度机制的文章，并在那篇文章中指出，我在 Raku 的多重分派语义方面还有不少需要实现的地方。从那以后，我在这个方向上取得了不小的进展。这篇文章包含了对所采取的方法的概述，以及一些非常粗略的性能测量。\n我的天啊，语义太多了 在 Raku 的所有分派中，多重分派是最复杂的。多重分派允许我们写一组候选者，然后根据参数的数量进行选择。\nmulti ok($condition, $desc) { say ($condition ?? \u0026#39;ok\u0026#39; !! \u0026#39;not ok\u0026#39;) ~ \u0026#34;- $desc\u0026#34;; } multi ok($condition) { ok($condition, \u0026#39;\u0026#39;); } 或根据参数的类型:\nmulti to-json(Int $i) { ~$i } multi to-json(Bool $b) { $b ?? \u0026#39;true\u0026#39; !! \u0026#39;false\u0026#39; } 而且不只是一个参数，而是可能有很多参数:\nmulti truncate(Str $str, Int $chars) { $str.chars \u0026lt; $chars ?? $str !! $str.substr(0, $chars) ~ \u0026#39;...\u0026#39; } multi truncate(Str $str, Str $after) { with $str.index($after) -\u0026gt; $pos { $str.substr(0, $pos) ~ \u0026#39;...\u0026#39; } else { $str } } 我们可以添加 where 子句来区分普通类型无法捕捉的属性上的候选者。\nmulti fac($n where $n \u0026lt;= 1) { 1 } multi fac($n) { $n * fac($n - 1) } 每当我们写出一组这样的 multi 候选列表时，编译器就会自动生成一个 proto 例程。这就是安装在符号表中的，存放候选列表的东西。然而，我们也可以写自己的 proto，并使用特殊的术语 {*} 来决定在哪一点上进行调度，如果有的话。\nproto mean($collection) { $collection.elems == 0 ?? Nil !! {*} } multi mean(@arr) { @arr.sum / @arr.elems } multi mean(%hash) { %hash.values.sum / %hash.elems } 候选者按窄度排序（使用拓扑排序）。如果多个候选者匹配，但它们窄度相同，那么这就是一个歧义错误。否则，我们调用最窄的一个。然后，我们选择的候选者可能会使用 callsame 和它的朋友们来推迟到下一个最窄的候选者，后者可能也会这样做，直到我们达到最一般的匹配的候选者。\n多重分派无处不在 Raku 在很大程度上依赖于多重分派。Raku 中的大多数操作符都被编译成对多重分派子程序的调用。即使是 $a+$b 也会是一个多重分派。这意味着高效地进行多重分派对性能真的很重要。考虑到其语义的丰富性，这有可能有点令人担忧。不过，也有好消息。\n大多数多重调度都很无聊 我们遇到的绝大多数情况是:\n 一个仅由参数和名义类型的数量所做的决定。 无 where 子句 无自定义 proto 无 callsame  这并不是说其他情况不重要，它们确实相当有用，而且它们的表现也是可取的。不过，在普通情况下，我们能省则省，也是可取的。例如，我们不希望急于计算每一个单次多重调度的全部可能的候选者，因为大多数时候只有第一个才是重要的。这不仅仅是时间上的问题：回想一下，新的调度机制会在每个调用点存储调度程序，如果我们在每个调用点存储所有匹配的候选程序列表，我们也会浪费很多内存。\n我们今天怎么做？ 如今 Rakuo 的情况如下:\n  如果调度只由元数和名义类型决定，并且你不使用扁平化的参数来调用它，它可能会表现得很好，甚至可能会享受到候选者的内联和消除在慢速路径上发生的重复类型检查。这要归功于 proto 持有一个 \u0026ldquo;dispatch cache\u0026rdquo;，这是一个在 VM 中实现的特例机制，它使用搜索树，每个参数有一个级别。\n  如果是这种情况，但它有一个自定义的 proto，也不会太差，虽然不会发生内联，它仍然可以使用搜索树。\n  如果它使用 where 子句，速度会很慢，因为搜索树只处理在每一个名义类型集合中找到一个候选者，所以我们不能使用它。\n  同样的道理也适用于 callsame，它的速度也会很慢。\n  实际上，今天的情况是，如果在热路径附近的任何地方，你根本不会在多重调度中使用 where子句（好吧，如果你知道热路径在哪里，并且知道这种调度很慢）。同理，callsame 也是如此，虽然那不太常触达。问题是，我们能不能用新的调度器做得更好？\n守卫类型 我们先看看最简单的情况是如何处理的，然后再从那里开始建立。(这其实是我在实现上的做法，但同时我也有一个大概的想法，我希望最终的结果是什么)。\n回忆一下这对候选者。\nmulti truncate(Str $str, Int $chars) { $str.chars \u0026lt; $chars ?? $str !! $str.substr(0, $chars) ~ \u0026#39;...\u0026#39; } multi truncate(Str $str, Str $after) { with $str.index($after) -\u0026gt; $pos { $str.substr(0, $pos) ~ \u0026#39;...\u0026#39; } else { $str } } 然后我们有一个调用 truncate($message, \u0026quot;\\n\u0026quot;)，其中 $message 是 Str 类型的。在新的调度机制下，调用是使用 raku-call dispatcher 进行的，它识别出这是一个 multi 方法调度，因此委托给 raku-multi。(multi 方法调度也会在那里结束)。\n调度的记录阶段 - 在我们第一次到达这个调用点时 - 将进行如下操作。\n 迭代候选者 如果某个候选者在参数数上不匹配，就直接丢弃它。由于 callsite 的形状是一个常数，而且我们在每个 callsite 都会计算 dispatch 程序，所以我们不需要为此建立任何防护措施。 如果在类型匹配并且成功了，注意涉及哪些参数，需要什么样的守卫。 如果没有匹配或者模棱两可，就报错，不产生调度程序。 否则，在确定了类型守卫后，将选定的候选程序委托给 raku-invoke 调度程序。  当我们再次到达同一个调用点时，我们可以运行调度程序，它可以快速检查参数类型是否与上次看到的参数类型相匹配，如果相匹配，我们就知道要调用哪个候选程序。这些检查非常便宜 - 比遍历所有候选者并检查每个候选者是否匹配要便宜得多。优化器以后可能会证明这些检查总是会成为事实，并消除它们。\n因此，整个调度过程 - 至少对于这个我们只有类型和元数的简单案例 - 可以向虚拟机 \u0026ldquo;解释\u0026rdquo; 为 \u0026ldquo;如果参数具有这些确切的类型，就调用这个例程\u0026rdquo;。这和我们对方法分派所做的差不多，除了我们只关心第一个参数的类型 - 调用者 - 和方法名的值。(还记得上一篇文章中说过，如果是 multi 方法调度，那么方法调度和 multi 方法调度都会守护第一个参数的类型，但是消除了重复，所以只做一次检查)。\n这就进入了恢复之洞 想出好的抽象是很难的，新的调度机制的很多挑战就在于此。Raku 有不少不同的类似调度的东西。然而，将它们全部直接编码在虚拟机中会导致很高的复杂度，这使得构建可靠的优化（甚至是可靠的未优化的实现！）具有挑战性。因此，我们的目标是研究出一套相对较小的原语，允许以这样一种方式向虚拟机 \u0026ldquo;解释\u0026rdquo; 调度，使其能够提供不错的性能。\n很明显，callsame 是一种调度恢复，但自定义 proto 这种情况和 where 子句这种情况呢？事实证明，这些也都可以用调度恢复的方式整齐地表达出来（where 子句情况需要在虚拟机层面增加一个小的内容，到时候可能对其他事情也有用）。不仅如此，用调度恢复来编码这些特性也是相当直接的，因此应该是高效的。我们教给专门人员的关于如何更好地使用调度恢复的每一个技巧，都可以让所有使用它们实现的语言特性也受益。\n自定义 proto 回顾这个例子。\nproto mean($collection) { $collection.elems == 0 ?? Nil !! {*} } 在这里，我们希望运行 proto 的主体，然后在 {*} 这里进行候选者的选择。相比之下，当我们没有自定义的 proto 时，我们希望简单地继续调用正确的多。\n为了达到这个目的，我首先将 multi 候选者的选择逻辑从 raku-multi 调度器移到了 raku-multi-core 调度器中。然后，raku-multi dispatcher 会检查我们是否有一个 \u0026ldquo;onlystar\u0026rdquo; proto（一个不需要我们运行的 proto）。如果有，它就会立即委托给 raku-multi-core。如果没有，它就将调度的参数保存为恢复初始化状态，然后调用 proto。proto 的 {*} 被编译成一个 dispatch resumption。然后，这个 resumption 委托给 raku-multi-core。或者，在代码中。\nnqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-register\u0026#39;, \u0026#39;raku-multi\u0026#39;, # Initial dispatch, only setting up resumption if we need to invoke the # proto. -\u0026gt; $capture { my $callee := nqp::captureposarg($capture, 0); my int $onlystar := nqp::getattr_i($callee, Routine, \u0026#39;$!onlystar\u0026#39;); if $onlystar { # Don\u0026#39;t need to invoke the proto itself, so just get on with the # candidate dispatch. nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;raku-multi-core\u0026#39;, $capture); } else { # Set resume init args and run the proto. nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-set-resume-init-args\u0026#39;, $capture); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;raku-invoke\u0026#39;, $capture); } }, # Resumption means that we have reached the {*} in the proto and so now # should go ahead and do the dispatch. Make sure we only do this if we # are signalled to that it\u0026#39;s a resume for an onlystar (resumption kind 5). -\u0026gt; $capture { my $track_kind := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-arg\u0026#39;, $capture, 0); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-guard-literal\u0026#39;, $track_kind); my int $kind := nqp::captureposarg_i($capture, 0); if $kind == 5 { nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;raku-multi-core\u0026#39;, nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-get-resume-init-args\u0026#39;)); } elsif !nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-next-resumption\u0026#39;) { nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;boot-constant\u0026#39;, nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-insert-arg-literal-obj\u0026#39;, $capture, 0, Nil)); } }); 合二为一 推迟到下一个候选者（例如用 callsame）和因为 where 子句失败而尝试下一个候选者看起来非常相似：两者都涉及遍历一个可能的候选者列表。有一些细节，但它们有很多共同点，如果能在使用新的 dispatcher 实现多重分派的过程中体现出来就更好了。\n在这之前，先说一个略显可怕的细节，当我们有 where 子句的时候，今天在 Rakuo 中是如何工作的。首先，调度器会做一个 \u0026ldquo;试用绑定\u0026rdquo;，它问一个问题：这个签名会不会绑定？要做到这一点，它必须评估所有的 where 子句。更糟糕的是，它还必须使用慢路径签名绑定器，它对签名进行解释，尽管我们在很多情况下可以编译它。如果候选者匹配，很好，我们选择它，然后调用它\u0026hellip;\u0026hellip;这将第二次运行 where 子句，作为编译后的签名绑定代码的一部分。这样做一点也不高效，除了在开发人员的时间上要高效得多，这也是为什么要这样做的原因。\n总之，毋庸置疑，在我使用新的调度器重新实现时，我相当希望尽可能避免这种重复的工作和慢路径绑定。而且，令人高兴的是，一个小小的补充提供了一个解决方案。有一个 op assertparamcheck，任何类型的参数检查都会被编译成（无论是类型检查、where 子句检查等），这将触发对一个函数的调用，该函数获取参数，也就是我们试图调用的东西，然后可以通过它们来产生错误信息。诀窍是提供一种调用例程的方法，使绑定失败后，不是调用报错函数，而是离开例程，然后做一个调度恢复! 这意味着我们可以将传递 where 子句检查失败变成一个调度恢复，然后会走到下一个候选者，并代替它进行尝试。\n琐碎VS非琐碎 这让我们得到了大部分的解决方法，但在常见的情况下，仍然存在内存和时间效率的问题，即没有恢复和没有 where 子句。我为这种情况创造了一个术语 \u0026ldquo;trivial multiple dispatch\u0026rdquo;，这使得其他情况变得 \u0026ldquo;non-trivial\u0026rdquo;。事实上，我甚至做了一个调度器，叫做 raku-multi-non-trivial! 我们有两种方式可以结束。\n 最初尝试寻找匹配的候选者，决定了我们必须考虑 where 子句。一旦我们看到是这种情况，我们就会继续制作一个可能匹配的候选者的完整列表。这是一个链表（原因见我之前的文章）。 最初尝试寻找匹配的候选者时，发现了一个可以纯粹根据参数数和名词类型来挑选的候选者。我们就此停止，而不是试图建立一个完整的候选列表，并运行匹配的候选。在 callsame 的情况下，我们最终进入琐碎的调度恢复处理程序，它 - 因为这种情况现在是非琐碎的 - 建立完整的候选者列表，从它上面剪下第一项（因为我们已经运行了那项），然后委托给 raku-multi-non-trivial。  在这个描述中失去了另一个重要的改进：今天，当有 where 子句时，我们完全失去了使用 MoarVM 多重调度缓存的能力，但在新的调度器下，我们在 callsite 存储了一个类型过滤的候选列表，然后使用廉价的类型守卫来检查它是否有效使用。\n初步结果 我做了一些基准测试，看看新的调度机制在今天 Raku 已知的几种次优情况下的表现。这些数字并不能反映出什么是可能的，因为目前专门人员对新的调度器还没有太多的了解。相反，它们反映了我们可以期望的最小改进。\n考虑这个基准，使用一个带有 where 子句的 multi 来递归实现 factorial。\nmulti fac($n where $n \u0026lt;= 1) { 1 } multi fac($n) { $n * fac($n - 1) } for ^100_000 { fac(10) } say now - INIT now; 这需要进行一些调整（并在环境变量下运行）以使用新的调度器；这些都是暂时的，直到我将 Rakudo 转换为默认使用新的调度器。\nuse nqp; multi fac($n where $n \u0026lt;= 1) { 1 } multi fac($n) { $n * nqp::dispatch(\u0026#39;raku-call\u0026#39;, \u0026amp;fac, $n - 1) } for ^100_000 { nqp::dispatch(\u0026#39;raku-call\u0026#39;, \u0026amp;fac, 10); } say now - INIT now; 在我的机器上，第一个运行时间为4.86秒，第二个运行时间为1.34秒。因此，在新的调度器下，运行时间只需过去的四分之一多一点 - 这已经是一个相当大的改进了。\n一个涉及 callsame 的案例也很有意思。这里是没有使用新调度器的情况。\nmulti fallback(Any $x) { \u0026#34;a$x\u0026#34; } multi fallback(Numeric $x) { \u0026#34;n\u0026#34; ~ callsame } multi fallback(Real $x) { \u0026#34;r\u0026#34; ~ callsame } multi fallback(Int $x) { \u0026#34;i\u0026#34; ~ callsame } for ^1_000_000 { fallback(4+2i); fallback(4.2); fallback(42); } say now - INIT now; 而配合临时调整使用新的调度器:\nuse nqp; multi fallback(Any $x) { \u0026#34;a$x\u0026#34; } multi fallback(Numeric $x) { \u0026#34;n\u0026#34; ~ new-disp-callsame } multi fallback(Real $x) { \u0026#34;r\u0026#34; ~ new-disp-callsame } multi fallback(Int $x) { \u0026#34;i\u0026#34; ~ new-disp-callsame } for ^1_000_000 { nqp::dispatch(\u0026#39;raku-call\u0026#39;, \u0026amp;fallback, 4+2i); nqp::dispatch(\u0026#39;raku-call\u0026#39;, \u0026amp;fallback, 4.2); nqp::dispatch(\u0026#39;raku-call\u0026#39;, \u0026amp;fallback, 42); } say now - INIT now; 在我的机器上，第一个运行时间为31.3s，第二个运行时间为11.5s，这意味着使用新的调度器，我们最终需要的时间只有当前 Rakudo 的三分之一多一点。\n这些都是相当鼓舞人心的，但正如前面提到的，大部分的多重调度都是琐碎的那种，没有使用这些功能。如果我在让其他事情变得更好的路上把最常见的情况变得更糟，那就不好了。现在还不能对此进行公平的比较：琐碎的多重分派在特化器中已经受到了很多关注，而且它还不能很好地优化使用新调度器的代码。值得注意的是，在这样的例子中。\nmulti m(Int) { } multi m(Str) { } for ^1_000_000 { m(1); m(\u0026#34;x\u0026#34;); } say now - INIT now; 内嵌和其他优化会将其变成一个空循环，这是很难做到的。不过有一件事我们已经可以做了：在禁用 specializer 的情况下运行它。新的调度器版本看起来是这样的。\nuse nqp; multi m(Int) { } multi m(Str) { } for ^1_000_000 { nqp::dispatch(\u0026#39;raku-call\u0026#39;, \u0026amp;m, 1); nqp::dispatch(\u0026#39;raku-call\u0026#39;, \u0026amp;m, \u0026#34;x\u0026#34;); } say now - INIT now; 结果分别是0.463s和0.332s。因此，基线执行时间 - 在特化器发挥其魔力之前 - 使用新的通用调度机制比使用我们目前使用的特化多重调度缓存要少。在做测量之前，我不知道这里会有什么预期。鉴于我们要从一个已经被剖析和调整过的特化机制转到一个没有受到如此关注的新的通用机制，我已经做好了最初做得差一点的准备，如果能做到平价就好了。在70%的时间里，跑进了70%的时间，这比我预期的进步更大。\n我期望，一旦特化器更好地理解新的调度机制，它也能把上面的内容变成一个空循环 - 不过，由于每次优化可以进行更多的迭代，这应该还是表现为新调度器的胜利。\n最后的想法 只要增加一个相对较小的功能，新的调度机制已经可以处理大部分的 Raku 多重调度语义。此外，即使在 specializer 和 JIT 没有真正能够做好的情况下，一些微基准已经显示出3倍-4倍的提升。这是一个很好的起点。\n在我们使用新的调度器出货 Rakudo 版本之前，还有很多工作要做。然而，多重调度是设计中剩下的最大威胁：它比其他种类的调度相当多的参与，而且很有可能一个意想不到的缺点会引发新一轮的设计工作，或者揭示出一般机制与基线未优化的情况下, 与更专业的机制相比，性能会很吃力。到目前为止，没有任何迹象表明这两种情况，我谨慎乐观地认为整体设计是差不多的。\n","permalink":"https://ohmyweekly.github.io/notes/2021-04-15-raku-multiple-dispatch-with-the-new-moarvm-dispatcher/","tags":["Raku"],"title":"Raku Multiple Dispatch With the New MoarVM Dispatcher"},{"categories":["rakulang"],"contents":"Rust 的标准库 Trait 之旅 Table of Contents\n Intro Trait Basics  Trait Items  Self Functions Methods Associated Types Generic Parameters Generic Types vs Associated Types   Scope Derive Macros Default Impls Generic Blanket Impls Subtraits \u0026amp; Supertraits Trait Objects Marker Traits Auto Traits Unsafe Traits   Auto Traits  Send \u0026amp; Sync Sized   General Traits  Default Clone Copy Any   Formatting Traits  Display \u0026amp; ToString Debug   Operator Traits  Comparison Traits  PartialEq \u0026amp; Eq Hash PartialOrd \u0026amp; Ord   Arithmetic Traits  Add \u0026amp; AddAssign   Closure Traits  FnOnce, FnMut, \u0026amp; Fn   Other Traits  Deref \u0026amp; DerefMut Index \u0026amp; IndexMut Drop     Conversion Traits  From \u0026amp; Into   Error Handling  Error   Conversion Traits Continued  TryFrom \u0026amp; TryInto FromStr AsRef \u0026amp; AsMut Borrow \u0026amp; BorrowMut ToOwned   Iteration Traits  Iterator IntoIterator FromIterator   I/O Traits  Read \u0026amp; Write   Conclusion Discuss Notifications Further Reading  Intro 你有没有想过，这两者之间有什么区别?\n Deref\u0026lt;Target = T\u0026gt;, AsRef\u0026lt;T\u0026gt; 和 Borrow\u0026lt;T\u0026gt;? Clone, Copy 和 ToOwned? From\u0026lt;T\u0026gt; 和 Into\u0026lt;T\u0026gt;? TryFrom\u0026lt;\u0026amp;str\u0026gt; 和 FromStr? FnOnce, FnMut, Fn 和 fn?  或者曾经问过自己这样的问题:\n 在我的 trait 中, 我什么时候使用关联类型, 什么时候使用泛型类型? 什么是泛型的 blanket 实现? subtrait 和 supertrait 是如何工作的? 为什么这个 trait 没有任何方法?  那么这篇文章就是为你准备的! 它回答了以上所有的问题以及更多的问题。我们将一起对 Rust 标准库中所有最流行、最常用的 trait 进行快速飞越之旅!\n你可以按顺序逐节阅读本文，也可以跳转到你最感兴趣的 trait，因为每个 trait 部分都有一个链接列表，链接到 “先决知识” 部分，你应该阅读这些链接，以便有足够的背景来理解当前部分的解释。\nTrait 基础 We\u0026rsquo;ll cover just enough of the basics so that the rest of the article can be streamlined without having to repeat the same explanations of the same concepts over and over as they reappear in different traits.\n我们将只涉及足够的基础知识，以便文章的其余部分可以精简，而不必在不同的 trait 中重新出现时重复相同的概念解释。\nTrait 项 Trait 项是指作为 trait 声明一部分的任何项。\nSelf Self 总是指实现类型。\ntraitTrait{// always returns i32 fn returns_num()-\u0026gt; i32;// returns implementing type fn returns_self()-\u0026gt; Self;}struct SomeType;struct OtherType;implTraitforSomeType{fn returns_num()-\u0026gt; i32 {5}// Self == SomeType fn returns_self()-\u0026gt; Self{SomeType}}implTraitforOtherType{fn returns_num()-\u0026gt; i32 {6}// Self == OtherType fn returns_self()-\u0026gt; Self{OtherType}}函数 Trait 函数是任何第一个参数不使用 self 关键字的函数。\ntraitDefault{// function fn default()-\u0026gt; Self;}Trait 函数可以通过 trait 或实现类型按照命名空间的方式来调用。\nfn main(){letzero: i32 =Default::default();letzero=i32::default();}方法 Trait 方法是指第一个参数使用 self 关键字并且类型为 Self、\u0026amp;Self、\u0026amp;mut Self 的任何函数。前面的类型也可以用 Box、Rc、Arc 或 Pin 来包装。\ntraitTrait{// methods fn takes_self(self);fn takes_immut_self(\u0026amp;self);fn takes_mut_self(\u0026amp;mutself);// above methods desugared fn takes_self(self: Self);fn takes_immut_self(self: \u0026amp;Self);fn takes_mut_self(self: \u0026amp;mutSelf);}// example from standard library traitToString{fn to_string(\u0026amp;self)-\u0026gt; String;}可以使用实现类型上的点运算符来调用方法。\nfn main(){letfive=5.to_string();}然而，与函数类似，它们也可以通过 trait 或实现类型按照命名空间的方式来调用。\nfn main(){letfive=ToString::to_string(\u0026amp;5);letfive=i32::to_string(\u0026amp;5);}关联类型 Trait 可以有关联类型。当我们需要在函数签名中使用 Self 以外的其他类型，但又希望类型由实现者选择，而不是在 trait 声明中硬编码时，这很有用。\ntraitTrait{type AssociatedType;fn func(arg: Self::AssociatedType);}struct SomeType;struct OtherType;// any type implementing Trait can // choose the type of AssociatedType implTraitforSomeType{type AssociatedType=i8;// chooses i8 fn func(arg: Self::AssociatedType){}}implTraitforOtherType{type AssociatedType=u8;// chooses u8 fn func(arg: Self::AssociatedType){}}fn main(){SomeType::func(-1_i8);// can only call func with i8 on SomeType OtherType::func(1_u8);// can only call func with u8 on OtherType }泛型参数 “泛型参数” 泛指泛型类型参数、泛型 lifetime 参数和泛型常量参数。由于这些说起来都很拗口，所以人们通常把它们缩写为 \u0026ldquo;generic types\u0026rdquo;, \u0026ldquo;lifetimes\u0026rdquo; 和 \u0026ldquo;generic consts\u0026rdquo;。由于 generic consts 没有在我们将要涉及的任何标准库 trait 中使用，所以它们不在本文的范围之内。\n我们可以使用参数来泛型化一个 trait 声明。\n// trait declaration generalized with lifetime \u0026amp; type parameters traitTrait\u0026lt;\u0026#39;a,T\u0026gt;{// signature uses generic type fn func1(arg: T);// signature uses lifetime fn func2(arg: \u0026amp;\u0026#39;ai32);// signature uses generic type \u0026amp; lifetime fn func3(arg: \u0026amp;\u0026#39;aT);}struct SomeType;impl\u0026lt;\u0026#39;a\u0026gt;Trait\u0026lt;\u0026#39;a,i8\u0026gt;forSomeType{fn func1(arg: i8){}fn func2(arg: \u0026amp;\u0026#39;ai32){}fn func3(arg: \u0026amp;\u0026#39;ai8){}}impl\u0026lt;\u0026#39;b\u0026gt;Trait\u0026lt;\u0026#39;b,u8\u0026gt;forSomeType{fn func1(arg: u8){}fn func2(arg: \u0026amp;\u0026#39;bi32){}fn func3(arg: \u0026amp;\u0026#39;bu8){}}可以为泛型类型提供默认值。最常用的默认值是 Self，但任何类型都可以。\n// make T = Self by default traitTrait\u0026lt;T=Self\u0026gt;{fn func(t: T){}}// any type can be used as the default traitTrait2\u0026lt;T=i32\u0026gt;{fn func2(t: T){}}struct SomeType;// omitting the generic type will // cause the impl to use the default // value, which is Self here implTraitforSomeType{fn func(t: SomeType){}}// default value here is i32 implTrait2forSomeType{fn func2(t: i32){}}// the default is overridable as we\u0026#39;d expect implTrait\u0026lt;String\u0026gt;forSomeType{fn func(t: String){}}// overridable here too implTrait2\u0026lt;String\u0026gt;forSomeType{fn func2(t: String){}}除了对 trait 进行参数化外，还可以对单个函数和方法进行参数化。\ntraitTrait{fn func\u0026lt;\u0026#39;a,T\u0026gt;(t: \u0026amp;\u0026#39;aT);}泛型类型 vs 关联类型 泛型类型和关联类型都将决定权交给了实现者，让他们决定在 trait 的函数和方法中应该使用哪种具体类型，所以本节试图解释什么时候使用一种类型而不是另一种类型。\n一般的经验法则是\n 当每个类型只能有一个 trait 的实现时，使用关联类型。 当每个类型可以有许多可能的 trait 的实现时，使用泛型类型。  假设我们想定义一个名为 Add 的 trait，它允许我们将值加在一起。下面是一个初始设计和只使用关联类型的实现。\ntraitAdd{type Rhs;type Output;fn add(self,rhs: Self::Rhs)-\u0026gt; Self::Output;}struct Point{x: i32,y: i32,}implAddforPoint{type Rhs=Point;type Output=Point;fn add(self,rhs: Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}fn main(){letp1=Point{x: 1,y: 1};letp2=Point{x: 2,y: 2};letp3=p1.add(p2);assert_eq!(p3.x,3);assert_eq!(p3.y,3);}比方说，我们想给 Point 添加和 i32 相加的能力，其中 i32 将和 x 和 y 成员相加。\ntraitAdd{type Rhs;type Output;fn add(self,rhs: Self::Rhs)-\u0026gt; Self::Output;}struct Point{x: i32,y: i32,}implAddforPoint{type Rhs=Point;type Output=Point;fn add(self,rhs: Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}implAddforPoint{// ❌ type Rhs=i32;type Output=Point;fn add(self,rhs: i32)-\u0026gt; Point{Point{x: self.x+rhs,y: self.y+rhs,}}}fn main(){letp1=Point{x: 1,y: 1};letp2=Point{x: 2,y: 2};letp3=p1.add(p2);assert_eq!(p3.x,3);assert_eq!(p3.y,3);letp1=Point{x: 1,y: 1};letint2=2;letp3=p1.add(int2);// ❌ assert_eq!(p3.x,3);assert_eq!(p3.y,3);}这抛出:\nerror[E0119]: conflicting implementations of trait `Add` for type `Point`:\r--\u0026gt; src/main.rs:23:1\r|\r12 | impl Add for Point {\r| ------------------ first implementation here\r...\r23 | impl Add for Point {\r| ^^^^^^^^^^^^^^^^^^ conflicting implementation for `Point`\r由于 Add trait 没有任何泛型类型的参数化，我们只能对每个类型进行一次实现，这意味着我们只能为 Rhs 和 Output 选择一次类型！为了允许 Point 和 Point 相加,以及 i32 和 Point 相加，我们必须将 Rhs 从关联类型重构为泛型类型，这将允许我们用不同的类型参数为 Rhs 多次实现 Point trait。\ntraitAdd\u0026lt;Rhs\u0026gt;{type Output;fn add(self,rhs: Rhs)-\u0026gt; Self::Output;}struct Point{x: i32,y: i32,}implAdd\u0026lt;Point\u0026gt;forPoint{type Output=Self;fn add(self,rhs: Point)-\u0026gt; Self::Output{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}implAdd\u0026lt;i32\u0026gt;forPoint{// ✅ type Output=Self;fn add(self,rhs: i32)-\u0026gt; Self::Output{Point{x: self.x+rhs,y: self.y+rhs,}}}fn main(){letp1=Point{x: 1,y: 1};letp2=Point{x: 2,y: 2};letp3=p1.add(p2);assert_eq!(p3.x,3);assert_eq!(p3.y,3);letp1=Point{x: 1,y: 1};letint2=2;letp3=p1.add(int2);// ✅ assert_eq!(p3.x,3);assert_eq!(p3.y,3);}比方说，我们添加了一个名为 Line 的新类型，它包含两个 Point，现在在我们的程序中，将两个 Point 相加应该产生一个 Line 而不是 Point。考虑到 Add trait 当前的设计，这是不可能的，因为 Output 是一个关联类型，但是我们可以通过将 Output 从关联类型重构为泛型类型来满足这些新的要求。\ntraitAdd\u0026lt;Rhs,Output\u0026gt;{fn add(self,rhs: Rhs)-\u0026gt; Output;}struct Point{x: i32,y: i32,}implAdd\u0026lt;Point,Point\u0026gt;forPoint{fn add(self,rhs: Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}implAdd\u0026lt;i32,Point\u0026gt;forPoint{fn add(self,rhs: i32)-\u0026gt; Point{Point{x: self.x+rhs,y: self.y+rhs,}}}struct Line{start: Point,end: Point,}implAdd\u0026lt;Point,Line\u0026gt;forPoint{// ✅ fn add(self,rhs: Point)-\u0026gt; Line{Line{start: self,end: rhs,}}}fn main(){letp1=Point{x: 1,y: 1};letp2=Point{x: 2,y: 2};letp3: Point=p1.add(p2);assert!(p3.x==3\u0026amp;\u0026amp;p3.y==3);letp1=Point{x: 1,y: 1};letint2=2;letp3=p1.add(int2);assert!(p3.x==3\u0026amp;\u0026amp;p3.y==3);letp1=Point{x: 1,y: 1};letp2=Point{x: 2,y: 2};letl: Line=p1.add(p2);// ✅ assert!(l.start.x==1\u0026amp;\u0026amp;l.start.y==1\u0026amp;\u0026amp;l.end.x==2\u0026amp;\u0026amp;l.end.y==2)}那么，上面的 Add trait 哪种最好呢？这真的取决于你的程序的要求! 合适的就是最好的。\n作用域 Trait 项不能使用，除非该 trait 在作用域内。大多数 Rustaceans 在第一次尝试写一个用 I/O 做任何事情的程序时，都会艰难地学会这一点，因为 Read 和 Write trait 不在标准库的预加载中。\nusestd::fs::File;usestd::io;fn main()-\u0026gt; Result\u0026lt;(),io::Error\u0026gt;{letmutfile=File::open(\u0026#34;Cargo.toml\u0026#34;)?;letmutbuffer=String::new();file.read_to_string(\u0026amp;mutbuffer)?;// ❌ read_to_string not found in File Ok(())}read_to_string(buf: \u0026amp;mut String) 由 std::io::Read trait 声明，并由 std::fs::File 结构体实现，但为了调用它，std::io::Read 必须在作用域内。\nusestd::fs::File;usestd::io;usestd::io::Read;// ✅ fn main()-\u0026gt; Result\u0026lt;(),io::Error\u0026gt;{letmutfile=File::open(\u0026#34;Cargo.toml\u0026#34;)?;letmutbuffer=String::new();file.read_to_string(\u0026amp;mutbuffer)?;// ✅ Ok(())}标准库中的 prelude 是标准库中的一个模块，即 std::prelude::v1，它在每个其他模块的顶部被自动导入，即 use std::prelude::v1::*。因此，下面的 trait 总是在作用域内，我们永远不需要显式导入它们，因为它们是 prelude 的一部分。\n AsMut AsRef Clone Copy Default Drop Eq Fn FnMut FnOnce From Into ToOwned IntoIterator Iterator PartialEq PartialOrd Send Sized Sync ToString Ord  派生宏 标准库导出了一些派生宏，如果一个类型的所有成员都实现了某个 trait, 我们可以使用这些宏来快速方便地在这个类型上实现该 trait。这些派生宏以它们所实现的 trait 命名。\n Clone Copy Debug Default Eq Hash Ord PartialEq PartialOrd  使用示例:\n// macro derives Copy \u0026amp; Clone impl for SomeType #[derive(Copy, Clone)]struct SomeType;注意：派生宏只是过程宏，可以做任何事情，没有硬性规定一定要实现一个 trait，也没有规定只有在类型的所有成员都实现一个 trait 的情况下才能工作，这些只是标准库中派生宏所遵循的约定。\n默认实现 Trait 可以为其函数和方法提供默认的实现。\ntraitTrait{fn method(\u0026amp;self){println!(\u0026#34;default impl\u0026#34;);}}struct SomeType;struct OtherType;// use default impl for Trait::method implTraitforSomeType{}implTraitforOtherType{// use our own impl for Trait::method fn method(\u0026amp;self){println!(\u0026#34;OtherType impl\u0026#34;);}}fn main(){SomeType.method();// prints \u0026#34;default impl\u0026#34; OtherType.method();// prints \u0026#34;OtherType impl\u0026#34; }如果一些 trait 方法可以只用其他 trait 方法来实现，这就特别方便。\ntraitGreet{fn greet(\u0026amp;self,name: \u0026amp;str)-\u0026gt; String;fn greet_loudly(\u0026amp;self,name: \u0026amp;str)-\u0026gt; String {self.greet(name)+\u0026#34;!\u0026#34;}}struct Hello;struct Hola;implGreetforHello{fn greet(\u0026amp;self,name: \u0026amp;str)-\u0026gt; String {format!(\u0026#34;Hello {}\u0026#34;,name)}// use default impl for greet_loudly }implGreetforHola{fn greet(\u0026amp;self,name: \u0026amp;str)-\u0026gt; String {format!(\u0026#34;Hola {}\u0026#34;,name)}// override default impl fn greet_loudly(\u0026amp;self,name: \u0026amp;str)-\u0026gt; String {letmutgreeting=self.greet(name);greeting.insert_str(0,\u0026#34;¡\u0026#34;);greeting+\u0026#34;!\u0026#34;}}fn main(){println!(\u0026#34;{}\u0026#34;,Hello.greet(\u0026#34;John\u0026#34;));// prints \u0026#34;Hello John\u0026#34; println!(\u0026#34;{}\u0026#34;,Hello.greet_loudly(\u0026#34;John\u0026#34;));// prints \u0026#34;Hello John!\u0026#34; println!(\u0026#34;{}\u0026#34;,Hola.greet(\u0026#34;John\u0026#34;));// prints \u0026#34;Hola John\u0026#34; println!(\u0026#34;{}\u0026#34;,Hola.greet_loudly(\u0026#34;John\u0026#34;));// prints \u0026#34;¡Hola John!\u0026#34; }标准库中的许多 trait 为它们的许多方法提供了默认的实现。\nGeneric Blanket Impls 通用全面实现是在泛型类型而不是具体类型上的实现。为了解释为什么以及如何使用，让我们从为数字类型编写一个 is_even 方法开始。\ntraitEven{fn is_even(self)-\u0026gt; bool;}implEvenfori8{fn is_even(self)-\u0026gt; bool {self%2_i8==0_i8}}implEvenforu8{fn is_even(self)-\u0026gt; bool {self%2_u8==0_u8}}implEvenfori16{fn is_even(self)-\u0026gt; bool {self%2_i16==0_i16}}// etc #[test]// ✅ fn test_is_even(){assert!(2_i8.is_even());assert!(4_u8.is_even());assert!(6_i16.is_even());// etc }显然，这是很啰嗦的。而且，我们所有的实现几乎都是一样的。此外，如果 Rust 决定在未来添加更多的数字类型，我们必须记得回到这段代码，并用新的数字类型更新它。我们可以使用一个通用的全面实现来解决所有这些问题。\nusestd::fmt::Debug;usestd::convert::TryInto;usestd::ops::Rem;traitEven{fn is_even(self)-\u0026gt; bool;}// generic blanket impl impl\u0026lt;T\u0026gt;EvenforTwhereT: Rem\u0026lt;Output=T\u0026gt;+PartialEq\u0026lt;T\u0026gt;+Sized,u8: TryInto\u0026lt;T\u0026gt;,\u0026lt;u8asTryInto\u0026lt;T\u0026gt;\u0026gt;::Error: Debug,{fn is_even(self)-\u0026gt; bool {// these unwraps will never panic self%2.try_into().unwrap()==0.try_into().unwrap()}}#[test]// ✅ fn test_is_even(){assert!(2_i8.is_even());assert!(4_u8.is_even());assert!(6_i16.is_even());// etc }Unlike default impls, which provide an impl, generic blanket impls provide the impl, so they are not overridable. 与默认实现不同，默认的实现提供了一个实现，而通用的全面实现提供了特定的实现，所以它们是不可覆盖的。\nusestd::fmt::Debug;usestd::convert::TryInto;usestd::ops::Rem;traitEven{fn is_even(self)-\u0026gt; bool;}impl\u0026lt;T\u0026gt;EvenforTwhereT: Rem\u0026lt;Output=T\u0026gt;+PartialEq\u0026lt;T\u0026gt;+Sized,u8: TryInto\u0026lt;T\u0026gt;,\u0026lt;u8asTryInto\u0026lt;T\u0026gt;\u0026gt;::Error: Debug,{fn is_even(self)-\u0026gt; bool {self%2.try_into().unwrap()==0.try_into().unwrap()}}implEvenforu8{// ❌ fn is_even(self)-\u0026gt; bool {self%2_u8==0_u8}}这抛出:\nerror[E0119]: conflicting implementations of trait `Even` for type `u8`:\r--\u0026gt; src/lib.rs:22:1\r|\r10 | / impl\u0026lt;T\u0026gt; Even for T\r11 | | where\r12 | | T: Rem\u0026lt;Output = T\u0026gt; + PartialEq\u0026lt;T\u0026gt; + Sized,\r13 | | u8: TryInto\u0026lt;T\u0026gt;,\r... |\r19 | | }\r20 | | }\r| |_- first implementation here\r21 | 22 | impl Even for u8 {\r| ^^^^^^^^^^^^^^^^ conflicting implementation for `u8`\rThese impls overlap, hence they conflict, hence Rust rejects the code to ensure trait coherence. Trait coherence is the property that there exists at most one impl of a trait for any given type. The rules Rust uses to enforce trait coherence, the implications of those rules, and workarounds for the implications are outside the scope of this article.\n这些实现重叠了，因此它们冲突，因此 Rust 拒绝了确保 trait 一致性的代码。Trait 一致性是指任何给定类型的 trait 最多存在一个实现的属性。Rust 用来强制执行 trait 一致性的规则，这些规则的含义，以及含义的变通方法都不在本文的范围内。\nSubtraits \u0026amp; Supertraits \u0026ldquo;subtrait\u0026rdquo; 中的 \u0026ldquo;sub\u0026rdquo; 指的是子集，\u0026ldquo;supertrait\u0026rdquo; 中的 \u0026ldquo;super\u0026rdquo; 指的是超集。如果我们有一个这样的 trait 声明。\ntraitSubtrait: Supertrait{}All of the types which impl Subtrait are a subset of all the types which impl Supertrait, or to put it in opposite but equivalent terms: all the types which impl Supertrait are a superset of all the types which impl Subtrait.\nAlso, the above is just syntax sugar for: 所有实现 Subtrait 的类型都是所有实现 Supertrait 的类型的子集，或者用相反但等价的词语来表达：所有实现 Supertrait 的类型都是所有实现 Subtrait 的类型的超集。\n另外，上面的代码只是下面这段代码的语法糖:\ntraitSubtraitwhereSelf: Supertrait{}这是一个微妙而又重要的区别，要理解的是，约束是在 Self 上的，即实现 Subtrait 的类型，而不是在 Subtrait 本身。后者是没有任何意义的，因为 trait 约束只能应用于具体的类型，这些类型可以实现 trait。Trait 不能实现其他 trait。\ntraitSupertrait{fn method(\u0026amp;self){println!(\u0026#34;in supertrait\u0026#34;);}}traitSubtrait: Supertrait{// this looks like it might impl or // override Supertrait::method but it // does not fn method(\u0026amp;self){println!(\u0026#34;in subtrait\u0026#34;)}}struct SomeType;// adds Supertrait::method to SomeType implSupertraitforSomeType{}// adds Subtrait::method to SomeType implSubtraitforSomeType{}// both methods exist on SomeType simultaneously // neither overriding or shadowing the other fn main(){SomeType.method();// ❌ ambiguous method call // must disambiguate using fully-qualified syntax \u0026lt;SomeTypeasSupertrait\u0026gt;::method(\u0026amp;st);// ✅ prints \u0026#34;in supertrait\u0026#34; \u0026lt;SomeTypeasSubtrait\u0026gt;::method(\u0026amp;st);// ✅ prints \u0026#34;in subtrait\u0026#34; }Furthermore, there are no rules for how a type must impl both a subtrait and a supertrait. It can use the methods from either in the impl of the other. 此外，没有规定一个类型必须同时实现一个 subtrait 和一个 supertrait。它可以在另一个类型的实现中使用其中一个类型的方法。\ntraitSupertrait{fn super_method(\u0026amp;mutself);}traitSubtrait: Supertrait{fn sub_method(\u0026amp;mutself);}struct CallSuperFromSub;implSupertraitforCallSuperFromSub{fn super_method(\u0026amp;mutself){println!(\u0026#34;in super\u0026#34;);}}implSubtraitforCallSuperFromSub{fn sub_method(\u0026amp;mutself){println!(\u0026#34;in sub\u0026#34;);self.super_method();}}struct CallSubFromSuper;implSupertraitforCallSubFromSuper{fn super_method(\u0026amp;mutself){println!(\u0026#34;in super\u0026#34;);self.sub_method();}}implSubtraitforCallSubFromSuper{fn sub_method(\u0026amp;mutself){println!(\u0026#34;in sub\u0026#34;);}}struct CallEachOther(bool);implSupertraitforCallEachOther{fn super_method(\u0026amp;mutself){println!(\u0026#34;in super\u0026#34;);ifself.0{self.0=false;self.sub_method();}}}implSubtraitforCallEachOther{fn sub_method(\u0026amp;mutself){println!(\u0026#34;in sub\u0026#34;);ifself.0{self.0=false;self.super_method();}}}fn main(){CallSuperFromSub.super_method();// prints \u0026#34;in super\u0026#34; CallSuperFromSub.sub_method();// prints \u0026#34;in sub\u0026#34;, \u0026#34;in super\u0026#34; CallSubFromSuper.super_method();// prints \u0026#34;in super\u0026#34;, \u0026#34;in sub\u0026#34; CallSubFromSuper.sub_method();// prints \u0026#34;in sub\u0026#34; CallEachOther(true).super_method();// prints \u0026#34;in super\u0026#34;, \u0026#34;in sub\u0026#34; CallEachOther(true).sub_method();// prints \u0026#34;in sub\u0026#34;, \u0026#34;in super\u0026#34; }Hopefully the examples above show that the relationship between subtraits and supertraits can be complex. Before introducing a mental model that neatly encapsulates all of that complexity let\u0026rsquo;s quickly review and establish the mental model we use for understanding trait bounds on generic types: 希望上面的例子能表明，subtrait 和 supertrait 之间的关系可能很复杂。在介绍一个能整齐地概括所有这些复杂性的心理模型之前，让我们快速回顾并建立我们用于理解泛型上的 trait 约束的心理模型。\nfn function\u0026lt;T: Clone\u0026gt;(t: T){// impl }Without knowing anything about the impl of this function we could reasonably guess that t.clone() gets called at some point because when a generic type is bounded by a trait that strongly implies it has a dependency on the trait. The mental model for understanding the relationship between generic types and their trait bounds is a simple and intuitive one: generic types depend on their trait bounds.\nNow let\u0026rsquo;s look the trait declaration for Copy: 在不了解这个函数的实现的情况下，我们可以合理地猜测 t.clone() 在某些时候会被调用，因为当一个泛型被一个 trait 约束时，强烈地意味着它对 trait 有依赖性。理解泛型与其 trait 约束之间关系的心理模型是一个简单而直观的模型：泛型 “依赖” 其 trait 约束。\n现在让我们看看 Copy 的 trait 声明。\ntraitCopy: Clone {}上面的语法看起来非常类似于在泛型类型上应用 trait 约束的语法，然而 Copy 根本不依赖于 Clone。我们前面开发的心理模型在这里并不能帮助我们。在我看来，理解 subtrait 和 supertrait 之间关系的最简单、最优雅的心理模型是：subtrait “精炼” 其 supertrait。\n“精炼” 这个词故意保持有些模糊，因为它在不同的语境中可以有不同的含义。\n subtrait 可能会使它的 supertrait 的方法更加特化，速度更快，使用更少的内存，例如：Copy: Clone subtrait 可以对 supertrait 的方法的实现做出额外的保证，例如 Eq: PartialEq, Ord: PartialOrd, ExactSizeIterator: Iterator subtrait 可能使 supertrait 的方法更灵活或更容易调用，例如 FnMut: FnOnce, `Fn: FnMut subtrait 可以扩展一个 supertrait，并添加新的方法，例如 DoubleEndedIterator: Iterator, ExactSizeIterator: Iterator  Trait 对象 泛型给了我们编译时的多态性，而 trait 对象给了我们运行时的多态性。我们可以使用 trait 对象来允许函数在运行时动态地返回不同的类型。\nfn example(condition: bool,vec: Vec\u0026lt;i32\u0026gt;)-\u0026gt; Box\u0026lt;dynIterator\u0026lt;Item=i32\u0026gt;\u0026gt;{letiter=vec.into_iter();ifcondition{// Has type: // Box\u0026lt;Map\u0026lt;IntoIter\u0026lt;i32\u0026gt;, Fn(i32) -\u0026gt; i32\u0026gt;\u0026gt; // But is cast to: // Box\u0026lt;dyn Iterator\u0026lt;Item = i32\u0026gt;\u0026gt; Box::new(iter.map(|n|n*2))}else{// Has type: // Box\u0026lt;Filter\u0026lt;IntoIter\u0026lt;i32\u0026gt;, Fn(\u0026amp;i32) -\u0026gt; bool\u0026gt;\u0026gt; // But is cast to: // Box\u0026lt;dyn Iterator\u0026lt;Item = i32\u0026gt;\u0026gt; Box::new(iter.filter(|\u0026amp;n|n\u0026gt;=2))}}Trait 对象还允许我们在集合中存储异构类型。\nusestd::f64::consts::PI;struct Circle{radius: f64,}struct Square{side: f64 }traitShape{fn area(\u0026amp;self)-\u0026gt; f64;}implShapeforCircle{fn area(\u0026amp;self)-\u0026gt; f64 {PI*self.radius*self.radius}}implShapeforSquare{fn area(\u0026amp;self)-\u0026gt; f64 {self.side*self.side}}fn get_total_area(shapes: Vec\u0026lt;Box\u0026lt;dynShape\u0026gt;\u0026gt;)-\u0026gt; f64 {shapes.into_iter().map(|s|s.area()).sum()}fn example(){letshapes: Vec\u0026lt;Box\u0026lt;dynShape\u0026gt;\u0026gt;=vec![Box::new(Circle{radius: 1.0}),// Box\u0026lt;Circle\u0026gt; cast to Box\u0026lt;dyn Shape\u0026gt; Box::new(Square{side: 1.0}),// Box\u0026lt;Square\u0026gt; cast to Box\u0026lt;dyn Shape\u0026gt; ];assert_eq!(PI+1.0,get_total_area(shapes));// ✅ }Trait 对象是不确定大小的，所以它们必须总是在指针后面。我们可以根据类型中是否存在 dyn 关键字来区分具体类型和 trait 对象。\nstruct Struct;traitTrait{}// regular struct \u0026amp;StructBox\u0026lt;Struct\u0026gt;Rc\u0026lt;Struct\u0026gt;Arc\u0026lt;Struct\u0026gt;// trait objects \u0026amp;dynTraitBox\u0026lt;dynTrait\u0026gt;Rc\u0026lt;dynTrait\u0026gt;Arc\u0026lt;dynTrait\u0026gt;并非所有的 trait 都可以转换为 trait 对象。如果一个 trait 满足这些要求，它就是对象安全的。\n trait 不需要 Self: Sized。 所有 trait 的方法都是对象安全的。  如果 trait 方法满足这些要求，它就是对象安全的。\n 方法需要 Self: Sized 或 该方法只在接收器位置使用 Self 类型。  理解为什么要求是这样的，与本文其他部分无关，但如果你仍然好奇，在 Sizedness in Rust 中会有介绍。\nMarker Traits 标记 trait 是没有 trait 项的 trait。它们的工作是将实现类型 “标记” 为具有某些属性，否则不可能用类型系统来表示。\n// Impling PartialEq for a type promises // that equality for the type has these properties: // - symmetry: a == b implies b == a, and // - transitivity: a == b \u0026amp;\u0026amp; b == c implies a == c // But DOES NOT promise this property: // - reflexivity: a == a traitPartialEq{fn eq(\u0026amp;self,other: \u0026amp;Self)-\u0026gt; bool;}// Eq has no trait items! The eq method is already // declared by PartialEq, but \u0026#34;impling\u0026#34; Eq // for a type promises this additional equality property: // - reflexivity: a == a traitEq: PartialEq {}// f64 impls PartialEq but not Eq because NaN != NaN // i32 impls PartialEq \u0026amp; Eq because there\u0026#39;s no NaNs :) Auto Traits 自动 trait 是指如果一个类型的所有成员都实现了这个 trait，那么这个 trait 就会被自动实现。“成员” 的含义取决于类型，例如：结构体的字段、枚举的变体、数组的元素、元组的项等等。\n所有的自动 trait 都是标记 trait，但不是所有的标记 trait 都是自动 trait。自动 trait 必须是标记 trait，这样编译器就可以为它们提供一个自动的缺省实现，如果它们有任何 trait 项，那就不可能了。\n自动 trait 的例子。\n// implemented for types which are safe to send between threads unsafeautotraitSend{}// implemented for types whose references are safe to send between threads unsafeautotraitSync{}Unsafe Traits Trait 可以被标记为不安全，以表明实现该 trait 可能需要不安全的代码。Send 和 Sync 都被标记为 unsafe，因为如果它们没有被自动实现，就意味着它一定包含一些非 Send 或非 Sync 成员，如果我们想手动标记类型为 Send 和 Sync，我们作为实现者必须格外小心，以确保没有数据竞争。\n// SomeType is not Send or Sync struct SomeType{not_send_or_sync: *const(),}// but if we\u0026#39;re confident that our impl doesn\u0026#39;t have any data // races we can explicitly mark it as Send and Sync using unsafe unsafeimplSendforSomeType{}unsafeimplSyncforSomeType{}Auto Traits Send \u0026amp; Sync 预备知识\n Marker Traits Auto Traits Unsafe Traits  unsafeautotraitSend{}unsafeautotraitSync{}如果一个类型是 Send，意味着在线程之间发送是安全的。如果一个类型是 Sync，这意味着在线程之间共享它的引用是安全的。更准确地说，如果且仅当 \u0026amp;T 是 Send 时，一些类型 T 是 Sync。\n几乎所有类型都是 Send 和 Sync 的。唯一值得注意的 Send 异常是 Rc，唯一值得注意的 Sync 异常是 Rc、Cell 和 RefCell。如果我们需要一个 Rc 的 Send 版本，我们可以使用 Arc。如果我们需要 Cell 或 RefCell 的 Sync 版本，我们可以 Mutex 或 RwLock。虽然如果我们使用 Mutex 或 RwLock 只是包裹一个原语类型，通常最好使用标准库提供的原子原语类型，如 AtomicBool、AtomicI32、AtomicUsize 等。\n几乎所有的类型都是 Sync，这可能会让一些人感到惊讶，但是是的，即使对于没有任何内部同步的类型也是如此。这要归功于 Rust 严格的借用规则。\n我们可以将同一数据的许多不可变的引用传递给许多线程，而且我们保证不会出现数据竞争，因为只要有任何不可变的引用存在， Rust 就会静态地保证底层数据不能被修改。\nusecrossbeam::thread;fn main(){letmutgreeting=String::from(\u0026#34;Hello\u0026#34;);letgreeting_ref=\u0026amp;greeting;thread::scope(|scoped_thread|{// spawn 3 threads fornin1..=3{// greeting_ref copied into every thread scoped_thread.spawn(move|_|{println!(\u0026#34;{} {}\u0026#34;,greeting_ref,n);// prints \u0026#34;Hello {n}\u0026#34; });}// line below could cause UB or data races but compiler rejects it greeting+=\u0026#34; world\u0026#34;;// ❌ cannot mutate greeting while immutable refs exist });// can mutate greeting after every thread has joined greeting+=\u0026#34; world\u0026#34;;// ✅ println!(\u0026#34;{}\u0026#34;,greeting);// prints \u0026#34;Hello world\u0026#34; }同样，我们可以将单个可变引用传递给一些数据到一个线程，我们可以保证不会出现数据竞争，因为 Rust 静态地保证了别名的可变引用不能存在，底层数据不能通过现有的单个可变引用以外的任何东西进行修改。\nusecrossbeam::thread;fn main(){letmutgreeting=String::from(\u0026#34;Hello\u0026#34;);letgreeting_ref=\u0026amp;mutgreeting;thread::scope(|scoped_thread|{// greeting_ref moved into thread scoped_thread.spawn(move|_|{*greeting_ref+=\u0026#34; world\u0026#34;;println!(\u0026#34;{}\u0026#34;,greeting_ref);// prints \u0026#34;Hello world\u0026#34; });// line below could cause UB or data races but compiler rejects it greeting+=\u0026#34;!!!\u0026#34;;// ❌ cannot mutate greeting while mutable refs exist });// can mutate greeting after the thread has joined greeting+=\u0026#34;!!!\u0026#34;;// ✅ println!(\u0026#34;{}\u0026#34;,greeting);// prints \u0026#34;Hello world!!!\u0026#34; }这就是为什么大多数类型都是 Sync 而不需要任何显式同步。如果我们需要在多个线程中同时修改一些数据 T，编译器不会让我们这样做，直到我们将数据包裹在 Arc\u0026lt;Mutex\u0026lt;T\u0026gt;\u0026gt; 或 Arc\u0026lt;RwLock\u0026lt;T\u0026gt;\u0026gt; 中，所以编译器强制要求在需要时使用显式同步。\nSized 预备知识\n Marker Traits Auto Traits  如果一个类型是 Sized 的，这意味着它的字节大小在编译时是已知的，并且可以将该类型的实例放在栈上。\n类型的大小和它的含义是一个微妙而又巨大的话题，它影响到语言的很多不同方面。它是如此重要，以至于我写了整整一篇文章，叫做 Sizedness in Rust，我强烈推荐任何想深入了解类型大小的人阅读。我总结一下与本文相关的几个关键内容。\n 所有的泛型类型都会得到一个隐式的 Sized 约束。  fn func\u0026lt;T\u0026gt;(t: \u0026amp;T){}// example above desugared fn func\u0026lt;T: Sized\u0026gt;(t: \u0026amp;T){}由于所有泛型类型都有一个隐式的 Sized 约束，如果我们想退出这个隐式约束，我们需要使用特殊的 \u0026ldquo;放宽约束\u0026rdquo; 语法 ?Sized，它目前只存在于 Sized trait。  // now T can be unsized fn func\u0026lt;T: ?Sized\u0026gt;(t: \u0026amp;T){}所有的 trait 都有一个隐式的 ?Sized 约束。  traitTrait{}// example above desugared traitTrait: ?Sized{}这是为了让 trait 对象可以实现 trait。同样，所有的琐碎细节都在 Sizedness in Rust 中。\nGeneral traits Default 预备知识\n Self Functions Derive Macros  traitDefault{fn default()-\u0026gt; Self;}可以构建 Default 类型的默认值。\nstruct Color{r: u8,g: u8,b: u8,}implDefaultforColor{// default color is black fn default()-\u0026gt; Self{Color{r: 0,g: 0,b: 0,}}}这对快速建立原型很有用，但在任何情况下，我们只需要一个类型的实例，而且对它是什么并不挑剔。\nfn main(){// just give me some color! letcolor=Color::default();}这也是一个我们可能想明确地暴露给我们的函数用户的选项。\nstruct Canvas;enum Shape{Circle,Rectangle,}implCanvas{// let user optionally pass a color fn paint(\u0026amp;mutself,shape: Shape,color: Option\u0026lt;Color\u0026gt;){// if no color is passed use the default color letcolor=color.unwrap_or_default();// etc }}Default 在我们需要构造泛型类型的泛型语境中也很有用。\nfn guarantee_length\u0026lt;T: Default\u0026gt;(mutvec: Vec\u0026lt;T\u0026gt;,min_len: usize)-\u0026gt; Vec\u0026lt;T\u0026gt;{for_in0..min_len.saturating_sub(vec.len()){vec.push(T::default());}vec}我们可以利用 Default 类型的另一种方式是使用 Rust 的结构体更新语法对结构体进行部分初始化。我们可以为 Color 设置一个 new 构造函数，将每个成员作为一个参数。\nimplColor{fn new(r: u8,g: u8,b: u8)-\u0026gt; Self{Color{r,g,b,}}}然而，我们也可以使用方便的构造函数，每个构造函数只接受一个特定的结构体成员，其他结构体成员则使用默认值。\nimplColor{fn red(r: u8)-\u0026gt; Self{Color{r,..Color::default()}}fn green(g: u8)-\u0026gt; Self{Color{g,..Color::default()}}fn blue(b: u8)-\u0026gt; Self{Color{b,..Color::default()}}}还有一个 Default 的派生宏，所以我们可以像这样编写 Color。\n// default color is still black // because u8::default() == 0 #[derive(Default)]struct Color{r: u8,g: u8,b: u8 }Clone 预备知识\n Self Methods Default Impls Derive Macros  traitClone{fn clone(\u0026amp;self)-\u0026gt; Self;// provided default impls fn clone_from(\u0026amp;mutself,source: \u0026amp;Self);}我们可以将 Clone 类型的不可变引用转换为自有值(owned values)，即 \u0026amp;T -\u0026gt; T。Clone 没有对这种转换的效率做出承诺，所以它可能是缓慢和昂贵的。为了快速地在一个类型上实现 Clone，我们可以使用派生宏。\n#[derive(Clone)]struct SomeType{cloneable_member1: CloneableType1,cloneable_member2: CloneableType2,// etc }// macro generates impl below implCloneforSomeType{fn clone(\u0026amp;self)-\u0026gt; Self{SomeType{cloneable_member1: self.cloneable_member1.clone(),cloneable_member2: self.cloneable_member2.clone(),// etc }}}Clone 也可以在泛型上下文中构建一个类型的实例。下面是上一节中的一个例子，除了使用 Clone 而不是 Default。\nfn guarantee_length\u0026lt;T: Clone\u0026gt;(mutvec: Vec\u0026lt;T\u0026gt;,min_len: usize,fill_with: \u0026amp;T)-\u0026gt; Vec\u0026lt;T\u0026gt;{for_in0..min_len.saturating_sub(vec.len()){vec.push(fill_with.clone());}vec}人们也经常使用克隆作为逃避的方法，以避免与借用检查器打交道。管理带有引用的结构体可能很有挑战性，但我们可以通过克隆将引用变成自有值(owned values)。\n// oof, we gotta worry about lifetimes 😟 struct SomeStruct\u0026lt;\u0026#39;a\u0026gt;{data: \u0026amp;\u0026#39;aVec\u0026lt;u8\u0026gt;,}// now we\u0026#39;re on easy street 😎 struct SomeStruct{data: Vec\u0026lt;u8\u0026gt;,}如果我们正在开发的程序的性能不是最重要的，那么我们就不需要为克隆数据而烦恼。Rust 是一种低级别的语言，暴露了很多低级别的细节，所以很容易被过早的优化所吸引，而不是真正解决手头的问题。对于许多程序来说，最好的优先顺序通常是首先建立正确性，其次是优雅性，第三是性能，只有在对程序进行剖析并确定了性能瓶颈之后才关注性能。这是很好的一般性建议，如果它不适用于你的特定程序，你就会知道。\nCopy 预备知识\n Marker Traits Subtraits \u0026amp; Supertraits Derive Macros  traitCopy: Clone {}我们复制 Copy 类型，例如：T -\u0026gt; T。Copy 承诺复制操作将是一个简单的按位(bitwise)拷贝，所以它将是非常快速和高效的。我们不能自己实现 Copy，只有编译器可以提供一个实现，但是我们可以通过使用 Copy 派生宏，以及 Clone 派生宏来告诉编译器这样做，因为 Copy 是 Clone 的一个子 trait。\n#[derive(Copy, Clone)]struct SomeType;Copy 完善了(refine) Clone。Clone 可能是缓慢和昂贵的，但 Copy 保证是快速和便宜的，所以 Copy 只是一个快速 Clone。如果一个类型实现了 Copy，这就使得 Clone 的实现变得微不足道了。\n// this is what the derive macro generates impl\u0026lt;T: Copy\u0026gt;CloneforT{// the clone method becomes just a copy fn clone(\u0026amp;self)-\u0026gt; Self{*self}}当一个类型被移动时，实现该类型的 Copy 会改变其行为。默认情况下，所有类型都有“移动语义”，但是一旦一个类型实现了 `Copy'，它就会得到“复制语义”。为了解释这两者之间的区别，我们来看看这些简单的场景。\n// a \u0026#34;move\u0026#34;, src: !Copy letdest=src;// a \u0026#34;copy\u0026#34;, src: Copy letdest=src;在这两种情况下，dest = src 对 src 的内容进行简单的按位复制，并将结果移动到 dest 中，唯一的区别是，在“移动”的情况下，借用检查器使 src 变量无效，并确保它以后不会被用于其他地方，而在“复制”的情况下，src 仍然有效并可使用。\n一言以蔽之。拷贝就“是”移动。移动就“是”拷贝。唯一的区别是借用检查器对它们的处理方式。\n关于移动的一个更具体的例子，假设 src 是一个 Vec\u0026lt;i32\u0026gt;，其内容是这样的。\n{data: *mut[i32],length: usize,capacity: usize }当我们写下 dest = src 时，我们的结果是：\nsrc={data: *mut[i32],length: usize,capacity: usize }dest={data: *mut[i32],length: usize,capacity: usize }这个时候，src 和 dest 都有对相同数据的别名可变引用，这是一个大忌，所以借用检查器使 src 变量无效，这样它就不能再被使用而不会产生编译错误。\n对于一个更具体的拷贝例子，假设 src 是一个 Option\u0026lt;i32\u0026gt;，它的内容是这样的：\n{is_valid: bool,data: i32 }现在，当我们写下 dest = src 时，我们的结果是：\nsrc={is_valid: bool,data: i32 }dest={is_valid: bool,data: i32 }这些都是可以同时使用的! 因此 Option\u0026lt;i32\u0026gt; 是可以 Copy 的。\n虽然 Copy 可以是一个自动 trait，但 Rust 语言的设计者决定让类型显式地选择复制语义，而不是在类型符合条件时默默地继承复制语义，因为后者会导致令人惊讶的混乱行为，经常导致错误。\nAny 预备知识\n Self Generic Blanket Impls Subtraits \u0026amp; Supertraits Trait Objects  traitAny: \u0026#39;static{fn type_id(\u0026amp;self)-\u0026gt; TypeId;}Rust 的多态性风格是参数化的，但如果我们想使用类似于动态类型语言的多态性风格，那么我们可以使用 Any trait 来模仿。我们不需要为我们的类型手动实现这个 trait，因为下面这个泛型全面实现已经覆盖了。\nimpl\u0026lt;T: \u0026#39;static+?Sized\u0026gt;AnyforT{fn type_id(\u0026amp;self)-\u0026gt; TypeId{TypeId::of::\u0026lt;T\u0026gt;()}}我们从 dyn Any 中得到 T 的方法是通过使用 downcast_ref::\u0026lt;T\u0026gt;() 和 downcast_mut::\u0026lt;T\u0026gt;() 方法。\nusestd::any::Any;#[derive(Default)]struct Point{x: i32,y: i32,}implPoint{fn inc(\u0026amp;mutself){self.x+=1;self.y+=1;}}fn map_any(mutany: Box\u0026lt;dynAny\u0026gt;)-\u0026gt; Box\u0026lt;dynAny\u0026gt;{ifletSome(num)=any.downcast_mut::\u0026lt;i32\u0026gt;(){*num+=1;}elseifletSome(string)=any.downcast_mut::\u0026lt;String\u0026gt;(){*string+=\u0026#34;!\u0026#34;;}elseifletSome(point)=any.downcast_mut::\u0026lt;Point\u0026gt;(){point.inc();}any}fn main(){letmutvec: Vec\u0026lt;Box\u0026lt;dynAny\u0026gt;\u0026gt;=vec![Box::new(0),Box::new(String::from(\u0026#34;a\u0026#34;)),Box::new(Point::default()),];// vec = [0, \u0026#34;a\u0026#34;, Point { x: 0, y: 0 }] vec=vec.into_iter().map(map_any).collect();// vec = [1, \u0026#34;a!\u0026#34;, Point { x: 1, y: 1 }] }这个 trait 很少需要使用，因为在大多数情况下，参数化多态性要优于临时多态性，后者也可以用枚举来模拟，因为枚举的类型更安全，需要的迂回更少。例如，我们可以把上面的例子写成这样。\n#[derive(Default)]struct Point{x: i32,y: i32,}implPoint{fn inc(\u0026amp;mutself){self.x+=1;self.y+=1;}}enum Stuff{Integer(i32),String(String),Point(Point),}fn map_stuff(mutstuff: Stuff)-\u0026gt; Stuff{match\u0026amp;mutstuff{Stuff::Integer(num)=\u0026gt;*num+=1,Stuff::String(string)=\u0026gt;*string+=\u0026#34;!\u0026#34;,Stuff::Point(point)=\u0026gt;point.inc(),}stuff}fn main(){letmutvec=vec![Stuff::Integer(0),Stuff::String(String::from(\u0026#34;a\u0026#34;)),Stuff::Point(Point::default()),];// vec = [0, \u0026#34;a\u0026#34;, Point { x: 0, y: 0 }] vec=vec.into_iter().map(map_stuff).collect();// vec = [1, \u0026#34;a!\u0026#34;, Point { x: 1, y: 1 }] }尽管 Any 很少被需要，但有时使用起来还是很方便的，我们将在后面的“错误处理”部分看到。\nFormatting Traits 我们可以使用 std::fmt 中的格式化宏将类型序列化为字符串，其中最著名的是 println!。我们可以将格式化参数传递给格式 str 中使用的 {} 占位符，然后用来选择使用哪个 trait 实现来序列化占位符的参数。\n   Trait Placeholder Description     Display {} display representation   Debug {:?} debug representation   Octal {:o} octal representation   LowerHex {:x} lowercase hex representation   UpperHex {:X} uppercase hex representation   Pointer {:p} memory address   Binary {:b} binary representation   LowerExp {:e} lowercase exponential representation   UpperExp {:E} uppercase exponential representation    Display \u0026amp; ToString 预备知识\n Self Methods Generic Blanket Impls  traitDisplay{fn fmt(\u0026amp;self,f: \u0026amp;mutFormatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; Result;}Display 类型可以被序列化为 String，这对程序的终端用户很友好。例如，给 Point 实现 Display:\nusestd::fmt;#[derive(Default)]struct Point{x: i32,y: i32,}implfmt::DisplayforPoint{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{write!(f,\u0026#34;({}, {})\u0026#34;,self.x,self.y)}}fn main(){println!(\u0026#34;origin: {}\u0026#34;,Point::default());// prints \u0026#34;origin: (0, 0)\u0026#34; // get Point\u0026#39;s Display representation as a String letstringified_point=format!(\u0026#34;{}\u0026#34;,Point::default());assert_eq!(\u0026#34;(0, 0)\u0026#34;,stringified_point);// ✅ }除了使用 format! 宏来获得一个类型的显示表示为 String 之外，我们还可以使用 ToString trait。\ntraitToString{fn to_string(\u0026amp;self)-\u0026gt; String;}我们没有必要自己去实现这个 trait。事实上，我们不能这样做，因为下面这个泛型全面实现，对于任何实现 Display 的类型，都自动实现 ToString。\nimpl\u0026lt;T: Display+?Sized\u0026gt;ToStringforT;将 ToString 与 Point 一起使用。\n#[test]// ✅ fn display_point(){letorigin=Point::default();assert_eq!(format!(\u0026#34;{}\u0026#34;,origin),\u0026#34;(0, 0)\u0026#34;);}#[test]// ✅ fn point_to_string(){letorigin=Point::default();assert_eq!(origin.to_string(),\u0026#34;(0, 0)\u0026#34;);}#[test]// ✅ fn display_equals_to_string(){letorigin=Point::default();assert_eq!(format!(\u0026#34;{}\u0026#34;,origin),origin.to_string());}Debug 预备知识\n Self Methods Derive Macros Display \u0026amp; ToString  traitDebug{fn fmt(\u0026amp;self,f: \u0026amp;mutFormatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; Result;}Debug 与 Display 有相同的签名。唯一的区别是，当我们使用 {:?} 格式符时，Debug 实现被调用。`Debug' 可以被派生。\nusestd::fmt;#[derive(Debug)]struct Point{x: i32,y: i32,}// derive macro generates impl below implfmt::DebugforPoint{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{f.debug_struct(\u0026#34;Point\u0026#34;).field(\u0026#34;x\u0026#34;,\u0026amp;self.x).field(\u0026#34;y\u0026#34;,\u0026amp;self.y).finish()}}为一个类型实现 Debug 也允许它在 dbg! 宏中使用，这比 println! 更有利于临时应急的打印日志。它的一些优点如下:\n dbg! 打印到 stderr 而不是 stdout，所以调试日志很容易与我们程序的实际 stdout 输出分开。 dbg! 打印传递给它的表达式，以及表达式所评估的值。 dbg! 拥有其参数的所有权，并返回这些参数，所以你可以在表达式中使用它。  fn some_condition()-\u0026gt; bool {true}// no logging fn example(){ifsome_condition(){// some code }}// println! logging fn example_println(){// 🤦 letresult=some_condition();println!(\u0026#34;{}\u0026#34;,result);// just prints \u0026#34;true\u0026#34; ifresult{// some code }}// dbg! logging fn example_dbg(){// 😍 ifdbg!(some_condition()){// prints \u0026#34;[src/main.rs:22] some_condition() = true\u0026#34; // some code }}唯一的缺点是，dbg! 在发布版本中不会被自动剥离，所以如果我们不想在最终的可执行文件中使用它，就必须从我们的代码中手动删除它。\nOperator Traits Rust 中的所有运算符都与 trait 相关。如果我们想为我们的类型实现运算符，就必须实现相关的 trait。\n   Trait(s) Category Operator(s) Description     Eq, PartialEq comparison == equality   Ord, PartialOrd comparison \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;= comparison   Add arithmetic + addition   AddAssign arithmetic += addition assignment   BitAnd arithmetic \u0026amp; bitwise AND   BitAndAssign arithmetic \u0026amp;= bitwise assignment   BitXor arithmetic ^ bitwise XOR   BitXorAssign arithmetic ^= bitwise XOR assignment   Div arithmetic / division   DivAssign arithmetic /= division assignment   Mul arithmetic * multiplication   MulAssign arithmetic *= multiplication assignment   Neg arithmetic - unary negation   Not arithmetic ! unary logical negation   Rem arithmetic % remainder   RemAssign arithmetic %= remainder assignment   Shl arithmetic \u0026lt;\u0026lt; left shift   ShlAssign arithmetic \u0026lt;\u0026lt;= left shift assignment   Shr arithmetic \u0026gt;\u0026gt; right shift   ShrAssign arithmetic \u0026gt;\u0026gt;= right shift assignment   Sub arithmetic - subtraction   SubAssign arithmetic -= subtraction assignment   Fn closure (...args) immutable closure invocation   FnMut closure (...args) mutable closure invocation   FnOnce closure (...args) one-time closure invocation   Deref other * immutable dereference   DerefMut other * mutable derenence   Drop other - type destructor   Index other [] immutable index   IndexMut other [] mutable index   RangeBounds other .. range    Comparison Traits    Trait(s) Category Operator(s) Description     Eq, PartialEq comparison == equality   Ord, PartialOrd comparison \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;= comparison    PartialEq \u0026amp; Eq 预备知识\n Self Methods Generic Parameters Default Impls Generic Blanket Impls Marker Traits Subtraits \u0026amp; Supertraits Sized  traitPartialEq\u0026lt;Rhs=Self\u0026gt;whereRhs: ?Sized,{fn eq(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;// provided default impls fn ne(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;}PartialEq\u0026lt;Rhs\u0026gt; 类型可以使用 == 运算符检查是否与 Rhs 类型相等。\n所有的 PartialEq\u0026lt;Rhs\u0026gt; 实现必须确保相等是对称的和传递的。这意味着对于所有的 a, b, 和 c:\n a == b 意味着 b == a (对称性) a == b \u0026amp;\u0026amp; b == c 意味着 a == c (传递性)  默认情况下 Rhs = Self，因为我们几乎总是想把一个类型的实例相互比较，而不是与不同类型的实例比较。这也自动保证了我们的实现是对称的和传递的。\nstruct Point{x: i32,y: i32 }// Rhs == Self == Point implPartialEqforPoint{// impl automatically symmetric \u0026amp; transitive fn eq(\u0026amp;self,other: \u0026amp;Point)-\u0026gt; bool {self.x==other.x\u0026amp;\u0026amp;self.y==other.y}}如果一个类型的所有成员都实现了 `PartialEq'，那么它可以被派生。\n#[derive(PartialEq)]struct Point{x: i32,y: i32 }#[derive(PartialEq)]enum Suit{Spade,Heart,Club,Diamond,}一旦为我们的类型实现了 PartialEq，我们也可以免费得到我们类型的引用之间的相等性比较，这要感谢这些泛型全面实现。\n// this impl only gives us: Point == Point #[derive(PartialEq)]struct Point{x: i32,y: i32 }// all of the generic blanket impls below // are provided by the standard library // this impl gives us: \u0026amp;Point == \u0026amp;Point impl\u0026lt;A,B\u0026gt;PartialEq\u0026lt;\u0026amp;\u0026#39;_B\u0026gt;for\u0026amp;\u0026#39;_AwhereA: PartialEq\u0026lt;B\u0026gt;+?Sized,B: ?Sized;// this impl gives us: \u0026amp;mut Point == \u0026amp;Point impl\u0026lt;A,B\u0026gt;PartialEq\u0026lt;\u0026amp;\u0026#39;_B\u0026gt;for\u0026amp;\u0026#39;_mutAwhereA: PartialEq\u0026lt;B\u0026gt;+?Sized,B: ?Sized;// this impl gives us: \u0026amp;Point == \u0026amp;mut Point impl\u0026lt;A,B\u0026gt;PartialEq\u0026lt;\u0026amp;\u0026#39;_mutB\u0026gt;for\u0026amp;\u0026#39;_AwhereA: PartialEq\u0026lt;B\u0026gt;+?Sized,B: ?Sized;// this impl gives us: \u0026amp;mut Point == \u0026amp;mut Point impl\u0026lt;A,B\u0026gt;PartialEq\u0026lt;\u0026amp;\u0026#39;_mutB\u0026gt;for\u0026amp;\u0026#39;_mutAwhereA: PartialEq\u0026lt;B\u0026gt;+?Sized,B: ?Sized;由于这个 trait 是泛型化的，我们可以定义不同类型之间的相等性。标准库利用这一点，允许检查许多类似字符串的类型，如String、\u0026amp;str、PathBuf、\u0026amp;Path、OsString、\u0026amp;OsStr 等之间的相等性。\n一般来说，我们只应该在不同类型之间实现相等性关系，如果它们实现同一种数据，并且类型之间的唯一区别是它们如何表示数据或如何允许与数据进行交互。\n这里有一个可爱但糟糕的例子，说明有人可能会被诱惑实现 PartialEq 来检查不符合上述标准的不同类型之间的相等。\n#[derive(PartialEq)]enum Suit{Spade,Club,Heart,Diamond,}#[derive(PartialEq)]enum Rank{Ace,Two,Three,Four,Five,Six,Seven,Eight,Nine,Ten,Jack,Queen,King,}#[derive(PartialEq)]struct Card{suit: Suit,rank: Rank,}// check equality of Card\u0026#39;s suit implPartialEq\u0026lt;Suit\u0026gt;forCard{fn eq(\u0026amp;self,other: \u0026amp;Suit)-\u0026gt; bool {self.suit==*other}}// check equality of Card\u0026#39;s rank implPartialEq\u0026lt;Rank\u0026gt;forCard{fn eq(\u0026amp;self,other: \u0026amp;Rank)-\u0026gt; bool {self.rank==*other}}fn main(){letAceOfSpades=Card{suit: Suit::Spade,rank: Rank::Ace,};assert!(AceOfSpades==Suit::Spade);// ✅ assert!(AceOfSpades==Rank::Ace);// ✅ }这很有效，而且有点道理。一张黑桃A的牌既是A又是黑桃，如果我们要写一个处理扑克牌的库，那么我们想让它简单方便地单独检查一张牌的花色和等级是合理的。然而，我们还缺少一些东西：对称性。 我们可以 Card == Suit 和 Card == Rank，但我们不能 Suit == Card 或 Rank == Card，所以让我们解决这个问题。\n// check equality of Card\u0026#39;s suit implPartialEq\u0026lt;Suit\u0026gt;forCard{fn eq(\u0026amp;self,other: \u0026amp;Suit)-\u0026gt; bool {self.suit==*other}}// added for symmetry implPartialEq\u0026lt;Card\u0026gt;forSuit{fn eq(\u0026amp;self,other: \u0026amp;Card)-\u0026gt; bool {*self==other.suit}}// check equality of Card\u0026#39;s rank implPartialEq\u0026lt;Rank\u0026gt;forCard{fn eq(\u0026amp;self,other: \u0026amp;Rank)-\u0026gt; bool {self.rank==*other}}// added for symmetry implPartialEq\u0026lt;Card\u0026gt;forRank{fn eq(\u0026amp;self,other: \u0026amp;Card)-\u0026gt; bool {*self==other.rank}}我们有对称性! 太好了。增加对称性只是打破了传递性！这是不可能的。哎呀。现在可以这样了：\nfn main(){// Ace of Spades leta=Card{suit: Suit::Spade,rank: Rank::Ace,};letb=Suit::Spade;// King of Spades letc=Card{suit: Suit::Spade,rank: Rank::King,};assert!(a==b\u0026amp;\u0026amp;b==c);// ✅ assert!(a==c);// ❌ }实现 PartialEq 以检查不同类型之间的相等关系的一个好例子是一个处理距离的程序，它使用不同的类型来代表不同的测量单位。\n#[derive(PartialEq)]struct Foot(u32);#[derive(PartialEq)]struct Yard(u32);#[derive(PartialEq)]struct Mile(u32);implPartialEq\u0026lt;Mile\u0026gt;forFoot{fn eq(\u0026amp;self,other: \u0026amp;Mile)-\u0026gt; bool {self.0==other.0*5280}}implPartialEq\u0026lt;Foot\u0026gt;forMile{fn eq(\u0026amp;self,other: \u0026amp;Foot)-\u0026gt; bool {self.0*5280==other.0}}implPartialEq\u0026lt;Mile\u0026gt;forYard{fn eq(\u0026amp;self,other: \u0026amp;Mile)-\u0026gt; bool {self.0==other.0*1760}}implPartialEq\u0026lt;Yard\u0026gt;forMile{fn eq(\u0026amp;self,other: \u0026amp;Yard)-\u0026gt; bool {self.0*1760==other.0}}implPartialEq\u0026lt;Foot\u0026gt;forYard{fn eq(\u0026amp;self,other: \u0026amp;Foot)-\u0026gt; bool {self.0*3==other.0}}implPartialEq\u0026lt;Yard\u0026gt;forFoot{fn eq(\u0026amp;self,other: \u0026amp;Yard)-\u0026gt; bool {self.0==other.0*3}}fn main(){leta=Foot(5280);letb=Yard(1760);letc=Mile(1);// symmetry assert!(a==b\u0026amp;\u0026amp;b==a);// ✅ assert!(b==c\u0026amp;\u0026amp;c==b);// ✅ assert!(a==c\u0026amp;\u0026amp;c==a);// ✅ // transitivity assert!(a==b\u0026amp;\u0026amp;b==c\u0026amp;\u0026amp;a==c);// ✅ assert!(c==b\u0026amp;\u0026amp;b==a\u0026amp;\u0026amp;c==a);// ✅ }Eq 是一个标记 trait，是 PartialEq\u0026lt;Self\u0026gt; 的子 trait。\ntraitEq: PartialEq\u0026lt;Self\u0026gt;{}如果我们为一个类型实现 Eq，在 PartialEq 所要求的对称性和传递性的基础上，我们还保证了自反性，即 对所有 a, a == a。在这个意义上，Eq 完善了 PartialEq，因为它代表了一个更严格的相等性版本。如果一个类型的所有成员都是Eq 的，那么 Eq 实现就可以为该类型派生。\n浮点类型是 PartialEq 的，但不是 Eq 的，因为 NaN != NaN。几乎所有其他的 PartialEq 类型都是 Eq，当然，除非它们包含浮点。\n一旦一个类型实现了 PartialEq 和 Debug，我们就可以在 assert_eq! 宏中使用它。我们也可以比较 PartialEq 类型的集合。\n#[derive(PartialEq, Debug)]struct Point{x: i32,y: i32,}fn example_assert(p1: Point,p2: Point){assert_eq!(p1,p2);}fn example_compare_collections\u0026lt;T: PartialEq\u0026gt;(vec1: Vec\u0026lt;T\u0026gt;,vec2: Vec\u0026lt;T\u0026gt;){// if T: PartialEq this now works! ifvec1==vec2{// some code }else{// other code }}Hash 预备知识\n Self Methods Generic Parameters Default Impls Derive Macros PartialEq \u0026amp; Eq  traitHash{fn hash\u0026lt;H: Hasher\u0026gt;(\u0026amp;self,state: \u0026amp;mutH);// provided default impls fn hash_slice\u0026lt;H: Hasher\u0026gt;(data: \u0026amp;[Self],state: \u0026amp;mutH);}这个 trait 与任何运算符无关，但谈论它的最好时机是在 PartialEq \u0026amp; Eq 之后，所以它在这里。Hash 类型可以使用 Hasher 进行散列。\nusestd:#️⃣:Hasher;usestd:#️⃣:Hash;struct Point{x: i32,y: i32,}implHashforPoint{fn hash\u0026lt;H: Hasher\u0026gt;(\u0026amp;self,hasher: \u0026amp;mutH){hasher.write_i32(self.x);hasher.write_i32(self.y);}}有一个派生宏，它生成的实现与上述相同。\n#[derive(Hash)]struct Point{x: i32,y: i32,}如果一个类型同时实现了 Hash 和 Eq，这些实现必须相互一致，即对于所有的 a 和 b，如果 a == b，那么 a.hash() == b.hash()。所以我们应该总是使用派生宏来实现两者，或者手动实现两者，但不能混合使用，否则就有可能破坏上述不变性。\n为一个类型实现 Eq 和 Hash 的主要好处是，它允许我们将该类型作为键存储在 HashMap 和 HashSet 中。\nusestd::collections::HashSet;// now our type can be stored // in HashSets and HashMaps! #[derive(PartialEq, Eq, Hash)]struct Point{x: i32,y: i32,}fn example_hashset(){letmutpoints=HashSet::new();points.insert(Point{x: 0,y: 0});// ✅ }PartialOrd \u0026amp; Ord 预备知识\n Self Methods Generic Parameters Default Impls Subtraits \u0026amp; Supertraits Derive Macros Sized PartialEq \u0026amp; Eq  enum Ordering{Less,Equal,Greater,}traitPartialOrd\u0026lt;Rhs=Self\u0026gt;: PartialEq\u0026lt;Rhs\u0026gt;whereRhs: ?Sized,{fn partial_cmp(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; Option\u0026lt;Ordering\u0026gt;;// provided default impls fn lt(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;fn le(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;fn gt(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;fn ge(\u0026amp;self,other: \u0026amp;Rhs)-\u0026gt; bool;}PartialOrd\u0026lt;Rhs\u0026gt; 类型可以使用 \u0026lt;, \u0026lt;=, \u0026gt;, 和 \u0026gt;= 运算符与 Rhs 类型进行比较。\n所有的 PartialOrd 实现必须确保比较是不对称的和传递的。这意味着对于所有的 a, b, 和 c:\n a \u0026lt; b 意味着 !(a \u0026gt; b) (不对称性) a \u0026lt; b \u0026amp;\u0026amp; b \u0026lt; c 意味着 a \u0026lt; c (传递性)  PartialOrd 是 PartialEq 的一个子 trait，它们的实现必须总是相互一致。\nfn must_always_agree\u0026lt;T: PartialOrd +PartialEq\u0026gt;(t1: T,t2: T){assert_eq!(t1.partial_cmp(\u0026amp;t2)==Some(Ordering::Equal),t1==t2);}PartialOrd 是对 PartialEq 的细化，当比较 PartialEq 类型时，我们可以检查它们是否相等，但当比较 PartialOrd 类型时，我们可以检查它们是否相等，如果它们不相等，我们可以检查它们是否不相等，因为第一项小于或大于第二项。\n默认情况下 Rhs = Self，因为我们几乎总是想把一个类型的实例相互比较，而不是和不同类型的实例比较。这也自动保证了我们的实现是对称的和传递的。\nusestd::cmp::Ordering;#[derive(PartialEq, PartialOrd)]struct Point{x: i32,y: i32 }// Rhs == Self == Point implPartialOrdforPoint{// impl automatically symmetric \u0026amp; transitive fn partial_cmp(\u0026amp;self,other: \u0026amp;Point)-\u0026gt; Option\u0026lt;Ordering\u0026gt;{Some(matchself.x.cmp(\u0026amp;other.x){Ordering::Equal=\u0026gt;self.y.cmp(\u0026amp;other.y),ordering=\u0026gt;ordering,})}}如果一个类型的所有成员都实现了 PartialOrd，那么它可以被派生。\n#[derive(PartialEq, PartialOrd)]struct Point{x: i32,y: i32,}#[derive(PartialEq, PartialOrd)]enum Stoplight{Red,Yellow,Green,}PartialOrd 派生宏基于其成员的字母顺序对类型进行排序。\n// generates PartialOrd impl which orders // Points based on x member first and // y member second because that\u0026#39;s the order // they appear in the source code #[derive(PartialOrd, PartialEq)]struct Point{x: i32,y: i32,}// generates DIFFERENT PartialOrd impl // which orders Points based on y member // first and x member second #[derive(PartialOrd, PartialEq)]struct Point{y: i32,x: i32,}Ord is a subtrait of Eq and PartialOrd\u0026lt;Self\u0026gt;: Ord 是 Eq 和 PartialOrd\u0026lt;Self\u0026gt; 的子 trait。\ntraitOrd: Eq +PartialOrd\u0026lt;Self\u0026gt;{fn cmp(\u0026amp;self,other: \u0026amp;Self)-\u0026gt; Ordering;// provided default impls fn max(self,other: Self)-\u0026gt; Self;fn min(self,other: Self)-\u0026gt; Self;fn clamp(self,min: Self,max: Self)-\u0026gt; Self;}如果我们为一个类型实现 Ord，在 PartialOrd 所要求的不对称性和传递性的基础上，我们还保证不对称性是完全的，即对于任何给定的 a 和 b，a == b 或 a \u0026gt; b 中只有一个是真的。在这个意义上，Ord 完善了 Eq 和 PartialOrd，因为它代表了一个更严格的比较版本。如果一个类型实现了 Ord，我们就可以用这个实现来实现 PartialOrd、PartialEq 和 Eq。\nusestd::cmp::Ordering;// of course we can use the derive macros here #[derive(Ord, PartialOrd, Eq, PartialEq)]struct Point{x: i32,y: i32,}// note: as with PartialOrd, the Ord derive macro // orders a type based on the lexicographical order // of its members // but here\u0026#39;s the impls if we wrote them out by hand implOrdforPoint{fn cmp(\u0026amp;self,other: \u0026amp;Self)-\u0026gt; Ordering{matchself.x.cmp(\u0026amp;self.y){Ordering::Equal=\u0026gt;self.y.cmp(\u0026amp;self.y),ordering=\u0026gt;ordering,}}}implPartialOrdforPoint{fn partial_cmp(\u0026amp;self,other: \u0026amp;Self)-\u0026gt; Option\u0026lt;Ordering\u0026gt;{Some(self.cmp(other))}}implPartialEqforPoint{fn eq(\u0026amp;self,other: \u0026amp;Self)-\u0026gt; bool {self.cmp(other)==Ordering::Equal}}implEqforPoint{}浮点数实现了 PartialOrd，但不是 Ord，因为 NaN \u0026lt; 0 == false 和 NaN \u0026gt;= 0 == false 同时为真。几乎所有其他的 PartialOrd 类型都是 Ord，当然，除非它们包含浮点数。\n一旦一个类型被认为是 Ord 的，我们就可以将其存储在 BTreeMap 和 BTreeSet 中，并且可以使用 sort() 方法对其进行排序，以及对数组、Vec 和 VecDeque 等任何类型的切片进行解引用。\nusestd::collections::BTreeSet;// now our type can be stored // in BTreeSets and BTreeMaps! #[derive(Ord, PartialOrd, PartialEq, Eq)]struct Point{x: i32,y: i32,}fn example_btreeset(){letmutpoints=BTreeSet::new();points.insert(Point{x: 0,y: 0});// ✅ }// we can also .sort() Ord types in collections! fn example_sort\u0026lt;T: Ord\u0026gt;(mutsortable: Vec\u0026lt;T\u0026gt;)-\u0026gt; Vec\u0026lt;T\u0026gt;{sortable.sort();sortable}Arithmetic Traits    Trait(s) Category Operator(s) Description     Add arithmetic + addition   AddAssign arithmetic += addition assignment   BitAnd arithmetic \u0026amp; bitwise AND   BitAndAssign arithmetic \u0026amp;= bitwise assignment   BitXor arithmetic ^ bitwise XOR   BitXorAssign arithmetic ^= bitwise XOR assignment   Div arithmetic / division   DivAssign arithmetic /= division assignment   Mul arithmetic * multiplication   MulAssign arithmetic *= multiplication assignment   Neg arithmetic - unary negation   Not arithmetic ! unary logical negation   Rem arithmetic % remainder   RemAssign arithmetic %= remainder assignment   Shl arithmetic \u0026lt;\u0026lt; left shift   ShlAssign arithmetic \u0026lt;\u0026lt;= left shift assignment   Shr arithmetic \u0026gt;\u0026gt; right shift   ShrAssign arithmetic \u0026gt;\u0026gt;= right shift assignment   Sub arithmetic - subtraction   SubAssign arithmetic -= subtraction assignment    仔细研究所有这些将是非常多余的。反正大多数只适用于数字类型。我们只讨论 Add 和 AddAssign，因为 + 操作符通常被重载来做其他事情，如向集合添加项目或将事物串联起来，这样我们就能覆盖最有趣的地方，而不会重复。\nAdd \u0026amp; AddAssign 预备知识\n Self Methods Associated Types Generic Parameters Generic Types vs Associated Types Derive Macros  traitAdd\u0026lt;Rhs=Self\u0026gt;{type Output;fn add(self,rhs: Rhs)-\u0026gt; Self::Output;}Add\u0026lt;Rhs, Output = T\u0026gt; 类型可以和 Rhs 类型相加，并将产生 T 作为输出。\n例子 Add\u0026lt;Point, Output = Point\u0026gt; 是针对 Point 实现的。\n#[derive(Clone, Copy)]struct Point{x: i32,y: i32,}implAddforPoint{type Output=Point;fn add(self,rhs: Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}fn main(){letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letp3=p1+p2;assert_eq!(p3.x,p1.x+p2.x);// ✅ assert_eq!(p3.y,p1.y+p2.y);// ✅ }但是如果我们只有对 Point 的引用呢？那我们还能让它们相加吗？让我们试试。\nfn main(){letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letp3=\u0026amp;p1+\u0026amp;p2;// ❌ }不幸的是没有。编译器会抛出异常：\nerror[E0369]: cannot add `\u0026amp;Point` to `\u0026amp;Point`\r--\u0026gt; src/main.rs:50:25\r|\r50 | let p3: Point = \u0026amp;p1 + \u0026amp;p2;\r| --- ^ --- \u0026amp;Point\r| |\r| \u0026amp;Point\r|\r= note: an implementation of `std::ops::Add` might be missing for `\u0026amp;Point`\r在 Rust 的类型系统中，对于某些类型 T 来说，T、\u0026amp;T 和 \u0026amp;mut T 都被视为唯一的不同类型，这意味着我们必须为它们分别提供 trait 实现。让我们为 \u0026amp;Point 定义一个 Add 实现：\nimplAddfor\u0026amp;Point{type Output=Point;fn add(self,rhs: \u0026amp;Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}fn main(){letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letp3=\u0026amp;p1+\u0026amp;p2;// ✅ assert_eq!(p3.x,p1.x+p2.x);// ✅ assert_eq!(p3.y,p1.y+p2.y);// ✅ }然而，有些事情还是感觉不大对劲。我们有两个独立的 Add 实现，分别用于 Point 和 \u0026amp;Point，它们目前做的是同样的事情，但不能保证将来也会这样做。例如，我们决定当我们把两个 Point 相加时，我们想创建一个包含这两个 Point 的 Line，而不是创建一个新的 Point，我们会像这样更新我们的 Add 程序：\nusestd::ops::Add;#[derive(Copy, Clone)]struct Point{x: i32,y: i32,}#[derive(Copy, Clone)]struct Line{start: Point,end: Point,}// we updated this impl implAddforPoint{type Output=Line;fn add(self,rhs: Point)-\u0026gt; Line{Line{start: self,end: rhs,}}}// but forgot to update this impl, uh oh! implAddfor\u0026amp;Point{type Output=Point;fn add(self,rhs: \u0026amp;Point)-\u0026gt; Point{Point{x: self.x+rhs.x,y: self.y+rhs.y,}}}fn main(){letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letline: Line=p1+p2;// ✅ letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letline: Line=\u0026amp;p1+\u0026amp;p2;// ❌ expected Line, found Point }我们目前对 \u0026amp;Point 的 Add 实现造成了不必要的维护负担，我们希望这个实现与 Point 的实现相匹配，而不必在每次改变 Point 的实现时都要手动更新。我们希望尽可能地保持我们的代码是 DRY（Don\u0026rsquo;t Repeat Yourself）。幸运的是这是可以实现的。\n// updated, DRY impl implAddfor\u0026amp;Point{type Output=\u0026lt;PointasAdd\u0026gt;::Output;fn add(self,rhs: \u0026amp;Point)-\u0026gt; Self::Output{Point::add(*self,*rhs)}}fn main(){letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letline: Line=p1+p2;// ✅ letp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};letline: Line=\u0026amp;p1+\u0026amp;p2;// ✅ }AddAssign\u0026lt;Rhs\u0026gt; 类型允许我们相加并分配 Rhs 类型给它们。Trait 声明如下：\ntraitAddAssign\u0026lt;Rhs=Self\u0026gt;{fn add_assign(\u0026amp;mutself,rhs: Rhs);}为 Point 和 \u0026amp;Point 类型的实现的例子如下：\nusestd::ops::AddAssign;#[derive(Copy, Clone)]struct Point{x: i32,y: i32 }implAddAssignforPoint{fn add_assign(\u0026amp;mutself,rhs: Point){self.x+=rhs.x;self.y+=rhs.y;}}implAddAssign\u0026lt;\u0026amp;Point\u0026gt;forPoint{fn add_assign(\u0026amp;mutself,rhs: \u0026amp;Point){Point::add_assign(self,*rhs);}}fn main(){letmutp1=Point{x: 1,y: 2};letp2=Point{x: 3,y: 4};p1+=\u0026amp;p2;p1+=p2;assert!(p1.x==7\u0026amp;\u0026amp;p1.y==10);}闭包 Traits    Trait(s) Category Operator(s) Description     Fn closure (...args) immutable closure invocation   FnMut closure (...args) mutable closure invocation   FnOnce closure (...args) one-time closure invocation    FnOnce, FnMut, \u0026amp; Fn 预备知识\n Self Methods Associated Types Generic Parameters Generic Types vs Associated Types Subtraits \u0026amp; Supertraits  traitFnOnce\u0026lt;Args\u0026gt;{type Output;fn call_once(self,args: Args)-\u0026gt; Self::Output;}traitFnMut\u0026lt;Args\u0026gt;: FnOnce\u0026lt;Args\u0026gt;{fn call_mut(\u0026amp;mutself,args: Args)-\u0026gt; Self::Output;}traitFn\u0026lt;Args\u0026gt;: FnMut\u0026lt;Args\u0026gt;{fn call(\u0026amp;self,args: Args)-\u0026gt; Self::Output;}虽然这些 trait 存在，但在稳定的 Rust 中，我们不可能为自己的类型实现这些特性。我们唯一能创建的实现这些 trait 的类型是闭包。根据闭包从其环境中捕获的内容，决定了它是实现了 FnOnce、FnMut 还是 Fn。\nFnOnce 闭包只能被调用一次，因为它在执行中会消耗一些值。\nfn main(){letrange=0..10;letget_range_count=||range.count();assert_eq!(get_range_count(),10);// ✅ get_range_count();// ❌ }迭代器上的 .count() 方法会消耗迭代器，所以它只能被调用一次。因此，我们的闭包只能被调用一次。这就是为什么当我们试图第二次调用它时，会出现这个错误。\nerror[E0382]: use of moved value: `get_range_count`\r--\u0026gt; src/main.rs:5:5\r|\r4 | assert_eq!(get_range_count(), 10);\r| ----------------- `get_range_count` moved due to this call\r5 | get_range_count();\r| ^^^^^^^^^^^^^^^ value used here after move\r|\rnote: closure cannot be invoked more than once because it moves the variable `range` out of its environment\r--\u0026gt; src/main.rs:3:30\r|\r3 | let get_range_count = || range.count();\r| ^^^^^\rnote: this value implements `FnOnce`, which causes it to be moved when called\r--\u0026gt; src/main.rs:4:16\r|\r4 | assert_eq!(get_range_count(), 10);\r| ^^^^^^^^^^^^^^^\rFnMut 闭包可以被多次调用，也可以改变它从环境中捕获的变量。我们可以说 FnMut 闭包是执行副作用的，或者说是有状态的。下面是一个闭包的例子，它通过跟踪到目前为止看到的最小值，从迭代器中过滤出所有非升序的值。\nfn main(){letnums=vec![0,4,2,8,10,7,15,18,13];letmutmin=i32::MIN;letascending=nums.into_iter().filter(|\u0026amp;n|{ifn\u0026lt;=min{false}else{min=n;true}}).collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;();assert_eq!(vec![0,4,8,10,15,18],ascending);// ✅ }FnMut 完善了 FnOnce，即 FnOnce 需要取得其参数的所有权，只能调用一次，但 FnMut 只需要取得可变的引用，可以多次调用。FnMut 可以在任何可以使用 FnOnce 的地方使用。\nFn 闭包可以被多次调用，并且不改变它从环境中捕获的任何变量。我们可以说 Fn 闭包没有副作用或无状态。下面是一个闭包的例子，它过滤掉了所有小于它从环境中捕获的迭代器中的某个栈变量的值。\nfn main(){letnums=vec![0,4,2,8,10,7,15,18,13];letmin=9;letgreater_than_9=nums.into_iter().filter(|\u0026amp;n|n\u0026gt;min).collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;();assert_eq!(vec![10,15,18,13],greater_than_9);// ✅ }Fn 细化了 FnMut，即 FnMut 需要可变的引用并可多次调用，但 Fn 只需要不可变的引用并可多次调用。Fn 可以用在任何可以使用 FnMut 的地方，包括可以使用 FnOnce 的地方。\n如果一个闭包没有从它的环境中捕获任何东西，那么从技术上讲，它不是一个闭包，而只是一个匿名声明的内联函数，并且可以作为一个普通的函数指针被转换、使用和传递，也就是 fn。函数指针可以在任何可以使用 Fn 的地方使用，这包括可以使用 FnMut 和 FnOnce 的地方。\nfn add_one(x: i32)-\u0026gt; i32 {x+1}fn main(){letmutfn_ptr: fn(i32)-\u0026gt; i32 =add_one;assert_eq!(fn_ptr(1),2);// ✅ // capture-less closure cast to fn pointer fn_ptr=|x|x+1;// same as add_one assert_eq!(fn_ptr(1),2);// ✅ }传递普通函数指针以代替闭包的例子:\nfn main(){letnums=vec![-1,1,-2,2,-3,3];letabsolutes: Vec\u0026lt;i32\u0026gt;=nums.into_iter().map(i32::abs).collect();assert_eq!(vec![1,1,2,2,3,3],absolutes);// ✅ }其他 Trait    Trait(s) Category Operator(s) Description     Deref other * immutable dereference   DerefMut other * mutable derenence   Drop other - type destructor   Index other [] immutable index   IndexMut other [] mutable index   RangeBounds other .. range    Deref \u0026amp; DerefMut 预备知识\n Self Methods Associated Types Subtraits \u0026amp; Supertraits Sized  traitDeref{type Target: ?Sized;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Self::Target;}traitDerefMut: Deref{fn deref_mut(\u0026amp;mutself)-\u0026gt; \u0026amp;mutSelf::Target;}可以使用解引用操作符 * 把实现了 Deref\u0026lt;Target = T\u0026gt; trait 的类型解引用为 T 类型。这对于智能指针类型（如 Box 和 Rc）有明显的用途。然而，我们很少看到在 Rust 代码中显式地使用解引用操作符，这是因为 Rust 的一个叫做 “强制解引用”(deref coercion)的特性。\n当类型作为函数参数传递、从函数返回或作为方法调用的一部分使用时，Rust 会自动地对类型进行解引用。这就是为什么我们可以将 \u0026amp;String 和 \u0026amp;Vec\u0026lt;T\u0026gt; 传递给期望 \u0026amp;str 和 \u0026amp;[T] 的函数的原因，因为 String 实现了 Deref\u0026lt;Target = str\u0026gt;, Vec\u0026lt;T\u0026gt; 实现了 Deref\u0026lt;Target = [T]\u0026gt;。\nDeref 和 DerefMut 只应为智能指针类型而实现。人们试图误用和滥用这些 trait 的最常见方式是试图将某种 OOP 式的数据继承塞进 Rust 中。这是行不通的。Rust 不是面向对象的。让我们来看看几种不同的情况，在哪些情况下，如何以及为什么它不起作用。让我们从这个例子开始:\nusestd::ops::Deref;struct Human{health_points: u32,}enum Weapon{Spear,Axe,Sword,}// a Soldier is just a Human with a Weapon struct Soldier{human: Human,weapon: Weapon,}implDerefforSoldier{type Target=Human;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.human}}enum Mount{Horse,Donkey,Cow,}// a Knight is just a Soldier with a Mount struct Knight{soldier: Soldier,mount: Mount,}implDerefforKnight{type Target=Soldier;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Soldier{\u0026amp;self.soldier}}enum Spell{MagicMissile,FireBolt,ThornWhip,}// a Mage is just a Human who can cast Spells struct Mage{human: Human,spells: Vec\u0026lt;Spell\u0026gt;,}implDerefforMage{type Target=Human;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.human}}enum Staff{Wooden,Metallic,Plastic,}// a Wizard is just a Mage with a Staff struct Wizard{mage: Mage,staff: Staff,}implDerefforWizard{type Target=Mage;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Mage{\u0026amp;self.mage}}fn borrows_human(human: \u0026amp;Human){}fn borrows_soldier(soldier: \u0026amp;Soldier){}fn borrows_knight(knight: \u0026amp;Knight){}fn borrows_mage(mage: \u0026amp;Mage){}fn borrows_wizard(wizard: \u0026amp;Wizard){}fn example(human: Human,soldier: Soldier,knight: Knight,mage: Mage,wizard: Wizard){// all types can be used as Humans borrows_human(\u0026amp;human);borrows_human(\u0026amp;soldier);borrows_human(\u0026amp;knight);borrows_human(\u0026amp;mage);borrows_human(\u0026amp;wizard);// Knights can be used as Soldiers borrows_soldier(\u0026amp;soldier);borrows_soldier(\u0026amp;knight);// Wizards can be used as Mages borrows_mage(\u0026amp;mage);borrows_mage(\u0026amp;wizard);// Knights \u0026amp; Wizards passed as themselves borrows_knight(\u0026amp;knight);borrows_wizard(\u0026amp;wizard);}因此，乍一看，上面的内容看起来很不错！然而，在仔细研究后，很快就发现了问题。首先，强制解引用只对引用起作用，所以当我们真正想要传递所有权时，它就不起作用了。\nfn takes_human(human: Human){}fn example(human: Human,soldier: Soldier,knight: Knight,mage: Mage,wizard: Wizard){// all types CANNOT be used as Humans takes_human(human);takes_human(soldier);// ❌ takes_human(knight);// ❌ takes_human(mage);// ❌ takes_human(wizard);// ❌ }此外，强制解引用在泛型上下文中不起作用。比方说，我们只在人类身上实现一些 trait：\ntraitRest{fn rest(\u0026amp;self);}implRestforHuman{fn rest(\u0026amp;self){}}fn take_rest\u0026lt;T: Rest\u0026gt;(rester: \u0026amp;T){rester.rest()}fn example(human: Human,soldier: Soldier,knight: Knight,mage: Mage,wizard: Wizard){// all types CANNOT be used as Rest types, only Human take_rest(\u0026amp;human);take_rest(\u0026amp;soldier);// ❌ take_rest(\u0026amp;knight);// ❌ take_rest(\u0026amp;mage);// ❌ take_rest(\u0026amp;wizard);// ❌ }另外，尽管强制解引用在很多地方都能工作，但并不是所有地方都能工作。它对操作数不起作用，尽管操作符只是方法调用的语法糖。比方说，我们想让 Mage(法师) 用 += 运算符来学习 Spell(咒语)，这很可爱。\nimplDerefMutforWizard{fn deref_mut(\u0026amp;mutself)-\u0026gt; \u0026amp;mutMage{\u0026amp;mutself.mage}}implAddAssign\u0026lt;Spell\u0026gt;forMage{fn add_assign(\u0026amp;mutself,spell: Spell){self.spells.push(spell);}}fn example(mutmage: Mage,mutwizard: Wizard,spell: Spell){mage+=spell;wizard+=spell;// ❌ wizard not coerced to mage here wizard.add_assign(spell);// oof, we have to call it like this 🤦 }在 OOP 式数据继承的语言中，方法中 self 的值总是等于调用该方法的类型，但在 Rust 中，self 的值总是等于实现该方法的类型。\nstruct Human{profession: \u0026amp;\u0026#39;staticstr,health_points: u32,}implHuman{// self will always be a Human here, even if we call it on a Soldier fn state_profession(\u0026amp;self){println!(\u0026#34;I\u0026#39;m a {}!\u0026#34;,self.profession);}}struct Soldier{profession: \u0026amp;\u0026#39;staticstr,human: Human,weapon: Weapon,}fn example(soldier: \u0026amp;Soldier){assert_eq!(\u0026#34;servant\u0026#34;,soldier.human.profession);assert_eq!(\u0026#34;spearman\u0026#34;,soldier.profession);soldier.human.state_profession();// prints \u0026#34;I\u0026#39;m a servant!\u0026#34; soldier.state_profession();// still prints \u0026#34;I\u0026#39;m a servant!\u0026#34; 🤦 }当在一个新类型上实现 Deref 或 DerefMut 时，上述的问题尤其严重。假设我们想创建一个 SortedVec 类型，它只是一个 Vec，但它总是按排序顺序排列。下面是我们如何做的。\nstruct SortedVec\u0026lt;T: Ord\u0026gt;(Vec\u0026lt;T\u0026gt;);impl\u0026lt;T: Ord\u0026gt;SortedVec\u0026lt;T\u0026gt;{fn new(mutvec: Vec\u0026lt;T\u0026gt;)-\u0026gt; Self{vec.sort();SortedVec(vec)}fn push(\u0026amp;mutself,t: T){self.0.push(t);self.0.sort();}}很明显，我们不能在这里实现 DerefMut\u0026lt;Target = Vec\u0026lt;T\u0026gt;\u0026gt;，否则任何使用 SortedVec 的人都可以轻易地破坏排序的顺序。然而，实现 Deref\u0026lt;Target = Vec\u0026lt;T\u0026gt;\u0026gt; 肯定是安全的，对吗？试着在下面的程序中发现这个错误。\nusestd::ops::Deref;struct SortedVec\u0026lt;T: Ord\u0026gt;(Vec\u0026lt;T\u0026gt;);impl\u0026lt;T: Ord\u0026gt;SortedVec\u0026lt;T\u0026gt;{fn new(mutvec: Vec\u0026lt;T\u0026gt;)-\u0026gt; Self{vec.sort();SortedVec(vec)}fn push(\u0026amp;mutself,t: T){self.0.push(t);self.0.sort();}}impl\u0026lt;T: Ord\u0026gt;DerefforSortedVec\u0026lt;T\u0026gt;{type Target=Vec\u0026lt;T\u0026gt;;fn deref(\u0026amp;self)-\u0026gt; \u0026amp;Vec\u0026lt;T\u0026gt;{\u0026amp;self.0}}fn main(){letsorted=SortedVec::new(vec![2,8,6,3]);sorted.push(1);letsortedClone=sorted.clone();sortedClone.push(4);}我们从未为 SortedVec 实现 Clone，所以当我们调用 .clone() 方法时，编译器使用强制解引用来解决对 Vec 的方法调用，所以它返回一个 Vec 而不是 SortedVec!\nfn main(){letsorted: SortedVec\u0026lt;i32\u0026gt;=SortedVec::new(vec![2,8,6,3]);sorted.push(1);// still sorted // calling clone on SortedVec actually returns a Vec 🤦 letsortedClone: Vec\u0026lt;i32\u0026gt;=sorted.clone();sortedClone.push(4);// sortedClone no longer sorted 💀 }总之，以上这些限制、约束或麻烦都不是 Rust 的缺点，因为 Rust 一开始就没有被设计成一种 OO 语言或支持任何 OOP 模式。\n本节的主要启示是，不要试图用 Deref 和 DerefMut 实现来表现可爱或聪明。它们实际上只适合于智能指针类型，目前只能在标准库中实现，因为智能指针类型目前需要不稳定的特性和编译器的魔法才能工作。如果我们想要类似于 Deref 和 DerefMut 的功能和行为，那么我们实际上可能要找的是 AsRef 和 AsMut，我们将在后面讨论。\nIndex \u0026amp; IndexMut 预备知识\n Self Methods Associated Types Generic Parameters Generic Types vs Associated Types Subtraits \u0026amp; Supertraits Sized  traitIndex\u0026lt;Idx: ?Sized\u0026gt;{type Output: ?Sized;fn index(\u0026amp;self,index: Idx)-\u0026gt; \u0026amp;Self::Output;}traitIndexMut\u0026lt;Idx\u0026gt;: Index\u0026lt;Idx\u0026gt;whereIdx: ?Sized{fn index_mut(\u0026amp;mutself,index: Idx)-\u0026gt; \u0026amp;mutSelf::Output;}我们可以将 [] 索引到有 T 值的 Index\u0026lt;T, Output = U\u0026gt; 类型，索引操作将返回 \u0026amp;U 值。对于语法糖，编译器会在任何从索引操作返回的值前面自动插入一个解引用运算符 *。\nfn main(){// Vec\u0026lt;i32\u0026gt; impls Index\u0026lt;usize, Output = i32\u0026gt; so // indexing Vec\u0026lt;i32\u0026gt; should produce \u0026amp;i32s and yet... letvec=vec![1,2,3,4,5];letnum_ref: \u0026amp;i32 =vec[0];// ❌ expected \u0026amp;i32 found i32 // above line actually desugars to letnum_ref: \u0026amp;i32 =*vec[0];// ❌ expected \u0026amp;i32 found i32 // both of these alternatives work letnum: i32 =vec[0];// ✅ letnum_ref=\u0026amp;vec[0];// ✅ }一开始有点让人困惑，因为看起来 Index trait 并不遵循它自己的方法签名，但实际上这只是有问题的语法糖。\n因为 Idx 是一个泛型类型，Index trait 可以为一个给定的类型实现很多次，在 Vec\u0026lt;T\u0026gt; 的情况下，我们不仅可以使用 usize 对其进行索引，我们还可以使用 Range\u0026lt;usize\u0026gt; 对其进行索引，以获得切片。\nfn main(){letvec=vec![1,2,3,4,5];assert_eq!(\u0026amp;vec[..],\u0026amp;[1,2,3,4,5]);// ✅ assert_eq!(\u0026amp;vec[1..],\u0026amp;[2,3,4,5]);// ✅ assert_eq!(\u0026amp;vec[..4],\u0026amp;[1,2,3,4]);// ✅ assert_eq!(\u0026amp;vec[1..4],\u0026amp;[2,3,4]);// ✅ }为了展示我们如何实现 Index，这里有一个有趣的例子，展示了我们如何使用一个新类型和 Index trait 来实现 Vec 的包装索引和负索引。\nusestd::ops::Index;struct WrappingIndex\u0026lt;T\u0026gt;(Vec\u0026lt;T\u0026gt;);impl\u0026lt;T\u0026gt;Index\u0026lt;usize\u0026gt;forWrappingIndex\u0026lt;T\u0026gt;{type Output=T;fn index(\u0026amp;self,index: usize)-\u0026gt; \u0026amp;T{\u0026amp;self.0[index%self.0.len()]}}impl\u0026lt;T\u0026gt;Index\u0026lt;i128\u0026gt;forWrappingIndex\u0026lt;T\u0026gt;{type Output=T;fn index(\u0026amp;self,index: i128)-\u0026gt; \u0026amp;T{letself_len=self.0.len()asi128;letidx=(((index%self_len)+self_len)%self_len)asusize;\u0026amp;self.0[idx]}}#[test]// ✅ fn indexes(){letwrapping_vec=WrappingIndex(vec![1,2,3]);assert_eq!(1,wrapping_vec[0_usize]);assert_eq!(2,wrapping_vec[1_usize]);assert_eq!(3,wrapping_vec[2_usize]);}#[test]// ✅ fn wrapping_indexes(){letwrapping_vec=WrappingIndex(vec![1,2,3]);assert_eq!(1,wrapping_vec[3_usize]);assert_eq!(2,wrapping_vec[4_usize]);assert_eq!(3,wrapping_vec[5_usize]);}#[test]// ✅ fn neg_indexes(){letwrapping_vec=WrappingIndex(vec![1,2,3]);assert_eq!(1,wrapping_vec[-3_i128]);assert_eq!(2,wrapping_vec[-2_i128]);assert_eq!(3,wrapping_vec[-1_i128]);}#[test]// ✅ fn wrapping_neg_indexes(){letwrapping_vec=WrappingIndex(vec![1,2,3]);assert_eq!(1,wrapping_vec[-6_i128]);assert_eq!(2,wrapping_vec[-5_i128]);assert_eq!(3,wrapping_vec[-4_i128]);}没有要求 Idx 类型必须是数字类型或 Range，它可以是一个枚举! 下面是一个例子，使用篮球位置索引到一个篮球队，以检索该队的球员。\nusestd::ops::Index;enum BasketballPosition{PointGuard,ShootingGuard,Center,PowerForward,SmallForward,}struct BasketballPlayer{name: \u0026amp;\u0026#39;staticstr,position: BasketballPosition,}struct BasketballTeam{point_guard: BasketballPlayer,shooting_guard: BasketballPlayer,center: BasketballPlayer,power_forward: BasketballPlayer,small_forward: BasketballPlayer,}implIndex\u0026lt;BasketballPosition\u0026gt;forBasketballTeam{type Output=BasketballPlayer;fn index(\u0026amp;self,position: BasketballPosition)-\u0026gt; \u0026amp;BasketballPlayer{matchposition{BasketballPosition::PointGuard=\u0026gt;\u0026amp;self.point_guard,BasketballPosition::ShootingGuard=\u0026gt;\u0026amp;self.shooting_guard,BasketballPosition::Center=\u0026gt;\u0026amp;self.center,BasketballPosition::PowerForward=\u0026gt;\u0026amp;self.power_forward,BasketballPosition::SmallForward=\u0026gt;\u0026amp;self.small_forward,}}}Drop 预备知识\n Self Methods  traitDrop{fn drop(\u0026amp;mutself);}如果一个类型实现了 Drop，那么当它超出作用域但在被销毁之前，drop 将在该类型上调用。我们很少需要为我们的类型实现这个功能，但是一个很好的例子是，如果一个类型持有一些外部资源，当类型被销毁时，这些资源需要被清理掉。\n在标准库中有一个 BufWriter 类型，允许我们对 Write 类型进行缓冲写入。然而，如果 BufWriter 在其缓冲区的内容被刷新到底层的 Write 类型之前就被销毁了呢？幸好这是不可能的! BufWriter 实现了 Drop trait，所以每当它离开作用域时，flush 总是被调用。\nimpl\u0026lt;W: Write\u0026gt;DropforBufWriter\u0026lt;W\u0026gt;{fn drop(\u0026amp;mutself){self.flush_buf();}}另外，Rust 中的 Mutex 没有 unlock() 方法，因为它们不需要这些方法。在一个 Mutex 上调用 lock() 会返回一个 MutexGuard，当 Mutex 超出作用域时，由于它的 Drop 实现，它会自动解锁。\nimpl\u0026lt;T: ?Sized\u0026gt;DropforMutexGuard\u0026lt;\u0026#39;_,T\u0026gt;{fn drop(\u0026amp;mutself){unsafe{self.lock.inner.raw_unlock();}}}一般来说，如果你在某些资源上实现一个抽象，在使用后需要清理，那么这就是使用 Drop trait 的一个很好的理由。\n转换 Traits From \u0026amp; Into 预备知识\n Self Functions Methods Generic Parameters Generic Blanket Impls  traitFrom\u0026lt;T\u0026gt;{fn from(T)-\u0026gt; Self;}From\u0026lt;T\u0026gt; 类型允许我们将 T 转换为 Self。\ntraitInto\u0026lt;T\u0026gt;{fn into(self)-\u0026gt; T;}Into\u0026lt;T\u0026gt; 类型允许我们将 Self 转换为 T。\n这些 trait 是一个硬币的两个不同侧面。我们只能为我们的类型实现 From\u0026lt;T\u0026gt;，因为 Into\u0026lt;T\u0026gt; 的实现是由下面这个泛型全面实现自动提供的。\nimpl\u0026lt;T,U\u0026gt;Into\u0026lt;U\u0026gt;forTwhereU: From\u0026lt;T\u0026gt;,{fn into(self)-\u0026gt; U{U::from(self)}}这两个 trait 存在的原因是，它允许我们以稍微不同的方式在泛型类型上编写 trait 约束(trait bound)。\nfn function\u0026lt;T\u0026gt;(t: T)where// these bounds are equivalent T: From\u0026lt;i32\u0026gt;,i32: Into\u0026lt;T\u0026gt;{// these examples are equivalent letexample: T=T::from(0);letexample: T=0.into();}关于何时使用这两种方法并没有硬性规定，所以在每种情况下都要选择最合理的方法。现在让我们看看一些关于 Point 的例子。\nstruct Point{x: i32,y: i32,}implFrom\u0026lt;(i32,i32)\u0026gt;forPoint{fn from((x,y): (i32,i32))-\u0026gt; Self{Point{x,y}}}implFrom\u0026lt;[i32;2]\u0026gt;forPoint{fn from([x,y]: [i32;2])-\u0026gt; Self{Point{x,y}}}fn example(){// using From letorigin=Point::from((0,0));letorigin=Point::from([0,0]);// using Into letorigin: Point=(0,0).into();letorigin: Point=[0,0].into();}实现不是对称的，所以如果我们想把 Point 转换成元组和数组，我们也必须显示地地添加这些。\nstruct Point{x: i32,y: i32,}implFrom\u0026lt;(i32,i32)\u0026gt;forPoint{fn from((x,y): (i32,i32))-\u0026gt; Self{Point{x,y}}}implFrom\u0026lt;Point\u0026gt;for(i32,i32){fn from(Point{x,y}: Point)-\u0026gt; Self{(x,y)}}implFrom\u0026lt;[i32;2]\u0026gt;forPoint{fn from([x,y]: [i32;2])-\u0026gt; Self{Point{x,y}}}implFrom\u0026lt;Point\u0026gt;for[i32;2]{fn from(Point{x,y}: Point)-\u0026gt; Self{[x,y]}}fn example(){// from (i32, i32) into Point letpoint=Point::from((0,0));letpoint: Point=(0,0).into();// from Point into (i32, i32) lettuple=\u0026lt;(i32,i32)\u0026gt;::from(point);lettuple: (i32,i32)=point.into();// from [i32; 2] into Point letpoint=Point::from([0,0]);letpoint: Point=[0,0].into();// from Point into [i32; 2] letarray=\u0026lt;[i32;2]\u0026gt;::from(point);letarray: [i32;2]=point.into();}From\u0026lt;T\u0026gt; 的一个普遍用途是缩减模板代码。假设我们在程序中加入一个包含三个 Point 的 Triangle 类型，这里有许多方法可以构建它。\nstruct Point{x: i32,y: i32,}implPoint{fn new(x: i32,y: i32)-\u0026gt; Point{Point{x,y}}}implFrom\u0026lt;(i32,i32)\u0026gt;forPoint{fn from((x,y): (i32,i32))-\u0026gt; Point{Point{x,y}}}struct Triangle{p1: Point,p2: Point,p3: Point,}implTriangle{fn new(p1: Point,p2: Point,p3: Point)-\u0026gt; Triangle{Triangle{p1,p2,p3}}}impl\u0026lt;P\u0026gt;From\u0026lt;[P;3]\u0026gt;forTrianglewhereP: Into\u0026lt;Point\u0026gt;{fn from([p1,p2,p3]: [P;3])-\u0026gt; Triangle{Triangle{p1: p1.into(),p2: p2.into(),p3: p3.into(),}}}fn example(){// manual construction lettriangle=Triangle{p1: Point{x: 0,y: 0,},p2: Point{x: 1,y: 1,},p3: Point{x: 2,y: 2,},};// using Point::new lettriangle=Triangle{p1: Point::new(0,0),p2: Point::new(1,1),p3: Point::new(2,2),};// using From\u0026lt;(i32, i32)\u0026gt; for Point lettriangle=Triangle{p1: (0,0).into(),p2: (1,1).into(),p3: (2,2).into(),};// using Triangle::new + From\u0026lt;(i32, i32)\u0026gt; for Point lettriangle=Triangle::new((0,0).into(),(1,1).into(),(2,2).into(),);// using From\u0026lt;[Into\u0026lt;Point\u0026gt;; 3]\u0026gt; for Triangle lettriangle: Triangle=[(0,0),(1,1),(2,2),].into();}对于何时、如何或为什么我们应该为我们的类型实现 From\u0026lt;T\u0026gt;，没有任何规则，所以这取决于我们对每种情况的最佳判断。\nInto\u0026lt;T\u0026gt; 的一个流行用法是使需要自有值(owned values)的函数在接受自有值或借用值时具有通用性。\nstruct Person{name: String,}implPerson{// accepts: // - String fn new1(name: String)-\u0026gt; Person{Person{name}}// accepts: // - String // - \u0026amp;String // - \u0026amp;str // - Box\u0026lt;str\u0026gt; // - Cow\u0026lt;\u0026#39;_, str\u0026gt; // - char // since all of the above types can be converted into String fn new2\u0026lt;N: Into\u0026lt;String\u0026gt;\u0026gt;(name: N)-\u0026gt; Person{Person{name: name.into()}}}错误处理 谈论错误处理和 Error trait 的最佳时机是在讨论完 Display、Debug、Any 和 From 之后，在讨论 TryFrom 之前，因此错误处理部分与转换 trait 部分尴尬地一分为二。\nError 预备知识\n Self Methods Default Impls Generic Blanket Impls Subtraits \u0026amp; Supertraits Trait Objects Display \u0026amp; ToString Debug Any From \u0026amp; Into  traitError: Debug+Display{// provided default impls fn source(\u0026amp;self)-\u0026gt; Option\u0026lt;\u0026amp;(dynError+\u0026#39;static)\u0026gt;;fn backtrace(\u0026amp;self)-\u0026gt; Option\u0026lt;\u0026amp;Backtrace\u0026gt;;fn description(\u0026amp;self)-\u0026gt; \u0026amp;str;fn cause(\u0026amp;self)-\u0026gt; Option\u0026lt;\u0026amp;dynError\u0026gt;;}在 Rust 中，错误被返回，而不是被抛出。我们来看看一些例子。\n由于整数类型除以0会引起 panic，如果我们想让我们的程序更安全、更明确，我们可以实现一个 safe_div 函数，返回一个 Result，就像这样。\nusestd::fmt;usestd::error;#[derive(Debug, PartialEq)]struct DivByZero;implfmt::DisplayforDivByZero{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{write!(f,\u0026#34;division by zero error\u0026#34;)}}implerror::ErrorforDivByZero{}fn safe_div(numerator: i32,denominator: i32)-\u0026gt; Result\u0026lt;i32,DivByZero\u0026gt;{ifdenominator==0{returnErr(DivByZero);}Ok(numerator/denominator)}#[test]// ✅ fn test_safe_div(){assert_eq!(safe_div(8,2),Ok(4));assert_eq!(safe_div(5,0),Err(DivByZero));}由于错误是返回的，而不是抛出的，所以必须显式处理，如果当前函数不能处理一个错误，它应该将其传播给调用者。传播错误的最习惯的方法是使用 ? 操作符，它只是现在被废弃的 try! 宏的语法糖，它只是做这个。\nmacro_rules!try{($expr:expr)=\u0026gt;{match$expr{// if Ok just unwrap the value Ok(val)=\u0026gt;val,// if Err map the err value using From and return Err(err)=\u0026gt;{returnErr(From::from(err));}}};}如果我们想写一个将文件读成 String 的函数，我们可以这样写，用 ? 将 io::Error 传播到它们可能出现的任何地方。\nusestd::io::Read;usestd::path::Path;usestd::io;usestd::fs::File;fn read_file_to_string(path: \u0026amp;Path)-\u0026gt; Result\u0026lt;String,io::Error\u0026gt;{letmutfile=File::open(path)?;// ⬆️ io::Error letmutcontents=String::new();file.read_to_string(\u0026amp;mutcontents)?;// ⬆️ io::Error Ok(contents)}但是，假设我们正在读取的文件实际上是一个数字列表，我们想把它们加在一起，我们会像这样更新我们的函数。\nusestd::io::Read;usestd::path::Path;usestd::io;usestd::fs::File;fn sum_file(path: \u0026amp;Path)-\u0026gt; Result\u0026lt;i32,/* What to put here? */\u0026gt;{letmutfile=File::open(path)?;// ⬆️ io::Error letmutcontents=String::new();file.read_to_string(\u0026amp;mutcontents)?;// ⬆️ io::Error letmutsum=0;forlineincontents.lines(){sum+=line.parse::\u0026lt;i32\u0026gt;()?;// ⬆️ ParseIntError }Ok(sum)}但是现在我们的 Result 的错误类型是什么？它可以返回一个 io::Error 或者一个 ParseIntError。我们将看一下解决这个问题的三种方法，从临时应急的方法开始，最后是最稳健的方法。\n第一种方法是认识到所有实现了 Error 的类型也实现了 Display，所以我们可以将所有的错误映射到 String，并使用 String 作为我们的错误类型。\nusestd::fs::File;usestd::io;usestd::io::Read;usestd::path::Path;fn sum_file(path: \u0026amp;Path)-\u0026gt; Result\u0026lt;i32,String\u0026gt;{letmutfile=File::open(path).map_err(|e|e.to_string())?;// ⬆️ io::Error -\u0026gt; String letmutcontents=String::new();file.read_to_string(\u0026amp;mutcontents).map_err(|e|e.to_string())?;// ⬆️ io::Error -\u0026gt; String letmutsum=0;forlineincontents.lines(){sum+=line.parse::\u0026lt;i32\u0026gt;().map_err(|e|e.to_string())?;// ⬆️ ParseIntError -\u0026gt; String }Ok(sum)}对每个错误进行字符串化处理的明显缺点是，我们丢弃了类型信息，这使得调用者更难处理错误。\n上述方法的一个非显而易见的好处是我们可以定制字符串以提供更多的特定环境信息。例如，ParseIntError 通常字符串化为 \u0026ldquo;invalid digit found in string\u0026rdquo;，这是非常模糊的，没有提到无效的字符串是什么或者它试图解析成什么整数类型。如果我们要调试这个问题，这个错误信息几乎是无用的。然而，我们可以通过自己提供所有与上下文相关的信息来使其明显改善。\nsum+=line.parse::\u0026lt;i32\u0026gt;().map_err(|_|format!(\u0026#34;failed to parse {} into i32\u0026#34;,line))?;第二种方法是利用标准库中的这种泛型全面实现。\nimpl\u0026lt;E: error::Error\u0026gt;From\u0026lt;E\u0026gt;forBox\u0026lt;dynerror::Error\u0026gt;;这意味着任何 Error 类型都可以通过 ? 运算符隐式地转换为 Box\u0026lt;dyn error::Error\u0026gt;，所以我们可以在任何产生错误的函数的 Result 返回类型中把错误类型设置为 Box\u0026lt;dyn error::Error\u0026gt;，? 运算符将为我们完成其余的工作。\nusestd::fs::File;usestd::io::Read;usestd::path::Path;usestd::error;fn sum_file(path: \u0026amp;Path)-\u0026gt; Result\u0026lt;i32,Box\u0026lt;dynerror::Error\u0026gt;\u0026gt;{letmutfile=File::open(path)?;// ⬆️ io::Error -\u0026gt; Box\u0026lt;dyn error::Error\u0026gt; letmutcontents=String::new();file.read_to_string(\u0026amp;mutcontents)?;// ⬆️ io::Error -\u0026gt; Box\u0026lt;dyn error::Error\u0026gt; letmutsum=0;forlineincontents.lines(){sum+=line.parse::\u0026lt;i32\u0026gt;()?;// ⬆️ ParseIntError -\u0026gt; Box\u0026lt;dyn error::Error\u0026gt; }Ok(sum)}虽然更加简洁，但这似乎与之前的方法有相同的缺点，即丢弃了类型信息。这大部分是真的，但是如果调用者知道我们函数的实现细节，他们仍然可以使用 error::Error 上的 downcast_ref() 方法来处理不同的错误类型，这和它在 dyn Any 类型上的作用是一样的。\nfn handle_sum_file_errors(path: \u0026amp;Path){matchsum_file(path){Ok(sum)=\u0026gt;println!(\u0026#34;the sum is {}\u0026#34;,sum),Err(err)=\u0026gt;{ifletSome(e)=err.downcast_ref::\u0026lt;io::Error\u0026gt;(){// handle io::Error }elseifletSome(e)=err.downcast_ref::\u0026lt;ParseIntError\u0026gt;(){// handle ParseIntError }else{// we know sum_file can only return one of the // above errors so this branch is unreachable unreachable!();}}}}第三种方法是最稳健和类型安全的方法，可以聚合这些不同的错误，是使用一个枚举建立我们自己的自定义错误类型。\nusestd::num::ParseIntError;usestd::fs::File;usestd::io;usestd::io::Read;usestd::path::Path;usestd::error;usestd::fmt;#[derive(Debug)]enum SumFileError{Io(io::Error),Parse(ParseIntError),}implFrom\u0026lt;io::Error\u0026gt;forSumFileError{fn from(err: io::Error)-\u0026gt; Self{SumFileError::Io(err)}}implFrom\u0026lt;ParseIntError\u0026gt;forSumFileError{fn from(err: ParseIntError)-\u0026gt; Self{SumFileError::Parse(err)}}implfmt::DisplayforSumFileError{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{matchself{SumFileError::Io(err)=\u0026gt;write!(f,\u0026#34;sum file error: {}\u0026#34;,err),SumFileError::Parse(err)=\u0026gt;write!(f,\u0026#34;sum file error: {}\u0026#34;,err),}}}implerror::ErrorforSumFileError{// the default impl for this method always returns None // but we can now override it to make it way more useful! fn source(\u0026amp;self)-\u0026gt; Option\u0026lt;\u0026amp;(dynerror::Error+\u0026#39;static)\u0026gt;{Some(matchself{SumFileError::Io(err)=\u0026gt;err,SumFileError::Parse(err)=\u0026gt;err,})}}fn sum_file(path: \u0026amp;Path)-\u0026gt; Result\u0026lt;i32,SumFileError\u0026gt;{letmutfile=File::open(path)?;// ⬆️ io::Error -\u0026gt; SumFileError letmutcontents=String::new();file.read_to_string(\u0026amp;mutcontents)?;// ⬆️ io::Error -\u0026gt; SumFileError letmutsum=0;forlineincontents.lines(){sum+=line.parse::\u0026lt;i32\u0026gt;()?;// ⬆️ ParseIntError -\u0026gt; SumFileError }Ok(sum)}fn handle_sum_file_errors(path: \u0026amp;Path){matchsum_file(path){Ok(sum)=\u0026gt;println!(\u0026#34;the sum is {}\u0026#34;,sum),Err(SumFileError::Io(err))=\u0026gt;{// handle io::Error },Err(SumFileError::Parse(err))=\u0026gt;{// handle ParseIntError },}}Conversion Traits Continued TryFrom \u0026amp; TryInto 预备知识\n Self Functions Methods Associated Types Generic Parameters Generic Types vs Associated Types Generic Blanket Impls From \u0026amp; Into Error  TryFrom and TryInto are the versions of From and Into. TryFrom 和 TryInto 是 From 和 Into 的不可靠版本。\ntraitTryFrom\u0026lt;T\u0026gt;{type Error;fn try_from(value: T)-\u0026gt; Result\u0026lt;Self,Self::Error\u0026gt;;}traitTryInto\u0026lt;T\u0026gt;{type Error;fn try_into(self)-\u0026gt; Result\u0026lt;T,Self::Error\u0026gt;;}Similarly to Into we cannot impl TryInto because its impl is provided by this generic blanket impl: 与 Into 类似，我们不能实现 TryInto，因为它的实现是由下面这个泛型全面实现提供的。\nimpl\u0026lt;T,U\u0026gt;TryInto\u0026lt;U\u0026gt;forTwhereU: TryFrom\u0026lt;T\u0026gt;,{type Error=U::Error;fn try_into(self)-\u0026gt; Result\u0026lt;U,U::Error\u0026gt;{U::try_from(self)}}Let\u0026rsquo;s say that in the context of our program it doesn\u0026rsquo;t make sense for Points to have x and y values that are less than -1000 or greater than 1000. This is how we\u0026rsquo;d rewrite our earlier From impls using TryFrom to signal to the users of our type that this conversion can now fail: 假设在我们的程序中，Point 的 x 和 y 的值小于 -1000 或大于 1000 是不合理的。这就是我们如何使用 TryFrom 重写我们先前的 From 实现，向我们类型的用户发出信号，这个转换现在可以失败。\nusestd::convert::TryFrom;usestd::error;usestd::fmt;struct Point{x: i32,y: i32,}#[derive(Debug)]struct OutOfBounds;implfmt::DisplayforOutOfBounds{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{write!(f,\u0026#34;out of bounds\u0026#34;)}}implerror::ErrorforOutOfBounds{}// now fallible implTryFrom\u0026lt;(i32,i32)\u0026gt;forPoint{type Error=OutOfBounds;fn try_from((x,y): (i32,i32))-\u0026gt; Result\u0026lt;Point,OutOfBounds\u0026gt;{ifx.abs()\u0026gt;1000||y.abs()\u0026gt;1000{returnErr(OutOfBounds);}Ok(Point{x,y})}}// still infallible implFrom\u0026lt;Point\u0026gt;for(i32,i32){fn from(Point{x,y}: Point)-\u0026gt; Self{(x,y)}}And here\u0026rsquo;s the refactored TryFrom\u0026lt;[TryInto\u0026lt;Point\u0026gt;; 3]\u0026gt; impl for Triangle: 这里是重构后的 TryFrom\u0026lt;[TryInto\u0026lt;Point\u0026gt;; 3]\u0026gt; 实现，用于 Triangle。\nusestd::convert::{TryFrom,TryInto};usestd::error;usestd::fmt;struct Point{x: i32,y: i32,}#[derive(Debug)]struct OutOfBounds;implfmt::DisplayforOutOfBounds{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{write!(f,\u0026#34;out of bounds\u0026#34;)}}implerror::ErrorforOutOfBounds{}implTryFrom\u0026lt;(i32,i32)\u0026gt;forPoint{type Error=OutOfBounds;fn try_from((x,y): (i32,i32))-\u0026gt; Result\u0026lt;Self,Self::Error\u0026gt;{ifx.abs()\u0026gt;1000||y.abs()\u0026gt;1000{returnErr(OutOfBounds);}Ok(Point{x,y})}}struct Triangle{p1: Point,p2: Point,p3: Point,}impl\u0026lt;P\u0026gt;TryFrom\u0026lt;[P;3]\u0026gt;forTrianglewhereP: TryInto\u0026lt;Point\u0026gt;,{type Error=P::Error;fn try_from([p1,p2,p3]: [P;3])-\u0026gt; Result\u0026lt;Self,Self::Error\u0026gt;{Ok(Triangle{p1: p1.try_into()?,p2: p2.try_into()?,p3: p3.try_into()?,})}}fn example()-\u0026gt; Result\u0026lt;Triangle,OutOfBounds\u0026gt;{lett: Triangle=[(0,0),(1,1),(2,2)].try_into()?;Ok(t)}FromStr 预备知识\n Self Functions Associated Types Error TryFrom \u0026amp; TryInto  traitFromStr{type Err;fn from_str(s: \u0026amp;str)-\u0026gt; Result\u0026lt;Self,Self::Err\u0026gt;;}FromStr types allow performing a fallible conversion from \u0026amp;str into Self. The idiomatic way to use FromStr is to call the .parse() method on \u0026amp;strs: FromStr 类型允许执行从 \u0026amp;str 到 Self 的错误转换。使用 FromStr 的习惯方法是对 \u0026amp;str 调用 .parse() 方法。\nusestd::str::FromStr;fn example\u0026lt;T: FromStr\u0026gt;(s: \u0026amp;\u0026#39;staticstr){// these are all equivalent lett: Result\u0026lt;T,_\u0026gt;=FromStr::from_str(s);lett=T::from_str(s);lett: Result\u0026lt;T,_\u0026gt;=s.parse();lett=s.parse::\u0026lt;T\u0026gt;();// most idiomatic }Point 实现的例子:\nusestd::error;usestd::fmt;usestd::iter::Enumerate;usestd::num::ParseIntError;usestd::str::{Chars,FromStr};#[derive(Debug, Eq, PartialEq)]struct Point{x: i32,y: i32,}implPoint{fn new(x: i32,y: i32)-\u0026gt; Self{Point{x,y}}}#[derive(Debug, PartialEq)]struct ParsePointError;implfmt::DisplayforParsePointError{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{write!(f,\u0026#34;failed to parse point\u0026#34;)}}implFrom\u0026lt;ParseIntError\u0026gt;forParsePointError{fn from(_e: ParseIntError)-\u0026gt; Self{ParsePointError}}implerror::ErrorforParsePointError{}implFromStrforPoint{type Err =ParsePointError;fn from_str(s: \u0026amp;str)-\u0026gt; Result\u0026lt;Self,Self::Err\u0026gt;{letis_num=|(_,c): \u0026amp;(usize,char)|matches!(c,\u0026#39;0\u0026#39;..=\u0026#39;9\u0026#39;|\u0026#39;-\u0026#39;);letisnt_num=|t: \u0026amp;(_,_)|!is_num(t);letget_num=|char_idxs: \u0026amp;mutEnumerate\u0026lt;Chars\u0026lt;\u0026#39;_\u0026gt;\u0026gt;|-\u0026gt; Result\u0026lt;(usize,usize),ParsePointError\u0026gt;{let(start,_)=char_idxs.skip_while(isnt_num).next().ok_or(ParsePointError)?;let(end,_)=char_idxs.skip_while(is_num).next().ok_or(ParsePointError)?;Ok((start,end))};letmutchar_idxs=s.chars().enumerate();let(x_start,x_end)=get_num(\u0026amp;mutchar_idxs)?;let(y_start,y_end)=get_num(\u0026amp;mutchar_idxs)?;letx=s[x_start..x_end].parse::\u0026lt;i32\u0026gt;()?;lety=s[y_start..y_end].parse::\u0026lt;i32\u0026gt;()?;Ok(Point{x,y})}}#[test]// ✅ fn pos_x_y(){letp=\u0026#34;(4, 5)\u0026#34;.parse::\u0026lt;Point\u0026gt;();assert_eq!(p,Ok(Point::new(4,5)));}#[test]// ✅ fn neg_x_y(){letp=\u0026#34;(-6, -2)\u0026#34;.parse::\u0026lt;Point\u0026gt;();assert_eq!(p,Ok(Point::new(-6,-2)));}#[test]// ✅ fn not_a_point(){letp=\u0026#34;not a point\u0026#34;.parse::\u0026lt;Point\u0026gt;();assert_eq!(p,Err(ParsePointError));}FromStr has the same signature as TryFrom\u0026lt;\u0026amp;str\u0026gt;. It doesn\u0026rsquo;t matter which one we impl for a type first as long as we forward the impl to the other one. Here\u0026rsquo;s a TryFrom\u0026lt;\u0026amp;str\u0026gt; impl for Point assuming it already has a FromStr impl: FromStr 与 TryFrom\u0026lt;\u0026amp;str\u0026gt; 的签名相同。只要我们把实现转发给另一个类型，哪一个实现并不重要。下面是一个针对 Point 的 TryFrom\u0026lt;\u0026amp;str\u0026gt; 实现，假设它已经有一个 FromStr 实现。\nimplTryFrom\u0026lt;\u0026amp;str\u0026gt;forPoint{type Error=\u0026lt;PointasFromStr\u0026gt;::Err;fn try_from(s: \u0026amp;str)-\u0026gt; Result\u0026lt;Point,Self::Error\u0026gt;{\u0026lt;PointasFromStr\u0026gt;::from_str(s)}}AsRef \u0026amp; AsMut 预备知识\n Self Methods Sized Generic Parameters Sized Deref \u0026amp; DerefMut  traitAsRef\u0026lt;T: ?Sized\u0026gt;{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;T;}traitAsMut\u0026lt;T: ?Sized\u0026gt;{fn as_mut(\u0026amp;mutself)-\u0026gt; \u0026amp;mutT;}?Sized 说明 T 类型是大小不确定的。As 作为介词, 表明发生了类型转换。\nAsRef is for cheap reference to reference conversions. However, one of the most common ways it\u0026rsquo;s used is to make functions generic over whether they take ownership or not:\nAsRef 是用于廉价的引用到引用的转换。然而，它最常见的使用方式之一是使函数在是否拥有所有权上通用。\n// accepts: // - \u0026amp;str // - \u0026amp;String fn takes_str(s: \u0026amp;str){// use \u0026amp;str }// accepts: // - \u0026amp;str // - \u0026amp;String // - String fn takes_asref_str\u0026lt;S: AsRef\u0026lt;str\u0026gt;\u0026gt;(s: S){lets: \u0026amp;str =s.as_ref();// use \u0026amp;str }fn example(slice: \u0026amp;str,borrow: \u0026amp;String,owned: String){takes_str(slice);takes_str(borrow);takes_str(owned);// ❌ takes_asref_str(slice);takes_asref_str(borrow);takes_asref_str(owned);// ✅ }The other most common use-case is returning a reference to inner private data wrapped by a type which protects some invariant. A good example from the standard library is String which is just a wrapper around Vec\u0026lt;u8\u0026gt;:\n另一个最常见的用例是返回一个对内部私有数据的引用，该数据由一个保护某些不变性的类型包裹。标准库中的一个很好的例子是 String，它只是 Vec\u0026lt;u8\u0026gt; 的一个包装器。\nstruct String {vec: Vec\u0026lt;u8\u0026gt;,}This inner Vec cannot be made public because if it was people could mutate any byte and break the String\u0026rsquo;s valid UTF-8 encoding. However, it\u0026rsquo;s safe to expose an immutable read-only reference to the inner byte array, hence this impl: 这个内部的 Vec 不能被公开，因为如果它被公开，人们可以改变任何字节并破坏 String 的有效 UTF-8 编码。然而，公开内部字节数组的不可变的只读引用是安全的，因此有了这个实现:\nimplAsRef\u0026lt;[u8]\u0026gt;forString;Generally, it often only makes sense to impl AsRef for a type if it wraps some other type to either provide additional functionality around the inner type or protect some invariant on the inner type.\nLet\u0026rsquo;s examine a example of bad AsRef impls: 一般来说，只有当一个类型包装了其他类型，为内部类型提供了额外的功能，或者保护了内部类型的某些不变性时，为其实现 AsRef 才有意义。\n让我们来看看一个不好的 AsRef 实现的例子。\nstruct User{name: String,age: u32,}implAsRef\u0026lt;String\u0026gt;forUser{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;String {\u0026amp;self.name}}implAsRef\u0026lt;u32\u0026gt;forUser{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;u32 {\u0026amp;self.age}}This works and kinda makes sense at first, but quickly falls apart if we add more members to User: 这在一开始是可行的，而且有点道理，但如果我们给 User 增加更多的成员，很快就会崩溃。\nstruct User{name: String,email: String,age: u32,height: u32,}implAsRef\u0026lt;String\u0026gt;forUser{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;String {// uh, do we return name or email here? }}implAsRef\u0026lt;u32\u0026gt;forUser{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;u32 {// uh, do we return age or height here? }}A User is composed of Strings and u32s but it\u0026rsquo;s not really the same thing as a String or a u32. Even if we had much more specific types: User 是由 String 和 u32 组成的，但它和 String 或 u32 并不是真正的一回事。即使我们有更具体的类型。\nstruct User{name: Name,email: Email,age: Age,height: Height,}It wouldn\u0026rsquo;t make much sense to impl AsRef for any of those because AsRef is for cheap reference to reference conversions between semantically equivalent things, and Name, Email, Age, and Height by themselves are not the same thing as a User.\nA good example where we would impl AsRef would be if we introduced a new type Moderator that just wrapped a User and added some moderation specific privileges: 实现 AsRef 对这些都没有意义，因为 AsRef 是用来在语义上等同的事物之间进行廉价的引用转换，而 Name、Email、Age 和 Height 本身就和 User 不是一回事。\n一个很好的例子是，如果我们引入一个新的类型 Moderator，它只是包裹了一个 User，并增加了一些特定的管理权限，我们就会使用 AsRef。\nstruct User{name: String,age: u32,}// unfortunately the standard library cannot provide // a generic blanket impl to save us from this boilerplate implAsRef\u0026lt;User\u0026gt;forUser{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;User{self}}enum Privilege{BanUsers,EditPosts,DeletePosts,}// although Moderators have some special // privileges they are still regular Users // and should be able to do all the same stuff struct Moderator{user: User,privileges: Vec\u0026lt;Privilege\u0026gt;}implAsRef\u0026lt;Moderator\u0026gt;forModerator{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Moderator{self}}implAsRef\u0026lt;User\u0026gt;forModerator{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;User{\u0026amp;self.user}}// this should be callable with Users // and Moderators (who are also Users) fn create_post\u0026lt;U: AsRef\u0026lt;User\u0026gt;\u0026gt;(u: U){letuser=u.as_ref();// etc }fn example(user: User,moderator: Moderator){create_post(\u0026amp;user);create_post(\u0026amp;moderator);// ✅ }This works because Moderators are just Users. Here\u0026rsquo;s the example from the Deref section except using AsRef instead: 这样做是因为 Moderator 就是 User。下面是 Deref 部分的例子，只是用 AsRef 代替。\nusestd::convert::AsRef;struct Human{health_points: u32,}implAsRef\u0026lt;Human\u0026gt;forHuman{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Human{self}}enum Weapon{Spear,Axe,Sword,}// a Soldier is just a Human with a Weapon struct Soldier{human: Human,weapon: Weapon,}implAsRef\u0026lt;Soldier\u0026gt;forSoldier{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Soldier{self}}implAsRef\u0026lt;Human\u0026gt;forSoldier{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.human}}enum Mount{Horse,Donkey,Cow,}// a Knight is just a Soldier with a Mount struct Knight{soldier: Soldier,mount: Mount,}implAsRef\u0026lt;Knight\u0026gt;forKnight{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Knight{self}}implAsRef\u0026lt;Soldier\u0026gt;forKnight{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Soldier{\u0026amp;self.soldier}}implAsRef\u0026lt;Human\u0026gt;forKnight{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.soldier.human}}enum Spell{MagicMissile,FireBolt,ThornWhip,}// a Mage is just a Human who can cast Spells struct Mage{human: Human,spells: Vec\u0026lt;Spell\u0026gt;,}implAsRef\u0026lt;Mage\u0026gt;forMage{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Mage{self}}implAsRef\u0026lt;Human\u0026gt;forMage{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.human}}enum Staff{Wooden,Metallic,Plastic,}// a Wizard is just a Mage with a Staff struct Wizard{mage: Mage,staff: Staff,}implAsRef\u0026lt;Wizard\u0026gt;forWizard{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Wizard{self}}implAsRef\u0026lt;Mage\u0026gt;forWizard{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Mage{\u0026amp;self.mage}}implAsRef\u0026lt;Human\u0026gt;forWizard{fn as_ref(\u0026amp;self)-\u0026gt; \u0026amp;Human{\u0026amp;self.mage.human}}fn borrows_human\u0026lt;H: AsRef\u0026lt;Human\u0026gt;\u0026gt;(human: H){}fn borrows_soldier\u0026lt;S: AsRef\u0026lt;Soldier\u0026gt;\u0026gt;(soldier: S){}fn borrows_knight\u0026lt;K: AsRef\u0026lt;Knight\u0026gt;\u0026gt;(knight: K){}fn borrows_mage\u0026lt;M: AsRef\u0026lt;Mage\u0026gt;\u0026gt;(mage: M){}fn borrows_wizard\u0026lt;W: AsRef\u0026lt;Wizard\u0026gt;\u0026gt;(wizard: W){}fn example(human: Human,soldier: Soldier,knight: Knight,mage: Mage,wizard: Wizard){// all types can be used as Humans borrows_human(\u0026amp;human);borrows_human(\u0026amp;soldier);borrows_human(\u0026amp;knight);borrows_human(\u0026amp;mage);borrows_human(\u0026amp;wizard);// Knights can be used as Soldiers borrows_soldier(\u0026amp;soldier);borrows_soldier(\u0026amp;knight);// Wizards can be used as Mages borrows_mage(\u0026amp;mage);borrows_mage(\u0026amp;wizard);// Knights \u0026amp; Wizards passed as themselves borrows_knight(\u0026amp;knight);borrows_wizard(\u0026amp;wizard);}Deref didn\u0026rsquo;t work in the prior version of the example above because deref coercion is an implicit conversion between types which leaves room for people to mistakenly formulate the wrong ideas and expectations for how it will behave. AsRef works above because it makes the conversion between types explicit and there\u0026rsquo;s no room leftover to develop any wrong ideas or expectations. Deref 在上述例子的先前版本中不起作用，因为 deref coercion 是一种隐式类型转换，为人们错误地制定错误的想法和期望留下了空间。AsRef 在上面起作用，因为它使类型间的转换变得明确，没有余地来发展任何错误的想法或期望。\nBorrow \u0026amp; BorrowMut 预备知识\n Self Methods Generic Parameters Subtraits \u0026amp; Supertraits Sized AsRef \u0026amp; AsMut PartialEq \u0026amp; Eq Hash PartialOrd \u0026amp; Ord  traitBorrow\u0026lt;Borrowed\u0026gt;whereBorrowed: ?Sized,{fn borrow(\u0026amp;self)-\u0026gt; \u0026amp;Borrowed;}traitBorrowMut\u0026lt;Borrowed\u0026gt;: Borrow\u0026lt;Borrowed\u0026gt;whereBorrowed: ?Sized,{fn borrow_mut(\u0026amp;mutself)-\u0026gt; \u0026amp;mutBorrowed;}These traits were invented to solve the very specific problem of looking up String keys in HashSets, HashMaps, BTreeSets, and BTreeMaps using \u0026amp;str values.\nWe can view Borrow\u0026lt;T\u0026gt; and BorrowMut\u0026lt;T\u0026gt; as stricter versions of AsRef\u0026lt;T\u0026gt; and AsMut\u0026lt;T\u0026gt;, where the returned reference \u0026amp;T has equivalent Eq, Hash, and Ord impls to Self. This is more easily explained with a commented example: 这些 trait 的发明是为了解决在 HashSet, HashMap, BTreeSet, 和 BTreeMap 中使用 \u0026amp;str 值查找 String 键的特殊问题。\n我们可以把 Borrow\u0026lt;T\u0026gt; 和 BorrowMut\u0026lt;T\u0026gt; 看作是 AsRef\u0026lt;T\u0026gt; 和 AsMut\u0026lt;T\u0026gt; 的更严格的版本，其中返回的引用 \u0026amp;T 与 Self 的 Eq、Hash 和 Ord 等值。用一个注释的例子可以更容易地解释这个问题。\nusestd::borrow::Borrow;usestd:#️⃣:Hasher;usestd::collections::hash_map::DefaultHasher;usestd:#️⃣:Hash;fn get_hash\u0026lt;T: Hash\u0026gt;(t: T)-\u0026gt; u64 {letmuthasher=DefaultHasher::new();t.hash(\u0026amp;muthasher);hasher.finish()}fn asref_example\u0026lt;Owned,Ref\u0026gt;(owned1: Owned,owned2: Owned)whereOwned: Eq +Ord+Hash+AsRef\u0026lt;Ref\u0026gt;,Ref: Eq +Ord+Hash{letref1: \u0026amp;Ref=owned1.as_ref();letref2: \u0026amp;Ref=owned2.as_ref();// refs aren\u0026#39;t required to be equal if owned types are equal assert_eq!(owned1==owned2,ref1==ref2);// ❌ letowned1_hash=get_hash(\u0026amp;owned1);letowned2_hash=get_hash(\u0026amp;owned2);letref1_hash=get_hash(\u0026amp;ref1);letref2_hash=get_hash(\u0026amp;ref2);// ref hashes aren\u0026#39;t required to be equal if owned type hashes are equal assert_eq!(owned1_hash==owned2_hash,ref1_hash==ref2_hash);// ❌ // ref comparisons aren\u0026#39;t required to match owned type comparisons assert_eq!(owned1.cmp(\u0026amp;owned2),ref1.cmp(\u0026amp;ref2));// ❌ }fn borrow_example\u0026lt;Owned,Borrowed\u0026gt;(owned1: Owned,owned2: Owned)whereOwned: Eq +Ord+Hash+Borrow\u0026lt;Borrowed\u0026gt;,Borrowed: Eq +Ord+Hash{letborrow1: \u0026amp;Borrowed=owned1.borrow();letborrow2: \u0026amp;Borrowed=owned2.borrow();// borrows are required to be equal if owned types are equal assert_eq!(owned1==owned2,borrow1==borrow2);// ✅ letowned1_hash=get_hash(\u0026amp;owned1);letowned2_hash=get_hash(\u0026amp;owned2);letborrow1_hash=get_hash(\u0026amp;borrow1);letborrow2_hash=get_hash(\u0026amp;borrow2);// borrow hashes are required to be equal if owned type hashes are equal assert_eq!(owned1_hash==owned2_hash,borrow1_hash==borrow2_hash);// ✅ // borrow comparisons are required to match owned type comparisons assert_eq!(owned1.cmp(\u0026amp;owned2),borrow1.cmp(\u0026amp;borrow2));// ✅ }It\u0026rsquo;s good to be aware of these traits and understand why they exist since it helps demystify some of the methods on HashSet, HashMap, BTreeSet, and BTreeMap but it\u0026rsquo;s very rare that we would ever need to impl these traits for any of our types because it\u0026rsquo;s very rare that we would ever need create a pair of types where one is the \u0026ldquo;borrowed\u0026rdquo; version of the other in the first place. If we have some T then \u0026amp;T will get the job done 99.99% of the time, and T: Borrow\u0026lt;T\u0026gt; is already implemented for all T because of a generic blanket impl, so we don\u0026rsquo;t need to manually impl it and we don\u0026rsquo;t need to create some U such that T: Borrow\u0026lt;U\u0026gt;.\n知道这些 trait 并理解它们存在的原因是很好的，因为这有助于解开 HashSet、HashMap、BTreeSet 和 BTreeMap 上的一些方法，但是我们很少需要为我们的任何类型实现这些 trait，因为我们很少需要创建一对类型，其中一个是另一个的 \u0026ldquo;借用\u0026rdquo; 版本。如果我们有一些 T，那么 T 在 99.99% 的情况下都能完成工作，而且 T: Borrow\u0026lt;T\u0026gt; 已经为所有的 T 实现了，因为有一个通用的一揽子实现，所以我们不需要手动实现它，我们也不需要创建一些 U，使 T: Borrow\u0026lt;U\u0026gt;。\nToOwned 预备知识\n Self Methods Default Impls Clone Borrow \u0026amp; BorrowMut  traitToOwned{type Owned: Borrow\u0026lt;Self\u0026gt;;fn to_owned(\u0026amp;self)-\u0026gt; Self::Owned;// provided default impls fn clone_into(\u0026amp;self,target: \u0026amp;mutSelf::Owned);}ToOwned is a more generic version of Clone. Clone allows us to take a \u0026amp;T and turn it into an T but ToOwned allows us to take a \u0026amp;Borrowed and turn it into a Owned where Owned: Borrow\u0026lt;Borrowed\u0026gt;.\nIn other words, we can\u0026rsquo;t \u0026ldquo;clone\u0026rdquo; a \u0026amp;str into a String, or a \u0026amp;Path into a PathBuf, or an \u0026amp;OsStr into an OsString, since the clone method signature doesn\u0026rsquo;t support this kind of cross-type cloning, and that\u0026rsquo;s what ToOwned was made for.\nFor similar reasons as Borrow and BorrowMut, it\u0026rsquo;s good to be aware of this trait and understand why it exists but it\u0026rsquo;s very rare we\u0026rsquo;ll ever need to impl it for any of our types.\nToOwned 是 Clone 的一个更通用的版本。Clone 允许我们把一个 \u0026amp;T 变成一个 T，但 ToOwned 允许我们把一个 \u0026amp;Borrowed 变成一个 Owned，其中 Owned: Borrow\u0026lt;Borrowed\u0026gt;。\n换句话说，我们不能把一个 \u0026amp;str 克隆成一个 String，或者把一个 \u0026amp;Path 克隆成一个 PathBuf，或者把一个 \u0026amp;OsStr 克隆成一个 OsString，因为 clone 方法签名不支持这种跨类型克隆，而这正是 ToOwned 的用途。\n出于与 Borrow 和 BorrowMut 类似的原因，知道这个 trait 并理解它存在的原因是很好的，但我们很少需要为我们的任何类型实现这个 trait。\nIteration Traits Iterator 预备知识\n Self Methods Associated Types Default Impls  traitIterator{type Item;fn next(\u0026amp;mutself)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;;// provided default impls fn size_hint(\u0026amp;self)-\u0026gt; (usize,Option\u0026lt;usize\u0026gt;);fn count(self)-\u0026gt; usize;fn last(self)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;;fn advance_by(\u0026amp;mutself,n: usize)-\u0026gt; Result\u0026lt;(),usize\u0026gt;;fn nth(\u0026amp;mutself,n: usize)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;;fn step_by(self,step: usize)-\u0026gt; StepBy\u0026lt;Self\u0026gt;;fn chain\u0026lt;U\u0026gt;(self,other: U)-\u0026gt; Chain\u0026lt;Self,\u0026lt;UasIntoIterator\u0026gt;::IntoIter\u0026gt;whereU: IntoIterator\u0026lt;Item=Self::Item\u0026gt;;fn zip\u0026lt;U\u0026gt;(self,other: U)-\u0026gt; Zip\u0026lt;Self,\u0026lt;UasIntoIterator\u0026gt;::IntoIter\u0026gt;whereU: IntoIterator;fn map\u0026lt;B,F\u0026gt;(self,f: F)-\u0026gt; Map\u0026lt;Self,F\u0026gt;whereF: FnMut(Self::Item)-\u0026gt; B;fn for_each\u0026lt;F\u0026gt;(self,f: F)whereF: FnMut(Self::Item);fn filter\u0026lt;P\u0026gt;(self,predicate: P)-\u0026gt; Filter\u0026lt;Self,P\u0026gt;whereP: FnMut(\u0026amp;Self::Item)-\u0026gt; bool;fn filter_map\u0026lt;B,F\u0026gt;(self,f: F)-\u0026gt; FilterMap\u0026lt;Self,F\u0026gt;whereF: FnMut(Self::Item)-\u0026gt; Option\u0026lt;B\u0026gt;;fn enumerate(self)-\u0026gt; Enumerate\u0026lt;Self\u0026gt;;fn peekable(self)-\u0026gt; Peekable\u0026lt;Self\u0026gt;;fn skip_while\u0026lt;P\u0026gt;(self,predicate: P)-\u0026gt; SkipWhile\u0026lt;Self,P\u0026gt;whereP: FnMut(\u0026amp;Self::Item)-\u0026gt; bool;fn take_while\u0026lt;P\u0026gt;(self,predicate: P)-\u0026gt; TakeWhile\u0026lt;Self,P\u0026gt;whereP: FnMut(\u0026amp;Self::Item)-\u0026gt; bool;fn map_while\u0026lt;B,P\u0026gt;(self,predicate: P)-\u0026gt; MapWhile\u0026lt;Self,P\u0026gt;whereP: FnMut(Self::Item)-\u0026gt; Option\u0026lt;B\u0026gt;;fn skip(self,n: usize)-\u0026gt; Skip\u0026lt;Self\u0026gt;;fn take(self,n: usize)-\u0026gt; Take\u0026lt;Self\u0026gt;;fn scan\u0026lt;St,B,F\u0026gt;(self,initial_state: St,f: F)-\u0026gt; Scan\u0026lt;Self,St,F\u0026gt;whereF: FnMut(\u0026amp;mutSt,Self::Item)-\u0026gt; Option\u0026lt;B\u0026gt;;fn flat_map\u0026lt;U,F\u0026gt;(self,f: F)-\u0026gt; FlatMap\u0026lt;Self,U,F\u0026gt;whereF: FnMut(Self::Item)-\u0026gt; U,U: IntoIterator;fn flatten(self)-\u0026gt; Flatten\u0026lt;Self\u0026gt;whereSelf::Item: IntoIterator;fn fuse(self)-\u0026gt; Fuse\u0026lt;Self\u0026gt;;fn inspect\u0026lt;F\u0026gt;(self,f: F)-\u0026gt; Inspect\u0026lt;Self,F\u0026gt;whereF: FnMut(\u0026amp;Self::Item);fn by_ref(\u0026amp;mutself)-\u0026gt; \u0026amp;mutSelf;fn collect\u0026lt;B\u0026gt;(self)-\u0026gt; BwhereB: FromIterator\u0026lt;Self::Item\u0026gt;;fn partition\u0026lt;B,F\u0026gt;(self,f: F)-\u0026gt; (B,B)whereF: FnMut(\u0026amp;Self::Item)-\u0026gt; bool,B: Default +Extend\u0026lt;Self::Item\u0026gt;;fn partition_in_place\u0026lt;\u0026#39;a,T,P\u0026gt;(self,predicate: P)-\u0026gt; usize whereSelf: DoubleEndedIterator\u0026lt;Item=\u0026amp;\u0026#39;amutT\u0026gt;,T: \u0026#39;a,P: FnMut(\u0026amp;T)-\u0026gt; bool;fn is_partitioned\u0026lt;P\u0026gt;(self,predicate: P)-\u0026gt; bool whereP: FnMut(Self::Item)-\u0026gt; bool;fn try_fold\u0026lt;B,F,R\u0026gt;(\u0026amp;mutself,init: B,f: F)-\u0026gt; RwhereF: FnMut(B,Self::Item)-\u0026gt; R,R: Try\u0026lt;Ok=B\u0026gt;;fn try_for_each\u0026lt;F,R\u0026gt;(\u0026amp;mutself,f: F)-\u0026gt; RwhereF: FnMut(Self::Item)-\u0026gt; R,R: Try\u0026lt;Ok=()\u0026gt;;fn fold\u0026lt;B,F\u0026gt;(self,init: B,f: F)-\u0026gt; BwhereF: FnMut(B,Self::Item)-\u0026gt; B;fn fold_first\u0026lt;F\u0026gt;(self,f: F)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereF: FnMut(Self::Item,Self::Item)-\u0026gt; Self::Item;fn all\u0026lt;F\u0026gt;(\u0026amp;mutself,f: F)-\u0026gt; bool whereF: FnMut(Self::Item)-\u0026gt; bool;fn any\u0026lt;F\u0026gt;(\u0026amp;mutself,f: F)-\u0026gt; bool whereF: FnMut(Self::Item)-\u0026gt; bool;fn find\u0026lt;P\u0026gt;(\u0026amp;mutself,predicate: P)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereP: FnMut(\u0026amp;Self::Item)-\u0026gt; bool;fn find_map\u0026lt;B,F\u0026gt;(\u0026amp;mutself,f: F)-\u0026gt; Option\u0026lt;B\u0026gt;whereF: FnMut(Self::Item)-\u0026gt; Option\u0026lt;B\u0026gt;;fn try_find\u0026lt;F,R\u0026gt;(\u0026amp;mutself,f: F)-\u0026gt; Result\u0026lt;Option\u0026lt;Self::Item\u0026gt;,\u0026lt;RasTry\u0026gt;::Error\u0026gt;whereF: FnMut(\u0026amp;Self::Item)-\u0026gt; R,R: Try\u0026lt;Ok=bool\u0026gt;;fn position\u0026lt;P\u0026gt;(\u0026amp;mutself,predicate: P)-\u0026gt; Option\u0026lt;usize\u0026gt;whereP: FnMut(Self::Item)-\u0026gt; bool;fn rposition\u0026lt;P\u0026gt;(\u0026amp;mutself,predicate: P)-\u0026gt; Option\u0026lt;usize\u0026gt;whereSelf: ExactSizeIterator +DoubleEndedIterator,P: FnMut(Self::Item)-\u0026gt; bool;fn max(self)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereSelf::Item: Ord;fn min(self)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereSelf::Item: Ord;fn max_by_key\u0026lt;B,F\u0026gt;(self,f: F)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereF: FnMut(\u0026amp;Self::Item)-\u0026gt; B,B: Ord;fn max_by\u0026lt;F\u0026gt;(self,compare: F)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereF: FnMut(\u0026amp;Self::Item,\u0026amp;Self::Item)-\u0026gt; Ordering;fn min_by_key\u0026lt;B,F\u0026gt;(self,f: F)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereF: FnMut(\u0026amp;Self::Item)-\u0026gt; B,B: Ord;fn min_by\u0026lt;F\u0026gt;(self,compare: F)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;whereF: FnMut(\u0026amp;Self::Item,\u0026amp;Self::Item)-\u0026gt; Ordering;fn rev(self)-\u0026gt; Rev\u0026lt;Self\u0026gt;whereSelf: DoubleEndedIterator;fn unzip\u0026lt;A,B,FromA,FromB\u0026gt;(self)-\u0026gt; (FromA,FromB)whereSelf: Iterator\u0026lt;Item=(A,B)\u0026gt;,FromA: Default +Extend\u0026lt;A\u0026gt;,FromB: Default +Extend\u0026lt;B\u0026gt;;fn copied\u0026lt;\u0026#39;a,T\u0026gt;(self)-\u0026gt; Copied\u0026lt;Self\u0026gt;whereSelf: Iterator\u0026lt;Item=\u0026amp;\u0026#39;aT\u0026gt;,T: \u0026#39;a+Copy;fn cloned\u0026lt;\u0026#39;a,T\u0026gt;(self)-\u0026gt; Cloned\u0026lt;Self\u0026gt;whereSelf: Iterator\u0026lt;Item=\u0026amp;\u0026#39;aT\u0026gt;,T: \u0026#39;a+Clone;fn cycle(self)-\u0026gt; Cycle\u0026lt;Self\u0026gt;whereSelf: Clone;fn sum\u0026lt;S\u0026gt;(self)-\u0026gt; SwhereS: Sum\u0026lt;Self::Item\u0026gt;;fn product\u0026lt;P\u0026gt;(self)-\u0026gt; PwhereP: Product\u0026lt;Self::Item\u0026gt;;fn cmp\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; OrderingwhereI: IntoIterator\u0026lt;Item=Self::Item\u0026gt;,Self::Item: Ord;fn cmp_by\u0026lt;I,F\u0026gt;(self,other: I,cmp: F)-\u0026gt; OrderingwhereF: FnMut(Self::Item,\u0026lt;IasIntoIterator\u0026gt;::Item)-\u0026gt; Ordering,I: IntoIterator;fn partial_cmp\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; Option\u0026lt;Ordering\u0026gt;whereI: IntoIterator,Self::Item: PartialOrd\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn partial_cmp_by\u0026lt;I,F\u0026gt;(self,other: I,partial_cmp: F)-\u0026gt; Option\u0026lt;Ordering\u0026gt;whereF: FnMut(Self::Item,\u0026lt;IasIntoIterator\u0026gt;::Item)-\u0026gt; Option\u0026lt;Ordering\u0026gt;,I: IntoIterator;fn eq\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialEq\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn eq_by\u0026lt;I,F\u0026gt;(self,other: I,eq: F)-\u0026gt; bool whereF: FnMut(Self::Item,\u0026lt;IasIntoIterator\u0026gt;::Item)-\u0026gt; bool,I: IntoIterator;fn ne\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialEq\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn lt\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialOrd\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn le\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialOrd\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn gt\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialOrd\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn ge\u0026lt;I\u0026gt;(self,other: I)-\u0026gt; bool whereI: IntoIterator,Self::Item: PartialOrd\u0026lt;\u0026lt;IasIntoIterator\u0026gt;::Item\u0026gt;;fn is_sorted(self)-\u0026gt; bool whereSelf::Item: PartialOrd\u0026lt;Self::Item\u0026gt;;fn is_sorted_by\u0026lt;F\u0026gt;(self,compare: F)-\u0026gt; bool whereF: FnMut(\u0026amp;Self::Item,\u0026amp;Self::Item)-\u0026gt; Option\u0026lt;Ordering\u0026gt;;fn is_sorted_by_key\u0026lt;F,K\u0026gt;(self,f: F)-\u0026gt; bool whereF: FnMut(Self::Item)-\u0026gt; K,K: PartialOrd\u0026lt;K\u0026gt;;}Iterator\u0026lt;Item = T\u0026gt; 类型可以被迭代，并会产生 T 类型。没有 IteratorMut trait。每个 Iterator 实现可以通过 Item 关联类型指定它是返回不可变引用、可变引用还是拥有其值。\n   Vec\u0026lt;T\u0026gt; 方法 返回     .iter() Iterator\u0026lt;Item = \u0026amp;T\u0026gt;   .iter_mut() Iterator\u0026lt;Item = \u0026amp;mut T\u0026gt;   .into_iter() Iterator\u0026lt;Item = T\u0026gt;    对于初学者来说，有些东西不是很明显，但中级 Rustaceans 认为是理所当然的，那就是大多数类型都不是他们本身的迭代器。如果一个类型是可迭代的，我们几乎总是实现一些自定义的迭代器类型来迭代它，而不是试图让它自己迭代。\nstruct MyType{items: Vec\u0026lt;String\u0026gt;}implMyType{fn iter(\u0026amp;self)-\u0026gt; implIterator\u0026lt;Item=\u0026amp;String\u0026gt;{MyTypeIterator{index: 0,items: \u0026amp;self.items}}}struct MyTypeIterator\u0026lt;\u0026#39;a\u0026gt;{index: usize,items: \u0026amp;\u0026#39;aVec\u0026lt;String\u0026gt;}impl\u0026lt;\u0026#39;a\u0026gt;IteratorforMyTypeIterator\u0026lt;\u0026#39;a\u0026gt;{type Item=\u0026amp;\u0026#39;aString;fn next(\u0026amp;mutself)-\u0026gt; Option\u0026lt;Self::Item\u0026gt;{ifself.index\u0026gt;=self.items.len(){None}else{letitem=\u0026amp;self.items[self.index];self.index+=1;Some(item)}}}为了便于教学，上面的例子展示了如何从头开始实现一个 Iterator，但在这种情况下，习惯性的解决方案将只是遵从 Vec 的 iter 方法。\nstruct MyType{items: Vec\u0026lt;String\u0026gt;}implMyType{fn iter(\u0026amp;self)-\u0026gt; implIterator\u0026lt;Item=\u0026amp;String\u0026gt;{self.items.iter()}}另外，这也是一个很好的通用全面实现，要注意。\nimpl\u0026lt;I: Iterator +?Sized\u0026gt;Iteratorfor\u0026amp;mutI;它说任何对迭代器的可变引用也是迭代器，这一点很有用，因为它允许我们使用 self 接收器，就像使用 \u0026amp;mut self 接收器一样。了解这一点很有用，因为它允许我们使用迭代器方法与 self 接收器，就像它们有 \u0026amp;mut self 接收器一样。\n举个例子，想象一下，我们有一个函数，它可以处理一个超过三个项的迭代器，但是函数的第一步是取出迭代器的前三项，并在迭代剩下的项之前分别处理它们，下面是一个初学者可能尝试写这个函数的方法。\nfn example\u0026lt;I: Iterator\u0026lt;Item=i32\u0026gt;\u0026gt;(mutiter: I){letfirst3: Vec\u0026lt;i32\u0026gt;=iter.take(3).collect();foriteminiter{// ❌ iter consumed in line above // process remaining items }}嗯，这很烦人。take 方法有一个 self 接收器，所以我们似乎不能在不消耗整个迭代器的情况下调用它。下面是上面代码的一个天真的重构。\nfn example\u0026lt;I: Iterator\u0026lt;Item=i32\u0026gt;\u0026gt;(mutiter: I){letfirst3: Vec\u0026lt;i32\u0026gt;=vec![iter.next().unwrap(),iter.next().unwrap(),iter.next().unwrap(),];foriteminiter{// ✅ // process remaining items }}这还算好的。然而，惯用\u0008的重构其实是这样的。\nfn example\u0026lt;I: Iterator\u0026lt;Item=i32\u0026gt;\u0026gt;(mutiter: I){letfirst3: Vec\u0026lt;i32\u0026gt;=iter.by_ref().take(3).collect();foriteminiter{// ✅ // process remaining items }}不太容易发现。但无论如何，现在我们知道了。\n另外，对于什么可以是迭代器，什么不能是迭代器，并没有什么规则或约定。如果类型是 Iterator，那么它就是一个迭代器。标准库中的一些创造性的例子如下。\nusestd::sync::mpsc::channel;usestd::thread;fn paths_can_be_iterated(path: \u0026amp;Path){forpartinpath{// iterate over parts of a path }}fn receivers_can_be_iterated(){let(send,recv)=channel();thread::spawn(move||{send.send(1).unwrap();send.send(2).unwrap();send.send(3).unwrap();});forreceivedinrecv{// iterate over received values }}IntoIterator 预备知识\n Self Methods Associated Types Iterator  traitIntoIteratorwhere\u0026lt;Self::IntoIterasIterator\u0026gt;::Item==Self::Item,{type Item;type IntoIter: Iterator;fn into_iter(self)-\u0026gt; Self::IntoIter;}IntoIterator 类型可以被转换为迭代器，因此得名。当一个类型在 for-in 循环中使用时，会调用 into_iter 方法。\n// vec = Vec\u0026lt;T\u0026gt; forvinvec{}// v = T // above line desugared forvinvec.into_iter(){}不仅 Vec 实现了 IntoIterator，如果我们想分别迭代不可变引用或可变引用而不是拥有其值，\u0026amp;Vec 和 \u0026amp;mut Vec 也分别实现了 IntoIterator。\n// vec = Vec\u0026lt;T\u0026gt; forvin\u0026amp;vec{}// v = \u0026amp;T // above example desugared forvin(\u0026amp;vec).into_iter(){}// vec = Vec\u0026lt;T\u0026gt; forvin\u0026amp;mutvec{}// v = \u0026amp;mut T // above example desugared forvin(\u0026amp;mutvec).into_iter(){}FromIterator 预备知识\n Self Functions Generic Parameters Iterator IntoIterator  traitFromIterator\u0026lt;A\u0026gt;{fn from_iter\u0026lt;T\u0026gt;(iter: T)-\u0026gt; SelfwhereT: IntoIterator\u0026lt;Item=A\u0026gt;;}FromIterator 类型可以从迭代器中创建，因此也被称为 FromIterator。FromIterator 最常见和最习惯的用法是调用 Iterator 上的 collect 方法。\nfn collect\u0026lt;B\u0026gt;(self)-\u0026gt; BwhereB: FromIterator\u0026lt;Self::Item\u0026gt;;将 Iterator\u0026lt;Item = char\u0026gt; 收集成 String 的例子如下:\nfn filter_letters(string: \u0026amp;str)-\u0026gt; String {string.chars().filter(|c|c.is_alphabetic()).collect()}标准库中的所有集合都实现了 IntoIterator 和 FromIterator，这样可以更容易在它们之间进行转换。\nusestd::collections::{BTreeSet,HashMap,HashSet,LinkedList};// String -\u0026gt; HashSet\u0026lt;char\u0026gt; fn unique_chars(string: \u0026amp;str)-\u0026gt; HashSet\u0026lt;char\u0026gt;{string.chars().collect()}// Vec\u0026lt;T\u0026gt; -\u0026gt; BTreeSet\u0026lt;T\u0026gt; fn ordered_unique_items\u0026lt;T: Ord\u0026gt;(vec: Vec\u0026lt;T\u0026gt;)-\u0026gt; BTreeSet\u0026lt;T\u0026gt;{vec.into_iter().collect()}// HashMap\u0026lt;K, V\u0026gt; -\u0026gt; LinkedList\u0026lt;(K, V)\u0026gt; fn entry_list\u0026lt;K,V\u0026gt;(map: HashMap\u0026lt;K,V\u0026gt;)-\u0026gt; LinkedList\u0026lt;(K,V)\u0026gt;{map.into_iter().collect()}// and countless more possible examples I/O Traits Read \u0026amp; Write 预备知识\n Self Methods Scope Generic Blanket Impls  traitRead{fn read(\u0026amp;mutself,buf: \u0026amp;mut[u8])-\u0026gt; Result\u0026lt;usize\u0026gt;;// provided default impls fn read_vectored(\u0026amp;mutself,bufs: \u0026amp;mut[IoSliceMut\u0026lt;\u0026#39;_\u0026gt;])-\u0026gt; Result\u0026lt;usize\u0026gt;;fn is_read_vectored(\u0026amp;self)-\u0026gt; bool;unsafefn initializer(\u0026amp;self)-\u0026gt; Initializer;fn read_to_end(\u0026amp;mutself,buf: \u0026amp;mutVec\u0026lt;u8\u0026gt;)-\u0026gt; Result\u0026lt;usize\u0026gt;;fn read_to_string(\u0026amp;mutself,buf: \u0026amp;mutString)-\u0026gt; Result\u0026lt;usize\u0026gt;;fn read_exact(\u0026amp;mutself,buf: \u0026amp;mut[u8])-\u0026gt; Result\u0026lt;()\u0026gt;;fn by_ref(\u0026amp;mutself)-\u0026gt; \u0026amp;mutSelfwhereSelf: Sized;fn bytes(self)-\u0026gt; Bytes\u0026lt;Self\u0026gt;whereSelf: Sized;fn chain\u0026lt;R: Read\u0026gt;(self,next: R)-\u0026gt; Chain\u0026lt;Self,R\u0026gt;whereSelf: Sized;fn take(self,limit: u64)-\u0026gt; Take\u0026lt;Self\u0026gt;whereSelf: Sized;}traitWrite{fn write(\u0026amp;mutself,buf: \u0026amp;[u8])-\u0026gt; Result\u0026lt;usize\u0026gt;;fn flush(\u0026amp;mutself)-\u0026gt; Result\u0026lt;()\u0026gt;;// provided default impls fn write_vectored(\u0026amp;mutself,bufs: \u0026amp;[IoSlice\u0026lt;\u0026#39;_\u0026gt;])-\u0026gt; Result\u0026lt;usize\u0026gt;;fn is_write_vectored(\u0026amp;self)-\u0026gt; bool;fn write_all(\u0026amp;mutself,buf: \u0026amp;[u8])-\u0026gt; Result\u0026lt;()\u0026gt;;fn write_all_vectored(\u0026amp;mutself,bufs: \u0026amp;mut[IoSlice\u0026lt;\u0026#39;_\u0026gt;])-\u0026gt; Result\u0026lt;()\u0026gt;;fn write_fmt(\u0026amp;mutself,fmt: Arguments\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; Result\u0026lt;()\u0026gt;;fn by_ref(\u0026amp;mutself)-\u0026gt; \u0026amp;mutSelfwhereSelf: Sized;}通用的全面实现值得了解。\nimpl\u0026lt;R: Read+?Sized\u0026gt;Readfor\u0026amp;mutR;impl\u0026lt;W: Write+?Sized\u0026gt;Writefor\u0026amp;mutW;这说明任何对 Read 类型的可变引用也是 Read，对 Write 也是如此。了解这一点很有用，因为它允许我们使用任何带有 self 接收器的方法，就像它有一个 \u0026amp;mut self 接收器一样。我们已经在 Iterator trait 部分介绍了如何做到这一点以及为什么它很有用，所以我不打算在这里再次重复。\n我想指出，\u0026amp;[u8] 实现了 Read，Vec\u0026lt;u8\u0026gt; 实现了 Write，所以我们可以很容易地使用 String 对我们的文件处理函数进行单元测试，这些函数很容易转换为 \u0026amp;[u8] 和 Vec\u0026lt;u8\u0026gt;。\nusestd::path::Path;usestd::fs::File;usestd::io::Read;usestd::io::Write;usestd::io;// function we want to test fn uppercase\u0026lt;R: Read,W: Write\u0026gt;(mutread: R,mutwrite: W)-\u0026gt; Result\u0026lt;(),io::Error\u0026gt;{letmutbuffer=String::new();read.read_to_string(\u0026amp;mutbuffer)?;letuppercase=buffer.to_uppercase();write.write_all(uppercase.as_bytes())?;write.flush()?;Ok(())}// in actual program we\u0026#39;d pass Files fn example(in_path: \u0026amp;Path,out_path: \u0026amp;Path)-\u0026gt; Result\u0026lt;(),io::Error\u0026gt;{letin_file=File::open(in_path)?;letout_file=File::open(out_path)?;uppercase(in_file,out_file)}// however in unit tests we can use Strings! #[test]// ✅ fn example_test(){letin_file: String =\u0026#34;i am screaming\u0026#34;.into();letmutout_file: Vec\u0026lt;u8\u0026gt;=Vec::new();uppercase(in_file.as_bytes(),\u0026amp;mutout_file).unwrap();letout_result=String::from_utf8(out_file).unwrap();assert_eq!(out_result,\u0026#34;I AM SCREAMING\u0026#34;);}结论 我们一起学到了很多东西! 事实上，太多了。它现在是我们的了:\nArtist credit: The Jenkins Comic\n讨论 在这里讨论这篇文章\n Github learnrust subreddit official Rust users forum Twitter lobste.rs rust subreddit  通知 当下一篇博文发布时，会收到通知\n Following pretzelhammer on Twitter or Watching this repo\u0026rsquo;s releases (click Watch -\u0026gt; click Custom -\u0026gt; select Releases -\u0026gt; click Apply)  更多阅读  Sizedness in Rust Common Rust Lifetime Misconceptions Learning Rust in 2020 Learn Assembly with Entirely Too Many Brainfuck Compilers  ","permalink":"https://ohmyweekly.github.io/notes/2021-05-19-a-tour-of-rust-standard-library-traits/","tags":["Raku"],"title":"Rust 的标准库 Trait 之旅"},{"categories":["rustlang","kafka"],"contents":"这是一个两部分的系列，帮助你开始使用 Rust 和 Kafka。我们将使用 rust-rdkafka crate，它本身就是基于 librdkafka（C库）的。\n在这篇文章中，我们将介绍 Kafka Producer API。\n初始设置 确保你安装了一个 Kafka broker - 本地设置应该足够了。当然，你也需要安装Rust - 你需要1.45或以上版本。\n在你开始之前，先克隆 GitHub repo。\ngit clone https://github.com/abhirockzz/rust-kafka-101 cd part1 检查 Cargo.toml 文件:\n... [dependencies] rdkafka = { version = \u0026quot;0.25\u0026quot;, features = [\u0026quot;cmake-build\u0026quot;,\u0026quot;ssl\u0026quot;] } ... 关于 cmake-build 功能的说明 rust-rdkafka 提供了几种解决 librdkafka 依赖关系的方法。我选择了 static 链接，其中 librdkafka 被编译。不过你也可以选择 dynamic 链接来引用本地安装的版本。\n 更多内容，请参考以下链接\n 好吧，我们先从基本的开始说起。\n简单的生产者 这里是一个基于 BaseProducer 的简单生产者。\nletproducer: BaseProducer=ClientConfig::new().set(\u0026#34;bootstrap.servers\u0026#34;,\u0026#34;localhost:9092\u0026#34;).set(\u0026#34;security.protocol\u0026#34;,\u0026#34;SASL_SSL\u0026#34;).set(\u0026#34;sasl.mechanisms\u0026#34;,\u0026#34;PLAIN\u0026#34;).set(\u0026#34;sasl.username\u0026#34;,\u0026#34;\u0026lt;update\u0026gt;\u0026#34;).set(\u0026#34;sasl.password\u0026#34;,\u0026#34;\u0026lt;update\u0026gt;\u0026#34;).create().expect(\u0026#34;invalid producer config\u0026#34;);send 方法开始产生消息 - 它是在紧缩 loop 中完成的，中间有一个 thread::sleep(不是在生产中会做的事情)，以使其更容易追踪/跟踪结果。键、值（有效载荷）和目标 Kafka 主题以 BaseRecord 的形式表示。\nforiin1..100{println!(\u0026#34;sending message\u0026#34;);producer.send(BaseRecord::to(\u0026#34;rust\u0026#34;).key(\u0026amp;format!(\u0026#34;key-{}\u0026#34;,i)).payload(\u0026amp;format!(\u0026#34;value-{}\u0026#34;,i)),).expect(\u0026#34;failed to send message\u0026#34;);thread::sleep(Duration::from_secs(3));} 你可以在文件 src/1_producer_simple.rs 中查看整个代码。\n 要测试生产者是否在工作 \u0026hellip; 运行这段代码:\n 只需将文件 src/1_producer_simple.rs 重命名为 main.rs。 执行 cargo run  你应该看到这个输出:\nsending message sending message sending message ... 到底发生了什么？要弄清楚 - 使用 Kafka CLI 消费者（或其他消费者客户端，如 kafkacat）连接到你的 Kafka 主题（我在上面的例子中使用 rust 作为 Kafka 主题的名称）。你应该看到消息流进来了。\n例如\n\u0026amp;KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic rust --from-beginning 生产者回调 我们现在是在瞎飞! 除非我们明确地创建一个消费者来查看我们的消息，否则我们不知道它们是否被发送到 Kafka。让我们通过实现 ProducerContext(trait)来解决这个问题，以挂接到 produce 事件 - 它就像一个回调。\n首先为 ClientContext trait 创建一个结构体和一个空的实现（这是必须的）。\nstruct ProducerCallbackLogger;implClientContextforProducerCallbackLogger{}现在到了主要部分，我们在 ProducerContext trait 中实现 delivery 函数。\nimplProducerContextforProduceCallbackLogger{type DeliveryOpaque=();fn delivery(\u0026amp;self,delivery_result: \u0026amp;rdkafka::producer::DeliveryResult\u0026lt;\u0026#39;_\u0026gt;,_delivery_opaque: Self::DeliveryOpaque,){letdr=delivery_result.as_ref();matchdr{Ok(msg)=\u0026gt;{letkey: \u0026amp;str =msg.key_view().unwrap().unwrap();println!(\u0026#34;produced message with key {} in offset {} of partition {}\u0026#34;,key,msg.offset(),msg.partition())}Err(producer_err)=\u0026gt;{letkey: \u0026amp;str =producer_err.1.key_view().unwrap().unwrap();println!(\u0026#34;failed to produce message with key {} - {}\u0026#34;,key,producer_err.0,)}}}}我们根据 DeliveryResult(毕竟它是一个 Result)来匹配成功(Ok)和失败(Err)的情况。我们所做的只是简单地记录这两种情况下的消息，因为这只是一个例子。你可以在这里做任何你想做的事情（虽然不要太疯狂！）。\n 我们忽略了 ProducerContext trait 的关联类型 DeliveryOpaque。\n 我们需要确保我们插入了 ProducerContext 的实现。我们通过使用 create_with_context 方法（而不是 create）来实现，并确保为 BaseProducer 提供正确的类型。\nletproducer: BaseProducer\u0026lt;ProduceCallbackLogger\u0026gt;=ClientConfig::new().set(....)....create_with_context(ProduceCallbackLogger{})...如何调用 \u0026ldquo;回调\u0026rdquo;？ 好了，我们有了实现，但我们需要一种方法来触发它! 其中一个方法就是在生产者上调用 flush。所以，我们可以把我们的 producer 写成这样。\n 添加 producer.flush(Duration::from_secs(3));, 并 注释掉 sleep (just for now)  producer.send(BaseRecord::to(\u0026#34;rust\u0026#34;).key(\u0026amp;format!(\u0026#34;key-{}\u0026#34;,i)).payload(\u0026amp;format!(\u0026#34;value-{}\u0026#34;,i)),).expect(\u0026#34;failed to send message\u0026#34;);producer.flush(Duration::from_secs(3));println!(\u0026#34;flushed message\u0026#34;);//thread::sleep(Duration::from_secs(3)); 等等，我们可以做得更好 send 方法是非阻塞的（默认），但通过在每次 send 后调用 flush，我们现在已经将其转换为同步调用 - 从性能角度来看，不推荐使用。\n我们可以通过使用 ThreadedProducer 来改善这种情况。它负责在后台线程中调用 poll 方法，以确保发送回调通知的传递。这样做非常简单 - 只需将类型从 BaseProducer 改为 ThreadedProducer 即可!\n#before: BaseProducer\u0026lt;ProduceCallbackLogger\u0026gt;#after: ThreadedProducer\u0026lt;ProduceCallbackLogger\u0026gt;而且，我们也不需要再调用 flush 了。\n...//producer.flush(Duration::from_secs(3)); //println!(\u0026#34;flushed message\u0026#34;); thread::sleep(Duration::from_secs(3));... 代码在 src/2_threaded_producer.rs 中可以找到。\n 再次运行该程序  将文件 src/2_threaded_producer.rs 重命名为 main.rs，并且 执行 cargo run  输出:\nsending message sending message produced message with key key-1 in offset 6 of partition 2 produced message with key key-2 in offset 3 of partition 0 sending message produced message with key key-3 in offset 7 of partition 2 正如预期的那样，你应该能够看到生产者事件回调，表示消息确实被发送到了 Kafka 主题。当然，你可以直接连接到主题，并再次检查，就像之前一样。\n\u0026amp;KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic rust --from-beginning  要尝试失败的情况，请尝试使用一个不正确的主题名称，并注意 delivery 实现的 Err 变体是如何被调用的。\n 发送 JSON 消息 到目前为止，我们只是发送 String 作为 key 和 value。JSON 是一种常用的消息格式，让我们看看如何使用它。\n假设我们要发送 User 信息，将使用这个结构体来表示。\nstruct User{id: i32,email: String,}然后我们可以使用 serde_json 库将其序列化为 JSON。我们所需要的就是使用 serde 中的自定义派生函数 - Deserialize 和 Serialize。\nuseserde::{Deserialize,Serialize};#[derive(Serialize, Deserialize, Debug)]struct User{id: i32,email: String,}修改生产者循环。  创建一个 User 实例 使用 to_string_pretty 将其序列化为 JSON 字符串。 在有效载荷中加入这一点  ...letuser_json=serde_json::to_string_pretty(\u0026amp;user).expect(\u0026#34;json serialization failed\u0026#34;);producer.send(BaseRecord::to(\u0026#34;rust\u0026#34;).key(\u0026amp;format!(\u0026#34;user-{}\u0026#34;,i)).payload(\u0026amp;user_json),).expect(\u0026#34;failed to send message\u0026#34;);... 你也可以使用 to_vec(而不是 to_string())将其转换为一个字节的Vec(Vec\u0026lt;u8\u0026gt;)。\n 要运行该程序\u0026hellip;  将文件 src/3_JSON_payload.rs 重命名为 main.rs，然后 执行 cargo run  从主题中消费:\n\u0026amp;KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic rust --from-beginning 你应该看到带有 String 的键（如 user-34）和 JSON 值的消息。\n{ \u0026#34;id\u0026#34;: 34, \u0026#34;email\u0026#34;: \u0026#34;user-34@foobar.com\u0026#34; } 有更好的方法吗？ 是的！如果你习惯了 Kafka Java 客户端中的声明式序列化/去序列化方法（可能其他客户端也一样），你可能不喜欢这种 \u0026ldquo;显式\u0026rdquo; 方法。只是为了让大家明白，这是你在 Java 中的做法。\nProperties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;localhost:9092\u0026#34;); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \u0026#34;org.apache.kafka.common.serialization.StringSerializer\u0026#34;); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \u0026#34;io.confluent.kafka.serializers.json.KafkaJsonSchemaSerializer\u0026#34;); .... ProducerRecord\u0026lt;String, User\u0026gt; record = new ProducerRecord\u0026lt;String, User\u0026gt;(topic, key, user); producer.send(record);  请注意，您只需将 Producer 配置为使用 KafkaJsonSchemaSerializer，User 类就会被序列化为 JSON。\n rust-rdkafka 用 ToBytes trait 提供了类似的东西。下面是它的样子。\npubtraitToBytes{/// Converts the provided data to bytes. fn to_bytes(\u0026amp;self)-\u0026gt; \u0026amp;[u8];}不言而喻吧？String、Vec\u0026lt;u8\u0026gt; 等都有现有的实现。所以你可以使用这些类型作为键或值，而不需要任何额外的工作 - 这正是我们刚刚做的。但问题是我们的方法是 \u0026ldquo;显式\u0026rdquo; 的，即我们将 User 结构转换为 JSON 字符串，并将其传递出去。\n如果我们可以为 User 实现 ToBytes 呢？ implToBytesforUser{fn to_bytes(\u0026amp;self)-\u0026gt; \u0026amp;[u8]{letb=serde_json::to_vec_pretty(\u0026amp;self).expect(\u0026#34;json serialization failed\u0026#34;);b.as_slice()}}你会看到一个编译器错误。\ncannot return value referencing local variable `b` returns a value referencing data owned by the current function  更多的背景资料，请参考 GitHub 的问题。我很乐意看到其他可以与 ToBytes 一起工作的例子 - 如果你有这方面的意见，请在留言中留下。\n TL;DR是，最好坚持用 \u0026ldquo;显式\u0026rdquo; 的方式做事，除非你有一个 ToBytes 的实现，\u0026ldquo;不涉及分配，不能失败\u0026rdquo;。\n总结 第一部分就到这里。第二部分将涉及围绕 Kafka 消费者的话题。\n原文链接: https://dev.to/abhirockzz/getting-started-with-kafka-and-rust-part-1-4hkb\n","permalink":"https://ohmyweekly.github.io/notes/2021-04-14-getting-started-with-kafka-and-rust-part1/","tags":["Rust"],"title":"Kafka 和 Rust入门 - 第一部分"},{"categories":["rustlang","error"],"contents":"《Rust 编程语言》中的示例项目对于向新的潜在 Rustaceans 介绍 Rust 的不同方面和特性是非常好的。在这篇文章中，我们将通过扩展《Rust 编程语言》中的 minigrep 项目，看看实现更强大的错误处理基础架构的一些不同方法。\nminigrep 项目在第12章中介绍，它引导读者构建一个简单版本的 grep 命令行工具，这是一个用于搜索文本的工具。例如，你会传入一个查询，你要搜索的文本，以及文本所在的文件名，然后得到包含查询文本的所有行。\n这篇文章的目标是用更强大的错误处理模式来扩展本书的 minigrep 实现，这样你就能更好地了解 Rust 项目中处理错误的不同方法。\n作为参考，你可以在这里找到本书的 minigrep 版本的最终代码。\n错误处理用例 当涉及到 Rust 项目的结构时，一个常见的模式是有一个 \u0026ldquo;库\u0026rdquo; 的部分和一个 \u0026ldquo;应用\u0026rdquo; 的部分，前者是主要的数据结构、函数和逻辑，后者是将库函数联系在一起。\n你可以在原始 minigrep 代码的文件结构中看到这一点：应用逻辑存在于 src/bin/main.rs 文件中，它只是一个薄薄的包裹，包裹着在 src/lib.rs 文件中定义的数据结构和函数；主函数所做的就是调用 minigrep::run。\n这一点很重要，因为取决于我们是在构建一个应用程序还是一个库，会改变我们处理错误的方式。\n当涉及到一个应用程序时，最终用户很可能不想知道是什么原因导致了一个错误的琐碎细节。事实上，应用程序的最终用户可能只应该在错误无法恢复的情况下被通知错误。在这种情况下，提供关于为什么发生不可恢复的错误的细节也是有用的，特别是当它与用户输入有关时。如果某种可恢复的错误发生在后台，应用程序的消费者可能不需要知道它。\n相反，当涉及到一个库时，最终用户是其他开发人员，他们正在使用该库并在其之上构建一些东西。在这种情况下，我们希望尽可能多地提供关于我们的库中发生的任何错误的相关细节。然后，库的消费者将决定他们想要如何处理这些错误。\n那么，当我们的项目中既有库部分又有应用部分时，这两种方法是如何一起发挥作用的呢？main 函数执行 minigrep::run 函数，并输出结果中出现的任何错误。所以我们大部分的错误处理工作将集中在库部分。\n浮现库错误 在 src/lib.rs 中，我们有两个函数，Config::new 和 run，它们可能会返回错误。\nimplConfig{pubfn new(mutargs: env::Args)-\u0026gt; Result\u0026lt;Config,\u0026amp;\u0026#39;staticstr\u0026gt;{args.next();letquery=matchargs.next(){Some(arg)=\u0026gt;arg,None=\u0026gt;returnErr(\u0026#34;Didn\u0026#39;t get a query string\u0026#34;),};letfilename=matchargs.next(){Some(arg)=\u0026gt;arg,None=\u0026gt;returnErr(\u0026#34;Didn\u0026#39;t get a file name\u0026#34;),};letcase_sensitive=env::var(\u0026#34;CASE_INSENSITIVE\u0026#34;).is_err();Ok(Config{query,filename,case_sensitive,})}pubfn run(config: Config)-\u0026gt; Result\u0026lt;(),Box\u0026lt;dynError\u0026gt;\u0026gt;{letcontents=fs::read_to_string(config.filename)?;letresults=ifconfig.case_sensitive{search(\u0026amp;config.query,\u0026amp;contents)}else{search_case_insensitive(\u0026amp;config.query,\u0026amp;contents)};forlineinresults{println!(\u0026#34;{}\u0026#34;,line);}Ok(())}确切有三个地方在返回错误：两个错误发生在 Config::new 函数中，该函数返回一个 Result\u0026lt;Config，\u0026amp;'static str\u0026gt;。在这种情况下，Result 的错误变体是一个静态字符串切片。\n在这里，当用户没有提供查询时，我们会返回一个错误。\nletquery=matchargs.next(){Some(arg)=\u0026gt;arg,None=\u0026gt;returnErr(\u0026#34;Didn\u0026#39;t get a query string\u0026#34;),};这里，当用户没有提供文件名时，我们会返回一个错误。\nletfilename=matchargs.next(){Some(arg)=\u0026gt;arg,None=\u0026gt;returnErr(\u0026#34;Didn\u0026#39;t get a file name\u0026#34;),};以这种方式将错误结构化为静态字符串的主要问题是，错误信息并没有被放置在一个中心位置，如果需要的话，我们可以轻松地重构它们。这也使得我们更难在相同类型的错误之间保持错误信息的一致性。\n第三种错误发生在 run 函数的顶部，它返回一个 Result\u0026lt;(), Box\u0026lt;dyn Error\u0026gt;\u0026gt;。在这种情况下，错误变体是一个实现 Error trait 的 trait 对象。换句话说，这个函数的错误变体是实现 Error trait 的类型的任何实例。\n在这里，我们将调用 fs::read_to_string 时可能发生的任何错误冒出来。\nletcontents=fs::read_to_string(config.filename)?;这适用于调用 fs::read_to_string 时可能出现的错误，因为这个函数能够返回多种类型的错误。因此，我们需要一种方法来表示这些不同的可能的错误类型；它们之间的共同点是它们都实现了 Error trait！最终，我们要做的是定义所有这些错误类型。\n最终，我们要做的是在一个中心位置定义所有这些不同类型的错误，并让它们都成为单一类型的变体。\n在一个中心类型中定义错误变种 我们将创建一个新的 src/error.rs 文件，并定义一个枚举 AppError，并在此过程中派生出 Debug trait，以便我们在需要时可以得到一个调试表示。我们将为这个枚举的每一个变体命名，使它们恰当地代表三种类型的错误。\n#[derive(Debug)]pubenum AppError{MissingQuery,MissingFilename,ConfigLoad,}第三个变体，ConfigLoad，映射到 Config::run 函数中调用 fs::read_to_string 时可能出现的错误。乍一看，这似乎有点不妥，因为如果该函数出现错误，那就是在读取提供的配置文件时出现了某种I/O问题。那么我们为什么不把它命名为 IOError 或者类似的东西呢？\n在这种情况下，由于我们是将一个标准库函数的错误浮出水面，所以描述浮出水面的错误是如何影响它的，而不是简单地重申它，这与我们的应用更相关。当 fs::read_to_string 发生错误时，会阻止我们的 Config 加载，所以这就是为什么我们把它命名为 ConfigLoad。\n现在我们有了这个类型，我们需要更新代码中所有返回错误的地方以利用这个 AppError 枚举。\n返回 AppError 的变体 在我们的 src/lib.rs 文件的顶部，我们需要声明我们的错误模块，并将 error::AppError 带入作用域。\nmod error;useerror::AppError;在我们的 Config::new 函数中，我们需要更新我们作为错误返回静态字符串切片的地方，以及函数本身的返回类型。\n-pubfn new(mutargs: env::Args)-\u0026gt; Result\u0026lt;Config,\u0026amp;\u0026#39;staticstr\u0026gt;+pubfn new(mutargs: env::Args)-\u0026gt; Result\u0026lt;Config,AppError\u0026gt;// --snip-- letquery=matchargs.next(){Some(arg)=\u0026gt;arg,-None=\u0026gt;returnErr(\u0026#34;Didn\u0026#39;t get a query string\u0026#34;),+None=\u0026gt;returnErr(AppError::MissingQuery),};letfilename=matchargs.next(){Some(arg)=\u0026gt;arg,-None=\u0026gt;returnErr(\u0026#34;Didn\u0026#39;t get a file name\u0026#34;),+None=\u0026gt;returnErr(AppError::MissingFilename),};// --snip-- 运行函数中的第三个错误，只需要我们更新它的返回类型，因为 ? 操作符已经负责将错误冒出来，并在发生时返回。\n-pubfn run(config: Config)-\u0026gt; Result\u0026lt;(),Box\u0026lt;dynError\u0026gt;\u0026gt;+pubfn run(config: Config)-\u0026gt; Result\u0026lt;(),AppError\u0026gt;好了，现在我们正在使用我们的错误变体，一旦发生，这些错误变体将被浮现到我们的 main 函数中并打印出来。但是我们不再有之前定义的实际错误信息了！我们可以用 thiserror 注释错误变体。\n用 thiserror 注释错误变体 thiserror crate 是一个常用的工具，它提供了一种符合人体工程学的方式来格式化 Rust 库中的错误信息。\n它允许我们在 AppError 枚举中用我们希望显示给最终用户的实际错误信息来注解每个变体。\n让我们在 Cargo.toml 中添加它作为依赖。\n[dependencies] thiserror = \u0026#34;1\u0026#34; 在 src/error.rs 中，我们将把 thiserror::Error trait 带入作用域，并让我们的 AppError 类型派生它。我们需要派生这个 trait，以便用 #[error] 块来注解每个枚举变量。现在我们指定我们希望为每个特定变量显示的错误信息。\n+usestd::io;+usethiserror::Error;-#[derive(Debug)]+#[derive(Debug, Error)]pubenum AppError{+#[error(\u0026#34;Didn\u0026#39;t get a query string\u0026#34;)]MissingQuery,+#[error(\u0026#34;Didn\u0026#39;t get a file name\u0026#34;)]MissingFilename,+#[error(\u0026#34;Could not load config\u0026#34;)]ConfigLoad{+#[from]+source: io::Error,+}}ConfigLoad 变体中增加了什么额外的东西？由于 ConfigLoad 错误只有在调用 fs::read_to_string 出现底层错误时才会发生，所以 ConfigLoad 变体实际上做的是围绕底层I/O错误提供额外的上下文。\nthiserror 允许我们通过用 #[from] 来注解一个低级错误，以将源码转换为我们自制的错误类型，从而将其包裹在额外的上下文中。这样一来，当一个I/O错误发生时（比如我们指定了一个要搜索的文件，但实际上并不存在），我们就会得到这样一个错误。\nCould not load config: Os { code: 2, kind: NotFound, message: \u0026quot;No such file or directory\u0026quot; } 如果没有它，产生的错误信息看起来像这样。\nOs { code: 2, kind: NotFound, message: \u0026quot;No such file or directory\u0026quot; } 对于我们库的消费者来说，要想找出这个错误的来源是比较困难的，额外的上下文帮助很大。\n你可以在这里找到使用这个错误的 minigrep 版本。\n更加手动的方法 现在，我们将换个角度，看看如何在不将其作为依赖的情况下，实现与 thiserror 相同的结果。\n在引擎盖下，thiserror 用程序宏执行了一些魔法，这对编译速度有明显的影响。在 minigrep 的情况下，我们的错误变体很少，而且项目也很小，所以依赖 thiserror 并不会增加多少编译时间，但是在一个更大更复杂的项目中，这可能是一个考虑因素。\n所以在这一点上，我们将把这篇文章撕掉，换成我们自己的手动实现来结束这篇文章。走这条路的好处是，我们只需要修改 src/error.rs 文件就可以实现所有必要的改变（当然，除了从我们的 Cargo.toml 中删除 thiserror 之外）。\n[dependencies] - thiserror = \u0026#34;1\u0026#34; 让我们删除所有 thiserror 提供给我们的注释。我们还将用 std::error::Error trait 替换 thiserror::Error trait。\n-usethiserror::Error;+usestd::error::Error;-#[derive(Debug, Error)]+#[derive(Error)]pubenum AppError{-#[error(\u0026#34;Didn\u0026#39;t get a query string\u0026#34;)]MissingQuery,-#[error(\u0026#34;Didn\u0026#39;t get a file name\u0026#34;)]MissingFilename,-#[error(\u0026#34;Could not load config\u0026#34;)]ConfigLoad{-#[from]source: io::Error,}}为了恢复我们刚刚擦除的所有功能，我们需要做三件事。\n 为 AppError 实现 Display trait，这样我们的错误变体就可以显示给用户了。 为 AppError 实现 Error trait。这个 trait 代表了对错误类型的基本期望，即它们实现了 Display 和 Debug，再加上获取错误底层源或原因的能力。 为 AppError 实现 From\u0026lt;io::Error\u0026gt;。这是必要的，这样我们就可以将从 fs::read_to_string 返回的I/O错误转换为 AppError 的实例。  这里是我们对 AppError 的 Display trait 的实现。它将每个错误变量映射为一个字符串，并将其写入到 Display formatter 中。\nusestd::fmt;implfmt::DisplayforAppError{fn fmt(\u0026amp;self,f: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{matchself{Self::MissingQuery=\u0026gt;f.write_str(\u0026#34;Didn\u0026#39;t get a query string\u0026#34;),Self::MissingFilename=\u0026gt;f.write_str(\u0026#34;Didn\u0026#39;t get a file name\u0026#34;),Self::ConfigLoad{source}=\u0026gt;write!(f,\u0026#34;Could not load config: {}\u0026#34;,source),}}}而这就是我们对 Error trait 的实现。要实现的主要方法是 Error::source 方法，它的目的是提供错误源的信息。对于我们的 AppError 类型，只有 ConfigLoad 会暴露任何底层源信息，即调用 fs::read_to_string 可能发生的I/O错误。在其他错误变体的情况下，没有底层的源信息需要暴露。\nusestd::error;implerror::ErrorforAppError{fn source(\u0026amp;self)-\u0026gt; Option\u0026lt;\u0026amp;(dynError+\u0026#39;static)\u0026gt;{matchself{Self::ConfigLoad{source}=\u0026gt;Some(source),_=\u0026gt;None,}}}返回类型的 \u0026amp;(dyn Error + 'static') 部分类似于我们之前看到的 Box\u0026lt;dyn Error\u0026gt; trait 对象。这里的主要区别是，trait 对象是在一个不可变的引用后面，而不是 Box 指针。这里的 'static lifetime 意味着 trait 对象本身只包含拥有的值，也就是说，它内部不存储任何引用。这是必要的，以便让编译器确信这里没有悬空指针的机会。\n最后，我们需要一种将 io::Error 转换为 AppError 的方法。我们将通过为 AppError for From\u0026lt;io::error\u0026gt; 来实现。\nimplFrom\u0026lt;io::Error\u0026gt;forAppError{fn from(source: io::Error)-\u0026gt; Self{Self::ConfigLoad{source}}}这个没什么好说的。如果我们得到一个 io::Error，我们要做的就是将其转换为 AppError，并将其封装在 ConfigLoad 变体中。\n这就是全部了，伙计们 你可以在这里找到这个版本的 minigrep 实现。\n总结 最后，我们讨论了《Rust编程语言》一书中介绍的原始 minigrep 实现在错误处理方面是如何有点欠缺的，以及如何考虑不同的错误处理用例。\n从那里，我们展示了如何使用 thiserror crate 将所有可能的错误变体集中到一个类型中。\n最后，我们剥开了 thiserror 提供的外衣，展示了如何手动复制同样的功能。\n希望大家能从这篇文章中学到一些东西!\n原文链接: https://dev.to/seanchen1991/a-beginner-s-guide-to-handling-errors-in-rust-40k2\n","permalink":"https://ohmyweekly.github.io/notes/2021-04-14-a-beginner-guide-to-handling-errors-in-rust/","tags":["Rust"],"title":"Rust 中处理错误的初级指南"},{"categories":["rustlang"],"contents":"Sizedness in Rust Table of Contents\n Intro Sizedness Sized Trait Sized in Generics Unsized Types  Slices Trait Objects Trait Object Limitations  Cannot Cast Unsized Types to Trait Objects Cannot create Multi-Trait Objects   User-Defined Unsized Types   Zero-Sized Types  Unit Type User-Defined Unit Structs Never Type User-Defined Pseudo Never Types PhantomData   Conclusion Discuss Notifications Further Reading  Intro Sizedness是Rust中最重要的概念之一。它与其他语言特性有很多微妙的交集，只是以_\u0026ldquo;x在编译时不知道大小\u0026rdquo;_错误信息的形式出现，而这些错误信息是每个Rustacean都非常熟悉的。在这篇文章中，我们将探讨从大小类型，到无大小类型，再到零大小类型的各种风味，同时研究它们的用例、好处、痛点和变通方法。\n我使用的短语表，以及它们的含义。\n   Phrase Shorthand for     sizedness property of being sized or unsized   sized type type with a known size at compile time   1) unsized type or2) DST dynamically-sized type, i.e. size not known at compile time   ?sized type type that may or may not be sized   unsized coercion coercing a sized type into an unsized type   ZST zero-sized type, i.e. instances of the type are 0 bytes in size   width single unit of measurement of pointer width   1) thin pointer or2) single-width pointer pointer that is 1 width   1) fat pointer or2) double-width pointer pointer that is 2 widths   1) pointer or2) reference some pointer of some width, width will be clarified by context   slice double-width pointer to a dynamically sized view into some array    Sizedness 在 Rust 中，如果在编译时可以确定类型的字节大小，那么就可以确定类型的大小。确定一个类型的大小对于能够在栈上为该类型的实例分配足够的空间是很重要的。固定大小的类型可以通过值或引用来传递。如果一个类型的大小不能在编译时确定，那么它被称为不确定大小类型或 DST，动态大小类型。由于不确定大小类型不能被放置在栈上，它们只能通过引用来传递。下面是一些固定大小类型和不确定大小类型的例子。\nusestd::mem::size_of;fn main(){// primitives assert_eq!(4,size_of::\u0026lt;i32\u0026gt;());assert_eq!(8,size_of::\u0026lt;f64\u0026gt;());// tuples assert_eq!(8,size_of::\u0026lt;(i32,i32)\u0026gt;());// arrays assert_eq!(0,size_of::\u0026lt;[i32;0]\u0026gt;());assert_eq!(12,size_of::\u0026lt;[i32;3]\u0026gt;());struct Point{x: i32,y: i32,}// structs assert_eq!(8,size_of::\u0026lt;Point\u0026gt;());// enums assert_eq!(8,size_of::\u0026lt;Option\u0026lt;i32\u0026gt;\u0026gt;());// get pointer width, will be // 4 bytes wide on 32-bit targets or // 8 bytes wide on 64-bit targets constWIDTH: usize =size_of::\u0026lt;\u0026amp;()\u0026gt;();// pointers to sized types are 1 width assert_eq!(WIDTH,size_of::\u0026lt;\u0026amp;i32\u0026gt;());assert_eq!(WIDTH,size_of::\u0026lt;\u0026amp;muti32\u0026gt;());assert_eq!(WIDTH,size_of::\u0026lt;Box\u0026lt;i32\u0026gt;\u0026gt;());assert_eq!(WIDTH,size_of::\u0026lt;fn(i32)-\u0026gt; i32\u0026gt;());constDOUBLE_WIDTH: usize =2*WIDTH;// unsized struct struct Unsized{unsized_field: [i32],}// pointers to unsized types are 2 widths assert_eq!(DOUBLE_WIDTH,size_of::\u0026lt;\u0026amp;str\u0026gt;());// slice assert_eq!(DOUBLE_WIDTH,size_of::\u0026lt;\u0026amp;[i32]\u0026gt;());// slice assert_eq!(DOUBLE_WIDTH,size_of::\u0026lt;\u0026amp;dynToString\u0026gt;());// trait object assert_eq!(DOUBLE_WIDTH,size_of::\u0026lt;Box\u0026lt;dynToString\u0026gt;\u0026gt;());// trait object assert_eq!(DOUBLE_WIDTH,size_of::\u0026lt;\u0026amp;Unsized\u0026gt;());// user-defined unsized type // unsized types size_of::\u0026lt;str\u0026gt;();// compile error size_of::\u0026lt;[i32]\u0026gt;();// compile error size_of::\u0026lt;dynToString\u0026gt;();// compile error size_of::\u0026lt;Unsized\u0026gt;();// compile error }我们如何确定固定大小类型的大小是直截了当的：所有原生类型和指针都有已知的大小，所有的结构体、元组、枚举和数组只是由原生类型和指针或其他嵌套的结构体、元组、枚举和数组组成，因此我们只需考虑到填充和对齐所需的额外字节，递归地计数字节即可。我们无法确定不确定大小类型的大小，原因同样简单明了：切片可以有任意数量的元素在其中，因此在运行时可以是任意大小的，trait 对象可以由任意数量的结构或枚举实现，因此在运行时也可以是任意大小的。\n专业提示\n 在Rust中，视图到数组中的动态大小的指针被称为切片。例如 \u0026amp;str 是一个\u0026quot;字符串切片\u0026quot;, \u0026amp;[i32] 一个 \u0026ldquo;i32 切片\u0026rdquo;。 切片是双倍宽度的，因为它们存储了一个指向数组的指针和数组中元素的数量。 trait 对象指针是双宽度的，因为它们存储了一个指向数据的指针和一个指向 vtable 的指针。 不确定大小的结构体指针是双倍宽度的，因为它们存储了一个指向结构体数据的指针和结构体的大小。 不确定大小的结构体只能有1个不确定大小的字段，而且必须是结构体中的最后一个字段。  为了让大家真正明白关于不确定大小类型的双宽度指针的点，这里有一个比较数组和切片的注释代码示例。\nusestd::mem::size_of;constWIDTH: usize =size_of::\u0026lt;\u0026amp;()\u0026gt;();constDOUBLE_WIDTH: usize =2*WIDTH;fn main(){// data length stored in type // an [i32; 3] is an array of three i32s letnums: \u0026amp;[i32;3]=\u0026amp;[1,2,3];// single-width pointer assert_eq!(WIDTH,size_of::\u0026lt;\u0026amp;[i32;3]\u0026gt;());letmutsum=0;// can iterate over nums safely // Rust knows it\u0026#39;s exactly 3 elements fornuminnums{sum+=num;}assert_eq!(6,sum);// unsized coercion from [i32; 3] to [i32] // data length now stored in pointer letnums: \u0026amp;[i32]=\u0026amp;[1,2,3];// double-width pointer required to also store data length assert_eq!(DOUBLE_WIDTH,size_of::\u0026lt;\u0026amp;[i32]\u0026gt;());letmutsum=0;// can iterate over nums safely // Rust knows it\u0026#39;s exactly 3 elements fornuminnums{sum+=num;}assert_eq!(6,sum);}这里还有一个注释的代码例子，比较结构体和 trait 对象。\nusestd::mem::size_of;constWIDTH: usize =size_of::\u0026lt;\u0026amp;()\u0026gt;();constDOUBLE_WIDTH: usize =2*WIDTH;traitTrait{fn print(\u0026amp;self);}struct Struct;struct Struct2;implTraitforStruct{fn print(\u0026amp;self){println!(\u0026#34;struct\u0026#34;);}}implTraitforStruct2{fn print(\u0026amp;self){println!(\u0026#34;struct2\u0026#34;);}}fn print_struct(s: \u0026amp;Struct){// always prints \u0026#34;struct\u0026#34; // this is known at compile-time s.print();// single-width pointer assert_eq!(WIDTH,size_of::\u0026lt;\u0026amp;Struct\u0026gt;());}fn print_struct2(s2: \u0026amp;Struct2){// always prints \u0026#34;struct2\u0026#34; // this is known at compile-time s2.print();// single-width pointer assert_eq!(WIDTH,size_of::\u0026lt;\u0026amp;Struct2\u0026gt;());}fn print_trait(t: \u0026amp;dynTrait){// print \u0026#34;struct\u0026#34; or \u0026#34;struct2\u0026#34; ? // this is unknown at compile-time t.print();// Rust has to check the pointer at run-time // to figure out whether to use Struct\u0026#39;s // or Struct2\u0026#39;s implementation of \u0026#34;print\u0026#34; // so the pointer has to be double-width assert_eq!(DOUBLE_WIDTH,size_of::\u0026lt;\u0026amp;dynTrait\u0026gt;());}fn main(){// single-width pointer to data lets=\u0026amp;Struct;print_struct(s);// prints \u0026#34;struct\u0026#34; // single-width pointer to data lets2=\u0026amp;Struct2;print_struct2(s2);// prints \u0026#34;struct2\u0026#34; // unsized coercion from Struct to dyn Trait // double-width pointer to point to data AND Struct\u0026#39;s vtable lett: \u0026amp;dynTrait=\u0026amp;Struct;print_trait(t);// prints \u0026#34;struct\u0026#34; // unsized coercion from Struct2 to dyn Trait // double-width pointer to point to data AND Struct2\u0026#39;s vtable lett: \u0026amp;dynTrait=\u0026amp;Struct2;print_trait(t);// prints \u0026#34;struct2\u0026#34; }关键要点\n 只有固定大小类型的实例才能被放置在栈上，也就是说，可以通过值来传递 不确定大小类型的实例不能放在栈上，必须通过引用来传递 指向不确定大小类型的指针是双宽度的，因为除了指向数据外，它们还需要做额外的记账工作，以跟踪数据的长度或指向一个 vtable  Sized Trait Rust中的 \u0026ldquo;Sized\u0026rdquo; trait 是一个自动 trait 和一个标记 trait。\n自动 trait 是指当一个类型通过某些条件时，自动实现的 trait。标记 trait 是标记一个类型具有特定属性的 trait。标记 trait 没有任何 trait 项，如方法、关联函数、关联常量或关联类型。所有的自动 trait 都是标记 trait，但不是所有的标记 trait 都是自动 trait。自动 trait 必须是标记 trait，所以编译器可以为它们提供一个自动的缺省实现，如果 trait 有任何 trait 项，这是不可能的。\n如果一个类型的所有成员也是 \u0026ldquo;确定大小的\u0026rdquo;，那么它就会得到一个自动的 Sized 实现。\u0026ldquo;成员\u0026quot;的含义取决于所包含的类型，例如：结构体的字段、枚举的变体、数组的元素、元组的项等等。一旦一个类型被 \u0026ldquo;标记\u0026rdquo; 了一个 Sized 的实现，这意味着在编译时就知道它的字节大小。\n其他自动标记 trait 的例子是 Send 和 Sync trait。如果跨线程发送一个类型是安全的，那么这个类型就是可 Send 的。如果在线程之间共享该类型的引用是安全的，那么该类型就是可 Sync 的。如果一个类型的所有成员都是可 Send 和 Sync 的, 那么这个类型就会得到自动的 Send 和 Sync 实现。Sized 的特殊之处在于它不可能选择退出，不像其他自动标记 trait 可以选择退出。\n#![feature(negative_impls)]// this type is Sized, Send, and Sync struct Struct;// opt-out of Send trait impl!SendforStruct{}// opt-out of Sync trait impl!SyncforStruct{}impl!SizedforStruct{}// compile error 这似乎是合理的，因为我们可能有理由不希望我们的类型被跨线程发送或共享，但是很难想象我们会希望编译器 \u0026ldquo;忘记\u0026rdquo; 我们类型的大小，并将其视为一个不确定大小的类型，因为这不会带来任何好处，只会让类型更难处理。\n另外，说得迂腐一点，Sized 在技术上并不是一个自动 trait，因为它没有使用 auto 关键字来定义，但是编译器对它的特殊处理使它的行为与自动 trait 非常相似，所以在实践中，把它看作是一个自动 trait 是可以的。\n关键要点\n Sized 是一个自动标记 trait  泛型中的 Sized 每当我们编写任何泛型代码时，每一个泛型类型参数都会被默认的 Sized trait 自动绑定，这一点并不明显。\n// this generic function... fn func\u0026lt;T\u0026gt;(t: T){}// ...desugars to... fn func\u0026lt;T: Sized\u0026gt;(t: T){}// ...which we can opt-out of by explicitly setting ?Sized... fn func\u0026lt;T: ?Sized\u0026gt;(t: T){}// compile error // ...which doesn\u0026#39;t compile since t doesn\u0026#39;t have // a known size so we must put it behind a pointer... fn func\u0026lt;T: ?Sized\u0026gt;(t: \u0026amp;T){}// compiles fn func\u0026lt;T: ?Sized\u0026gt;(t: Box\u0026lt;T\u0026gt;){}// compiles 专业提示\n ?Sized can be pronounced \u0026ldquo;optionally sized\u0026rdquo; or \u0026ldquo;maybe sized\u0026rdquo; and adding it to a type parameter\u0026rsquo;s bounds allows the type to be sized or unsized ?Sized in general is referred to as a \u0026ldquo;widening bound\u0026rdquo; or a \u0026ldquo;relaxed bound\u0026rdquo; as it relaxes rather than constrains the type parameter ?Sized is the only relaxed bound in Rust  So why does this matter? Well, any time we\u0026rsquo;re working with a generic type and that type is behind a pointer we almost always want to opt-out of the default Sized bound to make our function more flexible in what argument types it will accept. Also, if we don\u0026rsquo;t opt-out of the default Sized bound we\u0026rsquo;ll eventually get some surprising and confusing compile error messages.\nLet me take you on the journey of the first generic function I ever wrote in Rust. I started learning Rust before the dbg! macro landed in stable so the only way to print debug values was to type out println!(\u0026quot;{:?}\u0026quot;, some_value); every time which is pretty tedious so I decided to write a debug helper function like this:\n ?Sized 可以读作 \u0026ldquo;optionally sized\u0026rdquo; 或 \u0026ldquo;maybe sized\u0026rdquo;，将它添加到类型参数的绑定中，可以让类型被确定大小或不确定大小。 ?Sized 一般被称为 \u0026ldquo;拓宽绑定\u0026rdquo; 或 \u0026ldquo;宽松绑定\u0026rdquo;，因为它放松而不是约束类型参数。 ?Sized 是 Rust 中唯一的宽松绑定。  那么为什么这很重要呢？任何时候，当我们在处理泛型类型，并且该类型在一个指针后面时，我们几乎总是希望选择退出默认的 Sized 绑定，以使我们的函数在接受什么参数类型时更加灵活。另外，如果我们不选择退出默认的 Sized 绑定，我们最终会得到一些令人惊讶和困惑的编译错误信息。\n让我带你了解一下我在 Rust 中写的第一个泛型函数的历程。在 dbg! 宏登陆稳定版之前，我就开始学习 Rust 了，所以打印调试值的唯一方法就是每次都要打出 println!(\u0026quot;{:?}\u0026quot;, some_value);，这是很乏味的，所以我决定写一个像这样的调试帮助函数。\nusestd::fmt::Debug;fn debug\u0026lt;T: Debug\u0026gt;(t: T){// T: Debug + Sized println!(\u0026#34;{:?}\u0026#34;,t);}fn main(){debug(\u0026#34;my str\u0026#34;);// T = \u0026amp;str, \u0026amp;str: Debug + Sized ✔️ }到目前为止还不错，但函数会对传递给它的任何值拥有所有权，这有点烦人，所以我把函数改为只接受引用。\nusestd::fmt::Debug;fn dbg\u0026lt;T: Debug\u0026gt;(t: \u0026amp;T){// T: Debug + Sized println!(\u0026#34;{:?}\u0026#34;,t);}fn main(){dbg(\u0026#34;my str\u0026#34;);// \u0026amp;T = \u0026amp;str, T = str, str: Debug + !Sized ❌ }现在出现了这个错误。\nerror[E0277]: thesizeforvaluesoftype `str`cannotbeknownatcompilationtime--\u0026gt; src/main.rs:8:9|3|fn dbg\u0026lt;T: Debug\u0026gt;(t: \u0026amp;T){|-requiredbythisboundin`dbg`...8|dbg(\u0026#34;my str\u0026#34;);|^^^^^^^^doesn\u0026#39;thaveasizeknownatcompile-time|=help: thetrait`std::marker::Sized`isnotimplementedfor`str`=note: tolearnmore,visit\u0026lt;https://doc.rust-lang.org/book/ch19-04-advanced-types.html#dynamically-sized-types-and-the-sized-trait\u0026gt; help: considerrelaxingtheimplicit`Sized`restriction|3|fn dbg\u0026lt;T: Debug+?Sized\u0026gt;(t: \u0026amp;T){|当我第一次看到这个问题时，我发现它令人难以置信的混乱。尽管我的函数对参数的限制比以前更严格，但现在它却莫名其妙地抛出了一个编译错误！这是怎么回事？到底发生了什么？\n我已经在上面的代码注释中破坏了答案，但基本上。Rust 在编译过程中把 T 解析为具体类型时，会执行模式匹配。这里有几个表格可以帮助澄清。\n   Type T \u0026amp;T     \u0026amp;str T = \u0026amp;str T = str       Type Sized     str ❌   \u0026amp;str ✔️   \u0026amp;\u0026amp;str ✔️    这也是为什么我不得不在改成取用引用后，加了一个 ?Sized 的绑定，使函数能正常工作。下面是可以工作的函数。\nusestd::fmt::Debug;fn debug\u0026lt;T: Debug+?Sized\u0026gt;(t: \u0026amp;T){// T: Debug + ?Sized println!(\u0026#34;{:?}\u0026#34;,t);}fn main(){debug(\u0026#34;my str\u0026#34;);// \u0026amp;T = \u0026amp;str, T = str, str: Debug + !Sized ✔️ }关键要点\n 所有的泛型类型参数默认都是自动绑定 Sized。 如果我们有一个泛型函数，它的参数是指针后面的一些 T，例如 \u0026amp;T、Box\u0026lt;T\u0026gt;、Rc\u0026lt;T\u0026gt; 等，那么我们几乎总是希望用T: ?Sized 来退出默认的 Sized 约束。  Unsized 类型 切片 最常见的切片是字符串切片 \u0026amp;str 和数组切片 \u0026amp;[T]。切片的好处是许多其他类型也会对其进行 coerce，所以利用切片和 Rust 的自动类型 coerce，我们可以编写灵活的 API。\n类型 coerce 可以发生在几个地方，但最明显的是在函数参数和方法调用时。我们感兴趣的类型 coerce 是 deref coerce 和 unsized coerce。deref coerce 是指当 T 在 deref 操作之后被 coerce 成一个 U，即 T: Deref\u0026lt;Target = U\u0026gt;，例如 String.deref() -\u0026gt; str。不确定大小 coerce 是指 T 被 coerce 成 U，其中 T 是一个确定大小的类型，U 是一个不确定大小的类型，即 T: Unsize\u0026lt;U\u0026gt;，例如 [i32; 3] -\u0026gt; [i32]。\ntraitTrait{fn method(\u0026amp;self){}}implTraitforstr{// can now call \u0026#34;method\u0026#34; on // 1) str or // 2) String since String: Deref\u0026lt;Target = str\u0026gt; }impl\u0026lt;T\u0026gt;Traitfor[T]{// can now call \u0026#34;method\u0026#34; on // 1) any \u0026amp;[T] // 2) any U where U: Deref\u0026lt;Target = [T]\u0026gt;, e.g. Vec\u0026lt;T\u0026gt; // 3) [T; N] for any N, since [T; N]: Unsize\u0026lt;[T]\u0026gt; }fn str_fun(s: \u0026amp;str){}fn slice_fun\u0026lt;T\u0026gt;(s: \u0026amp;[T]){}fn main(){letstr_slice: \u0026amp;str =\u0026#34;str slice\u0026#34;;letstring: String =\u0026#34;string\u0026#34;.to_owned();// function args str_fun(str_slice);str_fun(\u0026amp;string);// deref coercion // method calls str_slice.method();string.method();// deref coercion letslice: \u0026amp;[i32]=\u0026amp;[1];letthree_array: [i32;3]=[1,2,3];letfive_array: [i32;5]=[1,2,3,4,5];letvec: Vec\u0026lt;i32\u0026gt;=vec![1];// function args slice_fun(slice);slice_fun(\u0026amp;vec);// deref coercion slice_fun(\u0026amp;three_array);// unsized coercion slice_fun(\u0026amp;five_array);// unsized coercion // method calls slice.method();vec.method();// deref coercion three_array.method();// unsized coercion five_array.method();// unsized coercion }关键要点\n 利用切片和 Rust 的自动类型强制，我们可以编写灵活的 API。  Trait 对象 Traits 默认是 ?Sized 的。这个程序:\ntraitTrait: ?Sized{}抛出这个错误:\nerror: `?Trait`isnotpermittedinsupertraits--\u0026gt; src/main.rs:1:14|1|traitTrait: ?Sized{}|^^^^^^|=note: traitsare`?Sized`bydefault我们很快就会讨论为什么 trait 默认为 ?Sized，但首先让我们问问自己，一个 trait 被 ?Sized 的含义是什么？让我们把上面的例子去掉。\ntraitTraitwhereSelf: ?Sized{}好的，默认情况下，trait 允许 self 是一个不确定大小的类型。正如我们前面所学，我们不能通过值来传递不确定大小的类型，所以这限制了我们在 trait 中定义方法的种类。应该是不可能写出一个通过取值来获取或返回 self 的方法，然而这令人惊讶的是，它的编译:\ntraitTrait{fn method(self);// compiles }然而，当我们试图实现该方法时，无论是通过提供一个默认的实现，还是通过实现一个不确定大小类型的 trait，我们都会得到编译错误。\ntraitTrait{fn method(self){}// compile error }implTraitforstr{fn method(self){}// compile error }抛出:\nerror[E0277]: thesizeforvaluesoftype `Self`cannotbeknownatcompilationtime--\u0026gt; src/lib.rs:2:15|2|fn method(self){}|^^^^doesn\u0026#39;thaveasizeknownatcompile-time|=help: thetrait`std::marker::Sized`isnotimplementedfor`Self`=note: tolearnmore,visit\u0026lt;https://doc.rust-lang.org/book/ch19-04-advanced-types.html#dynamically-sized-types-and-the-sized-trait\u0026gt; =note: alllocalvariablesmusthaveastaticallyknownsize=help: unsizedlocalsaregatedasanunstablefeaturehelp: considerfurtherrestricting`Self`|2|fn method(self)whereSelf: std::marker::Sized{}|^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^error[E0277]: thesizeforvaluesoftype `str`cannotbeknownatcompilationtime--\u0026gt; src/lib.rs:6:15|6|fn method(self){}|^^^^doesn\u0026#39;thaveasizeknownatcompile-time|=help: thetrait`std::marker::Sized`isnotimplementedfor`str`=note: tolearnmore,visit\u0026lt;https://doc.rust-lang.org/book/ch19-04-advanced-types.html#dynamically-sized-types-and-the-sized-trait\u0026gt; =note: alllocalvariablesmusthaveastaticallyknownsize=help: unsizedlocalsaregatedasanunstablefeature如果我们决心通过值来传递 self，我们可以通过显式绑定 trait 与 Sized 来解决第一个错误。\ntraitTrait: Sized {fn method(self){}// compiles }implTraitforstr{// compile error fn method(self){}}现在抛出:\nerror[E0277]: thesizeforvaluesoftype `str`cannotbeknownatcompilationtime--\u0026gt; src/lib.rs:7:6|1|traitTrait: Sized {|-----requiredbythisboundin`Trait`...7|implTraitforstr{|^^^^^doesn\u0026#39;thaveasizeknownatcompile-time|=help: thetrait`std::marker::Sized`isnotimplementedfor`str`=note: tolearnmore,visit\u0026lt;https://doc.rust-lang.org/book/ch19-04-advanced-types.html#dynamically-sized-types-and-the-sized-trait\u0026gt; 这并没有问题，因为我们知道，当我们将 trait 与 Sized 绑定后，我们就不能再为诸如 str 这样的不确定大小类型实现它了。另一方面，如果我们真的想为 str 实现 trait，另一种解决方案是保留 ?Sized trait，并通过引用传递 self。\ntraitTrait{fn method(\u0026amp;self){}// compiles }implTraitforstr{fn method(\u0026amp;self){}// compiles }与其将整个 trait 标记为 ?Sized 或 Sized，我们有更细化和精确的选择，将单个方法标记为 Sized，像这样。\ntraitTrait{fn method(self)whereSelf: Sized {}}implTraitforstr{}// compiles!? fn main(){\u0026#34;str\u0026#34;.method();// compile error }令人惊讶的是，Rust编译 impl Trait for str {} 时没有任何抱怨，但当我们试图在一个不确定大小的类型上调用 method 时，它最终还是抓到了错误，所以一切正常。这有点怪异，但为我们提供了一些灵活性，只要我们从不调用 Sized 方法，我们就可以用一些 Sized 方法为不确定大小的类型实现 trait。\ntraitTrait{fn method(self)whereSelf: Sized {}fn method2(\u0026amp;self){}}implTraitforstr{}// compiles fn main(){// we never call \u0026#34;method\u0026#34; so no errors \u0026#34;str\u0026#34;.method2();// compiles }现在回到最初的问题，为什么 trait 默认是 ?Sized？答案是 trait 对象。trait 对象本质上是不确定大小的，因为任何大小的类型都可以实现 trait，因此我们只有在 Trait: ?Sized 的情况下，才能为 dyn Trait 实现 Trait。用代码来说：\ntraitTrait: ?Sized{}// the above is REQUIRED for implTraitfordynTrait{// compiler magic here }// since `dyn Trait` is unsized // and now we can use `dyn Trait` in our program fn function(t: \u0026amp;dynTrait){}// compiles 如果我们尝试实际编译上述程序，我们会得到：\nerror[E0371]: theobjecttype `(dynTrait+\u0026#39;static)`automaticallyimplementsthetrait`Trait`--\u0026gt; src/lib.rs:5:1|5|implTraitfordynTrait{|^^^^^^^^^^^^^^^^^^^^^^^^`(dynTrait+\u0026#39;static)`automaticallyimplementstrait`Trait`这就是编译器告诉我们要冷静，因为它自动为 dyn Trait 提供了 Trait 的实现。同样，由于 dyn Trait 是不确定大小的，编译器只能在 Trait: ?Sized 的情况下提供这个实现。如果我们将 Trait 与 Sized 绑定，那么 Trait 就变成了 \u0026ldquo;对象不安全\u0026rdquo; 的了，这意味着我们不能将实现 Trait 的类型转为 dyn Trait 的 trait 对象。正如预期的那样，这个程序不能编译:\ntraitTrait: Sized {}fn function(t: \u0026amp;dynTrait){}// compile error 抛出:\nerror[E0038]: thetrait`Trait`cannotbemadeintoanobject--\u0026gt; src/lib.rs:3:18|1|traitTrait: Sized {}|----------...becauseitrequires`Self: Sized`|||thistraitcannotbemadeintoanobject...2|3|fn function(t: \u0026amp;dynTrait){}|^^^^^^^^^^thetrait`Trait`cannotbemadeintoanobject让我们尝试用 Sized 方法制作一个 ?Sized trait，看看能否将它转一个 trait 对象。\ntraitTrait{fn method(self)whereSelf: Sized {}fn method2(\u0026amp;self){}}fn function(arg: \u0026amp;dynTrait){// compiles arg.method();// compile error arg.method2();// compiles }正如我们之前看到的那样，只要我们不调用 trait 对象上的 Sized 方法，一切都没问题。\n关键要点\n 所有的 traits 默认都是 ?Sized 的。 Trait: ?Sized 是 impl Trait for dyn Trait 所必需的。 我们可以在每个方法的基础上要求 Self: Sized。 由 Sized 绑定的 trait 不能成为 trait 对象。  trait 对象限制 即使一个 traitt 是对象安全的，也会有一些与大小相关的边缘情况，这些情况限制了哪些类型可以转换为 trait 对象，以及一个 trait 对象可以表示多少个和什么样的 trait。\n不能将不确定大小的类型转换为 Trait 对象 fn generic\u0026lt;T: ToString\u0026gt;(t: T){}fn trait_object(t: \u0026amp;dynToString){}fn main(){generic(String::from(\u0026#34;String\u0026#34;));// compiles generic(\u0026#34;str\u0026#34;);// compiles trait_object(\u0026amp;String::from(\u0026#34;String\u0026#34;));// compiles, unsized coercion trait_object(\u0026#34;str\u0026#34;);// compile error, unsized coercion impossible }抛出:\nerror[E0277]: thesizeforvaluesoftype `str`cannotbeknownatcompilationtime--\u0026gt; src/main.rs:8:18|8|trait_object(\u0026#34;str\u0026#34;);// compile error |^^^^^doesn\u0026#39;thaveasizeknownatcompile-time|=help: thetrait`std::marker::Sized`isnotimplementedfor`str`=note: tolearnmore,visit\u0026lt;https://doc.rust-lang.org/book/ch19-04-advanced-types.html#dynamically-sized-types-and-the-sized-trait\u0026gt; =note: requiredforthecasttotheobjecttype `dynstd::string::ToString`为什么将一个 \u0026amp;String 传给一个期望得到 \u0026amp;dyn ToString 的函数，是因为类型胁迫。String 实现了 ToString，我们可以通过不确定大小的胁迫将 String 这样的确定大小的类型转换成 dyn ToString 这样的不确定大小的类型。str 也实现了 ToString，将 str 转换为 dyn ToString 也需要一个不确定大小的胁迫，但 str 已经是不确定大小的了！我们如何将一个已经是不确定大小的类型，变成另一个不确定大小的类型？\n\u0026amp;str 指针是双宽的，存储一个数据指针和数据长度。\u0026amp;dyn ToString 指针也是双宽度的，存储一个指向数据的指针和一个指向 vtable 的指针。要把一个 \u0026amp;str 胁迫成一个 \u0026amp;dyn toString，就需要一个三倍宽度的指针来存储一个指向数据的指针、数据长度和一个指向 vtable 的指针。Rust 不支持三倍宽度指针，所以不可能将一个不确定大小的类型转换成一个 trait 对象。\n前面2段用表格总结了一下。\n   Type Pointer to Data Data Length Pointer to VTable Total Width     \u0026amp;String ✔️ ❌ ❌ 1 ✔️   \u0026amp;str ✔️ ✔️ ❌ 2 ✔️   \u0026amp;String as \u0026amp;dyn ToString ✔️ ❌ ✔️ 2 ✔️   \u0026amp;str as \u0026amp;dyn ToString ✔️ ✔️ ✔️ 3 ❌    不能创建 Multi-Trait 对象 traitTrait{}traitTrait2{}fn function(t: \u0026amp;(dynTrait+Trait2)){}抛出:\nerror[E0225]: onlyautotraitscanbeusedasadditionaltraitsinatraitobject--\u0026gt; src/lib.rs:4:30|4|fn function(t: \u0026amp;(dynTrait+Trait2)){}|-----^^^^^^|||||additionalnon-autotrait||traitaliasusedintraitobjecttype (additionaluse)|firstnon-autotrait|traitaliasusedintraitobjecttype (firstuse)请记住，trait 对象指针是双宽度的：存储1个指向数据的指针和另一个指向 vtable 的指针，但这里有2个trait，所以有2个 vtable，这就需要 \u0026amp;(dyn Trait + Trait2) 指针是3个宽度。像 Send 和 Sync 这样的自动 trait 是允许的，因为它们没有方法，因此没有 vtable。\n这方面的变通方法是通过使用另一个 trait 来组合 vtable，比如这样。\ntraitTrait{fn method(\u0026amp;self){}}traitTrait2{fn method2(\u0026amp;self){}}traitTrait3: Trait+Trait2{}// auto blanket impl Trait3 for any type that also impls Trait \u0026amp; Trait2 impl\u0026lt;T: Trait+Trait2\u0026gt;Trait3forT{}// from `dyn Trait + Trait2` to `dyn Trait3` fn function(t: \u0026amp;dynTrait3){t.method();// compiles t.method2();// compiles }这个变通方法的一个缺点是，Rust 不支持 supertrait 向上转换。这意味着，如果我们有一个 dyn Trait3，我们不能在需要 dyn Trait 或 dyn Trait2 的地方使用它。这个程序不能编译。\ntraitTrait{fn method(\u0026amp;self){}}traitTrait2{fn method2(\u0026amp;self){}}traitTrait3: Trait+Trait2{}impl\u0026lt;T: Trait+Trait2\u0026gt;Trait3forT{}struct Struct;implTraitforStruct{}implTrait2forStruct{}fn takes_trait(t: \u0026amp;dynTrait){}fn takes_trait2(t: \u0026amp;dynTrait2){}fn main(){lett: \u0026amp;dynTrait3=\u0026amp;Struct;takes_trait(t);// compile error takes_trait2(t);// compile error }抛出:\nerror[E0308]: mismatchedtypes--\u0026gt; src/main.rs:22:17|22|takes_trait(t);|^expectedtrait`Trait`,foundtrait`Trait3`|=note: expectedreference`\u0026amp;dynTrait`foundreference`\u0026amp;dynTrait3`error[E0308]: mismatchedtypes--\u0026gt; src/main.rs:23:18|23|takes_trait2(t);|^expectedtrait`Trait2`,foundtrait`Trait3`|=note: expectedreference`\u0026amp;dynTrait2`foundreference`\u0026amp;dynTrait3`这是因为 dyn Trait3 是一个不同于 dyn Trait 和 dyn Trait 的类型，因为它们有不同的 vtable 布局，尽管 dyn Trait3 确实包含 dyn Trait 和 dyn Trait2 的所有方法。这里的变通办法是增加显式转换方法。\ntraitTrait{}traitTrait2{}traitTrait3: Trait+Trait2{fn as_trait(\u0026amp;self)-\u0026gt; \u0026amp;dynTrait;fn as_trait2(\u0026amp;self)-\u0026gt; \u0026amp;dynTrait2;}impl\u0026lt;T: Trait+Trait2\u0026gt;Trait3forT{fn as_trait(\u0026amp;self)-\u0026gt; \u0026amp;dynTrait{self}fn as_trait2(\u0026amp;self)-\u0026gt; \u0026amp;dynTrait2{self}}struct Struct;implTraitforStruct{}implTrait2forStruct{}fn takes_trait(t: \u0026amp;dynTrait){}fn takes_trait2(t: \u0026amp;dynTrait2){}fn main(){lett: \u0026amp;dynTrait3=\u0026amp;Struct;takes_trait(t.as_trait());// compiles takes_trait2(t.as_trait2());// compiles }这是一个简单而直接的工作方法，似乎是 Rust 编译器可以为我们自动完成的事情。Rust 并不羞于执行类型胁迫，正如我们在 deref 和 unsized 胁迫中所看到的那样，那么为什么没有 trait 向上胁迫呢？这是一个很好的问题，有一个熟悉的答案：Rust核心团队正在研究其他更高优先级和更高影响的功能。很公平。\n关键要点\n Rust 不支持宽度超过2的指针，所以\u0026hellip;  我们不能将不确定大小的类型转换 trait 对象 我们不能有多个 trait 对象，但我们可以通过将多个 trait 强转成一个 trait 来解决这个问题。    用户自定义的不确定大小类型 struct Unsized{unsized_field: [i32],}我们可以通过赋予结构体一个不确定大小的字段来定义一个不确定大小的结构体。不确定大小的结构体只能有1个不确定大小的字段，而且它必须是结构体中的最后一个字段。这是一个要求，这样编译器就可以在编译时确定结构中每个字段的起始偏移量，这对高效快速的字段访问非常重要。此外，使用双宽度指针最多只能跟踪一个不确定大小的字段，因为更多的不确定大小的字段将需要更多的宽度。\n那么我们到底该如何实例化这个东西呢？和我们处理任何不确定大小类型的方式一样：先做一个可确定大小的版本，然后胁迫它变成不确定大小的版本。然而，Unsized 的定义总是不确定大小的，没有办法制作它的可确定大小版本！唯一的变通办法是使结构体通用化，使它可以存在于确定大小的版本中和不确定大小的版本中。\nstruct MaybeSized\u0026lt;T: ?Sized\u0026gt;{maybe_sized: T,}fn main(){// unsized coercion from MaybeSized\u0026lt;[i32; 3]\u0026gt; to MaybeSized\u0026lt;[i32]\u0026gt; letms: \u0026amp;MaybeSized\u0026lt;[i32]\u0026gt;=\u0026amp;MaybeSized{maybe_sized: [1,2,3]};}那么这有什么用处呢？没有什么特别引人注目的，用户定义的不确定大小的类型现在是一个非常半成品的功能，它们的局限性超过了任何好处。这里提到它们纯粹是为了全面性。\n有趣的事实： std::fi::OsStr 和 std::path::Path 是标准库中的2个不确定大小的结构，你可能已经在不知不觉中使用过了。\n关键 要点\n 用户定义的不确定大小类型现在是一个半成品的功能，它们的局限性超过了任何好处  Zero-Sized 类型 Zero-Sized 乍听起来很奇异，但到处都在使用。\nUnit 类型 最常见的零大小类型是 Unit 类型: (). 所有的空块 {} 都评估为 ()，如果块是非空的，但最后一个表达式用分号 ; 丢弃，那么它也评估为 ()。例子如下:\nfn main(){leta: ()={};letb: i32 ={5};letc: ()={5;};}每一个没有显式返回类型的函数都会默认返回 ()。\n// with sugar fn function(){}// desugared fn function()-\u0026gt; (){}由于 () 是零字节，所以 () 的所有实例都是一样的，这使得 Default、PartialEq 和 Ord 的实现非常简单。\nusestd::cmp::Ordering;implDefaultfor(){fn default(){}}implPartialEqfor(){fn eq(\u0026amp;self,_other: \u0026amp;())-\u0026gt; bool {true}fn ne(\u0026amp;self,_other: \u0026amp;())-\u0026gt; bool {false}}implOrdfor(){fn cmp(\u0026amp;self,_other: \u0026amp;())-\u0026gt; Ordering{Ordering::Equal}}编译器理解 () 是零大小的，并优化了与 () 实例的交互。例如，Vec\u0026lt;()\u0026gt; 永远不会进行任何堆分配，从 Vec 中推送和弹出 () 只是增加和减少它的 len 字段。\nfn main(){// zero capacity is all the capacity we need to \u0026#34;store\u0026#34; infinitely many () letmutvec: Vec\u0026lt;()\u0026gt;=Vec::with_capacity(0);// causes no heap allocations or vec capacity changes vec.push(());// len++ vec.push(());// len++ vec.push(());// len++ vec.pop();// len-- assert_eq!(2,vec.len());}上面的例子没有实际应用，但是有没有什么情况下，我们可以有意义地利用上面的想法呢？令人惊讶的是，是的，我们可以通过将 Value 设置为 ()，从 HashMap\u0026lt;Key，Value\u0026gt; 中得到一个高效的 HashSet\u0026lt;Key\u0026gt; 实现，这正是 Rust 标准库中 HashSet 的工作原理。\n// std::collections::HashSet pubstruct HashSet\u0026lt;T\u0026gt;{map: HashMap\u0026lt;T,()\u0026gt;,}关键要点\n ZST 的所有实例都是彼此相等的。 Rust 编译器知道优化与 ZSTs 的交互。  用户自定义的 Unit 结构体 Unit 结构体是指不含任何字段的结构体，如\nstruct Struct;属性，使 Unit 结构体比 () 更有用。\n 我们可以在自己的 Unit 结构体上实现任何我们想要的 trait，Rust 的 trait 孤儿规则阻止我们实现标准库中定义的 () 的 trait。 在我们的程序中，Unit 结构体可以被赋予有意义的名称。 Unit 结构体，就像所有结构体一样，默认情况下是不可复制的，这在我们的程序中可能很重要。  Never 类型 第二种最常见的 ZST 是 never 类型: !。 之所以称为 never 类型，是因为它代表的是永远不会解析到任何值的计算。\n! 的几个有趣的特性使它不同于 ()。\n ! 可以被胁迫成任何其他类型。 不可能创建 ! 的实例。  第一个有趣的属性对人体工程学非常有用，允许我们使用像这样的方便的宏。\n// nice for quick prototyping fn example\u0026lt;T\u0026gt;(t: \u0026amp;[T])-\u0026gt; Vec\u0026lt;T\u0026gt;{unimplemented!()// ! coerced to Vec\u0026lt;T\u0026gt; }fn example2()-\u0026gt; i32 {// we know this parse call will never fail match\u0026#34;123\u0026#34;.parse::\u0026lt;i32\u0026gt;(){Some(num)=\u0026gt;num,None=\u0026gt;unreachable!(),// ! coerced to i32 }}fn example3(some_condition: bool)-\u0026gt; \u0026amp;\u0026#39;staticstr{if!some_condition{panic!()// ! coerced to \u0026amp;str }else{\u0026#34;str\u0026#34;}}break, continue 和 return 表达式也拥有类型 !:\nfn example()-\u0026gt; i32 {// we can set the type of x to anything here // since the block never evaluates to any value letx: String ={return123// ! coerced to String };}fn example2(nums: \u0026amp;[i32])-\u0026gt; Vec\u0026lt;i32\u0026gt;{letmutfiltered=Vec::new();fornuminnums{filtered.push(if*num\u0026lt;0{break// ! coerced to i32 }elseif*num%2==0{*num}else{continue// ! coerced to i32 });}filtered}! 的第二个有趣的属性允许我们在类型层面上将某些状态标记为不可能。让我们以这个函数签名为例。\nfn function()-\u0026gt; Result\u0026lt;Success,Error\u0026gt;;我们知道，如果函数返回并成功，Result 将包含一些类型为 Success 的实例，如果函数出错，Result 将包含一些类型为 Error 的实例。现在我们来对比一下这个函数的签名。\nfn function()-\u0026gt; Result\u0026lt;Success,!\u0026gt;;我们知道，如果函数返回并且成功了，Result 将持有一些类型为 Success 的实例，如果出错了\u0026hellip;但等等，它永远不会出错，因为不可能创建 ! 的实例。鉴于上面的函数签名，我们知道这个函数永远不会出错。那这个函数签名呢:\nfn function()-\u0026gt; Result\u0026lt;!,Error\u0026gt;;前面的反义词现在是真的：如果这个函数返回，我们知道它肯定出错了，因为成功是不可能的。\n前一个例子的实际应用是 FromStr 对 String 的实现，因为将 \u0026amp;str 转换为 String 是不可能失败的。\n#![feature(never_type)]usestd::str::FromStr;implFromStrforString{type Err =!;fn from_str(s: \u0026amp;str)-\u0026gt; Result\u0026lt;String,Self::Err\u0026gt;{Ok(String::from(s))}}后一个例子的实际应用是一个运行无限循环的函数，这个函数永远不打算返回，就像服务器响应客户端的请求一样，除非有一些错误。\n#![feature(never_type)]fn run_server()-\u0026gt; Result\u0026lt;!,ConnectionError\u0026gt;{loop{let(request,response)=get_request()?;letresult=request.process();response.send(result);}}这个 feature 标记是必要的，因为当 never 类型存在并在 Rust 内部工作时，在用户代码中使用它仍然被认为是实验性的。\n要点\n ! 可以被胁迫成任何其他类型。 不可能创建 ! 的实例，我们可以用它来标记某些状态，在类型级别上是不可能的。  用户定义的伪 Never 类型 虽然不可能定义一个可以强制到任何其他类型的类型，但可以定义一个不可能创建实例的类型，比如一个 enum，没有任何变体。\nenum Void{}这使得我们可以从前面的2个例子中移除 feature 标记，并使用稳定的 Rust 实现它们。\nenum Void{}// example 1 implFromStrforString{type Err =Void;fn from_str(s: \u0026amp;str)-\u0026gt; Result\u0026lt;String,Self::Err\u0026gt;{Ok(String::from(s))}}// example 2 fn run_server()-\u0026gt; Result\u0026lt;Void,ConnectionError\u0026gt;{loop{let(request,response)=get_request()?;letresult=request.process();response.send(result);}}这是 Rust 标准库使用的技术，因为 String 的 FromStr 实现的 Err 类型是 std::convert::Infallible，它被定义为:\npubenum Infallible{}PhantomData 第三种最常用的 ZST 可能是 PhantomData。PhantomData 是一个零大小的标记结构，它可以用来 \u0026ldquo;标记\u0026rdquo; 一个包含的结构体具有某些属性。它和它的自动标记 trait 表亲如 Sized、Send、Sync 等在目的上是相似的，但作为一个标记结构体的使用方式有点不同。对 PhantomData 进行彻底的解释并探索它的所有用例不在本文的范围内，所以我们只简单地介绍一个简单的例子。回顾一下前面介绍的这个代码片段。\n#![feature(negative_impls)]// this type is Send and Sync struct Struct;// opt-out of Send trait impl!SendforStruct{}// opt-out of Sync trait impl!SyncforStruct{}很不幸，我们必须使用一个 feature 标记，我们是否可以只使用稳定的 Rust 来达到同样的结果？我们已经了解到，一个类型只有当它的所有成员也是 Send 和 Sync 时才是 Send 和 Sync 的，所以我们可以像 Rc\u0026lt;()\u0026gt; 一样在 Struct 中添加一个 !Send 和 !Sync 成员。\nusestd::rc::Rc;// this type is not Send or Sync struct Struct{// adds 8 bytes to every instance _not_send_or_sync: Rc\u0026lt;()\u0026gt;,}这不太理想，因为它增加了 Struct 的每个实例的大小，而且我们现在每次要创建一个 Struct 时，还得凭空想象出一个 Rc\u0026lt;()\u0026gt;。由于 PhantomData 是一个 ZST，它解决了这两个问题。\nusestd::rc::Rc;usestd::marker::PhantomData;type NotSendOrSyncPhantom=PhantomData\u0026lt;Rc\u0026lt;()\u0026gt;\u0026gt;;// this type is not Send or Sync struct Struct{// adds no additional size to instances _not_send_or_sync: NotSendOrSyncPhantom,}关键要点\n PhantomData 是一个零大小的标记结构，它可以用来 \u0026ldquo;标记\u0026rdquo; 一个包含的结构体具有某些属性。  结论  只有确定大小类型的实例才能被放置在栈上，也就是说，可以通过值来传递 不确定大小类型的实例不能放在栈上，必须通过引用来传递。 指向不确定大小类型的指针是双宽度的，因为除了指向数据外，它们还需要做额外的记账工作，以跟踪数据的长度或指向一个 vtable。 Sized 是一个 \u0026ldquo;自动\u0026rdquo; 标记 trait。 所有的泛型类型参数默认都是自动绑定 Sized 的。 如果我们有一个泛型函数，它的参数是指针后面的一些 T，例如 \u0026amp;T、Box\u0026lt;T\u0026gt;、Rc\u0026lt;T\u0026gt; 等，那么我们几乎总是希望用 T: ?Sized 来退出默认的 Sized 约束。 利用切片和 Rust 的自动类型强制，我们可以编写灵活的 API。 所有的 trait 默认为 Sized。 Trait: ?Sized 是 impl Trait for dyn Trait 所必需的。 我们可以根据每个方法要求 Self: Sized。 由 Sized  绑定的 trait 不能被制作成 trait 对象。 Rust 不支持宽度超过2的指针，所以\u0026hellip;  我们不能将不确定大小的类型转换为 trait 对象 我们不能有多 trait 对象，但我们可以通过将多个 trait 转化成一个 trait 来解决这个问题。   用户定义的不确定大小的类型现在是一个半成品的功能，它们的局限性超过了任何好处 ZST 的所有实例都是彼此相等的。 Rust 编译器知道优化与 ZSTs 的交互。 ! 可以被胁迫成任何其他类型。 不可能创建 ! 的实例，我们可以用它来标记某些状态，在类型级别上是不可能的。 PhantomData 是一个零大小的标记结构，它可以用来 \u0026ldquo;标记\u0026rdquo; 一个包含的结构体具有某些属性。  讨论 在这里讨论本文:\n official Rust users forum learnrust subreddit Twitter rust subreddit Github  通知 当发表下一篇博文时，会收到通知:\n Following pretzelhammer on Twitter or Watching this repo\u0026rsquo;s releases (click Watch -\u0026gt; click Custom -\u0026gt; select Releases -\u0026gt; click Apply)  更多阅读  Common Rust Lifetime Misconceptions Tour of Rust\u0026rsquo;s Standard Library Traits Learning Rust in 2020 Learn Assembly with Entirely Too Many Brainfuck Compilers  ","permalink":"https://ohmyweekly.github.io/notes/2021-04-11-sizedness-in-rust/","tags":["Rust","Sizedness"],"title":"Sizedness in Rust"},{"categories":["Rust"],"contents":"本帖文档调用站点依赖注入模式。这是一个相当低级的样本，和企业 DI 没有什么关系。这个模式有点 Rust 特有。\n通常，当你实现一个需要用户提供一些功能的类型时，首先想到的是在构造函数中提供它。\nstruct Engine{config: Config,...}implEngine{fn new(config: Config)-\u0026gt; Engine{...}fn go(\u0026amp;mutself){...}}在这个例子中，我们实现了 Engine，调用者提供了 Config。\n另一种方法是将依赖关系传递给每个方法调用。\nstruct Engine{...}implEngine{fn new()-\u0026gt; Engine{...}fn go(\u0026amp;mutself,config: \u0026amp;Config){...}}在 Rust 中，后者(call-site injection)有时用 lifetime 更好。让我们来看看这些例子吧!\nLazy 字段 在第一个例子中，我们想根据其他字段惰性地计算一个字段的值。就像这样:\nstruct Widget{name: String,name_hash: Lazy\u0026lt;u64\u0026gt;,}implWidget{fn new(name: String)-\u0026gt; Widget{Widget{name,name_hash: Lazy::new(||{compute_hash(\u0026amp;self.name)}),}}}这个设计的问题是在 Rust 中无法使用。Lazy 中的闭包需要访问 self，而这将创建一个自引用的数据结构!\n解决的办法是在使用 Lazy 的地方提供闭包。\nstruct Widget{name: String,name_hash: OnceCell\u0026lt;u64\u0026gt;,}implWidget{fn new(name: String)-\u0026gt; Widget{Widget{name,name_hash: OnceCell::new(),}}fn name_hash(\u0026amp;self)-\u0026gt; u64 {*self.name_hash.get_or_init(||{compute_hash(\u0026amp;self.name)})}}间接哈希表 下一个例子是关于将一个自定义的哈希函数插入到哈希表中。在 Rust 的标准库中，这只能在类型级别上实现，通过实现类型的 Hash 特性。更通用的设计是在运行时用哈希函数给表做参数。这是 C++ 所做的。然而在 Rust 中，这就不够通用了。\n考虑一个字符串互译器，它将字符串存储在一个向量中，并额外维护一个基于哈希的索引。\nstruct Interner{vec: Vec\u0026lt;String\u0026gt;,set: HashSet\u0026lt;usize\u0026gt;,}implInterner{fn intern(\u0026amp;mutself,s: \u0026amp;str)-\u0026gt; usize {...}fn lookup(\u0026amp;self,i: usize)-\u0026gt; \u0026amp;str {...}}set 字段将字符串存储在一个哈希表中，但它是用相邻 vec 的索引来表示它们。\n用一个闭包来构造 set 不会成功，原因和 Lazy 一样 - 这将创建一个自引用结构。在 C++ 中，存在一个变通的方法 - 可以将 vec 装箱，并在 Interner 和闭包之间共享一个稳定的指针。在 Rust 中，这会产生别名，阻止使用 \u0026amp;mut Vec。\n奇怪的是，在 std API 中，使用排序的 vec 而不是哈希是可行的。\nstruct Interner{vec: Vec\u0026lt;String\u0026gt;,// Invariant: sorted set: Vec\u0026lt;usize\u0026gt;,}implInterner{fn intern(\u0026amp;mutself,s: \u0026amp;str)-\u0026gt; usize {letidx=self.set.binary_search_by(|\u0026amp;idx|{self.vec[idx].cmp(s)});matchidx{Ok(idx)=\u0026gt;self.set[idx],Err(idx)=\u0026gt;{letres=self.vec.len();self.vec.push(s.to_string());self.set.insert(idx,res);res}}}fn lookup(\u0026amp;self,i: usize)-\u0026gt; \u0026amp;str {...}}这是因为闭包是在调用站点而不是在构造站点供给的。\nhashbrown crate 通过 RawEntry 为哈希提供了这种风格的 API。\nPer 容器分配器 第三个例子来自 Zig 编程语言。与 Rust 不同，Zig 没有一个祝福的全局分配器。相反，Zig 中的容器有两种风味。\u0026ldquo;Managed\u0026rdquo; 风味接受一个分配器作为构造参数，并将其存储为一个字段（Source）。而 \u0026ldquo;Unmanaged\u0026rdquo; 风味则在每个方法中添加一个分配器参数（Source）。\n第二种方式更节俭 - 可以用一个分配器引用与许多容器。\n胖指针 最后一个例子来自于 Rust 语言本身。为了实现动态调度，Rust 使用了胖指针，它有两个字宽。第一个字指向对象，第二个字指向 vtable。这些指针是在泛用具体类型的时候制造的。\n这与 C++ 不同，C++ 的 vtable 指针是在构造过程中嵌入到对象本身中的。\n看了这些例子后，我对 Scala 式的隐式参数很热衷。考虑一下这段带有 Zig 风格向量的 Rust 代码的假设。\n{letmuta=get_allocator();letmutxs=Vec::new();letmutys=Vec::new();xs.push(\u0026amp;muta,1);ys.push(\u0026amp;muta,2);}这里的问题是 Drop - 释放向量需要访问分配器，而如何提供一个分配器并不清楚。Zig 通过使用 defer 语句而不是 destructors 躲避了这个问题。在使用隐式参数的 Rust 中，我想下面的方法可以用。\nimpl\u0026lt;implicita: \u0026amp;mutAllocator,T\u0026gt;DropforVec\u0026lt;T\u0026gt;最后，我想分享最后一个例子，CSDI 思维帮助我发现了一个更好的应用级架构。\nrust-analyzer 的很多行为是可以配置的。有嵌套提示的切换，完成度可以调整，一些功能根据编辑器的不同而有不同的工作方式。第一个实现是将一个全局的 Config 结构和其他分析状态一起存储。然后各个子系统读取这个 Config 的位。为了避免通过这个共享结构将不同的功能耦合在一起，配置键是动态的。\ntype Config=HashMap\u0026lt;String,String\u0026gt;;这个系统是可行的，但感觉相当笨拙。\n现在的实现要简单得多。现在每个方法都接受一个特定的 config 参数，而不是将一个单一的 Config 作为状态的一部分来存储。\nfn get_completions(analysis: \u0026amp;Analysis,config: \u0026amp;CompletionConfig,file: FileId,offset: usize,)fn get_inlay_hints(analysis: \u0026amp;Analysis,config: \u0026amp;HintsConfig,file: FileId,)不仅代码更简单，而且更灵活。因为配置不再是状态的一部分，所以可以根据上下文的不同，对同一功能使用不同的配置。例如，显式调用的完成和异步的完成可能是不同的。\n在 /r/rust 上讨论。\n原文链接: https://matklad.github.io/2020/12/28/csdi.html\n","permalink":"https://ohmyweekly.github.io/notes/2021-04-05-call-site-dependency-injection/","tags":["Rust"],"title":"Call Site Dependency Injection"},{"categories":["rustlang"],"contents":"我一直在重读 Ted Kaminski 关于软件设计的博客。我强烈推荐所有的文章，尤其是早期的文章（这是第一篇）。他设法提供了既不平凡又合理的设计建议（当然是主观判断），这是一个难得的标本!\n无论如何，这一系列的见解之一是，当设计一个抽象的概念时，我们总是要面对权力和属性之间的内在权衡。我们使用一个特定的抽象能表达的越多，我们对使用它的代码能说的就越少。然而，我们人类对更多表达能力的偏爱并非与生俱来。这一点在编程语言社区中很明显，用户不停地要求提供新功能，而语言设计者却说不。\n宏是一个在 \u0026ldquo;更强大\u0026quot;方面走得很远的语言功能。宏给了你一种在源代码上抽象的能力。作为交换，你放弃了（自动）推理表面语法的能力。作为一个具体的例子，重命名重构在具有强大宏系统的语言中并不能 100% 可靠地工作。\n我确实认为，在理想的世界里，对于一个想要扩展到巨大项目的语言来说，这是一个错误的交易。当你增加了更多的程序员、更多的年限和更多的数百万行代码时，自动推理和转换源代码的能力就会变得越来越重要。但是，请谨慎对待这一点 - 我显然是有偏见的，因为我花了几年时间开发 Rust IDE。\n也就是说，宏有巨大的吸引力 - 它们是语言设计师的胶带。宏很少是最好的工具，但它们几乎可以完成任何工作。语言设计是渐进式的。宏系统通过为许多功能提供一个现成的穷人的替代品来缓解设计压力。\n在这篇文章中，我想探讨一下 Rust 中宏的用途。目的是为了找到不放弃\u0026quot;推理源代码\u0026quot;属性的解决方案。\n字符串插值 到目前为止，最常见的使用情况是 format! 系列的宏。这里的无宏解决方案很直接 - 字符串插值语法。\nletkey=\u0026#34;number\u0026#34;;letvalue=||92;lett=f\u0026#34;$key: ${values()}\u0026#34;;assert_eq!(t.to_string(),\u0026#34;number: 92\u0026#34;);在 Rust 中，插值可能不应该直接构造一个字符串，而是可以产生一个实现 Display 的值（就像 format_args! 一样），这样可以避免分配。一个有趣的扩展是允许在格式字符串片段上迭代。这样一来，插值语法就可以用于 SQL 语句或命令行参数之类的东西，而不用担心引入注入漏洞。\nletarg=\u0026#34;my dir\u0026#34;;letcmd=f\u0026#34;ls $arg\u0026#34;.to_cmd();assert_eq!(cmd.to_string(),\u0026#34;ls \u0026#39;my dir\u0026#39;\u0026#34;);这篇关于 Julia 编程语言的文章解释了这个问题。 xshell crate 为 Rust 实现了这个想法。\nDerives 我认为在 Rust 中，宏的第二个最常见，也可能是最重要的用法是派生。Rust 是为数不多的能正确实现平等的语言之一（禁止比较苹果和橘子），但这关键取决于 derive(Eq) 的能力。这个领域常见的解决方案是编译器中的特殊 casing（Haskell 的派生）或运行时反射。\n但我最感兴趣的解决方案是 C# 源码生成器。这并不是什么新鲜事 - 这只是老式的（源码）代码生成器，只是具有很好的实现质量。你可以提供自定义的代码，这些代码在构建过程中被运行，它可以读取现有的源码并生成额外的文件，然后再添加到编译中。\n这个解决方案的优点在于它将所有的复杂性从语言中移出，移到了编译系统中。这意味着你可以免费获得基线工具支持。生成代码的 Goto 定义？就能用了。调试时想介入一些序列化代码？磁盘上有实际的源码，所以可以放心的去做! 你是比较喜欢用 printf 的人？好吧，你需要说服构建系统不要踩过你的改动，但是，否则，为什么不呢？\n此外，源码生成器的表现力明显更强。它们可以调用到 Roslyn 编译器来分析源代码，所以它们能够生成类型导向的代码。\n为了有用，源码生成器需要一些语言级别的支持，以便将一个实体分割到多个文件中。在 C# 中，部分类就扮演了这个角色。\n特定领域语言 宏的存在理由是嵌入式 DSL 的实现。我们希望在语言中引入自定义语法，以简洁地对程序的领域进行建模。例如，可以用宏来嵌入 Rust 代码中的 HTML 片段。\n对我个人来说，eDSL 不是要解决的问题，只是一个问题。引入一个新的子语言（即使是小的）会花费大量的认知复杂性预算。如果你偶尔需要它，最好坚持只把有点啰嗦的函数调用链在一起。如果你经常需要它，引入外部的 DSL 是有意义的，它有一个编译器，一个语言服务器，以及所有使编程富有成效的工具。对我来说，基于宏的 DSL 只是在成本效益曲线上不落像一个有趣的点。\n也就是说，Kotlin 编程语言很好地解决了强类型化、工具友好型 DSL 的问题（例子）。令人气愤的是，很难指出具体的解决方案是什么。就是\u0026hellip;\u0026hellip;主要是具体的语法。下面是一些成分。\n 闭包的语法是 { arg -\u0026gt; body }，或者直接是 { body }，所以闭包在语法上类似于块。 扩展方法（这只是静态方法的语法糖）。 Java 风格的隐式 this，它将名称引入到作用域中，而不需要显式声明。 TCP-preserving inline closures (这是唯一一个非语法特征)  尽管如此，这还不足以实现 Jetpack Compose UI DSL，它还需要一个编译器插件。\nsqlx 我想调用的一个有趣的 DSL 案例是 sqlx::query。它允许我们写这样的代码。\nletaccount=sqlx::query!(\u0026#34;select (1) as id, \u0026#39;Herp Derpinson\u0026#39; as name\u0026#34;).fetch_one(\u0026amp;mutconn).await?;// anonymous struct has `#[derive(Debug)]` for convenience println!(\u0026#34;{:?}\u0026#34;,account);println!(\u0026#34;{}: {}\u0026#34;,account.id,account.name);这一点我想是eDSL确实很拉风的几个案例之一。没有宏的情况下，我不知道该怎么做。使用字符串插值（高级版本，以保护不被注入），可以指定查询。使用源码生成器，可以检查查询的语法和类型，例如，在这种情况下，会出现类型错误。\nlet(id,name): (i32,f32)=query(\u0026#34;select (1) as id, \u0026#39;Herp Derpinson\u0026#39; as name\u0026#34;).fetch_one(\u0026amp;mutconn).await?;但这还不足以生成一个匿名结构体，也不足以摆脱动态 casts。\n有条件编译 Rust 还使用宏进行条件编译。这个用例令人信服地展示了\u0026quot;缺乏属性\u0026quot;方面的能力。处理特征组合是 Cargo 永远头痛的问题。当特征标志改变时，用户不得不反复重新编译大块的装箱图。在 CI 上用 Cargo test --no-default-features 捕捉类型错误是非常恼人的，尤其是当你在提交 PR 之前确实运行了 Cargo test。\u0026ldquo;添加特性\u0026quot;是一个无法选中的一厢情愿。\n在这种情况下，我不知道有什么好的无宏选择。但是，原则上，这似乎是可行的，如果将条件编译进一步推到编译器流水线的下游，推到代码生成和链接阶段。编译器可以在为一个函数生成机器代码之前，选择特定平台的版本，而不是在解析过程中提前丢弃一些代码。在此之前，它会检查该函数的所有条件编译版本是否具有相同的接口。这样一来，平台特定的类型错误就不可能出现了。\n占位符语法 最后一个我想介绍的用例是占位符语法。Rust 的 macro_call!(...) 语法开辟了一个很好的隔离区域，只要小括号是平衡的，任何东西都可以用。理论上，这允许语言设计者在确定某些东西之前先试验临时语法。在实践中，这看起来好像并没有什么好处？有人反对稳定 postfix .await，而不通过中间期与 await! 宏来稳定。而且，稳定之后，所有的语法讨论都立即被遗忘了？另一方面，我们确实有 try! -\u0026gt; ? 转变，但我不认为它有助于发现任何设计上的缺陷？至少，我们成功地稳定了那个不必要的限制性去语法糖。\n对于结论，我想绕回源码生成器。究竟是什么让它们比宏更容易被工具化？我认为有以下三个特性。第一，无论是输入还是输出，从根本上说，都是文本。没有中间的表示方式（比如 token 树），而这个元程序设施使用的是中间的表示方式。这意味着，它不需要与编译器深度集成。当然，在内部，该工具可以自由地对代码进行任意解析、类型检查和转换。其次，有一个阶段性的区分。源码生成器是一次执行，无序的。在元编程和名称解析之间没有来回，这又可以将\u0026quot;元\u0026quot;的部分保留在外面。第三，源码生成器只能添加代码，不能改变现有代码的含义。这意味着，在代码生成器的存在下，语义上合理的源码转换依然如此。\n就这样吧! 在 /r/rust 上讨论。\n原文链接: https://matklad.github.io/2021/02/14/for-the-love-of-macros.html\n","permalink":"https://ohmyweekly.github.io/notes/2021-04-05-for-the-love-of-macros/","tags":["Rust"],"title":"For the Love of Macros"},{"categories":["rakulang"],"contents":"Raku 与 PostgreSQL 的连接性一览 在我看来，Raku 是一门伟大的语言，我每天都在使用它，而且越来越多。我可以说它将取代我的 Perl 脚本。\nRaku 有一个广泛的模块库，当然包括数据库连接，这反过来又包括连接 PostgreSQL 的功能。 在这篇简单的文章中，我将快速演示如何使用 Raku 的一段代码来完成许多比数据库应用程序还琐碎的任务。 脚本是以增量的方式呈现的，所以连接数据库部分必须始终作为脚本的前言。\nDB::Pg 模块在某种程度上与 Perl 5 的 DBD::Pg 很相似，所以很多概念和方法名都会让人想起后者。\n安装方法 可以使用 zef 来安装 DB::Pg 模块。\n% zef install DB::Pg 根据你的系统速度和已经安装的库，可能需要几分钟的时间。\n如果你要使用 LISTEN/NOTIFY，你需要同时安装 epoLl.NET 和 EPOLl.NET。\n% zef install epoll 连接到数据库 现在可以使用 DB::Pg 模块连接到数据库。例如，一个简单的脚本可以接受命令行上的所有参数（清晰的文本！），可以是：\n#!raku use DB::Pg; sub MAIN( Str :$host = \u0026#39;miguel\u0026#39;, Str :$username = \u0026#39;luca\u0026#39;, Str :$password = \u0026#39;secet\u0026#39;, Str :$database = \u0026#39;testdb\u0026#39; ) { \u0026#34;Connecting $username@ $host/$database\u0026#34;.say; my $connection = DB::Pg.new: conninfo =\u0026gt; \u0026#34;host=$hostuser=$usernamepassword=$passworddbname=$database\u0026#34;; 如你所见，DB::Pg模块接受一个 conninfo 字符串。\n读取查询和结果 .query 方法允许向数据库发出读取查询。结果是一个 Result 类对象，它可以通过不同的方法来使用，最著名的是 .hash 和 .arrays，它们返回一连串的 hash 或 arrays，从查询中提取的每一行都有一个 .rows 和 .column 等特殊方法分别提供了查询返回的行数和结果集的列名。\n举个例子，这里是一个简单的查询。\nmy $query = \u0026#39;SELECT current_role, current_time\u0026#39;; my $results = $connection.query: $query; say \u0026#34;The query { $query }returned { $results.rows }rows with columns: { $results.columns.join( \u0026#39;, \u0026#39; ) }\u0026#34;; for $results.hashes -\u0026gt; $row { for $row.kv -\u0026gt; $column, $value { say \u0026#34;Column $column= $value\u0026#34;; } } 上面这段代码提供了一个类似于下面的输出。\n查询 SELECT current_role, current_time 返回1行，列数为： current_role, current_time。\nColumn current_role = luca Column current_time = 14:48:47.147983+02 光标 默认情况下，.query 方法将从查询中获取所有的行，这对于较大的数据集来说是一个问题。可以使用 .cursor 方法，它可以接受可选的批量大小（默认为1000个元组），并可选地接受将结果获取为哈希序列的指定器。\n作为一个简单的例子。\nfor $connection.cursor( \u0026#39;select * from raku\u0026#39;, fetch =\u0026gt; 2, :hash ) -\u0026gt; %row { say \u0026#34;====================\u0026#34;; for %row.kv -\u0026gt; $column, $value { say \u0026#34;Column [ $column] = $value\u0026#34;; } say \u0026#34;====================\u0026#34;; } 产生和输出像这样的东西。\n==================== Column [ pk ] = 2 Column [ t ] = This is value 0 ==================== ==================== Column [ pk ] = 3 Column [ t ] = This is value 1 ==================== ==================== Column [ t ] = This is value 2 Column [ pk ] = 4 ==================== ==================== Column [ pk ] = 5 Column [ t ] = This is value 3 ==================== ... 撰写声明 编写语句可以通过 .execute 方法来执行，如:\n$connection.execute: q\u0026lt;insert into raku( t ) values( \u0026#39;Hello World\u0026#39; )\u0026gt;; 交易和编制报表 为了处理事务，你需要访问被\u0026quot;屏蔽\u0026quot;到 DB::Pg 主对象中的数据库处理程序。数据库对象像往常一样提供了 .begin、.rollback、.commit等方法。\n此外，还可以使用 .prepare 方法来获得一个已准备好的语句，该语句可以被缓存并用于循环和重复性任务中。值得注意的是，.prepare 方法使用了 $1、$2 等参数占位符，当语句接受单个值时，必须在 .execute 中不指定索引。\n举个例子\nmy $database-handler = $connection.db; my $statement = $database-handler.prepare: \u0026#39;insert into raku( t ) values( $1 )\u0026#39;; $database-handler.begin; $statement.execute( \u0026#34;This is value $_\u0026#34; ) for 0 .. 10; $database-handler.commit; $database-handler.finish; 上述循环相当于一个SQL事务，如:\nBEGIN;INSERTINTOraku(t)VALUES(\u0026#39;This is value 0\u0026#39;);INSERTINTOraku(t)VALUES(\u0026#39;This is value 1\u0026#39;);INSERTINTOraku(t)VALUES(\u0026#39;This is value 2\u0026#39;);...INSERTINTOraku(t)VALUES(\u0026#39;This is value 10\u0026#39;);COMMIT;.finish 方法是必需的，因为 DB::Pg 处理缓存。请注意，.commit 和 .rollback 方法是流畅的，并返回一个对象实例，这样你就可以调用 .commit.finish。\n数据库与连接 缓存的处理方式是，当发出一个查询时，会打开一个新的连接并使用。一旦工作完成，连接就会返回到内部池中。DB::Pg::Database 对象做的工作和 DB::Pg 的一样，不同的是它不会自动将连接返回到池中，所以需要自己进行 . 完成。\n因此，你可以在两个对象上使用相同的 .query 和 .execute 方法，但 DB::Pg 会自动将连接返回到内部池中，而数据库对象则允许你对何时将连接返回到池中进行细粒度的控制。\n复制 PostgreSQL 提供了特殊的 COPY 命令，可以用来复制从和进入。有一个方法 .copy-in 可以执行 COPY FROM，而 COPY TO 可以在迭代循环中使用。\nmy $file = \u0026#39;/tmp/raku.csv\u0026#39;.IO.open: :w; for $connection.query: \u0026#39;COPY raku TO stdout (FORMAT CSV)\u0026#39; -\u0026gt; $row { $file.print: $row; } 以上将 CSV 结果导出到文本文件上。 如果要读回数据，可以发出 .copy-in 方法，但首先需要发出 SQL COPY。工作流程是\nissue a COPY FROM STDIN; use .copy-data to slurp all the data; use .copy-end to notify the database that the COPY is concluded. 对.copy-end的需求是一个建议：可以在一次运行中发出不同的.copy-data，例如从不同文件中导入数据。\n$database-handler = $connection.db; $database-handler.query: \u0026#39;COPY raku FROM STDIN (FORMAT CSV)\u0026#39;; $database-handler.copy-data: \u0026#39;/tmp/raku1.csv\u0026#39;.IO.slurp; $database-handler.copy-data: \u0026#39;/tmp/raku2.csv\u0026#39;.IO.slurp; $database-handler.copy-end; 转换器 可以指定转换器，即处理进出数据库的值的特殊角色；这让我想起了 DBI::Class 的 inflate 和 deflate 选项。 第一步是在 DB::Pg 中给转换器实例添加一个角色，这样的实例必须。\n 增加一个新的类型转换方法。 增加一个转换方法来处理类型字符串化的值，并返回新值（在任何 Raku 实例中）。  作为一个例子，下面将一个文本 PostgreSQL 类型转换为一个 Str Raku 对象，并在其内容上进行反转。\n$connection.converter does role fluca-converter { submethod BUILD { self.add-type( text =\u0026gt; Str ) } multi method convert( Str:U, Str:D $value) { $value.flip.uc; } } .say for $connection.query( \u0026#39;select * from raku\u0026#39; ).arrays; 产生类似于的输出。\n[442 DLROW OLLEH] [454 DLROW OLLEH] [466 DLROW OLLEH] 其中字符串 Hello World 被翻转。\nlisten 和 notify DB::Pg也可以处理LISTEN和NOTIFY，它们能够与Raku的react动态功能进行交互。 首先，创建一个简单的机制来通知一些事件。\ntestdb=\u0026gt; create or replace rule r_raku_insert as on insert to raku do also SELECT pg_notify( \u0026#39;insert_event\u0026#39;, \u0026#39;INSERTING ROW(S)\u0026#39; ); CREATE RULE testdb=\u0026gt; create or replace rule r_raku_delete as on delete to raku do also SELECT pg_notify( \u0026#39;delete_event\u0026#39;, \u0026#39;DELETING ROW(S)\u0026#39; ); CREATE RULE 现在，可以创建一个等待传入事件的 Raku 脚本。\nreact { whenever $connection.listen( \u0026#39;delete_event\u0026#39; ) { .say; } whenever $connection.listen( \u0026#39;insert_event\u0026#39; ) { .say; } } 目的是，每次发出一个事件，.listen 都会将消息有效载荷传递给 react 代码块。因此，发出一些 DELETE 和 INSERT 会导致输出。\nDELETING ROW(S) INSERTING ROW(S) INSERTING ROW(S) 可以通过 .unlisten 方法停止监听反应块。也可以通过 .notify 方法发出事件。\n总结 DB::Pg 是 PostgreSQL 的一个很好的驱动程序，它允许 Raku 直接在语言中利用很多功能。\n文章 A glance at Raku connectivity towards PostgreSQL 已经由 Luca Ferrari 发布在博客上。\n原文链接: https://fluca1978.github.io/2021/03/29/RakuPostgreSQL.html\n","permalink":"https://ohmyweekly.github.io/notes/2021-04-05-psql/","tags":["Raku"],"title":"Psql"},{"categories":["rustlang"],"contents":"学习 std::io::Error 在这篇文章中，我们将剖析 Rust 标准库中 std::io::Error 类型的实现。相关代码在这里：library/std/src/io/error.rs。\n你可以把这篇文章看成是其中之一。\n 一个标准库的特定位的研究 一个高级错误管理指南 一个漂亮的 API 设计案例  文章要求基本熟悉 Rust 错误处理。\n在设计一个用于 Result\u0026lt;T，E\u0026gt; 的 Error 类型时，主要的问题是\u0026quot;如何使用这个错误？\u0026quot;。通常，以下情况之一为真。\n 错误被程序化处理。消费者检查错误，所以它的内部结构需要在合理的程度上暴露出来。 错误被传播并显示给用户。消费者不检查 fmt::Display 之外的错误；所以它的内部结构可以被封装。  请注意，暴露实现细节和封装细节之间存在紧张关系。实现第一种情况的常见反模式是定义一个厨房水槽枚举。\npubenum Error{Tokio(tokio::io::Error),ConnectionDiscovery{path: PathBuf,reason: String,stderr: String,},Deserialize{source: serde_json::Error,data: String,},...,Generic(String),}这种方法有很多问题。\n首先，从底层库中暴露错误会使它们成为你的公共 API 的一部分。在你的依赖关系中的主要 semver bump 会要求你也做一个新的主要版本。\n其次，它将所有的实现细节都固定下来。例如，如果你注意到 ConnectionDiscovery 的大小是巨大的，那么将这个变体装箱将是一个突破性的变化。\n第三，它通常表明了一个更大的设计问题。厨房水槽错误将不同的故障模式打包成一种类型。但是，如果故障模式差异很大，处理起来可能就不合理了! 这说明情况看起来更像案例二。\n错误厨房水槽病的一个经常有效的治疗方法是将错误推送给调用者的模式。\n考虑这个例子:\nfn my_function()-\u0026gt; Result\u0026lt;i32,MyError\u0026gt;{letthing=dep_function()?;...Ok(92)}my_function 调用 dep_function，所以 MyError 应该可以从 DepError 转换过来。更好的写法可能是这样的。\nfn my_function(thing: DepThing)-\u0026gt; Result\u0026lt;i32,MyError\u0026gt;{...Ok(92)}在这个版本中，调用者被迫调用 dep_function 并处理其错误。这就用更多的类型交换了更多的类型安全。MyError 和 DepError 现在是不同的类型，调用者可以分别处理它们。如果 DepError 是 MyError 的变体，那么就需要进行运行时匹配。\n这个想法的一个极端版本是 sans-io 编程。大多数错误来自于 IO；如果你把所有的 IO 推给调用者，你就可以跳过大部分的错误处理。\n无论枚举方法多么糟糕，它确实实现了第一种情况的最大可检查性。\n以传播为中心的第二种情况下的错误管理，通常是通过使用盒状特质对象来处理。像 Box\u0026lt;dyn std::error::Error\u0026gt; 这样的类型可以从任何具体的错误中构造出来，可以通过 Display 打印出来，并且仍然可以选择通过动态下传来暴露底层错误。Anyhow crate 就是这种风格的一个很好的例子。\nstd::io::Error 的例子很有趣，因为它想同时具备上述两种风格。\n 这是 std，所以封装和面向未来是最重要的。 来自操作系统的 IO 错误往往可以被处理（比如 EWOULDBLOCK）。 对于系统编程语言来说，准确地暴露底层 OS 错误是很重要的。 未来潜在的操作系统错误集是没有限制的。 io::Error 也是一种词汇类型，应该可以表示一些不完全的 os 错误。例如，Rust Paths 可以包含内部的0字节，打开这样的路径应该在进行 syscall 之前返回一个 io::Error。  下面是 std::io::Error 的样子。\npubstruct Error{repr: Repr,}enum Repr{Os(i32),Simple(ErrorKind),Custom(Box\u0026lt;Custom\u0026gt;),}struct Custom{kind: ErrorKind,error: Box\u0026lt;dynerror::Error+Send+Sync\u0026gt;,}首先要注意的是，它内部是一个枚举，但这是一个隐藏得很好的实现细节。为了允许检查和处理各种错误条件，有一个单独的公共无字段种类枚举。\n#[derive(Clone, Copy)]#[non_exhaustive]pubenum ErrorKind{NotFound,PermissionDenied,Interrupted,...Other,}implError{pubfn kind(\u0026amp;self)-\u0026gt; ErrorKind{match\u0026amp;self.repr{Repr::Os(code)=\u0026gt;sys::decode_error_kind(*code),Repr::Custom(c)=\u0026gt;c.kind,Repr::Simple(kind)=\u0026gt;*kind,}}}虽然 ErrorKind 和 Repr 都是枚举，但公开暴露 ErrorKind 就没那么可怕了。一个 #[non_exhaustive]Copy 无字段枚举的设计空间是一个点 - 没有合理的替代方案或兼容性隐患。\n有些 io::Errors 只是原始的操作系统错误代码。\nimplError{pubfn from_raw_os_error(code: i32)-\u0026gt; Error{Error{repr: Repr::Os(code)}}pubfn raw_os_error(\u0026amp;self)-\u0026gt; Option\u0026lt;i32\u0026gt;{matchself.repr{Repr::Os(i)=\u0026gt;Some(i),Repr::Custom(..)=\u0026gt;None,Repr::Simple(..)=\u0026gt;None,}}}特定平台的 sys::decode_error_kind 函数负责将错误代码映射到 ErrorKind 枚举。所有这些都意味着代码可以通过检查 .kind() 来跨平台处理错误类别。然而，如果需要以一种依赖于操作系统的方式处理一个非常特殊的错误代码，这也是可能的。API 小心翼翼地提供了一个方便的抽象，而没有抽象掉重要的低级细节。\n一个 std::io::Error 也可以从一个 ErrorKind 中构造出来。\nimplFrom\u0026lt;ErrorKind\u0026gt;forError{fn from(kind: ErrorKind)-\u0026gt; Error{Error{repr: Repr::Simple(kind)}}}这提供了跨平台访问错误代码风格的错误处理。如果你需要尽可能快的错误，这很方便。\n最后，还有第三种完全自定义的变体表示。\nimplError{pubfn new\u0026lt;E\u0026gt;(kind: ErrorKind,error: E)-\u0026gt; ErrorwhereE: Into\u0026lt;Box\u0026lt;dynerror::Error+Send+Sync\u0026gt;\u0026gt;,{Self::_new(kind,error.into())}fn _new(kind: ErrorKind,error: Box\u0026lt;dynerror::Error+Send+Sync\u0026gt;,)-\u0026gt; Error{Error{repr: Repr::Custom(Box::new(Custom{kind,error})),}}pubfn get_ref(\u0026amp;self,)-\u0026gt; Option\u0026lt;\u0026amp;(dynerror::Error+Send+Sync+\u0026#39;static)\u0026gt;{match\u0026amp;self.repr{Repr::Os(..)=\u0026gt;None,Repr::Simple(..)=\u0026gt;None,Repr::Custom(c)=\u0026gt;Some(\u0026amp;*c.error),}}pubfn into_inner(self,)-\u0026gt; Option\u0026lt;Box\u0026lt;dynerror::Error+Send+Sync\u0026gt;\u0026gt;{matchself.repr{Repr::Os(..)=\u0026gt;None,Repr::Simple(..)=\u0026gt;None,Repr::Custom(c)=\u0026gt;Some(c.error),}}}需要注意的地方。\n  通用的 new 函数委托给单态的 _new 函数。这改善了编译时间，因为在单态化过程中需要重复的代码更少。我认为这也改善了一些运行时：_new 函数没有被标记为内联，所以会在调用处产生一个函数调用。这是好的，因为错误构造是冷路径，节省指令缓存是受欢迎的。\n  自定义变体被框住了 - 这是为了让整体 size_of 更小。错误的 on-the-stack 大小是很重要的：即使没有错误，你也要为此付出代价!\n  这两种类型都是指\u0026quot;静态错误\u0026quot;。\ntype A=\u0026amp;(dynerror::Error+Send+Sync+\u0026#39;static);type B=Box\u0026lt;dynerror::Error+Send+Sync\u0026gt;在 dyn Trait + '_ 中，'_ 被省略为 'static，除非 trait 对象是在引用后面，在这种情况下，它被省略为 \u0026amp;'a dyn Trait + 'a。\nget_ref, get_mut 和 into_inner 提供了对底层错误的完全访问。类似于 os_error 的情况，抽象模糊了细节，但也提供了钩子来获取底层数据的原样。\n同样，Display 的实现揭示了内部表示的最重要细节。\nimplfmt::DisplayforError{fn fmt(\u0026amp;self,fmt: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{match\u0026amp;self.repr{Repr::Os(code)=\u0026gt;{letdetail=sys::os::error_string(*code);write!(fmt,\u0026#34;{} (os error {})\u0026#34;,detail,code)}Repr::Simple(kind)=\u0026gt;write!(fmt,\u0026#34;{}\u0026#34;,kind.as_str()),Repr::Custom(c)=\u0026gt;c.error.fmt(fmt),}}}综上所述，std::io::Error:\n 封装了它的内部表现形式，并通过框定大的枚举变体来优化它。 通过 ErrorKind 模式提供了一种方便的方法来处理基于类别的错误。 完全暴露底层操作系统的错误（如果有的话）。  可以透明地包裹任何其他错误类型。\n最后一点意味着 io::Error 可以用于临时错误，因为 \u0026amp;str 和 String 可以转换为 Box\u0026lt;dyn std::error::Error\u0026gt;。\nio::Error::new(io::ErrorKind::Other,\u0026#34;something went wrong\u0026#34;)它也可以作为 anyhow 的简单替换。我想一些库可能会用这个来简化他们的错误处理。\nio::Error::new(io::ErrorKind::InvalidData,my_specific_error)例如，serde_json 提供了以下方法。\nfn from_reader\u0026lt;R,T\u0026gt;(rdr: R)-\u0026gt; Result\u0026lt;T,serde_json::Error\u0026gt;whereR: Read,T: DeserializeOwned,读取会因为 io::Error 而失败，所以 serde_json::Error 需要能够在内部表示 io::Error。我认为这是倒退的 (但我不知道整个上下文，如果被证明是错的我会很高兴！)，签名应该是这样的。\nfn from_reader\u0026lt;R,T\u0026gt;(rdr: R)-\u0026gt; Result\u0026lt;T,io::Error\u0026gt;whereR: Read,T: DeserializeOwned,那么，serde_json::Error 就不会有 Io 的变体，而会以 InvalidData 的形式被藏到 io::Error 中。 补遗, 2021-01-25\n重新阅读这篇文章，我现在认为正确的返回类型应该是：\nfn from_reader\u0026lt;R,T\u0026gt;(rdr: R,)-\u0026gt; Result\u0026lt;Result\u0026lt;T,serde_json::Error\u0026gt;,io::Error\u0026gt;whereR: Read,T: DeserializeOwned,这迫使 IO 和反序列化错误分开处理，这在这种情况下是有意义的。IO 错误可能是程序领域之外的硬件/环境问题，而序列化错误很可能是系统中的某个错误。\n我认为 std::io::Error 是一个非常了不起的类型，它能够在没有太多妥协的情况下为许多不同的用例服务。但我们是否可以做得更好呢？\nstd::io::Error 的首要问题是，当一个文件系统操作失败时，你不知道它是为哪个路径失败的。这是可以理解的 - Rust 是一种系统语言，所以它不应该比 OS 原生提供的东西增加多少脂肪。OS 返回的是一个整数返回代码，而将其与一个堆分配的 PathBuf 耦合在一起，可能是一个不可接受的开销!\n我很惊讶地得知，事实上，std 对每一个与路径相关的系统调用都会进行分配。\n它需要以某种形式存在。OS API 需要在字符串的结尾有一个不幸的零字节. 但我想知道对短路径使用堆栈分配的缓冲区是否有意义。可能不会 - 路径通常不会那么短，而且现代分配器能有效地处理瞬时分配。\n我不知道这里有什么明显的好办法。一个选择是在编译时（一旦我们得到 std-aware cargo）或运行时（a-la RUST_BACKTRACE）添加开关，以堆分配所有与路径相关的 IO 错误。一个类似形的问题是 io::Error 不携带 backtrace。\n另一个问题是，std::io::Error 的效率不高。\n它的体积是相当大的。\nassert_eq!(size_of::\u0026lt;io::Error\u0026gt;(),2*size_of::\u0026lt;usize\u0026gt;());对于自定义的情况，会产生双重的间接和分配。\nenum Repr{Os(i32),Simple(ErrorKind),// First Box :| Custom(Box\u0026lt;Custom\u0026gt;),}struct Custom{kind: ErrorKind,// Second Box :( error: Box\u0026lt;dynerror::Error+Send+Sync\u0026gt;,}我想我们现在可以解决这个问题了\n首先，我们可以通过使用一个瘦的特质对象来摆脱双重内向性，比如失败或 anyhow。现在 GlobalAlloc 已经存在，这是一个比较直接的实现。\n其次，我们可以利用指针是对齐的这一事实，将 Os 和 Simple 变体都用最小的有效位集储藏到 usize 中。我认为我们甚至可以发挥创意，使用第二个最小有意义的位，把第一个位留作小众。这样一来，即使是像 io::Result\u0026lt;i32\u0026gt; 这样的东西，也可以是指针大小的!\n本篇文章到此结束。下一次你要为你的库设计一个错误类型的时候，花点时间去看看 std::io::Error 的源头，你可能会发现一些值得偷的东西。\n讨论在 /r/rust.Net 上进行。\n额外的谜题 看看实现中的这一行。\nimplfmt::DisplayforError{fn fmt(\u0026amp;self,fmt: \u0026amp;mutfmt::Formatter\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; fmt::Result{match\u0026amp;self.repr{Repr::Os(code)=\u0026gt;{letdetail=sys::os::error_string(*code);write!(fmt,\u0026#34;{} (os error {})\u0026#34;,detail,code)}Repr::Simple(kind)=\u0026gt;write!(fmt,\u0026#34;{}\u0026#34;,kind.as_str()),Repr::Custom(c)=\u0026gt;c.error.fmt(fmt),}}}原文链接: https://matklad.github.io/2020/10/15/study-of-std-io-error.html\n","permalink":"https://ohmyweekly.github.io/notes/2021-04-05-std-error-in-rust/","tags":["Rust"],"title":"Std Error in Rust"},{"categories":["rustlang"],"contents":"Two Beautiful Rust Programs 这是一则 Rust 编程语言的短广告，目标是有经验的 C++ 开发者。作为一则广告，它只能吊起你的胃口，具体内容请参考其他资源。\n第一个程序:\nfn main(){letmutxs=vec![1,2,3];letx: \u0026amp;i32 =\u0026amp;xs[0];xs.push(92);println!(\u0026#34;{}\u0026#34;,*x);}这个程序创建了一个 32 位整数的向量(std::vector\u0026lt;int32_t\u0026gt;)，接收第一个元素 x 的引用，再向向量推送一个数字，然后使用 x。这个程序是错误的：扩展向量可能会使对元素的引用无效，而且 *x 可能会取消引用一个 danging 指针。\n这个程序的好处是它不会被编译。\nerror[E0502]: cannot borrow xs as mutable because it is also borrowed as immutable --\u0026gt; src/main.rs:4:5 let x: \u0026amp;i32 = \u0026amp;xs[0]; -- immutable borrow occurs here xs.push(92); ^^^^^^^^^^^ mutable borrow occurs here println!(x); - immutable borrow later used here Rust 编译器跟踪每块数据的别名状态，并禁止潜在的别名数据的突变。在这个例子中，x 和 xs 别名了向量在堆中存储的第一个整数。\nRust 不允许做傻事。\n第二个程序:\nusecrossbeam::scope;useparking_lot::{Mutex,MutexGuard};fn main(){letmutcounter=Mutex::new(0);scope(|s|{for_in0..10{s.spawn(|_|{for_in0..10{letmutguard: MutexGuard\u0026lt;i32\u0026gt;=counter.lock();*guard+=1;}});}}).unwrap();lettotal: \u0026amp;muti32=counter.get_mut();println!(\u0026#34;total = {}\u0026#34;,*total)}这个程序创建一个由 mutex 保护的整数计数器，生成10个线程，从每个线程开始将计数器递增10次，并打印出总数。\n计数器变量位于堆栈中，这些堆栈数据的指针与其他线程共享。线程必须锁定 mutex 才能进行增量。打印总数时，绕过 mutex 读取计数器，没有任何同步。\n这个程序的妙处在于，它的正确性依赖于几位精妙的推理，每一个推理都会被编译器检查。\n子线程不会逃离主函数 所以可以从它的堆栈中读取计数器\n子线程只通过 mutex 访问 counter。\n子线程将在我们从计数器中读出总数而不使用 mutex 时终止。\n如果这些约束中的任何一个被破坏，编译器就会拒绝该代码。没有必要使用 std::shared_ptr 只是为了防御性地确保内存不会在你的脚下被释放。\nRust 允许做危险的、聪明的、快速的事情，而不用担心引入未定义的行为。\n如果你喜欢你所看到的，这里有两本我推荐的书，可以让你更深入地了解 Rust。\n原文链接: https://matklad.github.io/2020/07/15/two-beautiful-programs.html\n","permalink":"https://ohmyweekly.github.io/notes/2021-04-05-two-beautiful-rust-programs/","tags":["Rust"],"title":"Two Beautiful Rust Programs"},{"categories":["rakulang"],"contents":"我的天啊，看来我是两年多来第一次写 Raku 内部的博文了。当然，两年前还不叫 Raku。总之，话不多说，继续说说这个共同的脑洞吧。\n什么是调度？ 我用 \u0026ldquo;dispatch\u0026rdquo; 来表示我们接受一组参数，最后根据这些参数采取一些行动的过程。一些熟悉的例子包括:\n  进行一个方法调用，比如 $basket.add($product, $quantity)。传统上，我们可能只调用 $product 和 $qauntity 作为参数，但就我的目的而言，所有的 $basket、方法名 \u0026ldquo;add\u0026rdquo;、$product 和 $quantity 都是 dispatch 的参数：它们是我们需要的东西，以便决定我们要做什么。\n  进行子程序调用，如 uc($youtube-comment)。由于 Raku sub 调用是词法解析的，所以在这种情况下，调度的参数是 \u0026amp;uc（查找子程序的结果）和 $youtube-comment。\n  调用多个调度子程序或方法，根据参数的数量和类型来决定调用一组候选程序中的哪一个。这个过程可以看作是发生在上述两个调度中的一个 \u0026ldquo;内部\u0026rdquo;，因为我们在 Raku 中既有多重调度子程序，也有方法。\n  乍一看，也许前两个看起来相当简单，第三个就有点手忙脚乱了 - 这也算是事实。然而，Raku 还有一些其他的特性，使得调度变得相当，嗯，有趣。例如：\n  wrap 允许我们包装任何 Routine (sub 或方法); 包装器可以选择用原来的参数或新的参数来服从原来的例程。\n  当进行多重调度时，我们可以写一个原型例程，让它选择何时 - 甚至是否 - 调用合适的候选者。\n  我们可以使用 callsame 这样的例程，以便在调度中推迟到下一个候选者。但这意味着什么呢？如果我们是在一个多重调度中，它意味着下一个最适用的候选者，如果有的话。如果我们是在一个方法调度中，那么它意味着一个基类的方法。(同样的事情也被用来实现去下一个封装者，或者，最终也是去最初封装的例程)。而且这些都可以结合起来：我们可以包装一个 multi 方法，这意味着我们可以有 3 个层次的东西，都有可能贡献下一个要调用的东西!\n  多亏了这一点，dispatch - 至少在 Raku 中 - 并不总是我们所做的事情并产生一个结果，而是一个可能会被要求继续多次进行的过程!\n最后，虽然我上面所写的例子都可以很清楚地看成是调度的例子，但在 Raku 中，其他一些常见的构造也可以表达为一种调度。分配是一个例子：它的语义取决于分配的目标和被分配的值，因此我们需要选择正确的语义。强制类型转换(Coercion)是另一个例子，返回值类型检查又是一个例子。\n为什么调度很重要？ Dispatch 在我们的程序中无处不在，它悄悄地把想做事情的代码和做事情的代码联系在一起。它的无处不在意味着它在程序性能中扮演着重要的角色。在最好的情况下，我们可以将成本降为零。在最坏的情况下，调度的成本高到足以超过作为调度结果的工作的成本。\n初步估计，当运行时\u0026quot;理解\u0026quot;调度时，性能至少会有些不错，但当运行时不理解时，很有可能会很糟糕。调度往往涉及到一个可以缓存的工作量，往往会有一些廉价的防护措施来验证缓存结果的有效性。例如，在方法调度中，天真地我们需要走一个线性化的继承图，并询问沿途遇到的每个类是否有指定名称的方法。显然，如果我们在每次方法调用时都这样做，速度不会非常快。然而，特定类型上的特定方法名（精确识别，不考虑子类）每次都会解析到同一个方法。因此，我们可以缓存查找的结果，只要调用者的类型与用于产生缓存结果的类型相匹配，就可以使用它。\n语言运行时的专门化与通用化机制 当一个人开始构建一个针对特定语言的运行时，并且必须在相当紧张的预算下完成时，要想获得某种可容忍的性能，最明显的方法就是将各种热路径语言语义烘焙到运行时中。这正是 MoarVM 的起步方式。因此，如果我们看看 MoarVM 几年前的样子，我们会发现这样的事情。\n 对方法缓存的一些支持 一个与 Raku 的多重调度语义高度绑定的多重调度缓存，只有在调度都是名义类型的时候才真正能够起到帮助作用（所以使用 where 的代价非常高）。 一种机制，用于指定如何在封装代码对象内部找到实际的代码句柄（例如，Sub 对象有一个私有属性，它持有识别要运行的字节码的低级代码句柄）。 一些有限的尝试，让我们能够在知道一个调度不会继续的情况下正确地进行优化 - 这需要编译器和运行时之间的谨慎合作（或者不那么外交地讲，这都是一个大黑客）。  这些今天都还在，然而也都在淘汰的路上。这个榜单最能说明问题的是什么，不包括在内。比如：\n 私有方法调用，需要不同的缓存 但最初的虚拟机设计限制了每一种类型的调用 合格的方法调用($obj.SomeType::method-name()) 体面优化调度恢复的方法  几年前，我开始部分解决这个问题，引入了一种机制，我称之为 \u0026ldquo;specializer 插件\u0026rdquo;。但首先，什么是特化器(specializer)？\nMoarVM 刚开始的时候，它是一个比较简单的字节码解释器。它只需要足够快的速度击败 Parrot VM 就可以获得相当的使用量，我认为在继续实现一些更有趣的优化之前，这一点非常重要（当时我们还没有今天这样的发布前自动测试基础设施，因此更多的是依赖于早期采用者的反馈）。总之，在能够像其他后端一样运行 Raku 语言后不久，我就开始了动态优化器的开发。它在程序被解释时收集类型统计，识别热代码，将其放入 SSA 形式，使用类型统计插入防护，将这些与字节码的静态属性一起使用来分析和优化，并为相关函数生成专门的字节码。这个字节码可以省略类型检查和各种查找，也可以使用一系列的内部操作，做出各种假设，由于优化器证明了程序的属性，这些假设是安全的。这被称为专门化的字节码，因为它的很多通用性 - 这将使它能够正确地工作在我们可能遇到的所有类型的值上 - 被删除了，转而工作在运行时实际发生的特殊情况下。(代码，尤其是动态语言中的代码，一般来说，理论上的通用性远远大于实践中的通用性。)\n这个组件 - 内部称为 \u0026ldquo;spesh\u0026rdquo; 的 specializer - 为 Raku 程序的性能带来了显著的进一步提升，随着时间的推移，它的复杂程度也在不断提高，并采用了内联和带有标量替换的转义分析等优化功能。这些并不是容易构建的东西 - 但一旦运行时拥有了它们，它们就会创造出以前不存在的设计可能性，并使在没有它们的情况下做出的决定看起来是次优的。\n值得注意的是，那些特殊情况下的语言特定机制，在早期为了获得一些速度而被嵌入到运行时中，反而成为了一种负担和瓶颈。它们具有复杂的语义，这意味着它们对优化器来说要么是不透明的（所以优化器无法对它们进行推理，意味着优化受到抑制），要么就是需要在优化器中进行特殊的封装（一种负担）。\n所以，回到 specializer 插件。我到了一个地步，我想承担像 $obj.?meth(\u0026quot;call me maybe\u0026quot;, dispatch)、$obj.SomeType::meth()(用类开始寻找的调度限定)，以及角色中的私有方法调用(不能静态解析)这样的性能。同时，我还准备实现一定量的转义分析，但意识到它的作用将非常有限，因为赋值在虚拟机中也被特例化了，有一大块不透明的 C 代码在做热路径的事情。\n但为什么我们要让 C 代码来做那些热路径的事情呢？嗯，因为让每个赋值都调用一个虚拟机级别的函数，做一堆检查和逻辑，花费太大了。为什么这样做成本很高？因为函数调用的开销和解释的成本。这在以前都是正确的。但是，若干年后的发展。\n 内联被实现了，并且可以消除做一个函数调用的开销。 我们可以编译成机器代码，消除解释开销。 我们当时的处境是，我们手头有 specializer 的类型信息，可以让我们消除 C 代码中的分支，但由于我们调用的只是一个不透明的函数，所以没有办法抓住这个机会  我解决了上面提到的分配问题和调度问题，引入了一个新的机制：specializer 插件。它们的工作原理如下。\n 当我们第一次到达字节码中的一个给定的调用点时，我们就会运行这个插件。它产生了一个要调用的代码对象，以及一组守卫（为了使用该代码对象结果而必须满足的条件）。 下一次到达时，我们检查是否满足守卫，如果满足，就用结果 如果没有，我们再运行一次插件，并在 callsite 处堆积一个防护集。 我们统计了一个给定的防护集成功的频率，然后将其用于 specializer  绝大多数情况下都是单态的，这意味着只产生一组守卫，而且之后总是成功的。因此，特殊化器可以将这些守卫编译到专门的字节码中，然后假设给定的目标调用者就是将被调用的守卫。(进一步，重复的守卫可以被消除，所以某个插件引入的守卫可能会减少到零)。\nSpecializer 插件感觉挺好的。一个新机制解决了多个优化头疼的问题。\n新的 MoarVM 调度机制是对一个相当简单的问题的回答：如果我们把所有与调度相关的特例机制去掉，而采用有点像 specializer 插件的机制，会怎么样？由此产生的机制需要是一个比 specializer 插件更强大的机制。进一步说，我可以学习特殊器插件的一些缺点。因此，虽然它们会在比较短的寿命后消失，但我认为可以说，如果没有这些经验，我就不会有能力设计新的 MoarVM 调度机制。\n调度操作和引导调度器 所有的方法缓存。所有的多重调度缓存。所有的 specializer 插件。所有用于在代码对象中解包字节码句柄的调用协议的东西。这一切都将被取消，取而代之的是一个新的调度指令。它的名字很无聊，叫 dispatch。它看起来像这样。\ndispatch_o result, \u0026#39;dispatcher-name\u0026#39;, callsite, arg0, arg1, ..., argN 这意味着：\n 使用名为 dispatcher-name 的调度器。 给它指定的参数寄存器（所引用的调用点表示参数的数量）。 将调度的对象结果放入寄存器结果中。  (旁白：这意味着一个新的调用约定，即我们不再将参数复制到参数缓冲区，而是将寄存器集的基数和一个指针传递到找到寄存器参数映射的字节码中，然后做一个查询 registers[map[argument_index]] 来获取一个参数的值。仅此一点，我们在解释时就很省事，因为我们不再需要每个参数绕着解释器循环了）。)\n有些参数可能是我们传统上称之为参数的东西。有些则是针对调度过程本身。这其实并不重要 - 但如果我们安排将只针对调度的参数放在前面（例如，方法名），而将针对调度目标的参数放在后面（例如，方法参数），则会更加理想。\n新的 bootstrap 机制提供了少量的内置调度器，它们的名字以 \u0026ldquo;boot-\u0026rdquo; 开头。它们是：\n boot-value - 取第一个参数并将其作为结果（身份函数，除了丢弃任何其他参数）。 boot-constant - 取第一个参数并将其作为结果，但同时也将其视为一个将始终产生的常量值（因此意味着优化器可以将任何用于计算该值的纯代码视为死值）。 boot-code - 取第一个参数（必须是虚拟机字节码句柄），并运行该字节码，将其余参数作为参数传给它；评估为字节码的返回值。 boot-syscall - 将第一个参数视为虚拟机提供的内置操作的名称，然后调用它，并将其余参数作为其参数。 boot-resume - 恢复正在进行的最上层调度。  差不多就是这样。我们构建的每一个调度器，为了教给运行时一些其他的调度行为，最终都会终止于其中一个。\n在引导程序的基础上 教 MoarVM 了解不同种类的调度，不外乎使用调度机制本身! 在大多数情况下，boot-syscall 被用来注册一个调度器，设置守卫，并提供与它们相匹配的结果。\n这里是一个最小的例子，取自 dispatcher 测试套件，展示了一个提供同一性功能的 dispatcher 的样子。\nnqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-register\u0026#39;, \u0026#39;identity\u0026#39;, -\u0026gt; $capture { nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;boot-value\u0026#39;, $capture); }); sub identity($x) { nqp::dispatch(\u0026#39;identity\u0026#39;, $x) } ok(identity(42) == 42, \u0026#39;Can define identity dispatch (1)\u0026#39;); ok(identity(\u0026#39;foo\u0026#39;) eq \u0026#39;foo\u0026#39;, \u0026#39;Can define identity dispatch (2)\u0026#39;); 在第一条语句中，我们调用 dispatcher-register MoarVM 系统调用，传递一个 dispatcher 的名称以及一个闭包，每次我们需要处理调度时，都会调用这个闭包（我倾向于将其称为\u0026quot;调度回调\u0026quot;）。它接收一个单一的参数，这是一个参数的捕获（其实不是 Raku 级别的捕获，但想法 - 一个包含一组调用参数的对象 - 是一样的）。\n每一个用户定义的调度器最终都应该使用 dispatcher-delegate，以便确定另一个调度器将控制权传递给它。在这种情况下，它立即委托给 boot-value - 这意味着它除了是 boot-value 内置调度器的包装器外，其实什么都不是。\nsub identity 包含一个调度操作的静态出现。鉴于我们两次调用 sub，我们在运行时将两次遇到这个 op，但这两次是非常不同的。\n第一次是 \u0026ldquo;记录\u0026rdquo; 阶段。参数形成一个捕获，回调运行，回调又将其传给引导值调度器，产生结果。这样就形成了一个极其简单的调度程序，它说结果应该是捕获中的第一个参数。由于没有守卫，所以这将永远是一个有效的结果。\n第二次遇到调度操作时，它那里已经记录了一个调度程序，所以我们处于运行模式。在 MoarVM 源码中开启调试模式，我们可以看到结果的调度程序是这样的。\nDispatch program (1 temporaries) Ops: Load argument 0 into temporary 0 Set result object value from temporary 0 也就是说，它将参数 0 读入一个临时位置，然后将其设置为调度的结果。请注意，没有提到我们经过了额外的一层调度，这些在结果调度程序中的成本为零。\n捕获操作 参数捕获是不可改变的。各种虚拟机系统调用的存在，可以通过一些调整将它们转化为新的参数捕获，例如删除或插入参数。这里还有一个测试套件的例子。\nnqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-register\u0026#39;, \u0026#39;drop-first\u0026#39;, -\u0026gt; $capture { my $capture-derived := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-drop-arg\u0026#39;, $capture, 0); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;boot-value\u0026#39;, $capture-derived); }); ok(nqp::dispatch(\u0026#39;drop-first\u0026#39;, \u0026#39;first\u0026#39;, \u0026#39;second\u0026#39;) eq \u0026#39;second\u0026#39;, \u0026#39;dispatcher-drop-arg works\u0026#39;); 这就在将捕获传递给引导值调度器之前丢弃了第一个参数 - 意味着它将返回第二个参数。回头看一下之前的身份函数的调度程序。你能猜到这个程序会是什么样子吗？\n好吧，就是这样。\nDispatch program (1 temporaries) Ops: Load argument 1 into temporary 0 Set result string value from temporary 0 同样，虽然在这样一个调度器的记录阶段，我们确实是创建了捕获对象，并做了一个调度器代理，但由此产生的调度程序要简单得多。\n下面是一个稍微复杂一点的例子。\nmy $target := -\u0026gt; $x { $x + 1 } nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-register\u0026#39;, \u0026#39;call-on-target\u0026#39;, -\u0026gt; $capture { my $capture-derived := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-insert-arg-literal-obj\u0026#39;, $capture, 0, $target); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;boot-code-constant\u0026#39;, $capture-derived); }); sub cot() { nqp::dispatch(\u0026#39;call-on-target\u0026#39;, 49) } ok(cot() == 50, \u0026#39;dispatcher-insert-arg-literal-obj works at start of capture\u0026#39;); ok(cot() == 50, \u0026#39;dispatcher-insert-arg-literal-obj works at start of capture after link too\u0026#39;); 这里，我们有一个存储在变量 $target 中的闭包。我们把它作为捕获的第一个参数插入，然后委托给 boot-code-constant，它将调用那个代码对象，并把其他调度参数传递给它。再次，在记录阶段，我们真正要做的事情是这样的。\n 创建一个新的捕获 在开始的时候插入一个代码对象。 委托给引导代码常量分配器，它\u0026hellip;。 \u0026hellip;在没有原始参数的情况下创建一个新的捕获，并使用这些参数运行字节码。  由此产生的调度程序呢？就是这个\nDispatch program (1 temporaries) Ops: Load collectable constant at index 0 into temporary 0 Skip first 0 args of incoming capture; callsite from 0 Invoke MVMCode in temporary 0 也就是说，加载我们要调用的常量字节码句柄，设置 args（在本例中等于传入捕获的参数），然后用这些参数调用字节码。参数的洗牌，又一次消失了。一般来说，只要我们做最终的字节码调用的参数是初始调度参数的尾巴，参数转换就会变得不过是一个指针的添加。\n守卫 目前看到的所有调度方案都是无条件的：一旦在某一通话地点记录下来，就应一直使用。要使这样的机制具有实用性，缺少的一大块就是守卫。守卫断言了一些属性，比如参数的类型或者参数是确定的（Int:D）还是不确定的（Int:U）。\n下面是一个有点长的测试用例，并在其中放置了一些解释。\n# A couple of classes for test purposes my class C1 { } my class C2 { } # A counter used to make sure we\u0026#39;re only invokving the dispatch callback as # many times as we expect. my $count := 0; # A type-name dispatcher that maps a type into a constant string value that # is its name. This isn\u0026#39;t terribly useful, but it is a decent small example. nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-register\u0026#39;, \u0026#39;type-name\u0026#39;, -\u0026gt; $capture { # Bump the counter, just for testing purposes. $count++; # Obtain the value of the argument from the capture (using an existing # MoarVM op, though in the future this may go away in place of a syscall) # and then obtain the string typename also. my $arg-val := nqp::captureposarg($capture, 0); my str $name := $arg-val.HOW.name($arg-val); # This outcome is only going to be valid for a particular type. We track # the argument (which gives us an object back that we can use to guard # it) and then add the type guard. my $arg := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-arg\u0026#39;, $capture, 0); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-guard-type\u0026#39;, $arg); # Finally, insert the type name at the start of the capture and then # delegate to the boot-constant dispatcher. nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;boot-constant\u0026#39;, nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-insert-arg-literal-str\u0026#39;, $capture, 0, $name)); }); # A use of the dispatch for the tests. Put into a sub so there\u0026#39;s a single # static dispatch op, which all dispatch programs will hang off. sub type-name($obj) { nqp::dispatch(\u0026#39;type-name\u0026#39;, $obj) } # Check with the first type, making sure the guard matches when it should # (although this test would pass if the guard were ignored too). ok(type-name(C1) eq \u0026#39;C1\u0026#39;, \u0026#39;Dispatcher setting guard works\u0026#39;); ok($count == 1, \u0026#39;Dispatch callback ran once\u0026#39;); ok(type-name(C1) eq \u0026#39;C1\u0026#39;, \u0026#39;Can use it another time with the same type\u0026#39;); ok($count == 1, \u0026#39;Dispatch callback was not run again\u0026#39;); # Test it with a second type, both record and run modes. This ensures the # guard really is being checked. ok(type-name(C2) eq \u0026#39;C2\u0026#39;, \u0026#39;Can handle polymorphic sites when guard fails\u0026#39;); ok($count == 2, \u0026#39;Dispatch callback ran a second time for new type\u0026#39;); ok(type-name(C2) eq \u0026#39;C2\u0026#39;, \u0026#39;Second call with new type works\u0026#39;); # Check that we can use it with the original type too, and it has stacked # the dispatch programs up at the same callsite. ok(type-name(C1) eq \u0026#39;C1\u0026#39;, \u0026#39;Call with original type still works\u0026#39;); ok($count == 2, \u0026#39;Dispatch callback only ran a total of 2 times\u0026#39;); 这个时候就会产生两个调度程序，一个是 C1。\nDispatch program (1 temporaries) Ops: Guard arg 0 (type=C1) Load collectable constant at index 1 into temporary 0 Set result string value from temporary 0 另一个是 C2:\nDispatch program (1 temporaries) Ops: Guard arg 0 (type=C2) Load collectable constant at index 1 into temporary 0 Set result string value from temporary 0 再一次，没有捕获操作、跟踪或调度器委托的遗留问题；调度程序对一个参数进行类型防护，然后产生结果字符串。整个对 $arg-val.HOW.name($arg-val) 的调用都被省略了，我们写的调度程序将知识进行了编码 - 以虚拟机能够理解的方式 - 一个类型的名称可以被认为是不可改变的。\n这个例子有点造作，但现在考虑一下，我们反而要查找一个方法，并在调用者类型上进行守卫：这就是一个方法缓存! 守护更多参数的类型，我们就有了一个多缓存。两者都做，我们就有了一个多方法缓存。\n后者很有意思，因为方法调度和多调度都想对调用者进行守护。事实上，在 MoarVM 中，今天会有两个这样的类型测试，直到我们到了特殊化器做工作并消除这些重复的守卫。然而，新的调度器并没有将调度器 - guard-类型当作一种命令式操作，将守卫写入结果调度程序中。相反，它声明相关的参数必须被防护。如果其他的调度器已经这样做了，那它就是幂等的。一旦我们委派通过的所有调度程序，在通往最终结果的路径上，都有了自己的发言权，就会发出守卫。\n有趣的是：特别细心的人会注意到，调度机制也被用作实现新的调度程序的一部分，事实上，这最终也将意味着特殊化者可以将调度程序特殊化，让它们也被 JIT 编译成更高效的东西。毕竟，从 MoarVM 的角度来看，这一切都只是要运行的字节码，只是有些字节码是告诉 VM 如何更高效地执行 Raku 程序的!\n恢复调度 可恢复调度器需要做两件事。\n 在注册调度器的时候，提供一个恢复回调和一个调度回调。 在 dispatch 回调中，指定一个捕获，这将形成恢复初始化状态。  当发生恢复时，将调用恢复回调，并提供恢复的任何参数。它还可以获得在 dispatch 回调中设置的 resume 初始化状态。resume 初始化状态包含了第一次恢复调度时继续进行调度所需要的东西。我们先来看看方法调度的工作原理，看一个具体的例子。我也会在此时，切换到看真正的 Rakudo 调度器，而不是简化的测试用例。\nRakudo 调度器利用授权、重复守卫和捕获操作都没有运行时成本的优势，在结果调度程序中，至少在我看来，很好地因素了一个有些复杂的调度过程。方法调度有多个切入点：普通无聊的 $obj.meth()，限定的 $obj.Type::meth()，以及调用我也许 $obj.?meth()。这些都有共同的 resume 语义 - 或者至少，只要我们在 resume 初始化状态中始终携带一个起始类型，也就是我们做方法调度的对象的类型，就可以使它们成为。\n这里是普通方法调度的切入点，去掉了报告缺失方法错误的无聊细节。\n# A standard method call of the form $obj.meth($arg); also used for the # indirect form $obj.\u0026#34;$name\u0026#34;($arg). It receives the decontainerized invocant, # the method name, and the the args (starting with the invocant including any # container). nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-register\u0026#39;, \u0026#39;raku-meth-call\u0026#39;, -\u0026gt; $capture { # Try to resolve the method call using the MOP. my $obj := nqp::captureposarg($capture, 0); my str $name := nqp::captureposarg_s($capture, 1); my $meth := $obj.HOW.find_method($obj, $name); # Report an error if there is no such method. unless nqp::isconcrete($meth) { !!! \u0026#39;Error reporting logic elided for brevity\u0026#39;; } # Establish a guard on the invocant type and method name (however the name # may well be a literal, in which case this is free). nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-guard-type\u0026#39;, nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-arg\u0026#39;, $capture, 0)); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-guard-literal\u0026#39;, nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-arg\u0026#39;, $capture, 1)); # Add the resolved method and delegate to the resolved method dispatcher. my $capture-delegate := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-insert-arg-literal-obj\u0026#39;, $capture, 0, $meth); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;raku-meth-call-resolved\u0026#39;, $capture-delegate); }); 现在是解析方法 dispatcher，也就是处理恢复的地方。首先，让我们看看正常的 dispatch 回调（恢复回调是包含的，但是是空的，我稍后会展示它）。\n# Resolved method call dispatcher. This is used to call a method, once we have # already resolved it to a callee. Its first arg is the callee, the second and # third are the type and name (used in deferral), and the rest are the args to # the method. nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-register\u0026#39;, \u0026#39;raku-meth-call-resolved\u0026#39;, # Initial dispatch -\u0026gt; $capture { # Save dispatch state for resumption. We don\u0026#39;t need the method that will # be called now, so drop it. my $resume-capture := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-drop-arg\u0026#39;, $capture, 0); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-set-resume-init-args\u0026#39;, $resume-capture); # Drop the dispatch start type and name, and delegate to multi-dispatch or # just invoke if it\u0026#39;s single dispatch. my $delegate_capture := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-drop-arg\u0026#39;, nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-drop-arg\u0026#39;, $capture, 1), 1); my $method := nqp::captureposarg($delegate_capture, 0); if nqp::istype($method, Routine) \u0026amp;\u0026amp; $method.is_dispatcher { nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;raku-multi\u0026#39;, $delegate_capture); } else { nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;raku-invoke\u0026#39;, $delegate_capture); } }, # Resumption -\u0026gt; $capture { ... \u0026#39;Will be shown later\u0026#39;; }); raku-meth-call 中有一个可以论证的欺骗：它实际上并没有插入调用者的类型对象来代替调用者。事实证明，这并不重要。否则，我认为注释（在真正的实现中也可以找到）很好地说明了这个问题。\n有一个重要的点可能并不清楚 - 但遵循了一个重复的主题 - 那就是恢复初始化状态的设置也更多的是一种声明式而不是命令式的东西：在调度的时候并没有运行时成本，而是我们在周围保留了足够的信息，以便能够在我们需要的时候重建恢复初始化状态。事实上，当我们处于恢复的运行阶段时，我们甚至不需要在创建捕获对象的意义上重建它）。\n现在说说复盘。我将介绍一个严重简化的版本，它只处理 callsame 语义（完整的东西也要处理 lastcall 和 nextcallee 这样的乐趣）。resume 初始化状态的存在是为了给 resumption 过程播种。一旦我们知道我们实际上确实要处理恢复，我们就可以做一些事情，比如计算我们想要走过的继承图中的全部方法列表。每个可恢复的调度器在调用栈上得到一个单一的存储槽，它可以用于它的状态。它可以在恢复的第一步中初始化这个，然后在我们走的时候更新它。或者更准确的说，它可以设置一个调度程序，在运行时就会这样做。\n对于我们将要走过的候选链来说，链接列表原来是一个非常方便的数据结构。我们可以通过跟踪当前节点来完成链接列表的工作，也就是说只需要有一个东西发生突变，也就是当前调度的状态。调度程序机制还提供了一种从对象中读取属性的方法，这就足以将遍历链接列表表达到调度程序中。这也意味着零分配。\n所以，不多说了，下面是链接列表（在 NQP 这个受限的 Raku 子集中，相当不如在完整的 Raku 中漂亮）。\n# A linked list is used to model the state of a dispatch that is deferring # through a set of methods, multi candidates, or wrappers. The Exhausted class # is used as a sentinel for the end of the chain. The current state of the # dispatch points into the linked list at the appropriate point; the chain # itself is immutable, and shared over (runtime) dispatches. my class DeferralChain { has $!code; has $!next; method new($code, $next) { my $obj := nqp::create(self); nqp::bindattr($obj, DeferralChain, \u0026#39;$!code\u0026#39;, $code); nqp::bindattr($obj, DeferralChain, \u0026#39;$!next\u0026#39;, $next); $obj } method code() { $!code } method next() { $!next } }; my class Exhausted {}; 最后是恢复处理。\nnqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-register\u0026#39;, \u0026#39;raku-meth-call-resolved\u0026#39;, # Initial dispatch -\u0026gt; $capture { ... \u0026#39;Presented earlier; }, # Resumption. The resume init capture\u0026#39;s first two arguments are the type #that we initially did a method dispatch against and the method name #respectively. -\u0026gt; $capture{ # Work out the next method to call, if any. This depends on if we have # an existing dispatch state (that is, a method deferral is already in # progress). my $init := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-get-resume-init-args\u0026#39;); my $state := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-get-resume-state\u0026#39;); my $next_method; if nqp::isnull($state) { # No state, so just starting the resumption. Guard on the # invocant type and name. my $track_start_type := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-arg\u0026#39;, $init, 0); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-guard-type\u0026#39;, $track_start_type); my $track_name := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-arg\u0026#39;, $init, 1); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-guard-literal\u0026#39;, $track_name); # Also guard on there being no dispatch state. my $track_state := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-resume-state\u0026#39;); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-guard-literal\u0026#39;, $track_state); # Build up the list of methods to defer through. my $start_type := nqp::captureposarg($init, 0); my str $name := nqp::captureposarg_s($init, 1); my @mro := nqp::can($start_type.HOW, \u0026#39;mro_unhidden\u0026#39;) ?? $start_type.HOW.mro_unhidden($start_type) !! $start_type.HOW.mro($start_type); my @methods; for @mro { my %mt := nqp::hllize($_.HOW.method_table($_)); if nqp::existskey(%mt, $name) { @methods.push(%mt{$name}); } } # If there\u0026#39;s nothing to defer to, we\u0026#39;ll evaluate to Nil (just don\u0026#39;t set # the next method, and it happens below). if nqp::elems(@methods) \u0026gt;= 2 { # We can defer. Populate next method. @methods.shift; # Discard the first one, which we initially called $next_method := @methods.shift; # The immediate next one # Build chain of further methods and set it as the state. my $chain := Exhausted; while @methods { $chain := DeferralChain.new(@methods.pop, $chain); } nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-set-resume-state-literal\u0026#39;, $chain); } } elsif !nqp::istype($state, Exhausted) { # Already working through a chain of method deferrals. Obtain # the tracking object for the dispatch state, and guard against # the next code object to run. my $track_state := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-resume-state\u0026#39;); my $track_method := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-attr\u0026#39;, $track_state, DeferralChain, \u0026#39;$!code\u0026#39;); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-guard-literal\u0026#39;, $track_method); # Update dispatch state to point to next method. my $track_next := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-attr\u0026#39;, $track_state, DeferralChain, \u0026#39;$!next\u0026#39;); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-set-resume-state\u0026#39;, $track_next); # Set next method, which we shall defer to. $next_method := $state.code; } else { # Dispatch already exhausted; guard on that and fall through to returning # Nil. my $track_state := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-track-resume-state\u0026#39;); nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-guard-literal\u0026#39;, $track_state); } # If we found a next method... if nqp::isconcrete($next_method) { # Call with same (that is, original) arguments. Invoke with those. # We drop the first two arguments (which are only there for the # resumption), add the code object to invoke, and then leave it # to the invoke or multi dispatcher. my $just_args := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-drop-arg\u0026#39;, nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-drop-arg\u0026#39;, $init, 0), 0); my $delegate_capture := nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-insert-arg-literal-obj\u0026#39;, $just_args, 0, $next_method); if nqp::istype($next_method, Routine) \u0026amp;\u0026amp; $next_method.is_dispatcher { nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;raku-multi\u0026#39;, $delegate_capture); } else { nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;raku-invoke\u0026#39;, $delegate_capture); } } else { # No method, so evaluate to Nil (boot-constant disregards all but # the first argument). nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-delegate\u0026#39;, \u0026#39;boot-constant\u0026#39;, nqp::dispatch(\u0026#39;boot-syscall\u0026#39;, \u0026#39;dispatcher-insert-arg-literal-obj\u0026#39;, $capture, 0, Nil)); } }); 这是相当多的内容，也是相当多的代码。但请记住，这只是运行在调度恢复的记录阶段。它还会在 callsame 的 callsite 产生一个调度程序，并带有通常的守卫和结果。隐式守卫是为我们在该点恢复的调度程序创建的。在最常见的情况下，这最终将是单形或双形的，尽管涉及多个调度或方法调度的嵌套的情况可能会产生一个更有形态的 callsite。\n我选取的设计迫使 resume 回调处理两种情况：第一次复用和后一次复用。这在几个方面都不理想。\n这对那些编写调度简历回调的人来说有点不方便。然而，这又不是特别常见的活动!\n这种差异导致两个调度程序堆积在一个调用点，而在其他情况下，这个调用点可能只得到一个 只有其中第二项真正重要。之所以不统一，是为了确保绝大多数从未恢复调度的电话，不会因其最终从未使用的功能而产生每次调度的费用。如果结果是使用该功能的人多花了一点成本，那就这样吧。事实上，早期的基准测试显示，使用新调度器的 callsame 与 wrap 和方法调用似乎比当前 Rakudo 中的速度快了 10 倍，这还没等专门人员对它有足够的了解，就已经进一步改进了!\n目前所做的事情 我上面讨论的所有内容都已经实现了，只是我可能在某个地方给人的印象是，使用新的 dispatcher 已经完全实现了多重调度，而现在还不是这样（没有处理 where 子句，也不支持调度恢复）。\n今后的步骤 下一步显然是要完全实现多调度的缺失部分。另一个缺失的语义是对 callwith 和 nextwith 的支持，当我们希望改变移动到下一个候选人时使用的参数。抛开其他一些小问题不谈，理论上来说，这至少可以让所有的 Raku 调度语义得到支持。\n目前，所有的标准方法调用（$obj.meth()）和其他调用（foo()和$foo()）都会通过现有的调度机制，而不是新的调度器。这些也需要迁移到新的调度器上，而且任何发现的错误都需要修复。这将使事情达到新调度器在语义上已经准备好的程度。\n之后是性能工作：确保专用器能够处理调度程序的防护和结果。最初的目标是，让常见调用形式的稳态性能至少与当前乐道主分支中的性能相同。已经很清楚了，对于一些到目前为止还很冰冷的东西来说，会有一些大的胜利，但它不应该以最常见的调度种类的退步为代价，因为这些调度种类之前已经得到了大量的优化努力。\n此外，NQP - 乐道编译器和运行时内脏的其他位写的乐的限制形式 - 也需要迁移到使用新的调度器。只有做到这一点，才有可能从 MoarVM 中扯出当前的方法缓存、多调度缓存等。\n一个悬而未决的问题是，如何处理 MoarVM 以外的后端。理想情况下，新的调度机制将被移植到这些地方。相当多的内容应该可以用 JVM 的 invokedynamic 来表达（而这一切可能会在基于 Truffle 的 Raku 实现中发挥得相当好，尽管我不确定目前是否有这方面的积极努力）。\n未来的机会 虽然我目前的重点是发布一个使用新调度机制的 Rakudo 和 MoarVM 版本，但这不会是旅程的终点。一些眼前的想法。\n 对角色的方法调用需要把角色打入一个类中， 所以方法查找会返回一个闭包来完成这个任务并替换调用者。这是一个很大的间接性；新的调度者可以获得 pun，并产生一个调度程序，用 punn 化的类类型对象替换角色类型对象，这将使每次调用的成本大大降低。 我期望使用新的 dispatcher 可以使句柄（dlegated）和 fallback（处理缺失的方法调用）机制都能有更好的表现 当前的 assuming - 用于为例程讨价还价或其他首要参数 - 的实现并不理想，利用新调度器的参数重写能力的实现可能会有更好的表现。 在新的调度机制的帮助下，一些新的语言功能也可能以高效的方式提供。例如，目前没有一种可靠的方式来尝试调用一段代码，如果签名绑定了就运行它，如果没有绑定就做其他事情。相反，像 Cro 路由器这样的东西，必须先做签名的试绑定，然后再做调用，这使得路由的成本相当高。还有一个建议已久的想法，就是通过签名与 when 构造提供模式匹配 (例如，when * -\u0026gt; ($x) {}; when * -\u0026gt; ($x, *@tail) { })，这和需求差不多，只是在一个不太动态的环境下。  最后\u0026hellip; 在新的调度机制上的工作比我最初预期的历程要长。设计的恢复部分特别具有挑战性，而且还有一些重要的细节需要处理。一路走来，大概有四种潜在的方法被抛弃了（虽然其中的元素都影响了我在这篇文章中描述的内容）。能坚持下来的抽象真的非常非常难。\n我最终也不得不从根本上离开几个月做 Raku 工作，在其他一些工作中感觉有点被压垮了，并且一直在与同样重要的 RakuAST 项目（它将因为能够承担新的调度器的存在而被简化，并且还为我提供了一系列更柔和的 Raku 黑客任务，而调度器的工作提供了很少的轻松选择）。\n鉴于这些，我很高兴终于看到了隧道尽头的光亮。剩下的工作是数不胜数的，而我们使用新的调度器发布 Rakudo 和 MoarVM 的那一天，感觉还需要几个月的时间（我希望写下这句话不是在诱惑命运！）。\n新的调度器可能是 MoarVM 自我创建以来最重要的变化，因为它看到我们删除了一堆从一开始就存在的东西。RakuAST 也将为 Rakudo 编译器带来十年来最大的架构变化。两者都是一个机会，将多年来学习的东西硬生生地折合到运行时和编译器中。我希望再过十年，当我回顾这一切的时候，至少会觉得自己这次犯了更多有趣的错误。\n原文链接: https://6guts.wordpress.com/2021/03/15/towards-a-new-general-dispatch-mechanism-in-moarvm/\n","permalink":"https://ohmyweekly.github.io/notes/2021-03-15-towards-a-new-general-dispatch-mechanism-in-moarvm/","tags":["Raku"],"title":"争取在 MoarVM 中建立一个新的总调度机制"},{"categories":["Flink"],"contents":"PimDaniel 提出了一个有趣的问题。\n我如何在匹配时测试 match is True : 这不起作用。\nif my ($type,$a,$b,$c) = ($v ~~ /^(\u0026#39;horiz\u0026#39;|\u0026#39;vertic\u0026#39;)\u0026#39;_\u0026#39;(\\d+)\u0026#39;_\u0026#39;(\\d+)\u0026#39;_\u0026#39;(\\d+)$/)\u0026gt;\u0026gt;.Str { ... } 好吧，我分2次做了1/捕捉并测试匹配，2/将匹配转换为Str。\n没有得到及时的回答，完全没有改进。我也找不到一个好的方法来快速完成这个任务。事实上我花了一个小时才破解这个螺母。这里的主要问题是，一个失败的匹配会产生 .Str 会抱怨的 Nil。所以让我们把 if 的布尔检查和转换为 Str 的过程分开。\nmy $a = \u0026#39;1 B\u0026#39;; if $a ~~ /(\u0026lt;digit\u0026gt;)\\s(\u0026lt;alpha\u0026gt;)/ -\u0026gt; $_ { my ($one, $B) = .deepmap: *.Str; say \u0026#34;$one$B\u0026#34;; } # OUTPUT: 1 B 通过将条件表达式的结果强行放入主题中，我们可以在匹配的结果上运行任何方法，但前提是 Match.bool 返回 true。我没有 CS* 学位，但如果 Raku-signatures 不会变成 turing complete，我会非常惊讶。\nif $a ~~ /(\u0026lt;digit\u0026gt;)\\s(\u0026lt;alpha\u0026gt;)/ -\u0026gt; Match (Str() $one, Str() $B) { dd $one; dd $B; } # OUTPUT: \u0026#34;1\u0026#34; \u0026#34;B\u0026#34; if 块的签名将 Match 胁迫为一个列表。我们选择其中的两个元素，并将这些元素胁迫为 Str。当然，我们可以根据捕获的位置来强制到任何我们喜欢的东西。\nRaku 中的 Regexes 被编译成相同的字节码，然后程序的其余部分。事实上，语法只是一个具有有趣语法的类。这就是为什么我们可以轻松地在 regex 里面运行Raku 代码。这意味着我们可以把整个程序从内部翻出来。\nmy @a = \u0026lt;1 B 3 D 4\u0026gt;; my @b; my $hit; for @a -\u0026gt; $e { @b.push: ($e ~~ /(\u0026lt;alpha\u0026gt;)||{ next }/).Str; } say @b; # OUTPUT: [B D] 在这里，如果匹配不成功，我们就跳过 .push，用 next 跳过循环体的其余部分。我们可以在 regex 内部发出任何控制异常。这意味着我们可以将整个过程粘在一个 sub 中，然后从 regex 中返回我们正在寻找的值。\nsub cherry-pick-numeric(Str $matchee) { $matchee ~~ m/(\u0026lt;digit\u0026gt;)\u0026amp;\u0026amp;{ return .Numeric }/; Empty } @b = do .\u0026amp;cherry-pick-numeric for @a; dd @b; # OUTPUT: Array @b = [1, 3, 4] Raku 已经酝酿了10年。这是一个巨大的任务。现在，困难的部分来了。我们必须从那庞大的语言中找到所有好的惯用法。好东西会降临到那些等待的人身上（IRC上）。\n*) 阅读。不要相信我写的任何东西。你已经被警告了。\n更新一下。\n我以真正的懒惰方式，想出了一个办法，在本该完成的工作之后，把匹配变成一个惰性列表。\n$a = \u0026#39;1B3D4\u0026#39;; my \\ll := gather $a ~~ m:g/[\u0026lt;alpha\u0026gt;\u0026amp;\u0026amp;{ take $/\u0026lt;alpha\u0026gt;.Str }]|[\u0026lt;digit\u0026gt;\u0026amp;\u0026amp;{ take $/.\u0026lt;digit\u0026gt;.Numeric }]|[{ say \u0026#39;step\u0026#39; }]/; say ll[0]; say ll[3]; # OUTPUT: 1 step step step D 技巧是用 :g 副词强制匹配一直运行到字符串的末尾。这个运行将被 take 打断（通过抛出 CX::Take），当从 gather 返回的 Seq 中询问下一个值时再继续。我不知道这是否是有效的内存思想。可能会有一个 Match 实例为每个 take 保留在身边。\n原文链接: https://gfldex.wordpress.com/2021/03/11/raku-is-a-match-for/\n","permalink":"https://ohmyweekly.github.io/notes/2021-03-12-raku-is-a-match-for-star/","tags":["Flink","Flink 官方文档"],"title":"Raku is a match for *"},{"categories":["Raku"],"contents":"class Student { has $.name; has $.sex; has $!age; method BUILD(:$!name) { } } my $stu = Student.new(:name(\u0026#39;赵今麦\u0026#39;), :sex(\u0026#39;女\u0026#39;)); dd $stu; # Student $stu = Student.new(name =\u0026gt; \u0026#34;赵今麦\u0026#34;, sex =\u0026gt; Any) 如果你加上 BUILD 方法, 你必须自己全权负责设置所有的东西。也就是既包括公共属性, 也包括私有属性。\n但你可以通过巧妙地命名参数来让你的生活更轻松。\nmethod BUILD (:$!name, :$!sex, :$!age) { } BUILD 方法可以初始化私有属性:\nclass Student { has $.name; has $.sex; has $!age; method BUILD(:$name, :$sex, :$age) { $!name = $name; $!sex = $sex; $!age = $age; } method heart() { return \u0026#39;♥ \u0026#39; ~ $!age } } my $stu = Student.new(:name(\u0026#39;赵今麦\u0026#39;), :sex(\u0026#39;女\u0026#39;), :age(18)); dd $stu; say $stu; say $stu.heart(); 也可以在 TWEAK 方法中初始化私有属性:\nclass Student { has $.name; has $.sex; has $!age; method TWEAK(:$name, :$sex, :$age) { $!age = $age + 1; } method heart() { return \u0026#39;♥\u0026#39; ~ $!age } } my $stu = Student.new(:name(\u0026#39;赵今麦\u0026#39;), :sex(\u0026#39;女\u0026#39;), :age(18)); dd $stu; say $stu; say $stu.heart(); 使用 TWEAK，你会得到与 BUILD 相同的参数，但所有的初始设置都已经完成了（BUILD 或公共属性的自动绑定，以及所有的默认值，再加上保证所需值的存在）。你只是有机会做一些最后的调整。\n","permalink":"https://ohmyweekly.github.io/notes/2021-03-07-difference-between-build-and-tweak-in-raku/","tags":["Raku","BUILD","TWEAK","Object"],"title":"BUILD 和 TWEAK 的区别"},{"categories":["Dart"],"contents":"空值安全是我们在 Dart 2.0 中用健全的静态类型系统取代了原来的不健全的可选类型系统后，对 Dart 做出的最大改变。当 Dart 刚推出的时候，编译时空安全是一个罕见的功能，需要长时间的介绍。今天，Kotlin、Swift、Rust 和其他语言都有自己的答案，这已经成为一个非常熟悉的问题。下面是一个例子。\n// Without null safety: bool isEmpty(String string) =\u0026gt; string.length == 0; main() { isEmpty(null); } 如果你在没有 null 安全的情况下运行这个 Dart 程序，它就会在调用.length 时抛出一个 NoSuchMethodError 异常。null 值是 Null 类的一个实例，而 Null 没有 \u0026ldquo;length\u0026rdquo; getter。运行时的失败很糟糕。这在像 Dart 这样的语言中尤其如此，因为它被设计成在终端用户的设备上运行。如果一个服务器应用程序失败了，你通常可以在任何人注意到之前重新启动它。但是当一个 Flutter 应用在用户的手机上崩溃时，他们并不高兴。当你的用户不高兴时，你也不高兴。\n开发者喜欢像 Dart 这样的静态类型语言，因为它们可以让类型检查器在编译时发现代码中的错误，通常就在 IDE 中。越早发现错误，就能越早修复它。当语言设计者谈论 \u0026ldquo;修复空值引用错误\u0026quot;时，他们的意思是丰富静态类型检查器，使语言能够检测到像上面试图在一个可能是空的值上调用 .length 这样的错误。\n对于这个问题，没有一个真正的解决方案。Rust 和 Kotlin 都有自己的方法，在这些语言的上下文中是有意义的。这个文档详细介绍了我们对 Dart 的答案。它包括对静态类型系统的修改，以及一系列其他的修改和新的语言特性，让你不仅能写出空值安全的代码，而且希望能享受这样做的乐趣。\n这个文档很长。如果你想看一些较短的文件，它只涵盖了你需要知道的东西，以便开始运行，请从概述开始。当你准备好了更深入的理解，并且有时间的时候，请回到这里，这样你就可以理解这个语言是如何处理 null 的，为什么我们要这样设计它，以及如何写出习惯的，现代的，空值安全的 Dart。(Spoiler alert: 它最终会出人意料地接近你今天写 Dart 的方式。)\n语言处理空值引用错误的各种方法各有优缺点。这些原则指导了我们的选择。\n 代码默认情况下应该是安全的。如果你写了新的 Dart 代码，并且没有使用任何显式的不安全特性，它永远不会在运行时抛出一个空值引用错误。所有可能的空值引用错误都会被静态地捕获。如果你想将一些检查推迟到运行时以获得更大的灵活性，你可以，但你必须通过使用一些在代码中文本可见的功能来选择。  换句话说，我们并不是给你一件救生衣，让你每次出海时都记得穿上它。相反，我们给你一艘不沉的船。除非你跳海，否则你会保持干燥。\n 空值安全代码应该很容易写。大多数现有的 Dart 代码都是动态正确的，不会出现空值引用错误。你喜欢你的 Dart 程序现在的样子，我们希望你能够继续这样写代码。安全性不应该要求牺牲可用性，对类型检查器进行忏悔，或者必须显著改变你的思维方式。  由此产生的空安全代码应该是完全健全的。在静态检查的上下文中，\u0026ldquo;健全\u0026quot;对不同的人意味着不同的东西。对我们来说，在空值安全的上下文中，这意味着如果一个表达式的静态类型不允许空，那么该表达式的任何可能的执行都不可能评估为空。语言主要通过静态检查来提供这种保证，但也可以涉及一些运行时检查。虽然，注意第一个原则：任何发生这些运行时检查的地方都将是你的选择）。\n健全性对于用户的信心很重要。一艘大部分时间都保持漂浮的船，并不是你热衷于在公海上冒险的船。但它对我们无畏的编译器黑客来说也很重要。当语言对程序的语义属性做出硬性保证时，意味着编译器可以执行假设这些属性为真的优化。当涉及到 null 时，这意味着我们可以生成更小的代码，消除不需要的 null 检查，以及更快的代码，不需要在调用方法之前验证接收器是非 null。\n有一个注意事项：我们只保证完全空值安全的 Dart 程序的健全性。Dart 支持包含新的空值安全代码和旧的遗留代码混合的程序。在这些混合版本的程序中，仍然可能发生空值引用错误。在一个混合版本的程序中，你可以在空值安全的部分获得所有的静态安全优势，但是在整个应用程序是空安全的之前，你不能获得完整的运行时健全性。\n请注意，消除 null 并不是一个目标。null 没有什么不好。相反，能够表示一个值的缺失真的很有用。直接在语言中构建对特殊的 \u0026ldquo;absence\u0026rdquo; 值的支持，使得处理缺失的工作变得灵活和可用。它是可选参数、方便的 ?. null-aware 操作符和默认初始化的基础。并不是 null 不好，而是让 null 去了你想不到的地方才会引起问题。\n因此，有了 null 安全，我们的目标是让你控制和洞察 null 可以流经你的程序的地方，并确定它不能流到某个地方，从而导致崩溃。\n类型系统中的空值 空值安全始于静态类型系统，因为其他一切都建立在静态类型系统之上。你的 Dart 程序中有一个完整的类型宇宙：像 int 和 String 这样的基元类型，像 List 这样的集合类型，以及所有你和你使用的包所定义的类和类型。在 null 安全之前，静态类型系统允许值 null 流入任何这些类型的表达式中。\n在类型理论的行话中，Null 类型被视为所有类型的一个子类型。\n在某些表达式上允许的操作集 - getters、setters、methods 和 operator - 由其类型定义。如果类型是 List，你可以对它调用. add() 或 []。如果它是 int，你可以调用 +。但是空值并没有定义任何这些方法。允许 null 流入其他类型的表达式意味着任何这些操作都可能失败。这就是 null 引用错误的真正症结所在 - 每一次失败都来自于试图在 null 上查找一个它没有的方法或属性。\n非可空类型和可空类型 Null 安全通过改变类型层次结构，从根本上消除了这个问题。Null 类型仍然存在，但它不再是所有类型的子类型。取而代之的是，类型层次结构是这样的。\n由于 Null 不再是子类型, 除了特殊的 Null 类之外，没有任何类型允许值为 null。我们已经将所有类型默认为不可空值。如果你有一个 String 类型的变量，它将总是包含一个字符串。在那里，我们已经修复了所有的空值引用错误。\n如果我们认为 null 根本没有用，我们可以在这里停止。但是 null 是有用的，所以我们仍然需要一种方法来处理它。可选参数就是一个很好的说明性案例。考虑一下这个 null 安全的 Dart 代码。\n// Using null safety: makeCoffee(String coffee, [String? dairy]) { if (dairy != null) { print(\u0026#39;$coffeewith $dairy\u0026#39;); } else { print(\u0026#39;Black $coffee\u0026#39;); } } 在这里，我们希望允许 dairy 参数接受任何字符串，或者接受 null 值，而不接受其他任何值。为了表达这一点，我们在底层基类型 String 的结尾处加上 ?。 从本质上讲，这就是定义了一个底层类型和 Null 类型的联合。所以，如果 Dart 有全功能的联合类型，那么 String? 将是 String|Null 的简写。\n使用可空类型 如果你有一个可空类型的表达式，你可以用这个结果做什么？由于我们的原则默认是安全的，所以答案是不多，我们不能让你对它调用底层类型的方法，因为如果值是空的，这些方法可能会失败。\n// Hypothetical unsound null safety: bad(String? maybeString) { print(maybeString.length); } main() { bad(null); } 如果我们让你运行它，就会崩溃。我们唯一可以安全地让你访问的方法和属性是由底层类型和 Null 类定义的。那就是 toString()、== 和 hashCode。因此，你可以使用可空类型作为映射键，将它们存储在集合中，将它们与其他值进行比较，并在字符串插值中使用它们，但仅此而已。\n它们如何与非可空类型交互？将一个不可空值类型传递给期望空值类型的东西总是安全的。如果一个函数接受 String 吗，那么传递一个 String 是允许的，因为它不会引起任何问题。我们通过使每个可空类型成为其底层类型的超类型来建立模型。你也可以安全地把 null 传给期望是可空类型的东西，所以 Null 也是每个可空类型的一个子类型。\n但是反过来说，把一个可空类型传递给期待底层非可空类型的东西是不安全的。期待一个 String 的代码可以在值上调用 String 方法。如果你把一个 String? 传给它，null 可能会流进来，这可能会失败。\n// Hypothetical unsound null safety: requireStringNotNull(String definitelyString) { print(definitelyString.length); } main() { String? maybeString = null; // Or not!  requireStringNotNull(maybeString); } 这个程序不安全，我们不应该允许它。然而，Dart 一直有这个东西，叫做隐式下传。例如，如果你把一个 Object 类型的值传递给一个期望为 String 的函数，类型检查器就会允许它。\n// Without null safety: requireStringNotObject(String definitelyString) { print(definitelyString.length); } main() { Object maybeString = \u0026#39;it is\u0026#39;; requireStringNotObject(maybeString); } 为了保持合理性，编译器在 requireStringNotObject() 的参数上默默地插入了一个 as String cast。这个转码可能会在运行时失败并抛出一个异常，但在编译时，Dart 说这是确定的。由于非可空类型被建模为可空类型的子类型，所以隐式下投会让你把一个 String? 传递给期待一个 String 的东西。允许这样做会违反我们默认安全的目标。所以，有了空值安全，我们就完全取消了隐式下传。\n这使得对 requireStringNotNull() 的调用会产生一个编译错误，这是你想要的。但这也意味着所有的隐式下包都会成为编译错误，包括对 requireStringNotObject() 的调用。你必须自己添加显式下传。\n// Using null safety: requireStringNotObject(String definitelyString) { print(definitelyString.length); } main() { Object maybeString = \u0026#39;it is\u0026#39;; requireStringNotObject(maybeString as String); } 我们认为这总体上是一个好的变化。在我们的印象中，大多数用户从来都不喜欢隐性降频。尤其是，你可能之前就被这个烧过。\n// Without null safety: List\u0026lt;int\u0026gt; filterEvens(List\u0026lt;int\u0026gt; ints) { return ints.where((n) =\u0026gt; n.isEven); } 发现错误了吗？.where() 方法是懒惰的，所以它返回的是一个 Iterable，而不是 List。这个程序在编译时，当它试图将 Iterable 投射到 filterEvens 声明它返回的 List 类型时，会在运行时抛出一个异常。移除隐式下投后，这就变成了一个编译错误。\n我们说到哪里了？对了，好吧，就好像我们把你程序中的类型宇宙分成了两半。\n有一个非空值类型的区域。这些类型让你可以访问所有有趣的方法，但永远不能包含 null。然后是一个由所有相应的可空类型组成的平行家族。这些类型允许 null，但你不能对它们做太多事情。我们让值从非可空侧流向可空侧，因为这样做是安全的，但不是其他方向。\n这样看来，可空类型基本上是无用的。它们没有方法，你无法摆脱它们。别担心，我们有一整套的功能来帮助你把值从可空型的一半移到另一边，我们很快就会讲到。\n顶部和底部 这一部分有点深奥。你可以跳过它，除了最后的两个子弹，除非你对类型系统感兴趣。想象一下，在你的程序中，所有的类型之间都有边缘，它们是彼此的子类型和超类型。如果你把它画出来，就像这个文档中的图一样，它将形成一个巨大的有向图，上面有像 Object 这样的超类型，下面有像你自己的类型这样的叶子类。\n如果这个有向图到了顶部，有一个单一的类型是超类型（直接或间接），这个类型就被称为顶部类型。同样，如果在那个底部有一个奇怪的类型是每个类型的子类型，你就有一个底部类型。在这种情况下，你的有向图是一个网格）。\n如果你的类型系统有顶层和底层类型，那是很方便的，因为这意味着像最小上界这样的类型级操作（类型推理使用它来根据一个条件表达式的两个分支的类型找出它的类型）总是可以产生一个类型。在 null 安全之前，Object 是 Dart 的顶层类型，Null 是其底层类型。\n由于现在 Object 是不可空的，所以它不再是顶类型。Null 不是它的子类型。Dart 没有命名的顶类型。如果你需要一个顶类型，你要 Object? 同样，Null 也不再是底层类型。如果是的话，一切都还会是 null。相反，我们添加了一个新的底层类型，名为 Never。\n在实践中，这意味着。\n如果你想表明你允许任何类型的值，就用 Object? 而不是 Object. 事实上，使用 Object 就变得很不寻常了，因为该类型意味着 \u0026ldquo;可能是任何可能的值，除了这个奇怪的禁止值 null\u0026rdquo;。\n在极少数情况下，你需要一个底层类型，用 Never 代替 Null。如果你不知道是否需要底层类型，你可能不需要。\n确保正确性 我们将类型的宇宙分为可空和不可空的两半。为了保持健全性和我们的原则，即除非你要求，否则你永远不会在运行时得到一个 null 引用错误，我们需要保证 null 永远不会出现在非 nullable 端的任何类型中。\n摆脱隐式下传，去掉 Null 这个底层类型，涵盖了类型在程序中跨赋值流转和在函数调用中从参数流转到参数的所有主要地方。剩下的主要的 null 可以潜入的地方是当一个变量第一次出现和离开一个函数的时候。所以会出现一些额外的编译错误。\n无效返回 如果一个函数的返回类型是非空的，那么通过该函数的每一条路径都必须到达一个返回值的返回语句。在 null 安全之前，Dart 对于缺失返回的情况非常宽松。比如说\n// Without null safety: String missingReturn() { // No return. } 如果你分析这个，你就会得到一个温柔的提示，也许你忘了一个返回，但如果没有，也没什么大不了的。这是因为如果执行到了函数体的末端，那么 Dart 就会隐式返回 null。由于每个类型都是可空的，所以从技术上讲，这个函数是安全的，尽管它可能不是你想要的。\n对于健全的非可空类型，这个程序是完全错误的，不安全的。在空值安全下，如果一个具有非可空值返回类型的函数不能可靠地返回一个值，你会得到一个编译错误。所谓 \u0026ldquo;可靠\u0026rdquo;，是指语言分析了所有通过函数的控制流路径。只要它们都能返回一些东西，它就满足了。这个分析是相当聪明的，所以即使这个函数也是可以的。\n// Using null safety: String alwaysReturns(int n) { if (n == 0) { return \u0026#39;zero\u0026#39;; } else if (n \u0026lt; 0) { throw ArgumentError(\u0026#39;Negative values not allowed.\u0026#39;); } else { if (n \u0026gt; 1000) { return \u0026#39;big\u0026#39;; } else { return n.toString(); } } } 我们将在下一节更深入地研究新的流程分析。\n未初始化的变量 当你声明一个变量时，如果你没有给它一个显式的初始化器，Dart 默认用 null 初始化变量。这很方便，但如果变量的类型是不可空的，显然是完全不安全的。所以我们必须对不可空值的变量进行严格的规定。\n 顶级变量和静态字段的声明必须有一个初始化器。由于这些变量可以在程序中的任何地方被访问和赋值，编译器不可能保证变量在被使用之前就已经被赋予了一个值。唯一安全的选择是要求声明本身有一个初始化表达式，产生一个正确类型的值。  // Using null safety: int topLevel = 0; class SomeClass { static int staticField = 0; }  实例字段必须在声明时有一个初始化器，使用初始化形式，或者在构造函数的初始化列表中初始化。这有很多行话。下面是例子。  // Using null safety: class SomeClass { int atDeclaration = 0; int initializingFormal; int initializationList; SomeClass(this.initializingFormal) : initializationList = 0; } 换句话说，只要字段在到达构造函数体之前就有一个值，就可以了。\n 局部变量是最灵活的情况。一个不可空的局部变量不需要有一个初始化器。这完全可以。  // Using null safety: int tracingFibonacci(int n) { int result; if (n \u0026lt; 2) { result = n; } else { result = tracingFibonacci(n - 2) + tracingFibonacci(n - 1); } print(result); return result; } 规则只是局部变量在使用前必须肯定分配。我们也可以依靠我所提到的新流分析来实现。只要每个通往变量使用的路径都先初始化它，使用就可以了。\n 可选参数必须有一个默认值。如果你没有为一个可选的位置参数或命名参数传递一个参数，那么语言就会用默认值来填充它。如果你不指定默认值，那么默认的默认值就是 null，如果参数的类型是不可空的，那就飞不起来了。  所以，如果你想让一个参数是可选的，你需要让它变成 null，或者指定一个有效的非空的默认值。\n这些限制听起来很繁琐，但在实践中并不太坏。它们与现有的围绕最终变量的限制非常相似，而且你可能已经使用这些限制多年，甚至没有真正注意到。另外，请记住，这些限制只适用于不可空值的变量。你总是可以让类型可空，然后让默认初始化为空。\n即便如此，这些规则也会造成摩擦。幸运的是，我们有一套新的语言特性来润滑最常见的模式，在这些新的限制下，你的速度变慢了。不过，首先，是时候谈谈流分析了。\n流程分析 控制流分析在编译器中已经存在多年。它大多被用户隐藏起来，在编译器优化过程中使用，但一些新的语言已经开始使用同样的技术来实现可见的语言功能。Dart 已经在类型推广的形式下有了一抹流分析。\n// With (or without) null safety: bool isEmptyList(Object object) { if (object is List) { return object.isEmpty; // \u0026lt;-- OK!  } else { return false; } } 请注意，在标记行中，我们可以在对象上调用 isEmpty。该方法定义在 List 上，而不是 Object 上。这是因为类型检查器会查看程序中所有的 is 表达式和控制流路径。如果某个控制流构造体只有在变量上的某个 is 表达式为真时才会执行，那么在这个构造体里面，变量的类型就会被 \u0026ldquo;推广 \u0026ldquo;到测试类型。\n在这里的例子中，if 语句的 then 分支只有在 object 实际包含一个 list 时才会运行。因此，Dart 将对象推广到 List 类型，而不是其声明的 Object 类型。这是一个方便的功能，但它是相当有限的。在 null 安全之前，下面的功能相同的程序无法工作。\n// Without null safety: bool isEmptyList(Object object) { if (object is! List) return false; return object.isEmpty; // \u0026lt;-- Error! } 同样，只有当对象包含一个 list 时，才能达到.isEmpty 的调用，所以这个程序是动态正确的。但是类型推广规则不够聪明，没有看到返回语句意味着只有当对象是一个 list 时才能到达第二个语句。\n对于空安全，我们把这种有限的分析方法，在几个方面做得更加强大。\n可到达性分析 首先，我们修复了长期以来的抱怨，即类型推广对早期返回和其他无法到达的代码路径并不聪明。当分析一个函数时，它现在会考虑到返回、break、抛出以及函数中任何其他可能提前终止执行的方式。在空安全下，这个函数。\n// Using null safety: bool isEmptyList(Object object) { if (object is! List) return false; return object.isEmpty; } 现在是完全有效的。因为 if 语句会在对象不是 List 时退出函数，所以 Dart 会在第二条语句中促进对象成为 List。这是一个非常好的改进，它帮助了很多 Dart 代码，甚至是与 nullability 无关的东西。\nNever - 不可达到的代码 您也可以对这种可达到性分析进行编程。新的底类型 Never 没有值。(什么样的值同时是 String、bool 和 int 呢？)那么一个表达式具有 Never 类型意味着什么呢？意味着该表达式永远不能成功完成评估。它必须抛出一个异常，中止，或者以其他方式确保期望表达式结果的周围代码永远不会运行。\n事实上，根据语言的规定，抛出表达式的静态类型是 Never。Never 类型在核心库中被声明，你可以将其作为类型注释。也许你有一个帮助函数，以方便抛出某种异常。\n// Using null safety: Never wrongType(String type, Object value) { throw ArgumentError(\u0026#39;Expected $type, but was ${value.runtimeType}.\u0026#39;); } 你可以这样使用。\n// Using null safety: class Point { final double x, y; bool operator ==(Object other) { if (other is! Point) wrongType(\u0026#39;Point\u0026#39;, other); return x == other.x \u0026amp;\u0026amp; y == other.y; } // Constructor and hashCode... } 这个程序分析起来没有错误。请注意，==方法的最后一行访问了其他的.x 和.y。尽管函数没有任何返回或抛出，但它已经被提升为 Point。控制流分析知道，wrongType()的声明类型是 Never，这意味着 if 语句的 then 分支必须以某种方式中止。由于第二条语句只有在 other 是 Point 时才能到达，所以 Dart 提倡使用它。\n换句话说，在你自己的 API 中使用 Never 可以让你扩展 Dart 的可达性分析。\n确定赋值分析 这个我简单的提到了局部变量。Dart 需要确保一个不可空的局部变量在读取之前总是被初始化。我们使用确定赋值分析来尽可能灵活地处理这个问题。该语言分析每个函数体，并通过所有控制流路径跟踪局部变量和参数的赋值。只要在每一条到达某个使用变量的路径上都对变量进行了赋值，就认为该变量已经初始化。这让你可以在没有初始化器的情况下声明一个变量，然后在之后使用复杂的控制流对其进行初始化，即使该变量具有不可空值的类型。\n我们还使用确定赋值分析来使最终变量更加灵活。在空安全之前，如果你需要以任何一种有趣的方式对局部变量进行初始化，那么使用 final 是很困难的。\n// Using null safety: int tracingFibonacci(int n) { final int result; if (n \u0026lt; 2) { result = n; } else { result = tracingFibonacci(n - 2) + tracingFibonacci(n - 1); } print(result); return result; } 这将是一个错误，因为结果变量是 final，但没有初始化器。在空安全下进行更智能的流分析，这个程序是没有问题的。分析可以知道，在每条控制流路径上，result 肯定是精确地初始化了一次，所以标记变量 final 的约束条件是满足的。\n对空检查的类型提升 更加智能的流程分析帮助了很多 Dart 代码，甚至是与空性无关的代码。但我们现在做这些改变并不是偶然的。我们把类型分为可空性和非可空性集。如果你有一个可空类型的值，你就不能真正对它做任何有用的事情。在值为空的情况下，这种限制是好的。它可以防止你崩溃。\n但如果值不是空的，能够把它移到非可空的一面，这样你就可以对它调用方法，这将是一件好事。对于局部变量和参数来说，流程分析是实现这一点的主要方法之一。我们已经扩展了类型提升，还可以查看 == null 和 != null 表达式。\n如果你检查一个可空类型的变量，看看它是不是空的，Dart 就会把这个变量推广到底层的非可空类型。\n// Using null safety: String makeCommand(String executable, [List\u0026lt;String\u0026gt;? arguments]) { var result = executable; if (arguments != null) { result += \u0026#39; \u0026#39; + arguments.join(\u0026#39; \u0026#39;); } } 这里，arguments 有一个可空的类型。通常，这禁止你对它调用.join()。但是由于我们在 if 语句中对该调用进行了保护，检查以确保该值不是空的，所以 Dart 将其从 List\u0026lt;String\u0026gt; 提升为 List\u0026lt;String\u0026gt;，并允许你在其上调用方法或将其传递给期望非空值列表的函数。\n这听起来是一件相当小的事情，但这种基于流程的对 null 检查的推广是使大多数现有 Dart 代码在 null 安全下工作的原因。大多数 Dart 代码都是动态正确的，并且确实通过在调用方法之前检查空值来避免抛出空值引用错误。新的关于 null 检查的流程分析将这种动态正确性变成了可证明的静态正确性。\n当然，它也能配合我们对可到达性的更智能的分析。上面的函数也可以写成一样。\n// Using null safety: String makeCommand(String executable, [List\u0026lt;String\u0026gt;? arguments]) { var result = executable; if (arguments == null) return result; return result + \u0026#39; \u0026#39; + arguments.join(\u0026#39; \u0026#39;); } 语言也比较聪明，什么样的表达方式会引起推广。显式 == null 或 != null 当然可以。但是，使用 as、assignments 或我们即将提到的后缀 ! 操作符的显式投掷也会导致提升。总的目标是，如果代码是动态正确的，而且静态地找出这一点是合理的，分析应该足够聪明。\n不必要的代码警告 拥有更智能的可达性分析，并知道 null 可能流经你的程序的地方，有助于确保你添加代码来处理 null。但我们也可以用同样的分析来检测你不需要的代码。在 null 安全之前，如果你写了这样的东西。\n// Using null safety: String checkList(List list) { if (list?.isEmpty) { return \u0026#39;Got nothing\u0026#39;; } return \u0026#39;Got something\u0026#39;; } Dart 没有办法知道那个 null-aware? 操作符是否有用。它只知道，你可以把 null 传给函数。但是在 null safe Dart 中，如果你用现在不可空的 List 类型注释了那个函数，那么它知道 list 永远不会是 null。这就意味着这个 ?. 永远不会做任何有用的事情，你可以而且应该只使用 . 类型。\n为了帮助你简化你的代码，我们已经为这样的不必要的代码添加了警告，现在静态分析已经精确到可以检测到它了。在一个不可空类型上使用一个 null-aware 操作符，甚至是像== null 或 != null 这样的检查，都会被报告为一个警告。\n当然，这也与非空类型的晋升有关。一旦一个变量被推广到一个不可空类型，如果你再次对它进行多余的 null 检查，你会得到一个警告。\n// Using null safety: checkList(List? list) { if (list == null) return \u0026#39;No list\u0026#39;; if (list?.isEmpty) { return \u0026#39;Empty list\u0026#39;; } return \u0026#39;Got something\u0026#39;; } 你在这里得到了一个警告，因为在它执行的时候，我们已经知道 list 不能为空。这些警告的目的不仅仅是清理无意义的代码。通过删除不需要的 null 检查，我们确保剩下的有意义的检查能够脱颖而出。我们希望您能够查看您的代码，并看到 null 可以在哪里流动。\n使用可空类型 我们现在已经把 null 收进了可空类型的集合。通过流程分析，我们可以安全地让一些非空值越过栅栏跳到非可空类型的一边，在那里我们可以使用它们。这是一个很大的进步，但如果我们在这里停下来，所产生的系统仍然是痛苦的限制。流程分析只对局部和参数有帮助。\n为了尽量恢复 Dart 在 null 安全之前的灵活性\u0026ndash;并且在某些地方超越它，我们有一些其他的新特性。\n更加智能的空感知方法 Dart 的 null aware 操作符 ?. 比 null safety 更早。运行时语义规定，如果接收者为空，那么右侧的属性访问将被跳过，表达式评价为空。\n// Without null safety: String notAString = null; print(notAString?.length); 这不是抛出一个异常，而是打印 \u0026ldquo;null\u0026rdquo;。null-aware 操作符是一个很好的工具，它使可空类型在 Dart 中可用。虽然我们不能让你在可空类型上调用方法，但我们可以也确实让你在它们上使用 null-aware 操作符。空值后安全版本的程序是。\n// Using null safety: String? notAString = null; print(notAString?.length); 它的工作原理和之前的一样。\n然而，如果你曾经在 Dart 中使用过 null-aware 操作符，当你在方法链中使用它们时，你可能会遇到一个烦恼。比方说，你想看看一个可能不存在的字符串的长度是否是一个偶数（不是一个特别现实的问题，我知道，但请和我一起工作）。\n// Using null safety: String? notAString = null; print(notAString?.length.isEven); 即使这个程序使用了?，但在运行时还是会抛出一个异常。问题在于.isEven 表达式的接收者是它左边的整个 notAString?.length 表达式的结果。该表达式的值为 null，所以我们在尝试调用.isEven 时得到一个空值引用错误。如果你曾经在 Dart 中使用过?.，你可能学到了一个苦涩的方法，那就是在你使用过一次之后，你必须将 null-aware 操作符应用到链中的每个属性或方法。\nString? notAString = null; print(notAString?.length?.isEven); 这很烦人，但更糟糕的是，它掩盖了重要信息。考虑一下：\n// Using null safety: showGizmo(Thing? thing) { print(thing?.doohickey?.gizmo); } 我有个问题要问你。Thing 上的 doohickey getter 可以返回 null 吗？看起来可以，因为你在结果上使用了?。但可能只是第二个?.只是为了处理 thing 为 null 的情况，而不是 doohickey 的结果。你无法判断。\n为了解决这个问题，我们借鉴了 C#设计相同功能的一个聪明的想法。当你在一个方法链中使用一个 null-aware 操作符时，如果接收者评估为 null，那么整个方法链的其余部分都会被短路并跳过。这意味着如果 doohickey 有一个不可空的返回类型，那么你可以也应该写。\n// Using null safety: showGizmo(Thing? thing) { print(thing?.doohickey.gizmo); } 事实上，如果你不这样做，你会在第二个?上得到一个不必要的代码警告。如果你看到这样的代码。\n// Using null safety: showGizmo(Thing? thing) { print(thing?.doohickey?.gizmo); } 那么你就可以肯定地知道，这意味着 doohickey 本身有一个可空的返回类型。每一个?对应一个可以导致 null 流入方法链的唯一路径。这使得方法链中的 null-aware 操作符既更简洁又更精确。\n在这时，我们又增加了几个其他的空感知操作符。\n// Using null safety:  // Null-aware cascade: receiver?..method(); // Null-aware index operator: receiver?[index]; 没有一个 null-aware 函数调用操作符，但你可以写。\n// Allowed with or without null safety: function?.call(arg1, arg2); Null 断言运算符 使用流式分析将一个可空型变量移到非可空型变量的伟大之处在于，这样做被证明是安全的。你可以在之前的可空型变量上调用方法，而不会放弃非可空型的任何安全或性能。\n但是，可空类型的许多有效用途无法以取悦静态分析的方式证明其安全性。比如说\n// Using null safety, incorrectly: class HttpResponse { final int code; final String? error; HttpResponse.ok() : code = 200; HttpResponse.notFound() : code = 404, error = \u0026#39;Not found\u0026#39;; String toString() { if (code == 200) return \u0026#39;OK\u0026#39;; return \u0026#39;ERROR $code${error.toUpperCase()}\u0026#39;; } } 如果你尝试运行这个，你在调用 toUpperCase()时得到一个编译错误。错误字段是可空的，因为它在成功的响应中不会有一个值。我们通过检查类可以看到，当错误信息为空时，我们永远不会访问它。但这需要理解代码的值和错误的可空性之间的关系。类型检查器是看不到这种联系的。\n换句话说，我们这些代码的人类维护者知道错误在我们使用它的时候不会是空的，我们需要一种方法来断言这一点。通常情况下，你使用 as cast 来断言类型，在这里你也可以做同样的事情。\n// Using null safety: String toString() { if (code == 200) return \u0026#39;OK\u0026#39;; return \u0026#39;ERROR $code${(error as String).toUpperCase()}\u0026#39;; } 如果投递失败，将错误地投递到不可空的 String 类型，会抛出一个运行时异常。否则，它将为我们提供一个非空值字符串，我们可以在其上调用方法。\n\u0026ldquo;投弃可空性 \u0026ldquo;经常出现，以至于我们有了一种新的速记语法。一个后缀的感叹号 (!) 将左边的表达式并将其投射到其底层的不可空类型上。所以上面的函数相当于\n// Using null safety: String toString() { if (code == 200) return \u0026#39;OK\u0026#39;; return \u0026#39;ERROR $code${error!.toUpperCase()}\u0026#39;; } 当底层类型是啰嗦的时候，这个单字符的 \u0026ldquo;bang 操作符 \u0026ldquo;特别方便。如果仅仅为了从某个类型中投弃一个单一的?，而不得不写成 Map\u0026lt;TransactionProviderFactory, List\u0026lt;Set\u0026gt;，那就真的很烦人了。\n当然，就像任何投射一样，使用 !的同时也会损失静态安全。必须在运行时检查投射以保持合理性，而且可能会失败并抛出一个异常。但是你可以控制这些转码被插入的位置，你可以通过查看你的代码随时看到它们。\nLate 变量 类型检查器不能证明代码安全的最常见的地方是围绕顶层变量和字段。下面是一个例子。\n// Using null safety, incorrectly: class Coffee { String _temperature; void heat() { _temperature = \u0026#39;hot\u0026#39;; } void chill() { _temperature = \u0026#39;iced\u0026#39;; } String serve() =\u0026gt; _temperature + \u0026#39; coffee\u0026#39;; } main() { var coffee = Coffee(); coffee.heat(); coffee.serve(); } 这里，heat()方法是在服务()之前调用的。这意味着 _temperature 在使用之前会被初始化为一个非空值。但是静态分析来确定这一点是不可行的。(对于像这个微不足道的例子来说可能是可行的，但是一般情况下，试图跟踪一个类的每一个实例的状态是难以解决的。)。\n因为类型检查器不能分析字段和顶层变量的用途，它有一个保守的规则，即不可空值字段必须在声明时初始化（或者在实例字段的构造函数初始化列表中）。所以 Dart 在这个类上报告了一个编译错误。\n你可以通过使字段可空，然后在用途上使用 null 断言操作符来修复这个错误。\n// Using null safety: class Coffee { String? _temperature; void heat() { _temperature = \u0026#39;hot\u0026#39;; } void chill() { _temperature = \u0026#39;iced\u0026#39;; } String serve() =\u0026gt; _temperature! + \u0026#39; coffee\u0026#39;; } 这样做很好，但它给类的维护者发出了一个混乱的信号。但它给类的维护者发出了一个混乱的信号。通过标记 _temperature 为 nullable，你暗示 null 对于该字段来说是一个有用的、有意义的值。但这不是我们的目的。_temperature 字段永远不应该在其 null 状态下被观察到。\n为了处理常见的延迟初始化的状态模式，我们添加了一个新的修饰符 late。你可以像这样使用它。\n// Using null safety: class Coffee { late String _temperature; void heat() { _temperature = \u0026#39;hot\u0026#39;; } void chill() { _temperature = \u0026#39;iced\u0026#39;; } String serve() =\u0026gt; _temperature + \u0026#39; coffee\u0026#39;; } 请注意，_temperature 字段的类型不可空，但没有初始化。另外，在使用它的时候也没有明确的空断言。有一些模型可以应用到迟到的语义中，但我是这样想的。晚期修饰符的意思是 \u0026ldquo;在运行时执行这个变量的约束，而不是在编译时执行\u0026rdquo;。这几乎就像 \u0026ldquo;late \u0026ldquo;这个词描述的是什么时候执行变量的保证。\n在这种情况下，由于该字段还没有确定初始化，所以每次读取该字段时，都会插入一个运行时检查，以确保它已经被分配了一个值。如果没有，就会抛出一个异常。给变量类型为 String 意味着 \u0026ldquo;你应该永远不会看到我的值不是字符串\u0026rdquo;，而后期修饰符意味着 \u0026ldquo;在运行时验证\u0026rdquo;。\n在某些方面，迟到修饰符比使用? \u0026ldquo;神奇\u0026rdquo;，因为对字段的任何使用都可能失败，而且在使用现场没有任何文字可见。但你确实必须在声明处写晚期才能得到这种行为，我们的信念是，在那里看到修饰符已经足够明确，这一点是可以维护的。\n作为回报，你可以得到比使用可空类型更好的静态安全。因为现在字段的类型是非可空的，所以试图将 null 或可空的 String 分配给字段是一个编译错误。晚期修饰符可以让你推迟初始化，但仍然禁止你把它当作一个可空变量来处理。\n惰性初始化 late 修饰符也有一些其他的特殊能力。这可能看起来很矛盾，但你可以在一个有初始化器的字段上使用 late。\n// Using null safety: class Weather { late int _temperature = _readThermometer(); } 当你这样做的时候，初始化器会变得懒惰。它不是在实例构造完成后立即运行，而是在第一次访问该字段时就延迟并缓慢运行。换句话说，它的工作方式与顶层变量或静态字段的初始化器完全一样。当初始化表达式的成本很高且可能不需要时，这可以很方便。\n当你在实例字段上使用后期，懒惰地运行初始化器会给你一个额外的奖励。通常实例字段初始化器不能访问这个，因为在所有字段初始化器完成之前，你不能访问新对象。但有了迟来的字段，就不再是这样了，所以你可以访问这个，调用方法，或者访问实例上的字段。\nlate final 变量 你也可以把 late 和 final 结合起来。\n// Using null safety: class Coffee { late final String _temperature; void heat() { _temperature = \u0026#39;hot\u0026#39;; } void chill() { _temperature = \u0026#39;iced\u0026#39;; } String serve() =\u0026gt; _temperature + \u0026#39; coffee\u0026#39;; } 与普通的 final 字段不同，您不必在声明中或在构造函数初始化列表中初始化该字段。你可以在以后的运行时对它进行赋值。但是你只能对它赋值一次，而且这个事实会在运行时被检查。如果你试图对它进行不止一次的赋值，就像这里同时调用 heat() 和 chill() 一样，第二次赋值就会抛出一个异常。这是对最终被初始化且之后不可改变的状态进行建模的好方法。\n换句话说，新的 late 修饰符结合 Dart 的其他变量修饰符，覆盖了 Kotlin 中 lateinit 和 Swift 中 lazy 的大部分功能空间。如果你想在局部变量上使用它，你甚至可以在局部变量上使用它，如果你想进行一点局部的懒惰评估。\n所需的命名参数 为了保证你永远不会看到一个具有不可空类型的空参数，类型检查器要求所有可选参数要么具有可空类型，要么具有默认值。如果你想让一个命名的参数有一个可空的类型而没有默认值呢？那就意味着你想要求调用者总是传递它。换句话说，你想要一个命名的参数，但不是可选的。\n我用这个表直观地展示了 Dart 参数的各种类型。\n mandatory optional +------------+------------+ positional | f(int x) | f([int x]) | +------------+------------+ named | ??? | f({int x}) | +------------+------------+ 由于不清楚的原因，Dart 长期以来一直支持这个表的三个角，但把 named+mandatory 的组合空了。在空安全的情况下，我们填补了这一点。你在参数前放上 required，就可以声明一个必要的命名参数。\n// Using null safety: function({int? a, required int? b, int? c, required int? d}) {} 在这里，所有的参数都必须以名字传递。参数 a 和 c 是可选的，可以省略。参数 b 和 d 是必填的，必须传递。注意，要求性与可空性无关。可空类型的命名参数可以是必需的，不可空类型的命名参数可以是可选的（如果它们有一个默认值）。\n这是另一个我认为无论空值安全性如何都能让 Dart 变得更好的特性之一。它只是让我觉得这个语言更加完整。\n抽象字段 Dart 的一个特点是它坚持了一个叫做统一访问原则的东西。用人话说就是字段与 getter 和 setter 是没有区别的。在某个 Dart 类中的 \u0026ldquo;属性 \u0026ldquo;是计算还是存储，这是一个实现细节。正因为如此，在使用抽象类定义接口的时候，一般都会使用字段声明。\nabstract class Cup { Beverage contents; } 其目的是让用户只实现该类，而不要扩展它。字段语法只是写一个 getter/setter 对的较短方式。\nabstract class Cup { Beverage get contents; set contents(Beverage); } 但 Dart 不知道这个类永远不会被用作具体类型。它把那个内容声明看作是一个真实的字段。而且，不幸的是，这个字段是不可空的，也没有初始化器，所以你得到一个编译错误。\n一个解决方法是使用显式的抽象 getter/setter 声明，就像第二个例子中那样。但这有点啰嗦，所以在 null 安全的情况下，我们还增加了对显式抽象字段声明的支持。\nabstract class Cup { abstract Beverage contents; } 这和第二个例子的行为完全一样。它只是用给定的名称和类型声明了一个抽象的 getter 和 setter。\n使用可空字段 这些新特性涵盖了许多常见的模式，并且在大多数时间里，让处理 null 的工作变得相当轻松。但即便如此，我们的经验是，可空字段仍然是困难的。在你能让字段迟到且不可空的情况下，你是金子般的存在。但在很多情况下，你需要检查字段是否有值，这就需要让它可空，这样你就可以观察到空。\n你可能会期望这样做是可行的。\n// Using null safety, incorrectly: class Coffee { String? _temperature; void heat() { _temperature = \u0026#39;hot\u0026#39;; } void chill() { _temperature = \u0026#39;iced\u0026#39;; } void checkTemp() { if (_temperature != null) { print(\u0026#39;Ready to serve \u0026#39; + _temperature + \u0026#39;!\u0026#39;); } } String serve() =\u0026gt; _temperature! + \u0026#39; coffee\u0026#39;; } 在 checkTemp()里面，我们检查 _temperature 是否为空。如果不是，我们访问它，并最终调用+。不幸的是，这是不允许的。基于流式的类型推广并不适用于字段，因为静态分析无法证明字段的值在你检查 null 和你使用它的点之间没有变化。考虑到在病理情况下，字段本身可能会被子类中的 getter 覆盖，在第二次调用时返回 null）。\n所以，既然我们关心健全性，那么字段就不会推广，上面的方法就不会编译。这是很烦人的。在像这里这样简单的情况下，你最好的选择是在字段的使用上打上一个! 这似乎是多余的，但这多少是 Dart 如今的行为方式。\n另一个有用的模式是先把字段复制到一个本地变量中，然后再使用它。\n// Using null safety: void checkTemp() { var temperature = _temperature; if (temperature != null) { print(\u0026#39;Ready to serve \u0026#39; + temperature + \u0026#39;!\u0026#39;); } } 由于类型推广确实适用于本地人，所以现在可以正常使用。如果你需要改变值，只要记得存储回字段，而不仅仅是本地。\n无效性和属性 像大多数现代静态类型的语言一样，Dart 有通用类和通用方法。它们与可空性的交互方式有一些看似反直觉的地方，但一旦你想清楚了其中的含义，就会明白。首先是 \u0026ldquo;这个类型是可空性的吗？\u0026ldquo;不再是一个简单的是或否的问题。考虑一下。\n// Using null safety: class Box\u0026lt;T\u0026gt; { final T object; Box(this.object); } main() { Box\u0026lt;String\u0026gt;(\u0026#39;a string\u0026#39;); Box\u0026lt;int?\u0026gt;(null); } 在 Box 的定义中，T 是一个可空类型还是一个不可空类型？正如你所看到的，它可以被实例化为任何一种类型。答案是 T 是一个潜在的可空类型。在一个通用类或方法的主体中，潜在可空类型具有可空类型和不可空类型的所有限制。\n前者意味着除了在 Object 上定义的少量方法外，你不能调用它的任何方法。后者意味着你必须在使用该类型的任何字段或变量之前初始化它们。这可能会使类型参数变得相当难处理。\n在实践中，有几种模式表现出来。在类似集合的类中，类型参数可以用任何类型实例化，你只需要处理这些限制。在大多数情况下，就像这里的例子一样，这意味着只要你需要处理一个类型参数的值，就必须确保你确实可以访问这个类型参数的值。幸运的是，集合类很少对其元素调用方法。\n在你无法访问一个值的地方，你可以使类型参数的使用是可空的。\n// Using null safety: class Box\u0026lt;T\u0026gt; { T? object; Box.empty(); Box.full(this.object); } 注意对象声明上的? 现在这个字段有一个显式可空的类型，所以可以不初始化它。\n当你使一个类型参数类型像这里的 T? 一样可空的时候，你可能需要把可空性抛掉。正确的方法是使用显式为 T 的转写，而不是使用 !操作符。\n// Using null safety: class Box\u0026lt;T\u0026gt; { final T? object; Box.empty(); Box.full(this.object); T unbox() =\u0026gt; object as T; } !操作符总是在值为 null 时抛出。但是如果类型参数已经被实例化为一个可空类型，那么 null 对于 T 来说是一个完全有效的值。\n// Using null safety: main() { var box = Box\u0026lt;int?\u0026gt;.full(null); print(box.unbox()); } 这个程序应该无误地运行。使用 T 就可以实现这一点。使用 !会抛出一个异常。\n其他通用类型有一些约束，限制了可以应用的类型参数的种类。\n// Using null safety: class Interval\u0026lt;T extends num\u0026gt; { T min, max; Interval(this.min, this.max); bool get isEmpty =\u0026gt; max \u0026lt;= min; } 如果绑定是不可空的，那么类型参数也是不可空的，这意味着你有不可空类型的限制\u0026ndash;你不能让字段和变量不初始化。这意味着你有不可空值类型的限制\u0026ndash;你不能让字段和变量不初始化。这里的示例类必须有一个初始化字段的构造函数。\n作为这种限制的回报，你可以调用在其绑定上声明的参数类型的值的任何方法。然而，拥有一个不可空的绑定确实会阻止你的通用类的用户用一个可空的类型参数来实例化它。对于大多数类来说，这可能是一个合理的限制。\n你也可以使用一个可空的绑定。\n// Using null safety: class Interval\u0026lt;T extends num?\u0026gt; { T min, max; bool get isEmpty { var localMin = min; var localMax = max; // No min or max means an open-ended interval.  if (localMin == null || localMax == null) return false; return localMax \u0026lt;= localMin; } } 这意味着在类的主体中，你可以灵活地将类型参数处理为 nullable。请注意，这次我们没有构造函数，这也没关系。字段将被隐式初始化为 null。你可以声明类型参数类型的未初始化变量。\n但是你也有 nullability 的限制\u0026ndash;你不能在该类型的变量上调用任何东西，除非你先处理好 nullability。在这里的例子中，我们复制局部变量中的字段，并检查这些局部变量是否为空，以便在使用\u0026lt;=之前，流分析将它们推广到非可空性类型。\n请注意，可空性绑定并不妨碍用户用非空类型实例化类。一个可空的边界意味着类型参数可以是可空的，而不是说它必须是可空的。事实上，如果你不写扩展子句，类型参数的默认约束是可空值约束 Object? 没有办法要求类型参数是可空的。如果你想让类型参数的使用可靠地是可空的，你可以在类的主体里面使用 T?\n核心库的变化 语言中还有一些其他的调整，但都是次要的。例如，没有 on 子句的 catch 的默认类型现在是 Object 而不是动态的。开关语句中的跌穿分析使用了新的流分析。\n剩下的真正对你有意义的变化是在核心库中。在我们开始进行空值安全大冒险之前，我们担心原来没有办法在不大规模破坏世界的情况下让我们的核心库实现空值安全。结果并没有那么可怕。有一些重大的变化，但大多数情况下，迁移很顺利。大多数核心库要么不接受 null，自然而然地迁移到非可空类型，要么接受并优雅地用可空类型接受它。\n不过有几个重要的角落。\nMap 索引操作符是可空的 这并不是真正的改变，更多的是一个需要知道的事情。Map 类的 index [] 操作符如果键不存在，则返回 null。这意味着该操作符的返回类型必须是可空的。V?\n我们可以将该方法改为当键不存在时抛出一个异常，然后给它一个更容易使用的非空值返回类型。但是，使用索引操作符并检查 null 以查看键是否不存在的代码是非常常见的，根据我们的分析，约占所有使用的一半。打破所有这些代码会让 Dart 生态系统燃起熊熊大火。\n相反，运行时的行为是一样的，因此返回类型必须是可空的。这意味着你一般不能立即使用 map 查询的结果。\n// Using null safety, incorrectly: var map = {\u0026#39;key\u0026#39;: \u0026#39;value\u0026#39;}; print(map[\u0026#39;key\u0026#39;].length); // Error. 当你试图在一个可空字符串上调用.length 时，会出现编译错误。在你知道键存在的情况下，你可以通过使用！.length 来教导类型检查器。\n// Using null safety: var map = {\u0026#39;key\u0026#39;: \u0026#39;value\u0026#39;}; print(map[\u0026#39;key\u0026#39;]!.length); // OK. 我们考虑过在 Map 中添加另一个方法来为你做这件事：查找 key，如果没有找到就抛出，否则就返回一个非空值。但是该怎么称呼它呢？没有一个名字比单字符的 !更短，也没有一个方法的名字比在调用现场看到一个内置语义的 !更清晰。所以，在地图中访问一个已知存在元素的习惯性方式是使用[]! 你会习惯的。\n没有未命名的 List 构造函数 List 上的未命名构造函数创建了一个给定大小的新列表，但没有初始化任何元素。如果你创建了一个非可空类型的 list，然后访问一个元素，这将会在健全性保证中留下一个非常大的漏洞。\n为了避免这种情况，我们完全删除了构造函数。在空安全代码中调用 List()是一个错误，即使是可空类型。这听起来很吓人，但实际上，大多数代码都是使用 list literals、List.filled()、List.generate()或者作为转换其他集合的结果来创建列表的。对于想要创建某种类型的空列表的边缘情况，我们添加了一个新的 List.empty()构造函数。\n在 Dart 中，创建一个完全未初始化的列表的模式一直让人觉得格格不入，现在更是如此。如果你的代码被这一点破坏了，你可以随时通过使用许多其他的方法来产生一个列表来修复它。\n不能在不可空的列表上设置较大的长度 这一点鲜为人知，但 List 上的 length getter 也有一个相应的 setter。你可以将长度设置为一个较短的值来截断列表。你也可以将它设置为一个较长的长度，以便用未初始化的元素填充列表。\n如果你对一个非空值类型的列表这样做，当你以后访问那些未写入的元素时，你会违反健全性。为了防止这种情况发生，如果（也只有当）列表的元素类型是不可空的，而你又将其设置为较长的长度时，长度设置器会抛出一个运行时异常。截断所有类型的列表仍然是可以的，你可以增长可空类型的列表。\n如果你定义了自己的列表类型，扩展了 ListBase 或应用了 ListMixin，那么这有一个重要的后果。这两种类型都提供了 insert()的实现，之前通过设置长度为插入的元素腾出空间。这样做会因空安全而失败，所以我们将 ListMixin（ListBase 共享）中 insert()的实现改为调用 add()。如果你想能够使用继承的 insert()方法，你的自定义列表类应该提供 add()的定义。\n不能在迭代之前或之后访问 Iterator.current Iterator 类是一个可变的 \u0026ldquo;游标 \u0026ldquo;类，用于遍历实现 Iterable 的类型的元素。在访问任何元素之前，你应该调用 moveNext()来前进到第一个元素。当该方法返回 false 时，你已经到达了终点，没有更多的元素。\n过去，如果你在第一次调用 moveNext()之前或在迭代结束后调用它，current 会返回 null。有了 null 安全，那就要求 current 的返回类型是 E? 而不是 E。这又意味着每个元素的访问都需要进行运行时空检查。\n鉴于几乎没有人以那种错误的方式访问当前元素，这些检查将毫无用处。由于在迭代之前或之后可能会有一个该类型的值，所以我们让迭代器的行为没有被定义，如果你在不应该调用它的时候调用它。大多数 Iterator 的实现都会抛出一个 StateError。\n总结 这是一个非常详细的关于 null 安全的语言和库变化的介绍。这是一个很大的东西，但这是一个相当大的语言变化。更重要的是，我们希望达到一个点，让 Dart 仍然感觉到凝聚力和可用性。这不仅需要改变类型系统，还需要改变其他一些围绕它的可用性功能。我们不希望它让人感觉像被栓上了 null safety。\n要带走的核心点是。\n  类型在默认情况下是不可空值的，而通过添加 ?. 来实现空值化。\n  可选参数必须是可空的，或者有一个默认值。可以使用 required 使命名参数成为非可选参数。不可空值的顶层变量和静态字段必须有初始化器。不可空值的实例字段必须在构造函数主体开始之前初始化。\n  如果接收者为空，则空感知操作符后的方法链会短路。有新的空感知级联(?..)和索引(?[])运算符。后缀的空断言 \u0026ldquo;bang\u0026rdquo; 运算符(!)将其可空操作数投射到底层的非可空类型。\n  流程分析让你可以安全地将可空的局部变量和参数转化为可用的非可空变量。新的流分析还对类型提升、缺失返回、不可达代码和变量初始化有更智能的规则。\n  late 修饰符让你可以在其他地方使用不可空类型和 final，否则你可能无法使用，但会牺牲运行时检查。它还为你提供了惰性初始化的字段。\n  List 类被修改为防止未初始化元素。\n  最后，一旦你吸收了所有这些，并让你的代码进入 null 安全的世界，你就会得到一个健全的程序，编译器可以优化，并且在你的代码中可以看到每一个可能发生运行时错误的地方。我们希望你觉得这样的努力是值得的。\n原文链接: https://dart.dev/null-safety/understanding-null-safety\n","permalink":"https://ohmyweekly.github.io/notes/2021-03-05-understanding-null-safety/","tags":["Dart","Dart 官方文档"],"title":"Understanding Null Safety"},{"categories":["Raku"],"contents":"Raku 有非常好的 Unicode 支持, 但是输入 Unicode 一直是一个很大的问题。在 REPL 中, 输入 Unicode 都不太方便。下面介绍几种方法来方便地输入 Unicode。\nComma 最方便的是使用 Raku 自己的 IDE, Comma。但是需要做两个设置:\n首先, 切换到 Comma IDE, 点击左上角菜单栏的 Comma 选项, 在弹出的 Preference 窗口中, 找到左侧菜单栏中的 Editor 选项, 点击 Code Style 中的 Raku, 在右侧的窗口的最上面, 找到 Schema 一行, 点击倒三角图标, 在下拉菜单中选择 Project。然后在第二行文字中找到 Editor Behavior, 勾选 Convert operators to Unicode。完成后点击右下角的 Apply, 然后点击 OK。\n这样当你在 Comma 中键入 \u0026gt;\u0026gt; 这样的运算符后, Comma 会自动把它转为 Unicode 形式的 »。目前已支持大部分 Unicode 运算符了。Comma 是跨平台的 Raku 编辑器, 支持 Windows、Linux 和 MacOS, 所以你想跨平台开发的话, Comma 值得一试。\nKitty 如果你想在 Vim 中开发 Raku, 又不想失去输入 Unicode 的乐趣, 请使用 Kitty 终端。点开 Kitty 终端, 使用快捷键 Ctrl + Shift + U 即可触发输入 Unnicode 的界面(也可以键入命令 kitty +kitten unicode_input 来触发)。再通过快捷键 F1、F2、F3 和 F4 (或通过 Ctrl+[ 和 Ctrl+] 切换标签)可以分别按代码搜索 Unicode、按名字搜索 Unicode、按表情搜索和收藏。\n例如在 REPL 中键入 Unicode 字符:\n$ raku Welcome to 𝐑𝐚𝐤𝐮𝐝𝐨™ v2021.02. Implementing the 𝐑𝐚𝐤𝐮™ programming language v6.d. Built on MoarVM version 2021.02. To exit type \u0026#39;exit\u0026#39; or \u0026#39;^D\u0026#39; \u0026gt; my @a = 1..5; \u0026gt; @a».sin 借助 Kitty, 数组 @a 后面的 » 可以这样输入, 先输入 @a, 然后使用快捷键 Shift + Ctrl + U 调出 Kitty 的 Unicode input 界面, 一般有两种方法, 第一种是通过快捷键 F1, 然后输入对应的 Unicode 编码, 再回车。第二种是通过快捷键 F2, 通过名字搜索, 例如输入单词 quota, 会出现一堆引号符号供你选择, 按上下方向键选中 », 再回车即可。Vim 中同理。\nJulia REPL 在 REPL 中输入 \\′ + 单词 + Tab, 例如输入 \\heartsuit` 再按 Tab, 就会自动变成 ♡\njulia\u0026gt; \\heartsuit IPython \\^ 输入上标, \\_p 输入下标。\n$ ipython \\^a \\_p VScode \\\\′ + 完整的单词 + Tab, VsCode 会自动转换为 Unicode。 例如 ^a和_p`\ndmenu #!/usr/bin/perl #use strict; #use warnings; #use autodie; #use utf8; open $FH, \u0026#34; | dmenu -i -l 30 -fn \u0026#39;-xos4-terminus-medium-r-*-*-10-*\u0026#39; | sed \u0026#39;s/^\\\\(.\\\\).*/\\\\1/\u0026#39; | tr -d \u0026#39;\\n\u0026#39; | { pbcopy; pbpaste; } \u0026#34;; # if you do not have the xos4-terminus font that makes characters a little bigger : # uncoment this line and use the second here document #open $FH, \u0026#34; | dmenu -i -l 30 | sed \u0026#39;s/^\\\\(.\\\\).*/\\\\1/\u0026#39; | tr -d \u0026#39;\\n\u0026#39; | { xsel -ib; xsel -ip; } \u0026#34;; # also you can compile dmenu from source and change the font size directly in the C done # in the file config.h / config.def.h # this heredoc is well aligned with the -xos4-terminus-medium-r-*-*-10-* font print $FH \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; « hyper operator » hyper operator ∈ membership ∉ non-membership ∪ set union ∩ set intersection ∖ set difference ≡ set equality ≢ set inequality ⊖ symmetric set difference ⊍ baggy multiplication ⊎ baggy addition ∅ empty set ⊆ subset ⊈ not a subset ⊂ strict subset ⊄ not a strict subset ∋ reverse membership ∌ reverse non-membership ⊇ superset ⊉ not a superset ⊃ strict superset ⊅ not a strict superset ∞ infinity ⚛ atomic operator π pi τ tau 𝑒 Euler\u0026#39;s number ∘ function composition × multiplication ÷ division ≤ inferior or equal ≥ greater or equal ≠ inequality − substraction ≅ approximatively equal … sequence operator ‘ left single quotation mark ’ right single quotation mark ‚ single low-9 quotation mark ” right double quotation mark “ left double quotation mark „ double low-9 quotation mark ｢ halfwidth left corner bracket ｣ halfwidth right corner bracket ⁺ plus superscript ⁻ minus superscript ⁰ 0 superscript ¹ 1 superscript ² 2 superscript ³ 3 superscript ⁴ 4 superscript ⁵ 5 superscript ⁶ 6 superscript ⁷ 7 superscript ⁸ 8 superscript ⁹ 9 superscript EOF ; # use this without -xos4-terminus-medium-r-*-*-10-* font # the default font may change given which font packages are installed and given the font configurations #print $FH \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; #« hyper operator #» hyper operator #∈ membership #∉ non-membership #∪ set union #∩ set intersection #∖ set difference #≡ set equality #≢ set inequality #⊖ symmetric set difference #⊍ baggy multiplication #⊎ baggy addition #∅ empty set #⊆ subset #⊈ not a subset #⊂ strict subset #⊄ not a strict subset #∋ reverse membership #∌ reverse non-membership #⊇ superset #⊉ not a superset #⊃ strict superset #⊅ not a strict superset #∞ infinity #⚛ atomic operator #π pi #τ tau #𝑒 Euler\u0026#39;s number #∘ function composition #× multiplication  #÷ division #≤ inferior or equal #≥ greater or equal #≠ inequality #− substraction #≅ approximatively equal #… sequence operator #‘ left single quotation mark #’ right single quotation mark #‚ single low-9 quotation mark  #” right double quotation mark #“ left double quotation mark #„ double low-9 quotation mark #｢ halfwidth left corner bracket #｣ halfwidth right corner bracket #⁺ plus superscript #⁻ minus superscript #⁰ 0 superscript #¹ 1 superscript  #² 2 superscript  #³ 3 superscript  #⁴ 4 superscript  #⁵ 5 superscript  #⁶ 6 superscript  #⁷ 7 superscript  #⁸ 8 superscript  #⁹ 9 superscript  #EOF #; close $FH; __END__ =pod =head1 NAME lazy_raku_unicode.pl =head1 SYNOPSIS This script allows you to select a Unicode symbol from a drop-down menu that you can simply paste into your text editor, terminal or IDE. All it does is send a heredoc through a pipe to dmenu, and copy to the clipboard the character that you selected. Dmenu is a program that reads lines from stdin, display them in a menu, and write to stdout the lines selected. It is not practical to execute this script from a terminal each time you want a character though. This is why it should be executed by sxhkd. Sxhkd is the program that will call this script each time you press the right key combination. Only the most useful Unicode characters are present to not make dmenu get too slow. (It is usually very fast when there is only ascii text.) This include most of the Raku operators that have a Unicode version, plus some quoting characters, superscripts, and mathematical constants. =head1 MANUAL INSTALLATION STEPS =head2 INSTALLING DMENU B\u0026lt;On Debian or Ubuntu :\u0026gt; sudo apt install suckless-tools B\u0026lt;On Archlinux :\u0026gt; sudo pacman -Sy dmenu B\u0026lt;Compile it from source :\u0026gt; https://tools.suckless.org/dmenu/ =head2 CHANGING DMENU FONT SIZE (OPTIIONAL) You have only two ways of changing the font size : Change the default font with the B\u0026lt;-fn\u0026gt; flag (See dmenu(1) for the specifics). dmenu -l 30 -fn \u0026#39;-xos4-terminus-medium-r-*-*-10-*\u0026#39; Modify directly the C source code of dmenu. Only the files B\u0026lt;config.h / config.def.h\u0026gt;. Simply edit this line and change 10 by 14 for example. static const char *fonts[] = { \u0026#34;monospace:size=10\u0026#34; }; Then execute sudo make install and you\u0026#39;re ready to go. =head2 INSTALLING SXHKD B\u0026lt;On Debian or Ubuntu :\u0026gt; sudo apt install sxhkd B\u0026lt;On Archlinux :\u0026gt; sudo pacman -Sy sxhkd =head2 CONFIGURING SXHKD B\u0026lt;Autostarting sxhkd at startup :\u0026gt; simply put this line in B\u0026lt;~/.xprofile\u0026gt; : sxhkd \u0026amp; B\u0026lt;Setting the keybinding :\u0026gt; Add these two lines in B\u0026lt;~/.config/sxhkd/sxhkdrc\u0026gt; ctrl + apostrophe /path/to/lazy_raku_unicode.pl The keybinding have to start at the beginning of a line, and the command has to follow on the next line and be preceded by a tabulation. To make sxhkd reload the config file, either kill it and relaunch it, or send a SIGUSR1 signal. This command can do it : pidof sxhkd | kill -SIGUSR1 $(cat /dev/stdin) If you want to use a different keybinding, please see sxhkd(1) and use xev(1) to find the name of keys. Here the name of some of the keys : ctrl, super, alt, Return . =head1 SEE ALSO dmenu(1), sxhkd(1), xev(1) =cut 参考链接: https://docs.raku.org/language/unicode_entry\n","permalink":"https://ohmyweekly.github.io/notes/2021-03-01-how-to-type-unicode-in-raku/","tags":["Raku","Unicode"],"title":"如何在 Raku 中输入 Unicode"},{"categories":["Raku"],"contents":"在我的上一篇文章中，我又一次为从 CORE 中增强类的方法而苦恼。这种挣扎完全没有必要，因为我并没有用增加的方法改变对象的状态。对于做更高级的东西，我可能不得不这样做。把手伸进 Raku 的内部这么深，我可能会把自己烫伤。既然我想做的是把我的代码绑在编译器的变化上，反正我可能会全身心地投入到 nqp-land 中去。\nmy \\j = 1 | 2 | 3; dd j; use nqp; .say for nqp::getattr(j, Junction, \u0026#39;$!eigenstates\u0026#39;); # OUTPUT: any(1, 2, 3) 1 2 3 我们可以使用 nqp 来获取私有属性，而不需要添加任何方法。这就有点儿不伦不类了。所以，让我们用一个伪方法来做一些 deboilerplating。\nsub pry(Mu $the-object is raw) { use InterceptAllMethods; class Interceptor { has Mu $!the-object; method ^find_method(Mu \\type, Str $name) { my method (Mu \\SELF:) is raw { use nqp; my $the-object := nqp::getattr(SELF, Interceptor, \u0026#39;$!the-object\u0026#39;); nqp::getattr($the-object, $the-object.WHAT, \u0026#39;$!\u0026#39; ~ $name) } } } use nqp; nqp::p6bindattrinvres(nqp::create(Interceptor), Interceptor, \u0026#39;$!the-object\u0026#39;, $the-object); } .say for j.\u0026amp;pry.eigenstates; # OUTPUT: 1 2 3 通过 InterceptAllMethods，lizmat 改变了类关键字的行为，允许我们提供一个 FALLBACK-method 来捕获任何方法，包括从 Mu 继承的方法。这反过来又允许 pry 返回的对象将任何方法调用转移到一个自定义的方法。在这个方法中，我们可以对 .\u0026amp;pry 被调用的对象做任何我们想做的事情。\n由于我们的特殊对象会拦截任何调用，甚至是 Mu 的调用，我们需要找到另一种方法来调用 .new。由于 .^ 不是 . 的特殊形式，我们可以用它来获得对类方法的访问。\nsub interceptor(Method $the-method){ use InterceptAllMethods; use nqp; sub (Mu $the-object is raw) { my class Interceptor { has Mu $!the-object; has Code $!the-method; method ^find_method(Mu \\type, Mu:D $name) { my method (Mu \\SELF: |c) is raw { $!the-method.($!the-object, $name, |c) } } method ^introspect(Mu \\type, Mu \\obj) { my method call-it() is raw { $!the-object } obj.\u0026amp;call-it; } method ^new(Mu \\type, $the-object!, $the-method) { nqp::p6bindattrinvres( nqp::p6bindattrinvres(nqp::create(Interceptor), Interceptor, \u0026#39;$!the-object\u0026#39;, $the-object), Interceptor, \u0026#39;$!the-method\u0026#39;, $the-method) } } # nqp::p6bindattrinvres( # nqp::p6bindattrinvres(nqp::create(Interceptor), Interceptor, \u0026#39;$!the-object\u0026#39;, $the-object), # Interceptor, \u0026#39;$!the-method\u0026#39;, $the-method); Interceptor.^new($the-object, $the-method) } } my \u0026amp;first-defined = interceptor( my method (Positional \\SELF: $name) { for SELF.flat -\u0026gt; $e { with $e.\u0026#34;$name\u0026#34;(|%_) { .return } } Nil } ); my $file = \u0026lt;file1.txt file2.txt file3.txt nohup.out\u0026gt;».IO.\u0026amp;first-defined.open(:r); dd $file; # OUTPUT: Handle $file = IO::Handle.new(path =\u0026gt; IO::Path.new(\u0026#34;nohup.out\u0026#34;, :SPEC(IO::Spec::Unix), :CWD(\u0026#34;/home/dex/projects/raku/tmp\u0026#34;)), chomp =\u0026gt; Bool::True, nl-in =\u0026gt; $[\u0026#34;\\n\u0026#34;, \u0026#34;\\r\\n\u0026#34;], nl-out =\u0026gt; \u0026#34;\\n\u0026#34;, encoding =\u0026gt; \u0026#34;utf8\u0026#34;) sub interceptor 接受一个方法并返回一个 sub。如果这个 sub 像方法一样被调用，它将把要被调用的方法的名称和调用者转发给一个自定义方法。当 .\u0026amp;first-defined 被调用时，会返回一个特殊的对象。让我们来看看它是什么。\nmy \\uhhh-special = \u0026lt;a b c\u0026gt;.\u0026amp;first-defined; dd uhhh-special.^introspect(uhhh-special); # OUTPUT: ($(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;), method \u0026lt;anon\u0026gt; (Positional \\SELF: $name, *%_) { #`(Method|93927752146784) ... }) 我们必须给 .^introspect 一个我们想看的对象，因为它的调用者是类 Interceptor 的类型对象。\n目前，我还不知道有什么办法（毕竟，我知道的只是足够多的东西，真的很危险。这是不幸的，因为 lizmat 决定重载关键字 class，而不是用不同的名字导出特殊的 Metamodel::ClassHOW。如果我们不想或不能有外部依赖，我们可以使用 MOP 来创建我们的类型对象。\nclass InterceptHOW is Metamodel::ClassHOW { method publish_method_cache(|) { } } sub ipry(Mu $the-object is raw) { my \\Interceptor = InterceptHOW.new_type(:name\u0026lt;Interceptor\u0026gt;); Interceptor.^add_attribute(Attribute.new(:name\u0026lt;$!the-object\u0026gt;, :type(Mu), :package(Interceptor))); Interceptor.^add_meta_method(\u0026#39;find_method\u0026#39;, my method find_method(Mu \\type, Str $name) { # say „looking for $name“; my method (Mu \\SELF:) is raw { use nqp; my $the-object := nqp::getattr(SELF, Interceptor, \u0026#39;$!the-object\u0026#39;); nqp::getattr($the-object, $the-object.WHAT, \u0026#39;$!\u0026#39; ~ $name) } }); Interceptor.^compose; use nqp; nqp::p6bindattrinvres(nqp::create(Interceptor), Interceptor, \u0026#39;$!the-object\u0026#39;, $the-object); } 当我写这篇文章的时候，我发现 .^add_meta_method 只有在提供给它的方法的第一个参数的名字和 Str 相同的时候才会工作。起初，我尝试了一个匿名方法，它最终出现在 .^meta_method_table 中，但从未被调用。我想这个 bug 其实并不重要，因为这个元方法根本没有被记录下来。如果我玩火，我没有权利抱怨烧伤。你会在野外的 Actions.nqp 中发现这个方法。Class 关键字并没有什么神奇的作用。Rakudo 只是使用 MOP 来构造类型对象。\n我们不能在 Raku 中重载赋值操作符。这其实并不需要，因为赋值是通过调用一个名为 STORE 的方法来实现的。由于我们得到了对 dispatch 的完全控制，我们可以拦截任何方法调用，包括一连串的方法调用。\nmulti sub methodify(%h, :$deeply!) { sub interceptor(%h, $parent = Nil){ use InterceptAllMethods; use nqp; class Interceptor is Callable { has Mu $!the-object; has Mu @!stack; method ^find_method(Mu \\type, Mu:D $name) { my method (Mu \\SELF: |c) is raw { my @new-stack = @!stack; my $the-object = $!the-object; if $name eq \u0026#39;STORE\u0026#39; { # workaround for rakudobug#4203 $the-object{||@new-stack.head(*-1)}:delete if $the-object{||@new-stack.head(*-1)}:exists; $the-object{||@new-stack} = c; return-rw c } else { @new-stack.push: $name; my \\nextlevel = SELF.^new($!the-object, @new-stack, $name); nextlevel } } } method ^introspect(Mu \\type, Mu \\obj) { my method call-it() is raw { $!the-object, @!stack } obj.\u0026amp;call-it; } method ^new(Mu \\type, $the-object!, @new-stack?, $name?) { $name ?? nqp::p6bindattrinvres( nqp::p6bindattrinvres(nqp::create(Interceptor), Interceptor, \u0026#39;$!the-object\u0026#39;, $the-object), Interceptor, \u0026#39;@!stack\u0026#39;, @new-stack) !! nqp::p6bindattrinvres(nqp::create(Interceptor), Interceptor, \u0026#39;$!the-object\u0026#39;, $the-object) } } Interceptor.^new(%h) } interceptor(%h) } my %h2; my $o2 = methodify(%h2, :deeply); $o2.a.b = 42; dd %h2; $o2.a.b.c = \u0026lt;answer\u0026gt;; dd %h2; say $o2.a.b.c; # OUTPUT: Hash %h2 = {:a(${:b(\\(42))})} Hash %h2 = {:a(${:b(${:c(\\(\u0026#34;answer\u0026#34;))})})} This type cannot unbox to a native string: P6opaque, Interceptor in block \u0026lt;unit\u0026gt; at /home/dex/projects/raku/any-chain.raku line 310 每当我们调用一个方法时，都会创建一个新的 Interceptor 实例，它存储了前一个方法的名称。这样我们就可以沿着方法调用链移动。由于赋值调用 STORE，我们可以将赋值转移到我们用作实际数据结构的 Hash 中。唉，检索值就不一样了，因为 Raku 不区分方法调用和 FETCH。在这里，龙比我强。我还是包含了这个一半失败的尝试，因为我对 slippy 半列表有很好的利用。这需要使用 v6.e.preview，让我踩到了一个 bug。可能还有更多这样的情况。所以请使用同样的，这样我们就可以在 .e 发布到野外之前，把所有的野兽都杀掉。\n能够完全控制方法调用链将是一件好事。也许我们可以用 RakuAST 来做到这一点。\n有了这些已经可以工作的东西，我们可以做一些有趣的事情。那些烦人的异常总是在拖我们的后腿。我们可以用 try 来化解它们，但那会破坏一个方法调用链。\nconstant no-argument-given = Mu.new; sub try(Mu $obj is raw, Mu $alternate-value = no-argument-given) { interceptor(my method (Mu \\SELF: $name, |c) { my $o = SELF; my \\m = $o.^lookup($name) orelse { my $bt = Backtrace.new; my $idx = $bt.next-interesting-index($bt.next-interesting-index + 1); (X::Method::NotFound.new(:method($name), :typename($o.^name)) but role :: { method vault-backtrace { False }}).throw(Backtrace.new($idx + 1)); } try { $o = $o.\u0026#34;$name\u0026#34;(|c); } $! ~~ Exception ?? $alternate-value.WHICH eqv no-argument-given.WHICH ?? $o !! $alternate-value !! $o }).($obj) } class C { has $.greeting; method might-throw { die \u0026#34;Not today love!\u0026#34; } method greet { say $.greeting } } C.new(greeting =\u0026gt; ‚Let\u0026#39;s make love!‘).\u0026amp;try.might-throw.greet; # OUTPUT: Let\u0026#39;s make love! 伪方法 try 将会化解任何异常，并允许继续调用 C 语言的方法。我必须用一个特殊的值来标记没有可选的参数 $alternate-value，因为它实际上可能会把异常对象变成 Nil。\n我很肯定还有很多这样的小帮手在等着我们去发现。未来可能会有一个模块，希望能帮助 Raku 成为一个好的编程语言。\n原文链接: https://gfldex.wordpress.com/2021/02/17/method-ish/\n","permalink":"https://ohmyweekly.github.io/notes/2021-02-17-method-isd/","tags":["Raku","Raku 博客"],"title":"Method-ish"},{"categories":["Raku"],"contents":"一直以来乐于助人的 raiph 在回答一个关于模式匹配的问题时希望得到 RakuAST，就像在 Haskell 中一样。有人提出用 MMD 来解决这个问题。这样做，得到一个贯穿的默认路径是无解的。由于 dispatch 简单来说就是模式匹配，我们只需要做一些额外的工作。简而言之，dispatcher 得到一个函数列表和一个带参数的列表。第一个接受所有参数的函数获胜。\nclass Hold { has $.key; } class Press { has $.key; } class Err { has $.msg; } sub else(\u0026amp;code) { \u0026amp;code } sub match($needle, *@tests) { for @tests.head(*-1) -\u0026gt; \u0026amp;f { if \u0026amp;f.cando(\\($needle)) { return f($needle); } } @tests.tail.(); } match Hold.new(:key\u0026lt;a\u0026gt;), -\u0026gt; Hold (:$key) { put „holding $key“; }, -\u0026gt; Press (:$key) { put „pressing $key“; }, -\u0026gt; Err (:$msg) { warn „ERR: $msg“ }, else { fail ‚unsopported‘ }; 方法 .cando 需要一个 Capture 来告诉我们一个 Routine 是否可以用一个给定的参数列表来调用。为了创建这样一个捕获，我们使用字面的 \\($arguments, $go, $here)。我们不在最后测试默认值。相反，当没有其他函数匹配时，我们会调用该函数。声明 sub else 只是为了美化。\n由于我们是在函数式的土地上，我们可以使用 Raku 提供给我们的所有方便的功能。\nmy \u0026amp;key-matcher = \u0026amp;match.assuming(*,[ -\u0026gt; Hold (:$key) { put „holding $key“; }, -\u0026gt; Press (:$key) { put „pressing $key“; }, -\u0026gt; Err (:$msg) { warn „ERR: $msg“ }, else { fail ‚unsopported‘ }; ]); sub key-source { gather loop { sleep 1; take (Hold.new(:key\u0026lt;a\u0026gt;), Press.new(:key\u0026lt;b\u0026gt;), Err.new(:msg\u0026lt;WELP!\u0026gt;), \u0026#39;unsupported\u0026#39;).pick; } } .\u0026amp;key-matcher for key-source; 我们要帮助 .assuming 有点理解 slurpies，把函数列表放在一个显式 Array 中。\n总有一种函数式的方法来解决一个问题。有时我们甚至可以从中得到一个整齐的语法。\n原文链接: https://gfldex.wordpress.com/2021/02/24/pattern-dispatch/\n","permalink":"https://ohmyweekly.github.io/notes/2021-02-28-pattern-dispatch/","tags":["Raku","Raku 博客"],"title":"模式分派"},{"categories":["Raku"],"contents":"我不太喜欢上一篇文章中使用匹配的语法。它的参数列表中的逗号看起来很奇怪，不合适。也许是因为我的眼睛习惯了给定的块。睡一觉就好了。\nsub accord(\u0026amp;c) { (c(CALLER::\u0026lt;$_\u0026gt;); succeed) if \u0026amp;c.cando(\\(CALLER::\u0026lt;$_\u0026gt;)) } given Err.new(:msg\u0026lt;a\u0026gt;) { accord -\u0026gt; Hold (:$key) { put „holding $key“; } accord -\u0026gt; Err (:$msg) { warn „ERR: $msg“ } default { fail ‚unsupported‘ } } 这是因为 accord 模仿了 when 的工作。它做了一些匹配，当 True 时调用一个块，并在每个块的结尾添加一个 success（通过抛出一个控制异常）。given 所做的只是设置主题。它还充当了 caller 的角色，所以我们可以通过一个伪包来访问它的 $_。利用 pointy 的签名来做解构是相当强大的。把这个添加到 CORE 中可能是个好主意。\n我们可能要把 Raku 的定义改成: \u0026ldquo;Raku 是一种高度可组合的编程语言\u0026rdquo;, 在这里，所有的东西都会落到实处。\u0026quot;\n更新一下。\n有些情况下，$_ 不是动态的。另外，success 正在抛出一个控制异常，而这些的处理程序是由 when 或默认添加的。这种情况是在编译时发生的，目前不能用宏来解决。第一个问题可以用黑魔法解决。后一个问题需要用默认块。我没有找到一种方法来提供一个合理的错误信息，如果缺少这个块。\nmulti sub accord(\u0026amp;c) { use nqp; $_ := nqp::getlexcaller(\u0026#39;$_\u0026#39;); (c($_); succeed) if \u0026amp;c.cando(\\($_)) } for @possibilities.roll(1) -\u0026gt; $needle { given $needle { accord -\u0026gt; Hold (:$key) { put „holding $key“; } accord -\u0026gt; Err (:$msg) { warn „ERR: $msg“ } default { warn ‚unsopported‘ } } } 原文链接: https://gfldex.wordpress.com/2021/02/25/custom-when/\n","permalink":"https://ohmyweekly.github.io/notes/2021-02-28-custom-when/","tags":["Raku","Rakulang"],"title":"自定义 when"},{"categories":["rakulang"],"contents":"在过去的几周里，我看到了一些文章和视频，讲述了在开源软件中构思是多么的伟大。这次我又想起了另一篇旧文Raku 是我的 MMORPG。它说，你可以从以下几个方面受益于开源软件。比如说，可以做一个大侠，基于一些开源软件来写软件。作为写手，可以写博客、微博等，对所选软件产生兴趣。或者你可以成为一个法师 - 实现新的功能和修复 bug。今天小编就带着弓箭手来告诉大家如何成为 Raku 编程语言的法师。\n选择一个任务 让我们挑选一些编译器的 bug，并修复它。让我们去 Rakudo 编译器 issues中选择我们想要修复的 bug。我滚动了一下 bug 列表，遇到了解析 - 运气不错，我前段时间一直在研究编译器语法，看了一本这方面的好书。找到了四个问题。\n 标签为 LTA （Less Than Awesome - 当真实行为与直觉预期不同时）- 我们暂时把它划掉。 标签 \u0026ldquo;需要共识\u0026rdquo; - 我们只想修复一个不复杂的 bug - 肯定要划掉。 标签为 \u0026ldquo;grammar and actions\u0026rdquo; 的关于一个可能死的代码是一个很好的候选人的第一个任务。  任务确定后，现在我们需要配置工作环境。在 Windows、Linux 和 macOS 中，一切应该都差不多。我将通过 macOS 的例子来告诉你。\n建立工作环境 为源码和我们建立的编译器建立文件夹。\nmkdir ~/dev-rakudo \u0026amp;\u0026amp; mkdir ~/dev-rakudo-install Rakudo 编译器由三部分组成。\n 虚拟机。现在有三种 - JVM、JS 和 MoarVM。我们以 MoarVM 为最稳定的一个。 NQP（Not Quite Perl），是一种低级（中级）语言的实现，它是 Raku 的一个 \u0026ldquo;子集\u0026rdquo;。虚拟机可以执行用 NQP 编写的代码。 Rakudo 编译器本身，用 NQP 和 Raku 编写。  下载并编译这三个组件。我分别花了一分半钟、半分钟和两分半钟才编好。\ncd ~/dev-rakudo \u0026amp;\u0026amp; git clone git@github.com:MoarVM/MoarVM.git \u0026amp;\u0026amp; cd MoarVM perl Configure.pl --prefix ~/dev-rakudo-install \u0026amp;\u0026amp; make -j 4 \u0026amp;\u0026amp; make install cd ~/dev-rakudo \u0026amp;\u0026amp; git clone git@github.com:Raku/nqp.git \u0026amp;\u0026amp; cd nqp perl Configure.pl --backend=moar --prefix ~/dev-rakudo-install \u0026amp;\u0026amp; make -j 4 \u0026amp;\u0026amp; make install cd ~/dev-rakudo \u0026amp;\u0026amp; git clone git@github.com:rakudo/rakudo.git \u0026amp;\u0026amp; cd rakudo perl Configure.pl --backend=moar --prefix ~/dev-rakudo-install \u0026amp;\u0026amp; make -j 4 \u0026amp;\u0026amp; make install 注意参数。--prefix 显示了 make install 命令后可执行文件的复制位置，--backend=moar 表示正在使用的虚拟机，而 -j 4 则要求跨多线程并行化（以防加快进度）。现在我们已经建立了 Rakudo 编译器 ~/dev-rakudo-install/bin/raku。我们还需要官方的编译器测试套件。你应该把它和它的代码一起放在文件夹里。\ncd ~/dev-rakudo/rakudo \u0026amp;\u0026amp; git clone https://github.com/Raku/roast.git t/spec 我们先进行测试。这种情况很常见，有些测试甚至在新的变化之前就失败了。我们需要辨别出来，这样以后就不会害怕这些变化破坏了一些不必要的东西。\n这里和下面我将在 ~/dev-rakudo/rakudo 文件夹中工作，除非另有说明。\n\u0026gt; make spectest [...] Test Summary Report ------------------- t/spec/S32-str/utf8-c8.t (Wstat: 65280 Tests: 54 Failed: 0) Non-zero exit status: 255 Parse errors: Bad plan. You planned 66 tests but ran 54. Files=1346, Tests=117144, 829 wallclock secs (27.70 usr 6.04 sys + 2638.79 cusr 210.98 csys = 2883.51 CPU) Result: FAIL make: *** [m-spectest5] Error 1 14分钟内共运行了1,346个文件中的117,144次测试。一些与utf8相关的测试由于某种原因失败了，其他的都能正常工作。我们已经准备好去工作了!\n让我们来看看问题的陈述 问题陈述说，某个元运算符 R 在 colonpair 上出了问题。我打开文档，搜索 R 这个词，但下拉列表中没有这个名字的元运算符。我试着输入 metaop，看到的是反向元操作符(R)。原来，如果你想把二元运算的操作数按相反的顺序写出来，你可以在其符号前使用前缀 R。\nsay 3 R- 2 == -1 # Output: True Colonpair 是命名对的语法。它看起来就像名字前面有一个冒号，前面有一个括号，有一个值。例如 :foo(42) 是一个名称为 foo、值为 42 的对儿。这个语法通常用于在调用函数时，向函数传递一个命名参数中的值。\nsub sub-with-named-parameter(:$foo) { say $foo; } sub-with-named-parameter(:foo(42)); # Output: 42 如果一个函数参数不是命名的，而是位置的，那么在用命名对调用时，就会出现编译错误。\nsub sub-without-named-parameter($foo) { # \u0026lt;- 没有冒号 say $foo; } sub-without-named-parameter(:foo(42)); # Unexpected named argument \u0026#39;foo\u0026#39; passed 如果你在调用这样的函数时用括号包围一个参数，整个参数对将被传递到位置参数。\nsub sub-without-named-parameter($foo) { say $foo; } sub-without-named-parameter((:foo(42))); # Output: foo =\u0026gt; 42 在 Raku 中，你可以写一个函数来捕获所有传递给它的参数并分析它们。这是在单个参数 - 捕获前用竖线完成的。\nsub sub-with-capture(|foo) { # \u0026lt;- 参数捕获 say foo; } sub-with-capture(:foo(42)); # Output: \\(:foo(42)) sub-with-capture(42); # Output: \\(42) sub-with-capture(:foo(3 Z- 2)); # Output: \\(:foo((1,).Seq)) sub-with-capture(:foo(3 R- 2)); # Output: \\(-1) 倒数第二行使用了 Z 元操作符 - zip 操作符。它将左右两部分作为一个列表，按顺序每次从它们中抽取一个元素，并进行操作，从而形成一个序列。\n在最后一行，只用了我们需要的 R 元操作符。在这种情况下，它不是一个对，而是一个常量，它被传递到函数中。我们可以假设这是元运算符工作方式的一些特殊性，但用 Z 的例子表明并非如此。其实这是一个 bug - 当一个对被传递到一个使用 R 元运算符的函数中时，它的值会被转换。\n我们需要一个新的测试 为了确保未来的变化能够修复错误的行为，我们需要写一个新的测试。在测试文件中不难找到 R 元操作符的测试（S03-metops/reverse.t）。下面我将补充以下测试。\n# https://github.com/rakudo/rakudo/issues/1632 { sub subroutine($expected, |actual) { is actual.gist, $expected, \u0026#34;Сolonpair exists\u0026#34; } subroutine(\u0026#39;\\(:foo(-1))\u0026#39;, :foo(3 R- 2)); } 该测试有一个功能，有两个参数 - 正常和捕获。在函数体中，第一个参数和传递的 Capture 的字符串表示进行比较。你可以使用 make 对新构建的编译器进行单独测试。\n\u0026gt; make t/spec/S03-metaops/reverse.t [...] ok 69 - [R~]= not ok 70 - Colonpair exists # Failed test 'Colonpair exists' # at t/spec/S03-metaops/reverse.t line 191 # expected: '\\(:foo(-1))' # got: '\\(-1)' # You planned 69 tests, but ran 70 # You failed 1 test of 70 你可以看到，测试失败了（如预期）。还有一个单独的说明，系统预计69次测试，但收到70次。这是基于 TAP 的测试系统的特点 - 必须在文件的顶部修正传递给 plan 函数的数字。现在测试崩溃了，但编号没有受到影响。你可以开始修复它。\n凝视法 一开始我很相信任务上的标签 - 如果是解析的话，一定是源码解析阶段的某个地方出现了问题。目前我的认识如下：\n 基础解析器代码在文件 rakudo/src/Perl6/Grammar.nqp 中。 这个解析器是从 nqp/src/HLL/Grammar.nqp 文件中的基础解析器继承的。 元操作符的解析和工作方式都差不多，你可以通过仔细观察来发现不同之处。  我在基础解析器代码中找到了对元操作符的引用。\ntoken infix_prefix_meta_operator:sym\u0026lt;R\u0026gt; {\u0026lt;sym\u0026gt;\u0026lt;infixish(\u0026#39;R\u0026#39;)\u0026gt;{}\u0026lt;.can_meta($\u0026lt;infixish\u0026gt;, \u0026#34;reverse the args of\u0026#34;)\u0026gt;\u0026lt;O=.revO($\u0026lt;infixish\u0026gt;)\u0026gt;} token infix_prefix_meta_operator:sym\u0026lt;Z\u0026gt; {\u0026lt;sym\u0026gt;\u0026lt;infixish(\u0026#39;Z\u0026#39;)\u0026gt;{}\u0026lt;.can_meta($\u0026lt;infixish\u0026gt;, \u0026#34;zip with\u0026#34;)\u0026gt;\u0026lt;O(|%list_infix)\u0026gt;} 这需要对 Raku grammar 有一定的了解。据我所知，原来这两个元运算符在解析上并没有根本的区别。一段时间后，在解析器的源代码中挖得够多了，我开始怀疑解析工作是否正确。认为代码 my $r = :foo(3 R- 2); say $r; # Output: foo =\u0026gt; -1 正确工作的建议 - 问题恰恰发生在调用函数时。显然，我白白相信了任务栏上的标签。\n编译器将帮助我们 颇为迟钝的我想起了我从一开始就应该做的事情。Rakudo 编译器有 --target 调试开关。它取编译器阶段的名称，你想将其结果输出到控制台并退出。我想看看 --target=parse（因为我只知道这一个）。\n我从 ~/dev-rakudo/rakudo 文件夹中使用 rakumo-m，这样我就不必等待通过 make install 命令将所需文件复制到 ~/dev-rakudo-install。简单的脚本可以这样运行。更复杂的脚本必须在 make install 之后从 -install 中运行。\n\u0026gt; cat ~/test.raku sub s(|c) { say c } s(:foo(3 R- 2)); s(:foo(3 Z- 2)); \u0026gt; ./rakudo-m --target=parse ~/test.raku [...] - args: (:foo(3 R- 2)) - semiarglist: :foo(3 R- 2) - arglist: 1 matches - EXPR: :foo(3 R- 2) - colonpair: :foo(3 R- 2) - identifier: foo - coloncircumfix: (3 R- 2) - circumfix: (3 R- 2) - semilist: 3 R- 2 - statement: 1 matches - EXPR: R- 2 [...] - args: (:foo(3 Z- 2)) - semiarglist: :foo(3 Z- 2) - arglist: 1 matches - EXPR: :foo(3 Z- 2) - colonpair: :foo(3 Z- 2) - identifier: foo - coloncircumfix: (3 Z- 2) - circumfix: (3 Z- 2) - semilist: 3 Z- 2 - statement: 1 matches - EXPR: Z- 2 [...] 结论：R 和 Z 的解析是一样的。\n这不是解析 所有被解析的东西都会被传递给所谓的 Action，把字词变成一棵语法树。在我们的例子中，Actions 位于文件 rakudo/src/Perl6/Actions.nqp 和 nqp/src/HLL/Actions.nqp 中。这里就比较容易搞清楚了，毕竟是代码，是 grammar。\n我在主 Actions 中找到了以下代码。\n[...] elsif $\u0026lt;infix_prefix_meta_operator\u0026gt; { [...] if $metasym eq 'R' { $helper := '\u0026amp;METAOP_REVERSE'; $t := nqp::flip($t) if $t; } elsif $metasym eq 'X' { $helper := '\u0026amp;METAOP_CROSS'; $t := nqp::uc($t); } elsif $metasym eq 'Z' { $helper := '\u0026amp;METAOP_ZIP'; $t := nqp::uc($t); } my $metapast := QAST::Op.new( :op\u0026lt;call\u0026gt;, :name($helper), WANTED($basepast,'infixish') ); $metapast.push(QAST::Var.new(:name(baseop_reduce($base\u0026lt;OPER\u0026gt;\u0026lt;O\u0026gt;.made)), :scope\u0026lt;lexical\u0026gt;)) if $metasym eq 'X' || $metasym eq 'Z'; [...] 它说，如果在代码中解析了元操作符 R、Z 或 X，就应该在语法树中添加一些 METAOP_ 函数调用。在 Z 和 X 的情况下，它会多一个参数，即某种还原函数。所有这些功能都可以在 rakudo/src/core.c/metaops.pm6 中找到。\nsub METAOP_REVERSE(\\op) is implementation-detail { -\u0026gt; |args { op.(|args.reverse) } } sub METAOP_ZIP(\\op, \u0026amp;reduce) is implementation-detail { nqp::if(op.prec(\u0026#39;thunky\u0026#39;).starts-with(\u0026#39;.\u0026#39;), -\u0026gt; +lol { my $arity = lol.elems; [...] }, -\u0026gt; +lol { Seq.new(Rakudo::Iterator.ZipIterablesOp(lol,op)) } ) } 给你：\n \\op 是由我们的元操作符，即-，在前面的操作。 Trait implementation-detail 只是表明这不是公共代码，是编译器实现的一部分。 由于-操作没有笨重的特性，所以 \u0026amp;reduce 函数不会参与计算，Z 的结果是 Seq.new(...)。 R 的结果是一个操作调用 - 参数顺序相反。  这时我想起还有一个 - 目标，即星。它将显示行动的结果。\n\u0026gt; ./rakudo-m --target=ast ~/test.raku [...] - QAST::Op(call \u0026amp;s) \u0026lt;sunk\u0026gt; :statement_id\u0026lt;4\u0026gt; s(:foo(3 R- 2)) - QAST::Op+{QAST::SpecialArg}(call :named\u0026lt;foo\u0026gt;) \u0026lt;wanted\u0026gt; :statement_id\u0026lt;5\u0026gt; :before_promotion\u0026lt;?\u0026gt; R- - QAST::Op(call \u0026amp;METAOP_REVERSE) \u0026lt;wanted\u0026gt; :is_pure\u0026lt;?\u0026gt; - QAST::Var(lexical \u0026amp;infix:\u0026lt;-\u0026gt;) \u0026lt;wanted\u0026gt; - QAST::Want \u0026lt;wanted\u0026gt; 3 - QAST::WVal(Int) - Ii - QAST::IVal(3) 3 - QAST::Want \u0026lt;wanted\u0026gt; 2 - QAST::WVal(Int) - Ii - QAST::IVal(2) 2 [...] - QAST::Op(call \u0026amp;s) \u0026lt;sunk\u0026gt; :statement_id\u0026lt;7\u0026gt; s(:foo(3 Z- 2)) - QAST::Op+{QAST::SpecialArg}(:named\u0026lt;foo\u0026gt;) \u0026lt;wanted\u0026gt; :statement_id\u0026lt;8\u0026gt; :before_promotion\u0026lt;?\u0026gt; Z- - QAST::Op(call \u0026amp;METAOP_ZIP) \u0026lt;wanted\u0026gt; :is_pure\u0026lt;?\u0026gt; - QAST::Var(lexical \u0026amp;infix:\u0026lt;-\u0026gt;) \u0026lt;wanted\u0026gt; - QAST::Var(lexical \u0026amp;METAOP_REDUCE_LEFT) - QAST::Want \u0026lt;wanted\u0026gt; 3 - QAST::WVal(Int) - Ii - QAST::IVal(3) 3 - QAST::Want \u0026lt;wanted\u0026gt; 2 - QAST::WVal(Int) - Ii - QAST::IVal(2) 2 [...] 一如所料。除了调用不同的 METAOP_ 函数外，所有的东西几乎都是一样的。从它们的代码中我们可以知道，原则上这些函数的不同之处在于返回值的类型 - 分别是 Int 和 Seq。众所周知，Raku 对不同类型的对象的上下文相当敏感\u0026hellip;\u0026hellip;我想，它关注的可能是返回值。我试着用下面的方式修改代码。\nsub METAOP_REVERSE(\\op) is implementation-detail { -\u0026gt; |args { Seq.new(op.(|args.reverse)) } } 编译、运行。\n\u0026gt; make [...] Stage start : 0.000 Stage parse : 61.026 Stage syntaxcheck: 0.000 Stage ast : 0.000 Stage optimize : 7.076 Stage mast : 14.120 Stage mbc : 3.941 [...] \u0026gt; ./rakudo-m ~/test.raku \\(-1) \\(:foo((1,).Seq)) 一切都没有改变。所以，不是返回值\u0026hellip;\u0026hellip;想了想，不知道为什么结果又是 -1 而不是 (-1,).Seq。而且，从代码来看，Seq 根本就没有一个合适的构造函数。下一次，作为一些疯狂的事情，我尝试调用 METAOP_REVERSE 结果只是为了崩溃。\nsub METAOP_REVERSE(\\op) is implementation-detail { -\u0026gt; |args { die } } 编译、运行。\n\u0026gt; make [...] \u0026gt; ./rakudo-m ~/test.raku \\(-1) \\(:foo((1,).Seq)) 怎么会呢？语法树中包含了对 METAOP_REVERSE 的调用，它的代码应该是折叠的，但计算仍然进行，我们得到 -1。\n这些都不是《行动》。\n这里我的目光落在编译器的构建日志上。它是一些阶段被列在那里。我随机试了 --target=mast。\n\u0026gt; ./rakudo-m --target=mast ~/test.raku [...] MAST::Frame name\u0026lt;s\u0026gt;, cuuid\u0026lt;1\u0026gt; Local types: 0\u0026lt;obj\u0026gt;, 1\u0026lt;obj\u0026gt;, 2\u0026lt;obj\u0026gt;, 3\u0026lt;obj\u0026gt;, 4\u0026lt;int\u0026gt;, 5\u0026lt;str\u0026gt;, 6\u0026lt;obj\u0026gt;, 7\u0026lt;obj\u0026gt;, 8\u0026lt;obj\u0026gt;, Lexical types: 0\u0026lt;obj\u0026gt;, 1\u0026lt;obj\u0026gt;, 2\u0026lt;obj\u0026gt;, 3\u0026lt;obj\u0026gt;, 4\u0026lt;obj\u0026gt;, Lexical names: 0\u0026lt;c\u0026gt;, 1\u0026lt;$¢\u0026gt;, 2\u0026lt;$!\u0026gt;, 3\u0026lt;$/\u0026gt;, 4\u0026lt;$*DISPATCHER\u0026gt;, Lexical map: $!\u0026lt;2\u0026gt;, c\u0026lt;0\u0026gt;, $*DISPATCHER\u0026lt;4\u0026gt;, $¢\u0026lt;1\u0026gt;, $/\u0026lt;3\u0026gt;, Outer: name\u0026lt;\u0026lt;unit\u0026gt;\u0026gt;, cuuid\u0026lt;2\u0026gt; [...] 某种不可读的矩阵。星号和桅杆之间有一个阶段性的优化。\n\u0026gt; ./rakudo-m --target=optimize ~/test.raku [...] - QAST::Op(callstatic \u0026amp;s) \u0026lt;sunk\u0026gt; :statement_id\u0026lt;4\u0026gt; s(:foo(3 R- 2)) - QAST::Op(call \u0026amp;infix:\u0026lt;-\u0026gt;) :METAOP_opt_result\u0026lt;?\u0026gt; - QAST::Want \u0026lt;wanted\u0026gt; 2 - QAST::WVal(Int) - Ii - QAST::IVal(2) 2 - QAST::Want \u0026lt;wanted\u0026gt; 3 - QAST::WVal(Int) - Ii - QAST::IVal(3) 3 [...] - QAST::Op(callstatic \u0026amp;s) \u0026lt;sunk\u0026gt; :statement_id\u0026lt;7\u0026gt; s(:foo(3 Z- 2)) - QAST::Op+{QAST::SpecialArg}(call :named\u0026lt;foo\u0026gt;) \u0026lt;wanted\u0026gt; :statement_id\u0026lt;8\u0026gt; :before_promotion\u0026lt;?\u0026gt; Z- - QAST::Op(callstatic \u0026amp;METAOP_ZIP) \u0026lt;wanted\u0026gt; :is_pure\u0026lt;?\u0026gt; - QAST::Var(lexical \u0026amp;infix:\u0026lt;-\u0026gt;) \u0026lt;wanted\u0026gt; - QAST::Var(lexical \u0026amp;METAOP_REDUCE_LEFT) - QAST::Want \u0026lt;wanted\u0026gt; 3 - QAST::WVal(Int) - Ii - QAST::IVal(3) 3 - QAST::Want \u0026lt;wanted\u0026gt; 2 - QAST::WVal(Int) - Ii - QAST::IVal(2) 2 [...] 哈，就是这样。在优化阶段后，行将失踪。\n QAST::Op+{QAST::SpecialArg}(call :named\u0026lt;foo\u0026gt;) \u0026lt;wanted\u0026gt; :statement_id\u0026lt;5\u0026gt; :before_promotion\u0026lt;?\u0026gt; R-. 并将整个 METAOP_REVERSE 调用替换为通常的操作 (\u0026amp;infix:\u0026lt;-\u0026gt;)。所以问题一定在优化器的某个地方。  只有在 optim_nameless_call 方法中才会提到 \u0026amp;METAOP_ASSIGN，其中 QAST::Op+{QAST::SpecialArg}(call :named\u0026lt;foo\u0026gt;)。显然，这个操作负责生成一个命名对 - 它已经有了一个名字（命名参数），它需要计算值。从优化 _ 无名方法的执行路径来看，我们可以得出结论，我们对最后一个块感兴趣。\n[...] elsif self.op_eq_core($metaop, '\u0026amp;METAOP_REVERSE') { return NQPMu unless nqp::istype($metaop[0], QAST::Var) \u0026amp;\u0026amp; nqp::elems($op) == 3; return QAST::Op.new(:op\u0026lt;call\u0026gt;, :name($metaop[0].name), $op[2], $op[1]).annotate_self: 'METAOP_opt_result', 1; } [...] 让我提醒你，优化前的树是这样的。\n[...] - QAST::Op(call \u0026amp;s) \u0026lt;sunk\u0026gt; :statement_id\u0026lt;4\u0026gt; s(:foo(3 R- 2)) - QAST::Op+{QAST::SpecialArg}(call :named\u0026lt;foo\u0026gt;) \u0026lt;wanted\u0026gt; :statement_id\u0026lt;5\u0026gt; :before_promotion\u0026lt;?\u0026gt; R- - QAST::Op(call \u0026amp;METAOP_REVERSE) \u0026lt;wanted\u0026gt; :is_pure\u0026lt;?\u0026gt; - QAST::Var(lexical \u0026amp;infix:\u0026lt;-\u0026gt;) \u0026lt;wanted\u0026gt; - QAST::Want \u0026lt;wanted\u0026gt; 3 - QAST::Want \u0026lt;wanted\u0026gt; 2 [...] 而精简之后，是这样的。\n[...] - QAST::Op(callstatic \u0026amp;s) \u0026lt;sunk\u0026gt; :statement_id\u0026lt;4\u0026gt; s(:foo(3 R- 2)) - QAST::Op(call \u0026amp;infix:\u0026lt;-\u0026gt;) :METAOP_opt_result\u0026lt;?\u0026gt; - QAST::Want \u0026lt;wanted\u0026gt; 2 - QAST::Want \u0026lt;wanted\u0026gt; 3 [...] 也就是说，优化 _nameless_call 做了以下工作。\n如果我们的 QAST::Op+{QAST::SpecialArg} 操作没有三个参数，如果 METAOP_REVERSE 调用没有一个正确的类型，我们就返回空。这不是我们的情况。 否则，我们将返回一个新的操作，代替我们的 QAST::Op+{QAST::SpecialArg} 操作，以相反的顺序调用 \u0026amp;infix:\u0026lt;-\u0026gt; 参数。就是说，把结果打包成一对就没了。\n在摸索了一下如何解决这个问题，并阅读了 QAST::SpecialArg 和 QAST::Node 的实现后，我想到了下面的代码。\n[...] elsif self.op_eq_core($metaop, '\u0026amp;METAOP_REVERSE') { return NQPMu unless nqp::istype($metaop[0], QAST::Var) \u0026amp;\u0026amp; nqp::elems($op) == 3; my $opt_result := QAST::Op.new(:op\u0026lt;call\u0026gt;, :name($metaop[0].name), $op[2], $op[1]).annotate_self: 'METAOP_opt_result', 1; if $op.named { $opt_result.named($op.named) } # 添加选项 named if $op.flat { $opt_result.flat($op.flat) } # 添加选项 flat return $opt_result; } [...] 还有木头。\n[...] - QAST::Op(callstatic \u0026amp;s) \u0026lt;sunk\u0026gt; :statement_id\u0026lt;4\u0026gt; s(:foo(3 R- 2)) - QAST::Op+{QAST::SpecialArg}(call \u0026amp;infix:\u0026lt;-\u0026gt; :named\u0026lt;foo\u0026gt;) :METAOP_opt_result\u0026lt;?\u0026gt; - QAST::Want \u0026lt;wanted\u0026gt; 2 - QAST::Want \u0026lt;wanted\u0026gt; 3 [...] 命名的参数返回到它的位置。测试也开始通过。\n\u0026gt; make t/spec/S03-metaops/reverse.t [...] All tests successful. Files=1, Tests=70, 3 wallclock secs ( 0.03 usr 0.01 sys + 3.61 cusr 0.17 csys = 3.82 CPU) Result: PASS 我们本可以就此打住，但这是编译器优化器的代码，它的结果是一个有两个整数参数的方法调用。我认为这在某种程度上是次优的。如果我们将返回表达式改为返回 self.visit_op: $opt_result;，对产生的非优化操作调用优化器，那么产生的树就会像这样。\n[...] - QAST::Op(callstatic \u0026amp;s) \u0026lt;sunk\u0026gt; :statement_id\u0026lt;4\u0026gt; s(:foo(3 R- 2)) - QAST::Want+{QAST::SpecialArg}(:named\u0026lt;foo\u0026gt;) - QAST::WVal+{QAST::SpecialArg}(Int :named\u0026lt;foo\u0026gt;) - QAST::IVal(-1) [...] 现在一切都很理想。\n分享成果 我们已经到了终点线。现在我们只需要分享我们的经验。\n 重要的是：运行所有的 make spectest 测试，确保没有新的东西被破坏。 在 GitHub 上使用 Rakudo 编译器和测试制作 fork 仓库。 将 fork 仓库添加为新的 git 远程仓库。 cd ~/dev-rakudo/rakudo \u0026amp;\u0026amp; git remote add fork 。 cd ~/dev-rakudo/t/spec \u0026amp;\u0026amp; git remote add fork 。  重要：确保两个仓库在 git 中都有正确的用户名和用户邮箱。\n提交到两个版本库，详细说明你为什么做了哪些改动，并添加对原始问题跟踪器的引用。\n运行提交。\ncd ~/dev-rakudo/rakudo \u0026amp;\u0026amp; git push fork cd ~/dev-rakudo/t/spec \u0026amp;\u0026amp; git push fork 向两个仓库提出拉取请求。在他们的描述中，最好是相互参照和原任务。\n结论 对开源软件的贡献是：\n 趣味性和趣味性。 给你的感觉是，你正在做一些有用的事情，你真的是。 让你认识新的有趣和专业的人（任何关于 Raku 的问题都会在 #raku IRC 频道中得到回答）。 解决非标准任务，没有截止日期的压力，是一种很好的体验。  选择你觉得最舒服的角色等级，去做新的任务吧!\n","permalink":"https://ohmyweekly.github.io/notes/2021-02-16-contributing-to-raku/","tags":["Raku","Rakulang"],"title":"为最年轻的 Raku 贡献力量"},{"categories":["Raku"],"contents":"[113] 发表于2021年1月30日。 这是我对 Perl 每周挑战#097 的回应。\n挑战 #097.1: 凯撒密码 给你一个只包含字母 A..Z 的字符串 $S 和一个数字 $N。 写一个脚本，用凯撒密码对给定的字符串 $S 进行加密，左移大小为 $N。\n例子: 输入: $S = \u0026ldquo;THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\u0026rdquo;, $N = 3 输出: \u0026ldquo;QEB NRFZH YOLTK CLU GRJMP LSBO QEB IXWV ALD\u0026rdquo;\nPlain: ABCDEFGHIJKLMNOPQRSTUVWXYZ Cipher: XYZABCDEFGHIJKLMNOPQRSTUVW\nPlaintext: THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG Ciphertext: QEB NRFZH YOLTK CLU GRJMP LSBO QEB IXWV ALD\n\u0026ldquo;只用字母 A...Z\u0026rdquo; 的表述是错误的，因为例子中也有几个空格。因此，应该允许这些空格。\n文件: caesar-cipher\n#! /usr/bin/env raku subset AZ-space of Str where /^ \u0026lt;[ A .. Z \\s ]\u0026gt;+ $/; # [1] subset PosInt of Int where -25 \u0026lt;= $_ \u0026lt;= 25; # [2] unit sub MAIN (AZ-space $S = \u0026#39;THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\u0026#39;, PosInt $N = 3); # [3] say $S.comb.map({ caesar($_, $N) }).join; # [4] sub caesar ($char, $shift) { return $char if $char eq \u0026#34;\u0026#34;; # [5] my $code = $char.ord; # [6] $code -= $shift; # [7] $code += 26 if $code \u0026lt; 65; # \u0026#39;A\u0026#39; # [8] $code -= 26 if $code \u0026gt; 90; # \u0026#39;Z\u0026#39; # [8a] return $code.chr; # [9] } [1] 所允许的字符 (或 «特定领域字母»).\n[2] 挑战说左移值是一个数字。允许除整数以外的任何东西是没有意义的，所以我把值限制在这个类型。负值应该是可以的，它们意味着右移值（而不是左移）。\n[3] 参数，默认值为挑战中给出的值。\n[4] 将字符串分割成单个字符（用梳子(comb)，在每个字符上应用 \u0026ldquo;caesar\u0026rdquo; 函数（用map），再次将字符连接成一个字符串（用join），然后打印出来。\n[5] 不移动空格。\n[6] 获取字符的代码点。\n[7] 减去移位值（当我们向左移位时，或在字母表中降低移位值）。\n[8] 绕回, 如果我们移出A-Z范围，这里为更低 - 或更高的 [8b]。\n[9] 获取指定代码点的字符。\n查看 docs.raku.org/routine/ord 获取更多关于 ord 的信息。\n查看 docs.raku.org/routine/chr 获取更多关于 chr 的信息。\n运行它:\n$ ./caesar-cipher 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' 3 QEB NRFZH YOLTK CLU GRJMP LSBO QEB IXWV ALD $ ./caesar-cipher 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' -3 WKH TXLFN EURZQ IRA MXPSV RYHU WKH ODCB GRJ $ ./caesar-cipher 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' 13 GUR DHVPX OEBJA SBK WHZCF BIRE GUR YNML QBT $ ./caesar-cipher 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' -13 GUR DHVPX OEBJA SBK WHZCF BIRE GUR YNML QBT Raku 有一个 ords 变体，它接收一整个字符串，而不是一个字符作为 ord。还有 chrs，它接收一个代码点数组，并将它们变成一个字符串，而不是像 chr 那样接收一个字符的代码点。让我们用它们来写一个更短的程序。\n文件: caesar-cipher-map\n#! /usr/bin/env raku subset AZ-space of Str where /^ \u0026lt;[ A .. Z \\s ]\u0026gt;+ $/; subset PosInt of Int where -25 \u0026lt;= $_ \u0026lt;= 25; unit sub MAIN (AZ-space $S = \u0026#39;THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\u0026#39;, PosInt $N = 3); say caesar($S, $N); sub caesar ($string, $shift) { return $string.ords.map({$_ == 32 ?? 32 !! (($_ - $shift - 65) % 26 ) + 65}).chrs; # #################### # 1a ############# ############ # 1b # 1c ## 1d } [1] 我们使用map来改变各个代码点。我们让代码点为32的空间单独存在[1a]。每一个其他的值我们都还原成0到25之间的数字（通过减去第一个字母的代码点（A：65）和移位值[1b]）。模数运算符 (%) 为我们处理负值，做正确的事情。例如：-2 % 26 -\u0026gt; 24 [1c]。然后我们添加调整值到它们应该在的位置(从A到Z)[1d]，然后我们将整个数组的代码点变成一个字符串。\n查看 docs.raku.org/routine/ords 获取更多关于 ords 的信息。\n查看 docs.raku.org/routine/chrs 获取更多关于 chrs 的信息。\n运行它的结果和之前一样。\n$ ./caesar-cipher-map 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' 3 QEB NRFZH YOLTK CLU GRJMP LSBO QEB IXWV ALD $ ./caesar-cipher-map 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' -3 WKH TXLFN EURZQ IRA MXPSV RYHU WKH ODCB GRJ $ ./caesar-cipher-map 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' 13 GUR DHVPX OEBJA SBK WHZCF BIRE GUR YNML QBT $ ./caesar-cipher-map 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' -13 GUR DHVPX OEBJA SBK WHZCF BIRE GUR YNML QBT Perl 版本 这是对第一个 Raku 版的直接翻译。\nFile: caesar-cipher-perl\n#! /usr/bin/env perl use strict; use warnings; use feature \u0026#39;say\u0026#39;; use feature \u0026#39;signatures\u0026#39;; no warnings \u0026#34;experimental::signatures\u0026#34;; my $S = shift(@ARGV) // \u0026#39;THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\u0026#39;; die \u0026#34;Illegal characters\u0026#34; unless $S =~/^[A-Z\\s]+$/; my $N = shift(@ARGV) // 3; die \u0026#34;Illegal shift $N\u0026#34; if $N !~ /^\\-?\\d+$/ || $N \u0026lt; -25 || $N \u0026gt; 25; say join(\u0026#34;\u0026#34;, map { caesar($_, $N) } split(//, $S)); sub caesar ($char, $shift) { return $char if $char eq \u0026#34; \u0026#34;; my $code = ord($char); $code -= $shift; $code += 26 if $code \u0026lt; 65; # \u0026#39;A\u0026#39; $code -= 26 if $code \u0026gt; 90; # \u0026#39;Z\u0026#39; return chr($code); } 运行它的结果和 Raku 版一样。\n$ ./caesar-cipher-perl 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' 3 QEB NRFZH YOLTK CLU GRJMP LSBO QEB IXWV ALD $ ./caesar-cipher-perl 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' -3 WKH TXLFN EURZQ IRA MXPSV RYHU WKH ODCB GRJ $ ./caesar-cipher-perl 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' 13 GUR DHVPX OEBJA SBK WHZCF BIRE GUR YNML QBT $ ./caesar-cipher-perl 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG' -13 GUR DHVPX OEBJA SBK WHZCF BIRE GUR YNML QBT 挑战 #097.2：二进制子字符串(Binary Substrings) 给你一个二进制字符串 $B 和一个整数 $S。\n写一个脚本来拆分大小为 $S 的二进制字符串 $B，然后找出使其相同的最小翻转次数。\n例 1: 输入: $B = “101100101”, $S = 3 输出: 1\n二进制子字符串: \u0026ldquo;101\u0026rdquo;: 0 flip \u0026ldquo;100\u0026rdquo;: 1 flip to make it \u0026ldquo;101\u0026rdquo; \u0026ldquo;101\u0026rdquo;: 0 flip\n例 2: 输入 $B = “10110111”, $S = 4 输出: 2\n二进制子字符串: \u0026ldquo;1011\u0026rdquo;: 0 flip \u0026ldquo;0111\u0026rdquo;: 2 flips to make it \u0026ldquo;1011\u0026rdquo;\n我们先从第一个例子中的二进制子字符串中砍掉3个字符块。\n\u0026gt; say \u0026quot;101100101\u0026quot;.comb(3); # -\u0026gt; (101 100 101) \u0026gt; say \u0026quot;1011001010\u0026quot;.comb(3); # -\u0026gt; (101 100 101 0) 第二行显示了如果长度不匹配会发生什么。这就给了我们一个非法的值，因为我们不能将一位数翻转为三位数的值。所以我们必须添加一个检查。\n然后我们将第一个子串与其余的子串进行比较，一次一个。在这里使用bitwise XOR（Exclusive OR）运算符是一个合理的选择。这给了我们一个二进制值，其中1的数量就是该子串的翻转次数。Raku确实有一个XOR运算符。+^. 但是它 \u0026ldquo;将两个参数都强制为Int，并进行位智XOR操作\u0026rdquo;(根据文档\u0026quot;；参见docs.raku.org/language/operators#infix_+^)。\n我们可以在进行XOR操作之前，将二进制值转换为十进制值。让我们试试。\n获取翻转的次数。\n\u0026gt; say (\u0026quot;10101\u0026quot;.parse-base(2) +^ \u0026quot;10111\u0026quot;.parse-base(2)).base(2).comb.sum; # -\u0026gt; 1 \u0026gt; say (\u0026quot;11101\u0026quot;.parse-base(2) +^ \u0026quot;10111\u0026quot;.parse-base(2)).base(2).comb.sum; # -\u0026gt; 2 这当然可行，但需要大量的代码。所以我将使用一个更简单的方法 - 逐个比较每个数字。\nFile: binary-substring\n#! /usr/bin/env raku subset BinaryString where /^ \u0026lt;[01]\u0026gt;+ $/; # [1] subset PosInt of Int where * \u0026gt; 0; # [2] unit sub MAIN (BinaryString $B = \u0026#39;101100101\u0026#39;, # [1] PosInt $S where $B.chars %% $S = 3, # [2] :v(:$verbose)); my @B = $B.comb($S.Int); # [3] my $first = @B.shift; # [4] my $total = 0; # [5] for @B -\u0026gt; $current # [6] { my $flip = bit-diff($first, $current); # [7] $total += $flip; # [8] say \u0026#34;: $first-\u0026gt; $current-\u0026gt; Flip: $flip\u0026#34; if $verbose; } say $total; # [9] sub bit-diff ($a, $b) # [7] { my $flip = 0; # [10] for ^$a.chars -\u0026gt; $index # [11] { $flip++ if $a.substr($index,1) ne $b.substr($index,1); # [12] } return $flip; } [1] 确保二进制字符串是合法的（只包含 \u0026ldquo;0 \u0026ldquo;和 \u0026ldquo;1\u0026rdquo;）。\n[2] 确保是一个正整数，同时确保字符串是被它偶数分割的。(例如，\u0026ldquo;4 \u0026ldquo;给我们提供了长度为4的子串，如果最后一个较短，程序将中止。)\n[3] 梳子通常用于将一个字符串分割成单个字符，但我们可以通过指定长度来获得每个子字符串中的多个字符，比如这样。\n[4] 例子首先将第一个子串与自己进行比较，给出零翻转。这是愚蠢的(ish)，所以我跳过这一点，把第一个子串移出。\n[5] 结果会到这里。\n[6] 对于每一个子串（除了第一个，见[4]）。\n[7] 获取每个子串的翻转次数。\n[8] 并将其添加到总数中。\n[9] 打印它。\n[10] 翻转的数量会在这里。\n[11] 对于两个子串中的每个索引（具有相同的长度）。\n[12] - 如果给定位置上的字符不同，则在总数的基础上加1，意味着移动。\nSee docs.raku.org/routine/comb for more information about comb.\n运行它。\n$ ./binary-substring \u0026quot;101100101\u0026quot; 3 1 $ ./binary-substring -v \u0026quot;101100101\u0026quot; 3 : 101 -\u0026gt; 100 -\u0026gt; Flip: 1 : 101 -\u0026gt; 101 -\u0026gt; Flip: 0 1 $ ./binary-substring \u0026quot;10110111\u0026quot; 4 2 $ ./binary-substring -v \u0026quot;10110111\u0026quot; 4 : 1011 -\u0026gt; 0111 -\u0026gt; Flip: 2 2 看起来不错。\nPerl 这是对 Raku 版本的直接翻译，只是我必须实现 \u0026ldquo;comb\u0026rdquo;。\n文件： binary-substring-perl\n#! /usr/bin/env perl use strict; use warnings; use feature \u0026#39;say\u0026#39;; use feature \u0026#39;signatures\u0026#39;; use Getopt::Long; no warnings \u0026#34;experimental::signatures\u0026#34;; my $verbose = 0; GetOptions(\u0026#34;verbose\u0026#34; =\u0026gt; \\$verbose); my $B = shift(@ARGV) // \u0026#39;101100101\u0026#39;; die \u0026#34;Not a binary number\u0026#34; unless $B =~ /^[01]+$/; my $S = shift(@ARGV) // 3; die \u0026#34;Not an integer\u0026#34; unless $S =~ /^[1-9][0-9]*$/; die \u0026#34;Not a legal length\u0026#34; if length($B) % $S; my @B = comb($B, $S); my $first = shift(@B); my $total = 0; for my $current (@B) { my $flip = bit_diff($first, $current); $total += $flip; say \u0026#34;: $first-\u0026gt; $current-\u0026gt; Flip: $flip\u0026#34; if $verbose; } say $total; sub bit_diff ($a, $b) { my $flip = 0; for my $index (0 .. length($a)) { $flip++ if substr($a, $index,1) ne substr($b, $index,1); } return $flip; } sub comb ($string, $length = 1) # [1] { my @result; while ($string) { push(@result, substr($string, 0, $length)); $string = substr($string, $length); } return @result; } [1] 缺失的 Raku 例程 \u0026ldquo;comb\u0026rdquo;。可选的第二个参数指定了它所返回的每个子串中所包含的（第一个参数的）子串长度。\n运行它的结果与 Raku 版本相同。\n$ ./binary-substring-perl \u0026quot;101100101\u0026quot; 3 1 $ ./binary-substring-perl -v \u0026quot;101100101\u0026quot; 3 : 101 -\u0026gt; 100 -\u0026gt; Flip: 1 : 101 -\u0026gt; 101 -\u0026gt; Flip: 0 1 $ ./binary-substring-perl \u0026quot;101100111\u0026quot; 3 2 $ ./binary-substring-perl -v \u0026quot;101100111\u0026quot; 3 : 101 -\u0026gt; 100 -\u0026gt; Flip: 1 : 101 -\u0026gt; 111 -\u0026gt; Flip: 1 2 就是这样。\n","permalink":"https://ohmyweekly.github.io/notes/2021-01-30-caesarean-substrings-with-raku-and-perl/","tags":["Raku","Rakulang"],"title":"Caesarean Substrings With Raku and Perl"},{"categories":["Flink"],"contents":"If Sets Would DWIM 每当我在 Raku 中使用集合的时候，它们经常无法 DWIM。这是一个简短的探索，看看是否可以改进 DWIMminess。\n我最近重新审视了我前段时间写的一个利用 (-) 集差运算符的脚本。这段代码有一个 bug 潜伏在那里，显而易见，因为下面的代码并没有按照我的直觉去做。\nmy @allowed = \u0026lt;m c i p l o t\u0026gt;; my @chars = \u0026#39;impolitic\u0026#39;.comb; my @remainder = @allowed (-) @chars; if +@remainder == 0 { say \u0026#39;pangram\u0026#39;; } else { say \u0026#34;unused: [{@remainder.join(\u0026#39;\u0026#39;)}]\u0026#34;; } unused: [] 错误的原因是 (-) 产生了一个 Set，而赋值给 @remainder 会产生1项的 Array。总是这样。但不方便的是，当它是一个空集合时，它就会字符串化为一个空字符串，这只是帮助掩盖了这个潜伏的错误。\nmy @items = \u0026lt;a b c d e\u0026gt; (-) \u0026lt;a b c d e\u0026gt;; say @items.raku; say +@items; [Set.new()] 1 解决方法比较简单。只要不赋值给数组就可以了。使用一个标量容器来代替。\nmy $items = \u0026lt;a b c d e\u0026gt; (-) \u0026lt;a b d\u0026gt;; say $items.raku; say +$items; Set.new(\u0026quot;e\u0026quot;,\u0026quot;c\u0026quot;) 2 甚至是关联容器也可以。\nmy %items = \u0026lt;a b c d e\u0026gt; (-) \u0026lt;a b d\u0026gt;; say %items.raku; say +%items; {:c(Bool::True), :e(Bool::True)} 2 或在赋值前明确地取出键的列表。\nmy @items = (\u0026lt;a b c d e\u0026gt; (-) \u0026lt;a b d\u0026gt;).keys; say @items.raku; say +@items; [\u0026quot;e\u0026quot;, \u0026quot;c\u0026quot;] 2 很好，起作用了。只是不要用数组容器来处理 Setty 这样的东西。只是这并不能阻止我的直觉时不时地碰上这个错误。同一类的 bug 在我的代码中出现过好几次，因为它实在是太容易犯错了。Raku 不会告诉我，我做错了什么，因为也许是故意的。但重要的是， Raku 没有设法 DWIM。\n我可以采取的另一个方法是养成添加类型信息的习惯。这样确实可以让 Raku 在我掉进这个陷阱的时候告诉我。\nmy Str @a = \u0026lt;a b c d e\u0026gt; (-) \u0026lt;a b d\u0026gt;; Type check failed in assignment to @a; expected Str but got Set (Set.new(\u0026quot;e\u0026quot;,\u0026quot;c\u0026quot;)) in sub at EVAL_0 line 3 in block \u0026lt;unit\u0026gt; at EVAL_0 line 5 in block \u0026lt;unit\u0026gt; at -e line 1 这是一个明显的例子，添加类型信息有助于 Raku 编译器帮助我避免引入这种 bug。\n实验 - 为 Set 自定义数组存储 我开始研究核心设置(core setting)，看看可以做什么。我惊喜地发现，我可以在 Array.STORE 的多重分派中添加我正在寻找的语义。\nuse MONKEY; augment class Array { multi method STORE(Array:D: Set \\item --\u0026gt; Array:D) { self.STORE(item.keys) } } my @a = \u0026lt;a b c d e\u0026gt; (-) \u0026lt;a b d\u0026gt;; say @a.raku; say +@a; [\u0026quot;c\u0026quot;, \u0026quot;e\u0026quot;] 2 分享这个似乎是谨慎的，看看我的小 DWIM 是否有任何我没有考虑到的问题或缺点。一个可能的缺点是，如果你需要这样做的话，你需要使用 , 来强制将一个集合变成一个数组。\nmy @a = \u0026lt;a b c d e\u0026gt; (-) \u0026lt;a b d\u0026gt; , ; say @a.raku; [Set.new(\u0026quot;e\u0026quot;,\u0026quot;c\u0026quot;)] 下一步是什么 我希望这能引发关于这个问题以及其他我们的直觉和 Raku 的行为不太一致的情况的讨论。也许还有其他相关的语言边缘可以被磨平，以消除这种危害。\n后续 在 Reddit 上有一些非常有启发性的讨论，涵盖了语言语义和各种替代方法。公平地说，我建议的方法引入了更多的不一致性，而不是价值，但讨论可能会导致一个语言一致的解决方案。\n","permalink":"https://ohmyweekly.github.io/notes/2021-02-07-if-sets-would-dwim/","tags":["Raku","Rakulang"],"title":"如果集合如我所想"},{"categories":["Rust"],"contents":"Grammar 与许多解析工具一样，pest 使用与 Rust 代码不同的正式 grammar 进行操作。pest 使用的格式称为解析表达式 grammar，或 PEG。当构建一个项目时，pest 会自动将位于单独文件中的 PEG 编译成您可以调用的普通 Rust 函数。\n如何激活 pest 大多数项目至少会有两个使用 pest 的文件：解析器 (比如 src/parser/mod.rs) 和 grammar (src/parser/grammar.pest)。假设它们在同一个目录下。\nusepest::Parser;#[derive(Parser)]#[grammar = \u0026#34;parser/grammar.pest\u0026#34;]// relative to project `src` struct MyParser;每当你编译这个文件时，pest 会自动使用 grammar 文件生成这样的项。\npubenum Rules{/* ... */}implParserforMyParser{pubfn parse(Rules,\u0026amp;str)-\u0026gt; pest::Pairs{/* ... */}}你永远不会看到 enum Rules 或 impl Parser 的纯文本。这些代码只存在于编译过程中。然而，您可以像使用其他枚举一样使用 Rules，并且您可以通过 Parser API 章节中描述的 Pairs 接口使用 parse(...)。\n关于 PEGs 的警告! 解析表达式 grammar 看起来和你可能习惯的其他解析工具很相似，比如正则表达式、BNF grammar 和其他工具（Yacc/Bison、LALR、CFG）。然而，PEGs 的行为却有微妙的不同。PEGs 是急切的、非回溯的、有序的、不含糊的。\n如果你不认识以上任何一个名字，不要害怕! 你已经比认识的人快了一步 - 当你使用 pest 的 PEGs 时，你不会被与其他工具的比较所绊倒。\n如果你之前使用过其他解析工具，一定要仔细阅读下一节。我们会提到一些关于 PEGs 的常见错误。\n解析表达式语法 解析表达式语法(PEG)只是严格地表示了如果你用手写一个解析器会写的简单的命令式代码。\nnumber = { // To recognize a number... ASCII_DIGIT+ // take as many ASCII digits as possible (at least one). } expression = { // To recognize an expression... number // first try to take a number... | \u0026quot;true\u0026quot; // or, if that fails, the string \u0026quot;true\u0026quot;. } 事实上，pest 产生的代码与上面注释中的伪代码十分相似。\nEagerness 当在输入字符串上运行重复的 PEG 表达式时。\nASCII_DIGIT+ // one or more characters from '0' to '9' 它尽可能多地运行该表达式（\u0026ldquo;急切地\u0026quot;或 \u0026ldquo;贪婪地\u0026quot;匹配）。它要么成功，消耗它所匹配的任何内容，并将剩余的输入传递到解析器的下一步。\n\u0026quot;42 boxes\u0026quot; ^ Running ASCII_DIGIT+ \u0026quot;42 boxes\u0026quot; ^ Successfully took one or more digits! \u0026quot; boxes\u0026quot; ^ Remaining unparsed input. 或失败，什么也不消耗。\n\u0026quot;galumphing\u0026quot; ^ Running ASCII_DIGIT+ Failed to take one or more digits! \u0026quot;galumphing\u0026quot; ^ Remaining unparsed input (everything). 如果一个表达式未能匹配，那么这个失败就会向上传播，最终导致解析失败，除非这个失败在 grammar 中的某个地方被\u0026quot;抓住\u0026rdquo;。选择操作符是\u0026quot;捕获\u0026quot;这种失败的一种方法。\n有序选择 选择操作符，写成一条竖线 |，是有序的。PEG 表达式 first | second 的意思是 \u0026ldquo;先试 first，但如果失败了，再试 second\u0026quot;。\n在许多情况下，顺序并不重要。例如，\u0026quot;true\u0026quot; | \u0026quot;false\u0026quot; 将匹配字符串 \u0026quot;true\u0026quot; 或字符串 \u0026quot;false\u0026quot;（如果两者都不出现，则失败）。\n然而，有时顺序确实很重要。考虑一下 PEG 表达式 \u0026quot;a\u0026quot; | \u0026quot;ab\u0026quot;。你可能期望它能匹配字符串 \u0026quot;a\u0026quot; 或字符串 \u0026quot;ab\u0026quot;。但事实并非如此 - 该表达式的意思是 \u0026ldquo;尝试 \u0026quot;a\u0026quot;；但如果失败，则尝试 \u0026quot;ab\u0026quot;。如果你正在匹配字符串 \u0026ldquo;abc\u0026rdquo;，尝试 \u0026quot;a\u0026quot; 不会失败；相反，它将成功匹配 \u0026quot;a\u0026quot;，留下 \u0026quot;bc\u0026quot; 未被解析。\n一般来说，当编写一个有选择的解析器时，把最长或最具体的选择放在前面，而把最短或最一般的选择放在最后。\n非回溯 在解析过程中，一个 PEG 表达式要么成功，要么失败。如果成功了，下一步就照常进行。但如果它失败了，整个表达式就会失败。引擎不会后退再试。\n请看下面这个 grammar，在字符串 \u0026quot;frumious\u0026quot; 上进行匹配。\nword = { // to recognize a word... ANY* // take any character, zero or more times... ~ ANY // followed by any character } 你可能期望这条规则能够解析任何至少包含一个字符（相当于 ANY+）的输入字符串。但它不会。相反，第一个 ANY* 会急切地吃掉整个字符串 - 它会得偿所愿的。然后，下一个 ANY 将一无所有，所以它会失败。\n\u0026quot;frumious\u0026quot; ^ (word) \u0026quot;frumious\u0026quot; ^ (ANY*) Success! Continue to `ANY` with remaining input \u0026quot;\u0026quot;. \u0026quot;\u0026quot; ^ (ANY) Failure! Expected one character, but found end of string. 在有回溯功能的系统中（比如正则表达式），你会往后退一步，\u0026ldquo;吐出\u0026quot;一个字符，然后再试。但 PEG 不会这样做。在规则 first~second 中，一旦 first 解析成功，就已经消耗了一些字符，永远不会再回来，second 只能在 first 没有消耗的输入上运行。\n毫不含糊 这些规则构成了一个优雅而简单的系统。每个 PEG 规则都会在输入字符串的剩余部分上运行，消耗尽可能多的输入。一旦一个规则完成，剩下的输入就会被传递给解析器的其他部分。\n例如，表达式 ASCII_DIGIT+，\u0026ldquo;一个或多个数字\u0026rdquo;，将始终匹配可能的最大的连续数字序列。不存在意外地让后面的规则回溯并以一种不直观和非局部的方式窃取一些数字的危险。\n这与其他解析工具形成了鲜明的对比，比如正则表达式和 CFG，在这些工具中，规则的结果往往取决于一些距离的代码。事实上，LR解析器中著名的\u0026quot;移位/还原冲突\u0026quot;在 PEG 中并不存在问题。\n不要惊慌 这一切在一开始可能有点反常。但正如你所看到的，基本的逻辑是非常简单和直接的。你可以琐碎地逐步完成任何 PEG 表达式的执行。\n 试试这个。 如果它成功了，就尝试下一件事。 否则，尝试另一件事。  (this ~ next_thing) | (other_thing) 这些规则结合在一起，使得 PEG 成为编写解析器的非常愉快的工具。\npet 解析器的语法 pet grammar 是规则的列表。规则是这样定义的。\nmy_rule = { ... } another_rule = { // comments are preceded by two slashes ... // whitespace goes anywhere } 由于规则名被翻译成 Rust enum 变体，所以不允许成为 Rust 关键字。\n定义规则的左大括号 { 前面可以有影响其操作的符号。\nsilent_rule = _{ ... } atomic_rule = @{ ... } 表达式 Grammar 规则是由表达式建立起来的（因此称为\u0026quot;解析表达式文法\u0026rdquo;）。这些表达式是对如何解析输入字符串的简明、正式的描述。\n表达式是可以组合的：它们可以从其他表达式中构建出来，也可以互相嵌套，以产生任意复杂的规则（尽管你应该将非常复杂的表达式分解成多个规则，以使它们更容易管理）。\nPEG 表达式既适用于高级意义，如\u0026quot;一个函数签名，后面是一个函数体\u0026rdquo;，也适用于低级意义，如\u0026quot;一个分号，后面是换行\u0026rdquo;。组合形式\u0026quot;后面是\u0026rdquo;，即序列操作符，在这两种情况下都是一样的。\n终端 最基本的规则是双引号的文字字符串。\u0026quot;text\u0026quot;。\n如果一个字符串前面有一个逗号，那么它可以不区分大小写（仅适用于 ASCII 字符）: ^\u0026quot;text\u0026quot;。\n在一个范围内的单个字符被写成两个单引号字符，用两个点分开：'0'...'9'。\n你可以用特殊规则 ANY 来匹配任何单个字符。这相当于 '\\u{00}'...'\\u{10FFFF}'，任何一个 Unicode 字符。\n\u0026quot;a literal string\u0026quot; ^\u0026quot;ASCII case-insensitive string\u0026quot; 'a'..'z' ANY 最后，你可以直接写出其他规则的名称来引用它们，甚至可以递归使用规则。\nmy_rule = { \u0026quot;slithy \u0026quot; ~ other_rule } other_rule = { \u0026quot;toves\u0026quot; } recursive_rule = { \u0026quot;mimsy \u0026quot; ~ recursive_rule } 序列 序列运算符写成一个波浪号 ~。\nfirst ~ and_then (\u0026quot;abc\u0026quot;) ~ (^\u0026quot;def\u0026quot;) ~ ('g'..'z') // matches \u0026quot;abcDEFr\u0026quot; 当匹配一个序列表达式时，尝试匹配 first。如果 first 匹配成功，则接下来尝试 and_then。但是，如果 first 失败，则整个表达式失败。\n表达式的列表可以与序列链在一起，这表明所有的组件必须出现，按照指定的顺序。\n有序选择 选择运算符写成一条竖线 |。\nfirst | or_else (\u0026quot;abc\u0026quot;) | (^\u0026quot;def\u0026quot;) | ('g'..'z') // matches \u0026quot;DEF\u0026quot; 当匹配一个选择表达式时，尝试匹配 first。如果 first 匹配成功，则整个表达式立即成功。但是，如果 first 失败，接下来会尝试 or_else。\n注意，first 和 or_else 总是在同一个位置尝试，即使 first 在失败之前匹配了一些输入。当遇到解析失败时，引擎会尝试下一个有序的选择，就像没有匹配到输入一样。失败的解析永远不会消耗任何输入。\nstart = { \u0026quot;Beware \u0026quot; ~ creature } creature = { (\u0026quot;the \u0026quot; ~ \u0026quot;Jabberwock\u0026quot;) | (\u0026quot;the \u0026quot; ~ \u0026quot;Jubjub bird\u0026quot;) } \u0026quot;Beware the Jubjub bird\u0026quot; ^ (start) Parses via the second choice of `creature`, even though the first choice matched \u0026quot;the \u0026quot; successfully. 借用术语，把这种操作看成是\u0026quot;交替\u0026quot;或简单的 \u0026ldquo;OR\u0026rdquo;，有点诱人，但这是误导。之所以特别使用 \u0026ldquo;选择\u0026rdquo; 这个词，是因为这个操作不仅仅是逻辑上的 \u0026ldquo;OR\u0026rdquo;。\n重复 有两个重复运算符：星号 * 和加号 +。它们被放在一个表达式之后。星号 * 表示前面的表达式可以出现零次或多次。加号 + 表示前面的表达式可以出现一次或多次（必须至少出现一次）。\n问号运算符 ? 类似，但它表示表达式是可选的 - 它可以出现0次或1次。\n(\u0026quot;zero\u0026quot; ~ \u0026quot;or\u0026quot; ~ \u0026quot;more\u0026quot;)* (\u0026quot;one\u0026quot; | \u0026quot;or\u0026quot; | \u0026quot;more\u0026quot;)+ (^\u0026quot;optional\u0026quot;)? 请注意，expr* 和 expr? 总是会成功，因为它们被允许匹配零次。例如，\u0026quot;a\u0026quot;* ~ \u0026quot;b\u0026quot;? 即使在空的输入字符串上也会成功。\n其他重复次数可以用大括号来表示。\nexpr{n} // exactly n repetitions expr{m, n} // between m and n repetitions, inclusive expr{, n} // at most n repetitions expr{m, } // at least m repetitions 因此，expr* 等同于 expr{0，}；expr+ 等同于 expr{1，}；expr? 等同于 expr{0，1}。\n谓词 在表达式前面加上安括号 \u0026amp; 或感叹号 !，就会变成一个不消耗任何输入的谓词。你可能知道这些运算符为 \u0026ldquo;向前查看\u0026rdquo; 或 \u0026ldquo;不进位\u0026rdquo;。\n写成安培符 \u0026amp; 的正式谓词试图匹配其内部表达式。如果内部表达式成功，解析就会继续，但位置与谓词相同 - \u0026amp;foo ~ bar 因此是一种 \u0026ldquo;AND\u0026rdquo; 语句。\u0026ldquo;输入字符串必须匹配 foo AND bar\u0026quot;。如果内部表达式失败，整个表达式也会失败。\n写成感叹号的否定谓词 !，试图匹配其内部表达式。如果内部表达式失败，则谓词成功，并在与谓词相同的位置继续解析。如果内部表达式成功，则谓词失败 - !foo ~ bar 因此是一种 \u0026ldquo;NOT\u0026rdquo; 语句。\u0026ldquo;输入的字符串必须与 bar 匹配，但不能是 foo\u0026quot;。\n这就引出了一个常见的惯用法，意思是\u0026quot;任何字符但是\u0026rdquo;：\nnot_space_or_tab = { !( // if the following text is not \u0026quot; \u0026quot; // a space | \u0026quot;\\t\u0026quot; // or a tab ) ~ ANY // then consume one character } triple_quoted_string = { \u0026quot;'''\u0026quot; ~ triple_quoted_character* ~ \u0026quot;'''\u0026quot; } triple_quoted_character = { !\u0026quot;'''\u0026quot; // if the following text is not three apostrophes ~ ANY // then consume one character } 操作符优先级和分组 (WIP) 重复运算符星号 *、加号 + 和问号 ? 适用于紧接前面的表达式。\n\u0026quot;One \u0026quot; ~ \u0026quot;or \u0026quot; ~ \u0026quot;more. \u0026quot;+ \u0026quot;One \u0026quot; ~ \u0026quot;or \u0026quot; ~ (\u0026quot;more. \u0026quot;+) are equivalent and match \u0026quot;One or more. more. more. more. \u0026quot; 较大的表达式可以通过用括号包围来重复。\n(\u0026quot;One \u0026quot; ~ \u0026quot;or \u0026quot; ~ \u0026quot;more. \u0026quot;)+ matches \u0026quot;One or more. One or more. \u0026quot; 重复运算符的优先性最高，其次是谓词运算符、序列运算符，最后是有序选择。\nmy_rule = { \u0026quot;a\u0026quot;* ~ \u0026quot;b\u0026quot;? | \u0026amp;\u0026quot;b\u0026quot;+ ~ \u0026quot;a\u0026quot; } // equivalent to my_rule = { ( (\u0026quot;a\u0026quot;*) ~ (\u0026quot;b\u0026quot;?) ) | ( (\u0026amp;(\u0026quot;b\u0026quot;+)) ~ \u0026quot;a\u0026quot; ) } 输入的开始和结束 规则 SOI 和 EOI 分别匹配输入字符串的开始和结束。两者都不消耗任何文本。它们只表明解析器当前是否在输入的一个边缘。\n例如，为了确保一条规则匹配整个输入，其中任何语法错误都会导致解析失败（而不是成功但不完整的解析）。\nmain = { SOI ~ (...) ~ EOI } 隐含的空白 许多语言和文本格式允许在逻辑标记之间任意留白和注释。例如，Rust 认为 4+5 相当于 4 + 5 和 4 /* comment */ + 5。\n可选规则 WHITESPACE 和 COMMENT 实现了这种行为。如果定义了这两个规则中的任何一个(或两个)，它们将被隐式地插入到每个序列和每个重复之间(原子规则除外)。\nexpression = { \u0026quot;4\u0026quot; ~ \u0026quot;+\u0026quot; ~ \u0026quot;5\u0026quot; } WHITESPACE = _{ \u0026quot; \u0026quot; } COMMENT = _{ \u0026quot;/*\u0026quot; ~ (!\u0026quot;*/\u0026quot; ~ ANY)* ~ \u0026quot;*/\u0026quot; } \u0026quot;4+5\u0026quot; \u0026quot;4 + 5\u0026quot; \u0026quot;4 + 5\u0026quot; \u0026quot;4 /* comment */ + 5\u0026quot; 正如你所看到的，WHITESPACE 和 COMMENT 是重复运行的，所以它们只需要匹配一个空白字符或一个注释。上面的 grammar 相当于。\nexpression = { \u0026quot;4\u0026quot; ~ (ws | com)* ~ \u0026quot;+\u0026quot; ~ (ws | com)* ~ \u0026quot;5\u0026quot; } ws = _{ \u0026quot; \u0026quot; } com = _{ \u0026quot;/*\u0026quot; ~ (!\u0026quot;*/\u0026quot; ~ ANY)* ~ \u0026quot;*/\u0026quot; } 请注意，隐式空格不会插入规则的开头或结尾 - 例如，表达式不匹配 \u0026quot; 4+5 \u0026quot;。如果你想在规则的开头和结尾加入隐式空格，你需要把它夹在两个空规则之间（通常是 SOI 和 EOI，如上所述）。\nWHITESPACE = _{ \u0026quot; \u0026quot; } expression = { \u0026quot;4\u0026quot; ~ \u0026quot;+\u0026quot; ~ \u0026quot;5\u0026quot; } main = { SOI ~ expression ~ EOI } \u0026quot;4+5\u0026quot; \u0026quot; 4 + 5 \u0026quot; (请务必将 WHITESPACE 和 COMMENT 规则标记为静默规则，除非你想在其他规则中看到它们！)\n静默规则和原子规则 静默规则就像普通规则一样 - 当运行时，它们的功能是一样的 - 除了它们不产生 pairs或 tokens。如果一条规则是静默的，那么它永远不会出现在解析结果中。\n要创建一个静默规则，请在左边的大括号 { 前加上一个下划线 _。\nsilent = _{ ... } 原子 pest 有两种原子规则：原子和复合原子。要做一个，在左大括号 { 前写上一个符号。\natomic = @{ ... } compound_atomic = ${ ... } 这两种原子规则都可以防止隐式空格：在原子规则中，波浪号 ~ 表示 \u0026ldquo;紧接着\u0026rdquo;，重复操作符（星号 * 和加号 +）没有隐式分隔。此外，所有从原子规则中调用的其他规则也被视为原子规则。\n两者的区别在于它们如何产生内部规则的标记。在一个原子规则中，内部匹配规则是静默的。相比之下，复合原子规则会像普通规则一样产生内部 token。\n当您要解析的文本忽略空白时，原子规则是很有用的，除了少数情况，例如文字字符串。在这种情况下，您可以编写 WHITESPACE 或 COMMENT 规则，然后使您的字符串匹配规则成为原子规则。\n非原子的 有时候，你会想要取消原子解析的效果。例如，你可能想在表达式内部进行字符串插值，里面的表达式仍然可以像正常的一样有空格。\n#!/bin/env python3 print(f\u0026#34;The answer is {2 + 4}.\u0026#34;) 这是你使用非原子规则的地方。在定义的大括号前面写一个感叹号 ! 无论是否从原子规则中调用，该规则都将作为非原子规则运行。\nfstring = @{ \u0026quot;\\\u0026quot;\u0026quot; ~ ... } expr = !{ ... } 堆栈(WIP) pest 维护了一个可以直接从 grammar 中操作的栈。一个表达式可以用关键字 PUSH 进行匹配并推到栈上，然后再用关键字 PEEK 和 POP 进行精确匹配。\n使用栈可以对完全相同的文本进行多次匹配，而不是相同的模式。\n例如:\nsame_text = { PUSH( \u0026quot;a\u0026quot; | \u0026quot;b\u0026quot; | \u0026quot;c\u0026quot; ) ~ POP } same_pattern = { (\u0026quot;a\u0026quot; | \u0026quot;b\u0026quot; | \u0026quot;c\u0026quot;) ~ (\u0026quot;a\u0026quot; | \u0026quot;b\u0026quot; | \u0026quot;c\u0026quot;) } 在这种情况下，same_pattern 会匹配 \u0026quot;ab\u0026quot;，而 same_text 不会。\n一个实际的用途是解析 Rust 的 \u0026ldquo;原始字符串字面值\u0026quot;，它看起来像这样。\nconstraw_str: \u0026amp;str =r###\u0026#34; Some number of number signs # followed by a quotation mark \u0026#34;. Quotation marks can be used anywhere inside: \u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;, as long as one is not followed by a matching number of number signs, which ends the string: \u0026#34;###;当解析一个原始字符串时，我们必须跟踪引号前出现了多少个数字符号 #。我们可以使用栈来完成这个任务。\nraw_string = { \u0026quot;r\u0026quot; ~ PUSH(\u0026quot;#\u0026quot;*) ~ \u0026quot;\\\u0026quot;\u0026quot; // push the number signs onto the stack ~ raw_string_interior ~ \u0026quot;\\\u0026quot;\u0026quot; ~ POP // match a quotation mark and the number signs } raw_string_interior = { ( !(\u0026quot;\\\u0026quot;\u0026quot; ~ PEEK) // unless the next character is a quotation mark // followed by the correct amount of number signs, ~ ANY // consume one character )* } 小抄    语法 含义 语法 含义     foo = { ... } regular rule baz = @{ ... } atomic   bar = _{ ... } silent` qux = ${ ... } compound-atomic     plugh = !{ ... } non-atomic   \u0026quot;abc\u0026quot; exact string ^\u0026quot;abc\u0026quot; case insensitive   'a'..'z' character range ANY any character   foo ~ bar sequence `baz qux`   foo* zero or more bar+ one or more   baz? optional qux{n} exactly n   qux{m, n} between m and n (inclusive)     \u0026amp;foo positive predicate !bar negative predicate   PUSH(baz) match and push     POP match and pop PEEK match without pop    内置规则 除了 ANY，匹配任何单一的 Unicode 字符外，pest 还提供了几条规则，让解析文本更加方便。\nASCII 规则 在可打印的 ASCII 字符中，它通常对匹配字母字符和数字很有用。对于数字，pest 提供了常见的（基数）的数字。\n   Built-in rule Equivalent     ASCII_DIGIT '0'..'9'   ASCII_NONZERO_DIGIT '1'..'9'   ASCII_BIN_DIGIT '0'..'1'   ASCII_OCT_DIGIT '0'..'7'   ASCII_HEX_DIGIT `\u0026lsquo;0\u0026rsquo;..\u0026lsquo;9\u0026rsquo;    对于字母字符，要区分大写和小写。\n   Built-in rule Equivalent     ASCII_ALPHA_LOWER 'a'..'z'   ASCII_ALPHA_UPPER 'A'..'Z'   ASCII_ALPHA `\u0026lsquo;a\u0026rsquo;..\u0026lsquo;z\u0026rsquo;    And for miscellaneous use:\n   Built-in rule Meaning Equivalent     ASCII_ALPHANUMERIC any digit or letter `ASCII_DIGIT   NEWLINE any line feed format `\u0026quot;\\n\u0026rdquo;    统一码规则 为了更容易正确解析任意 Unicode 文本，pest 包含了大量对应 Unicode 字符属性的规则。这些规则分为一般类别和二进制属性规则。\nUnicode 字符根据其一般用途被划分为不同的类别。每一个字符都属于一个类别，就像每一个 ASCII 字符都是一个控制字符、一个数字、一个字母、一个符号或一个空格一样。\n此外，每个 Unicode 字符都有一个二进制属性列表（真或假），它满足或不满足这些属性。字符可以属于任何数量的这些属性，这取决于它们的含义。\n例如，字符 \u0026ldquo;A\u0026rdquo;，\u0026ldquo;拉丁文大写字母A\u0026rdquo;，属于一般的 \u0026ldquo;大写字母\u0026rdquo; 类别，因为它的一般用途是字母。它具有 \u0026ldquo;大写字母\u0026rdquo; 的二元属性，但不具有 \u0026ldquo;表情符号\u0026rdquo; 的属性。相比之下，\u0026ldquo;负数平方的拉丁文大写字母A\u0026rdquo; 这个字符，因为在文本中一般不作为字母出现，所以属于一般类别 \u0026ldquo;其他符号\u0026rdquo;。它同时具有 \u0026ldquo;大写字母\u0026rdquo; 和 \u0026ldquo;表情符号\u0026rdquo; 的二元属性。\n详情请参考《Unicode 标准》第四章。\n一般类别 从形式上看，类别是不重叠的：每个 Unicode 字符正好属于一个类别，没有一个类别包含另一个类别。然而，由于某些类别组经常一起使用，pest 在下面暴露了类别的层次结构。例如，规则 CASED_LETTER 在技术上不是 Unicode 通用类别，而是匹配属于 UPPERCASE_LETTER 或LOWERCASE_LETTER 的字符，这些都是通用类别。\n LETTER CASED_LETTER UPPERCASE_LETTER LOWERCASE_LETTER TITLECASE_LETTER MODIFIER_LETTER OTHER_LETTER MARK NONSPACING_MARK SPACING_MARK ENCLOSING_MARK NUMBER DECIMAL_NUMBER LETTER_NUMBER OTHER_NUMBER PUNCTUATION CONNECTOR_PUNCTUATION DASH_PUNCTUATION OPEN_PUNCTUATION CLOSE_PUNCTUATION INITIAL_PUNCTUATION FINAL_PUNCTUATION OTHER_PUNCTUATION SYMBOL MATH_SYMBOL CURRENCY_SYMBOL MODIFIER_SYMBOL OTHER_SYMBOL SEPARATOR SPACE_SEPARATOR LINE_SEPARATOR PARAGRAPH_SEPARATOR OTHER CONTROL FORMAT SURROGATE PRIVATE_USE UNASSIGNED  Binary properties 这些属性中有许多是用来定义 Unicode 文本算法的，如双向算法和文本分割算法。这类属性对于大多数解析器来说可能并不有用。\n但是，XID_START 和 XID_CONTINUE 这两个属性特别值得注意，因为它们被定义为 \u0026ldquo;协助标识符的标准处理\u0026rdquo;，\u0026ldquo;如编程语言变量\u0026rdquo;。详见技术报告31。\n ALPHABETIC BIDI_CONTROL CASE_IGNORABLE CASED CHANGES_WHEN_CASEFOLDED CHANGES_WHEN_CASEMAPPED CHANGES_WHEN_LOWERCASED CHANGES_WHEN_TITLECASED CHANGES_WHEN_UPPERCASED DASH DEFAULT_IGNORABLE_CODE_POINT DEPRECATED DIACRITIC EXTENDER GRAPHEME_BASE GRAPHEME_EXTEND GRAPHEME_LINK HEX_DIGIT HYPHEN IDS_BINARY_OPERATOR IDS_TRINARY_OPERATOR ID_CONTINUE ID_START IDEOGRAPHIC JOIN_CONTROL LOGICAL_ORDER_EXCEPTION LOWERCASE MATH NONCHARACTER_CODE_POINT OTHER_ALPHABETIC OTHER_DEFAULT_IGNORABLE_CODE_POINT OTHER_GRAPHEME_EXTEND OTHER_ID_CONTINUE OTHER_ID_START OTHER_LOWERCASE OTHER_MATH OTHER_UPPERCASE PATTERN_SYNTAX PATTERN_WHITE_SPACE PREPENDED_CONCATENATION_MARK QUOTATION_MARK RADICAL REGIONAL_INDICATOR SENTENCE_TERMINAL SOFT_DOTTED TERMINAL_PUNCTUATION UNIFIED_IDEOGRAPH UPPERCASE VARIATION_SELECTOR WHITE_SPACE XID_CONTINUE XID_START  例子: JSON JSON 是一种流行的数据序列化格式，它源于 JavaScript 的语法。JSON 文档是树状的，并且可能是递归的\u0026ndash;对象和数组这两种数据类型可以包含其他值，包括其他对象和数组。\n下面是一个 JSON 文档的例子。\n{ \u0026#34;nesting\u0026#34;: { \u0026#34;inner object\u0026#34;: {} }, \u0026#34;an array\u0026#34;: [1.5, true, null, 1e-6], \u0026#34;string with escaped double quotes\u0026#34; : \u0026#34;\\\u0026#34;quick brown foxes\\\u0026#34;\u0026#34; } 让我们写一个程序，将 JSON 解析成一个 Rust 对象，也就是抽象语法树，然后将 AST 序列化回 JSON。\n设置 我们将从定义 Rust 中的 AST 开始。每个 JSON 数据类型都由一个枚举变体来表示。\nenum JSONValue\u0026lt;\u0026#39;a\u0026gt;{Object(Vec\u0026lt;(\u0026amp;\u0026#39;astr,JSONValue\u0026lt;\u0026#39;a\u0026gt;)\u0026gt;),Array(Vec\u0026lt;JSONValue\u0026lt;\u0026#39;a\u0026gt;\u0026gt;),String(\u0026amp;\u0026#39;astr),Number(f64),Boolean(bool),Null,}为了避免反序列化字符串时的复制，JSONValue 从原始未解析的 JSON 中借用字符串。为了使其工作，我们不能解释字符串转义序列：输入字符串 \u0026ldquo;\\n\u0026rdquo; 将由 JSONValue::String(\u0026quot;\\n\u0026rdquo;) 表示，这是一个有两个字符的 Rust 字符串，尽管它表示的是一个只有一个字符的 JSON 字符串。\n让我们继续看序列化器。为了清晰起见，它使用分配的 Strings，而不是提供 std::fmt::Display 的实现，后者会更习惯。\nfn serialize_jsonvalue(val: \u0026amp;JSONValue)-\u0026gt; String {useJSONValue::*;matchval{Object(o)=\u0026gt;{letcontents: Vec\u0026lt;_\u0026gt;=o.iter().map(|(name,value)|format!(\u0026#34;\\\u0026#34;{}\\\u0026#34;:{}\u0026#34;,name,serialize_jsonvalue(value))).collect();format!(\u0026#34;{{{}}}\u0026#34;,contents.join(\u0026#34;,\u0026#34;))}Array(a)=\u0026gt;{letcontents: Vec\u0026lt;_\u0026gt;=a.iter().map(serialize_jsonvalue).collect();format!(\u0026#34;[{}]\u0026#34;,contents.join(\u0026#34;,\u0026#34;))}String(s)=\u0026gt;format!(\u0026#34;\\\u0026#34;{}\\\u0026#34;\u0026#34;,s),Number(n)=\u0026gt;format!(\u0026#34;{}\u0026#34;,n),Boolean(b)=\u0026gt;format!(\u0026#34;{}\u0026#34;,b),Null=\u0026gt;format!(\u0026#34;null\u0026#34;),}}请注意，在对象和数组的情况下，函数会递归地调用自己。这种模式出现在整个解析器中。AST创建函数在解析结果中递归迭代，而语法的规则也包括了自己。\ngrammar 的编写 让我们从 whitespace 开始。JSON 空格可以出现在任何地方，除了字符串内部（必须单独解析）和数字中的数字之间（不允许）。这使得它很适合 pest 的隐式空白。在 src/json.pest:\nWHITESPACE = _{ \u0026quot; \u0026quot; | \u0026quot;\\t\u0026quot; | \u0026quot;\\r\u0026quot; | \u0026quot;\\n\u0026quot; } JSON 规范包括解析 JSON 字符串的图。我们可以直接从该页面写出语法。让我们把 object 写成一个用逗号分隔的对的序列。\nobject = { \u0026quot;{\u0026quot; ~ \u0026quot;}\u0026quot; | \u0026quot;{\u0026quot; ~ pair ~ (\u0026quot;,\u0026quot; ~ pair)* ~ \u0026quot;}\u0026quot; } pair = { string ~ \u0026quot;:\u0026quot; ~ value } array = { \u0026quot;[\u0026quot; ~ \u0026quot;]\u0026quot; | \u0026quot;[\u0026quot; ~ value ~ (\u0026quot;,\u0026quot; ~ value)* ~ \u0026quot;]\u0026quot; } 对象和数组规则展示了如何用分隔符解析一个潜在的空列表。有两种情况：一种是空列表，另一种是至少有一个元素的列表。这是必要的，因为数组中的逗号，如 [0，1，]，在 JSON 中是非法的。\n现在我们可以写 value，它代表任何单一的数据类型。我们将模仿我们的 AST，将 boolean 和 null 写成单独的规则。\nvalue = _{ object | array | string | number | boolean | null } boolean = { \u0026quot;true\u0026quot; | \u0026quot;false\u0026quot; } null = { \u0026quot;null\u0026quot; } 让我们把字符串的逻辑分成三个部分。char 是一个匹配字符串中任何逻辑字符的规则，包括任何反斜杠转义序列。inner 代表字符串的内容，不包括周围的双引号。string 匹配字符串的内部内容，包括周围的双引号。\nchar 规则使用成语 !(...) ~ ANY，它匹配除了括号中给出的字符之外的任何字符。在这种情况下，除了双引号 \u0026quot;\u0026quot; 和反斜杠 \\ 之外，任何字符在字符串内部都是合法的，这需要单独的解析逻辑。\nstring = ${ \u0026quot;\\\u0026quot;\u0026quot; ~ inner ~ \u0026quot;\\\u0026quot;\u0026quot; } inner = @{ char* } char = { !(\u0026quot;\\\u0026quot;\u0026quot; | \u0026quot;\\\\\u0026quot;) ~ ANY | \u0026quot;\\\\\u0026quot; ~ (\u0026quot;\\\u0026quot;\u0026quot; | \u0026quot;\\\\\u0026quot; | \u0026quot;/\u0026quot; | \u0026quot;b\u0026quot; | \u0026quot;f\u0026quot; | \u0026quot;n\u0026quot; | \u0026quot;r\u0026quot; | \u0026quot;t\u0026quot;) | \u0026quot;\\\\\u0026quot; ~ (\u0026quot;u\u0026quot; ~ ASCII_HEX_DIGIT{4}) } 因为 string 被标记为复原子，所以 string token 对也会包含一个 inner 对。因为 inner 被标记为原子，所以在 inner 中不会出现 char 对。由于这些规则是原子性的，所以在不同的标记之间不允许有空格。\n数字有四个逻辑部分：一个可选的符号、一个整数部分、一个可选的分数部分和一个可选的指数。我们将把数字标记为原子，这样它的部分之间就不能出现空白。\nnumber = @{ \u0026quot;-\u0026quot;? ~ (\u0026quot;0\u0026quot; | ASCII_NONZERO_DIGIT ~ ASCII_DIGIT*) ~ (\u0026quot;.\u0026quot; ~ ASCII_DIGIT*)? ~ (^\u0026quot;e\u0026quot; ~ (\u0026quot;+\u0026quot; | \u0026quot;-\u0026quot;)? ~ ASCII_DIGIT+)? } 我们需要一个最终规则来表示整个 JSON 文件。JSON 文件的唯一合法内容是一个对象或数组。我们将把这个规则标记为沉默，这样一个解析后的 JSON 文件只包含两个标记对：解析后的值本身，以及 EOI 规则。\njson = _{ SOI ~ (object | array) ~ EOI } AST 生成 让我们把 grammar 编译成 Rust。\nexterncratepest;#[macro_use]externcratepest_derive;usepest::Parser;#[derive(Parser)]#[grammar = \u0026#34;json.pest\u0026#34;]struct JSONParser;我们将写一个同时处理解析和 AST 生成的函数。该函数的用户可以在输入字符串上调用它，然后将返回的结果作为 JSONValue 或解析错误。\nusepest::error::Error;fn parse_json_file(file: \u0026amp;str)-\u0026gt; Result\u0026lt;JSONValue,Error\u0026lt;Rule\u0026gt;\u0026gt;{letjson=JSONParser::parse(Rule::json,file)?.next().unwrap();// ... }现在我们需要根据规则，递归处理 Pair。我们知道 json 是一个对象或者数组，但是这些值本身可能包含一个对象或者数组！这时，我们就需要写一个辅助递归函数，直接将 Pair 解析成 JSONValue。最合理的处理方式是写一个辅助递归函数，直接将 Pair 解析成 JSONValue。\nfn parse_json_file(file: \u0026amp;str)-\u0026gt; Result\u0026lt;JSONValue,Error\u0026lt;Rule\u0026gt;\u0026gt;{// ... usepest::iterators::Pair;fn parse_value(pair: Pair\u0026lt;Rule\u0026gt;)-\u0026gt; JSONValue{matchpair.as_rule(){Rule::object=\u0026gt;JSONValue::Object(pair.into_inner().map(|pair|{letmutinner_rules=pair.into_inner();letname=inner_rules.next().unwrap().into_inner().next().unwrap().as_str();letvalue=parse_value(inner_rules.next().unwrap());(name,value)}).collect(),),Rule::array=\u0026gt;JSONValue::Array(pair.into_inner().map(parse_value).collect()),Rule::string=\u0026gt;JSONValue::String(pair.into_inner().next().unwrap().as_str()),Rule::number=\u0026gt;JSONValue::Number(pair.as_str().parse().unwrap()),Rule::boolean=\u0026gt;JSONValue::Boolean(pair.as_str().parse().unwrap()),Rule::null=\u0026gt;JSONValue::Null,Rule::json|Rule::EOI|Rule::pair|Rule::value|Rule::inner|Rule::char|Rule::WHITESPACE=\u0026gt;unreachable!(),}}// ... }对象和数组的情况值得特别注意。数组令牌对的内容只是一个值的序列。由于我们使用的是 Rust 迭代器，我们可以简单地将每个值递归地映射到它的解析 AST 节点，然后将它们收集到一个 Vec 中。对于对象，过程是类似的，除了迭代器是在对上，我们需要分别从对上提取名称和值。\n数字和布尔的情况下，使用 Rust 的 str::parse 方法将解析后的字符串转换为相应的 Rust 类型。每一个合法的 JSON 数字都可以直接解析成一个 Rust 浮点数！我们在 Rust 的 str::parse 方法上运行 parse_value。\n我们对解析结果运行 parse_value 来完成转换。\nfn parse_json_file(file: \u0026amp;str)-\u0026gt; Result\u0026lt;JSONValue,Error\u0026lt;Rule\u0026gt;\u0026gt;{// ... Ok(parse_value(json))}精加工 我们的主要功能现在非常简单。首先，我们从一个名为 data.json 的文件中读取 JSON 数据。接下来，我们将文件内容解析成一个 JSON AST。最后，我们将 AST 序列化回一个字符串并打印出来。\nusestd::fs;fn main(){letunparsed_file=fs::read_to_string(\u0026#34;data.json\u0026#34;).expect(\u0026#34;cannot read file\u0026#34;);letjson: JSONValue=parse_json_file(\u0026amp;unparsed_file).expect(\u0026#34;unsuccessful parse\u0026#34;);println!(\u0026#34;{}\u0026#34;,serialize_jsonvalue(\u0026amp;json));}试试吧! 将本章顶部的示例文档复制到 data.json 中，然后运行程序! 你应该看到这样的东西。\n$ cargo run [ ... ] {\u0026quot;nesting\u0026quot;:{\u0026quot;inner object\u0026quot;:{}},\u0026quot;an array\u0026quot;:[1.5,true,null,0.000001],\u0026quot;string with escaped double quotes\u0026quot;:\u0026quot;\\\u0026quot;quick brown foxes\\\u0026quot;\u0026quot;} 例子: J 语言 J 语言是一种受 APL 影响的数组编程语言。在 J 语言中，对单个数字(2*3)的操作可以很容易地应用于整个数字列表(2*3 4 5，返回 6 8 10)。\nJ 中的操作符被称为动词。动词要么是一元的（取一个参数，如 *: 3，\u0026ldquo;3 的平方\u0026rdquo;），要么是二元的（取两个参数，两边各一个，如 5 - 4，\u0026ldquo;5减4\u0026rdquo;）。\n下面是一个 J 程序的例子。\n\u0026#39;A string\u0026#39; *: 1 2 3 4 matrix =: 2 3 $ 5 + 2 3 4 5 6 7 10 * matrix 1 + 10 20 30 1 2 3 + 10 residues =: 2 | 0 1 2 3 4 5 6 7 residues 使用 J 的解释器运行上述程序，在标准输出上得到如下结果。\nA string 1 4 9 16 70 80 90 100 110 120 11 21 31 11 12 13 0 1 0 1 0 1 0 1 在这一节中，我们将为 J 的一个子集写一个 grammar，然后我们将通过一个解析器，通过迭代 pest 给我们的规则来建立一个 AST。你可以在本书的资源库中找到完整的源代码。\nGrammar 我们将从程序规则开始，逐节建立 grammar。\nprogram = _{ SOI ~ \u0026quot;\\n\u0026quot;* ~ (stmt ~ \u0026quot;\\n\u0026quot;+) * ~ stmt? ~ EOI } 每个 J 程序都包含由一个或多个换行符分隔的语句。请注意前面的下划线，它告诉 pest 屏蔽 program 规则 - 我们不想让 program 作为一个 token 出现在解析流中，我们想要的是底层语句。\n语句就是一个简单的表达式，由于只有一种这样的可能性，所以我们也将这个 stmt 规则屏蔽，这样我们的解析器就会收到一个底层 expr 的迭代器。\nstmt = _{ expr } 表达式可以是对变量标识符的赋值，也可以是单项表达式、对偶表达式、单个字符串或术语数组。\nexpr = { assgmtExpr | monadicExpr | dyadicExpr | string | terms } 一元表达式由一个动词组成，其唯一的操作数在右边；三元表达式的操作数在动词的两边。赋值表达式将标识符与表达式相关联。\n在 J 中，没有操作符的优先性 - 求值是右联的（从右到左），括号内的表达式先被求值。\nmonadicExpr = { verb ~ expr } dyadicExpr = { (monadicExpr | terms) ~ verb ~ expr } assgmtExpr = { ident ~ \u0026quot;=:\u0026quot; ~ expr } 项的列表应该至少包含一个十进制、整数、标识符或小括号表达式；我们只关心这些基础值，所以我们用前导下划线屏蔽 term 规则。\nterms = { term+ } term = _{ decimal | integer | ident | \u0026quot;(\u0026quot; ~ expr ~ \u0026quot;)\u0026quot; } J 的几个动词在这个 grammar 中是有定义的，J 的全部词汇要广泛得多。\nverb = { \u0026quot;\u0026gt;:\u0026quot; | \u0026quot;*:\u0026quot; | \u0026quot;-\u0026quot; | \u0026quot;%\u0026quot; | \u0026quot;#\u0026quot; | \u0026quot;\u0026gt;.\u0026quot; | \u0026quot;+\u0026quot; | \u0026quot;*\u0026quot; | \u0026quot;\u0026lt;\u0026quot; | \u0026quot;=\u0026quot; | \u0026quot;^\u0026quot; | \u0026quot;|\u0026quot; | \u0026quot;\u0026gt;\u0026quot; | \u0026quot;$\u0026quot; } 现在我们可以进入词法规则了。J 中的数字和平常一样，除了负数用前导的 _ 下划线表示外（因为 - 是一个动词，它作为单项式执行否定，作为对偶式执行减法）。J 中的标识符必须以字母开头，但之后可以包含数字。字符串由单引号包围；引号本身可以通过用附加引号转义来嵌入。\n请注意我们如何使用 pest 的 @ 修饰符使这些规则中的每一条都是原子的，这意味着隐式空白是被禁止的，而且内部规则（即 ident 中的 ASCII_ALPHA）变为 silent - 当我们的解析器接收到这些 token 时，它们将是终端的。\ninteger = @{ \u0026quot;_\u0026quot;? ~ ASCII_DIGIT+ } decimal = @{ \u0026quot;_\u0026quot;? ~ ASCII_DIGIT+ ~ \u0026quot;.\u0026quot; ~ ASCII_DIGIT* } ident = @{ ASCII_ALPHA ~ (ASCII_ALPHANUMERIC | \u0026quot;_\u0026quot;)* } string = @{ \u0026quot;'\u0026quot; ~ ( \u0026quot;''\u0026quot; | (!\u0026quot;'\u0026quot; ~ ANY) )* ~ \u0026quot;'\u0026quot; } J 中的空白只由空格和制表符组成。换行的意义在于它们是对语句的定界，因此它们不在本规则之内。\nWHITESPACE = _{ \u0026quot; \u0026quot; | \u0026quot;\\t\u0026quot; } 最后，我们必须处理注释。J 中的注释以 NB. 开始，一直到它们所在行的末尾。关键的是，我们决不能消耗注释行末的换行；这是为了将注释之前的任何语句与后续行的语句分开。\nCOMMENT = _{ \u0026quot;NB.\u0026quot; ~ (!\u0026quot;\\n\u0026quot; ~ ANY)* } 解析和 AST 生成 本节将介绍一个使用上述 grammar 的解析器。这里省略了库中的内容和自明的代码，你可以在本书的资源库中找到解析器的全部内容。\n首先我们将枚举我们 grammar 中定义的动词，区分一元动词和二元动词。这些枚举将在我们的 AST 中作为标签使用。\npubenum MonadicVerb{Increment,Square,Negate,Reciprocal,Tally,Ceiling,ShapeOf,}pubenum DyadicVerb{Plus,Times,LessThan,LargerThan,Equal,Minus,Divide,Power,Residue,Copy,LargerOf,LargerOrEqual,Shape,}那么我们就来列举一下 AST 的各类节点。\npubenum AstNode{Print(Box\u0026lt;AstNode\u0026gt;),Integer(i32),DoublePrecisionFloat(f64),MonadicOp{verb: MonadicVerb,expr: Box\u0026lt;AstNode\u0026gt;,},DyadicOp{verb: DyadicVerb,lhs: Box\u0026lt;AstNode\u0026gt;,rhs: Box\u0026lt;AstNode\u0026gt;,},Terms(Vec\u0026lt;AstNode\u0026gt;),IsGlobal{ident: String,expr: Box\u0026lt;AstNode\u0026gt;,},Ident(String),Str(CString),}为了解析 J 程序中的顶层语句，我们有下面的 parse 函数，它接受一个字符串形式的 J 程序，并将其传递给 pest 进行解析。我们得到一个 Pair 的序列。正如 grammar 中所规定的那样，一个语句只能由一个表达式组成，所以下面的匹配会解析这些顶层表达式中的每一个，并将它们包装在一个 Print AST 节点中，以符合 J 解释器的 REPL 行为。\npubfn parse(source: \u0026amp;str)-\u0026gt; Result\u0026lt;Vec\u0026lt;AstNode\u0026gt;,Error\u0026lt;Rule\u0026gt;\u0026gt;{letmutast=vec![];letpairs=JParser::parse(Rule::program,source)?;forpairinpairs{matchpair.as_rule(){Rule::expr=\u0026gt;{ast.push(Print(Box::new(build_ast_from_expr(pair))));}_=\u0026gt;{}}}Ok(ast)}AST 节点是通过遍历 Pair 迭代器，按照我们 grammar 文件中设定的期望值，从表达式中构建出来的。常见的行为被抽象出单独的函数，如 parse_monadic_verb 和 parse_dyadic_verb，代表表达式本身的 Pair 则在递归调用 build_ast_from_expr 中传递。\nfn build_ast_from_expr(pair: pest::iterators::Pair\u0026lt;Rule\u0026gt;)-\u0026gt; AstNode{matchpair.as_rule(){Rule::expr=\u0026gt;build_ast_from_expr(pair.into_inner().next().unwrap()),Rule::monadicExpr=\u0026gt;{letmutpair=pair.into_inner();letverb=pair.next().unwrap();letexpr=pair.next().unwrap();letexpr=build_ast_from_expr(expr);parse_monadic_verb(verb,expr)}// ... other cases elided here ... }}二元动词从它们的字符串表示方式直接映射到 AST 节点。\nfn parse_dyadic_verb(pair: pest::iterators::Pair\u0026lt;Rule\u0026gt;,lhs: AstNode,rhs: AstNode)-\u0026gt; AstNode{AstNode::DyadicOp{lhs: Box::new(lhs),rhs: Box::new(rhs),verb: matchpair.as_str(){\u0026#34;+\u0026#34;=\u0026gt;DyadicVerb::Plus,\u0026#34;*\u0026#34;=\u0026gt;DyadicVerb::Times,\u0026#34;-\u0026#34;=\u0026gt;DyadicVerb::Minus,\u0026#34;\u0026lt;\u0026#34;=\u0026gt;DyadicVerb::LessThan,\u0026#34;=\u0026#34;=\u0026gt;DyadicVerb::Equal,\u0026#34;\u0026gt;\u0026#34;=\u0026gt;DyadicVerb::LargerThan,\u0026#34;%\u0026#34;=\u0026gt;DyadicVerb::Divide,\u0026#34;^\u0026#34;=\u0026gt;DyadicVerb::Power,\u0026#34;|\u0026#34;=\u0026gt;DyadicVerb::Residue,\u0026#34;#\u0026#34;=\u0026gt;DyadicVerb::Copy,\u0026#34;\u0026gt;.\u0026#34;=\u0026gt;DyadicVerb::LargerOf,\u0026#34;\u0026gt;:\u0026#34;=\u0026gt;DyadicVerb::LargerOrEqual,\u0026#34;$\u0026#34;=\u0026gt;DyadicVerb::Shape,_=\u0026gt;panic!(\u0026#34;Unexpected dyadic verb: {}\u0026#34;,pair.as_str()),},}}如同一元动词一样。\nfn parse_monadic_verb(pair: pest::iterators::Pair\u0026lt;Rule\u0026gt;,expr: AstNode)-\u0026gt; AstNode{AstNode::MonadicOp{verb: matchpair.as_str(){\u0026#34;\u0026gt;:\u0026#34;=\u0026gt;MonadicVerb::Increment,\u0026#34;*:\u0026#34;=\u0026gt;MonadicVerb::Square,\u0026#34;-\u0026#34;=\u0026gt;MonadicVerb::Negate,\u0026#34;%\u0026#34;=\u0026gt;MonadicVerb::Reciprocal,\u0026#34;#\u0026#34;=\u0026gt;MonadicVerb::Tally,\u0026#34;\u0026gt;.\u0026#34;=\u0026gt;MonadicVerb::Ceiling,\u0026#34;$\u0026#34;=\u0026gt;MonadicVerb::ShapeOf,_=\u0026gt;panic!(\u0026#34;Unsupported monadic verb: {}\u0026#34;,pair.as_str()),},expr: Box::new(expr),}}最后，我们定义了一个函数来处理数字和字符串等项。数字需要一些操作来处理 J 的前导下划线，表示否定，但除此之外，处理过程是典型的。\nfn build_ast_from_term(pair: pest::iterators::Pair\u0026lt;Rule\u0026gt;)-\u0026gt; AstNode{matchpair.as_rule(){Rule::integer=\u0026gt;{letistr=pair.as_str();let(sign,istr)=match\u0026amp;istr[..1]{\u0026#34;_\u0026#34;=\u0026gt;(-1,\u0026amp;istr[1..]),_=\u0026gt;(1,\u0026amp;istr[..]),};letinteger: i32 =istr.parse().unwrap();AstNode::Integer(sign*integer)}Rule::decimal=\u0026gt;{letdstr=pair.as_str();let(sign,dstr)=match\u0026amp;dstr[..1]{\u0026#34;_\u0026#34;=\u0026gt;(-1.0,\u0026amp;dstr[1..]),_=\u0026gt;(1.0,\u0026amp;dstr[..]),};letmutflt: f64 =dstr.parse().unwrap();ifflt!=0.0{// Avoid negative zeroes; only multiply sign by nonzeroes. flt*=sign;}AstNode::DoublePrecisionFloat(flt)}Rule::expr=\u0026gt;build_ast_from_expr(pair),Rule::ident=\u0026gt;AstNode::Ident(String::from(pair.as_str())),unknown_term=\u0026gt;panic!(\u0026#34;Unexpected term: {:?}\u0026#34;,unknown_term),}}运行解析器 现在我们可以定义一个 main 函数，将 J 程序传递给我们的 pest-enabled 解析器。\nfn main(){letunparsed_file=std::fs::read_to_string(\u0026#34;example.ijs\u0026#34;).expect(\u0026#34;cannot read ijs file\u0026#34;);letastnode=parse(\u0026amp;unparsed_file).expect(\u0026#34;unsuccessful parse\u0026#34;);println!(\u0026#34;{:?}\u0026#34;,\u0026amp;astnode);}在 example.ijs 中使用这段代码。\n_2.5 ^ 3 *: 4.8 title =: 'Spinning at the Boundary' *: _1 2 _3 4 1 2 3 + 10 20 30 1 + 10 20 30 1 2 3 + 10 2 | 0 1 2 3 4 5 6 7 another =: 'It''s Escaped' 3 | 0 1 2 3 4 5 6 7 (2+1)*(2+2) 3 * 2 + 1 1 + 3 % 4 x =: 100 x - 1 y =: x - 1 y 当我们运行解析器时，我们会在标准输出上得到以下抽象语法树。\n$ cargo run [ ... ] [Print(DyadicOp { verb: Power, lhs: DoublePrecisionFloat(-2.5), rhs: Integer(3) }), Print(MonadicOp { verb: Square, expr: DoublePrecisionFloat(4.8) }), Print(IsGlobal { ident: \u0026quot;title\u0026quot;, expr: Str(\u0026quot;Spinning at the Boundary\u0026quot;) }), Print(MonadicOp { verb: Square, expr: Terms([Integer(-1), Integer(2), Integer(-3), Integer(4)]) }), Print(DyadicOp { verb: Plus, lhs: Terms([Integer(1), Integer(2), Integer(3)]), rhs: Terms([Integer(10), Integer(20), Integer(30)]) }), Print(DyadicOp { verb: Plus, lhs: Integer(1), rhs: Terms([Integer(10), Integer(20), Integer(30)]) }), Print(DyadicOp { verb: Plus, lhs: Terms([Integer(1), Integer(2), Integer(3)]), rhs: Integer(10) }), Print(DyadicOp { verb: Residue, lhs: Integer(2), rhs: Terms([Integer(0), Integer(1), Integer(2), Integer(3), Integer(4), Integer(5), Integer(6), Integer(7)]) }), Print(IsGlobal { ident: \u0026quot;another\u0026quot;, expr: Str(\u0026quot;It\\'s Escaped\u0026quot;) }), Print(DyadicOp { verb: Residue, lhs: Integer(3), rhs: Terms([Integer(0), Integer(1), Integer(2), Integer(3), Integer(4), Integer(5), Integer(6), Integer(7)]) }), Print(DyadicOp { verb: Times, lhs: DyadicOp { verb: Plus, lhs: Integer(2), rhs: Integer(1) }, rhs: DyadicOp { verb: Plus, lhs: Integer(2), rhs: Integer(2) } }), Print(DyadicOp { verb: Times, lhs: Integer(3), rhs: DyadicOp { verb: Plus, lhs: Integer(2), rhs: Integer(1) } }), Print(DyadicOp { verb: Plus, lhs: Integer(1), rhs: DyadicOp { verb: Divide, lhs: Integer(3), rhs: Integer(4) } }), Print(IsGlobal { ident: \u0026quot;x\u0026quot;, expr: Integer(100) }), Print(DyadicOp { verb: Minus, lhs: Ident(\u0026quot;x\u0026quot;), rhs: Integer(1) }), Print(IsGlobal { ident: \u0026quot;y\u0026quot;, expr: DyadicOp { verb: Minus, lhs: Ident(\u0026quot;x\u0026quot;), rhs: Integer(1) } }), Print(Ident(\u0026quot;y\u0026quot;))] ","permalink":"https://ohmyweekly.github.io/notes/2021-01-20-pest-grammars/","tags":["Rust","Rust Grammar"],"title":"Pest Grammars"},{"categories":["Rust"],"contents":"例子: INI INI(initialization 的简称)文件是简单的配置文件。由于没有标准的格式，我们将编写一个能够解析这个例子文件的程序。\nusername = noha password = plain_text salt = NaCl [server_1] interface=eth0 ip=127.0.0.1 document_root=/var/www/example.org [empty_section] [second_server] document_root=/var/www/example.com ip= interface=eth1 每一行都包含一个键和值，中间用等号隔开；或者包含一个用方括号括起来的章节名；或者是空白，没有任何意义。\n每当出现一个节名，下面的键和值就属于该节，直到下一个节名。文件开头的键值对属于一个隐式的 \u0026ldquo;空\u0026quot;节。\n编写 grammar 首先使用 Cargo 初始化一个新项目，添加依赖关系 pest = \u0026quot;2.0\u0026quot; 和 pest_derive = \u0026quot;2.0\u0026quot;。创建一个新文件 src/ini.pest 来保存 grammar。\n我们文件中感兴趣的文本 - username、/var/www/example.org 等 - 只由几个字符组成。让我们制定一个规则来识别该集合中的单个字符。内置的规则 ASCII_ALPHANUMERIC 是表示任何大写或小写 ASCII 字母或任何数字的快捷方式。\nchar = { ASCII_ALPHANUMERIC | \u0026quot;.\u0026quot; | \u0026quot;_\u0026quot; | \u0026quot;/\u0026quot; } 节名和属性键不能为空，但属性值可以为空（如上文中的 ip= 行）。也就是说，前者由一个或多个字符组成，char+; 后者由零或多个字符组成，char*。我们将其含义分为两条规则。\nname = { char+ } value = { char* } 现在很容易表达这两种输入行。\nsection = { \u0026quot;[\u0026quot; ~ name ~ \u0026quot;]\u0026quot; } property = { name ~ \u0026quot;=\u0026quot; ~ value } 最后，我们需要一个规则来表示整个输入文件。表达式 (section | property)? 匹配 section、property，否则什么也不匹配。使用内置规则 NEWLINE 来匹配行尾。\nfile = { SOI ~ ((section | property)? ~ NEWLINE)* ~ EOI } 要将解析器编译成 Rust，我们需要在 src/main.rs 中添加以下内容。\nexterncratepest;#[macro_use]externcratepest_derive;usepest::Parser;#[derive(Parser)]#[grammar = \u0026#34;ini.pest\u0026#34;]pubstruct INIParser;程序初始化 现在我们可以读取文件，并用 pest 进行解析。\nusestd::collections::HashMap;usestd::fs;fn main(){letunparsed_file=fs::read_to_string(\u0026#34;config.ini\u0026#34;).expect(\u0026#34;cannot read file\u0026#34;);letfile=INIParser::parse(Rule::file,\u0026amp;unparsed_file).expect(\u0026#34;unsuccessful parse\u0026#34;)// unwrap the parse result .next().unwrap();// get and unwrap the `file` rule; never fails // ... }我们将使用嵌套的 HashMap 来表达属性列表。外层哈希 map 将以章节名称作为键，以章节内容（内部哈希 map）作为值。每个内部哈希 map 将有属性键和属性值。例如，要访问 server_1 的 document_root，我们可以写 properties[\u0026quot;server_1\u0026quot;][\u0026quot;document_root\u0026quot;]。隐含的 \u0026ldquo;空\u0026quot;节将由常规部分表示，名称为空字符串 \u0026quot;\u0026quot;，这样 properties[\u0026quot;\u0026quot;][\u0026quot;salt\u0026quot;] 就是有效的。\nfn main(){// ... letmutproperties: HashMap\u0026lt;\u0026amp;str,HashMap\u0026lt;\u0026amp;str,\u0026amp;str\u0026gt;\u0026gt;=HashMap::new();// ... }请注意，哈希 map 的键和值都是 \u0026amp;str，即借用的字符串。pest 解析器不会复制他们解析的输入，而是借用。所有用于检查解析结果的方法都会返回从原始解析字符串中借用字符串。\n主循环 现在我们解释解析结果。我们循环浏览文件的每一行，这一行要么是节名，要么是键值属性对。如果遇到一个节名，我们更新一个变量。如果遇到一个属性对，我们就获取一个对当前章节的哈希 map 的引用，然后把这个属性对插入到这个哈希 map 中。\n// ... letmutcurrent_section_name=\u0026#34;\u0026#34;;forlineinfile.into_inner(){matchline.as_rule(){Rule::section=\u0026gt;{letmutinner_rules=line.into_inner();// { name } current_section_name=inner_rules.next().unwrap().as_str();}Rule::property=\u0026gt;{letmutinner_rules=line.into_inner();// { name ~ \u0026#34;=\u0026#34; ~ value } letname: \u0026amp;str =inner_rules.next().unwrap().as_str();letvalue: \u0026amp;str =inner_rules.next().unwrap().as_str();// Insert an empty inner hash map if the outer hash map hasn\u0026#39;t // seen this section name before. letsection=properties.entry(current_section_name).or_default();section.insert(name,value);}Rule::EOI=\u0026gt;(),_=\u0026gt;unreachable!(),}}// ... 在输出方面，我们用漂亮的打印 Debug 格式简单地转储哈希 map。\nfn main(){// ... println!(\u0026#34;{:#?}\u0026#34;,properties);}空白 如果你把本章顶部的例子 INI 文件复制到 config.ini 文件中并运行程序，它将无法解析。我们已经忘记了等号周围的可选空格!\n对于大型 grammar 来说，处理空白会很不方便。显示地编写 whitespace 规则并手动插入空白会让 grammar 变得难以阅读和修改。pest 提供了一个特殊规则 WHITESPACE 的解决方案。如果定义了 WHITESPACE，它将被隐式地运行，尽可能多次地在每个波浪号 ~ 和每个重复之间运行（例如，* 和 +）。对于我们的 INI 解析器，只有空格才是合法的 whitespace。\nWHITESPACE = _{ \u0026quot; \u0026quot; } 我们用一个前导的下划线 _{ ... } 来标记 WHITESPACE 规则的静默。}. 这样，即使它匹配，也不会出现在其他规则中。如果它不是静默的，解析就会复杂得多，因为对 Pairs::next(...) 的每次调用都有可能返回 Rule::WHITESPACE 而不是想要的下一条规则。\n但是等等! 节名、键或值中不应该有空格！目前，空格是自动插入的。目前，在 name = { char+ } 中，空格会自动插入字符之间。对空格敏感的规则需要用前导符号 @{ ... } 来标记原子。}. 在原子规则中，自动的空白处理是被禁用的，而内部规则是静默的。\nname = @{ char+ } value = @{ char* } 完工 试试吧！确保文件 config.ini 存在，然后运行程序! 你应该看到这样的东西。\n$ cargo run [ ... ] { \u0026quot;\u0026quot;: { \u0026quot;password\u0026quot;: \u0026quot;plain_text\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;noha\u0026quot;, \u0026quot;salt\u0026quot;: \u0026quot;NaCl\u0026quot; }, \u0026quot;second_server\u0026quot;: { \u0026quot;ip\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;document_root\u0026quot;: \u0026quot;/var/www/example.com\u0026quot;, \u0026quot;interface\u0026quot;: \u0026quot;eth1\u0026quot; }, \u0026quot;server_1\u0026quot;: { \u0026quot;interface\u0026quot;: \u0026quot;eth0\u0026quot;, \u0026quot;document_root\u0026quot;: \u0026quot;/var/www/example.org\u0026quot;, \u0026quot;ip\u0026quot;: \u0026quot;127.0.0.1\u0026quot; } } ","permalink":"https://ohmyweekly.github.io/notes/2021-01-19-parser-api-example-ini/","tags":["Rust","Grammar"],"title":"Parser API - 解析 INI"},{"categories":["Flink"],"contents":"元编程 在Julia语言中，Lisp最强的遗产是它对元编程的支持。和Lisp一样，Julia也将自己的代码表示为语言本身的数据结构。由于代码是由可以在语言内部创建和操作的对象来表示的，所以程序可以转换和生成自己的代码。这使得复杂的代码生成不需要额外的构建步骤，也允许真正的Lisp式的宏在抽象语法树的层次上操作。相比之下，预处理器的 \u0026ldquo;宏 \u0026ldquo;系统，就像C和C++一样，在任何实际的解析或解释发生之前，都会进行文本操作和替换。由于Julia中所有的数据类型和代码都是由Julia数据结构来表示的，因此，强大的反射功能可以像其他数据一样探索程序及其类型的内部。\n程序表示 每个Julia程序都是以字符串的形式开始的。\njulia\u0026gt; prog = \u0026#34;1 + 1\u0026#34; \u0026#34;1 + 1\u0026#34; 接下来会发生什么？\n下一步是将每个字符串解析成一个对象，称为表达式，用Julia类型 Expr 表示。\njulia\u0026gt; ex1 = Meta.parse(prog) :(1 + 1) julia\u0026gt; typeof(ex1) Expr Expr对象包含两部分。\na 符号标识表达式的种类。符号是一个内嵌的字符串标识符（下文将详细讨论）。\njulia\u0026gt; ex1.head :call 表达式参数，可以是符号、其他表达式或文字值。\njulia\u0026gt; ex1.args 3-element Array{Any,1}: :+ 1 1 也可以直接用前缀符号构造表达式。\njulia\u0026gt; ex2 = Expr(:call, :+, 1, 1) :(1 + 1) 上面构造的两个表达式\u0026ndash;通过解析和直接构造\u0026ndash;是等价的。\njulia\u0026gt; ex1 == ex2 true 这里的关键点是，Julia代码在内部被表示为一个数据结构，可以从语言本身访问。\ndump函数提供了Expr对象的缩进和注释显示。\njulia\u0026gt; dump(ex2) Expr head: Symbol call args: Array{Any}((3,)) 1: Symbol + 2: Int64 1 3: Int64 1 Expr对象也可以被嵌套。\njulia\u0026gt; ex3 = Meta.parse(\u0026#34;(4 + 4) / 2\u0026#34;) :((4 + 4) / 2) 另一种查看表达式的方法是使用Meta.show_sexpr，它可以显示给定Expr的S表达式形式，这对于Lisp的用户来说可能看起来非常熟悉。下面是一个例子，说明如何在嵌套的Expr上显示。\njulia\u0026gt; Meta.show_sexpr(ex3) (:call, :/, (:call, :+, 4, 4), 2) 符号\n在Julia中，:字符有两种语法用途。第一种形式是创建一个Symbol，一个内部字符串，作为表达式的一个构件。\njulia\u0026gt; :foo :foo julia\u0026gt; typeof(ans) Symbol 符号构造函数接受任何数量的参数，并通过将它们的字符串表示连接在一起来创建一个新的符号。\njulia\u0026gt; :foo == Symbol(\u0026#34;foo\u0026#34;) true julia\u0026gt; Symbol(\u0026#34;func\u0026#34;,10) :func10 julia\u0026gt; Symbol(:var,\u0026#39;_\u0026#39;,\u0026#34;sym\u0026#34;) :var_sym 请注意，要使用 : 语法，符号的名称必须是一个有效的标识符。否则必须使用Symbol(str)构造函数。\n在表达式的上下文中，符号用于指示对变量的访问；当表达式被评估时，符号会被替换为在适当的作用域中与该符号绑定的值。\n有时，为了避免在解析时产生歧义，需要在 : 的参数周围加括号。\njulia\u0026gt; :(:) :(:) julia\u0026gt; :(::) :(::) 表达式和评价\n 引用  字符的第二个语法目的是在不使用显式Expr构造函数的情况下创建表达式对象。这就是所谓的引用。在Julia代码的单条语句周围，用成对的括号跟上 : 字符，就可以根据所附的代码生成一个Expr对象。下面是用于引用一个算术表达式的简短形式的例子。\n  julia\u0026gt; ex = :(a+b*c+1) :(a + b * c + 1) julia\u0026gt; typeof(ex) Expr (要查看这个表达式的结构，可以试试ex.head和ex.args，或者使用上面的dump或Meta.@dump)\n注意，可以使用Meta.parse或直接使用Expr形式构造等价的表达式。\njulia\u0026gt; :(a + b*c + 1) == Meta.parse(\u0026#34;a + b*c + 1\u0026#34;) == Expr(:call, :+, :a, Expr(:call, :*, :b, :c), 1) true 解析器提供的表达式一般只有符号、其他表达式和字面值作为其args，而Julia代码构建的表达式可以有任意的运行时值，没有字面形式作为args。在这个具体的例子中，+和a是符号，*(b,c)是一个子表达式，1是一个64位有符号整数的文字形式。\n对于多个表达式，还有第二种引用的语法形式：用引号\u0026hellip;\u0026hellip;结尾括起来的代码块。\njulia\u0026gt; ex = quote x = 1 y = 2 x + y end quote #= none:2 =# x = 1 #= none:3 =# y = 2 #= none:4 =# x + y end julia\u0026gt; typeof(ex) Expr 插值\n用值参数直接构造Expr对象是很强大的，但与 \u0026ldquo;正常的 \u0026ldquo;Julia语法相比，Expr构造函数可能很乏味。作为一种替代方法，Julia允许将字元或表达式插值到引用的表达式中。插值由前缀$表示。\n在这个例子中，变量a的值被内插了。\njulia\u0026gt; a = 1; julia\u0026gt; ex = :($a + b) :(1 + b) 不支持向未引用的表达式插值，并会导致编译时错误。\njulia\u0026gt; $a + b ERROR: syntax: \u0026#34;$\u0026#34; expression outside quote 在这个例子中，元组(1,2,3)作为表达式被内插到一个条件测试中。\njulia\u0026gt; ex = :(a in $:((1,2,3)) ) :(a in (1, 2, 3)) 在表达式插值中使用$是有意让人联想到字符串插值和命令插值。表达式插值可以方便的、可读的程序化构造复杂的Julia表达式。\n劈叉插值\n请注意，$插值语法只允许在一个包围表达式中插入一个表达式。偶尔，你有一个表达式数组，需要它们全部成为包围表达式的参数。这可以用语法$(xs\u0026hellip;)来完成。例如，下面的代码生成了一个函数调用，其中的参数数是通过编程确定的。\njulia\u0026gt; args = [:x, :y, :z]; julia\u0026gt; :(f(1, $(args...))) :(f(1, x, y, z)) 嵌套引用\n自然，引号表达式有可能包含其他引号表达式。在这些情况下，理解内插是如何工作的可能有点棘手。考虑一下这个例子。\njulia\u0026gt; x = :(1 + 2); julia\u0026gt; e = quote quote $x end end quote #= none:1 =# $(Expr(:quote, quote #= none:1 =# $(Expr(:$, :x)) end)) end 注意，结果中包含$x，这意味着x还没有被评估。换句话说，$表达式 \u0026ldquo;属于 \u0026ldquo;内部引号表达式，因此它的参数只有在内部引号表达式时才会被评估。\njulia\u0026gt; eval(e) quote #= none:1 =# 1 + 2 end 但是，外引号表达式能够对内引号中的$内的值进行插值。这是用多个$来完成的。\njulia\u0026gt; e = quote quote $$x end end quote #= none:1 =# $(Expr(:quote, quote #= none:1 =# $(Expr(:$, :(1 + 2))) end)) end 注意到(1+2)现在出现在结果中，而不是符号x。对这个表达式进行评估，得到一个内插的3。\njulia\u0026gt; eval(e) quote #= none:1 =# 3 end 这种行为背后的直觉是，x对每个$都会被评估一次：一个$的工作原理类似于eval(:x)，给出x的值，而两个$的工作原理相当于eval(eval(:x))。\nQuoteNode\n引号形式在AST中的通常表示是一个带头:quote的Expr。\njulia\u0026gt; dump(Meta.parse(\u0026#34;:(1+2)\u0026#34;)) Expr head: Symbol quote args: Array{Any}((1,)) 1: Expr head: Symbol call args: Array{Any}((3,)) 1: Symbol + 2: Int64 1 3: Int64 2 正如我们所看到的，这类表达式支持用$进行插值，但是在某些情况下，有必要在不进行插值的情况下引用代码。这种引用还没有语法，但在内部表示为一个类型为QuoteNode的对象。\njulia\u0026gt; eval(Meta.quot(Expr(:$, :(1+2)))) 3 julia\u0026gt; eval(QuoteNode(Expr(:$, :(1+2)))) :($(Expr(:$, :(1 + 2)))) 该解析器产生的QuoteNodes用于简单的引用项目，如符号。\njulia\u0026gt; dump(Meta.parse(\u0026#34;:x\u0026#34;)) QuoteNode value: Symbol x QuoteNode还可以用于某些高级元编程任务。\n评估表达式\n给定一个表达式对象，可以使用eval使Julia在全局范围内对其进行评估（执行）。\njulia\u0026gt; :(1 + 2) :(1 + 2) julia\u0026gt; eval(ans) 3 julia\u0026gt; ex = :(a + b) :(a + b) julia\u0026gt; eval(ex) ERROR: UndefVarError: b not defined [...] julia\u0026gt; a = 1; b = 2; julia\u0026gt; eval(ex) 3 每个模块都有自己的 eval 函数，它可以在全局范围内评估表达式。传递给eval的表达式并不局限于返回值\u0026ndash;它们也可以产生副作用，改变模块环境的状态。\njulia\u0026gt; ex = :(x = 1) :(x = 1) julia\u0026gt; x ERROR: UndefVarError: x not defined julia\u0026gt; eval(ex) 1 julia\u0026gt; x 1 在这里，对表达式对象的评价会导致一个值被分配给全局变量x。\n由于表达式只是Expr对象，可以通过编程构造，然后进行评估，因此可以动态生成任意代码，然后使用eval进行运行。下面是一个简单的例子。\njulia\u0026gt; a = 1; julia\u0026gt; ex = Expr(:call, :+, a, :b) :(1 + b) julia\u0026gt; a = 0; b = 2; julia\u0026gt; eval(ex) 3 a的值用于构造表达式ex，该表达式将+函数应用于值1和变量b，注意a和b的使用方式的重要区别。\n变量a的值在表达式构造时被用作表达式中的即时值。因此，当表达式被评估时，a的值不再重要：表达式中的值已经是1，无论a的值是多少，都是独立的。 另一方面，在表达式的构造中使用了符号:b，所以变量b的值在那个时候是无关紧要的\u0026ndash;:b只是一个符号，变量b甚至不需要被定义。但在表达式评估时，符号:b的值是通过查找变量b的值来解决的。 表达式上的函数\n如上所述，Julia的一个极其有用的特性是在Julia本身内部生成和操作Julia代码的能力。我们已经看到了一个函数返回Expr对象的例子：parse函数，它接收一串Julia代码并返回相应的Expr。一个函数也可以接受一个或多个Expr对象作为参数，并返回另一个Expr。下面是一个简单的、激励性的例子。\njulia\u0026gt; function math_expr(op, op1, op2) expr = Expr(:call, op, op1, op2) return expr end math_expr (generic function with 1 method) julia\u0026gt; ex = math_expr(:+, 1, Expr(:call, :*, 4, 5)) :(1 + 4 * 5) julia\u0026gt; eval(ex) 21 作为另一个例子，这里有一个函数，它可以将任何数字参数翻倍，但不考虑表达式。\njulia\u0026gt; function make_expr2(op, opr1, opr2) opr1f, opr2f = map(x -\u0026gt; isa(x, Number) ? 2*x : x, (opr1, opr2)) retexpr = Expr(:call, op, opr1f, opr2f) return retexpr end make_expr2 (generic function with 1 method) julia\u0026gt; make_expr2(:+, 1, 2) :(2 + 4) julia\u0026gt; ex = make_expr2(:+, 1, Expr(:call, :*, 5, 8)) :(2 + 5 * 8) julia\u0026gt; eval(ex) 42 宏 宏提供了一种将生成的代码包含在程序的最后主体中的方法。宏将一个参数元组映射到一个返回的表达式，生成的表达式直接被编译，而不需要运行时 eval 调用。宏参数可以包括表达式、字面值和符号。\n基础知识 这里有一个特别简单的宏。\njulia\u0026gt; macro sayhello() return :( println(\u0026#34;Hello, world!\u0026#34;) ) end @sayhello (macro with 1 method) 在 Julia 的语法中，宏有一个专门的字符：@(at 符号)，后面是 macro NAME ... end 块中声明的唯一名称\u0026hellip;.。在这个例子中，编译器将用 @sayhello 替换所有的实例。\n:( println(\u0026#34;Hello, world!\u0026#34;) ) 当在 REPL 中输入 @sayhello 时，表达式会立即执行，因此我们只看到求值结果。\njulia\u0026gt; @sayhello() Hello, world! 现在，考虑一个稍微复杂的宏。\njulia\u0026gt; macro sayhello(name) return :( println(\u0026#34;Hello, \u0026#34;, $name) ) end @sayhello (macro with 1 method) 这个宏只取一个参数：name。当遇到 @sayhello 时，引用的表达式会被展开，将参数的值内插到最终的表达式中。\njulia\u0026gt; @sayhello(\u0026#34;human\u0026#34;) Hello, human 我们可以使用函数 macroexpand 查看引用的返回表达式（重要提示：这是调试宏的一个极其有用的工具）。\njulia\u0026gt; ex = macroexpand(Main, :(@sayhello(\u0026#34;human\u0026#34;)) ) :(Main.println(\u0026#34;Hello, \u0026#34;, \u0026#34;human\u0026#34;)) julia\u0026gt; typeof(ex) Expr 我们可以看到，\u0026ldquo;human\u0026rdquo; 的字面值已经被插进了表达式中。\n此外，还存在一个宏 @macroexpand，也许比 macroexpand 函数更方便一些。\njulia\u0026gt; @macroexpand @sayhello \u0026#34;human\u0026#34; :(println(\u0026#34;Hello, \u0026#34;, \u0026#34;human\u0026#34;)) 等等：为什么是宏？ 我们在前一节已经看到了一个函数 f(::Expr...) -\u0026gt; Expr。其实，macroexpand 也是这样一个函数。那么，为什么要有宏的存在呢？\n宏是必要的，因为它们在代码解析时执行，因此，宏允许程序员在完整程序运行之前生成并包含自定义代码的片段。为了说明两者的区别，请考虑下面的例子。\njulia\u0026gt; macro twostep(arg) println(\u0026#34;I execute at parse time. The argument is: \u0026#34;, arg) return :(println(\u0026#34;I execute at runtime. The argument is: \u0026#34;, $arg)) end @twostep (macro with 1 method) julia\u0026gt; ex = macroexpand(Main, :(@twostep :(1, 2, 3)) ); I execute at parse time. The argument is: :((1, 2, 3)) 当调用 macroexpand 时，会执行对 println 的第一次调用。结果的表达式只包含第二个 println。\njulia\u0026gt; typeof(ex) Expr julia\u0026gt; ex :(println(\u0026#34;I execute at runtime. The argument is: \u0026#34;, $(Expr(:copyast, :($(QuoteNode(:((1, 2, 3))))))))) julia\u0026gt; eval(ex) I execute at runtime. The argument is: (1, 2, 3) 宏调用 宏的调用有以下一般语法。\n@name expr1 expr2 ... @name(expr1, expr2, ...) 注意在宏名前有区别的 @，第一种形式的参数表达式之间没有逗号，第二种形式的 @ 名后没有空格。两种样式不能混用。例如，下面的语法与上面的例子不同，它将元组 (expr1, expr2, \u0026hellip;) 作为一个参数传递给宏。\n@name (expr1, expr2, ...) 在数组字面量（或解析）上调用宏的另一种方法是将两者并列，而不使用括号。在这种情况下，数组将是唯一输入宏的表达式。下面的语法是等价的（与 @name [a b] * v 不同）。\n@name[a b] * v @name([a b]) * v 需要强调的是，宏以表达式、字面值或符号的形式接收其参数。探索宏参数的一种方法是在宏体中调用 show 函数。\njulia\u0026gt; macro showarg(x) show(x) # ... remainder of macro, returning an expression end @showarg (macro with 1 method) julia\u0026gt; @showarg(a) :a julia\u0026gt; @showarg(1+1) :(1 + 1) julia\u0026gt; @showarg(println(\u0026#34;Yo!\u0026#34;)) :(println(\u0026#34;Yo!\u0026#34;)) 除了给定的参数列表之外，每个宏都会被传递额外的参数 __source__ 和 __module__。\n参数 __source__ 提供了关于来自宏调用的 @ 符号的解析器位置的信息（以 LineNumberNode 对象的形式）。这使得宏能够包含更好的错误诊断信息，并且通常被日志、字符串解析器宏和文档等使用，例如，也被用来实现 @LINE、@FILE 和 @DIR 宏。\n位置信息可以通过引用 __source__.line 和 __source__.file 来访问。\njulia\u0026gt; macro __LOCATION__(); return QuoteNode(__source__); end @__LOCATION__ (macro with 1 method) julia\u0026gt; dump( @__LOCATION__( )) LineNumberNode line: Int64 2 file: Symbol none 参数 __module__ 提供了关于宏调用的扩展上下文的信息（以 Module 对象的形式）。这允许宏查找上下文信息，如现有的绑定，或者将该值作为额外的参数插入到在当前模块中做自省的运行时函数调用中。\n构建一个高级宏 这里是 Julia 的 @assert 宏的简化定义。\njulia\u0026gt; macro assert(ex) return :( $ex ? nothing : throw(AssertionError($(string(ex)))) ) end @assert (macro with 1 method) 这个宏可以这样使用。\njulia\u0026gt; @assert 1 == 1.0 julia\u0026gt; @assert 1 == 0 ERROR: AssertionError: 1 == 0 宏调用在解析时扩展到它的返回结果。这就相当于写。\n1 == 1.0 ? nothing : throw(AssertionError(\u0026#34;1 == 1.0\u0026#34;)) 1 == 0 ? nothing : throw(AssertionError(\u0026#34;1 == 0\u0026#34;)) 也就是说，在第一次调用中，表达式 :(1 == 1.0) 被拼接到测试条件槽中，而 string(:(1 == 1.0)) 的值被拼接到断言消息槽中。这样构造出来的整个表达式，就被放到了发生 @assert 宏调用的语法树中。然后在执行时，如果测试表达式评估为真，那么返回 nothing，而如果测试为假，则会引发一个错误，表明断言表达式是假的。注意，如果把这个写成函数就不行了，因为只有条件的值，不可能在错误信息中显示计算条件的表达式。\nJulia Base 中 @assert 的实际定义比较复杂。它允许用户有选择地指定自己的错误信息，而不是只打印失败的表达式。就像在参数数可变的函数（Varargs Functions）中一样，在最后一个参数后面用省略号来指定。\njulia\u0026gt; macro assert(ex, msgs...) msg_body = isempty(msgs) ? ex : msgs[1] msg = string(msg_body) return :($ex ? nothing : throw(AssertionError($msg))) end @assert (macro with 1 method) 现在 @assert 有两种操作模式，取决于它接收到的参数数量！如果只有一个参数，那么 msgs 捕获的表达式元组将是空的，它的行为和上面的简单定义一样。如果只有一个参数，msgs 捕获的表达式元组将是空的，它的行为与上面的简单定义相同。但现在如果用户指定了第二个参数，它将被打印在消息正文中，而不是失败的表达式。你可以用 @macroexpand 宏来检查宏扩展的结果。\njulia\u0026gt; @macroexpand @assert a == b :(if Main.a == Main.b Main.nothing else Main.throw(Main.AssertionError(\u0026#34;a == b\u0026#34;)) end) julia\u0026gt; @macroexpand @assert a==b \u0026#34;a should equal b!\u0026#34; :(if Main.a == Main.b Main.nothing else Main.throw(Main.AssertionError(\u0026#34;a should equal b!\u0026#34;)) end) 实际的 @assert 宏还可以处理另一种情况：如果除了打印 \u0026ldquo;a should equal b\u0026rdquo; 之外，我们还想打印它们的值呢？人们可能会天真地尝试在自定义消息中使用字符串插值，例如，@assert a==b \u0026quot;a ($a) should equal b ($b)!\u0026quot;，但这在上面的宏中不会像预期的那样工作。你能明白为什么吗？从字符串插值回想一下，插值后的字符串会被改写成对字符串的调用。比较一下。\njulia\u0026gt; typeof(:(\u0026#34;a should equal b\u0026#34;)) String julia\u0026gt; typeof(:(\u0026#34;a ($a) should equal b ($b)!\u0026#34;)) Expr julia\u0026gt; dump(:(\u0026#34;a ($a) should equal b ($b)!\u0026#34;)) Expr head: Symbol string args: Array{Any}((5,)) 1: String \u0026#34;a (\u0026#34; 2: Symbol a 3: String \u0026#34;) should equal b (\u0026#34; 4: Symbol b 5: String \u0026#34;)!\u0026#34; 因此，现在宏不是在 msg_body 中得到一个普通的字符串，而是接收一个完整的表达式，这个表达式需要被评估，以便按照预期的方式显示。这可以直接拼接到返回的表达式中，作为字符串调用的一个参数；完整的实现请参见 error.jl。\n@assert 宏很好地利用了拼接成引号的表达式，简化了宏体内部对表达式的操作。\n卫生宏 在比较复杂的宏中会出现一个问题，那就是卫生问题。简而言之，宏必须确保它们在返回的表达式中引入的变量不会意外地与它们扩展到的周围代码中的现有变量发生冲突。相反，作为参数传入宏中的表达式往往要在周围代码的上下文中进行评估，与现有变量进行交互和修改。另一个关注点来自于一个事实，即一个宏可能会在与它被定义的模块不同的地方被调用。在这种情况下，我们需要确保所有的全局变量被解析到正确的模块中。与具有文本宏扩展的语言（如C语言）相比，Julia 已经有了很大的优势，因为它只需要考虑返回的表达式。所有其他的变量（比如上面 @assert 中的 msg）都遵循正常的作用域块行为。\n为了证明这些问题，让我们考虑写一个 @time 宏，它接受一个表达式作为参数，记录时间，评估表达式，再次记录时间，打印前后时间的差值，然后以表达式的值作为其最终值。这个宏可能是这样的。\nmacro time(ex) return quote local t0 = time_ns() local val = $ex local t1 = time_ns() println(\u0026#34;elapsed time: \u0026#34;, (t1-t0)/1e9, \u0026#34; seconds\u0026#34;) val end end 在这里，我们希望 t0、t1 和 val 是私有的临时变量，我们希望 time 引用 Julia Base 中的 time 函数，而不是用户可能拥有的任何 time 变量（同样适用于 println）。想象一下，如果用户表达式 ex 也包含了对一个叫 t0 的变量的赋值，或者定义了自己的 time 变量，可能会出现的问题。我们可能会得到错误，或者神秘的不正确行为。\nJulia 的宏扩展器用以下方式解决了这些问题。首先，宏结果中的变量被分为局部或全局。如果一个变量被分配给（而不是声明为全局）、声明为局部，或者被用作函数参数名，那么它被认为是局部的。否则，它被认为是全局变量。局部变量就会被重命名为唯一的（使用 gensym 函数，生成新的符号），全局变量则在宏定义环境中解决。因此，上述两个问题都得到了处理；宏的局部变量不会与任何用户变量冲突，time 和 println 将引用 Julia Base 定义。\n然而，仍然存在一个问题。考虑下面这个宏的使用。\nmodule MyModule import Base.@time time() = ... # compute something @time time() end 这里的用户表达式 ex 是对 time 的调用，但不是宏使用的那个 time 函数。它显然是指 MyModule.time。因此我们必须安排 ex 中的代码在宏调用环境中进行解析。这可以通过用 esc 对表达式进行\u0026quot;转义\u0026quot;来实现。\nmacro time(ex) ... local val = $(esc(ex)) ... end 以这种方式包装的表达式，宏扩展器不会管它，只需逐字粘贴到输出中即可。因此它将在宏调用环境中被解析。\n在必要的时候，可以利用这种转义机制来\u0026quot;违反\u0026quot;卫生，以便引入或操作用户变量。例如，下面的宏在调用环境中将 x 设为零。\njulia\u0026gt; macro zerox() return esc(:(x = 0)) end @zerox (macro with 1 method) julia\u0026gt; function foo() x = 1 @zerox return x # is zero end foo (generic function with 1 method) julia\u0026gt; foo() 0 这种对变量的操作应谨慎使用，但偶尔也很方便。\n掌握正确的卫生规则可能是一个艰巨的挑战。在使用宏之前，你可能需要考虑一个函数闭包是否足够。另一个有用的策略是将尽可能多的工作推迟到运行时。例如，许多宏简单地将其参数包裹在 QuoteNode 或其他类似的 Expr 中。一些例子包括 @task body，它简单地返回 schedule(Task(()-\u0026gt; $body))，以及 @eval expr，它简单地返回 eval(QuoteNode(expr))。\n为了演示，我们可以将上面的 @time 例子重写为。\nmacro time(expr) return :(timeit(() -\u0026gt; $(esc(expr)))) end function timeit(f) t0 = time_ns() val = f() t1 = time_ns() println(\u0026#34;elapsed time: \u0026#34;, (t1-t0)/1e9, \u0026#34; seconds\u0026#34;) return val end 然而，我们不这样做是有充分的理由的：将 expr 包装在一个新的作用域块中（匿名函数）也会稍微改变表达式的含义（其中任何变量的作用域），同时我们希望 @time 可以使用，而对被包装的代码影响最小。\n宏和调度 宏，就像 Julia 函数一样，是通用的。这意味着它们也可以有多个方法定义，这要归功于多重分派。\njulia\u0026gt; macro m end @m (macro with 0 methods) julia\u0026gt; macro m(args...) println(\u0026#34;$(length(args))arguments\u0026#34;) end @m (macro with 1 method) julia\u0026gt; macro m(x,y) println(\u0026#34;Two arguments\u0026#34;) end @m (macro with 2 methods) julia\u0026gt; @m \u0026#34;asd\u0026#34; 1 arguments julia\u0026gt; @m 1 2 Two arguments 然而我们应该记住，宏调度是基于交给宏的 AST 类型，而不是 AST 在运行时评估的类型。\njulia\u0026gt; macro m(::Int) println(\u0026#34;An Integer\u0026#34;) end @m (macro with 3 methods) julia\u0026gt; @m 2 An Integer julia\u0026gt; x = 2 2 julia\u0026gt; @m x 1 arguments 代码生成 当需要大量重复的模板代码时，通常会以编程方式生成，以避免冗余。在大多数语言中，这需要一个额外的构建步骤，以及一个单独的程序来生成重复的代码。在 Julia 中，表达式插值和 eval 允许这样的代码生成在程序执行的正常过程中进行。例如，考虑以下自定义类型\nstruct MyNumber x::Float64 end # output 我们想为其添加一些方法。我们可以在下面的循环中以编程的方式进行。\nfor op = (:sin, :cos, :tan, :log, :exp) eval(quote Base.$op(a::MyNumber) = MyNumber($op(a.x)) end) end # output 现在我们可以用我们的自定义类型来使用这些函数。\njulia\u0026gt; x = MyNumber(π) MyNumber(3.141592653589793) julia\u0026gt; sin(x) MyNumber(1.2246467991473532e-16) julia\u0026gt; cos(x) MyNumber(-1.0) 这样一来，Julia 就像自己的预处理器一样，可以从语言内部生成代码。上面的代码可以使用 : 前缀引号的形式，写得稍显生硬。\nfor op = (:sin, :cos, :tan, :log, :exp) eval(:(Base.$op(a::MyNumber) = MyNumber($op(a.x)))) end 不过，这种使用 eval(quote(...)) 模式生成的语言内代码很常见，所以 Julia 自带了一个宏来缩写这种模式。\nfor op = (:sin, :cos, :tan, :log, :exp) @eval Base.$op(a::MyNumber) = MyNumber($op(a.x)) end @eval 宏重写了这个调用，使之与上述较长版本的调用完全等同。对于较长的生成代码块，给 @eval 的表达式参数可以是一个块。\n@eval begin # multiple lines end 非标准字符串字面值 从 Strings 中回想一下，以标识符为前缀的字符串字元称为非标准字符串字元，其语义可能与未加前缀的字符串字面值不同。例如\nr\u0026rdquo;^\\s*(?:#|$) \u0026ldquo;产生一个正则表达式对象，而不是一个字符串。 b \u0026ldquo;DATA\\xff\\u2200 \u0026ldquo;是一个[68,65,84,65,255,226,136,128]的字节数组文字。 也许令人惊讶的是，这些行为并没有被硬编码到Julia解析器或编译器中。相反，它们是由一个通用机制提供的自定义行为，任何人都可以使用：前缀的字符串字元被解析为对特别命名的宏的调用。例如，正则表达式宏就如下。\nmacro r_str(p) Regex(p) end 就是这样。这个宏表示应该将字符串字词r\u0026rdquo;^\\s*(?:#|$) \u0026ldquo;的字面内容传递给@r_str宏，并将扩展的结果放入字符串字词出现的语法树中。换句话说，表达式r\u0026rdquo;^\\s*(?:#|$) \u0026ldquo;相当于将下面的对象直接放入语法树中。\nRegex(\u0026#34;^\\\\s*(?:#|\\$)\u0026#34;) 字符串形式不仅更短、更方便，而且效率更高：由于正则表达式是编译的，而Regex对象实际上是在代码编译时创建的，所以编译只发生一次，而不是每次执行代码时。考虑一下如果正则表达式发生在循环中。\nfor line = lines m = match(r\u0026#34;^\\s*(?:#|$)\u0026#34;, line) if m === nothing # non-comment else # comment end end 由于正则表达式r\u0026rdquo;^/ds*(?:#|$) \u0026ldquo;在解析这段代码时被编译并插入语法树中，所以该表达式只被编译一次，而不是每次循环执行时都被编译。为了在不使用宏的情况下实现这个目标，必须这样写这个循环。\nre = Regex(\u0026#34;^\\\\s*(?:#|\\$)\u0026#34;) for line = lines m = match(re, line) if m === nothing # non-comment else # comment end end 此外，如果编译器不能确定regex对象在所有循环中都是恒定的，某些优化可能就无法实现，这就使得这个版本的效率仍然不如上面更方便的文字形式。当然，在有些情况下，非字面形式还是比较方便的：如果需要在正则表达式中插入一个变量，就必须采取这种比较啰嗦的方式；在正则表达式模式本身是动态的，有可能在每次循环迭代时发生变化的情况下，必须在每次迭代时构造一个新的正则表达式对象。然而，在绝大多数用例中，正则表达式并不是基于运行时数据来构造的。在这大多数情况下，将正则表达式写成编译时值的能力是非常宝贵的。\n与非标准字符串字元一样，非标准命令字元也是使用命令字元语法的前缀变体存在的。命令字元customliteral被解析为@custom_cmd \u0026ldquo;literal\u0026rdquo;。Julia本身不包含任何非标准的命令字元，但包可以利用这种语法。除了语法不同和用_cmd后缀代替_str后缀外，非标准命令字元的行为与非标准字符串字元完全相同。\n如果两个模块提供了名称相同的非标准字符串或命令字元，可以用模块名称来限定字符串或命令字元。例如，如果Foo和Bar都提供了非标准的字符串字元@x_str，那么可以写成Foo.x \u0026ldquo;字元 \u0026ldquo;或Bar.x \u0026ldquo;字元 \u0026ldquo;来区分两者。\n用户定义字符串字元的机制是深刻的、强大的。不仅Julia的非标准字元用它来实现，而且命令字元语法（echo \u0026quot;Hello, $person\u0026quot;）也用下面这个看似无害的宏来实现。\nmacro cmd(str) :(cmd_gen($(shell_parse(str)[1]))) end 当然，这个宏定义中使用的函数中隐藏着大量的复杂性，但它们只是函数，完全是用Julia编写的。你可以阅读它们的源码，并精确地看到它们的作用\u0026ndash;它们所做的只是构造表达式对象，以便插入到你的程序的语法树中。\n生成的函数\n一个非常特殊的宏是@generated，它允许你定义所谓的生成函数。这些函数能够根据其参数的类型生成专门的代码，其灵活性和/或代码量比使用多重调度时更少。宏在解析时与表达式一起工作，不能访问其输入的类型，而生成函数在参数类型已知时得到扩展，但函数尚未编译。\n生成函数声明不是执行一些计算或动作，而是返回一个引号的表达式，然后形成与参数类型相对应的方法的主体。当一个生成函数被调用时，它返回的表达式会被编译，然后运行。为了提高效率，通常会对结果进行缓存。而为了使之可推断，只有有限的语言子集可以使用。因此，生成函数提供了一种灵活的方式，将工作从运行时转移到编译时，但代价是对允许的构造有更大的限制。\n在定义生成函数时，与普通函数有五个主要区别。\n你用 @generated 宏来注释函数声明。这在AST中添加了一些信息，让编译器知道这是一个生成函数。 在生成函数的主体中，你只能访问参数的类型，而不能访问它们的值。 你不是计算一些东西或执行一些操作，而是返回一个引号的表达式，当它被评估时，就会执行你想要的东西。 生成函数只允许调用在生成函数定义之前定义的函数。(如果不遵守这一点，可能会得到引用未来世界时代函数的MethodErrors。) 生成的函数不得突变或观察任何非常态的全局状态（包括，例如，IO、锁、非本地字典或使用hasmethod）。这意味着它们只能读取全局常量，不能有任何副作用。换句话说，它们必须是完全纯粹的。由于实现上的限制，这也意味着它们目前不能定义闭包或生成器。 用一个例子来说明这一点是最简单的。我们可以将一个生成函数foo声明为\njulia\u0026gt; @generated function foo(x) Core.println(x) return :(x * x) end foo (generic function with 1 method) 请注意，函数体返回的是一个引号表达式，即:(x * x)，而不仅仅是x * x的值。\n从调用者的角度来看，这和正则函数是一样的；事实上，你不必知道你调用的是正则函数还是生成函数。让我们来看看foo是如何表现的。\njulia\u0026gt; x = foo(2); # note: output is from println() statement in the body Int64 julia\u0026gt; x # now we print x 4 julia\u0026gt; y = foo(\u0026#34;bar\u0026#34;); String julia\u0026gt; y \u0026#34;barbar\u0026#34; 所以，我们看到，在生成函数的主体中，x是传递的参数的类型，而生成函数返回的值，是我们从定义中返回的引用表达式的评估结果，现在的值是x。\n如果我们用已经使用过的类型再次评估foo，会发生什么？\njulia\u0026gt; foo(4) 16 注意，没有打印出Int64的结果。我们可以看到，这里只针对特定的参数类型集执行了一次生成函数的主体，结果被缓存。之后，对于本例来说，第一次调用时生成函数返回的表达式被重新用作方法体。但是，实际的缓存行为是一种实现定义的性能优化，所以过于依赖这种行为是无效的。\n生成函数的次数可能只有一次，但也可能更频繁，或者看起来根本没有发生。因此，你永远不应该写一个有副作用的生成函数\u0026ndash;副作用何时发生，多久发生一次，都是没有定义的。(这对宏来说也是如此\u0026ndash;就像对宏一样，在生成函数中使用eval是一个标志，表明你做了一些错误的事情。) 然而，与宏不同的是，运行时系统无法正确处理对eval的调用，所以不允许使用它。\n同样重要的是看@生成函数如何与方法重新定义交互。按照正确的@生成函数不能观察任何可突变的状态或引起任何全局状态的突变的原则，我们看到以下行为。观察生成函数不能调用任何在生成函数本身定义之前没有定义的方法。\n最初f(x)有一个定义\njulia\u0026gt; f(x) = \u0026#34;original definition\u0026#34;; 定义其他使用f(x)的操作。\njulia\u0026gt; g(x) = f(x); julia\u0026gt; @generated gen1(x) = f(x); julia\u0026gt; @generated gen2(x) = :(f(x)); 现在我们为f(x)添加一些新的定义。\njulia\u0026gt; f(x::Int) = \u0026#34;definition for Int\u0026#34;; julia\u0026gt; f(x::Type{Int}) = \u0026#34;definition for Type{Int}\u0026#34;; 并比较这些结果有何不同。\njulia\u0026gt; f(1) \u0026#34;definition for Int\u0026#34; julia\u0026gt; g(1) \u0026#34;definition for Int\u0026#34; julia\u0026gt; gen1(1) \u0026#34;original definition\u0026#34; julia\u0026gt; gen2(1) \u0026#34;definition for Int\u0026#34; 生成函数的每个方法都有自己的定义函数视图。\njulia\u0026gt; @generated gen1(x::Real) = f(x); julia\u0026gt; gen1(1) \u0026#34;definition for Type{Int}\u0026#34; 上面的生成函数foo例子并没有做任何普通函数foo(x) = x * x不能做的事情（除了在第一次调用时打印类型，并产生较高的开销）。然而，生成函数的强大之处在于它能够根据传递给它的类型计算不同的引号表达式。\njulia\u0026gt; @generated function bar(x) if x \u0026lt;: Integer return :(x ^ 2) else return :(x) end end bar (generic function with 1 method) julia\u0026gt; bar(4) 16 julia\u0026gt; bar(\u0026#34;baz\u0026#34;) \u0026#34;baz\u0026#34; (当然，这个人为的例子可以更容易地使用多个调度来实现\u0026hellip;)\n滥用这一点将破坏运行时系统并导致未定义的行为。\njulia\u0026gt; @generated function baz(x) if rand() \u0026lt; .9 return :(x^2) else return :(\u0026#34;boo!\u0026#34;) end end baz (generic function with 1 method) 由于生成的函数主体是非确定的，所以它的行为以及所有后续代码的行为都是未定义的。\n不要抄袭这些例子!\n这些例子希望对说明生成函数的工作方式有所帮助，包括在定义端和调用端；但是，不要复制它们，原因如下。\nfoo函数有副作用 (对Core.println的调用), 而这些副作用究竟何时发生, 多久发生一次, 或发生多少次, 都是没有定义的 bar函数解决了一个问题，而这个问题最好用多重调度来解决\u0026ndash;定义bar(x) = x和bar(x::Integer) = x ^ 2会做同样的事情，但它既简单又快速。 baz函数是病态的 请注意，在生成的函数中不应该尝试的操作集是没有限制的，运行时系统目前只能检测到无效操作的一个子集。还有许多其他的操作会在不通知的情况下简单地破坏运行时系统，通常以微妙的方式与坏定义没有明显的联系。因为函数生成器是在推理过程中运行的，它必须尊重该代码的所有限制。\n一些不应该尝试的操作包括。\n缓存本地指针 以任何方式与Core.Compiler的内容或方法进行交互。 观察任何可变状态。\n对生成的函数的推理可以在任何时候运行，包括在您的代码试图观察或突变此状态时。 占用任何锁。你调用的C代码可以在内部使用锁，（例如，调用malloc是没有问题的，即使大多数实现在内部需要锁），但不要试图在执行Julia代码时持有或获取任何锁。 调用在生成函数主体之后定义的任何函数。对于增量加载的预编译模块，这个条件是放宽的，允许调用模块中的任何函数。 好了，现在我们对生成函数的工作原理有了更好的理解，让我们用它们来构建一些更高级的（有效的）功能\u0026hellip;\n一个高级的例子\n茱莉亚的基础库有一个内部的sub2ind函数，用来计算一个线性索引到n维数组中，基于n个多线性索引的集合，换句话说，就是计算可以用A[i]来索引到数组A中的索引i，而不是A[x,y,z,\u0026hellip;]。一种可能的实现方式如下。\njulia\u0026gt; function sub2ind_loop(dims::NTuple{N}, I::Integer...) where N ind = I[N] - 1 for i = N-1👎1 ind = I[i]-1 + dims[i]*ind end return ind + 1 end sub2ind_loop (generic function with 1 method) julia\u0026gt; sub2ind_loop((3, 5), 1, 2) 4 同样的事情也可以用递归来完成。\njulia\u0026gt; sub2ind_rec(dims::Tuple{}) = 1; julia\u0026gt; sub2ind_rec(dims::Tuple{}, i1::Integer, I::Integer...) = i1 == 1 ? sub2ind_rec(dims, I...) : throw(BoundsError()); julia\u0026gt; sub2ind_rec(dims::Tuple{Integer, Vararg{Integer}}, i1::Integer) = i1; julia\u0026gt; sub2ind_rec(dims::Tuple{Integer, Vararg{Integer}}, i1::Integer, I::Integer...) = i1 + dims[1] * (sub2ind_rec(Base.tail(dims), I...) - 1); julia\u0026gt; sub2ind_rec((3, 5), 1, 2) 4 这两种实现虽然不同，但本质上都是一样的：在数组的维度上进行运行时循环，将每个维度的偏移量收集到最后的索引中。\n然而，我们在循环中所需要的所有信息都嵌入在参数的类型信息中。因此，我们可以利用生成函数将迭代移动到编译时；用编译器的说法，我们使用生成函数手动展开循环。身体变得几乎相同，但我们不是计算线性指数，而是建立一个计算指数的表达式。\njulia\u0026gt; @generated function sub2ind_gen(dims::NTuple{N}, I::Integer...) where N ex = :(I[$N] - 1) for i = (N - 1)👎1 ex = :(I[$i] - 1 + dims[$i] * $ex) end return :($ex + 1) end sub2ind_gen (generic function with 1 method) julia\u0026gt; sub2ind_gen((3, 5), 1, 2) 4 这将产生什么代码呢？\n一个简单的方法是将主体提取到另一个（常规）函数中。\njulia\u0026gt; @generated function sub2ind_gen(dims::NTuple{N}, I::Integer...) where N return sub2ind_gen_impl(dims, I...) end sub2ind_gen (generic function with 1 method) julia\u0026gt; function sub2ind_gen_impl(dims::Type{T}, I...) where T \u0026lt;: NTuple{N,Any} where N length(I) == N || return :(error(\u0026#34;partial indexing is unsupported\u0026#34;)) ex = :(I[$N] - 1) for i = (N - 1)👎1 ex = :(I[$i] - 1 + dims[$i] * $ex) end return :($ex + 1) end sub2ind_gen_impl (generic function with 1 method) 现在我们可以执行sub2ind_gen_impl并检查它返回的表达式。\njulia\u0026gt; sub2ind_gen_impl(Tuple{Int,Int}, Int, Int) :(((I[1] - 1) + dims[1] * (I[2] - 1)) + 1) 所以，这里要用到的方法体根本不包含循环\u0026ndash;只是索引到两个元组，乘法和加/减法。所有的循环都是在编译时进行的，我们完全避免了执行过程中的循环。因此，我们对每个类型只进行一次循环，在本例中，每N个类型只循环一次（除非在函数生成一次以上的边缘情况下\u0026ndash;见上面的免责声明）。\n可选生成的函数\n生成函数可以在运行时实现高效率，但也有编译时间成本：每一个具体参数类型的组合都必须生成一个新的函数体。通常情况下，Julia能够编译 \u0026ldquo;通用 \u0026ldquo;版本的函数，这些函数将适用于任何参数，但对于生成函数，这是不可能的。这意味着大量使用生成函数的程序可能无法静态编译。\n为了解决这个问题，语言提供了编写正常的、非生成函数的替代实现的语法。应用到上面的sub2ind例子中，会是这样的。\nfunction sub2ind_gen(dims::NTuple{N}, I::Integer...) where N if N != length(I) throw(ArgumentError(\u0026#34;Number of dimensions must match number of indices.\u0026#34;)) end if @generated ex = :(I[$N] - 1) for i = (N - 1)👎1 ex = :(I[$i] - 1 + dims[$i] * $ex) end return :($ex + 1) else ind = I[N] - 1 for i = (N - 1)👎1 ind = I[i] - 1 + dims[i]*ind end return ind + 1 end end 在内部，这段代码创建了函数的两个实现：一个是生成的，其中使用了if @generated中的第一个块，另一个是正常的，其中使用了 else块。在if @generated块的then部分内部，代码的语义与其他生成函数相同：参数名指的是类型，代码应该返回一个表达式。可能会出现多个if @生成块，在这种情况下，生成的实现使用所有的then块，而备用的实现使用所有的else块。\n注意，我们在函数的顶部添加了一个错误检查。这段代码在两个版本中都是通用的，并且在两个版本中都是运行时代码（它将被引用并作为生成版本的表达式返回）。这意味着局部变量的值和类型在代码生成时是不可用的\u0026ndash;代码生成代码只能看到参数的类型。\n在这种风格的定义中，代码生成功能本质上是一种可选的优化。编译器在方便的情况下会使用它，但其他情况下可能会选择使用正常的实现来代替。这种风格是首选，因为它允许编译器做出更多的决定，并以更多的方式编译程序，而且正常代码比代码生成代码更易读。但是，使用哪种实现取决于编译器的实现细节，所以两种实现的行为必须完全相同。\n","permalink":"https://ohmyweekly.github.io/notes/2020-12-29-metaprogramming/","tags":["Julia","Julia 官方文档"],"title":"元编程"},{"categories":["Flink"],"contents":"FlinkCEP 是在 Flink 之上实现的复杂事件处理（CEP）库。它允许你在无尽的事件流中检测事件模式, 让你有机会掌握数据中的重要内容。\n本页介绍了 Flink CEP 中可用的 API 调用。我们首先介绍 Pattern API, 它允许你指定你想在你的流中检测的模式, 然后介绍你如何检测并对匹配的事件序列采取行动。然后, 我们将介绍 CEP 库在处理事件时间的延迟时做出的假设, 以及如何将你的工作从旧版 Flink 迁移到 Flink-1.3。\n入门 如果你想直接进入, 设置一个 Flink 程序, 并将 FlinkCEP 依赖关系添加到项目的 pom.xml 中。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-cep-scala_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.12.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 信息：FlinkCEP 不是二进制发行版的一部分。请在这里查看如何与它链接进行集群执行。\n现在你可以开始使用模式 API 编写你的第一个 CEP 程序了。\n注意: 你想应用模式匹配的 DataStream 中的事件必须实现适当的 equals() 和 hashCode() 方法, 因为 FlinkCEP 使用它们来比较和匹配事件。\nval input: DataStream[Event] = ... val pattern = Pattern.begin[Event](\u0026#34;start\u0026#34;).where(_.getId == 42) .next(\u0026#34;middle\u0026#34;).subtype(classOf[SubEvent]).where(_.getVolume \u0026gt;= 10.0) .followedBy(\u0026#34;end\u0026#34;).where(_.getName == \u0026#34;end\u0026#34;) val patternStream = CEP.pattern(input, pattern) val result: DataStream[Alert] = patternStream.process( new PatternProcessFunction[Event, Alert]() { override def processMatch( `match`: util.Map[String, util.List[Event]], ctx: PatternProcessFunction.Context, out: Collector[Alert]): Unit = { out.collect(createAlertFrom(pattern)) } }) Pattern API 模式 API 允许你定义你想从输入流中提取的复杂模式序列。\n每个复杂模式序列由多个简单模式组成, 即寻找具有相同属性的单个事件的模式。从现在开始, 我们将把这些简单模式称为模式, 而最终我们要在流中寻找的复杂模式序列, 就是模式序列。你可以把模式序列看成是这样的模式图, 根据用户指定的条件, 从一个模式过渡到下一个模式, 例如 event.getName().equals(\u0026ldquo;end\u0026rdquo;)。一个匹配是一个输入事件的序列, 它通过有效的模式转换序列, 访问复杂模式图的所有模式。\n注意: 每个模式必须有一个唯一的名称, 你以后用它来识别匹配事件。\n注意: 模式名称不能包含字符 \u0026ldquo;:\u0026quot;。\n在本节的其余部分, 我们将首先介绍如何定义 单个模式, 然后介绍如何将单个模式组合成 复杂模式。\n单个模式 模式可以是单个模式, 也可以是循环模式。单元模式只接受一个事件, 而循环模式可以接受多个事件。在模式匹配符号中, 模式 \u0026ldquo;a b+ c?d\u0026rdquo;(或 \u0026ldquo;a\u0026rdquo;, 后面跟着一个或多个 \u0026ldquo;b\u0026rdquo;, 可选地跟着一个 \u0026ldquo;c\u0026rdquo;, 后面跟着一个 \u0026ldquo;d\u0026rdquo;), a、c? 和 d 是单个模式, 而 b+ 是循环模式。默认情况下, 模式是一个单个模式, 你可以通过使用量词将其转换为一个循环模式。每个模式可以有一个或多个条件, 基于这些条件, 它可以接受事件。\n量词 在 FlinkCEP 中, 你可以使用这些方法来指定循环模式：pattern.oneOrMore(), 用于期望给定事件出现一次或多次的模式(例如前面提到的 b+)；以及 pattern.times(#ofTimes), 用于期望给定事件出现的特定次数的模式, 例如 4 个 a；以及 pattern.times(#fromTimes, #toTimes), 用于期望给定事件的特定最小出现次数和最大出现次数的模式, 例如 2-4 个 a。\n你可以使用 pattern.greedy() 方法使循环模式变得贪婪, 但你还不能使分组模式变得贪婪。你可以使用 pattern.option() 方法使所有模式, 不管是否循环, 都是可选的。\n对于名为 start 的模式, 以下是有效的量词。\n// expecting 4 occurrences start.times(4) // expecting 0 or 4 occurrences start.times(4).optional() // expecting 2, 3 or 4 occurrences start.times(2, 4) // expecting 2, 3 or 4 occurrences and repeating as many as possible start.times(2, 4).greedy() // expecting 0, 2, 3 or 4 occurrences start.times(2, 4).optional() // expecting 0, 2, 3 or 4 occurrences and repeating as many as possible start.times(2, 4).optional().greedy() // expecting 1 or more occurrences start.oneOrMore() // expecting 1 or more occurrences and repeating as many as possible start.oneOrMore().greedy() // expecting 0 or more occurrences start.oneOrMore().optional() // expecting 0 or more occurrences and repeating as many as possible start.oneOrMore().optional().greedy() // expecting 2 or more occurrences start.timesOrMore(2) // expecting 2 or more occurrences and repeating as many as possible start.timesOrMore(2).greedy() // expecting 0, 2 or more occurrences start.timesOrMore(2).optional() // expecting 0, 2 or more occurrences and repeating as many as possible start.timesOrMore(2).optional().greedy() 条件\n对于每个模式, 你可以指定一个条件, 传入的事件必须满足这个条件才能被\u0026quot;接受\u0026quot;到模式中, 例如, 它的值应该大于 5, 或者大于之前接受的事件的平均值。你可以通过 pattern.where()、pattern.or() 或 pattern.until() 方法来指定事件属性的条件。这些条件可以是 IterativeConditions 或 SimpleConditions。\n迭代条件。这是最通用的条件类型。你可以通过这种方式指定一个条件, 该条件基于之前接受的事件的属性或其中一个子集的统计量来接受后续事件。\n下面是一个迭代条件的代码, 如果一个名为 \u0026ldquo;middle\u0026rdquo; 的模式的名称以 \u0026ldquo;foo\u0026rdquo; 开头, 并且如果该模式之前接受的事件的价格加上当前事件的价格之和不超过 5.0 的值, 则接受该模式的下一个事件。迭代条件可以发挥强大的作用, 尤其是与循环模式相结合, 例如 oneOrMore()。\nmiddle.oneOrMore() .subtype(classOf[SubEvent]) .where( (value, ctx) =\u0026gt; { lazy val sum = ctx.getEventsForPattern(\u0026#34;middle\u0026#34;).map(_.getPrice).sum value.getName.startsWith(\u0026#34;foo\u0026#34;) \u0026amp;\u0026amp; sum + value.getPrice \u0026lt; 5.0 } ) 注意：调用 ctx.getEventsForPattern(\u0026hellip;) 可以为给定的潜在匹配找到所有之前接受的事件。这个操作的成本可能会有所不同, 所以在实现你的条件时, 尽量减少它的使用。\n描述的上下文使人们也可以访问事件的时间特征。更多信息请看时间上下文。\n简单条件。这种类型的条件扩展了前面提到的 IterativeCondition 类, 仅根据事件本身的属性来决定是否接受一个事件。\nstart.where(event =\u0026gt; event.getName.startsWith(\u0026#34;foo\u0026#34;)) 最后, 你还可以通过 pattern.subtype(subClass) 方法将接受的事件类型限制为初始事件类型的一个子类型（这里是 Event）。\nstart.subtype(classOf[SubEvent]).where(subEvent =\u0026gt; ... /* some condition */) 组合条件。如上所示, 你可以将子类型条件与其他条件结合起来。这对每个条件都适用。你可以通过依次调用 where() 来任意组合条件。最后的结果将是各个条件的结果的逻辑 AND。要使用 OR 组合条件, 可以使用 or() 方法, 如下所示。\npattern.where(event =\u0026gt; ... /* some condition */).or(event =\u0026gt; ... /* or condition */) 停止条件：如果是循环模式(oneOrMore() 和 oneOrMore().option()), 你也可以指定一个停止条件, 例如, 接受值大于 5 的事件, 直到值的总和小于 50。\n为了更好地理解它, 请看下面的例子。给定：\n像 \u0026ldquo;(a+ until b)\u0026rdquo; (一个或多个 \u0026ldquo;a\u0026rdquo; 直到 \u0026ldquo;b\u0026rdquo;) 这样的模式\n输入事件的序列 \u0026ldquo;a1\u0026rdquo; \u0026ldquo;c\u0026rdquo; \u0026ldquo;a2\u0026rdquo; \u0026ldquo;b\u0026rdquo; \u0026ldquo;a3\u0026rdquo;\n该库将输出结果: {a1 a2} {a1} {a2} {a3}.\n正如你所看到的 {a1 a2 a3} 或 {a2 a3} 由于停止条件没有返回。\n where(条件) - 定义当前模式的条件。要匹配模式, 一个事件必须满足条件。多个连续的 where() 子句会导致其条件被 AND 化。  pattern.where(event =\u0026gt; ... /* some condition */)  or(条件) - 添加一个新的条件, 该条件与现有的条件相匹配。一个事件只有在通过至少一个条件的情况下才能与模式匹配。  pattern.where(event =\u0026gt; ... /* some condition */) .or(event =\u0026gt; ... /* alternative condition */)  until(条件) - 指定循环模式的停止条件。意思是如果发生了与给定条件相匹配的事件, 则不会再接受更多的事件进入模式。  仅与 oneOrMore() 结合使用。\n注意：它允许在事件条件下对相应的模式进行清洗状态。\npattern.oneOrMore().until(event =\u0026gt; ... /* some condition */)  subtype(subClass)\t- 为当前模式定义一个子类型条件。只有当一个事件属于这个子类型时, 它才能与模式相匹配。  pattern.subtype(classOf[SubEvent])  oneOrMore() - 指定该模式期望匹配事件至少出现一次。  默认情况下, 使用的是放宽的内部连续（在后续事件之间）。关于内部连续性的更多信息, 请参见 consecutive。\n注意：建议使用 until() 或 within() 来启用状态清除。\npattern.oneOrMore()  timesOrMore(#times) - 指定该模式期望一个匹配事件至少出现 #times 次。  默认情况下, 使用的是放宽的内部连续（在后续事件之间）。关于内部相邻性的更多信息, 请参见 consecutive。\npattern.timesOrMore(2)  times(#ofTimes) - 指定该模式期望匹配事件的准确出现次数。  默认情况下, 使用的是放宽的内部连续性（在后续事件之间）。关于内部相邻性的更多信息, 请参见 consecutive。\npattern.times(2)  times(#fromTimes, #toTimes) - 指定该模式期望匹配事件的 #fromTimes 和 #toTimes 之间出现。  默认情况下, 使用的是放宽的内部连续性（在后续事件之间）。关于内部相邻性的更多信息, 请参见 consecutive。\npattern.times(2, 4)  optional() - 指定该模式是可选的, 即它可能根本不会出现。这适用于上述所有量词。  pattern.oneOrMore().optional()  greedy() - 指定该模式是贪婪的, 即会尽可能多的重复。这只适用于量词, 目前不支持组模式。  pattern.oneOrMore().greedy() 组合模式 现在你已经看到了单个模式的样子, 现在是时候看看如何将它们组合成一个完整的模式序列了。\n一个模式序列必须从一个初始模式开始, 如下所示。\nval start : Pattern[Event, _] = Pattern.begin(\u0026#34;start\u0026#34;) 下一步, 你可以通过指定它们之间所需的毗连条件, 将更多的模式附加到你的模式序列中。FlinkCEP 支持以下形式的事件之间的相邻性。\n 严格相邻: 希望所有匹配的事件严格地一个接一个出现, 中间没有任何非匹配的事件。 Relaxed Contiguity: 忽略匹配事件之间出现的非匹配事件。 非决定性的松弛相邻性（Non-Deterministic Relaxed Contiguity）。进一步放宽相邻性, 允许忽略一些匹配事件的额外匹配。  要在连续模式之间应用它们, 你可以使用:\n next(), 用于严格相邻, followedBy(), 用于松散相邻, 和 followedByAny(), 用于非确定性的松散相邻。  或\n notNext(), 如果你不希望一个事件类型直接跟随另一个事件类型 notFollowedBy(), 如果你不想让一个事件类型位于两个其他事件类型之间的任何地方。  注意：模式序列不能以 notFollowedBy() 结束。\n注意： NOT 模式不能在前面加上一个可选模式。\n// strict contiguity val strict: Pattern[Event, _] = start.next(\u0026#34;middle\u0026#34;).where(...) // relaxed contiguity val relaxed: Pattern[Event, _] = start.followedBy(\u0026#34;middle\u0026#34;).where(...) // non-deterministic relaxed contiguity val nonDetermin: Pattern[Event, _] = start.followedByAny(\u0026#34;middle\u0026#34;).where(...) // NOT pattern with strict contiguity val strictNot: Pattern[Event, _] = start.notNext(\u0026#34;not\u0026#34;).where(...) // NOT pattern with relaxed contiguity val relaxedNot: Pattern[Event, _] = start.notFollowedBy(\u0026#34;not\u0026#34;).where(...) 松散毗连意味着只有第一个后续的匹配事件才会被匹配, 而对于非确定性的松散毗连, 同一开头会发出多个匹配。举个例子, 一个模式 \u0026ldquo;a b\u0026rdquo;, 给定事件序列 \u0026ldquo;a\u0026rdquo;, \u0026ldquo;c\u0026rdquo;, \u0026ldquo;b1\u0026rdquo;, \u0026ldquo;b2\u0026rdquo;, 将得到以下结果。\n\u0026ldquo;a\u0026rdquo; 和 \u0026ldquo;b\u0026rdquo; 之间有严格的毗连性。{} (不匹配), \u0026ldquo;a\u0026rdquo; 后面的 \u0026ldquo;c\u0026rdquo; 会导致 \u0026ldquo;a\u0026rdquo; 被丢弃。\n\u0026ldquo;a\u0026rdquo; 和 \u0026ldquo;b\u0026rdquo; 之间的松散相邻性。{a b1}, 因为松散连续性被看作是 \u0026ldquo;跳过非匹配事件, 直到下一个匹配事件\u0026rdquo;。\n\u0026ldquo;a\u0026rdquo; 和 \u0026ldquo;b\u0026rdquo; 之间的非确定性松散相邻性。{a b1}, {a b2}, 因为这是最一般的形式。\n也可以定义一个时间约束, 让模式有效。例如, 你可以通过 pattern.within() 方法定义一个模式应该在 10 秒内发生。处理时间和事件时间都支持时间模式。\n注意: 模式序列只能有一个时间约束。如果在不同的单个模式上定义了多个这样的约束, 那么就采用最小的约束。\nnext.within(Time.seconds(10)) 循环模式中的相邻性\n你可以在循环模式中应用与上一节讨论的相同的相邻性条件。相邻性将被应用在这样一个模式中的元素之间。为了举例说明, 模式序列 \u0026ldquo;a b+ c\u0026rdquo;（\u0026ldquo;a\u0026rdquo; 后面跟着一个或多个 \u0026ldquo;b\u0026rdquo; 的任意（非确定的松散的）序列, 后面跟着一个 \u0026ldquo;c\u0026rdquo;）, 输入 \u0026ldquo;a\u0026rdquo;、\u0026ldquo;b1\u0026rdquo;、\u0026ldquo;d1\u0026rdquo;、\u0026ldquo;b2\u0026rdquo;、\u0026ldquo;d2\u0026rdquo;、\u0026ldquo;b3\u0026rdquo;、\u0026ldquo;c\u0026rdquo;, 会有以下结果。\n严格相邻性：{a b3 c} - \u0026ldquo;b1\u0026rdquo; 后面的 \u0026ldquo;d1\u0026rdquo; 会导致 \u0026ldquo;b1\u0026rdquo; 被丢弃, \u0026ldquo;b2\u0026rdquo; 也会因为 \u0026ldquo;d2\u0026rdquo; 而被丢弃。\n放宽相邻性：{a b1 c}, {a b1 b2 c}, {a b1 b2 b3 c}, {a b2 c}, {a b2 b3 c}, {a b3 c} - \u0026ldquo;d\u0026rdquo; 被忽略。\n非确定性的松弛相邻性：{a b1 c}, {a b1 b2 c}, {a b1 b3 c}, {a b1 b2 b3 c}, {a b2 c}, {a b2 b3 c}, {a b3 c} - 注意{a b1 b3 c}, 这是 \u0026ldquo;b\u0026rdquo; 之间松弛相邻性的结果。\n对于循环模式(例如 oneOrMore() 和 times()), 默认是放宽毗连性。如果你想要严格的相邻性, 你必须通过使用 continuous() 调用来明确指定, 如果你想要非确定性的松弛相邻性, 你可以使用 allowCombinations() 调用。\n consecutive()  与 oneOrMore() 和 times() 一起使用, 并在匹配的事件之间施加严格的毗连性, 即任何不匹配的元素都会中断匹配（如 next()）。\n如果不应用, 则使用宽松的连续性（如 followedBy()）。\n例如, 像这样的模式。\nPattern.begin(\u0026#34;start\u0026#34;).where(_.getName().equals(\u0026#34;c\u0026#34;)) .followedBy(\u0026#34;middle\u0026#34;).where(_.getName().equals(\u0026#34;a\u0026#34;)) .oneOrMore().consecutive() .followedBy(\u0026#34;end1\u0026#34;).where(_.getName().equals(\u0026#34;b\u0026#34;)) 将为一个输入序列生成以下匹配。C D A1 A2 A3 D A4 B\n与连续应用。{C A1 B}, {C A1 A2 B}, {C A1 A2 A3 B}。\n不连续应用。{C A1 B}, {C A1 A2 B}, {C A1 A2 A3 B}, {C A1 A2 A3 A4 B}。\n allowCombinations()  与 oneOrMore()和 times()一起使用, 并在匹配的事件之间施加非确定性的松散相邻性（如 followedByAny()）。\n如果不应用, 则使用松散的相邻性（如 followedBy()）。\n例如, 像这样的模式。\nPattern.begin(\u0026#34;start\u0026#34;).where(_.getName().equals(\u0026#34;c\u0026#34;)) .followedBy(\u0026#34;middle\u0026#34;).where(_.getName().equals(\u0026#34;a\u0026#34;)) .oneOrMore().allowCombinations() .followedBy(\u0026#34;end1\u0026#34;).where(_.getName().equals(\u0026#34;b\u0026#34;)) 将为一个输入序列生成以下匹配。C D A1 A2 A3 D A4 B\n启用组合。{C A1 B}、{C A1 A2 B}、{C A1 A3 B}、{C A1 A4 B}、{C A1 A2 A3 B}、{C A1 A2 A4 B}、{C A1 A3 A4 B}、{C A1 A2 A3 A4 B}、{C A1 A2 A3 A4 B}。\n不启用组合。{C A1 B}, {C A1 A2 B}, {C A1 A2 A3 B}, {C A1 A2 A3 A4 B}。\n模式组 也可以定义一个模式序列作为 begin、followBy、followByAny 和 next 的条件。该模式序列将被视为逻辑上的匹配条件, 并将返回一个 GroupPattern, 并且可以对 GroupPattern 应用 oneOrMore()、times(#ofTimes)、times(#fromTimes、#toTimes)、optional()、continuous()、allowCombinations()。\nval start: Pattern[Event, _] = Pattern.begin( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;start_middle\u0026#34;).where(...) ) // strict contiguity val strict: Pattern[Event, _] = start.next( Pattern.begin[Event](\u0026#34;next_start\u0026#34;).where(...).followedBy(\u0026#34;next_middle\u0026#34;).where(...) ).times(3) // relaxed contiguity val relaxed: Pattern[Event, _] = start.followedBy( Pattern.begin[Event](\u0026#34;followedby_start\u0026#34;).where(...).followedBy(\u0026#34;followedby_middle\u0026#34;).where(...) ).oneOrMore() // non-deterministic relaxed contiguity val nonDetermin: Pattern[Event, _] = start.followedByAny( Pattern.begin[Event](\u0026#34;followedbyany_start\u0026#34;).where(...).followedBy(\u0026#34;followedbyany_middle\u0026#34;).where(...) ).optional()  begin(#name) - 定义一个起始模式。  val start = Pattern.begin[Event](\u0026#34;start\u0026#34;)  begin(#pattern_sequence) - 定义一个起始模式。  val start = Pattern.begin( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) )  next(#name) - 添加一个新的模式。一个匹配事件必须直接接替前一个匹配事件（严格相邻）。  val next = start.next(\u0026#34;middle\u0026#34;)  next(#pattern_sequence) - 添加一个新的模式。一个匹配事件的序列必须直接接替前一个匹配事件（严格相邻）。  val next = start.next( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) )  followedBy(#name) - 添加一个新的模式。其他事件可以发生在一个匹配事件和上一个匹配事件之间（松散的相邻性）。  val followedBy = start.followedBy(\u0026#34;middle\u0026#34;)  followedBy(#pattern_sequence)\t- 添加一个新的模式。其他事件可以发生在一系列匹配事件和前一个匹配事件之间（放松的相邻性）。  val followedBy = start.followedBy( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) )  followedByAny(#name) - 添加一个新的模式。在一个匹配事件和上一个匹配事件之间可以发生其他事件, 并且对每一个备选匹配事件都会呈现备选匹配（非确定性的松散毗连性）。  val followedByAny = start.followedByAny(\u0026#34;middle\u0026#34;)  followedByAny(#pattern_sequence) - 添加一个新的模式。在一个匹配事件序列和前一个匹配事件之间可以发生其他事件, 并且将为每一个可供选择的匹配事件序列呈现备选匹配（非确定性的松散毗连性）。  val followedByAny = start.followedByAny( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) ) notNext() - 添加一个新的否定模式。一个匹配（负值）事件必须直接接替前一个匹配事件（严格的相邻性）, 以使部分匹配被丢弃。\nval notNext = start.notNext(\u0026#34;not\u0026#34;)  notFollowedBy() - 添加一个新的负模式。即使在匹配（负值）事件和前一个匹配事件之间发生了其他事件, 部分匹配事件序列也会被丢弃（松散的相邻性）。  val notFollowedBy = start.notFollowedBy(\u0026#34;not\u0026#34;) within(time) - 定义事件序列匹配模式的最大时间间隔。如果一个未完成的事件序列超过了这个时间, 它将被丢弃。\npattern.within(Time.seconds(10)) After Match Skip Strategy 对于一个给定的模式, 同一个事件可能会被分配给多个成功的匹配。要控制一个事件将被分配到多少个匹配中, 你需要指定名为 AfterMatchSkipStrategy 的跳过策略。有五种类型的跳过策略, 如下所示。\n NO_SKIP: 每一个可能的匹配都会被发出。 SKIP_TO_NEXT：丢弃每一个局部的匹配, 从相同的事件开始, 发射匹配开始。 SKIP_PAST_LAST_EVENT: 丢弃每一个在匹配开始后但结束前开始的部分匹配。 SKIP_TO_FIRST: 丢弃每个在匹配开始后但在 PatternName 的第一个事件发生之前开始的部分匹配。 SKIP_TO_LAST: 丢弃在匹配开始后但在 PatternName 的最后一个事件发生之前开始的每一个部分匹配。  注意, 当使用 SKIP_TO_FIRST 和 SKIP_TO_LAST 跳过策略时, 还应该指定一个有效的 PatternName。\n例如, 对于给定的模式 b+ c 和数据流 b1 b2 b3 c, 这四种跳过策略的区别如下。\n   Skip Strategy 结果 描述     NO_SKIP b1 b2 b3 c b2 b3 c b3 c 找到匹配的 b1 b2 b3 c 后, 匹配过程不会丢弃任何结果。   SKIP_TO_NEXT b1 b2 b3 c b2 b3 c b3 c 找到匹配的 b1 b2 b3 c 后, 匹配过程不会丢弃任何结果, 因为没有其他匹配可以从 b1 开始。   SKIP_PAST_LAST_EVENT b1 b2 b3 c 在找到匹配的 b1 b2 b3 c 后, 匹配过程将放弃所有开始的部分匹配。   SKIP_TO_FIRST[b] b1 b2 b3 c b2 b3 c b3 c 找到匹配的 b1 b2 b3 c 后, 匹配过程会尝试丢弃所有在 b1 之前开始的部分匹配, 但没有这样的匹配。因此, 没有任何匹配结果会被丢弃。   SKIP_TO_LAST[b] b1 b2 b3 c b3 c 找到匹配的 b1 b2 b3 c 后, 匹配过程会尝试丢弃所有在 b3 之前开始的部分匹配。有一个这样的匹配 b2 b3 c。    还可以看看另一个例子, 以更好地了解 NO_SKIP 和 SKIP_TO_FIRST 的区别：模式: (a | b | c) (b | c) c+.greedy d 和序列: a b c1 c2 c3 d 那么结果将是:\n   Skip Strategy 结果 描述     NO_SKIP a b c1 c2 c3 d b c1 c2 c3 d c1 c2 c3 d 找到匹配的 a b c1 c2 c3 d 后, 匹配过程不会丢弃任何结果。   SKIP_TO_FIRST[c*] a b c1 c2 c3 d c1 c2 c3 d 在找到匹配的 a b c1 c2 c3 d 后, 匹配过程将丢弃所有在 c1 之前开始的部分匹配, 有一个这样的匹配 b c1 c2 c3 d。有一个这样的匹配 b c1 c2 c3 d。    为了更好地理解 NO_SKIP 和 SKIP_TO_NEXT 的区别, 请看下面的例子: Pattern: a b+ 和序列: a b1 b2 b3 那么结果将是:\n   Skip Strategy 结果 描述     NO_SKIP a b1 a b1 b2 a b1 b2 b3 找到匹配的 b1 后, 匹配过程不会丢弃任何结果。   SKIP_TO_NEXT a b1 在找到匹配的 b1 后, 匹配过程将丢弃从 a 开始的所有部分匹配, 这意味着既不能生成 b1 b2, 也不能生成 b1 b2 b3。    要指定使用哪种跳过策略, 只需调用 AfterMatchSkipStrategy 来创建一个 AfterMatchSkipStrategy。\n   功能 描述     AfterMatchSkipStrategy.noSkip() 创建一个 NO_SKIP 跳过策略   AfterMatchSkipStrategy.skipToNext() 创建一个 SKIP_TO_NEXT 跳过策略。   AfterMatchSkipStrategy.skipPastLastEvent() 创建一个 SKIP_PAST_LAST_EVENT 跳过策略。   AfterMatchSkipStrategy.skipToFirst(patternName) 用引用的模式名 patternName 创建一个 SKIP_TO_FIRST 跳过策略。   AfterMatchSkipStrategy.skipToLast(patternName) 用引用的模式名 patternName 创建一个 SKIP_TO_LAST 跳过策略。    然后通过调用跳过策略来应用于模式。\nval skipStrategy = ... Pattern.begin(\u0026#34;patternName\u0026#34;, skipStrategy) 注意 对于 SKIP_TO_FIRST/LAST 有两个选项来处理没有元素映射到指定变量的情况。默认情况下, 将使用 NO_SKIP 策略。另一个选项是在这种情况下抛出异常。我们可以通过以下方式启用这个选项\nAfterMatchSkipStrategy.skipToFirst(patternName).throwExceptionOnMiss() 检测模式 在指定了你要寻找的模式序列后, 现在是时候将其应用到你的输入流中以检测潜在的匹配。要针对你的模式序列运行事件流, 你必须创建一个 PatternStream。给定一个输入流输入、一个模式模式和一个可选的比较器比较器, 用于在 EventTime 的情况下对具有相同时间戳的事件或在同一时刻到达的事件进行排序, 你可以通过调用创建 PatternStream。\nval input : DataStream[Event] = ... val pattern : Pattern[Event, _] = ... var comparator : EventComparator[Event] = ... // optional  val patternStream: PatternStream[Event] = CEP.pattern(input, pattern, comparator) 输入流可以是 keyed 的, 也可以是 non-keyed 的, 这取决于你的使用情况。\n注意: 在 non-keyed 流上应用模式将导致作业的并行度等于 1。\n从模式中选择 一旦你获得了一个 PatternStream, 你就可以对检测到的事件序列进行转换。建议的方法是通过 PatternProcessFunction 来实现。\nPatternProcessFunction 有一个 processMatch 方法, 它对每个匹配的事件序列都会被调用。它以 Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; 的形式接收匹配, 其中键是你的模式序列中每个模式的名称, 值是该模式的所有接受事件的列表（IN 是你的输入元素的类型）。给定模式的事件是按时间戳排序的。返回每个模式所接受的事件列表的原因是, 当使用循环模式(例如 oneToMany() 和 times())时, 一个给定模式可能会接受多个事件。\nclass MyPatternProcessFunction\u0026lt;IN, OUT\u0026gt; extends PatternProcessFunction\u0026lt;IN, OUT\u0026gt; { @Override public void processMatch(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; match, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; IN startEvent = match.get(\u0026#34;start\u0026#34;).get(0); IN endEvent = match.get(\u0026#34;end\u0026#34;).get(0); out.collect(OUT(startEvent, endEvent)); } } PatternProcessFunction 提供了对 Context 对象的访问。通过它, 我们可以访问与时间相关的特性, 如当前处理时间或当前匹配的时间戳（这是分配给匹配的最后一个元素的时间戳）。更多信息请看时间上下文。通过这个上下文, 我们还可以将结果发送到一个侧输出。\n处理超时的部分模式 当一个模式通过 within 关键字附加了一个窗口长度时, 部分事件序列有可能因为超过窗口长度而被丢弃。要对一个超时的部分匹配采取行动, 可以使用 TimedOutPartialMatchHandler 接口。该接口应该以混搭的方式使用。这意味着你可以在你的 PatternProcessFunction 中额外实现这个接口。TimedOutPartialMatchHandler 提供了额外的 processTimedOutMatch 方法, 该方法将为每个超时部分匹配调用。\nclass MyPatternProcessFunction\u0026lt;IN, OUT\u0026gt; extends PatternProcessFunction\u0026lt;IN, OUT\u0026gt; implements TimedOutPartialMatchHandler\u0026lt;IN\u0026gt; { @Override public void processMatch(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; match, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; ... } @Override public void processTimedOutMatch(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; match, Context ctx) throws Exception; IN startEvent = match.get(\u0026#34;start\u0026#34;).get(0); ctx.output(outputTag, T(startEvent)); } } 注意: processTimedOutMatch 不给人访问主输出的机会。但你仍然可以通过 Context 对象, 通过侧输出来发出结果。\n方便的 API 前面提到的 PatternProcessFunction 是在 Flink 1.8 中引入的, 从那时起, 它就是推荐的与匹配交互的方式。人们仍然可以使用老式的 API, 比如 select/flatSelect, 内部会被翻译成 PatternProcessFunction。\nval patternStream: PatternStream[Event] = CEP.pattern(input, pattern) val outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) val result: SingleOutputStreamOperator[ComplexEvent] = patternStream.flatSelect(outputTag){ (pattern: Map[String, Iterable[Event]], timestamp: Long, out: Collector[TimeoutEvent]) =\u0026gt; out.collect(TimeoutEvent()) } { (pattern: mutable.Map[String, Iterable[Event]], out: Collector[ComplexEvent]) =\u0026gt; out.collect(ComplexEvent()) } val timeoutResult: DataStream[TimeoutEvent] = result.getSideOutput(outputTag) 在 CEP 库中的时间 处理事件时间的延迟 在 CEP 中, 处理元素的顺序很重要。为了保证元素在事件时间工作时以正确的顺序进行处理, 一个传入的元素最初会被放在一个缓冲区中, 在这个缓冲区中, 元素根据其时间戳按升序排序, 当一个水印到达时, 这个缓冲区中所有时间戳小于水印的元素都会被处理。这意味着水印之间的元素是按事件时间顺序处理的。\n注意: 当在事件时间内工作时, 该库假定水印的正确性。\n为了保证水印之间的元素按事件时间顺序处理, Flink 的 CEP 库假设水印的正确性, 并将时间戳小于最后看到的水印的元素视为迟到元素。迟到的元素不会被进一步处理。另外, 你可以指定一个 sideOutput 标签来收集最后一次看到的水印之后的迟到元素, 你可以这样使用。\nval patternStream: PatternStream[Event] = CEP.pattern(input, pattern) val lateDataOutputTag = OutputTag[String](\u0026#34;late-data\u0026#34;) val result: SingleOutputStreamOperator[ComplexEvent] = patternStream .sideOutputLateData(lateDataOutputTag) .select{ pattern: Map[String, Iterable[ComplexEvent]] =\u0026gt; ComplexEvent() } val lateData: DataStream[String] = result.getSideOutput(lateDataOutputTag) 时间上下文 在 PatternProcessFunction 以及 IterativeCondition 中, 用户可以访问一个实现 TimeContext 的上下文, 如下所示。\n/** * Enables access to time related characteristics such as current processing time or timestamp of * currently processed element. Used in {@link PatternProcessFunction} and * {@link org.apache.flink.cep.pattern.conditions.IterativeCondition} */ @PublicEvolving public interface TimeContext { /** * Timestamp of the element currently being processed. * * \u0026lt;p\u0026gt;In case of {@link org.apache.flink.streaming.api.TimeCharacteristic#ProcessingTime} this * will be set to the time when event entered the cep operator. */ long timestamp(); /** Returns the current processing time. */ long currentProcessingTime(); } 这个上下文让用户可以访问处理事件的时间特征（在 IterativeCondition 的情况下是传入记录, 在 PatternProcessFunction 的情况下是匹配）。调用 TimeContext#currentProcessingTime 总是给你当前处理时间的值, 这个调用应该比调用 System.currentTimeMillis()更可取。\n在 TimeContext#timestamp() 的情况下, 返回的值等于 EventTime 中分配的时间戳。在 ProcessingTime 中, 这将等于所述事件进入 cep 运算符的时间点(或者在 PatternProcessFunction 的情况下生成匹配时)。这意味着该值将在对该方法的多次调用中保持一致。\n例子 下面的例子是在事件的键控数据流上检测模式 start, middle(name = \u0026ldquo;error\u0026rdquo;) -\u0026gt; end(name = \u0026ldquo;critical\u0026rdquo;)。这些事件通过其 id 进行 keyed, 一个有效的模式必须在 10 秒内出现。整个处理过程是以事件时间来完成的。\nval env : StreamExecutionEnvironment = ... val input : DataStream[Event] = ... val partitionedInput = input.keyBy(event =\u0026gt; event.getId) val pattern = Pattern.begin[Event](\u0026#34;start\u0026#34;) .next(\u0026#34;middle\u0026#34;).where(_.getName == \u0026#34;error\u0026#34;) .followedBy(\u0026#34;end\u0026#34;).where(_.getName == \u0026#34;critical\u0026#34;) .within(Time.seconds(10)) val patternStream = CEP.pattern(partitionedInput, pattern) val alerts = patternStream.select(createAlert(_)) 从旧版本(1.3 前)迁移到 1.4 以上版本 迁移到 1.4+ 版本 在 Flink-1.4 中, CEP 库与 \u0026lt;= Flink 1.2 的向后兼容性被取消。不幸的是, 无法恢复曾经在 1.2.x 下运行的 CEP 作业。\n迁移到 1.3.x Flink-1.3 中的 CEP 库有很多新的特性, 这导致了 API 的一些变化。在这里, 我们描述了为了能够在 Flink-1.3 中运行, 你需要对你的旧 CEP 作业进行的修改。在做了这些改变并重新编译你的作业后, 你将能够从旧版作业的保存点恢复执行, 也就是说, 不需要重新处理你过去的数据。\n所需的更改是:\n  改变你的条件（在 where(\u0026hellip;) 子句中的条件）来扩展 SimpleCondition 类, 而不是实现 FilterFunction 接口。\n  改变你的函数作为参数提供给 select(\u0026hellip;) 和 flatSelect(\u0026hellip;) 方法, 以期望与每个模式相关联的事件列表(Java 中为 List, Scala 中为 Iterable)。这是因为增加了循环模式后, 多个输入事件可以匹配一个（循环）模式。\n  Flink 1.1 和 1.2 中的 followBy() 暗示了非确定性的松散毗连性（见这里）。在 Flink 1.3 中, 这一点发生了变化, followBy() 意味着松散毗连, 而 followByAny() 应该在需要非确定性松散毗连的情况下使用。\n  ","permalink":"https://ohmyweekly.github.io/notes/2020-12-20-flink-cep-complex-event-processing-for-flink/","tags":["Flink","Flink 官方文档"],"title":"FlinkCEP - Flink 的复杂事件处理"},{"categories":["Flink"],"contents":"执行模式(流/批) DataStream API 支持不同的运行时执行模式，你可以根据用例的要求和作业的特点从中选择。\nDataStream API 有一种\u0026quot;经典\u0026quot;的执行行为，我们称之为 STREAMING 执行模式。这应该用于需要连续增量处理并预计无限期保持在线的无边界作业。\n此外，还有一种批式执行模式，我们称之为 BATCH 执行模式。这种执行作业的方式更容易让人联想到批处理框架，如 MapReduce。这应该用于有边界的作业，对于这些作业，你有一个已知的固定输入，并且不会连续运行。\nApache Flink 对流和批处理的统一方法意味着，无论配置何种执行模式，在有界输入上执行的 DataStream 应用都会产生相同的最终结果。重要的是要注意这里的 final 是什么意思：在 streaming 模式下执行的作业可能会产生增量更新（想想数据库中的 upserts），而 batch 作业在最后只会产生一个最终结果。如果解释正确的话，最终的结果是一样的，但是到达那里的方式可能是不同的。\n通过启用 BATCH 执行，我们允许 Flink 应用额外的优化，而这些优化只有在我们知道我们的输入是有边界的情况下才能进行。例如，可以使用不同的 join/aggregation 策略，此外还可以使用不同的 shuffle 实现，允许更高效的任务调度和故障恢复行为。下面我们将介绍一些执行行为的细节。\n什么时候可以/应该使用 BATCH 执行模式？ BATCH 执行模式只能用于有边界的 Job/Link 程序。边界性是数据源的一个属性，它告诉我们在执行之前，来自该数据源的所有输入是否都是已知的，或者是否会有新的数据出现，可能是无限的。而一个作业，如果它的所有源都是有界的，则是有界的，否则就是无界的。\n另一方面，STREAMING 执行模式既可以用于有界作业，也可以用于无界作业。\n作为经验法则，当你的程序是有界的时候，你应该使用 BATCH 执行模式，因为这样会更有效率。当你的程序是无边界的时候，你必须使用 STREAMING 执行模式，因为只有这种模式足够通用，能够处理连续的数据流。\n一个明显的例外情况是，当你想使用一个有界作业来引导一些作业状态，然后你想在一个无界作业中使用。例如，通过使用 STREAMING 模式运行一个有界作业，取一个保存点，然后在一个无界作业上恢复该保存点。这是一个非常特殊的用例，当我们允许将保存点作为 BATCH 执行作业的额外输出时，这个用例可能很快就会过时。\n另一个可能使用 STREAMING 模式运行有边界作业的情况是为最终将在无边界源中运行的代码编写测试时。对于测试来说，在这些情况下使用有界源可能更自然。\n配置 BATCH 执行模式 执行模式可以通过 execute.runtim-mode 设置来配置。有三种可能的值:\n STREAMING: 经典的 DataStream 执行模式(默认) BATCH: 在 DataStream API 上进行批量式执行 AUTOMATIC：让系统根据源的边界性来决定  这可以通过 bin/flink run ... 的命令行参数进行配置，或者在创建/配置 StreamExecutionEnvironment 时进行编程。\n下面是如何通过命令行配置执行模式:\n$ bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar 这个例子展示了如何在代码中配置执行模式:\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRuntimeMode(RuntimeExecutionMode.BATCH);  注意：我们建议用户不要在程序中设置运行模式，而是在提交应用程序时使用命令行进行设置。保持应用程序代码的免配置可以让程序更加灵活，因为同一个应用程序可以在任何执行模式下执行。\n 执行行为 本节概述了 BATCH 执行模式的执行行为，并与 STREAMING 执行模式进行了对比。详细内容请参考介绍该功能的 FLIP-134 和 FLIP-140。\n任务调度和网络洗牌(Shuffle) Flink 作业(job)由不同的操作(operation)组成，这些操作在数据流图中连接在一起。系统决定如何在不同的进程/机器（TaskManager）上安排这些操作的执行，以及如何在它们之间洗牌（发送）数据。\n多个操作/运算符可以使用一种称为链式的功能链在一起。Flink 认为作为调度单位的一组一个或多个（链式）运算符(operators )被称为任务(task)。通常，子任务(subtask)一词用来指在多个 TaskManager 上并行运行的单个任务实例，但我们在这里只使用任务(task)一词。\n任务调度和网络洗牌对于 BATCH 和 STREAMING 执行模式的工作方式不同。主要是由于我们知道我们的输入数据在 BATCH 执行模式下是有边界的，这使得 Flink 可以使用更高效的数据结构和算法。\n我们将用这个例子来解释任务调度和网络传输的差异。\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource\u0026lt;String\u0026gt; source = env.fromElements(...); source.name(\u0026#34;source\u0026#34;) .map(...).name(\u0026#34;map1\u0026#34;) .map(...).name(\u0026#34;map2\u0026#34;) .rebalance() .map(...).name(\u0026#34;map3\u0026#34;) .map(...).name(\u0026#34;map4\u0026#34;) .keyBy((value) -\u0026gt; value) .map(...).name(\u0026#34;map5\u0026#34;) .map(...).name(\u0026#34;map6\u0026#34;) .sinkTo(...).name(\u0026#34;sink\u0026#34;); 暗示操作之间1对1连接模式的操作，如 map()、flatMap() 或 filter()，可以直接将数据转发到下一个操作，这使得这些操作可以链在一起。这意味着 Flink 通常不会在它们之间插入网络洗牌。\n而 keyBy() 或 rebalance() 等操作则需要在不同的任务并行实例之间进行数据洗牌。这就会引起网络洗牌。\n对于上面的例子，Flink 会把操作分组为这样的任务。\n Task1: source, map1 和 map2 Task2: map3, map4 Task3: map5, map6 和 sink  而我们在任务1和2，以及任务2和3之间进行网络洗牌。这是该作业的可视化表示。\nSTREAMING 执行模式 在 STREAMING 执行模式下，所有任务需要一直在线/运行。这使得 Flink 可以通过整个管道立即处理新的记录，而我们需要的是连续和低延迟的流处理。这也意味着分配给一个任务的 TaskManagers 需要有足够的资源来同时运行所有的任务。\n网络洗牌是流水线式的，这意味着记录会被立即发送到下游任务，并在网络层上进行一些缓冲。同样，这也是需要的，因为当处理连续的数据流时，在任务（或任务的管道）之间没有自然的数据点（时间点）可以物化。这与 BATCH 执行模式形成了鲜明的对比，在 BATCH 执行模式下，中间的结果可以被具体化，如下所述。\nBATCH 执行模式 在 BATCH 执行模式下，一个作业的任务可以被分离成可以一个接一个执行的阶段。我们之所以能做到这一点，是因为输入是有边界的，因此 Flink 可以在进入下一个阶段之前完全处理管道的一个阶段。在上面的例子中，工作会有三个阶段，对应着被洗牌障碍分开的三个任务。\n分阶段处理并不是像上面针对 STREAMING 模式所解释的那样，立即向下游任务发送记录，而是需要 Flink 将任务的中间结果物化到一些非永续存储中，让下游任务在上游任务已经下线后再读取。这将增加处理的延迟，但也会带来其他有趣的特性。首先，这允许 Flink 在故障发生时回溯到最新的可用结果，而不是重新启动整个任务。另一个副作用是，BATCH 作业可以在更少的资源上执行（就 TaskManagers 的可用槽而言），因为系统可以一个接一个地顺序执行任务。\nTaskManagers 将至少在下游任务没有消耗它们的情况下保留中间结果。(从技术上讲，它们将被保留到消耗的流水线区域产生它们的输出为止)。在这之后，只要空间允许，它们就会被保留，以便在失败的情况下，可以回溯到前面提到的结果。\n状态后端/状态 在 STREAMING 模式下，Flink 使用 StateBackend 来控制状态的存储方式和检查点的工作方式。\n在 BATCH 模式下，配置的状态后端被忽略。取而代之的是，keyed 操作的输入按键分组（使用排序），然后我们依次处理一个键的所有记录。这样就可以同时只保留一个键的状态。当转到下一个键时，一个给定键的状态将被丢弃。\n关于这方面的背景信息，请参见 FLIP-140。\n事件时间/水印 在支持事件时间方面，Flink 的流运行时建立在一个悲观的假设上，即事件可能会出现顺序外，即一个时间戳t的事件可能会在一个时间戳t+1的事件之后出现。正因为如此，系统永远无法确定在给定的时间戳T下，未来不会再有时间戳 t\u0026lt;T 的元素出现。为了摊平这种失序性对最终结果的影响，同时使系统实用，在 STREAMING 模式下，Flink 使用了一种名为 Watermarks 的启发式方法。一个带有时间戳T的水印标志着没有时间戳 t\u0026lt;T 的元素会跟随。\n在 BATCH 模式下，输入的数据集是事先已知的，不需要这样的启发式，因为至少可以按照时间戳对元素进行排序，从而按照时间顺序进行处理。对于熟悉流的读者来说，在 BATCH 中，我们可以假设\u0026quot;完美的水印\u0026quot;。\n鉴于上述情况，在 BATCH 模式下，我们只需要在输入的末尾有一个与每个键相关的 MAX_WATERMARK，如果输入流没有键，则在输入的末尾有一个。基于这个方案，所有注册的定时器都会在时间结束时触发，用户定义的 WatermarkAssigners 或 WatermarkStrategies 会被忽略。\n处理时间 处理时间是指在处理记录的具体实例上，处理记录的机器上的挂钟时间。根据这个定义，我们看到，基于处理时间的计算结果是不可重复的。这是因为同一条记录被处理两次，会有两个不同的时间戳。\n尽管如此，在 STREAMING 模式下使用处理时间还是很有用的。原因与流媒体管道经常实时摄取其无限制的输入有关，所以事件时间和处理时间之间存在相关性。此外，由于上述原因，在 STREAMING 模式下，事件时间的1h往往可以几乎是1h的处理时间，也就是挂钟时间。所以使用处理时间可以用于早期（不完全）发射，给出预期结果的提示。\n在批处理世界中，这种相关性并不存在，因为在批处理世界中，输入的数据集是静态的，是预先知道的。鉴于此，在 BATCH 模式中，我们允许用户请求当前的处理时间，并注册处理时间计时器，但与事件时间的情况一样，所有的计时器都要在输入结束时发射。\n在概念上，我们可以想象，在作业执行过程中，处理时间不会提前，当整个输入处理完毕后，我们会快进到时间结束。\n故障恢复 在 STREAMING 执行模式下，Flink 使用检查点进行故障恢复。请看一下检查点文档，了解关于这个和如何配置它的实践文档。关于通过状态快照进行容错，也有一个比较入门的章节，从更高的层面解释了这些概念。\nCheckpointing 用于故障恢复的特点之一是，Flink 在发生故障时，会从检查点重新启动所有正在运行的任务。这可能比我们在 BATCH 模式下所要做的事情更昂贵（如下文所解释），这也是如果你的任务允许的话应该使用 BATCH 执行模式的原因之一。\n在 BATCH 执行模式下，Flink 会尝试并回溯到之前的处理阶段，对于这些阶段，仍然有中间结果。潜在地，只有失败的任务（或它们在图中的前辈）才需要重新启动，与从检查点重新启动所有任务相比，可以提高作业的处理效率和整体处理时间。\n重要的考虑因素 与经典的 STREAMING 执行模式相比，在 BATCH 模式下，有些东西可能无法按照预期工作。一些功能的工作方式会略有不同，而其他功能则不支持。\nBATCH 模式下的行为变化。\n \u0026ldquo;滚动\u0026quot;操作，如 reduce() 或 sum()，会对 STREAMING 模式下每一条新记录发出增量更新。在 BATCH 模式下，这些操作不是\u0026quot;滚动\u0026rdquo;。它们只发出最终结果。  BATCH 模式下不支持的:\n Checkpointing 和任何依赖于 checkpointing 的操作都不工作。 广播状态 迭代  自定义操作符应谨慎执行，否则可能会有不恰当的行为。更多细节请参见下面的补充说明。\n检查点 如上所述，批处理程序的故障恢复不使用检查点。\n重要的是要记住，因为没有检查点，某些功能如 CheckpointListener，以及因此，Kafka 的 EXACTLY_ONCE 模式或 StreamingFileSink 的 OnCheckpointRollingPolicy 将无法工作。如果你需要一个在 BATCH 模式下工作的事务型接收器，请确保它使用 FLIP-143 中提出的统一接收器 API。\n你仍然可以使用所有的状态原语，只是用于故障恢复的机制会有所不同。\n广播状态 引入这个特性是为了让用户实现这样的用例：一个\u0026quot;控制\u0026quot;流需要被广播到所有下游任务，而广播的元素，例如规则，需要应用到另一个流的所有输入元素。\n在这种模式下，Flink 不提供关于读取输入的顺序的保证。像上面这样的用例在流媒体世界中是有意义的，因为在这个世界中，作业预计会运行很长时间，而输入数据是事先不知道的。在这些设置中，需求可能会随着时间的推移而改变，这取决于输入的数据。\n但在批处理世界中，我们认为这种用例没有太大意义，因为输入（包括元素和控制流）是静态的，而且是预先知道的。\n我们计划在未来为BATCH处理支持这种模式的变化，即完全先处理广播端。\n编写自定义操作符  注意：自定义操作符是 Apache Flink 的一种高级使用模式。对于大多数的使用情况，可以考虑使用(keyed-)过程函数来代替。\n 在编写自定义操作符时，记住 BATCH 执行模式的假设是很重要的。否则，一个在 STREAMING 模式下运行良好的操作符可能会在 BATCH 模式下产生错误的结果。操作符永远不会被限定在一个特定的键上，这意味着他们看到了 Flink 试图利用的 BATCH 处理的一些属性。\n首先你不应该在一个操作符内缓存最后看到的水印。在 BATCH 模式下，我们会逐个键处理记录。因此，水印会在每个键之间从 MAX_VALUE 切换到 MIN_VALUE。你不应该认为水印在一个操作符中总是上升的。出于同样的原因，定时器将首先按键的顺序发射，然后按每个键内的时间戳顺序发射。此外，不支持手动更改键的操作。\n","permalink":"https://ohmyweekly.github.io/notes/2020-12-10-execution-mode/","tags":["Flink","Flink 官方文档"],"title":"执行模式(批/流)"},{"categories":["Flink"],"contents":"应用构件 Stateful Functions 为构建事件驱动应用程序提供了一个框架。在这里，我们将解释 Stateful Function 架构的重要方面。\n事件输入 有状态函数应用正好坐在事件驱动的领域，所以自然要从把事件摄入系统开始。\n在有状态函数中，将记录摄入系统的组件称为事件入口。这可以是任何东西，从 Kafka 主题，到 messsage 队列，再到 http 请求 - 任何能够将数据引入系统并触发初始函数开始计算的东西。\n有状态函数 该图的核心是命名的有状态函数。\n把这些函数看作是你的服务的构件。它们可以任意地相互发送消息，这也是这个框架摆脱传统的流处理观点的一种方式。这些函数可以以任意的、可能是循环的、甚至是往返的方式相互通信，而不是建立一个静态的数据流 DAG。\n如果你熟悉 actor 编程，这在组件之间动态消息的能力上确实有某些相似之处。然而，也有一些显著的区别。\n持续状态 首先是所有函数都有本地嵌入的状态，即所谓的持久化状态。\nApache Flink 的核心优势之一就是它能够提供容错的本地状态。当在一个函数内部，当它在执行一些计算时，你总是在本地变量中处理本地状态。\n容错 对于状态和消息传递，Stateful Functions 能够提供用户从现代数据处理框架中期望的精确的一次保证。\n在失败的情况下，整个世界的状态（包括持久化的状态和消息）都会被回滚，以模拟完全无故障的执行。\n这些保证是在不需要数据库的情况下提供的，相反，Stateful Function 利用了 Apache Flink 的成熟快照机制。\n事件出口 最后，应用程序可以通过事件出口向外部系统输出数据。\n当然，函数执行任意计算，可以随心所欲，这包括进行 RPC 调用和连接到其他系统。通过使用事件出口，应用程序可以利用建立在 Apache Flink 连接器生态系统之上的预建集成。\n","permalink":"https://ohmyweekly.github.io/notes/2020-12-02-application-building-block/","tags":["Flink","Flink 官方文档"],"title":"Application Building Blocks"},{"categories":["Flink"],"contents":"逻辑函数 有状态函数是以逻辑方式分配的，这意味着系统可以用有限的资源支持无限制的实例数量。逻辑实例在不被主动调用时不使用CPU、内存或线程，所以理论上没有可以创建的实例数量上限。我们鼓励用户根据对其应用最合理的情况，尽可能地对其应用进行细化建模，而不是围绕资源限制来设计应用。\n函数地址 在本地环境中，对象的地址和对它的引用是一样的。但是在有状态函数应用程序中，函数实例是虚拟的，它们的运行位置不会暴露给用户。相反，地址是用来引用系统中特定的有状态函数的。\n一个地址由两个组件组成，一个是 FunctionType，一个是 ID。函数类型类似于面向对象语言中的类，它声明了地址所引用的函数类型。ID 是一个主键，它将函数调用的范围限定在函数类型的一个特定实例上。\n当一个函数被调用时，所有的操作\u0026ndash;包括对持久化状态的读和写 - 都会被限定在当前地址上。\n例如，想象有一个有状态函数应用程序来跟踪仓库的库存。一个可能的实现可以包括一个库存函数，它可以跟踪一个特定物品的库存单位数量；这将是函数类型。然后，仓库管理的每个 SKU 都会有一个该类型的逻辑实例。如果是服装，可能会有一个衬衫的实例和另一个裤子的实例；\u0026ldquo;衬衫\u0026quot;和 \u0026ldquo;裤子\u0026quot;将是两个 ID。每个实例都可以独立地进行交互和消息传递。应用程序可以根据库存中物品的类型自由创建实例。\n函数生命周期 逻辑函数既不被创建也不被销毁，而是在应用程序的整个生命周期中始终存在。当应用程序启动时，框架的每个并行工作者将为每个函数类型创建一个物理对象。这个对象将用于执行该类型的所有逻辑实例，该类型的逻辑实例由该特定的 worker 运行。第一次向一个地址发送消息时，它将像该实例一直存在一样，其持久化状态为空。\n清除一个类型的所有持久化状态与销毁它是一样的。如果一个实例没有状态，也没有主动运行，那么它就不占用 CPU，不占用线程，也不占用内存。\n一个实例在其一个或多个持久化值中存储了数据，它只占用存储该数据所需的资源。状态存储由 Apache Flink 运行时管理，并存储在配置的状态后端。\n","permalink":"https://ohmyweekly.github.io/notes/2020-12-02-logical-functions/","tags":["Flink","Flink 官方文档"],"title":"Logical Functions"},{"categories":["Flink"],"contents":"Python 演练 Stateful Functions 为构建健壮的、有状态的事件驱动的应用程序提供了一个平台。它提供了对状态和时间的精细控制，这使得高级系统的实现成为可能。在本步骤指南中，您将学习如何使用 Stateful Functions API 构建有状态的应用程序。\n你要构建什么？ 就像软件中所有伟大的介绍一样，这个演练将从开头开始：打招呼。该应用程序将运行一个简单的函数，该函数将接受一个请求并以问候语进行响应。它不会试图涵盖所有复杂的应用程序开发，而是专注于构建一个有状态的函数 - 这是你实现业务逻辑的地方。\n先决条件 这个演练假设您对 Python 有一定的了解，但即使您来自不同的编程语言，您也应该能够跟上。\n帮助，我卡住了 如果你被卡住了，请查看社区支持资源。特别是 Apache Flink 的用户邮件列表，一直被认为是 Apache 项目中最活跃的一个，也是快速获得帮助的好方法。\n如何跟进 如果你想跟上，你需要一台装有 Python 3 以及 Docker 的电脑。\n 注意：为了简洁起见，本演练中的每个代码块可能不包含完整的周边类。完整的代码可以在本页底部找到。\n 你可以通过点击这里下载一个包含骨架项目的 zip 文件。\n解压包后，你会发现一些文件。这些文件包括 dockerfiles 和数据生成器，用于在本地自包含环境中运行此演练。\n$ tree statefun-walkthrough statefun-walkthrough ├── Dockerfile ├── docker-compose.yml ├── generator │ ├── Dockerfile │ ├── event-generator.py │ └── messages_pb2.py ├── greeter │ ├── Dockerfile │ ├── greeter.py │ ├── messages.proto │ ├── messages_pb2.py │ └── requirements.txt └── module.yaml 从事件开始 Stateful Functions 是一个事件驱动的系统，所以开发从定义我们的事件开始。问候者应用程序将使用协议缓冲区定义其事件。当一个特定用户的问候请求被摄入时，它将被路由到相应的函数。响应将返回一个适当的问候。第三种类型，SeenCount，是一个实用类，后期将用于帮助管理用户到目前为止被看到的次数。\nsyntax = \u0026quot;proto3\u0026quot;; package example; // External request sent by a user who wants to be greeted message GreetRequest { // The name of the user to greet string name = 1; } // A customized response sent to the user message GreetResponse { // The name of the user being greeted string name = 1; // The users customized greeting string greeting = 2; } // An internal message used to store state message SeenCount { // The number of times a users has been seen so far int64 seen = 1; } 我们的第一个函数 在底层，消息是使用有状态的函数来处理的，也就是任何绑定到 StatefulFunction 运行时的两个参数函数。函数用 @function.bind 装饰器绑定到运行时。当绑定一个函数时，它会被注解为一个函数类型。这是在向这个函数发送消息时用来引用它的名称。\n当你打开文件 greeter/greeter.py 时，你应该看到以下代码。\nfrom statefun import StatefulFunctions functions = StatefulFunctions() @functions.bind(\u0026#34;example/greeter\u0026#34;) def greet(context, greet_request): pass 一个有状态函数需要两个参数，即上下文和消息。上下文提供了对有状态函数运行时功能的访问，如状态管理和消息传递。您将在本演练中探索其中的一些功能。\n另一个参数是传递给这个函数的输入消息。默认情况下，消息是以 protobuf Any 的形式传递的。如果一个函数只接受一个已知的类型，你可以使用 Python 3 类型语法覆盖消息类型。这样您就不需要对消息进行拆包或检查类型。\nfrom messages_pb2 import GreetRequest from statefun import StatefulFunctions functions = StatefulFunctions() @functions.bind(\u0026#34;example/greeter\u0026#34;) def greet(context, greet_request: GreetRequest): pass 发送回复 有状态函数接受消息，也可以将消息发送出去。消息可以被发送到其他函数，以及外部系统（或出口）。\n一个流行的外部系统是 Apache Kafka。第一步，让我们更新 greeter/greeter.py 中的函数，通过向 Kafka 主题发送问候语来响应每个输入。\nfrom messages_pb2 import GreetRequest, GreetResponse from statefun import StatefulFunctions functions = StatefulFunctions() @functions.bind(\u0026#34;example/greeter\u0026#34;) def greet(context, greet_request: GreetRequest): response = GreetResponse() response.name = greet_request.name response.greeting = \u0026#34;Hello {}\u0026#34;.format(greet_request.name) egress_message = kafka_egress_record(topic=\u0026#34;greetings\u0026#34;, key=greet_request.name, value=response) context.pack_and_send_egress(\u0026#34;example/greets\u0026#34;, egress_message) 对于每条消息，都会构造一个响应，并发送到一个名为 greetings 的 Kafka 主题，该主题按名称分区。egress_message 被发送到一个名为 example/greets 的出口。这个标识符指向一个特定的 Kafka 集群，并在下面的部署中进行配置。\n一个有状态的 Hello 这是一个很好的开端，但并没有展现出有状态函数的真正威力 - 与状态一起工作。假设你想根据每个用户发送请求的次数，为他们生成个性化的响应。\ndef compute_greeting(name, seen): \u0026#34;\u0026#34;\u0026#34; Compute a personalized greeting, based on the number of times this @name had been seen before. \u0026#34;\u0026#34;\u0026#34; templates = [\u0026#34;\u0026#34;, \u0026#34;Welcome %s\u0026#34;, \u0026#34;Nice to see you again %s\u0026#34;, \u0026#34;Third time is a charm %s\u0026#34;] if seen \u0026lt; len(templates): greeting = templates[seen] % name else: greeting = \u0026#34;Nice to see you at the %d-nth time %s!\u0026#34; % (seen, name) response = GreetResponse() response.name = name response.greeting = greeting return response 为了\u0026quot;记住\u0026quot;多条问候信息，你需要将一个持久化的值域（ seen_count ）关联到 Greet 函数。对于每个用户，函数现在可以跟踪他们被看到的次数。\n@functions.bind(\u0026#34;example/greeter\u0026#34;) def greet(context, greet_request: GreetRequest): state = context.state(\u0026#39;seen_count\u0026#39;).unpack(SeenCount) if not state: state = SeenCount() state.seen = 1 else: state.seen += 1 context.state(\u0026#39;seen_count\u0026#39;).pack(state) response = compute_greeting(greet_request.name, state.seen) egress_message = kafka_egress_record(topic=\u0026#34;greetings\u0026#34;, key=greet_request.name, value=response) context.pack_and_send_egress(\u0026#34;example/greets\u0026#34;, egress_message) 状态 seen_count 始终是当前名称的范围，因此它可以独立地跟踪每个用户。\n连接在一起 有状态的 Function 应用程序使用 http 与 Apache Flink 运行时进行通信。Python SDK 提供了一个 RequestReplyHandler，它可以基于 RESTful HTTP POSTS 自动分配函数调用。RequestReplyHandler 可以使用任何 HTTP 框架暴露。\n一个流行的 Python web 框架是 Flask。它可以用来快速、轻松地将应用程序暴露给 Apache Flink 运行时。\nfrom statefun import StatefulFunctions from statefun import RequestReplyHandler functions = StatefulFunctions() @functions.bind(\u0026#34;example/greeter\u0026#34;) def greeter(context, message: GreetRequest): pass handler = RequestReplyHandler(functions) # Serve the endpoint from flask import request from flask import make_response from flask import Flask app = Flask(__name__) @app.route(\u0026#39;/statefun\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def handle(): response_data = handler(request.data) response = make_response(response_data) response.headers.set(\u0026#39;Content-Type\u0026#39;, \u0026#39;application/octet-stream\u0026#39;) return response if __name__ == \u0026#34;__main__\u0026#34;: app.run() 配置运行时 有状态函数运行时通过向 Flask 服务器进行 http 调用来向 greeter 函数发出请求。要做到这一点，它需要知道它可以使用什么端点来到达服务器。这也是配置我们连接到输入和输出 Kafka 主题的好时机。配置在一个名为 module.yaml 的文件中。\nversion:\u0026#34;1.0\u0026#34;module:meta:type:remotespec:functions:- function:meta:kind:httptype:example/greeterspec:endpoint:http://python-worker:8000/statefunstates:- seen_countmaxNumBatchRequests:500timeout:2miningresses:- ingress:meta:type:statefun.kafka.io/routable-protobuf-ingressid:example/namesspec:address:kafka-broker:9092consumerGroupId:my-group-idtopics:- topic:namestypeUrl:com.googleapis/example.GreetRequesttargets:- example/greeteregresses:- egress:meta:type:statefun.kafka.io/generic-egressid:example/greetsspec:address:kafka-broker:9092deliverySemantic:type:exactly-oncetransactionTimeoutMillis:100000这个配置做了一些有趣的事情。\n首先是声明我们的函数 example/greeter。它包括它可以到达的端点以及函数可以访问的状态。\ningress 是将 GreetRequest 消息路由到函数的输入 Kafka 主题。除了 broker 地址和消费者组等基本属性，它还包含一个目标列表。这些是每个消息将被发送到的函数。\n出口是输出的 Kafka 集群。它包含 broker 特定的配置，但允许每个消息路由到任何主题。\n部署 现在已经构建了 greeter 应用程序，是时候部署了。部署 Stateful Function 应用程序最简单的方法是使用社区提供的基础映像并加载你的模块。基础镜像提供了 Stateful Function 运行时，它将使用提供的 module.yaml 来为这个特定的工作进行配置。这可以在根目录下的 Docker 文件中找到。\nFROM flink-statefun:2.2.0 RUN mkdir -p /opt/statefun/modules/greeter ADD module.yaml /opt/statefun/modules/greeter 现在您可以使用提供的 Docker 设置在本地运行此应用程序。\n$ docker-compose up -d 那么，要想在行动中看到例子，就看看话题问候出来的内容。\ndocker-compose logs -f event-generator 想更进一步？ 这个 Greeter 永远不会忘记一个用户。试着修改这个函数，使它能够为任何没有与系统交互的用户花超过60秒的时间重置 seen_count。\n查看 Python SDK 页面以获得更多关于如何实现这一功能的信息。\n完整应用 from messages_pb2 import SeenCount, GreetRequest, GreetResponse from statefun import StatefulFunctions from statefun import RequestReplyHandler from statefun import kafka_egress_record functions = StatefulFunctions() @functions.bind(\u0026#34;example/greeter\u0026#34;) def greet(context, greet_request: GreetRequest): state = context.state(\u0026#39;seen_count\u0026#39;).unpack(SeenCount) if not state: state = SeenCount() state.seen = 1 else: state.seen += 1 context.state(\u0026#39;seen_count\u0026#39;).pack(state) response = compute_greeting(greet_request.name, state.seen) egress_message = kafka_egress_record(topic=\u0026#34;greetings\u0026#34;, key=greet_request.name, value=response) context.pack_and_send_egress(\u0026#34;example/greets\u0026#34;, egress_message) def compute_greeting(name, seen): \u0026#34;\u0026#34;\u0026#34; Compute a personalized greeting, based on the number of times this @name had been seen before. \u0026#34;\u0026#34;\u0026#34; templates = [\u0026#34;\u0026#34;, \u0026#34;Welcome %s\u0026#34;, \u0026#34;Nice to see you again %s\u0026#34;, \u0026#34;Third time is a charm %s\u0026#34;] if seen \u0026lt; len(templates): greeting = templates[seen] % name else: greeting = \u0026#34;Nice to see you at the %d-nth time %s!\u0026#34; % (seen, name) response = GreetResponse() response.name = name response.greeting = greeting return response handler = RequestReplyHandler(functions) # # Serve the endpoint # from flask import request from flask import make_response from flask import Flask app = Flask(__name__) @app.route(\u0026#39;/statefun\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def handle(): response_data = handler(request.data) response = make_response(response_data) response.headers.set(\u0026#39;Content-Type\u0026#39;, \u0026#39;application/octet-stream\u0026#39;) return response if __name__ == \u0026#34;__main__\u0026#34;: app.run() ","permalink":"https://ohmyweekly.github.io/notes/2020-12-02-python-walkthrough/","tags":["Flink","Flink 官方文档"],"title":"Python 演练"},{"categories":["Flink"],"contents":"SDK 有状态函数应用程序由一个或多个模块组成。一个模块是一个由运行时加载的函数捆绑，并提供给消息。来自所有加载模块的函数都是多路复用的，并且可以自由地相互发送消息。\n有状态函数支持两种类型的模块。远程模块和嵌入式模块。\n远程模块 远程模块作为 Apache Flink® 运行时的外部进程运行；在同一容器中，作为 sidecar，使用无服务器平台或其他外部位置。这种模块类型可以支持任何数量的语言 SDK。远程模块通过 YAML 配置文件在系统中注册。\n技术指标 一个远程模块配置由一个元部分和一个规范部分组成。meta 包含了模块的辅助信息；而 spec 则描述了模块中包含的功能并定义了它们的持久值。\n定义函数 module.spec.functions 声明了一个由远程模块实现的函数对象列表。一个函数通过一些属性来描述。\n function.meta.kind  用于与远程功能通信的协议。 所支持的值 - http   function.meta.type  函数类型那个, 被定义为 \u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt;。   function.spec.endpoint  函数可到达的端点。 所支持的 schemes: http, https. 使用 http+unix 或 https+unix 方案支持通过 UNIX 域套接字进行传输。 当使用 UNIX 域套接字时，端点格式是: http+unix://\u0026lt;socket-file-path\u0026gt;/\u0026lt;serve-url-path\u0026gt;。例如, http+unix:///uds.sock/path/of/url。   function.spec.states  在远程函数中声明的持久化值的列表 每个条目由 name 属性和可选的 expireAfter 属性组成。 expireAfter 的默认值为 0，表示状态过期被禁用。   function.spec.maxNumBatchRequests  在调用系统背压之前，一个函数可以处理的特定地址的最大记录数。 默认值：1000   function.spec.timeout  运行时在失败前等待远程函数返回的最长时间。这涵盖了整个调用过程，包括连接到函数端点、编写请求、函数处理和读取响应。 默认值：1分钟   function.spec.connectTimeout  运行时等待连接到远程函数端点的最长时间。 默认值：10秒。   function.spec.readTimeout  运行时等待单个读IO操作的最大时间，如读取调用响应。 默认值：10秒。   function.spec.writeTimeout  运行时等待单个写IO操作的最大时间，比如写调用请求。 默认值：10秒。    完整示例 version:\u0026#34;2.0\u0026#34;module:meta:type:remotespec:functions:- function:meta:kind:httptype:example/greeterspec:endpoint:http://\u0026lt;host-name\u0026gt;/statefunstates:- name:seen_countexpireAfter:5minmaxNumBatchRequests:500timeout:2min嵌入式模块 嵌入式模块与 Apache Flink® 运行时共存，并嵌入其中。\n这种模块类型只支持基于 JVM 的语言，并通过实现 StatefulFunctionModule 接口来定义。嵌入模块提供了一个单一的配置方法，有状态的函数根据其函数类型与系统绑定。运行时配置可以通过 globalConfiguration 来实现，它是应用程序 flink-conf.yaml 中前缀 statefun.module.global-config 下的所有配置以及以 --key value 形式传递的任何命令行参数的联合。\npackage org.apache.flink.statefun.docs; import java.util.Map; import org.apache.flink.statefun.sdk.spi.StatefulFunctionModule; public class BasicFunctionModule implements StatefulFunctionModule { public void configure(Map\u0026lt;String, String\u0026gt; globalConfiguration, Binder binder) { // Declare the user function and bind it to its type \tbinder.bindFunctionProvider(FnWithDependency.TYPE, new CustomProvider()); // Stateful functions that do not require any configuration \t// can declare their provider using java 8 lambda syntax \tbinder.bindFunctionProvider(Identifiers.HELLO_TYPE, unused -\u0026gt; new FnHelloWorld()); } } 嵌入式模块利用 Java 的服务提供者接口（SPI）进行发现。这意味着每个 JAR 都应该在 META_INF/services 资源目录下包含一个文件 org.apache.flink.statefun.sdk.spi.StatefulFunctionModule，该文件列出了它提供的所有可用模块。\norg.apache.flink.statefun.docs.BasicFunctionModule ","permalink":"https://ohmyweekly.github.io/notes/2020-12-02-sdk/","tags":["Flink","Flink 官方文档"],"title":"Sdk"},{"categories":["Flink"],"contents":"分布式架构 一个有状态的 Functions 部署是由几个组件交互在一起组成的。在这里，我们将描述这些组件及其相互之间的关系和 Apache Flink 运行时。\n高层视图 一个 Stateful Functions 部署由一组 Apache Flink Stateful Functions 进程和可选的执行远程函数的各种部署组成。\nFlink Worker 进程（TaskManagers）从入口系统（Kafka、Kinesis 等）接收事件并将其路由到目标函数。它们调用函数并将产生的消息路由到下一个各自的目标函数。指定用于出口的消息被写入出口系统（同样，Kafka、Kinesis\u0026hellip;）。\n组成部分 繁重的工作由 Apache Flink 进程完成，它管理状态，处理消息传递，并调用有状态的函数。Flink 集群通常由一个主进程和多个工作者（TaskManagers）组成。\n除了 Apache Flink 进程，完整的部署还需要 ZooKeeper（用于主站故障转移）和批量存储（S3、HDFS、NAS、GCS、Azure Blob Store等）来存储 Flink 的检查点。而部署时不需要数据库，Flink 进程也不需要持久化卷。\n逻辑同位，物理分离 许多流处理器的一个核心原则是，应用逻辑和应用状态必须是共位的。这种方法是它们开箱即用的一致性的基础。Stateful Functions 采用了一种独特的方法，在逻辑上将状态和计算共置，但允许在物理上将它们分开。\n  逻辑上的共置。消息传递、状态访问/更新和函数调用被紧密地管理在一起，与 Flink 的 DataStream API 的方式相同。状态按键分片，消息按键路由到状态。每个 key 一次有一个写入器，也是对函数调用进行调度。\n  物理分离。函数可以远程执行，消息和状态访问作为调用请求的一部分。这样，函数就可以像无状态进程一样独立管理。\n  函数的部署风格 有状态的函数本身可以以不同的方式部署，这些方式可以相互交换某些特性：一方面是松散的耦合和独立的扩展，另一方面是性能开销。每个函数模块可以是不同的种类，所以有些函数可以远程运行，而有些函数可以嵌入式运行。\n远程函数 远程功能采用上述物理分离的原则，同时保持逻辑上的同位。状态/消息层（即 Flink 进程）和功能层是独立部署、管理和扩展的。\n功能调用通过 HTTP/gRPC 协议发生，并通过服务将调用请求路由到任何可用的端点，例如 Kubernetes（负载平衡）服务、Lambda 的 AWS 请求网关等。因为调用是自足的（包含消息、状态、访问计时器等），所以目标函数可以像任何无状态的应用程序一样对待。\n详情请参考 Python SDK 和远程模块的文档。\n共置函数 部署函数的另一种方式是与 Flink JVM 进程共处。在这样的设置中，每个 Flink TaskManager 将与坐在\u0026quot;旁边\u0026quot;的一个 Function 进程对话。一种常见的方式是使用 Kubernetes 这样的系统，部署由 Flink 容器和 Function 侧车容器组成的 pod；两者通过 pod-local 网络进行通信。\n这种模式支持不同的语言，同时避免了要通过 Service/LoadBalancer 来路由调用，但它不能独立扩展状态和计算部分。\n这种部署方式类似于 Flink 的 Table API 和 API Beam 的可移植层部署和执行非 JVM 函数的方式。\n嵌入式函数 嵌入式函数类似于 Stateful Functions 1.0 的执行模式，也类似于 Flink 的 Java/Scala 流处理 API。函数在 JVM 中运行，直接调用消息和状态访问。这是最有性能的方式，不过代价是只支持 JVM 语言。函数的更新意味着更新 Flink 集群。\n按照数据库的类比，嵌入式 Functions 有点像存储程序，但方式更有原则。这里的函数是实现标准接口的普通 Java/Scala/Kotlin 函数，可以在任何 IDE 中开发/测试。\n","permalink":"https://ohmyweekly.github.io/notes/2020-12-02-distributed-architecture/","tags":["Flink","Flink 官方文档"],"title":"分布式架构"},{"categories":["PySpark"],"contents":"Koalas 是一个开源项目，它为 pandas 提供了一个 drop-in 的替代品，可以高效地扩展到数百个工人节点，用于日常的数据科学和机器学习。自去年首次推出以来，经过一年多的开发，Koalas 1.0 已经发布。\npandas 是数据科学家中常用的 Python 包，但它并不能扩展到大数据。当他们的数据变得庞大时，他们必须从一开始就选择和学习另一个系统，如 Apache Spark，以采用和转换他们现有的工作负载。 Koalas 通过提供 pandas 等效的 API 来填补这个空白，这些 API 可以在 Apache Spark 上工作。其中很多在之前的博文中已经介绍过，其中还包括使用 Koalas 时的最佳实践。\nKoalas 不仅对 pandas 用户有用，对 PySpark 用户也很有用，因为 Koalas 支持很多 PySpark 难以实现的功能。例如，Spark 用户可以通过 Koalas 绘图 API 直接从 PySpark DataFrame 中绘制数据，类似于 pandas。PySpark DataFrame 更符合 SQL 标准，而 Koalas DataFrame 更接近 Python 本身，这为在某些情况下使用 Python 提供了更直观的工作方式。在 Koalas 文档中，有各种 pandas 对应的 API 实现。\n在这篇博文中，我们重点介绍 PySpark 用户如何利用自己的知识和 PySpark 与 Koalas 之间的原生交互，更快地编写代码。我们包含了许多自带的例子，如果你安装了带 Koalas 的 Spark，或者你正在使用 Databricks Runtime，你可以运行这些例子。从 Databricks Runtime 7.1 开始，Koalas 就被打包在一起，所以您无需手动安装就可以运行。\nKoalas 和 PySpark DataFrames 在深究之前，我们先来看看 Koalas 和 PySpark DataFrames 的一般区别。\n从外观上看，它们是不同的。Koalas DataFrames 无缝地沿用了 pandas DataFrames 的结构，并在底层下实现了一个索引/标识符。而 PySpark DataFrame 则更趋向于符合关系型数据库中的关系/表，并且没有唯一的行标识符。\n在内部，Koalas DataFrames 是建立在 PySpark DataFrames 上的。Koalas 将 pandas APIs 翻译成 Spark SQL 的逻辑计划。该计划由复杂而强大的 Spark SQL 引擎优化和执行，Spark 社区不断对其进行改进。Koalas 还沿用 Spark 的懒惰评估语义，以实现性能的最大化。为了实现 pandas DataFrame 结构和 pandas 丰富的 API，需要隐式排序，Koalas DataFrames 的内部元数据表示 pandas 等价的索引和列标签映射到 PySpark DataFrame 中的列。\n即使 Koalas 利用 PySpark 作为执行引擎，但与 PySpark 相比，你仍然可能面临轻微的性能下降。正如在 Virgin Hyperloop One 的迁移经验中所讨论的，主要原因通常是:\n 使用了默认索引。构建默认索引的开销取决于数据大小、集群组成等。因此，总是希望避免使用默认索引。关于这一点将在下面的其他章节中详细讨论。 PySpark 和 pandas 中的一些 API 名称相同，但语义不同。例如，Koalas DataFrame 和 PySpark DataFrame 都有 count API。前者统计每列/行的非 NA/null 条目数，后者统计检索到的行数，包括包含 null 的行。  \u0026gt;\u0026gt;\u0026gt; ks.DataFrame({\u0026#39;a\u0026#39;: [1, 2, 3], \u0026#39;b\u0026#39;: [4, 5, 6]}).count() a 3 b 3 \u0026gt;\u0026gt;\u0026gt; spark.createDataFrame( ... [[1, 4], [2, 5], [3, 6]], schema=[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;]).count() 3 从 PySpark DataFrames 转换到 PySpark DataFrames 对于一个 PySpark 用户来说，很高兴知道你可以很容易地在 Koalas DataFrame 和 PySpark DataFrame 之间来回切换，以及在底层发生了什么，这样你就不需要害怕进入 Koalas 世界，在 Spark 上应用高扩展性的 pandas API。\n to_koalas()  当导入 Koalas 包时，它会自动将 to_koalas()方法附加到 PySpark DataFrames 中。你可以简单地使用这个方法将 PySpark DataFrames 转换为 Koalas DataFrames。\n假设你有一个 PySpark DataFrame。\n\u0026gt;\u0026gt;\u0026gt; sdf = spark.createDataFrame([(1, 10.0, \u0026#39;a\u0026#39;), (2, 20.0, \u0026#39;b\u0026#39;), (3, 30.0, \u0026#39;c\u0026#39;)],schema=[\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;z\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; sdf.show() +---+----+---+ | x| y| z| +---+----+---+ | 1|10.0| a| | 2|20.0| b| | 3|30.0| c| +---+----+---+ 首先，导入 Koalas 包。传统上使用 ks 作为包的别名。\n\u0026gt;\u0026gt;\u0026gt; import databricks.koalas as ks 如上所述，用 to_koalas()方法将 Spark DataFrame 转换为 Koalas DataFrame。\n\u0026gt;\u0026gt;\u0026gt; kdf = sdf.to_koalas() \u0026gt;\u0026gt;\u0026gt; kdf x y z 0 1 10.0 a 1 2 20.0 b 2 3 30.0 c kdf 是一个由 PySpark DataFrame 创建的 Koalas DataFrame。当真正需要数据时，计算会被懒惰地执行，例如显示或存储计算的数据，与 PySpark 相同。\n to_spark()  接下来，你还应该知道如何从 Koalas 回到 PySpark DataFrame。你可以在 Koalas DataFrame 上使用 to_spark()方法。\n\u0026gt;\u0026gt;\u0026gt; sdf_from_kdf = kdf.to_spark() \u0026gt;\u0026gt;\u0026gt; sdf_from_kdf.show() +---+----+---+ | x| y| z| +---+----+---+ | 1|10.0| a| | 2|20.0| b| | 3|30.0| c| +---+----+---+ 现在你又有了一个 PySpark DataFrame。请注意，现在已经没有 Koalas DataFrame 中包含的索引列了。下面将讨论处理索引的最佳实践。\n索引和 index_col 如上图所示，Koalas 内部管理了几列作为 \u0026ldquo;索引 \u0026ldquo;列，以表示 pandas 的索引。这些 \u0026ldquo;索引 \u0026ldquo;列用于通过 loc/iloc 索引器访问行，或者用于 sort_index()方法中，而不指定排序键列，甚至用于结合两个以上 DataFrame 或 Series 的操作时匹配相应的行，例如 df1+df2，等等。\n如果 PySpark DataFrame 中已经有这样的列，可以使用 index_col 参数来指定索引列。\n\u0026gt;\u0026gt;\u0026gt; kdf_with_index_col = sdf.to_koalas(index_col=\u0026#39;x\u0026#39;) # or index_col=[\u0026#39;x\u0026#39;] \u0026gt;\u0026gt;\u0026gt; kdf_with_index_col y z x 1 10.0 a 2 20.0 b 3 30.0 c 这时，列 x 不被视为常规列之一，而是索引。\n如果你有多个列作为索引，你可以传递列名列表。\n\u0026gt;\u0026gt;\u0026gt; sdf.to_koalas(index_col=[\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;]) z x y 1 10.0 a 2 20.0 b 3 30.0 c 当回到 PySpark DataFrame 时，你还可以使用 index_col 参数来保存索引列。\n\u0026gt;\u0026gt;\u0026gt; kdf_with_index_col.to_spark(index_col=\u0026#39;index\u0026#39;).show() # or index_col=[\u0026#39;index\u0026#39;] +-----+----+---+ |index| y| z| +-----+----+---+ | 1|10.0| a| | 2|20.0| b| | 3|30.0| c| +-----+----+---+ 否则，就会失去指数，如下图。\n\u0026gt;\u0026gt;\u0026gt; kdf_with_index_col.to_spark().show() +----+---+ | y| z| +----+---+ |10.0| a| |20.0| b| |30.0| c| +----+---+ 列名的数量应与索引列的数量一致。\n\u0026gt;\u0026gt;\u0026gt; kdf.to_spark(index_col=[\u0026#39;index1\u0026#39;, \u0026#39;index2\u0026#39;]).show() Traceback (most recent call last): ... ValueError: length of index columns is 1; however, the length of the given \u0026#39;index_col\u0026#39; is 2. 默认索引 正如你所看到的，如果你不指定 index_col 参数，就会创建一个新的列作为索引。\n\u0026gt;\u0026gt;\u0026gt; sdf.to_koalas() x y z 0 1 10.0 a 1 2 20.0 b 2 3 30.0 c 列从哪里来？\n答案是 \u0026ldquo;默认索引\u0026rdquo;。如果没有指定 index_col 参数，Koalas 会自动将一列作为索引附加到 DataFrame 中。有三种类型的默认索引。\u0026ldquo;sequence\u0026rdquo;、\u0026ldquo;distributed-sequence \u0026ldquo;和 \u0026ldquo;distributed\u0026rdquo;。每种类型都有其独特的特点和局限性，比如性能惩罚。为了减少性能开销，强烈建议在从 PySpark DataFrame 转换时通过 index_col 指定索引列。\n当 Koalas 不知道哪一列是用来做索引时，也会使用默认索引。例如，reset_index()没有任何参数，它试图将所有的索引数据转换为常规列，并重新创建一个索引。\n\u0026gt;\u0026gt;\u0026gt; kdf_with_index_col.reset_index() x y z 0 1 10.0 a 1 2 20.0 b 2 3 30.0 c 你可以通过设置 Koalas 选项 \u0026ldquo;compute.default_index_type\u0026rdquo; 来改变默认的索引类型。\nks.set_option(\u0026#39;compute.default_index_type\u0026#39;, \u0026#39;sequence\u0026#39;) 或\nks.options.compute.default_index_type = \u0026#39;sequence\u0026#39; 顺序型 目前 Koalas 中默认使用 \u0026ldquo;序列 \u0026ldquo;类型，因为它像 pandas 一样保证了索引的连续递增。但是，它内部使用了一个非分区窗口函数，这意味着所有的数据都需要收集到一个节点中。如果节点的内存不足，性能会明显下降，或者出现 OutOfMemoryError。\n\u0026gt;\u0026gt;\u0026gt; ks.set_option(\u0026#39;compute.default_index_type\u0026#39;, \u0026#39;sequence\u0026#39;) \u0026gt;\u0026gt;\u0026gt; spark.range(5).to_koalas() id 0 0 1 1 2 2 3 3 4 4 分散式 当使用 \u0026ldquo;分布式-序列 \u0026ldquo;索引时，性能惩罚没有 \u0026ldquo;序列 \u0026ldquo;类型那么显著。它以分布式的方式计算和生成索引，但它需要另一个额外的 Spark Job 来内部生成全局序列。它也不能保证结果的自然顺序。一般来说，它会变成一个不断增加的数字。\n\u0026gt;\u0026gt;\u0026gt; ks.set_option(\u0026#39;compute.default_index_type\u0026#39;, \u0026#39;distributed-sequence\u0026#39;) \u0026gt;\u0026gt;\u0026gt; spark.range(5).to_koalas() id 3 3 1 1 2 2 4 4 0 0 分散型 \u0026ldquo;分布式 \u0026ldquo;索引几乎没有性能上的惩罚，而且总是创建单调增加的数字。如果索引只是需要作为每行的唯一数字，或行的顺序，这种索引类型将是最佳选择。但是，这些数字有一个不确定的间隙。这意味着这种索引类型不太可能被用作结合两个以上 DataFrames 或 Series 的操作的索引。\n\u0026gt;\u0026gt;\u0026gt; ks.set_option(\u0026#39;compute.default_index_type\u0026#39;, \u0026#39;distributed\u0026#39;) \u0026gt;\u0026gt;\u0026gt; spark.range(5).to_koalas() id 17179869184 0 34359738368 1 60129542144 2 77309411328 3 94489280512 4 比较 正如你所看到的，每种索引类型都有其独特的特征，如下表所示。考虑到你的工作负载，应该谨慎选择默认的索引类型。\n   分布式计算 Map 端操作 连续递增 性能     sequence No, 在单个 worker 节点中 No, 需要 shuffle Yes   distributed-sequence Yes Yes, 但需要另一个 Spark job Yes, 在大多数情况下   distributed Yes Yes No    参见 Koalas 文档中的默认索引类型。\n使用 Spark I/O 在 pandas 中，有很多函数可以读写数据，在 Koalas 中也是如此。\n下面是 pandas 中的函数列表，Koalas 在下面使用了 Spark I/O。\n DataFrame.to_csv / ks.read_csv DataFrame.to_json / ks.read_json DataFrame.to_parquet / ks.read_parquet ks.read_sql_table ks.read_sql_query  API 和它们的参数沿用了 pandas 对应的 API。不过，目前在行为上有细微的差别。例如，pandas 的 read_csv 可以通过 http 协议读取文件，但 Koalas 仍然不支持，因为底层的 Spark 引擎本身并不支持。\n这些 Koalas 函数还有 index_col 参数，用来指定哪些列应该被用作索引，或者索引列名应该是什么，类似于上面介绍的 to_koalas()或 to_spark()函数。如果你不指定，就会附加默认的索引，或者索引列丢失。\n例如，如果你不指定 index_col 参数，默认索引就会被附加，如下图所示\u0026ndash;为了简单起见，使用了分布式默认索引。\n\u0026gt;\u0026gt;\u0026gt; kdf.to_csv(\u0026#39;/path/to/test.csv\u0026#39;) \u0026gt;\u0026gt;\u0026gt; kdf_read_csv = ks.read_csv(\u0026#39;/path/to/test.csv\u0026#39;) \u0026gt;\u0026gt;\u0026gt; kdf_read_csv x y z 0 2 20.0 b 8589934592 3 30.0 c 17179869184 1 10.0 a 而如果指定 index_col 参数，指定的列就会变成一个索引。\n\u0026gt;\u0026gt;\u0026gt; kdf.to_csv(\u0026#39;/path/to/test.csv\u0026#39;, index_col=\u0026#39;index\u0026#39;) \u0026gt;\u0026gt;\u0026gt; kdf_read_csv_with_index_col = ks.read_csv(\u0026#34;/path/to/test.csv\u0026#34;, index_col=\u0026#39;index\u0026#39;) \u0026gt;\u0026gt;\u0026gt; kdf_read_csv_with_index_col x y z index 2 3 30.0 c 1 2 20.0 b 0 1 10.0 a 此外，每个函数都需要关键字参数来设置 Spark 中 DataFrameWriter 和 DataFrameReader 的选项。给定的键直接传递给它们的选项并配置行为。当 pandas-origin 参数不足以操作你的数据，但 PySpark 支持缺失的功能时，这很有用。\n\u0026gt;\u0026gt;\u0026gt; # nullValue is the option specific to Spark’s CSV I/O. \u0026gt;\u0026gt;\u0026gt; ks.read_csv(\u0026#39;/path/to/test.csv\u0026#39;, index_col=\u0026#39;index\u0026#39;, nullValue=\u0026#39;b\u0026#39;) x y z index 2 3 30.0 c 1 2 20.0 None 0 1 10.0 a Koalas 特定的 I/O 功能 除了以上来自 pandas 的功能外，Koalas 还有自己的功能。\n DataFrame.to_table / ks.read_table DataFrame.to_spark_io / ks.read_spark_io DataFrame.to_delta / ks.read_delta  首先，DataFrame.to_table 和 ks.read_table 是只需指定表名就可以写入和读取 Spark 表。这分别类似于 Spark 中的 DataFrameWriter.saveAsTable 和 DataFrameReader.table。\n其次，DataFrame.to_spark_io 和 ks.read_spark_io 是用于一般的 Spark I/O。为了方便使用，有几个可选的参数，其他都是关键字参数。你可以自由设置 Spark 中 DataFrameWriter.save 和 DataFrameReader.load 使用的选项。\n\u0026gt;\u0026gt;\u0026gt; # \u0026#39;compression\u0026#39; is a Spark specific option. \u0026gt;\u0026gt;\u0026gt; kdf.to_spark_io(\u0026#39;/path/to/test.orc\u0026#39;, format=\u0026#39;orc\u0026#39;, index_col=\u0026#39;index\u0026#39;, compression=\u0026#34;snappy\u0026#34;) \u0026gt;\u0026gt;\u0026gt; kdf_read_spark_io = ks.read_spark_io(\u0026#39;/path/to/test.orc\u0026#39;, format=\u0026#39;orc\u0026#39;, index_col=\u0026#39;index\u0026#39;) \u0026gt;\u0026gt;\u0026gt; kdf_read_spark_io x y z index 1 2 20.0 b 0 1 10.0 a 2 3 30.0 c 上例中的 ORC 格式在 pandas 中是不支持的，但 Koalas 可以写和读，因为底层的 Spark I/O 支持它。\n最后，如果你安装了 Delta Lake，Koalas 也可以写和读 Delta 表。\nDelta Lake 是一个开源的存储层，为数据湖带来了可靠性。Delta Lake 提供了 ACID 事务、可扩展的元数据处理，并统一了流式和批处理数据。\n与其他文件源不同的是，read_delta 函数可以让用户指定表的版本来进行时间旅行。\n\u0026gt;\u0026gt;\u0026gt; kdf.to_delta(\u0026#39;/path/to/test.delta\u0026#39;, index_col=\u0026#39;index\u0026#39;) \u0026gt;\u0026gt;\u0026gt; kdf_read_delta = ks.read_delta(\u0026#39;/path/to/test.delta\u0026#39;, index_col=\u0026#39;index\u0026#39;) \u0026gt;\u0026gt;\u0026gt; kdf_read_delta x y z index 0 1 10.0 a 1 2 20.0 b 2 3 30.0 c \u0026gt;\u0026gt;\u0026gt; # Update the data and overwrite the Delta table \u0026gt;\u0026gt;\u0026gt; kdf[\u0026#39;x\u0026#39;] = kdf[\u0026#39;x\u0026#39;] + 10 \u0026gt;\u0026gt;\u0026gt; kdf[\u0026#39;y\u0026#39;] = kdf[\u0026#39;y\u0026#39;] * 10 \u0026gt;\u0026gt;\u0026gt; kdf[\u0026#39;x\u0026#39;] = kdf[\u0026#39;x\u0026#39;] * 2 \u0026gt;\u0026gt;\u0026gt; kdf.to_delta(\u0026#39;/path/to/test.delta\u0026#39;, index_col=\u0026#39;index\u0026#39;) \u0026gt;\u0026gt;\u0026gt; # Read the latest data \u0026gt;\u0026gt;\u0026gt; ks.read_delta(\u0026#39;/path/to/test.delta\u0026#39;, index_col=\u0026#39;index\u0026#39;) x y z index 0 22 100.0 a 1 24 200.0 b 2 26 300.0 c \u0026gt;\u0026gt;\u0026gt; # Read the data of version 0 \u0026gt;\u0026gt;\u0026gt; ks.read_delta(\u0026#39;/path/to/test.delta\u0026#39;, version=0, index_col=\u0026#39;index\u0026#39;) x y z index 0 1 10.0 a 1 2 20.0 b 2 3 30.0 c 详情请看 Delta Lake。\nSpark accessor Koalas 为用户提供了 spark 接入器，让用户更容易地利用现有的 PySpark API。\nSeries.spark.transform 和 Series.spark.apply Series.spark accessor 有变换和应用函数来处理底层的 Spark Column 对象。\n例如，假设你有以下 Koalas DataFrame。\n\u0026gt;\u0026gt;\u0026gt; kdf = ks.DataFrame({\u0026#39;a\u0026#39;: [1, 2, 3, 4]]}) \u0026gt;\u0026gt;\u0026gt; kdf a 0 1 1 2 2 3 3 4 你可以使用 astype 函数来铸造类型，但如果你还不习惯，你可以使用 Series.spark.transform 函数来铸造 Spark 列。\n\u0026gt;\u0026gt;\u0026gt; import numpy as np \u0026gt;\u0026gt;\u0026gt; from pyspark.sql.types import DoubleType \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; kdf[\u0026#39;a_astype_double\u0026#39;] = kdf.a.astype(np.float64) \u0026gt;\u0026gt;\u0026gt; kdf[\u0026#39;a_cast_double\u0026#39;] = kdf.a.spark.transform(lambda scol: scol.cast(DoubleType())) \u0026gt;\u0026gt;\u0026gt; kdf[[\u0026#39;a\u0026#39;, \u0026#39;a_astype_double\u0026#39;, \u0026#39;a_cast_double\u0026#39;]] a a_astype_double a_cast_double 0 1 1.0 1.0 1 2 2.0 2.0 2 3 3.0 3.0 3 4 4.0 4.0 传递给 Series.spark.transform 函数的用户函数取用 Spark 的 Column 对象，可以使用 PySpark 函数对其进行操作。\n也可以在 transform/apply 函数中使用 pyspark.sql.function 的函数。\n\u0026gt;\u0026gt;\u0026gt; from pyspark.sql import functions as F \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; kdf[\u0026#39;a_sqrt\u0026#39;] = kdf.a.spark.transform(lambda scol: F.sqrt(scol)) \u0026gt;\u0026gt;\u0026gt; kdf[\u0026#39;a_log\u0026#39;] = kdf.a.spark.transform(lambda scol: F.log(scol)) \u0026gt;\u0026gt;\u0026gt; kdf[[\u0026#39;a\u0026#39;, \u0026#39;a_sqrt\u0026#39;, \u0026#39;a_log\u0026#39;]] a a_sqrt a_log 0 1 1.000000 0.000000 1 2 1.414214 0.693147 2 3 1.732051 1.098612 3 4 2.000000 1.386294 Series.spark.transform 的用户函数应该返回与其输入相同长度的 Spark 列，而 Series.spark.apply 的用户函数可以返回不同长度的 Spark 列，比如调用聚合函数。\n\u0026gt;\u0026gt;\u0026gt; kdf.a.spark.apply(lambda scol: F.collect_list(scol)) 0 [1, 2, 3, 4] Name: a, dtype: object DataFrame.spark.apply 同样，DataFrame.spark accessor 也有一个 apply 函数。用户函数接受并返回一个 Spark DataFrame，并可以应用任何转换。如果你想在 Spark DataFrame 中保留索引列，你可以设置 index_col 参数。在这种情况下，用户函数必须在返回的 Spark DataFrame 中包含一个同名的列。\n\u0026gt;\u0026gt;\u0026gt; kdf.spark.apply(lambda sdf: sdf.selectExpr(\u0026#34;index * 10 as index\u0026#34;, \u0026#34;a + 1 as a\u0026#34;), index_col=\u0026#34;index\u0026#34;) a index 0 2 10 3 20 4 30 5 如果你省略 index_col，它将使用默认的索引。\n\u0026gt;\u0026gt;\u0026gt; kdf.spark.apply(lambda sdf: sdf.selectExpr(\u0026#34;a + 1 as a\u0026#34;)) a 17179869184 2 42949672960 3 68719476736 4 94489280512 5 Spark schema 你可以通过 DataFrame.spark.schema 和 DataFrame.spark.print_schema 查看当前的底层 Spark 模式。如果你想知道包括索引列在内的模式，它们都需要 index_col 参数。\n\u0026gt;\u0026gt;\u0026gt; import numpy as np \u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; kdf = ks.DataFrame({\u0026#39;a\u0026#39;: list(\u0026#39;abc\u0026#39;), ... \u0026#39;b\u0026#39;: list(range(1, 4)), ... \u0026#39;c\u0026#39;: np.arange(3, 6).astype(\u0026#39;i1\u0026#39;), ... \u0026#39;d\u0026#39;: np.arange(4.0, 7.0, dtype=\u0026#39;float64\u0026#39;), ... \u0026#39;e\u0026#39;: [True, False, True], ... \u0026#39;f\u0026#39;: pd.date_range(\u0026#39;20130101\u0026#39;, periods=3)}, ... columns=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; # Print the schema out in Spark’s DDL formatted string \u0026gt;\u0026gt;\u0026gt; kdf.spark.schema().simpleString() \u0026#39;struct\u0026lt;a:string,b:bigint,c:tinyint,d:double,e:boolean,f:timestamp\u0026gt;\u0026#39; \u0026gt;\u0026gt;\u0026gt; kdf.spark.schema(index_col=\u0026#39;index\u0026#39;).simpleString() \u0026#39;struct\u0026lt;index:bigint,a:string,b:bigint,c:tinyint,d:double,e:boolean,f:timestamp\u0026gt;\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Print out the schema as same as Spark’s DataFrame.printSchema() \u0026gt;\u0026gt;\u0026gt; kdf.spark.print_schema() root |-- a: string (nullable = false) |-- b: long (nullable = false) |-- c: byte (nullable = false) |-- d: double (nullable = false) |-- e: boolean (nullable = false) |-- f: timestamp (nullable = false) \u0026gt;\u0026gt;\u0026gt; kdf.spark.print_schema(index_col=\u0026#39;index\u0026#39;) root |-- index: long (nullable = false) |-- a: string (nullable = false) |-- b: long (nullable = false) |-- c: byte (nullable = false) |-- d: double (nullable = false) |-- e: boolean (nullable = false) |-- f: timestamp (nullable = false) 解释 Spark 计划 如果你想知道当前的 Spark 计划，你可以使用 DataFrame.spark.explain()。\n\u0026gt;\u0026gt;\u0026gt; # Same as Spark’s DataFrame.explain() \u0026gt;\u0026gt;\u0026gt; kdf.spark.explain() == Physical Plan == Scan ExistingRDD[...] \u0026gt;\u0026gt;\u0026gt; kdf.spark.explain(True) == Parsed Logical Plan == ... == Analyzed Logical Plan == ... == Optimized Logical Plan == ... == Physical Plan == Scan ExistingRDD[...] \u0026gt;\u0026gt;\u0026gt; # New style of mode introduced from Spark 3.0. \u0026gt;\u0026gt;\u0026gt; kdf.spark.explain(mode=\u0026#34;extended\u0026#34;) == Parsed Logical Plan == ... == Analyzed Logical Plan == ... == Optimized Logical Plan == ... == Physical Plan == Scan ExistingRDD[...] 缓存 spark 访问器还提供了缓存相关的函数，cache、persist、unpersist 和 store_level 属性。你可以使用 cache 函数作为上下文管理器来解除缓存的 persist。让我们看一个例子。\n\u0026gt;\u0026gt;\u0026gt; from pyspark import StorageLevel \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; with kdf.spark.cache() as cached: ... print(cached.spark.storage_level) ... Disk Memory Deserialized 1x Replicated \u0026gt;\u0026gt;\u0026gt; with kdf.spark.persist(StorageLevel.MEMORY_ONLY) as cached: ... print(cached.spark.storage_level) ... Memory Serialized 1x Replicated 当上下文完成后，缓存会自动清除。如果你想保留它的缓存，你可以按照下面的方法来做。\n\u0026gt;\u0026gt;\u0026gt; cached = kdf.spark.cache() \u0026gt;\u0026gt;\u0026gt; print(cached.spark.storage_level) Disk Memory Deserialized 1x Replicated 当不再需要它时，你必须显式调用 DataFrame.spark.unpersist()来从缓存中删除它。\n\u0026gt;\u0026gt;\u0026gt; cached.spark.unpersist() 提示 在 Koalas 中，有一些类似于 join 的操作，比如合并、加入和更新。虽然实际的 join 方法取决于底层的 Spark 计划器，但你仍然可以用 ks.broadcast()函数或 DataFrame.spark.hint()方法指定一个提示。\n\u0026gt;\u0026gt;\u0026gt; kdf1 = ks.DataFrame({\u0026#39;key\u0026#39;: [\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;, \u0026#39;baz\u0026#39;, \u0026#39;foo\u0026#39;], ... \u0026#39;value\u0026#39;: [1, 2, 3, 5]}, ... columns=[\u0026#39;key\u0026#39;, \u0026#39;value\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; kdf2 = ks.DataFrame({\u0026#39;key\u0026#39;: [\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;, \u0026#39;baz\u0026#39;, \u0026#39;foo\u0026#39;], ... \u0026#39;value\u0026#39;: [5, 6, 7, 8]}, ... columns=[\u0026#39;key\u0026#39;, \u0026#39;value\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; kdf1.merge(kdf2, on=\u0026#39;key\u0026#39;).explain() == Physical Plan == ... ... SortMergeJoin ... ... \u0026gt;\u0026gt;\u0026gt; kdf1.merge(ks.broadcast(kdf2), on=\u0026#39;key\u0026#39;).explain() == Physical Plan == ... ... BroadcastHashJoin ... ... \u0026gt;\u0026gt;\u0026gt; kdf1.merge(kdf2.spark.hint(\u0026#39;broadcast\u0026#39;), on=\u0026#39;key\u0026#39;).explain() == Physical Plan == ... ... BroadcastHashJoin ... ... 特别是，如果底层 Spark 是 3.0 或以上版本，DataFrame.spark.hint()更有用，因为 Spark 3.0 中提供了更多的提示。\n结束语 Koalas DataFrame 与 PySpark DataFrame 相似，因为 Koalas 内部使用 PySpark DataFrame。在外部，Koalas DataFrame 的工作方式就像 pandas DataFrame 一样。\n为了填补这个空白，Koalas 有许多功能，对于熟悉 PySpark 的用户来说，可以轻松地使用 Koalas 和 PySpark DataFrame。虽然在转换过程中需要额外的注意处理索引，但 Koalas 为 PySpark 用户提供了两种 DataFrame 之间的简单转换，为 PySpark 提供了读/写的输入/输出 API，并提供了 spark 访问器以暴露 PySpark 友好的功能，如缓存和内部探索 DataFrame。此外，spark 访问器还提供了一种自然的方式来玩弄 Koalas 系列和 PySpark 列。\nPySpark 用户可以从 Koalas 中获益，如上图所示。请在 Databricks Runtime 中试用这些示例并了解更多信息。\n阅读更多 要了解更多关于 Koalas 的信息，请看以下资源。\n 试试附带的笔记本 在 Apache Spark 上阅读之前的博客《从 pandas 到 Koalas 的 10 分钟》。 Spark+AI 峰会 2020 演讲 \u0026ldquo;Koalas: Pandas on Apache Spark\u0026rdquo; Spark+AI 峰会 2020 演讲 \u0026ldquo;Koalas: 从 Pandas 轻松过渡到 Apache Spark\u0026rdquo;  原文链接: https://databricks.com/blog/2020/08/11/interoperability-between-koalas-and-apache-spark.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-10-04-interoperability-between-koalas-and-apache-spark/","tags":["PySpark","Koalas"],"title":"Koalas 和 Apache Spark 之间的互操作性"},{"categories":["rakulang"],"contents":"贪婪 junction 的奇闻异事 说明 Raku 的 junction 是如何贪婪的设计，以及一个建议。\nRaku 有一个整洁的功能，叫做 Junction。在这篇短文中，我想强调一下 junction 与函数交互的一个特殊后果：它们是贪婪的，我的意思是它们会无意中把函数的其他参数变成 junction。为了说明这种行为，我将使用一个闭包创建一个 pair 数据结构，它可以接受两个不同类型的值。\nenum RGB \u0026lt;R G B\u0026gt;; # Pair Constructor: the arguments of pair() are captured # in a closure that is returned sub pair(\\x, \\y) { sub (\u0026amp;p){ p(x, y) } } 所以 pair 接受两个任意类型的参数，并返回一个以函数为参数的闭包。我们将使用这个函数来访问存储在 pair 中的值。我将把这些访问(accessor)函数称为 fst 和 snd。\n# Accessors to get the values from the closure my sub fst (\u0026amp;p) {p( sub (\\x,\\y){x})} my sub snd (\u0026amp;p) {p( sub (\\x,\\y){y})} 做实际选择的函数是由 fst 和 snd 返回的匿名子程序，这纯粹是为了让我可以将它们应用于 pair，而不是必须将它们作为参数传递。让我们看一个例子，一个 Int 和一个 RGB 的 pair。\nmy \\p1 = pair 42, R; if ( 42 == fst p1) { say snd p1;\t#=\u0026gt;says \u0026#34;R\u0026#34; } 所以我们用两个值调用 pair 来创建一个 pair，并使用 fst 和 snd 来访问 pair 中的值。这是一个不可变的数据结构，所以不可能进行更新。\n现在让我们使用 junction 作为其中一个参数。\n# Example instance with a \u0026#39;one\u0026#39;-type junction my Junction \\p1j = pair (42^43),R; if ( 42 == fst p1j) { say snd p1j; #=\u0026gt;one(R, R) } 这里发生的情况是，原始参数 R 已经不可逆转地变成了与自己的 junction，尽管我们从未明确地在 R 上创建过 junction，但还是发生了这种情况。这是将 junction 类型应用于函数的结果，它不是一个 bug，只是 junction 行为的一个影响。更详细的解释，请看我的文章\u0026quot;重构 Raku 的 Junction\u0026quot;。\nRaku 关于 junction 的文档中说，你不应该真正尝试从 junction 中获取值。\n\u0026ldquo;Junction 是用来作为布尔上下文中的匹配器，不支持 junction 的自省。如果你觉得有自省 junction 的冲动，请使用 Set 或相关类型代替。\u0026rdquo;\n然而，有一个 FAQ 勉强地告诉你如何做。FAQ 再次警告不要这样做。\n\u0026ldquo;如果你想从 junction 中提取值（特征态），你可能做错了什么，应该用 Set 来代替。\u0026rdquo;\n然而，正如我所举的例子所证明的那样，从 junction 中恢复值是有明确的用例的。当然，仅仅因为另一个值恰好是 junction，存储在 pair 中的其中一个值就变得不可访问，这不是我们的本意。\n因此，我建议增加一个折叠(collapse)函数，允许将这些无意中出现的 junction 值折叠成它们的原始值。\nif ( 42 == fst p1j) { say collapse(snd p1j); #=\u0026gt;says \u0026#39;R\u0026#39; } 该函数的实现取自上述常见问题，并增加了一个检查，以确保 junction 上的所有值都相同。\nsub collapse(Junction \\j) { my @vvs; -\u0026gt; Any \\s {push @vvs, s }.(j); my $v = shift @vvs; my @ts = grep {!($_ ~~ $v)}, @vvs; if (@ts.elems==0) { $v } else { die \u0026#34;Can\u0026#39;t collapse this Junction: elements are not identical: {$v,@vvs}\u0026#34;; } } 如果能把这个功能作为一个 collapse 方法添加到 Junction 类中就更好了。\n原文链接: https://gist.github.com/wimvanderbauwhede/85fb4b88ec53a0b8149e6c05740adcf8\n","permalink":"https://ohmyweekly.github.io/notes/2020-10-04-the-strange-case-of-the-greedy-junction/","tags":["Raku","Junction"],"title":"贪婪 Junction 的奇闻异事"},{"categories":["rakulang"],"contents":"重构 Raku 的 Junction Raku 中的 junction 很酷，但乍一看它们并没有遵循静态类型化的规则。我对它们的形式化类型语义很好奇，所以我从功能、静态类型的角度对 junction 进行了解构和重构。\nRaku 中的 Junction Raku 有一个整洁的功能叫做 Junction。Junction 是一个无序的复合值。当使用 junction 代替值时，会对每个结点(junction)元素进行操作，结果是所有这些操作符的返回值的结点(junction)。当在布尔上下文中使用 junction 时，结点(junction)会折叠成一个值。Junction 的类型可以是 all(\u0026amp;)、any(|)、one(^) 或 none (空结点)。\n例如:\nmy $j = 11|22; # short for any(11,22) if 33 == $j + 11 { say \u0026#39;yes\u0026#39;; } say so 3 == (1..30).one; #=\u0026gt;True say so (\u0026#34;a\u0026#34; ^ \u0026#34;b\u0026#34; ^ \u0026#34;c\u0026#34;) eq \u0026#34;a\u0026#34;; #= True 函数 so 强制使用布尔上下文。\nJunction 有 Junction 类型，我很好奇 Junction 的类型规则，因为乍一看有些奇怪。比方说我们有一个函数 sq 从 Int 到 Int。\nsub sq(Int $x --\u0026gt; Int) { $x*$x } my Int $res = sq(11); # OK say $res; #=\u0026gt;121 现在让我们定义一个类型为任何 Int 值的 Junction。\nmy Junction $j = 11 | 22; 当我们将 sq 应用于 $j 时，我们没有得到一个类型错误，即使函数的类型是 :(Int --\u0026gt; Int)，Junction 的类型是 Junction。相反，我们得到的是一个结果的 Junction。\nsay sq($j); #=\u0026gt;any(121, 484) 如果我们像之前一样将其赋值给一个类型为 Int 的变量，我们会得到一个类型错误。\nmy Int $rj = sq($j); #=\u0026gt;Type check failed in assignment to $rj; expected Int but got Junction (any(121, 484)) 取而代之的是，现在返回值的类型为 Junction。\nmy Junction $rj = sq(11|22); # OK 所以，Junction 类型可以代替任何其他类型，但这样一来，操作也变成了 Junction。\n另一方面，Junction 是由其组成值隐式类型的，尽管它们看起来是不透明的 Junction 类型。例如，如果我们创建了一个由 Str 值组成的 Junction，并试图将这个 Junction 的值传递到 sq 中，我们会得到一个类型错误。\nmy $sj = \u0026#39;11\u0026#39; | \u0026#39;22\u0026#39;; say $sj.WHAT; #=\u0026gt;(Junction) my Junction $svj = sq($sj); #= Type check failed in binding to parameter \u0026#39;x\u0026#39;; expected Int but got Str (\u0026#34;11\u0026#34;) Junction 不遵循静态类型规则 虽然这样做是有道理的(如果原始函数期望使用 Int，我们不希望它与 Str 一起工作)，但这确实违背了静态类型化的规则，即使是子类型化。如果一个参数的类型是 Int，那么可以使用类型图中低于它的任何类型来代替。但是 Int 和 Junction 的简化类型图如下。\nInt -\u0026gt; Cool -\u0026gt; Any -\u0026gt; Mu \u0026lt;- Junction  所以 Junction 永远不是 Any 以下任何东西的子类型。因此，将 Junction 放在类型为 Any 或其子类型的槽中应该是一个类型错误。\n此外，由于 Junction 类型是不透明的（即它不是一个参数化的类型），它不应该持有任何关于 Junction 内部值的类型的信息。然而它却对这些不可见、不可访问的值进行了类型检查。\n那么这里到底发生了什么？\n一个工作假设 一个工作假设是，Junction 类型并不真正取代任何其他类型：它只是一个语法糖，使它看起来如此。\n重构 Junction 的第一部分：类型 让我们试着重建这个。我们的目的是想出一个数据类型和一些动作，以复制观察到的 Raku Junction 的行为。首先我们讨论一下类型，为了清晰起见，使用 Haskell 符号。然后我介绍 Raku 中的实现。这个实现将像 Raku 的原生 Junction 一样，但没有神奇的语法糖。通过这种方式，我证明了 Raku 的 Junction 毕竟遵循了正确的类型规则。\nJunction 类型 Junction 是一个由 Junction 类型 JType 和一组值组成的数据结构。为了方便起见，我将这个值集限制为单一类型，同时也是因为混合类型的 Junction 其实没有什么意义。我使用一个列表来模拟这个集合，同样是为了方便。因为 Junction 可以包含任何类型的值，所以它是一个多态的代数数据类型。\ndata JType = JAny | JAll | JOne | JNone data Junction a = Junction JType [a] 应用结点 对一个 Junction 做任何事情都意味着对它应用一个函数。我们可以考虑三种情况，我为每一种情况介绍一个特别定制的操作符。\n 将非 Junction 函数应用于 Junction 表达式  (•￮) :: (a -\u0026gt; b) -\u0026gt; Junction a -\u0026gt; Junction b  将 Junction 函数应用于非 Junction 表达式。  (￮•) :: Junction (b -\u0026gt; c) -\u0026gt; b -\u0026gt; Junction c  将 Junction 函数应用于 Junction 表达式，创建一个嵌套 Junction。  (￮￮) :: Junction (b -\u0026gt; c) -\u0026gt; Junction b -\u0026gt; Junction (Junction c) 为了方便，我们还可以在 Junction a 和 a 之间创建自定义比较运算符。\n-- and similar for /-, \u0026gt;, \u0026lt;, \u0026lt;=,\u0026gt;= (￮==•) :: Junction a -\u0026gt; a -\u0026gt; Bool 折叠 Junction 那么我们就有了 so，布尔强制函数。它的作用是将一个布尔的 Junction 折叠成一个布尔。\nso :: Junction Bool -\u0026gt; Bool 最后我们有 collapse，它从一个 Junction 返回值，前提是它是一个 Junction，所有存储的值都是一样的。\ncollapse :: (Show a,Eq a) =\u0026gt; Junction a -\u0026gt; a 这似乎是一个奇怪的函数，但由于 Junction 的行为，它是必要的。正如我们将看到的，上述语义意味着 Junction 是贪婪的：如果一个函数的一个参数是 Junction，那么所有其他参数也会成为 Junction，但 Junction 中的所有值都是相同的。我已经在\u0026quot;贪婪 Junction 的奇怪情况\u0026ldquo;中讨论过这个问题，但我们现在可以将这种行为形式化。\n重新审视贪婪 Junction 的奇怪情况 假设我们有一个两个参数的函数 f :: a -\u0026gt; b -\u0026gt; c，我们对第一个参数应用一个结点 j :: Junction a 应用到第一个参数 f •￮ j 上，那么结果是一个部分应用的函数，包裹在一个 Junction 上：fp :: Junction b -\u0026gt; c。如果我们现在想用 fp ￮• v 将这个函数应用于一个非结点的值 v :: b，那么结果就是 Junction c 类型的。\n现在，让我们考虑类型 c 是 forall d . (a -\u0026gt; b -\u0026gt; d) -\u0026gt; d 的特殊情况。所以我们有 Junction(forall d . (a-\u0026gt;b-\u0026gt;d) -\u0026gt; d)。这是一个函数，它接受一个函数参数并返回该函数的返回类型的东西。我们使用 forall，所以 d 可以是任何东西，但在实践中我们希望它是 a 或 b。\n假设我们将这个函数(称它为 p)应用于 fst :: a-\u0026gt;b-\u0026gt;a，使用 p ￮• fst，那么我们得到 Junction a。但是如果我们将它应用于 snd :: a-\u0026gt;b-\u0026gt;b，使用 p ￮• snd，那么我们得到 Junction b。\n这就是形式上基于类型的分析，为什么我们不能从一个 pair 中返回一个非 Junction 的值，在\u0026rdquo;贪婪 Junction 的奇怪情况\u0026ldquo;中已经解释过。而这也是我们需要 collapse 函数的原因。\n重构 Junction 的第2部分：Raku 的实现。 我们从创建 Junction 类型开始，为四种 Junction 类型使用一个枚举，为实际的 Junction 数据类型使用一个角色。\n# The types of Junctions enum JType \u0026lt;JAny JAll JOne JNone \u0026gt;; # The actual Junction type role Junction[\\jt, @vs] { has JType $.junction-type=jt; has @.values=@vs; } 接下来是四种类型的 Junction 的构造函数（下划线，避免与内建函数的名称冲突）。\nour sub all_(@vs) { Junction[ JAll, @vs].new; } our sub any_(@vs) { Junction[ JAny, @vs].new; } our sub one_(@vs) { Junction[ JOne, @vs].new; } our sub none_(@vs) { Junction[ JNone, @vs].new; } 将一个（单参数）函数应用于 junction 参数。\nsub infix:\u0026lt;●○\u0026gt;( \u0026amp;f, \\j ) is export { my \\jt=j.junction-type; my @vs = j.values; Junction[ jt, map( {\u0026amp;f($_)}, @vs)].new; } 要将 Junction 内的函数应用于非 Junction 的参数:\nsub infix:\u0026lt;○●\u0026gt;( \\jf, \\v ) is export { my \\jt=jf.junction-type; my @fs = jf.values; Junction[ jt, map( {$_( v)}, @fs)].new; } 将一个函数应用于两个 junction 参数，相当于将一个 junction 内的函数应用于一个 junction。这里有一个复杂的问题。Raku 对嵌套施加了一个排序，即所有的嵌套总是外嵌套。因此，我们必须检查 junction 的类型，如果需要的话，我们必须交换映射。\nsub infix:\u0026lt;○○\u0026gt;( \\jf, \\jv ) is export { my \\jft= jf.junction-type; my @fs = jf.values; my \\jvt = jv.junction-type; my @vs = jv.values; if (jvt == JAll and jft != JAll) { Junction[ jvt, map( sub (\\v){jf ○● v}, @vs)].new; } else { Junction[ jft, map( sub (\u0026amp;f){ \u0026amp;f ●○ jv}, @fs)].new; } } 为了完整，这里是 ○==● 的定义。○!=●、○\u0026gt;● 等的定义是类似的。\nsub infix:\u0026lt;○==● \u0026gt;( \\j, \\v ) is export { sub (\\x){x==v} ●○ j } 接下来我们有 so，它把布尔值的 junction 变成了布尔值。\nour sub so (\\jv) { my @vs = jv.values; given jv.junction-type { when JAny { elems(grep {$_}, @vs) \u0026gt;0} when JAll { elems(grep {!$_}, @vs)==0} when JOne { elems(grep {$_}, @vs)==1} when JOne { elems(grep {$_}, @vs)==0} } } 最后我们有 collapse，正如贪婪 Junction 的文章中所定义的那样， collapse 返回 Junction 的值，只要它们都是一样的。\nour sub collapse( \\j ) { my \\jt=j.junction-type; my @vvs = j.values; my $v = shift @vvs; my @ts = grep {!($_ ~~ $v)}, @vvs; if (@ts.elems==0) { $v } else { die \u0026#34;Can\u0026#39;t collapse this Junction: elements are not identical: {$v,@vvs}\u0026#34;; } } Junction 清理 现在我们再来看看我们的工作假说，将 Raku 的 Junction 上的动作解释为上述类型和操作符的语法糖。\nsub sq(Int $x --\u0026gt; Int) { $x*$x } my Junction $j = 11 | 22; my Junction $rj = sq($j); 去语法塘后这变成了:\nmy Junction $j = any_ [11,22]; my Junction $rj = \u0026amp;sq ●○ $j; 类似地,\nif ($j == 42) {...} 变成了:\nif (so ($j ○==● 42)) {...} 和其他布尔上下文类似。\n如果我们仔细看贪婪 Junction 文章中的 pair 例子，那么将 junction 应用到一个有多个参数的函数上:\nmy Junction \\p1j = pair R,(42^43); 去语法塘后变为:\nmy Junction \\p1j = \u0026amp;pair.assuming(R) ●○ one_ [42,43]; 我们使用 .assuming() 是因为我们需要部分应用。不管我们是先应用非 Junction 参数还是 Junction 参数，都没有关系。\nmy \\p1jr = ( sub ($y){ \u0026amp;pair.assuming(*,$y) } ●○ one_ [42,43] ) ○● R; 最后，举一个两个参数都是 Junction 的例子。由于 ○○ 的定义，应用的顺序并不重要。\nsub m(\\x,\\y){x*y} my \\p4 = ( sub (\\x){ \u0026amp;m.assuming(x) } ●○ any_ [11,22] ) ○○ all_ [33,44]; my \\p4r = ( sub (\\x){ \u0026amp;m.assuming(*,x) } ●○ all_ [33,44] ) ○○ any_ [11,22]; 结论 从 Raku 的 junction 的神奇类型行为实际上是语法糖的假设出发，我使用多态代数数据类型重构了 junction 类型和它的动作，并表明 Raku 的行为作为语法糖的解释对于所提出的实现是成立的。换句话说，Raku 的 Junction 确实遵循静态类型规则。\n原文链接: https://gist.github.com/wimvanderbauwhede/19cc1e8d04e9a477f58cfe7288a6172e\n","permalink":"https://ohmyweekly.github.io/notes/2020-10-04-reconstructing-raku-junctions/","tags":["Raku","Junction"],"title":"重构 Raku 的 Junction"},{"categories":["git"],"contents":"这篇博文涵盖了我最喜欢的一些设置，用于配置 ssh 客户端的行为（即在 ssh_config 的 man 页面中的内容）。无论你是想添加一些额外的安全约束，减少失败，还是防止腕隧道，ssh_config 都是一个经常未被充分利用的强大工具。\n本文将介绍一些修改 ssh_config 文件的有用方法，以达到更高的安全和控制程度。这篇文章并不是关于通过 sshd_config 进行服务器端配置，后者值得单独写一篇文章。\n什么是 ssh_config? 一些工程师可能会惊讶于 ssh 客户端行为有多少是可以通过配置文件来配置的。如果没有配置文件，为 ssh 指定命令行参数很快就会变得很麻烦。\nssh -i /users/virag/keys/us-west/ed25519 -p 1024 -l virag \\ myserver.aws-west.example.com 这句话太长了，一次都打不完，更不用说一天打多次了。如果你要管理多台服务器和虚拟机，创建一个自定义的 ~/.ssh/ssh_config 是修剪常用 ssh 命令的好方法。\n我们可以通过编辑 ssh_config，将上面的例子缩短为 ssh myserver。\nHost myserver Hostname myserver.aws-west.example.com User virag Port 1024 IdentityFile /users/virag/keys/us-west/ed25519 优雅而简单。现在我们有了基础知识，让我们看看这里到底发生了什么。我选择的 ed25519 在比较 SSH 密钥-RSA、DSA、ECDSA 或 EdDSA?\nssh_config 如何工作 ssh 客户端从三个地方读取配置，顺序如下:\n 系统范围内的 /etc/ssh/ssh_config 在 ~/.ssh/ssh_config 中的用户特定配置。 直接提供给 ssh 的命令行标志  这意味着命令行标志(#1)可以覆盖用户特定的配置(#2)，可以覆盖全局配置(#3)\n当连接参数被重复使用时，通常在 ssh_config 中定义这些参数比较容易，它们会在连接时自动应用。虽然它们通常是在用户第一次运行 ssh 时创建的，但目录和文件可以通过以下方式手动创建。\ntouch ~/.ssh/ssh_config 回到上面的例子，你可能会注意到 ssh_config 是以主机头开始的段落来组织的。\nHost [alias] Option1 [Value] Option2 [Value] Option3 [Value] 虽然技术上没有必要，但这种缩进的格式很容易被人类阅读。然而，ssh 客户端并不关心这种格式化，相反，它将通过将命令行中输入的 ssh 参数与所有主机头匹配来获取配置参数。通配符也可以作为主机头的一部分。考虑一下:\nHost myserver2 Hostname myserver2.aws-west.example.com Host myserver* Hostname myserver1.aws-west.example.com User virag Port 1024 使用 myserver1 的别名，我们可以从第二节中得到我们所期望的东西。\nHostname myserver1.aws-west.example.com User virag Port 1024 但 myserver2 也有类似的选项列表。\nHostname myserver2.aws-west.example.com User virag Port 1024 ssh 客户端通过模式匹配获取这些信息，并在向下顺序读取文件时锁定值。因为 myserver2 同时匹配了 myserver2 和 myserver*，所以它会先从 myserver2 中获取 Hostname 值。然后，当到了第二种模式匹配时，就会使用 User 和 Port 的值，但 Hostname 字段已经被填满。让我再重复一遍，ssh 接受每个选项的第一个值。\n常见的 SSH 配置选项 在 man 5 ssh_config 中，有近 100 个 ssh_config 选项。我整理了一份我个人使用的清单，其中许多选项将在后面的文章中使用。\n  Port - 远程 ssh 守护进程运行的端口。如果守护进程运行在默认的 22 号端口上，则不需要定义这个选项。在不同的端口上运行 ssh 守护进程被认为是一个很好的做法，因为这样可以减少僵尸探测的数量。\n  Hostname - 用于建立连接的真实主机名，如 DNS 或 IP 地址。这对缩短主机名很有用。例如，你可以让一个方便的 ssh mongo 带你到 mongo-12.staging.example.com。\n  ProxyJump - 这个选项将通过连接的服务器进行隧道简化为一个标志，-J，用一个别名来命名中间主机（本地客户端和最终目的地之间的主机）。这只适用于较新的客户端（OpenSSH 7.3+）。下面我将详细介绍这个。\n  ForwardAgent \u0026amp; AddKeysToAgent - 在主机之间跳转（当你在另一个 ssh 会话中再次键入 ssh 时）需要重复验证。要做到这一点，ssh 凭证必须存储在中间服务器上，但这不是一个安全的做法。这两个选项允许另一个通常被称为 ssh-agent 的进程自动将你的本地 ssh 凭证加载到内存中，并通过一个安全转发的 UNIX 套接字将其提供给中间机器的 ssh 客户端。ForwardAgent 可以实现这种转发行为，而 AddKeysToAgent 则可以自动将密钥加载到内存中。我将在下面提供更多细节。\n  IdentityFile - 这个选项指定了 ssh 客户端应该尝试验证的密钥的路径。这并不妨碍 ssh 客户端尝试 ~/.ssh 或 ssh-agent 中的密钥。常用于由于某种原因，密钥没有存储在默认位置的情况下。\n  IdentitiesOnly - 通常和 IdentityFile 一起使用，这个选项会告诉 ssh 客户端到底要提交哪个密钥，并放弃 ~/.ssh 或 ssh-agent 中的任何密钥。因为如果尝试了太多无效的密钥，ssh 会抛出一个认证错误，这个选项可以帮助客户端精确地识别要提交的密钥。即使在 ssh_config 中启用了 IdentitiesOnly，任何在命令行输入的身份信息也会被尝试。\n  CertificateFile - 考虑到密钥在很大程度上已经过时了，这个选项可以和 IdentityFile 一起使用来指定要提交的证书。这并不总是必要的。当证书颁发机构签署一个密钥来创建证书时，-cert.pub 将自动附加到密钥的文件名中。在加载密钥之前，ssh 客户端将首先尝试使用预期的命名惯例从提供的文件名中加载证书。然而，如果密钥和证书文件名不遵循这种模式，那么必须使用 CertificateFile，否则将无法找到证书。阅读更多关于为什么你应该使用证书。\n  SetEnv \u0026amp; SendEnv - 这些选项允许 ssh 客户端向指定的主机发送本地环境变量。主机服务器必须通过在 /etc/ssh/sshd_config 中将 AcceptEnv 设置为 Yes 来接受这些环境变量。\n  ServerAliveInterval \u0026amp; ServerAliveCountMax -如果 ssh 客户端在指定的时间间隔内没有收到任何数据，它将请求主机服务器做出响应。这可以防止负载均衡器和服务器因不活动而放弃连接。\n  HostKeyAlias - ssh 客户端会被指示使用 ~/.ssh/known_hosts 中的密钥别名，而不是 HostName。这对于具有动态变化的 IP 地址的主机或在一台主机上运行的多个服务器来说非常有用。\n  PreferredAuthentication - 这个选项决定了验证方法的尝试顺序。默认值是 gssapi-with-mic, hostbased, publickey, keyboard-interactive 和 password。\n  组织你的 SSH 配置 在前面两节所学内容的基础上进行扩展，让我们看看当我们拥有一支规模不大的舰队时，如何组织 ssh_config。以下面的场景为例。\n Virag 在六个环境下工作： 东岸和西岸 AWS 区域的 Dev、Test 和 Prod。 Virag 有普通用户访问开发和产品环境的权限，但在测试环境中是 root 用户。 Prod 环境有更严格的安全控制  我没有记住几个 ssh 命令组合，而是编辑了我的本地配置文件。\nHost east-prod HostName east-prod.prod.example.com Host *-prod HostName west-prod.prod.example.com User virag PasswordAuthentication no PubKeyAuthentication yes IdentityFile /users/virag/keys/production/ed25519 Host east-test HostName east-test.test.example.com Host *-test HostName west-test.test.example.com User root Host east-dev HostName east-dev.east.example.com Host *-dev HostName west-dev.west.example.com User virag Host * !prod PreferredAuthentications publickey Host * HostName bastion.example.com User Default ServerAliveInternal 120 ServerAliveCountMax 5 如果我们运行 ssh east-test，我们的全部选项列表将是：\nHostName east-test.test.example.com User root PreferredAuthentications publickey ServerAliveInternal 30 ServerAliveCountMax 5 客户端通过与 east-test、*-test、* !prod 和 * 匹配来获取预期的选项值。你可能会注意到 Host * stanza 将适用于任何 ssh 参数。换句话说，Host * 定义了所有用户的全局设置。这对于应用客户端可用的安全控制特别有用。上面，我们只用了两个，但有几个关键字会加强安全，如 CheckHostIP，HashKnownHosts，StrictHostKeyChecking，以及更多隐藏的宝石。\n需要注意的是。因为 ssh 客户端是按顺序解释选项的, 通用配置应该放在文件的底部。如果放在最上面，在客户端读取下面的特定主机选项之前，选项值就会被固定下来。在上面的案例中，把 Host * 放在文件的开头会导致用户为 Default。\n如果出现一次性的情况，一定要记住，在命令行输入的选项会覆盖 ssh_config 中的选项：ssh -o \u0026quot;User=root\u0026quot; dev。\n使用 SSH 代理 ssh 跳转服务器是站在客户端和其余 ssh 队伍之间的代理。跳跃主机通过强制所有的 ssh 流量通过一个单一的加固的位置，并最大限度地减少个别节点的 ssh 端点到外部世界，从而将威胁降到最低。\n配置多跳设置的一种方法是在我们的跳转服务器上存储目标服务器的私钥。不要这样做。跳跃服务器通常是一个多用户环境，这意味着任何拥有高权限的单方都可以破坏任何私钥。解决这种安全威胁的方法是启用代理转发，我们在 AddKeysToAgent 和 ForwardAgent 中简单地提到了这个方法。鉴于这种方法是多么常见，当我建议也不要这样做时，你可能会感到惊讶。为了了解原因，我们来深入了解一下。\n代理转发是如何工作的？ ssh-agent 是一个独立于 SSH 的密钥管理器。它在内存中保存用于验证的私钥和证书。它不向磁盘写入或导出密钥。相反，代理的转发功能允许我们的本地代理通过现有的 ssh 连接到达，并通过环境变量在远程服务器上进行认证。基本上，当客户端 ssh 接收到密钥挑战时，代理会将这些挑战上游转发到我们本地的机器上，通过本地存储的私钥构造挑战响应，并转发回下游的目的服务器进行认证。我个人在研究的时候觉得这种直观的解释很有帮助。\n在幕后，ssh-agent 绑定到一个 unix-domain 的 socket 上与其他程序通信（$SSH_AUTH_SOCK 环境变量）。问题是，任何在链上任何地方拥有 root 权限的人都可以使用创建的 socket 来劫持我们本地的 ssh-agent。尽管套接字文件受到操作系统很好的保护，但一个 root 用户可以冒充其他用户，将 ssh 客户端指向自己的恶意代理。从本质上来说，使用代理转发就等于在整个链条上与任何一台机器上的 root 用户共享私钥。(深入阅读使用 ssh-agent 的陷阱)\n事实上，关于 ForwardAgent 的 man 页面上写着。\n \u0026ldquo;代理转发应谨慎启用。有能力绕过远程主机（对代理的 Unix-domain 套接字）上的文件权限的用户可以通过转发连接访问本地代理。攻击者无法从代理中获取密钥材料，然而他们可以对密钥进行操作，使他们能够使用加载到代理中的身份进行验证。\u0026rdquo;\n 使用 ProxyJump 代替 为了在跳转服务器中导航，我们其实并不需要代理转发。现代的方法是使用 ProxyJump 或其命令行等价物 -J。\nHost myserver HostName myserver.example.com User virag IdentityFile /users/virag/keys/ed25519 ProxyJump jump Host jump HostName jump.example.com User default ProxyJump 没有通过代理转发密钥挑战响应，而是将我们本地客户端的 stdin 和 stdout 转发到目的主机。这样一来，我们不需要在 jump.example.com 上运行 ssh。sshd 直接连接到 myserver.example.com，并将该连接的控制权交给我们的本地客户端。作为一个额外的好处，由于跳转服务器在 ssh 隧道内是加密的，所以它不能看到任何通过它的流量。设置跳转服务器而不让 ssh 直接访问它的能力是安全和正确的 ssh 设置的一个重要组成部分。\n多跳的代理跳转 让我们模拟一个更复杂的场景。我们正试图从家里访问公司网络深处的一个关键资源。我们必须首先通过一个具有动态 IP 的外部堡垒主机、一个内部跳转主机，最后到达资源。每台服务器必须针对我们机器上的唯一本地密钥进行验证。\n多重跳转的 ssh 示意图:\n再一次，我们的本地配置文件将包含执行 ssh myserver 所需的一切。\nHost myserver HostName myserver.example.com User virag IdentityFile /users/virag/keys/myserver-cert.pub ProxyJump jump Host bastion #Used because HostName is unreliable as IP address changes frequently HostKeyAlias bastion.example User external Host jump HostName jump.example.com User internal IdentityFile /users/virag/keys/jump-cert.pub ProxyJump bastion 现在想象一下，我们必须用内部配置的 OpenSSH 管理全国各地多个云提供商的几百个环境。(你可能会嗤之以鼻，但我们已经听过这些故事了！)仅仅依靠运行时命令，同时宣称要维护可信的安全程度是不可能的。在这种规模下，有效地管理一个舰队需要有意识地架构子网、DNS、代理链、密钥、文件结构等，这些子网、DNS、代理链、密钥、文件结构等遵循可预测的模式，并且可以抄录到 ~/.ssh/ssh_config 中。\n结束语 从本文中得到的更广泛的启示是让生活变得简单。即使是最简单的配置选项，也可以通过巧妙的方式来实现。这些可以让我们保持对强大安全性的承诺，并最大限度地减少人为错误。\n如果你想了解更多关于 ssh 访问的最佳实践，你可以考虑以下文章。\n 如何正确使用 SSH Teleport: OpenSSH 的现代替代品  原文链接: https://gravitational.com/blog/ssh-config/\n","permalink":"https://ohmyweekly.github.io/notes/2020-10-01-ssh-configuration/","tags":["git","config"],"title":"SSH Configuration: ssh_config"},{"categories":["Flink"],"contents":"为你的 Flink 程序注册一个自定义的序列器 如果你在 Flink 程序中使用的自定义类型不能被 Flink 类型序列化器序列化，Flink 就会回到使用通用的 Kryo 序列化器。你可以用 Kryo 注册你自己的序列化器或像 Google Protobuf 或 Apache Thrift 这样的序列化系统。要做到这一点，只需在 Flink 程序的 ExecutionConfig 中注册类型类和序列化器。\nfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // register the class of the serializer as serializer for a type env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, MyCustomSerializer.class); // register an instance as serializer for a type MySerializer mySerializer = new MySerializer(); env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, mySerializer); 请注意，你的自定义序列化器必须扩展 Kryo 的序列化器类。在 Google Protobuf 或 Apache Thrift 的情况下，这已经为你完成了。\nfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // register the Google Protobuf serializer with Kryo env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, ProtobufSerializer.class); // register the serializer included with Apache Thrift as the standard serializer // TBaseSerializer states it should be initialized as a default Kryo serializer env.getConfig().addDefaultKryoSerializer(MyCustomType.class, TBaseSerializer.class); 为了使上面的例子有效，你需要在 Maven 项目文件(pom.xml)中加入必要的依赖关系。在依赖关系部分，为 Apache Thrift 添加以下内容。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.twitter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;chill-thrift\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.7.6\u0026lt;/version\u0026gt; \u0026lt;!-- exclusions for dependency conversion --\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;com.esotericsoftware.kryo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kryo\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- libthrift is required by chill-thrift --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.thrift\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;libthrift\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.11.0\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;servlet-api\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 对于 Google Protobuf，你需要以下 Maven 依赖。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.twitter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;chill-protobuf\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.7.6\u0026lt;/version\u0026gt; \u0026lt;!-- exclusions for dependency conversion --\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;com.esotericsoftware.kryo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kryo\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- We need protobuf for chill-protobuf --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.protobuf\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;protobuf-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.7.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 请根据需要调整两个库的版本。\n使用 Kryo 的 JavaSerializer 的问题。 如果你为你的自定义类型注册了 Kryo 的 JavaSerializer，你可能会遇到 ClassNotFoundExceptions，即使你的自定义类型类包含在提交的用户代码 jar 中。这是由于 Kryo 的 JavaSerializer 的一个已知问题，它可能会错误地使用错误的 classloader。\n在这种情况下，你应该使用 org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer 来代替解决这个问题。这是 Flink 中重新实现的 JavaSerializer，它可以确保使用用户代码类加载器。\n更多细节请参考 FLINK-6025。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/custom_serializers.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-custom-serializer/","tags":["Flink","Flink 官方文档"],"title":"Custom Serializer"},{"categories":["Flink"],"contents":"Hive 读写 使用 HiveCatalog 和 Flink 与 Hive 的连接器，Flink 可以从 Hive 数据中读取和写入数据，作为 Hive 批处理引擎的替代。请务必按照说明在你的应用中加入正确的依赖关系。同时请注意，Hive 连接器只适用于 blink planner。\n从 Hive 读取数据 假设 Hive 在其默认的数据库中包含一个名为 people 的单表，该表包含多条记录。\nhive\u0026gt; show databases; OK default Time taken: 0.841 seconds, Fetched: 1 row(s) hive\u0026gt; show tables; OK Time taken: 0.087 seconds hive\u0026gt; CREATE TABLE mytable(name string, value double); OK Time taken: 0.127 seconds hive\u0026gt; SELECT * FROM mytable; OK Tom 4.72 John 8.0 Tom 24.2 Bob 3.14 Bob 4.72 Tom 34.9 Mary 4.79 Tiff 2.72 Bill 4.33 Mary 77.7 Time taken: 0.097 seconds, Fetched: 10 row(s) 数据准备好后，你可以连接到现有的 Hive 安装并开始查询。\nFlink SQL\u0026gt; show catalogs; myhive default_catalog # ------ Set the current catalog to be \u0026#39;myhive\u0026#39; catalog if you haven\u0026#39;t set it in the yaml file ------ Flink SQL\u0026gt; use catalog myhive; # ------ See all registered database in catalog \u0026#39;mytable\u0026#39; ------ Flink SQL\u0026gt; show databases; default # ------ See the previously registered table \u0026#39;mytable\u0026#39; ------ Flink SQL\u0026gt; show tables; mytable # ------ The table schema that Flink sees is the same that we created in Hive, two columns - name as string and value as double ------  Flink SQL\u0026gt; describe mytable; root |-- name: name |-- type: STRING |-- name: value |-- type: DOUBLE # ------ Select from hive table or hive view ------  Flink SQL\u0026gt; SELECT * FROM mytable; name value __________ __________ Tom 4.72 John 8.0 Tom 24.2 Bob 3.14 Bob 4.72 Tom 34.9 Mary 4.79 Tiff 2.72 Bill 4.33 Mary 77.7 查询 Hive 视图 如果你需要查询 Hive 视图，请注意。\n在查询该目录中的视图之前，必须先使用 Hive 目录作为当前目录。可以通过 Table API 中的 tableEnv.useCatalog(...) 或者 SQL Client 中的 USE CATALOG \u0026hellip;来实现。 Hive 和 Flink SQL 有不同的语法，例如，不同的保留关键字和字元。请确保视图的查询与 Flink 语法兼容。\n写入 Hive 同样，也可以使用 INSERT 子句将数据写入 hive 中。\n考虑有一个名为 \u0026ldquo;mytable \u0026ldquo;的示例表，表中有两列：name 和 age，类型为 string 和 int。\n# ------ INSERT INTO will append to the table or partition, keeping the existing data intact ------  Flink SQL\u0026gt; INSERT INTO mytable SELECT \u0026#39;Tom\u0026#39;, 25; # ------ INSERT OVERWRITE will overwrite any existing data in the table or partition ------  Flink SQL\u0026gt; INSERT OVERWRITE mytable SELECT \u0026#39;Tom\u0026#39;, 25; 我们也支持分区表，考虑有一个名为 myparttable 的分区表，有四列：name、age、my_type 和 my_date，在 type 中\u0026hellip;my_type 和 my_date 是分区键。\n# ------ Insert with static partition ------  Flink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION (my_type=\u0026#39;type_1\u0026#39;, my_date=\u0026#39;2019-08-08\u0026#39;) SELECT \u0026#39;Tom\u0026#39;, 25; # ------ Insert with dynamic partition ------  Flink SQL\u0026gt; INSERT OVERWRITE myparttable SELECT \u0026#39;Tom\u0026#39;, 25, \u0026#39;type_1\u0026#39;, \u0026#39;2019-08-08\u0026#39;; # ------ Insert with static(my_type) and dynamic(my_date) partition ------  Flink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION (my_type=\u0026#39;type_1\u0026#39;) SELECT \u0026#39;Tom\u0026#39;, 25, \u0026#39;2019-08-08\u0026#39;; 格式 我们测试了以下表格存储格式：文本、csv、SequenceFile、ORC 和 Parquet。\n优化 分区修剪 Flink 使用分区修剪作为一种性能优化，以限制 Flink 在查询 Hive 表时读取的文件和分区的数量。当你的数据被分区后，当查询符合某些过滤条件时，Flink 只会读取 Hive 表中的分区子集。\n投影下推 Flink 利用投影下推，通过从表扫描中省略不必要的字段，最大限度地减少 Flink 和 Hive 表之间的数据传输。\n当一个表包含许多列时，它尤其有利。\n限制下推 对于带有 LIMIT 子句的查询，Flink 会尽可能地限制输出记录的数量，以减少跨网络传输的数据量。\n读取时的向量优化 当满足以下条件时，会自动使用优化功能。\n 格式： ORC 或 Parquet。 没有复杂数据类型的列，如 hive 类型: List, Map, Struct, Union。  这个功能默认是开启的。如果出现问题，可以使用这个配置选项来关闭 Vectorized Optimization。\ntable.exec.hive.fallback-mapred-reader=true Source 并行性推断 默认情况下，Flink 根据分割次数来推断 hive 源的并行度，分割次数是根据文件的数量和文件中的块数来推断的。\nFlink 允许你灵活配置并行度推断的策略。你可以在 TableConfig 中配置以下参数（注意，这些参数会影响作业的所有源）。\n   Key Default Type Description     table.exec.hive.infer-source-parallelism true Boolean 如果为真，则根据分割数来推断源的并行度，如果为假，则根据配置来设置源的并行度。如果为 false，则通过配置来设置源的并行度。   table.exec.hive.infer-source-parallelism.max 1000 Integer 设置源运算符的最大推断并行度。    路线图 我们正在规划并积极开发支持功能，如:\n ACID 表 分桶表 更多格式  更多功能需求请联系社区 https://flink.apache.org/community.html#mailing-lists\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_read_write.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","Hive"],"title":"Hive Read and Write"},{"categories":["Flink"],"contents":"Hive 流 一个典型的 hive 作业是周期性地安排执行的，所以会有较大的延迟。\nFlink 支持以流式的形式写入、读取和加入 hive 表。\n流式数据有三种类型。\n 将流式数据写入 Hive 表。 以流的形式增量读取 Hive 表。 流式表使用 Temporal 表连接 Hive 表。  流式写入 Hive 表支持流式写入，基于 Filesystem Streaming Sink。\nHive Streaming Sink 重用 Filesystem Streaming Sink，将 Hadoop OutputFormat/RecordWriter 整合到流式写入。Hadoop RecordWriters 是 Bulk-encoded Formats，Bulk Formats 在每个检查点上滚动文件。\n默认情况下，现在只有重命名提交者，这意味着 S3 文件系统不能支持精确的 once，如果你想在 S3 文件系统中使用 Hive 流媒体汇，你可以在 TableConfig 中把以下参数配置为 false，以使用 Flink 原生写入器（只对 parquet 和 orc 有效）（注意这些参数会影响所有作业的汇）。\n   Key Default Type Description     table.exec.hive.fallback-mapred-writer true Boolean 如果是假的，用 flink native writer 写 parquet 和 orc 文件；如果是真的，用 hadoop mapred record writer 写 parquet 和 orc 文件。    下面展示了如何使用流接收器写一个流式查询，将数据从 Kafka 写到一个有 partition-commit 的 Hive 表中，并运行一个批处理查询将这些数据读回。\nSETtable.sql-dialect=hive;CREATETABLEhive_table(user_idSTRING,order_amountDOUBLE)PARTITIONEDBY(dtSTRING,hrSTRING)STOREDASparquetTBLPROPERTIES(\u0026#39;partition.time-extractor.timestamp-pattern\u0026#39;=\u0026#39;$dt $hr:00:00\u0026#39;,\u0026#39;sink.partition-commit.trigger\u0026#39;=\u0026#39;partition-time\u0026#39;,\u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;,\u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;metastore,success-file\u0026#39;);SETtable.sql-dialect=default;CREATETABLEkafka_table(user_idSTRING,order_amountDOUBLE,log_tsTIMESTAMP(3),WATERMARKFORlog_tsASlog_ts-INTERVAL\u0026#39;5\u0026#39;SECOND)WITH(...);-- streaming sql, insert into hive table INSERTINTOTABLEhive_tableSELECTuser_id,order_amount,DATE_FORMAT(log_ts,\u0026#39;yyyy-MM-dd\u0026#39;),DATE_FORMAT(log_ts,\u0026#39;HH\u0026#39;)FROMkafka_table;-- batch sql, select with partition pruning SELECT*FROMhive_tableWHEREdt=\u0026#39;2020-05-20\u0026#39;andhr=\u0026#39;12\u0026#39;;流式读取 为了提高 hive 读取的实时性，Flink 支持实时 Hive 表流读取。\n 分区表，监控分区的生成，并逐步读取新分区。 非分区表，监控文件夹中新文件的生成，并增量读取新文件。  甚至可以采用 10 分钟级别的分区策略，利用 Flink 的 Hive 流式读取和 Hive 流式写入，大大提高 Hive 数据仓库的实时性能，达到准实时分钟级别。\n   Key Default Type Description     streaming-source.enable false Boolean 是否启用流媒体源。注意：请确保每个分区/文件都是以原子方式写入，否则读者可能会得到不完整的数据。请确保每个分区/文件都应该以原子方式写入，否则读者可能会得到不完整的数据。   streaming-source.monitor-interval 1 m Duration 连续监控分区/文件的时间间隔。   streaming-source.consume-order create-time String 流源的消耗顺序，支持 create-time 和 partition-time。create-time 比较的是分区/文件的创建时间，这不是 Hive metaStore 中的分区创建时间，而是文件系统中的文件夹/文件修改时间；partition-time 比较的是分区名称所代表的时间，如果分区文件夹以某种方式得到更新，比如在文件夹中添加新文件，就会影响数据的消耗方式。对于非分区表，这个值应该一直是 \u0026ldquo;创建时间\u0026rdquo;。   streaming-source.consume-start-offset 1970-00-00 String 流式消费的起始偏移量。如何解析和比较偏移量取决于你的顺序。对于创建时间和分区时间，应该是一个时间戳字符串（yyyy-[m]m-[d]d [hh:mm:ss]）。对于分区时间，将使用分区时间提取器从分区中提取时间。    注意:\n 监控策略是现在扫描位置路径中的所有目录/文件。如果分区太多，会出现性能问题。 非分区的流式读取需要将每个文件原子地放入目标目录中。 分区的流式读取要求在 hive metastore 的视图中原子地添加每个分区。这意味着新添加到现有分区的数据不会被消耗掉。 流读取不支持 Flink DDL 中的水印语法。所以它不能用于窗口操作符。  下面展示了如何增量读取 Hive 表。\nSELECT*FROMhive_table/*+ OPTIONS(\u0026#39;streaming-source.enable\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;streaming-source.consume-start-offset\u0026#39;=\u0026#39;2020-05-20\u0026#39;) */;Hive 表作为临时表 您可以使用 Hive 表作为时态表，并将流式数据加入其中。请按照示例来了解如何连接一个时态表。\n在执行 join 时，Hive 表将被缓存在 TM 内存中，并在 Hive 表中查找来自流的每一条记录，以决定是否找到匹配。你不需要任何额外的设置就可以使用 Hive 表作为时态表。但可以选择用以下属性配置 Hive 表缓存的 TTL。缓存过期后，将再次扫描 Hive 表以加载最新的数据。\n   Key Default Type Description     lookup.join.cache.ttl 60 min Duration 在查找连接中构建表的缓存 TTL（例如 10 分钟）。默认情况下，TTL 为 60 分钟。    注意:\n 每个加入子任务都需要保留自己的 Hive 表的缓存。请确保 Hive 表可以放入 TM 任务槽的内存中。 你应该为 lookup.join.cache.ttl 设置一个相对较大的值。如果你的 Hive 表需要太频繁的更新和重载，你可能会有性能问题。 目前，每当缓存需要刷新时，我们只是简单地加载整个 Hive 表。没有办法区分新数据和旧数据。  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","Hive"],"title":"Hive Streaming"},{"categories":["Flink"],"contents":"Hive 函数 通过 HiveModule 使用 Hive 内置功能 HiveModule 将 Hive 内置函数作为 Flink 系统（内置）函数提供给 Flink SQL 和 Table API 用户。\n具体信息请参考 HiveModule。\n Scala  val name = \u0026#34;myhive\u0026#34; val version = \u0026#34;2.3.4\u0026#34; tableEnv.loadModue(name, new HiveModule(version));  YAML  modules:- name:coretype:core- name:myhivetype:hive 注意，旧版本中的一些 Hive 内置功能存在线程安全问题。我们建议用户给自己的 Hive 打上补丁来修复它们。  Hive 用户定义的函数 用户可以在 Flink 中使用他们现有的 Hive 用户定义函数。\n支持的 UDF 类型包括:\n UDF GenericUDF GenericUDTF UDAF GenericUDAFResolver2  在查询规划和执行时，Hive 的 UDF 和 GenericUDF 会自动翻译成 Flink 的 ScalarFunction，Hive 的 GenericUDTF 会自动翻译成 Flink 的 TableFunction，Hive 的 UDAF 和 GenericUDAFResolver2 会翻译成 Flink 的 AggregateFunction。\n要使用 Hive 的用户定义函数，用户必须做到:\n 设置一个由 Hive Metastore 支持的 HiveCatalog 作为会话的当前目录，其中包含该函数。 在 Flink 的 classpath 中加入一个包含该函数的 jar。 使用 Blink 计划器。  使用 Hive 用户定义函数 假设我们在 Hive Metastore 中注册了以下 Hive 函数。\n/** * Test simple udf. Registered under name \u0026#39;myudf\u0026#39; */ public class TestHiveSimpleUDF extends UDF { public IntWritable evaluate(IntWritable i) { return new IntWritable(i.get()); } public Text evaluate(Text text) { return new Text(text.toString()); } } /** * Test generic udf. Registered under name \u0026#39;mygenericudf\u0026#39; */ public class TestHiveGenericUDF extends GenericUDF { @Override public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException { checkArgument(arguments.length == 2); checkArgument(arguments[1] instanceof ConstantObjectInspector); Object constant = ((ConstantObjectInspector) arguments[1]).getWritableConstantValue(); checkArgument(constant instanceof IntWritable); checkArgument(((IntWritable) constant).get() == 1); if (arguments[0] instanceof IntObjectInspector || arguments[0] instanceof StringObjectInspector) { return arguments[0]; } else { throw new RuntimeException(\u0026#34;Not support argument: \u0026#34; + arguments[0]); } } @Override public Object evaluate(DeferredObject[] arguments) throws HiveException { return arguments[0].get(); } @Override public String getDisplayString(String[] children) { return \u0026#34;TestHiveGenericUDF\u0026#34;; } } /** * Test split udtf. Registered under name \u0026#39;mygenericudtf\u0026#39; */ public class TestHiveUDTF extends GenericUDTF { @Override public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException { checkArgument(argOIs.length == 2); // TEST for constant arguments \tcheckArgument(argOIs[1] instanceof ConstantObjectInspector); Object constant = ((ConstantObjectInspector) argOIs[1]).getWritableConstantValue(); checkArgument(constant instanceof IntWritable); checkArgument(((IntWritable) constant).get() == 1); return ObjectInspectorFactory.getStandardStructObjectInspector( Collections.singletonList(\u0026#34;col1\u0026#34;), Collections.singletonList(PrimitiveObjectInspectorFactory.javaStringObjectInspector)); } @Override public void process(Object[] args) throws HiveException { String str = (String) args[0]; for (String s : str.split(\u0026#34;,\u0026#34;)) { forward(s); forward(s); } } @Override public void close() { } } 从 Hive CLI 中，我们可以看到他们已经注册了。\nhive\u0026gt; show functions; OK ...... mygenericudf myudf myudtf 然后，用户可以在 SQL 中使用它们作为。\nFlink SQL\u0026gt; select mygenericudf(myudf(name), 1) as a, mygenericudf(myudf(age), 1) as b, s from mysourcetable, lateral table(myudtf(name, 1)) as T(s); 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_functions.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","Hive"],"title":"Hive 函数"},{"categories":["Flink"],"contents":"Hive 方言 从 1.11.0 开始，当使用 Hive 方言时，Flink 允许用户用 Hive 语法编写 SQL 语句。通过提供与 Hive 语法的兼容性，我们旨在提高与 Hive 的互操作性，减少用户为了执行不同的语句而需要在 Flink 和 Hive 之间切换的情况。\n使用 Hive 方言 Flink 目前支持两种 SQL 方言：默认和 Hive。在使用 Hive 语法编写之前，需要先切换到 Hive 方言。下面介绍如何通过 SQL Client 和 Table API 来设置方言。同时注意，你可以为你执行的每一条语句动态切换方言。不需要重新启动会话来使用不同的方言。\nSQL 客户端 SQL 方言可以通过 table.sql-dialect 属性来指定，因此你可以在你的 SQL 客户端的 yaml 文件的配置部分设置初始方言。因此，你可以在 SQL 客户端的 yaml 文件的配置部分设置要使用的初始方言。\nexecution:planner:blinktype:batchresult-mode:tableconfiguration:table.sql-dialect:hive你也可以在 SQL 客户端启动后设置方言。\nFlink SQL\u0026gt; set table.sql-dialect=hive; -- to use hive dialect [INFO] Session property has been set. Flink SQL\u0026gt; set table.sql-dialect=default; -- to use default dialect [INFO] Session property has been set. Table API You can set dialect for your TableEnvironment with Table API.\nfrom pyflink.table import * settings = EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build() t_env = BatchTableEnvironment.create(environment_settings=settings) # to use hive dialect t_env.get_config().set_sql_dialect(SqlDialect.HIVE) # to use default dialect t_env.get_config().set_sql_dialect(SqlDialect.DEFAULT) DDL 本节列出了 Hive 方言支持的 DDL。在这里我们将主要关注语法。关于每个 DDL 语句的语义，你可以参考 Hive 文档。\nDATABASE  Show  SHOWDATABASES; Create  CREATE(DATABASE|SCHEMA)[IFNOTEXISTS]database_name[COMMENTdatabase_comment][LOCATIONfs_path][WITHDBPROPERTIES(property_name=property_value,...)]; Alter  更新属性\nALTER(DATABASE|SCHEMA)database_nameSETDBPROPERTIES(property_name=property_value,...);更新所有者\nALTER(DATABASE|SCHEMA)database_nameSETOWNER[USER|ROLE]user_or_role;更新位置\nALTER(DATABASE|SCHEMA)database_nameSETLOCATIONfs_path; Drop  DROP(DATABASE|SCHEMA)[IFEXISTS]database_name[RESTRICT|CASCADE]; Use  USEdatabase_name;TABLE  Show  SHOWTABLES; Create  CREATE[EXTERNAL]TABLE[IFNOTEXISTS]table_name[(col_namedata_type[column_constraint][COMMENTcol_comment],...[table_constraint])][COMMENTtable_comment][PARTITIONEDBY(col_namedata_type[COMMENTcol_comment],...)][[ROWFORMATrow_format][STOREDASfile_format]][LOCATIONfs_path][TBLPROPERTIES(property_name=property_value,...)]row_format::DELIMITED[FIELDSTERMINATEDBYchar[ESCAPEDBYchar]][COLLECTIONITEMSTERMINATEDBYchar][MAPKEYSTERMINATEDBYchar][LINESTERMINATEDBYchar][NULLDEFINEDASchar]|SERDEserde_name[WITHSERDEPROPERTIES(property_name=property_value,...)]file_format::SEQUENCEFILE|TEXTFILE|RCFILE|ORC|PARQUET|AVRO|INPUTFORMATinput_format_classnameOUTPUTFORMAToutput_format_classnamecolumn_constraint::NOTNULL[[ENABLE|DISABLE][VALIDATE|NOVALIDATE][RELY|NORELY]]table_constraint::[CONSTRAINTconstraint_name]PRIMARYKEY(col_name,...)[[ENABLE|DISABLE][VALIDATE|NOVALIDATE][RELY|NORELY]] Alter  重命名\nALTER TABLE table_name RENAME TO new_table_name; 更新属性\nALTERTABLEtable_nameSETTBLPROPERTIES(property_name=property_value,property_name=property_value,...);更新位置\nALTERTABLEtable_name[PARTITIONpartition_spec]SETLOCATIONfs_path;partition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。\n更新文件格式\nALTERTABLEtable_name[PARTITIONpartition_spec]SETFILEFORMATfile_format;partition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。\n更新 SerDe 属性\nALTERTABLEtable_name[PARTITIONpartition_spec]SETSERDEserde_class_name[WITHSERDEPROPERTIESserde_properties];ALTERTABLEtable_name[PARTITIONpartition_spec]SETSERDEPROPERTIESserde_properties;serde_properties::(property_name=property_value,property_name=property_value,...)partition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。\n添加分区\nALTERTABLEtable_nameADD[IFNOTEXISTS](PARTITIONpartition_spec[LOCATIONfs_path])+; Drop Partitions  ALTERTABLEtable_nameDROP[IFEXISTS]PARTITIONpartition_spec[,PARTITIONpartition_spec,...]; 新增/替换 列  ALTERTABLEtable_nameADD|REPLACECOLUMNS(col_namedata_type[COMMENTcol_comment],...)[CASCADE|RESTRICT] Change Column  ALTERTABLEtable_nameCHANGE[COLUMN]col_old_namecol_new_namecolumn_type[COMMENTcol_comment][FIRST|AFTERcolumn_name][CASCADE|RESTRICT]; Drop  DROPTABLE[IFEXISTS]table_name;VIEW  Create  CREATEVIEW[IFNOTEXISTS]view_name[(column_name,...)][COMMENTview_comment][TBLPROPERTIES(property_name=property_value,...)]ASSELECT...; Alter  注意：改变视图只在表 API 中工作，但不支持通过 SQL 客户端。\n重命名\nALTERVIEWview_nameRENAMETOnew_view_name;更新属性\nALTERVIEWview_nameSETTBLPROPERTIES(property_name=property_value,...); Update As Select  ALTERVIEWview_nameASselect_statement; Drop  DROPVIEW[IFEXISTS]view_name;FUNCTION  Show  SHOWFUNCTIONS; Create  CREATEFUNCTIONfunction_nameASclass_name;Drop\nDROPFUNCTION[IFEXISTS]function_name;DML NSERT INSERT(INTO|OVERWRITE)[TABLE]table_name[PARTITIONpartition_spec]SELECT...;partition_spec，如果存在的话，可以是完整规格或部分规格。如果 partition_spec 是部分规格，动态分区列名可以省略。\nDQL 目前，Hive 方言支持的 DQL 语法与 Flink SQL 相同。详情请参考 Flink SQL 查询。而且建议切换到默认方言来执行 DQL。\n注意事项 以下是使用 Hive 方言的一些注意事项。\n Hive 方言只能用于操作 Hive 表，而不是通用表。而且 Hive 方言应该和 HiveCatalog 一起使用。 虽然所有的 Hive 版本都支持相同的语法，但是否有特定的功能还是取决于你使用的 Hive 版本。例如，更新数据库位置只在 Hive-2.4.0 或更高版本中支持。 Hive 和 Calcite 有不同的保留关键字集。例如，在 Calcite 中默认是保留关键字，而在 Hive 中是非保留关键字。即使是 Hive 方言，你也必须用反引号（`）来引用这些关键字，才能将它们作为标识符使用。 由于扩展查询不兼容，在 Flink 中创建的视图不能在 Hive 中查询。  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_dialect.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","Hive"],"title":"Hive 方言"},{"categories":["Flink"],"contents":"Hive 集成 Apache Hive 已经确立了自己作为数据仓库生态系统的焦点。它不仅是大数据分析和 ETL 的 SQL 引擎，也是一个数据管理平台，在这里，数据被发现、定义和发展。\nFlink 与 Hive 提供了两方面的整合。\n第一是利用 Hive 的 Metastore 作为一个持久性目录，与 Flink 的 HiveCatalog 进行跨会话存储 Flink 特定的元数据。例如，用户可以通过使用 HiveCatalog 将 Kafka 或 ElasticSearch 表存储在 Hive Metastore 中，并在以后的 SQL 查询中重复使用。\n二是提供 Flink 作为读写 Hive 表的替代引擎。\nHiveCatalog 的设计是 \u0026ldquo;开箱即用\u0026rdquo;，与现有的 Hive 安装兼容。您不需要修改现有的 Hive Metastore，也不需要改变数据位置或表的分区。\n支持的 Hive 版本 Flink 支持以下 Hive 版本。\n 1.0  1.0.0 1.0.1   1.1  1.1.0 1.1.1   1.2  1.2.0 1.2.1 1.2.2   2.0  2.0.0 2.0.1   2.1  2.1.0 2.1.1   2.2  2.2.0   2.3  2.3.0 2.3.1 2.3.2 2.3.3 2.3.4 2.3.5 2.3.6   3.1  3.1.0 3.1.1 3.1.2    请注意 Hive 本身在不同的版本有不同的功能，这些问题不是 Flink 造成的。\n 1.2.0 及以后版本支持 Hive 内置函数。 3.1.0 及以后版本支持列约束，即 PRIMARY KEY 和 NOT NULL。 在 1.2.0 及以后的版本中，支持修改表的统计数据。 在 1.2.0 及以后的版本中支持 DATE 列统计。 在 2.0.x 中不支持写入 ORC 表。  依赖性 为了与 Hive 集成，你需要在 Flink 发行版的 /lib/ 目录下添加一些额外的依赖关系，以使集成工作在 Table API 程序或 SQL 客户端中。另外，你也可以将这些依赖项放在一个专门的文件夹中，并分别用 -C 或 -l 选项将它们添加到 classpath 中，用于 Table API 程序或 SQL Client。\nApache Hive 是建立在 Hadoop 上的，所以首先需要 Hadoop 依赖，请参考提供 Hadoop 类。\n有两种方法可以添加 Hive 依赖。首先是使用 Flink 的捆绑式 Hive jars。你可以根据你使用的 metastore 的版本来选择捆绑的 Hive jar。第二种是分别添加每个所需的 jar。如果你使用的 Hive 版本没有在这里列出，第二种方式就会很有用。\n使用捆绑的 Hive jar 下表列出了所有可用的捆绑的 hive jar，你可以选择一个到 Flink 发行版的 /lib/ 目录下。\n   Metastore version Maven dependency SQL Client JAR     1.0.0 - 1.2.2 flink-sql-connector-hive-1.2.2 Download   2.0.0 - 2.2.0 flink-sql-connector-hive-2.2.0 Download   2.3.0 - 2.3.6 flink-sql-connector-hive-2.3.6 Download   3.0.0 - 3.1.2 flink-sql-connector-hive-3.1.2 Download    用户定义的依赖性 请在下面找到不同 Hive 主要版本所需的依赖关系。\n Hive 2.3.4  /flink-1.11.0 /lib // Flink's Hive connector.Contains flink-hadoop-compatibility and flink-orc jars flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.3.4.jar  Hive 1.0.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-metastore-1.0.0.jar hive-exec-1.0.0.jar libfb303-0.9.0.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core  Hive 1.1.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-metastore-1.1.0.jar hive-exec-1.1.0.jar libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core  Hive 1.2.1  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-metastore-1.2.1.jar hive-exec-1.2.1.jar libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core  Hive 2.0.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.0.0.jar  Hive 2.1.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.1.0.jar  Hive 2.2.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.2.0.jar // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3.jar aircompressor-0.8.jar // transitive dependency of orc-core  Hive 3.1.0  /flink-1.11.0 /lib // Flink's Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-3.1.0.jar libfb303-0.9.3.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately Program maven 如果你正在构建你自己的程序，你需要在你的 mvn 文件中加入以下依赖关系。建议不要在生成的 jar 文件中包含这些依赖关系。你应该在运行时添加上面所说的依赖关系。\n\u0026lt;!-- Flink Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-hive_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Hive Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hive-exec\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${hive.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 连接到 Hive 通过表环境或 YAML 配置，使用目录接口和 HiveCatalog 连接到现有的 Hive 安装。\n如果 hive-conf/hive-site.xml 文件存储在远程存储系统中，用户应先将 hive 配置文件下载到本地环境中。\n请注意，虽然 HiveCatalog 不需要特定的规划师，但读/写 Hive 表只适用于 blink 规划师。因此强烈建议您在连接 Hive 仓库时使用 blink planner。\nHiveCatalog 能够自动检测使用中的 Hive 版本。建议不要指定 Hive 版本，除非自动检测失败。\n以 Hive 2.3.4 版本为例。\nval settings = EnvironmentSettings.newInstance().inBatchMode().build() val tableEnv = TableEnvironment.create(settings) val name = \u0026#34;myhive\u0026#34; val defaultDatabase = \u0026#34;mydatabase\u0026#34; val hiveConfDir = \u0026#34;/opt/hive-conf\u0026#34; // a local path  val hive = new HiveCatalog(name, defaultDatabase, hiveConfDir) tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, hive) // set the HiveCatalog as the current catalog of the session tableEnv.useCatalog(\u0026#34;myhive\u0026#34;) DDL 建议使用 Hive 方言在 Flink 中执行 DDL 来创建 Hive 表、视图、分区、函数。\nDML Flink 支持 DML 写入 Hive 表。请参考读写 Hive 表的细节。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","Hive"],"title":"Hive 集成 - 概览"},{"categories":["Flink"],"contents":"HiveCatalog Hive Metastore 经过多年的发展，已经成为 Hadoop 生态系统中事实上的元数据中心。很多公司在生产中都有一个 Hive Metastore 服务实例来管理他们所有的元数据，无论是 Hive 元数据还是非 Hive 元数据，都是真理的来源。\n对于同时部署了 Hive 和 Flink 的用户，HiveCatalog 可以让他们使用 Hive Metastore 来管理 Flink 的元数据。\n对于只有 Flink 部署的用户来说，HiveCatalog 是 Flink 开箱即用的唯一持久化目录。如果没有持久化目录，用户使用 Flink SQL CREATE DDL 必须在每个会话中反复创建元对象，比如 Kafka 表，这就浪费了很多时间。HiveCatalog 填补了这一空白，使用户只需创建一次表和其他元对象，以后就可以跨会话方便地引用和管理它们。\n设置 HiveCatalog 依赖性 在 Flink 中设置 HiveCatalog 需要与 Flink-Hive 集成相同的依赖关系。\n配置 在 Flink 中设置 HiveCatalog 需要与 Flink-Hive 集成相同的配置。\n如何使用 HiveCatalog 一旦配置得当，HiveCatalog 应该可以直接使用。用户可以用 DDL 创建 Flink 元对象，并在创建后立即看到它们。\nHiveCatalog 可以用来处理两种表。Hive 兼容表和通用表。Hive-compatible 表是指那些以 Hive 兼容的方式存储的表，在存储层的元数据和数据方面都是如此。因此，通过 Flink 创建的 Hive 兼容表可以从 Hive 端进行查询。\n而通用表则是针对 Flink 的。当使用 HiveCatalog 创建通用表时，我们只是使用 HMS 来持久化元数据。虽然这些表对 Hive 是可见的，但 Hive 不太可能理解这些元数据。因此在 Hive 中使用这样的表会导致未定义的行为。\nFlink 使用属性 \u0026ldquo;is_generic\u0026rdquo; 来判断一个表是与 Hive 兼容还是通用。当用 HiveCatalog 创建一个表时，它默认被认为是通用的。如果你想创建一个 Hive 兼容的表，请确保在你的表属性中把 is_generic 设置为 false。\n如上所述，通用表不应该从 Hive 使用。在 Hive CLI 中，你可以调用 describe FORMATTED 对一个表进行检查，通过检查 is_generic 属性来决定它是否是通用的。通用表会有 is_generic=true。\n例子 我们在这里将通过一个简单的例子进行讲解。\n第 1 步：设置 Hive Metastore。\n有一个 Hive Metastore 在运行。\n在这里，我们在本地路径 /opt/hive-conf/hive-site.xml 中设置一个本地 Hive Metastore 和我们的 hive-site.xml 文件。我们有如下的一些配置。\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;metadata is stored in a MySQL server\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;MySQL JDBC driver class\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;...\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;user name for connecting to mysql server\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;...\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;password for connecting to mysql server\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.uris\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;thrift://localhost:9083\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;IP address (or fully-qualified domain name) and port of the metastore host\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.schema.verification\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 用 Hive Cli 测试连接到 HMS。运行一些命令，我们可以看到我们有一个名为 default 的数据库，里面没有表。\nhive\u0026gt; show databases; OK default Time taken: 0.032 seconds, Fetched: 1 row(s) hive\u0026gt; show tables; OK Time taken: 0.028 seconds, Fetched: 0 row(s) 步骤 2：配置 Flink 集群和 SQL CLI\n将所有 Hive 的依赖关系添加到 Flink 发行版的 /lib 目录下，并修改 SQL CLI 的 yaml 配置文件 sql-cli-defaults.yaml 如下。\nexecution:planner:blinktype:streaming...current-catalog:myhive # set the HiveCatalog as the current catalog of the sessioncurrent-database:mydatabasecatalogs:- name:myhivetype:hivehive-conf-dir:/opt/hive-conf # contains hive-site.xml第三步：建立 Kafka 集群\nBootstrap 一个本地的 Kafka 2.3.0 集群，主题命名为 \u0026ldquo;test\u0026rdquo;，并以 name 和 age 的元组形式向主题产生一些简单的数据。\nlocalhost$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test \u0026gt;tom,15 \u0026gt;john,21 这些消息可以通过启动 Kafka 控制台消费者看到。\nlocalhost$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning tom,15 john,21 第四步：启动 SQL Client，用 Flink SQL DDL 创建一个 Kafka 表。\n启动 Flink SQL Client，通过 DDL 创建一个简单的 Kafka 2.3.0 表，并验证其模式。\nFlinkSQL\u0026gt;CREATETABLEmykafka(nameString,ageInt)WITH(\u0026#39;connector.type\u0026#39;=\u0026#39;kafka\u0026#39;,\u0026#39;connector.version\u0026#39;=\u0026#39;universal\u0026#39;,\u0026#39;connector.topic\u0026#39;=\u0026#39;test\u0026#39;,\u0026#39;connector.properties.bootstrap.servers\u0026#39;=\u0026#39;localhost:9092\u0026#39;,\u0026#39;format.type\u0026#39;=\u0026#39;csv\u0026#39;,\u0026#39;update-mode\u0026#39;=\u0026#39;append\u0026#39;);[INFO]Tablehasbeencreated.FlinkSQL\u0026gt;DESCRIBEmykafka;root|-- name: STRING |-- age: INT 验证该表也是通过 Hive Cli 对 Hive 可见的，注意该表有属性 is_generic=true。\nhive\u0026gt; show tables; OK mykafka Time taken: 0.038 seconds, Fetched: 1 row(s) hive\u0026gt; describe formatted mykafka; OK # col_name data_type comment # Detailed Table Information Database: default Owner: null CreateTime: ...... LastAccessTime: UNKNOWN Retention: 0 Location: ...... Table Type: MANAGED_TABLE Table Parameters: flink.connector.properties.bootstrap.servers\tlocalhost:9092 flink.connector.topic\ttest flink.connector.type\tkafka flink.connector.version\tuniversal flink.format.type csv flink.generic.table.schema.0.data-type\tVARCHAR(2147483647) flink.generic.table.schema.0.name\tname flink.generic.table.schema.1.data-type\tINT flink.generic.table.schema.1.name\tage flink.update-mode append is_generic true transient_lastDdlTime\t...... # Storage Information SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: serialization.format\t1 Time taken: 0.158 seconds, Fetched: 36 row(s) 第五步：运行 Flink SQL 查询 Kakfa 表。\n在 Flink 集群中，无论是单机还是 yarn-session，从 Flink SQL Client 中运行一个简单的选择查询。\nFlink SQL\u0026gt; select * from mykafka; 在 Kafka 主题中多产生一些信息。\nlocalhost$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning tom,15 john,21 kitty,30 amy,24 kaiky,18 你现在应该可以在 SQL Client 中看到 Flink 产生的结果，如。\n SQL Query Result (Table) Refresh: 1 s Page: Last of 1 name age tom 15 john 21 kitty 30 amy 24 kaiky 18 支持的类型 HiveCatalog 支持通用表的所有 Flink 类型。\n对于 Hive 兼容的表，HiveCatalog 需要将 Flink 数据类型映射到相应的 Hive 类型，如下表所述。\n   Flink Data Type Hive Data Type     CHAR(p) CHAR(p)   VARCHAR(p) VARCHAR(p)   STRING STRING   BOOLEAN BOOLEAN   TINYINT TINYINT   SMALLINT SMALLINT   INT INT   BIGINT LONG   FLOAT FLOAT   DOUBLE DOUBLE   DECIMAL(p, s) DECIMAL(p, s)   DATE DATE   TIMESTAMP(9) TIMESTAMP   BYTES BINARY   ARRAY LIST   MAP\u0026lt;K, V\u0026gt; MAP\u0026lt;K, V\u0026gt;   ROW STRUCT    关于类型映射需要注意的地方。\n Hive 的 CHAR(p) 最大长度为 255。 Hive 的 VARCHAR(p) 最大长度为 65535。 Hive 的 MAP 只支持基元键类型，而 Flink 的 MAP 可以是任何数据类型。 不支持 Hive 的 UNION 类型。 Hive 的 TIMESTAMP 的精度总是 9，不支持其他精度。而 Hive 的 UDF 则可以处理精度\u0026lt;=9 的 TIMESTAMP 值。 Hive 不支持 Flink 的 TIMESTAMP_WITH_TIME_ZONE、TIMESTAMP_WITH_LOCAL_TIME_ZONE 和 MULTISET。 Flink 的 INTERVAL 类型还不能映射到 Hive 的 INTERVAL 类型。  Scala Shell 注意：由于目前 Scala Shell 并不支持 blink planner，所以不建议在 Scala Shell 中使用 Hive 连接器。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_catalog.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-hivecatalog/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"HiveCatalog"},{"categories":["Flink"],"contents":"流式聚合 SQL 是数据分析中使用最广泛的语言。Flink 的 Table API 和 SQL 可以让用户用更少的时间和精力定义高效的流分析应用。此外，Flink Table API 和 SQL 还进行了有效的优化，它集成了大量的查询优化和调整后的运算符实现。但并不是所有的优化都是默认启用的，所以对于一些工作负载，可以通过开启一些选项来提高性能。\n在本页面中，我们将介绍一些有用的优化选项和流式聚合的内部结构，在某些情况下会带来很大的改善。\n注意: 目前，本页面中提到的优化选项仅在 Blink 计划器中支持。\n注意: 目前，流式聚合的优化只支持无边界聚合。未来将支持窗口聚合的优化。\n默认情况下，无界聚合运算符对输入记录进行逐一处理，即：（1）从状态中读取累加器，（2）累加/缩减记录到累加器，（3）将累加器写回状态，（4）下一条记录将从（1）开始重新做处理。这种处理模式可能会增加 StateBackend 的开销（尤其是对于 RocksDB StateBackend）。此外，在生产中很常见的数据偏斜也会使问题更加严重，容易出现作业背压的情况。\n迷你批处理(MiniBatch)聚合 迷你批处理(mini-batch)聚合的核心思想是将一捆输入缓存在聚合运算器内部的缓冲区中。当触发处理该捆输入时，每个键只需要一个操作来访问状态。这样可以大大降低状态开销，获得更好的吞吐量。但是，这可能会增加一些延迟，因为它缓冲了一些记录，而不是在瞬间处理它们。这就是吞吐量和延迟之间的权衡。\n下图解释了迷你批处理聚合如何减少状态操作。\nMiniBatch 优化默认为禁用。为了启用此优化，您应该设置选项 table.exec.mini-batch.enabled、table.exec.mini-batch.allow-latency 和 table.exec.mini-batch.size。请参阅配置页面了解更多详情。\n下面的示例展示了如何启用这些选项。\n// instantiate table environment val tEnv: TableEnvironment = ... // access flink configuration val configuration = tEnv.getConfig().getConfiguration() // set low-level key-value options configuration.setString(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) // enable mini-batch optimization configuration.setString(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) // use 5 seconds to buffer input records configuration.setString(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) // the maximum number of records can be buffered by each aggregate operator task Local-Global 聚合 Local-Global 是为了解决数据偏斜问题而提出的，将一个分组聚合分为两个阶段，即先在上游做局部聚合，然后在下游做全局聚合，这类似于 MapReduce 中的 Combine+Reduce 模式。例如，考虑以下 SQL。\nSELECTcolor,sum(id)FROMTGROUPBYcolor有可能数据流中的记录是倾斜的，因此一些聚合运算符实例要处理的记录比其他实例多得多，从而导致热点。本地聚合可以帮助将一定数量的具有相同键的输入累积到一个累积器中。全局聚合将只接收减少的累加器，而不是大量的原始输入。这可以显著降低网络洗牌和状态访问的成本。本地聚合每次积累的输入数量是基于小批量间隔的。这意味着本地-全局聚合取决于迷你批量优化的启用。\n下图显示了本地-全局聚合如何提高性能。\n下面的例子展示了如何启用本地-全局聚合。\n// instantiate table environment val tEnv: TableEnvironment = ... // access flink configuration val configuration = tEnv.getConfig().getConfiguration() // set low-level key-value options configuration.setString(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) // local-global aggregation depends on mini-batch is enabled configuration.setString(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) configuration.setString(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) configuration.setString(\u0026#34;table.optimizer.agg-phase-strategy\u0026#34;, \u0026#34;TWO_PHASE\u0026#34;) // enable two-phase, i.e. local-global aggregation Split Distinct 聚合 Local-Global 优化对于一般的聚合，如 SUM、COUNT、MAX、MIN、AVG，可以有效地消除数据偏斜。但在处理不同的聚合时，其性能并不理想。\n例如，如果我们想分析今天有多少独特的用户登录。我们可以有如下查询:\nSELECTday,COUNT(DISTINCTuser_id)FROMTGROUPBYday如果 distinct key（即 user_id）的值很稀疏，count distinct 不好减少记录。即使启用了局部-全局优化，也没有什么帮助。因为累加器仍然包含了几乎所有的原始记录，而全局聚合将是瓶颈（大部分重度累加器是由一个任务处理的，即在同一天）。\n这个优化的思路是将不同的聚合（如 COUNT(DISTINCT col)）分成两个层次。第一层聚合由组键和一个额外的桶键进行洗牌。桶键使用 HASH_CODE(distinct_key) % BUCKET_NUM 计算。BUCKET_NUM 默认为 1024，可以通过 table.optimizer.distinct-agg.split.bucket-num 选项进行配置。第二次聚合是按原组键进行洗牌，用 SUM 聚合不同桶的 COUNT DISTINCT 值。因为相同的 distinct key 只会在同一个 bucket 中计算，所以转换是等价的。桶键起到了额外的组键的作用，分担组键中热点的负担。桶键使得工作具有可扩展性，可以解决 distinct aggregations 中的数据偏斜/热点问题。\n拆分不同的聚合后，上面的查询会自动改写成下面的查询:\nSELECTday,SUM(cnt)FROM(SELECTday,COUNT(DISTINCTuser_id)ascntFROMTGROUPBYday,MOD(HASH_CODE(user_id),1024))GROUPBYday下图显示了拆分不同的聚合如何提高性能（假设颜色代表天数，字母代表 user_id）。\n注：以上是最简单的例子，可以从这个优化中受益。除此之外，Flink 还支持拆分更复杂的聚合查询，例如，多个不同键的不同聚合（如 COUNT(DISTINCT a)，SUM(DISTINCT b)），以及与其他非不同聚合（如 SUM, MAX, MIN, COUNT）一起工作。\n注意，目前，分割优化不支持包含用户定义的 AggregateFunction 的聚合。\n下面的例子展示了如何启用拆分不同的聚合优化。\n// instantiate table environment val tEnv: TableEnvironment = ... tEnv.getConfig // access high-level configuration  .getConfiguration // set low-level key-value options  .setString(\u0026#34;table.optimizer.distinct-agg.split.enabled\u0026#34;, \u0026#34;true\u0026#34;) // enable distinct agg split 在 Distinct 聚合上使用 FILTER 修饰符 在某些情况下，用户可能需要从不同的维度来计算 UV(唯一访客)的数量，例如：Android 的 UV、iPhone 的 UV、Web 的 UV 以及总的 UV。很多用户会选择 CASE WHEN 来支持，比如。\nSELECTday,COUNT(DISTINCTuser_id)AStotal_uv,COUNT(DISTINCTCASEWHENflagIN(\u0026#39;android\u0026#39;,\u0026#39;iphone\u0026#39;)THENuser_idELSENULLEND)ASapp_uv,COUNT(DISTINCTCASEWHENflagIN(\u0026#39;wap\u0026#39;,\u0026#39;other\u0026#39;)THENuser_idELSENULLEND)ASweb_uvFROMTGROUPBYday但是，在这种情况下，建议使用 FILTER 语法，而不是 CASE WHEN。因为 FILTER 更符合 SQL 标准，会得到更多的性能提升。FILTER 是用于聚合函数上的修饰符，用于限制聚合中使用的值。用 FILTER 修饰符替换上面的例子，如下所示。\nSELECTday,COUNT(DISTINCTuser_id)AStotal_uv,COUNT(DISTINCTuser_id)FILTER(WHEREflagIN(\u0026#39;android\u0026#39;,\u0026#39;iphone\u0026#39;))ASapp_uv,COUNT(DISTINCTuser_id)FILTER(WHEREflagIN(\u0026#39;wap\u0026#39;,\u0026#39;other\u0026#39;))ASweb_uvFROMTGROUPBYdayFlink SQL 优化器可以识别同一个独立键上的不同过滤参数。例如，在上面的例子中，三个 COUNT DISTINCT 都在 user_id 列上。那么 Flink 就可以只使用一个共享状态实例而不是三个状态实例来减少状态访问和状态大小。在一些工作负载中，这可以得到显著的性能提升。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tuning/streaming_aggregation_optimization.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-streaming-aggregation/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"Streaming Aggregation"},{"categories":["Flink"],"contents":"用户自定义源和接收器 动态表是 Flink 的表与 SQL API 的核心概念，用于统一处理有界和无界数据。\n因为动态表只是一个逻辑概念，Flink 并不拥有数据本身。相反，动态表的内容存储在外部系统（如数据库、键值存储、消息队列）或文件中。\n动态源和动态汇可以用来从外部系统读取和写入数据。在文档中，源和汇通常被总结为连接器一词。\nFlink 为 Kafka、Hive 和不同的文件系统提供了预定义的连接器。有关内置表源和汇的更多信息，请参阅连接器部分。\n本页主要介绍如何开发一个自定义的、用户定义的连接器。\n注意: Flink 1.11 中引入了新的表源和表汇接口，作为 FLIP-95 的一部分。同时工厂接口也被重新设计。FLIP-95 还没有完全实现。许多能力接口还不支持(例如用于过滤器或分区推倒)。如果有必要，还请看一下旧 table source 和接收器的页面。为了向后兼容，这些接口仍然被支持。\n概述 在许多情况下，实现者不需要从头开始创建一个新的连接器，而是希望稍微修改现有的连接器或挂入现有的堆栈。在其他情况下，实现者希望创建专门的连接器。\n本节将为这两种用例提供帮助。它解释了表连接器的一般架构，从 API 中的纯声明到将在集群上执行的运行时代码。\n填充的箭头显示了在翻译过程中，对象如何从一个阶段转换到下一个阶段的其他对象。\n元数据 表 API 和 SQL 都是声明式 API。这包括表的声明。因此，执行 CREATE TABLE 语句的结果是更新目标目录中的元数据。\n对于大多数目录实现来说，外部系统中的物理数据不会因为这样的操作而被修改。特定于连接器的依赖关系还不必存在于 classpath 中。在 WITH 子句中声明的选项既不进行验证，也不进行其他解释。\n动态表（通过 DDL 创建或由目录提供）的元数据被表示为 CatalogTable 的实例。表名将在必要时在内部被解析为 CatalogTable。\n计划(Planning) 当涉及到表程序的规划和优化时，需要将 CatalogTable 解析为 DynamicTableSource（用于在 SELECT 查询中读取）和 DynamicTableSink（用于在 INSERT INTO 语句中写入）。\nDynamicTableSourceFactory 和 DynamicTableSinkFactory 提供了连接器特有的逻辑，用于将 CatalogTable 的元数据翻译成 DynamicTableSource 和 DynamicTableSink 的实例。在大多数情况下，工厂的目的是验证选项（如示例中的 \u0026lsquo;port\u0026rsquo; = \u0026lsquo;5022\u0026rsquo;），配置编码/解码格式（如果需要），并创建表连接器的参数化实例。\n默认情况下，DynamicTableSourceFactory 和 DynamicTableSinkFactory 的实例是通过 Java 的服务提供商接口（SPI）发现的。连接器选项（如本例中的\u0026rsquo;连接器'=\u0026lsquo;自定义\u0026rsquo;）必须对应一个有效的工厂标识符。\n虽然在类的命名中可能并不明显，但 DynamicTableSource 和 DynamicTableSink 也可以被看作是有状态的工厂，最终产生具体的运行时实现来读取/写入实际数据。\n规划者使用源和汇实例来执行特定连接器的双向通信，直到找到一个最佳的逻辑计划。根据可选声明的能力接口（如 Supp SupportsProjectionPushDown 或 Supp SupportsOverwrite），规划者可能会对一个实例进行更改，从而对生成的运行时实现进行突变。\n运行时 逻辑规划完成后，规划师将从表连接器中获取运行时实现。运行时逻辑在 Flink 的核心连接器接口中实现，如 InputFormat 或 SourceFunction。\n这些接口被另一层抽象归为 ScanRuntimeProvider、LookupRuntimeProvider 和 SinkRuntimeProvider 的子类。\n例如，OutputFormatProvider(提供 org.apache.flink.api.common.io.OutputFormat) 和 SinkFunctionProvider(提供 org.apache.flink.streaming.api.function.sink.SinkFunction) 都是规划者可以处理的 SinkRuntimeProvider 的具体实例。\n扩展点 本节解释了用于扩展 Flink 的表连接器的可用接口。\n动态表因素 动态表工厂用于根据目录和会话信息为外部存储系统配置动态表连接器。\norg.apache.flink.table.fants.DynamicTableSourceFactory 可以实现来构造一个 DynamicTableSource。\norg.apache.flink.table.fants.DynamicTableSinkFactory 可以被实现来构造一个 DynamicTableSink。\n默认情况下，使用连接器选项的值作为工厂标识符和 Java 的服务提供者接口来发现工厂。\n在 JAR 文件中，可以在服务文件中添加对新实现的引用。\nMETA-INF/services/org.apache.flink.table.factory.Factory。\n框架将检查单个匹配的工厂，该工厂由工厂标识符和请求的基类（如 DynamicTableSourceFactory）唯一识别。\n如果有必要，工厂发现过程可以由目录实现绕过。为此，目录需要在 org.apache.flink.table.catalog.Catalog#getFactory 中返回一个实现请求的基类的实例。\n动态 Table Source 根据定义，动态表可以随时间变化。\n当读取一个动态表时，其内容可以被认为是。\n 一个变化日志（有限的或无限的），所有的变化都会被持续消耗，直到变化日志耗尽。这由 ScanTableSource 接口来表示。 一个持续变化的或非常大的外部表，其内容通常不会被完全读取，而是在必要时查询单个值。这由 LookupTableSource 接口来表示。  一个类可以同时实现这两个接口。规划师根据指定的查询来决定它们的用途。\n扫描 Table Source ScanTableSource 在运行时扫描来自外部存储系统的所有行，扫描的行不一定只包含插入，也可以包含更新和删除。\n扫描的行不一定只包含插入，也可以包含更新和删除。因此，该表源可用于读取（有限或无限）的变更日志。返回的变更日志模式表示计划员在运行时可以预期的变更集。\n对于常规的批处理方案，源可以发出只插入行的有界流。\n对于常规的流式方案，源可以发出只插入行的无界流。\n对于变化数据捕获（CDC）场景，源可以发出有界或无界的流，包含插入、更新和删除行。\nTable Source 可以实现更多的能力接口，如 Supp SupportsProjectionPushDown，可能在规划期间突变一个实例。所有的能力都列在 org.apache.flink.table.connector.source.abilities 包和 org.apache.flink.table.connector.source.ScanTableSource 的文档中。\nScanTableSource 的运行时实现必须产生内部数据结构。因此，记录必须以 org.apache.flink.table.data.RowData 的形式发出。框架提供了运行时转换器，这样一个源仍然可以在普通的数据结构上工作，并在最后进行转换。\n查询 Table Source LookupTableSource 在运行时通过一个或多个键来查找外部存储系统的行。\n与 ScanTableSource 相比，LookupTableSource 不需要读取整个表，可以在必要的时候从外部表（可能是不断变化的）中懒惰地获取单个值。\n与 ScanTableSource 相比，LookupTableSource 目前只支持发出只插入的变化。\n不支持更多的能力。更多信息请参见 org.apache.flink.table.connector.source.LookupTableSource 的文档。\nLookupTableSource 的运行时实现是一个 TableFunction 或 AsyncTableFunction。该函数将在运行时调用给定的查找键的值。\n动态 Table Sink 根据定义，动态表可以随时间变化。\n在编写动态表时，可以始终将内容视为一个 changelog（有限或无限），对于这个 changelog，所有的变化都会被连续写出来，直到 changelog 用完为止。返回的 changelog 模式表明了 sink 在运行时接受的变化集。\n对于常规的批处理方案，sink 可以只接受只插入的行，并写出有界流。\n对于常规的流式方案，sink 可以只接受只插入的行，并且可以写出无约束的流。\n对于变化数据捕获（CDC）场景，table sink 可以写出有界流或无界流，有插入、更新和删除行。\nTable sink 可以实现更多的能力接口，如 SupportsOverwrite，可能在规划期间突变一个实例。所有的能力都列在 org.apache.flink.table.connector.sink.abilities 包和 org.apache.flink.table.connector.sink.DynamicTableSink 的文档中。\nDynamicTableSink 的运行时实现必须消耗内部数据结构。因此，记录必须被接受为 org.apache.flink.table.data.RowData。该框架提供了运行时转换器，这样一个 sink 仍然可以在普通的数据结构上工作，并在开始时执行转换。\n编码/解码格式 一些表连接器接受不同的格式，对键和/或值进行编码和解码。\n格式的工作模式类似于 DynamicTableSourceFactory-\u0026gt;DynamicTableSource-\u0026gt;ScanRuntimeProvider，工厂负责翻译选项，源头负责创建运行时逻辑。\n因为格式可能位于不同的模块中，所以使用 Java 的服务提供者接口发现它们，类似于表工厂。为了发现格式工厂，动态表工厂会搜索与工厂标识符和连接器特定基类相对应的工厂。\n例如，Kafka 表源需要一个 DeserializationSchema 作为解码格式的运行时接口。因此，Kafka 表源工厂使用 value.format 选项的值来发现一个 DeserializationFormatFactory。\n目前支持以下格式工厂。\n org.apache.flink.table.factories.DeserializationFormatFactory org.apache.flink.table.factories.SerializationFormatFactory  格式工厂将选项翻译成 EncodingFormat 或 DecodingFormat。这些接口是另一种工厂，为给定的数据类型产生专门的格式运行时逻辑。\n例如，对于 Kafka table source 工厂，DeserializationFormatFactory 将返回一个 EncodingFormat\u0026lt;DeserializationSchema\u0026gt;，它可以传递到 Kafka 表源中。\n全栈示例 本节简要介绍了如何实现一个扫描表源，其解码格式支持 changelog 语义。这个例子说明了所有提到的组件如何一起发挥作用。它可以作为一个参考实现。\n特别是，它展示了如何\n 创建解析和验证选项的工厂。 实现表连接器。 实现和发现自定义格式。 并使用提供的实用程序，如数据结构转换器和 FactoryUtil。  Table Source 使用一个简单的单线程 SourceFunction 来打开一个监听传入字节的套接字。原始字节由一个可插拔的格式解码成行。该格式期望以 changelog 标志作为第一列。\n我们将使用上面提到的大部分接口来实现下面的 DDL。\nCREATETABLEUserScores(nameSTRING,scoreINT)WITH(\u0026#39;connector\u0026#39;=\u0026#39;socket\u0026#39;,\u0026#39;hostname\u0026#39;=\u0026#39;localhost\u0026#39;,\u0026#39;port\u0026#39;=\u0026#39;9999\u0026#39;,\u0026#39;byte-delimiter\u0026#39;=\u0026#39;10\u0026#39;,\u0026#39;format\u0026#39;=\u0026#39;changelog-csv\u0026#39;,\u0026#39;changelog-csv.column-delimiter\u0026#39;=\u0026#39;|\u0026#39;);由于该格式支持 changelog 语义，我们能够在运行时摄取更新，并创建一个能够持续评估变化数据的更新视图。\nSELECTname,SUM(score)FROMUserScoresGROUPBYname;使用以下命令在终端中摄取数据。\n\u0026gt; nc -lk 9999 INSERT|Alice|12 INSERT|Bob|5 DELETE|Alice|12 INSERT|Alice|18 工厂 本节说明了如何将来自目录的元数据翻译成具体的连接器实例。\n这两个工厂都被添加到 META-INF/services 目录中。\nSocketDynamicTableFactory\nSocketDynamicTableFactory 将目录表翻译成表源。由于表源需要解码格式，为了方便，我们使用提供的 FactoryUtil 发现格式。\nimport org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.configuration.ConfigOption; import org.apache.flink.configuration.ConfigOptions; import org.apache.flink.configuration.ReadableConfig; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.connector.source.DynamicTableSource; import org.apache.flink.table.data.RowData; import org.apache.flink.table.factories.DeserializationFormatFactory; import org.apache.flink.table.factories.DynamicTableSourceFactory; import org.apache.flink.table.factories.FactoryUtil; import org.apache.flink.table.types.DataType; public class SocketDynamicTableFactory implements DynamicTableSourceFactory { // define all options statically  public static final ConfigOption\u0026lt;String\u0026gt; HOSTNAME = ConfigOptions.key(\u0026#34;hostname\u0026#34;) .stringType() .noDefaultValue(); public static final ConfigOption\u0026lt;Integer\u0026gt; PORT = ConfigOptions.key(\u0026#34;port\u0026#34;) .intType() .noDefaultValue(); public static final ConfigOption\u0026lt;Integer\u0026gt; BYTE_DELIMITER = ConfigOptions.key(\u0026#34;byte-delimiter\u0026#34;) .intType() .defaultValue(10); // corresponds to \u0026#39;\\n\u0026#39;  @Override public String factoryIdentifier() { return \u0026#34;socket\u0026#34;; // used for matching to `connector = \u0026#39;...\u0026#39;`  } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; requiredOptions() { final Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; options = new HashSet\u0026lt;\u0026gt;(); options.add(HOSTNAME); options.add(PORT); options.add(FactoryUtil.FORMAT); // use pre-defined option for format  return options; } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; optionalOptions() { final Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; options = new HashSet\u0026lt;\u0026gt;(); options.add(BYTE_DELIMITER); return options; } @Override public DynamicTableSource createDynamicTableSource(Context context) { // either implement your custom validation logic here ...  // or use the provided helper utility  final FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context); // discover a suitable decoding format  final DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; decodingFormat = helper.discoverDecodingFormat( DeserializationFormatFactory.class, FactoryUtil.FORMAT); // validate all options  helper.validate(); // get the validated options  final ReadableConfig options = helper.getOptions(); final String hostname = options.get(HOSTNAME); final int port = options.get(PORT); final byte byteDelimiter = (byte) (int) options.get(BYTE_DELIMITER); // derive the produced data type (excluding computed columns) from the catalog table  final DataType producedDataType = context.getCatalogTable().getSchema().toPhysicalRowDataType(); // create and return dynamic table source  return new SocketDynamicTableSource(hostname, port, byteDelimiter, decodingFormat, producedDataType); } } ChangelogCsvFormatFactory\nChangelogCsvFormatFactory 将特定格式的选项翻译成一种格式。SocketDynamicTableFactory 中的 FactoryUtil 负责相应地调整选项键，并处理像 changelog-csv.column-delimiter 那样的前缀。\n因为这个工厂实现了 DeserializationFormatFactory，所以它也可以用于其他支持反序列化格式的连接器，比如 Kafka 连接器。\nimport org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.configuration.ConfigOption; import org.apache.flink.configuration.ConfigOptions; import org.apache.flink.configuration.ReadableConfig; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.data.RowData; import org.apache.flink.table.factories.FactoryUtil; import org.apache.flink.table.factories.DeserializationFormatFactory; import org.apache.flink.table.factories.DynamicTableFactory; public class ChangelogCsvFormatFactory implements DeserializationFormatFactory { // define all options statically  public static final ConfigOption\u0026lt;String\u0026gt; COLUMN_DELIMITER = ConfigOptions.key(\u0026#34;column-delimiter\u0026#34;) .stringType() .defaultValue(\u0026#34;|\u0026#34;); @Override public String factoryIdentifier() { return \u0026#34;changelog-csv\u0026#34;; } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; requiredOptions() { return Collections.emptySet(); } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; optionalOptions() { final Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; options = new HashSet\u0026lt;\u0026gt;(); options.add(COLUMN_DELIMITER); return options; } @Override public DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; createDecodingFormat( DynamicTableFactory.Context context, ReadableConfig formatOptions) { // either implement your custom validation logic here ...  // or use the provided helper method  FactoryUtil.validateFactoryOptions(this, formatOptions); // get the validated options  final String columnDelimiter = formatOptions.get(COLUMN_DELIMITER); // create and return the format  return new ChangelogCsvFormat(columnDelimiter); } } Table Source 和解码格式 本节说明了如何从规划层的实例转化为运到集群的运行时实例。\nSocketDynamicTableSource\n在规划过程中会用到 SocketDynamicTableSource。在我们的例子中，我们没有实现任何可用的能力接口。因此，主要的逻辑可以在 getScanRuntimeProvider(...) 中找到，我们在其中实例化了所需的 SourceFunction 和其运行时的 DeserializationSchema。这两个实例都被参数化为返回内部数据结构（即 RowData）。\nimport org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.table.connector.ChangelogMode; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.connector.source.DynamicTableSource; import org.apache.flink.table.connector.source.ScanTableSource; import org.apache.flink.table.connector.source.SourceFunctionProvider; import org.apache.flink.table.data.RowData; import org.apache.flink.table.types.DataType; public class SocketDynamicTableSource implements ScanTableSource { private final String hostname; private final int port; private final byte byteDelimiter; private final DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; decodingFormat; private final DataType producedDataType; public SocketDynamicTableSource( String hostname, int port, byte byteDelimiter, DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; decodingFormat, DataType producedDataType) { this.hostname = hostname; this.port = port; this.byteDelimiter = byteDelimiter; this.decodingFormat = decodingFormat; this.producedDataType = producedDataType; } @Override public ChangelogMode getChangelogMode() { // in our example the format decides about the changelog mode  // but it could also be the source itself  return decodingFormat.getChangelogMode(); } @Override public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderContext) { // create runtime classes that are shipped to the cluster  final DeserializationSchema\u0026lt;RowData\u0026gt; deserializer = decodingFormat.createRuntimeDecoder( runtimeProviderContext, producedDataType); final SourceFunction\u0026lt;RowData\u0026gt; sourceFunction = new SocketSourceFunction( hostname, port, byteDelimiter, deserializer); return SourceFunctionProvider.of(sourceFunction, false); } @Override public DynamicTableSource copy() { return new SocketDynamicTableSource(hostname, port, byteDelimiter, decodingFormat, producedDataType); } @Override public String asSummaryString() { return \u0026#34;Socket Table Source\u0026#34;; } } ChangelogCsvFormat\nChangelogCsvFormat 是一种解码格式，在运行时使用 DeserializationSchema。它支持发出 INSERT 和 DELETE 更改。\nimport org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.table.connector.ChangelogMode; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.connector.source.DynamicTableSource; import org.apache.flink.table.connector.source.DynamicTableSource.DataStructureConverter; import org.apache.flink.table.data.RowData; import org.apache.flink.table.types.DataType; import org.apache.flink.table.types.logical.LogicalType; import org.apache.flink.types.RowKind; public class ChangelogCsvFormat implements DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; { private final String columnDelimiter; public ChangelogCsvFormat(String columnDelimiter) { this.columnDelimiter = columnDelimiter; } @Override @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public DeserializationSchema\u0026lt;RowData\u0026gt; createRuntimeDecoder( DynamicTableSource.Context context, DataType producedDataType) { // create type information for the DeserializationSchema  final TypeInformation\u0026lt;RowData\u0026gt; producedTypeInfo = (TypeInformation\u0026lt;RowData\u0026gt;) context.createTypeInformation( producedDataType); // most of the code in DeserializationSchema will not work on internal data structures  // create a converter for conversion at the end  final DataStructureConverter converter = context.createDataStructureConverter(producedDataType); // use logical types during runtime for parsing  final List\u0026lt;LogicalType\u0026gt; parsingTypes = producedDataType.getLogicalType().getChildren(); // create runtime class  return new ChangelogCsvDeserializer(parsingTypes, converter, producedTypeInfo, columnDelimiter); } @Override public ChangelogMode getChangelogMode() { // define that this format can produce INSERT and DELETE rows  return ChangelogMode.newBuilder() .addContainedKind(RowKind.INSERT) .addContainedKind(RowKind.DELETE) .build(); } } 运行时 为了完整起见，本节说明了 SourceFunction 和 DeserializationSchema 的运行时逻辑。\nChangelogCsvDeserializer\nChangelogCsvDeserializer 包含了一个简单的解析逻辑，用于将字节转换为带有行种类的整数行和字符串。最后的转换步骤将这些转换为内部数据结构。\nimport org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.table.connector.RuntimeConverter.Context; import org.apache.flink.table.connector.source.DynamicTableSource.DataStructureConverter; import org.apache.flink.table.data.RowData; import org.apache.flink.table.types.logical.LogicalType; import org.apache.flink.table.types.logical.LogicalTypeRoot; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; public class ChangelogCsvDeserializer implements DeserializationSchema\u0026lt;RowData\u0026gt; { private final List\u0026lt;LogicalType\u0026gt; parsingTypes; private final DataStructureConverter converter; private final TypeInformation\u0026lt;RowData\u0026gt; producedTypeInfo; private final String columnDelimiter; public ChangelogCsvDeserializer( List\u0026lt;LogicalType\u0026gt; parsingTypes, DataStructureConverter converter, TypeInformation\u0026lt;RowData\u0026gt; producedTypeInfo, String columnDelimiter) { this.parsingTypes = parsingTypes; this.converter = converter; this.producedTypeInfo = producedTypeInfo; this.columnDelimiter = columnDelimiter; } @Override public TypeInformation\u0026lt;RowData\u0026gt; getProducedType() { // return the type information required by Flink\u0026#39;s core interfaces  return producedTypeInfo; } @Override public void open(InitializationContext context) { // converters must be open  converter.open(Context.create(ChangelogCsvDeserializer.class.getClassLoader())); } @Override public RowData deserialize(byte[] message) { // parse the columns including a changelog flag  final String[] columns = new String(message).split(Pattern.quote(columnDelimiter)); final RowKind kind = RowKind.valueOf(columns[0]); final Row row = new Row(kind, parsingTypes.size()); for (int i = 0; i \u0026lt; parsingTypes.size(); i++) { row.setField(i, parse(parsingTypes.get(i).getTypeRoot(), columns[i + 1])); } // convert to internal data structure  return (RowData) converter.toInternal(row); } private static Object parse(LogicalTypeRoot root, String value) { switch (root) { case INTEGER: return Integer.parseInt(value); case VARCHAR: return value; default: throw new IllegalArgumentException(); } } @Override public boolean isEndOfStream(RowData nextElement) { return false; } } SocketSourceFunction\nSocketSourceFunction 打开一个套接字并消耗字节。它通过给定的字节定界符（默认为 \\n）分割记录，并将解码委托给一个可插拔的 DeserializationSchema。源函数只能以 1 的并行度工作。\nimport org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.api.java.typeutils.ResultTypeQueryable; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.functions.source.RichSourceFunction; import org.apache.flink.table.data.RowData; public class SocketSourceFunction extends RichSourceFunction\u0026lt;RowData\u0026gt; implements ResultTypeQueryable\u0026lt;RowData\u0026gt; { private final String hostname; private final int port; private final byte byteDelimiter; private final DeserializationSchema\u0026lt;RowData\u0026gt; deserializer; private volatile boolean isRunning = true; private Socket currentSocket; public SocketSourceFunction(String hostname, int port, byte byteDelimiter, DeserializationSchema\u0026lt;RowData\u0026gt; deserializer) { this.hostname = hostname; this.port = port; this.byteDelimiter = byteDelimiter; this.deserializer = deserializer; } @Override public TypeInformation\u0026lt;RowData\u0026gt; getProducedType() { return deserializer.getProducedType(); } @Override public void open(Configuration parameters) throws Exception { deserializer.open(() -\u0026gt; getRuntimeContext().getMetricGroup()); } @Override public void run(SourceContext\u0026lt;RowData\u0026gt; ctx) throws Exception { while (isRunning) { // open and consume from socket  try (final Socket socket = new Socket()) { currentSocket = socket; socket.connect(new InetSocketAddress(hostname, port), 0); try (InputStream stream = socket.getInputStream()) { ByteArrayOutputStream buffer = new ByteArrayOutputStream(); int b; while ((b = stream.read()) \u0026gt;= 0) { // buffer until delimiter  if (b != byteDelimiter) { buffer.write(b); } // decode and emit record  else { ctx.collect(deserializer.deserialize(buffer.toByteArray())); buffer.reset(); } } } } catch (Throwable t) { t.printStackTrace(); // print and continue  } Thread.sleep(1000); } } @Override public void cancel() { isRunning = false; try { currentSocket.close(); } catch (Throwable t) { // ignore  } } } 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sourceSinks.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-user-defined-sources-and-sinks/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"User Defined Sources and Sinks"},{"categories":["Flink"],"contents":"数据类型和序列化 Apache Flink 以独特的方式处理数据类型和序列化，包含自己的类型描述符、通用类型提取和类型序列化框架。本文档描述了这些概念和它们背后的原理。\n支持的数据类型 Flink 对 DataSet 或 DataStream 中的元素类型进行了一些限制。这样做的原因是系统分析类型以确定高效的执行策略。\n有七种不同类别的数据类型。\n Java Tuples 和 Scala Case 类 Java POJOs Primitive Types Regular Classes Values Hadoop Writables 特殊类型  Tuples 和 Case 类 Scala case 类（以及 Scala tuples，它是 case 类的一种特殊类型），是包含固定数量的各种类型的字段的复合类型。元组字段由它们的 1-offset 名称寻址，例如第一个字段为 _1。case 类字段用它们的名字来访问。\ncase class WordCount(word: String, count: Int) val input = env.fromElements( WordCount(\u0026#34;hello\u0026#34;, 1), WordCount(\u0026#34;world\u0026#34;, 2)) // Case Class Data Set  input.keyBy(\u0026#34;word\u0026#34;)// key by field expression \u0026#34;word\u0026#34;  val input2 = env.fromElements((\u0026#34;hello\u0026#34;, 1), (\u0026#34;world\u0026#34;, 2)) // Tuple2 Data Set  input2.keyBy(0, 1) // key by field positions 0 and 1 POJOs 如果 Java 和 Scala 类满足以下要求，Flink 会将其作为一种特殊的 POJO 数据类型。\n  类必须是公共的。\n  它必须有一个没有参数的公共构造函数（默认构造函数）。\n  所有字段要么是公共的，要么必须通过 getter 和 setter 函数来访问。对于一个名为 foo 的字段，getter 和 setter 方法必须命名为 getFoo() 和  setFoo()。\n  字段的类型必须由注册的序列器支持。\n  POJOs 通常用 PojoTypeInfo 表示，并用 PojoSerializer 序列化（使用 Kryo 作为可配置的回退）。例外的情况是当 POJOs 实际上是 Avro 类型（Avro 特定记录）或作为 \u0026ldquo;Avro 反射类型 \u0026ldquo;产生。在这种情况下，POJO 由 AvroTypeInfo 表示，并通过 AvroSerializer 序列化。如果需要，您也可以注册您自己的自定义序列化器；更多信息请参见序列化。\nFlink 分析 POJO 类型的结构，也就是说，它学习 POJO 的字段。因此，POJO 类型比一般类型更容易使用。此外，Flink 可以比一般类型更有效地处理 POJO。\n下面的例子显示了一个简单的 POJO，它有两个公共字段。\nclass WordWithCount(var word: String, var count: Int) { def this() { this(null, -1) } } val input = env.fromElements( new WordWithCount(\u0026#34;hello\u0026#34;, 1), new WordWithCount(\u0026#34;world\u0026#34;, 2)) // Case Class Data Set  input.keyBy(\u0026#34;word\u0026#34;)// key by field expression \u0026#34;word\u0026#34; 原始类型 Flink 支持所有的 Java 和 Scala 基元类型，如 Integer, String 和 Double。\n通用类类型 Flink 支持大多数 Java 和 Scala 类（API 和自定义）。限制适用于包含不能序列化的字段的类，如文件指针、I/O 流或其他本地资源。遵循 Java Beans 约定的类一般都能很好地工作。\n所有没有被确定为 POJO 类型的类（见上面的 POJO 要求）都被 Flink 作为一般类类型处理。Flink 将这些数据类型视为黑盒，无法访问它们的内容（例如，为了有效的排序）。一般类型使用序列化框架 Kryo 进行去/序列化。\n值 Value 类型手动描述它们的序列化和反序列化。它们不需要通过一个通用的序列化框架，而是通过实现 org.apache.flinktypes.Value 接口的读写方法，为这些操作提供自定义代码。当通用序列化会非常低效时，使用 Value 类型是合理的。一个例子是一个数据类型，它将一个稀疏的元素向量实现为一个数组。知道数组大部分是零，就可以对非零元素使用特殊的编码，而通用序列化会简单地写入所有数组元素。\norg.apache.flinktypes.CopyableValue 接口以类似的方式支持手动内部克隆逻辑。\nFlink 自带了预先定义的 Value 类型，对应基本数据类型。ByteValue、ShortValue、IntValue、LongValue、FloatValue、DoubleValue、StringValue、CharValue、BooleanValue）。这些 Value 类型作为基本数据类型的可变体。它们的值可以被改变，允许程序员重用对象并减轻垃圾收集器的压力。\nHadoop 可写类型 你可以使用实现 org.apache.hadoop.Writable 接口的类型。在 write() 和 readFields() 方法中定义的序列化逻辑将被用于序列化。\n特殊类型 你可以使用特殊类型，包括 Scala 的 Either、Option 和 Try。Java API 对 Either 有自己的自定义实现。类似于 Scala 的 Either，它代表了两种可能的类型的值，左或右。Either 对于错误处理或需要输出两种不同类型记录的操作符来说非常有用。\n类型擦除和类型推断。 注意：本节只与 Java 相关。\nJava 编译器在编译后会丢弃很多通用类型信息。这在 Java 中被称为类型清除。这意味着在运行时，一个对象的实例不再知道它的通用类型。例如，DataStream\u0026lt;String\u0026gt; 和 DataStream\u0026lt;Long\u0026gt; 的实例在 JVM 看来是一样的。\nFlink 在准备执行程序时（调用程序的主方法时）需要类型信息。Flink Java API 试图重建以各种方式扔掉的类型信息，并将其显式存储在数据集和运算符中。你可以通过 DataStream.getType() 来检索类型。该方法返回一个 TypeInformation 的实例，这是 Flink 内部表示类型的方式。\n类型推理有其局限性，在某些情况下需要程序员的 \u0026ldquo;配合\u0026rdquo;。例如从集合中创建数据集的方法，如 ExecutionEnvironment.fromCollection()，你可以传递一个描述类型的参数。但是像 MapFunction\u0026lt;I, O\u0026gt; 这样的通用函数也可能需要额外的类型信息。\nResultTypeQueryable 接口可以由输入格式和函数实现，以明确地告诉 API 它们的返回类型。函数被调用的输入类型通常可以通过前面操作的结果类型来推断。\nFlink 中的类型处理 Flink 试图推断出很多关于分布式计算过程中交换和存储的数据类型的信息。把它想象成一个数据库，推断表的模式。在大多数情况下，Flink 自己就能无缝地推断出所有必要的信息。有了类型信息，Flink 就可以做一些很酷的事情。\n  Flink 对数据类型了解得越多，序列化和数据布局方案就越好。这对于 Flink 中的内存使用范式相当重要（尽可能在堆内/堆外对序列化数据进行工作，并使序列化非常便宜）。\n  最后，在大多数情况下，这也免去了用户对序列化框架的担心，也免去了对类型的注册。\n  一般来说，关于数据类型的信息是在飞行前阶段需要的\u0026ndash;也就是说，当程序对 DataStream 和 DataSet 进行调用时，以及在对 execute()、print()、count() 或 collect() 进行任何调用之前。\n最常见的问题 用户最经常需要与 Flink 的数据类型处理进行交互的问题是。\n  注册子类型。如果函数签名只描述了超类型，但它们在执行过程中实际使用了这些子类型，那么让 Flink 意识到这些子类型可能会提高很多性能。为此，可以在 StreamExecutionEnvironment 或 ExecutionEnvironment 上为每个子类型调用 .registerType(clazz)。\n  注册自定义序列器。Flink 对于那些自己不透明处理的类型又回到了 Kryo。并非所有类型都能被 Kryo 无缝处理（因此也能被 Flink 处理）。例如，许多 Google Guava 集合类型在默认情况下不能很好地工作。解决的办法是为导致问题的类型注册额外的序列器。在 StreamExecutionEnvironment 或 ExecutionEnvironment 上调用.getConfig().addDefaultKryoSerializer( clazz, serializer)。许多库中都有额外的 Kryo 序列化器。有关使用自定义序列器的更多细节，请参见自定义序列器。\n  添加类型提示。有时，当 Flink 尽管使用了所有技巧也无法推断出通用类型时，用户必须传递一个类型提示。这一般只在 Java API 中才需要。类型提示一节对此进行了更详细的描述。\n  手动创建一个 TypeInformation。对于一些 API 调用来说，这可能是必要的，由于 Java 的通用类型擦除，Flink 不可能推断出数据类型。详情请看创建 TypeInformation 或 TypeSerializer。\n  Flink 的 TypeInformation 类 TypeInformation 类是所有类型描述符的基础类。它揭示了类型的一些基本属性，并且可以生成序列器，在特殊化中，可以生成类型的比较器。(注意，Flink 中的比较器的作用远不止定义一个顺序\u0026ndash;它们基本上是处理键的实用程序)\n在内部，Flink 对类型做了如下区分。\n  基本类型。所有的 Java 基元和它们的盒子形式，加上 void，String，Date，BigDecimal 和 BigInteger。\n  基元数组和对象数组：基本类型：所有的 Java 基元和它们的盒子形式，加上 void、String、Date、BigDecimal 和 BigInteger。\n  复合型\n Flink Java Tuples(Flink Java API 的一部分)：最多 25 个字段，不支持 null 字段。 Scala case 类（包括 Scala tuples）：不支持 null 字段。 Row：具有任意数量字段的元组，支持 null 字段。 POJOs：遵循某种 bean-like 模式的类。    辅助类型(Option、Either、Lists、Maps\u0026hellip;)\n  通用类型。这些类型不会由 Flink 本身序列化，而是由 Kryo 序列化。\n  POJOs 特别值得关注，因为它们支持创建复杂类型和在键的定义中使用字段名：dataSet.join(another).where(\u0026quot;name\u0026quot;).equalTo(\u0026quot;personName\u0026quot;)。它们对运行时也是透明的，可以被 Flink 非常有效地处理。\nPOJO 类型的规则 如果满足以下条件，Flink 将数据类型识别为 POJO 类型（并允许 \u0026ldquo;按名称 \u0026ldquo;字段引用）。\n 类是公共的和独立的（没有非静态的内部类）。 该类有一个公共的无参数构造函数。 该类（以及所有超级类）中所有非静态、非瞬态的字段要么是公共的（是非最终的），要么有一个公共的 getter- 和 setter- 方法，遵循 Java beans 中 getter 和 setter 的命名惯例。  请注意，当用户定义的数据类型不能被识别为 POJO 类型时，它必须被处理为 GenericType 并通过 Kryo 进行序列化。\n创建一个 TypeInformation 或 TypeSerializer。 要为一个类型创建 TypeInformation 对象，请使用语言特定的方式。\n在 Scala 中，Flink 使用在编译时运行的宏，并捕捉所有通用类型信息，而它仍然可用。\n// important: this import is needed to access the \u0026#39;createTypeInformation\u0026#39; macro function import org.apache.flink.streaming.api.scala._ val stringInfo: TypeInformation[String] = createTypeInformation[String] val tupleInfo: TypeInformation[(String, Double)] = createTypeInformation[(String, Double)] 你仍然可以使用与 Java 中相同的方法作为后备。\n要创建一个 TypeSerializer，只需在 TypeInformation 对象上调用 typeInfo.createSerializer(config)。\nconfig 参数的类型是 ExecutionConfig，并持有程序注册的自定义序列器的信息。在可能的情况下，尽量传递程序的正确的 ExecutionConfig。你通常可以通过调用 getExecutionConfig() 从 DataStream 或 DataSet 中获取它。在函数内部（比如 MapFunction），你可以通过将函数变成 Rich Function，然后调用 getRuntimeContext().getExecutionConfig() 来获取它。\nScala API 中的类型信息 Scala 通过类型清单和类标签对运行时类型信息有非常详细的概念。一般来说，类型和方法可以访问其通用参数的类型\u0026ndash;因此，Scala 程序不会像 Java 程序那样受到类型擦除的影响。\n此外，Scala 允许通过 Scala Macros 在 Scala 编译器中运行自定义代码\u0026ndash;这意味着每当你编译一个针对 Flink 的 Scala API 编写的 Scala 程序时，一些 Flink 代码就会被执行。\n我们在编译过程中使用宏来查看所有用户函数的参数类型和返回类型\u0026ndash;这时当然所有的类型信息都是完全可用的。在宏中，我们为函数的返回类型（或参数类型）创建一个 TypeInformation，并将其作为操作的一部分。\n没有隐式值的证据参数错误 在 TypeInformation 不能被创建的情况下，程序编译失败，并出现 \u0026ldquo;could not find implicit value for evidence parameter of type TypeInformation\u0026rdquo; 的错误。\n一个常见的原因是生成 TypeInformation 的代码没有被导入。请确保导入整个 flink.api.scala 包。\nimport org.apache.flink.api.scala._ 另一个常见的原因是通用方法，它可以在下面的章节中进行修复。\n通用方法 请考虑以下案例。\ndef selectFirst[T](input: DataSet[(T, _)]) : DataSet[T] = { input.map { v =\u0026gt; v._1 } } val data : DataSet[(String, Long) = ... val result = selectFirst(data) 对于这样的通用方法，每次调用时，函数参数和返回类型的数据类型可能不一样，在定义方法的站点不知道。上面的代码会导致一个错误，即没有足够的隐含证据。\n在这种情况下，必须在调用站点生成类型信息并传递给方法。Scala 为此提供了隐式参数。\n下面的代码告诉 Scala 将 T 的类型信息带入函数中。然后，类型信息将在方法被调用的站点生成，而不是在方法被定义的站点生成。\ndef selectFirst[T : TypeInformation](input: DataSet[(T, _)]) : DataSet[T] = { input.map { v =\u0026gt; v._1 } } Java API 中的类型信息 在一般情况下，Java 会擦除通用类型信息，而 Flink 试图通过反射来重建尽可能多的类型信息，使用 Java 保留的少量信息（主要是函数签名和子类信息）。Flink 试图通过反射来重建尽可能多的类型信息，使用 Java 保留的少量信息（主要是函数签名和子类信息）。这个逻辑还包含了一些简单的类型推理，用于函数的返回类型取决于其输入类型的情况。\npublic class AppendOne\u0026lt;T\u0026gt; implements MapFunction\u0026lt;T, Tuple2\u0026lt;T, Long\u0026gt;\u0026gt; { public Tuple2\u0026lt;T, Long\u0026gt; map(T value) { return new Tuple2\u0026lt;T, Long\u0026gt;(value, 1L); } } 在有些情况下，Flink 无法重建所有的通用类型信息。在这种情况下，用户必须通过类型提示来帮忙。\nJava API 中的类型提示 在 Flink 无法重建被擦除的通用类型信息的情况下，Java API 提供了所谓的类型提示。类型提示告诉系统一个函数产生的数据流或数据集的类型。\nDataSet\u0026lt;SomeType\u0026gt; result = dataSet .map(new MyGenericNonInferrableFunction\u0026lt;Long, SomeType\u0026gt;()) .returns(SomeType.class); returns 语句指定产生的类型，在本例中是通过一个类。提示支持类型定义，通过:\n 类，适用于非参数化类型(无属类) TypeHint 以 returns(new TypeHint\u0026lt;Tuple2\u0026lt;Integer, SomeType\u0026gt;\u0026gt;(){}) 的形式存在。TypeHint 类可以捕获通用类型信息，并将其保存到运行时（通过匿名子类）。  Java 8 lambdas 的类型提取。 Java 8 lambdas 的类型提取与非 lambdas 的工作方式不同，因为 lambdas 不与扩展函数接口的实现类相关联。\n目前，Flink 试图找出哪个方法实现了 lambda，并使用 Java 的通用签名来确定参数类型和返回类型。然而，并不是所有的编译器都能为 lambdas 生成这些签名（在写这篇文档时，只有 4.5 以后的 Eclipse JDT 编译器能可靠地生成）。\nPOJO 类型的序列化 PojoTypeInfo 正在为 POJO 内部的所有字段创建序列器。标准类型，如 int、long、String 等，由 Flink 附带的序列器处理。对于所有其他类型，我们回到了 Kryo。\n如果 Kryo 不能处理类型，你可以要求 PojoTypeInfo 使用 Avro 来序列化 POJO。要做到这一点，你必须调用\nfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().enableForceAvro(); 请注意，Flink 是用 Avro 序列化器自动序列化 Avro 生成的 POJO。\n如果您想让整个 POJO 类型被 Kryo 序列化器处理，请设置\nfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().enableForceKryo(); 如果 Kryo 无法序列化你的 POJO，你可以在 Kryo 中添加一个自定义序列化器，使用\nenv.getConfig().addDefaultKryoSerializer(Class\u0026lt;?\u0026gt; type, Class\u0026lt;? extends Serializer\u0026lt;?\u0026gt;\u0026gt; serializerClass) 这些方法有不同的变体。\n禁用 Kryo Fallback 有些情况下，程序可能希望明确避免使用 Kryo 作为通用类型的后备。最常见的情况是希望确保所有类型都能通过 Flink 自己的序列化器或通过用户定义的自定义序列化器进行有效序列化。\n下面的设置会在遇到会通过 Kryo 的数据类型时引发异常。\nenv.getConfig().disableGenericTypes(); 使用工厂定义类型信息 类型信息工厂允许将用户定义的类型信息插入到 Flink 类型系统中。你必须实现 org.apache.flink.api.common.typeinfo.TypeInfoFactory 来返回你的自定义类型信息。如果相应的类型已经被 @org.apache.flink.api.common.typeinfo.TypeInfo 注解，那么在类型提取阶段就会调用这个工厂。\n类型信息工厂可以在 Java 和 Scala API 中使用。\n在类型的层次结构中，在向上遍历时将选择最接近的工厂，然而，内置工厂具有最高的优先级。工厂也比 Flink 的内置类型有更高的优先级，因此你应该知道你在做什么。\n下面的例子展示了如何在 Java 中使用工厂来注释一个自定义类型 MyTuple 并为其提供自定义类型信息。\n注解的自定义类型:\n@TypeInfo(MyTupleTypeInfoFactory.class) public class MyTuple\u0026lt;T0, T1\u0026gt; { public T0 myfield0; public T1 myfield1; } 工厂供应自定义类型信息:\npublic class MyTupleTypeInfoFactory extends TypeInfoFactory\u0026lt;MyTuple\u0026gt; { @Override public TypeInformation\u0026lt;MyTuple\u0026gt; createTypeInfo(Type t, Map\u0026lt;String, TypeInformation\u0026lt;?\u0026gt;\u0026gt; genericParameters) { return new MyTupleTypeInfo(genericParameters.get(\u0026#34;T0\u0026#34;), genericParameters.get(\u0026#34;T1\u0026#34;)); } } 方法 createTypeInfo(Type, Map\u0026lt;String, TypeInformation\u0026lt;?\u0026gt;) 为工厂的目标类型创建类型信息。参数提供了关于类型本身的附加信息，以及类型的通用类型参数（如果可用）。\n如果你的类型包含了可能需要从 Flink 函数的输入类型中导出的通用参数，请确保同时实现 org.apache.flink.api.common.typeinfo.TypeInformation#getGenericParameters 来实现通用参数到类型信息的双向映射。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-datatypes-serialization/","tags":["Flink","Flink 官方文档","Serialization"],"title":"数据类型和序列化"},{"categories":["Flink"],"contents":"配置 默认情况下，Table \u0026amp; SQL API 是预先配置的，可以在可接受的性能下产生准确的结果。\n根据表程序的要求，可能需要调整某些参数进行优化。例如，无约束的流程序可能需要确保所需的状态大小是有上限的（参见流概念）。\n概述 在每个表环境中，TableConfig 都提供了配置当前会话的选项。\n对于常见或重要的配置选项，TableConfig 提供了 getter 和 setter 方法，并提供了详细的内联文档。\n对于更高级的配置，用户可以直接访问底层的键值映射。下面的章节列出了所有可用的选项，可以用来调整 Flink Table \u0026amp; SQL API 程序。\n注意: 由于在执行操作时，选项会在不同的时间点被读取，因此建议在实例化表环境后尽早设置配置选项。\n// instantiate table environment val tEnv: TableEnvironment = ... // access flink configuration val configuration = tEnv.getConfig().getConfiguration() // set low-level key-value options configuration.setString(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) configuration.setString(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) configuration.setString(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) 注意：目前，键值选项只支持 Blink 计划器。目前，键值选项只支持 Blink 计划器。\n执行选项 以下选项可以用来调整查询执行的性能。\n   Key Default Type Description     table.exec.async-lookup.buffer-capacity(Batch/Streaming) 100 Integer 异步查找连接可以触发的最大异步 i/o 操作数。   table.exec.async-lookup.timeout(Batch/Streaming) \u0026ldquo;3 min\u0026rdquo; String 异步操作完成的异步超时时间。   table.exec.disabled-operators(Batch) (none) String 主要用于测试。一个以逗号分隔的操作符名称列表，每个名称代表一种被禁用的操作符。可以禁用的操作符包括 \u0026ldquo;NestedLoopJoin\u0026rdquo;、\u0026ldquo;ShuffleHashJoin\u0026rdquo;、\u0026ldquo;BroadcastHashJoin\u0026rdquo;、\u0026ldquo;SortMergeJoin\u0026rdquo;、\u0026ldquo;HashAgg\u0026rdquo;、\u0026ldquo;SortAgg\u0026rdquo;。默认情况下，没有任何操作符被禁用。   table.exec.mini-batch.allow-latency(Streaming) \u0026ldquo;-1 ms\u0026rdquo; String 最大延迟可以用于 MiniBatch 来缓冲输入记录。MiniBatch 是一种优化，用于缓冲输入记录以减少状态访问。MiniBatch 会在允许的延迟间隔和达到最大缓冲记录数时触发。注意：如果 table.exec.mini-batch.enabled 被设置为 true，其值必须大于零。   table.exec.mini-batch.enabled(Streaming) false Boolean 指定是否启用 MiniBatch 优化。MiniBatch 是对输入记录进行缓冲以减少状态访问的优化。默认情况下，这个配置是禁用的。要启用这个功能，用户应该将这个配置设置为 true。注意：如果启用了 Mini-batch，必须设置\u0026rsquo;table.exec.mini-batch.allow-latency\u0026rsquo;和\u0026rsquo;table.exec.mini-batch.size'。   table.exec.mini-batch.size(Streaming) -1 Long MiniBatch 可以缓冲的输入记录的最大数量。MiniBatch 是对输入记录进行缓冲的优化，以减少状态访问。MiniBatch 会在允许的延迟间隔和达到最大缓冲记录数时触发。注意：MiniBatch 目前只适用于非窗口聚合。如果 table.exec.mini-batch.enabled 被设置为 true，其值必须为正。   table.exec.resource.default-parallelism(Batch/Streaming) -1 Integer 为所有操作符（如 aggregation、join、filter）设置默认的并行性，以便与并行实例一起运行。这个配置的优先级高于 StreamExecutionEnvironment 的并行性（实际上，这个配置覆盖了 StreamExecutionEnvironment 的并行性）。值为-1 表示没有设置默认的并行性，那么它将回落到使用 StreamExecutionEnvironment 的并行性。   table.exec.shuffle-mode(Batch) \u0026ldquo;ALL_EDGES_BLOCKING\u0026rdquo; String 设置执行 shuffle 的模式。接受的值是, ALL_EDGES_BLOCKING: 所有边缘都将使用阻塞洗牌。FORWARD_EDGES_PIPELINED: 正向边缘将使用流水线洗牌，其他边缘将使用阻塞洗牌。POINTWISE_EDGES_PIPELINED: POINTWISE_EDGES_PIPELINED: 点向边缘将使用管道式洗牌，其他边缘将被阻挡。POINTWISE_EDGES_PIPELINED: 点向边缘包括前向和重新缩放边缘。ALL_EDGES_PIPELINED: 所有的边缘都将使用 pipelined shuffle，其他的边缘则使用 blocks。所有边缘都将使用流水线洗牌。batch: 与 ALL_EDGES_BLOCKING 相同。已废弃。pipelined: 与 ALL_EDGES_PIPELINED 相同。已被弃用。注意：Blocking shuffle 意味着数据将在发送到消费者任务之前被完全生成。Pipelined shuffle 意味着数据一旦被生产出来，就会被发送到消费者任务中。   table.exec.sink.not-null-enforcer(Batch/Streaming) ERROR Enum,可能的值: [ERROR, DROP] 表上的 NOT NULL 列约束强制要求不能将空值插入到表中。Flink 支持 \u0026ldquo;错误\u0026rdquo;（默认）和 \u0026ldquo;放弃 \u0026ldquo;执行行为。默认情况下，当 NOT NULL 列中写入空值时，Flink 会检查值并抛出运行时异常。用户可以将行为改为\u0026rsquo;drop'，在不出现异常的情况下默默地删除这些记录。   table.exec.sort.async-merge-enabled(Batch) true Boolean 是否异步合并排序后的 spill 文件。   table.exec.sort.default-limit(Batch) -1 Integer 当用户在下单后没有设置限价时，默认限价。-1 表示该配置被忽略。   table.exec.sort.max-num-file-handles(Batch) 128 Integer 外部合并排序的最大扇入量。它限制了每个操作者的文件句柄数。如果太小，可能会造成中间合并。但如果太大，会造成同时打开的文件太多，消耗内存，导致随机读取。   table.exec.source.idle-timeout(Streaming) \u0026ldquo;-1 ms\u0026rdquo; String 当一个源在超时时间内没有收到任何元素时，它将被标记为暂时空闲。这样下游任务就可以提前打水印，而不需要在这个源空闲时等待它的水印。   table.exec.spill-compression.block-size(Batch) \u0026ldquo;64 kb\u0026rdquo; String 溢出数据时做压缩时使用的内存大小。内存越大，压缩比越高，但作业会消耗更多的内存资源。   table.exec.spill-compression.enabled(Batch) true Boolean 是否压缩溢出数据。目前我们只支持压缩溢出数据的排序和哈希-agg 和哈希-join 操作符。   table.exec.window-agg.buffer-size-limit(Batch) 100000 Integer 设置组窗口 agg 运算符中使用的窗口元素缓冲区大小限制。    优化选项 以下选项可以用来调整查询优化器的行为，以获得更好的执行计划。\n   Key Default Type Description     table.optimizer.agg-phase-strategy(Batch/Streaming) \u0026ldquo;AUTO\u0026rdquo; String 聚合阶段的策略。只能设置 AUTO、TWO_PHASE 或 ONE_PHASE。AUTO：集合阶段无特殊执行器。选择两阶段聚合还是一阶段聚合取决于成本。TWO_PHASE: 强制使用两级聚合，其中包括 localAggregate 和 globalAggregate。请注意，如果聚合调用不支持优化为两阶段，我们仍将使用一个阶段的聚合。ONE_PHASE: 强制使用只有 CompleteGlobalAggregate 的单阶段聚合。   table.optimizer.distinct-agg.split.bucket-num(Streaming) 1024 Integer 配置拆分不同聚合时的桶数。这个数字在一级聚合中用于计算一个桶键\u0026rsquo;hash_code(distinct_key) % BUCKET_NUM'，这个桶键在拆分后作为一个额外的组键使用。   table.optimizer.distinct-agg.split.enabled(Streaming) false Boolean 指示优化器是否将 distinct aggregation(例如 COUNT(DISTINCT col)，SUM(DISTINCT col))分成两级。第一层聚合由一个额外的 key 进行洗牌，这个 key 是用 distinct_key 和 buckets 数量的 hashcode 计算出来的。当 distinct aggregation 中存在数据倾斜时，这种优化是非常有用的，并提供了扩展作业的能力。默认为 false。   table.optimizer.join-reorder-enabled(Batch/Streaming) false Boolean 在优化器中启用连接重排序。默认为禁用。   table.optimizer.join.broadcast-threshold(Batch) 1048576 Long 配置在执行连接时将向所有工作节点广播的表的最大字节数。将此值设置为-1，则禁用广播。   table.optimizer.reuse-source-enabled(Batch/Streaming) true Boolean 当它为真时，优化器将尝试找出重复的表源并重用它们。这只有在 table.optimizer.reuse-sub-plan-enabled 为真时才会生效。   table.optimizer.reuse-sub-plan-enabled(Batch/Streaming) true Boolean 当它为真时，优化器将尝试找出重复的子计划并重用它们。   table.optimizer.source.predicate-pushdown-enabled(Batch/Streaming) true Boolean 当该值为真时，优化器将向下推送谓词到 FilterableTableSource 中。默认值为 true。    Table 选项 以下选项可用于调整表计划器(planner)的行为:\n   Key Default Type Description     table.dynamic-table-options.enabled(Batch/Streaming) false Boolean 启用或禁用 OPTIONS 提示，用于动态指定表选项，如果禁用，则如果指定了任何 OPTIONS 提示，就会产生异常。   table.sql-dialect(Batch/Streaming) \u0026ldquo;default\u0026rdquo; String SQL 方言定义了如何解析一个 SQL 查询。不同的 SQL 方言可能支持不同的 SQL 语法。目前支持的方言有：默认和 hive。    原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/config.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-25-configuration/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"配置"},{"categories":["Flink"],"contents":"ALTER 语句 ALTER 语句用于修改目录中注册的表/视图/函数定义。\nFlink SQL 目前支持以下 ALTER 语句。\n ALTER TABLE ALTER DATABASE ALTER FUNCTION  运行 ALTER 语句 ALTER 语句可以用 TableEnvironment 的 executeSql()方法执行，也可以在 SQL CLI 中执行。executeSql()方法在 ALTER 操作成功时返回 \u0026ldquo;OK\u0026rdquo;，否则将抛出一个异常。\n下面的例子展示了如何在 TableEnvironment 和 SQL CLI 中运行 ALTER 语句。\nval settings = EnvironmentSettings.newInstance()... val tableEnv = TableEnvironment.create(settings) // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); // a string array: [\u0026#34;Orders\u0026#34;] val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print()  // rename \u0026#34;Orders\u0026#34; to \u0026#34;NewOrders\u0026#34; tableEnv.executeSql(\u0026#34;ALTER TABLE Orders RENAME TO NewOrders;\u0026#34;) // a string array: [\u0026#34;NewOrders\u0026#34;] val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() ALTER TABLE  Rename Table  ALTERTABLE[catalog_name.][db_name.]table_nameRENAMETOnew_table_name将给定的表名重命名为另一个新表名。\n 设置或更改表属性  ALTERTABLE[catalog_name.][db_name.]table_nameSET(key1=val1,key2=val2,...)设置指定表格中的一个或多个属性。如果某个属性已经在表中被设置，则用新的属性覆盖旧的值。\nALTER DATABASE ALTERDATABASE[catalog_name.]db_nameSET(key1=val1,key2=val2,...)在指定的数据库中设置一个或多个属性。如果某个属性已经在数据库中被设置，则用新的属性覆盖旧的值。\nALTER FUNCTION ALTER[TEMPORARY|TEMPORARYSYSTEM]FUNCTION[IFEXISTS][catalog_name.][db_name.]function_nameASidentifier[LANGUAGEJAVA|SCALA|PYTHON]用新的标识符和可选的语言标签改变一个目录函数。如果一个函数在目录中不存在，就会抛出一个异常。\n如果语言标签是 JAVA/SCALA，标识符是 UDF 的完整 classpath。关于 Java/Scala UDF 的实现，请参考 User-defined Functions 了解详情。\n如果语言标签是 PYTHON，标识符是 UDF 的完全限定名，例如 pyflink.table.test.test_udf.add。关于 Python UDF 的实现，更多细节请参考 Python UDFs。\nTEMPORARY\n改变具有目录和数据库命名空间的临时目录功能，并覆盖目录功能。\nTEMPORARY SYSTEM\n更改没有命名空间的临时系统函数，并覆盖内置函数。\nIF EXISTS\n如果函数不存在，就不会发生任何事情。\nLANGUAGE JAVA|SCALA|PYTHON\n语言标签，用于指导 flink 运行时如何执行函数。目前只支持 JAVA、SCALA 和 PYTHON，函数的默认语言是 JAVA。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/alter.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","SQL"],"title":"Alter 语句"},{"categories":["Flink"],"contents":"Catalogs 目录提供了元数据，如数据库、表、分区、视图以及访问数据库或其他外部系统中存储的数据所需的功能和信息。\n数据处理中最关键的一个方面是管理元数据。它可能是短暂的元数据，如临时表，或针对表环境注册的 UDF。或者是永久性的元数据，比如 Hive Metastore 中的元数据。目录为管理元数据提供了统一的 API，并使其可以从表 API 和 SQL 查询中访问。\nCatalog 使用户能够引用数据系统中现有的元数据，并自动将它们映射到 Flink 的相应元数据中。例如，Flink 可以将 JDBC 表自动映射到 Flink 表，用户不必在 Flink 中手动重新编写 DDL。Catalog 大大简化了用户现有系统上手 Flink 所需的步骤，大大提升了用户体验。\nCatalog 类型 GenericInMemoryCatalog GenericInMemoryCatalog 是一个目录的内存实现。所有对象只在会话的生命周期内可用。\nJdbcCatalog JdbcCatalog 使用户能够通过 JDBC 协议将 Flink 与关系型数据库连接起来。PostgresCatalog 是目前 JDBC Catalog 的唯一实现。关于设置目录的更多细节，请参见 JdbcCatalog 文档。\nHiveCatalog HiveCatalog 有两个目的，一是作为纯 Flink 元数据的持久化存储，二是作为读写现有 Hive 元数据的接口。Flink 的 Hive 文档提供了设置目录和与现有 Hive 安装接口的完整细节。\n警告: Hive Metastore 将所有的元对象名称都存储为小写。这与 GenericInMemoryCatalog 不同，后者是区分大小写的。\n用户定义的 Catalog 目录是可插拔的，用户可以通过实现 Catalog 接口来开发自定义目录。要在 SQL CLI 中使用自定义目录，用户应该通过实现 CatalogFactory 接口同时开发目录和它对应的目录工厂。\n目录工厂定义了一组属性，用于在 SQL CLI 引导时配置目录。该属性集将被传递给发现服务，服务会尝试将属性与 CatalogFactory 匹配，并启动相应的目录实例。\n如何创建和注册 Flink Table 到目录上 使用 SQL DDL 用户可以使用 SQL DDL 在 Table API 和 SQL 中创建目录中的表。\n Flink SQL  // the catalog should have been registered via yaml file Flink SQL\u0026gt; CREATE DATABASE mydb WITH (...); Flink SQL\u0026gt; CREATE TABLE mytable (name STRING, age INT) WITH (...); Flink SQL\u0026gt; SHOW TABLES; mytable 详细信息，请查看 Flink SQL CREATE DDL。\n Scala  val tableEnv = ... // Create a HiveCatalog val catalog = new HiveCatalog(\u0026#34;myhive\u0026#34;, null, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;) // Register the catalog tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, catalog) // Create a catalog database tableEnv.executeSql(\u0026#34;CREATE DATABASE mydb WITH (...)\u0026#34;) // Create a catalog table tableEnv.executeSql(\u0026#34;CREATE TABLE mytable (name STRING, age INT) WITH (...)\u0026#34;) tableEnv.listTables() // should return the tables in current catalog and database. For detailed information, please check out Flink SQL CREATE DDL.\n使用 Java, Scala 或 Python 用户可以使用 Java、Scala 或 Python 来编程创建目录表。\nimport org.apache.flink.table.api._ import org.apache.flink.table.catalog._ import org.apache.flink.table.catalog.hive.HiveCatalog import org.apache.flink.table.descriptors.Kafka val tableEnv = TableEnvironment.create(EnvironmentSettings.newInstance.build) // Create a HiveCatalog val catalog = new HiveCatalog(\u0026#34;myhive\u0026#34;, null, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;) // Register the catalog tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, catalog) // Create a catalog database catalog.createDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...)) // Create a catalog table val schema = TableSchema.builder() .field(\u0026#34;name\u0026#34;, DataTypes.STRING()) .field(\u0026#34;age\u0026#34;, DataTypes.INT()) .build() catalog.createTable( new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogTableImpl( schema, new Kafka() .version(\u0026#34;0.11\u0026#34;) .... .startFromEarlist() .toProperties(), \u0026#34;my comment\u0026#34; ), false ) val tables = catalog.listTables(\u0026#34;mydb\u0026#34;) // tables should contain \u0026#34;mytable\u0026#34; Catalog API 注意：这里只列出了目录程序的 API，用户可以通过 SQL DDL 实现许多相同的功能。用户可以通过 SQL DDL 实现许多相同的功能。详细的 DDL 信息，请参考 SQL CREATE DDL。\n数据库操作 // create database catalog.createDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...), false); // drop database catalog.dropDatabase(\u0026#34;mydb\u0026#34;, false); // alter database catalog.alterDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...), false); // get database catalog.getDatabase(\u0026#34;mydb\u0026#34;); // check if a database exist catalog.databaseExists(\u0026#34;mydb\u0026#34;); // list databases in a catalog catalog.listDatabases(); Table 操作 // create table catalog.createTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogTableImpl(...), false); // drop table catalog.dropTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), false); // alter table catalog.alterTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogTableImpl(...), false); // rename table catalog.renameTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), \u0026#34;my_new_table\u0026#34;); // get table catalog.getTable(\u0026#34;mytable\u0026#34;); // check if a table exist or not catalog.tableExists(\u0026#34;mytable\u0026#34;); // list tables in a database catalog.listTables(\u0026#34;mydb\u0026#34;); 视图操作 // create view catalog.createTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), new CatalogViewImpl(...), false); // drop view catalog.dropTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), false); // alter view catalog.alterTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogViewImpl(...), false); // rename view catalog.renameTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), \u0026#34;my_new_view\u0026#34;, false); // get view catalog.getTable(\u0026#34;myview\u0026#34;); // check if a view exist or not catalog.tableExists(\u0026#34;mytable\u0026#34;); // list views in a database catalog.listViews(\u0026#34;mydb\u0026#34;); Partition 操作 // create view catalog.createPartition( new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...), new CatalogPartitionImpl(...), false); // drop partition catalog.dropPartition(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...), false); // alter partition catalog.alterPartition( new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...), new CatalogPartitionImpl(...), false); // get partition catalog.getPartition(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...)); // check if a partition exist or not catalog.partitionExists(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...)); // list partitions of a table catalog.listPartitions(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;)); // list partitions of a table under a give partition spec catalog.listPartitions(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...)); // list partitions of a table by expression filter catalog.listPartitionsByFilter(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), Arrays.asList(epr1, ...)); Function 操作 // create function catalog.createFunction(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), new CatalogFunctionImpl(...), false); // drop function catalog.dropFunction(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), false); // alter function catalog.alterFunction(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), new CatalogFunctionImpl(...), false); // get function catalog.getFunction(\u0026#34;myfunc\u0026#34;); // check if a function exist or not catalog.functionExists(\u0026#34;myfunc\u0026#34;); // list functions in a database catalog.listFunctions(\u0026#34;mydb\u0026#34;); 目录的 Table API 和 SQL 注册目录 用户可以访问一个名为 default_catalog 的默认内存目录，这个目录总是默认创建的。该目录默认有一个名为 default_database 的单一数据库。用户也可以在现有的 Flink 会话中注册额外的目录。\n Scala  tableEnv.registerCatalog(new CustomCatalog(\u0026#34;myCatalog\u0026#34;));  YAML  所有使用 YAML 定义的目录必须提供一个 type 属性，指定目录的类型。以下类型是开箱即用的。\n   Catalog Type Value     GenericInMemory generic_in_memory   Hive hive    catalogs:- name:myCatalogtype:custom_cataloghive-conf-dir:...更改当前目录和数据库 Flink 将始终搜索当前目录和数据库中的表、视图和 UDF。\n Scala  tableEnv.useCatalog(\u0026#34;myCatalog\u0026#34;); tableEnv.useDatabase(\u0026#34;myDb\u0026#34;);  Flink SQL  Flink SQL\u0026gt; USE CATALOG myCatalog; Flink SQL\u0026gt; USE myDB; 通过提供 catalog.database.object 形式的完全限定名称，可以访问非当前目录的元数据。\n Scala  tableEnv.from(\u0026#34;not_the_current_catalog.not_the_current_db.my_table\u0026#34;);  Flink SQL  Flink SQL\u0026gt; SELECT * FROM not_the_current_catalog.not_the_current_db.my_table; 列出可用的目录  Scala  tableEnv.listCatalogs();  Flink SQL  Flink SQL\u0026gt; show catalogs; 列出可用的数据库  Scala  tableEnv.listDatabases();  Flink SQL  Flink SQL\u0026gt; show databases; 列出可用的表  Scala  tableEnv.listTables();  Flink SQL  Flink SQL\u0026gt; show tables; 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/catalogs.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-catalogs/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","Catalogs"],"title":"Catalogs"},{"categories":["Flink"],"contents":"CREATE 语句 CREATE 语句用于在当前或指定的目录中注册一个表/视图/函数。注册的表/视图/函数可以在 SQL 查询中使用。\nFlink SQL 目前支持以下 CREATE 语句。\n CREATE TABLE CREATE DATABASE CREATE VIEW CREATE FUNCTION  运行一条 CREATE 语句 CREATE 语句可以用 TableEnvironment 的 executeSql()方法执行，也可以在 SQL CLI 中执行。executeSql()方法对于一个成功的 CREATE 操作会返回\u0026rsquo;OK'，否则会抛出一个异常。\n下面的例子展示了如何在 TableEnvironment 和 SQL CLI 中运行 CREATE 语句。\n// Scala val settings = EnvironmentSettings.newInstance()... val tableEnv = TableEnvironment.create(settings) // SQL query with a registered table // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); // run a SQL query on the Table and retrieve the result as a new Table val result = tableEnv.sqlQuery( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); // Execute insert SQL with a registered table // register a TableSink tableEnv.executeSql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (\u0026#39;connector.path\u0026#39;=\u0026#39;/path/to/file\u0026#39; ...)\u0026#34;); // run an insert SQL on the Table and emit the result to the TableSink tableEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) Flink SQL\u0026gt; CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; CREATE TABLE RubberOrders (product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;; [INFO] Submitting SQL update statement to the cluster... CREATE TABLE CREATETABLE[catalog_name.][db_name.]table_name({\u0026lt;column_definition\u0026gt;|\u0026lt;computed_column_definition\u0026gt;}[,...n][\u0026lt;watermark_definition\u0026gt;][\u0026lt;table_constraint\u0026gt;][,...n])[COMMENTtable_comment][PARTITIONEDBY(partition_column_name1,partition_column_name2,...)]WITH(key1=val1,key2=val2,...)[LIKEsource_table[(\u0026lt;like_options\u0026gt;)]]\u0026lt;column_definition\u0026gt;:column_namecolumn_type[\u0026lt;column_constraint\u0026gt;][COMMENTcolumn_comment]\u0026lt;column_constraint\u0026gt;:[CONSTRAINTconstraint_name]PRIMARYKEYNOTENFORCED\u0026lt;table_constraint\u0026gt;:[CONSTRAINTconstraint_name]PRIMARYKEY(column_name,...)NOTENFORCED\u0026lt;computed_column_definition\u0026gt;:column_nameAScomputed_column_expression[COMMENTcolumn_comment]\u0026lt;watermark_definition\u0026gt;:WATERMARKFORrowtime_column_nameASwatermark_strategy_expression\u0026lt;like_options\u0026gt;:{{INCLUDING|EXCLUDING}{ALL|CONSTRAINTS|PARTITIONS}|{INCLUDING|EXCLUDING|OVERWRITING}{GENERATED|OPTIONS|WATERMARKS}}[,...]用给定的名称创建一个表。如果目录中已经存在同名表，则抛出一个异常。\n计算列\n计算列是使用 \u0026ldquo;column_name AS computed_column_expression\u0026rdquo; 语法生成的虚拟列。它是由一个非查询表达式生成的，这个表达式使用同一张表中的其他列，而不是实际存储在表中。例如，计算列可以定义为 cost AS price * quantity。表达式可以包含物理列、常量、函数或变量的任意组合。表达式不能包含子查询。\n计算列在 Flink 中通常用于在 CREATE TABLE 语句中定义时间属性。可以通过 proc AS PROCTIME() 使用系统 proctime() 函数轻松定义一个处理时间属性。另一方面，计算列可以用来派生事件时间列，因为事件时间列可能需要从现有的字段中派生出来，比如原来的字段不是 TIMESTAMP(3)类型，或者嵌套在 JSON 字符串中。\n注意：\n 在源表上定义的计算列是在从源表读取后计算出来的，它可以用在下面的 SELECT 查询语句中。 计算列不能作为 INSERT 语句的目标。在 INSERT 语句中，SELECT 子句的模式应该与没有计算列的目标表的模式相匹配。  WATERMARK\nWATERMARK 定义了表的事件时间属性，其形式为 WATERMARK FOR rowtime_column_name AS watermark_strategy_expression。\nrowtime_column_name 定义了一个现有的列，该列被标记为表的事件时间属性。这个列的类型必须是 TIMESTAMP(3)，并且是模式中的顶层列。它可以是一个计算列。\nwatermark_strategy_expression 定义了水印生成策略。它允许任意的非查询表达式，包括计算列，来计算水印。表达式的返回类型必须是 TIMESTAMP(3)，它表示自 Epoch 以来的时间戳。只有当返回的水印是非空的，并且它的值大于之前发出的本地水印时，才会发出水印（以保留升水印的契约）。水印生成表达式由框架对每条记录进行评估。框架将定期发射最大的生成水印。如果当前的水印仍然与上一个水印相同，或者是空的，或者返回的水印值小于上一次发射的水印值，那么将不会发射新的水印。水印是在 pipeline.auto-watermark-interval 配置定义的时间间隔内发出的。如果水印间隔为 0ms，如果生成的水印不是空的，并且大于最后一个水印，则每条记录都会发出水印。\n当使用事件时间语义时，表必须包含事件时间属性和水印策略。\nFlink 提供了几种常用的水印策略。\n  严格的升序时间戳。WATERMARK FOR rowtime_column AS rowtime_column。\n  发出迄今为止观察到的最大时间戳的水印。时间戳小于最大时间戳的行不会迟到。\n  升序时间戳。WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL \u0026lsquo;0.001\u0026rsquo; SECOND.\n  发出迄今为止观察到的最大时间戳的水印减 1。时间戳等于或小于最大时间戳的行不会迟到。\n  绑定出顺序性的时间戳。WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL \u0026lsquo;string\u0026rsquo; timeUnit.\n  发出水印，水印是最大观察到的时间戳减去指定的延迟，例如：WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL \u0026lsquo;5\u0026rsquo; SECOND 是 5 秒的延迟水印策略。\nCREATETABLEOrders(userBIGINT,productSTRING,order_timeTIMESTAMP(3),WATERMARKFORorder_timeASorder_time-INTERVAL\u0026#39;5\u0026#39;SECOND)WITH(...);PRIMARYKEYPRIMARY KEY\nFlink 利用优化的一个提示。它告诉我们一个表或视图的一列或一组列是唯一的，它们不包含空值。主键中的两列都不能为空。因此，主键可以唯一地识别表中的某一行。\n主键约束既可以和列定义一起声明（列约束），也可以作为单行（表约束）。对于这两种情况，只能将其声明为一个单子。如果你同时定义了多个主键约束，就会抛出一个异常。\n有效性检查\nSQL 标准规定，一个约束可以是 ENFORCED 或 NOT ENFORCED。这控制了约束检查是否会在输入/输出数据上执行。Flink 并不拥有数据，因此我们要支持的唯一模式是 NOT ENFORCED 模式。用户要确保查询强制执行密钥的完整性。\nFlink 会假设主键的正确性，假设列的空性与主键的列对齐。连接器应该确保这些是对齐的。\n注意事项: 在 CREATE TABLE 语句中，创建主键约束会改变列的可空性，也就是说，有主键约束的列是不可空的。\nPARTITIONED BY\n按指定的列对创建的表进行分区。如果该表被用作文件系统汇，则会为每个分区创建一个目录。\nWITH OPTIONS\n表属性用于创建表源/接收器。这些属性通常用于查找和创建底层连接器。\n表达式 key1=val1 的键和值都应该是字符串文字。关于不同连接器的所有支持的表属性，请参见连接到外部系统中的详细信息。\n注释：表名可以有三种格式。表名可以有三种格式。\n catalog_name.db_name.table_name db_name.table_name table_name。  对于 catalog_name.db_name.table_name，表将被注册到元存储中，目录名为 \u0026ldquo;catalog_name\u0026rdquo;，数据库名为 \u0026ldquo;db_name\u0026rdquo;；对于 db_name.table_name，表将被注册到执行表环境的当前目录中，数据库名为 \u0026ldquo;db_name\u0026rdquo;；对于 table_name，表将被注册到执行表环境的当前目录和数据库中。\n注意事项: 用 CREATE TABLE 语句注册的表既可以作为表源，也可以作为表汇，在 DMLs 中没有引用之前，我们不能决定它是作为表源还是表汇使用。\nLIKE 子句\nLIKE 子句是 SQL 特征的变体/组合（特征 T171，\u0026ldquo;表定义中的 LIKE 子句\u0026quot;和特征 T173，\u0026ldquo;表定义中的扩展 LIKE 子句\u0026rdquo;）。该子句可用于根据现有表的定义创建一个表。此外，用户还可以扩展原表或排除其中的某些部分。与 SQL 标准不同的是，该子句必须在 CREATE 语句的顶层定义。这是因为该子句适用于定义的多个部分，而不仅仅是模式部分。\n你可以使用该子句来重用（并可能覆盖）某些连接器属性，或者为外部定义的表添加水印。例如，您可以为 Apache Hive 中定义的表添加水印。\n请考虑下面的示例语句。\nCREATETABLEOrders(userBIGINT,productSTRING,order_timeTIMESTAMP(3))WITH(\u0026#39;connector\u0026#39;=\u0026#39;kafka\u0026#39;,\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;earliest-offset\u0026#39;);CREATETABLEOrders_with_watermark(-- Add watermark definition WATERMARKFORorder_timeASorder_time-INTERVAL\u0026#39;5\u0026#39;SECOND)WITH(-- Overwrite the startup-mode \u0026#39;scan.startup.mode\u0026#39;=\u0026#39;latest-offset\u0026#39;)LIKEOrders;由此产生的 Orders_with_watermark 表将等同于用以下语句创建的表。\nCREATETABLEOrders_with_watermark(userBIGINT,productSTRING,order_timeTIMESTAMP(3),WATERMARKFORorder_timeASorder_time-INTERVAL\u0026#39;5\u0026#39;SECOND)WITH(\u0026#39;connector\u0026#39;=\u0026#39;kafka\u0026#39;,\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;latest-offset\u0026#39;);可以用同类选项控制表功能的合并逻辑。\n您可以控制以下的合并行为:\n CONSTRAINTS - 主键和唯一键等约束条件。 GENERATED-计算列 OPTIONS - 描述连接器和格式属性的连接器选项。 PARTITIONS - 表的分区 WATERMARKS - 水印声明  有三种不同的合并策略。\n INCLUDING - 包括源表的特征，对重复的条目失败，例如，如果两个表中都存在相同键的选项。 EXCLUDING - 不包含源表的给定特征。 OVERWRITING - 包括源表的特征，用新表的属性覆盖源表的重复条目，例如，如果两个表中都存在具有相同键的选项，则将使用当前语句中的选项。  此外，可以使用 INCLUDING/EXCLUDING ALL 选项来指定如果没有定义特定的策略应该是什么，即如果使用 EXCLUDING ALL INCLUDING WATERMARKS，则只从源表中包含水印。\n例子：\n-- A source table stored in a filesystem CREATETABLEOrders_in_file(userBIGINT,productSTRING,order_time_stringSTRING,order_timeASto_timestamp(order_time))PARTITIONEDBYuserWITH(\u0026#39;connector\u0026#39;=\u0026#39;filesystem\u0026#39;\u0026#39;path\u0026#39;=\u0026#39;...\u0026#39;);-- A corresponding table we want to store in kafka CREATETABLEOrders_in_kafka(-- Add watermark definition WATERMARKFORorder_timeASorder_time-INTERVAL\u0026#39;5\u0026#39;SECOND)WITH(\u0026#39;connector\u0026#39;:\u0026#39;kafka\u0026#39;...)LIKEOrders_in_file(-- Exclude everything besides the computed columns which we need to generate the watermark for. -- We do not want to have the partitions or filesystem options as those do not apply to kafka. EXCLUDINGALLINCLUDINGGENERATED);如果您没有提供同类选项，则默认使用 INCLUDING ALL OVERWRITING OPTIONS。\n注意: 您无法控制合并物理字段的行为。这些字段将被合并，就像您应用 INCLUDING 策略一样。\nCREATE CATALOG CREATECATALOGcatalog_nameWITH(key1=val1,key2=val2,...)用给定的目录属性创建一个目录。如果已经存在同名的目录，则会产生异常。\nWITH OPTIONS\n目录属性，用于存储与本目录相关的额外信息。表达式 key1=val1 的键和值都应该是字符串文字。\n更多详情请查看目录。\nCREATE DATABASE CREATEDATABASE[IFNOTEXISTS][catalog_name.]db_name[COMMENTdatabase_comment]WITH(key1=val1,key2=val2,...)用给定的数据库属性创建一个数据库，如果目录中已经存在同名的数据库，则抛出异常。如果目录中已经存在相同名称的数据库，则会抛出异常。\nIF NOT EXISTS\n如果数据库已经存在，则不会发生任何事情。\nWITH OPTIONS\n数据库属性，用于存储与本数据库相关的额外信息。表达式 key1=val1 的键和值都应该是字符串文字。\nCREATE VIEW CREATE[TEMPORARY]VIEW[IFNOTEXISTS][catalog_name.][db_name.]view_name[{columnName[,columnName]*}][COMMENTview_comment]ASquery_expression用给定的查询表达式创建一个视图，如果目录中已经存在同名的视图，则抛出异常。如果目录中已经存在同名的视图，则会抛出异常。\nTEMPORARY\n创建具有目录和数据库命名空间并覆盖视图的临时视图。\nIF NOT EXISTS\n如果视图已经存在，则不会发生任何事情。\nCREATE FUNCTION CREATE[TEMPORARY|TEMPORARYSYSTEM]FUNCTION[IFNOTEXISTS][catalog_name.][db_name.]function_nameASidentifier[LANGUAGEJAVA|SCALA|PYTHON]创建一个目录函数，该函数具有目录和数据库的名称空间，并带有标识符和可选的语言标签。如果目录中已经存在同名函数，则会抛出一个异常。\n如果语言标签是 JAVA/SCALA，标识符是 UDF 的完整 classpath。关于 Java/Scala UDF 的实现，请参考 User-defined Functions 了解详情。\n如果语言标签是 PYTHON，标识符是 UDF 的完全限定名，例如 pyflink.table.test.test_udf.add。关于 Python UDF 的实现，更多细节请参考 Python UDFs。\nTEMPORARY\n创建具有目录和数据库命名空间并覆盖目录功能的临时目录功能。\nTEMPORARY SYSTEM\n创建没有命名空间并覆盖内置函数的临时系统函数。\nIF NOT EXISTS\n如果函数已经存在，则不会发生任何事情。\nLANGUAGE JAVA|SCALA|PYTHON\n语言标签，用于指示 Flink 运行时如何执行函数。目前只支持 JAVA、SCALA 和 PYTHON，函数的默认语言是 JAVA。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/create.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-create-statements/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"Create 语句"},{"categories":["Flink"],"contents":"DataSet 转换 本文档深入介绍了 DataSets 上可用的转换。关于 Flink Java API 的一般介绍，请参考编程指南。\n对于密集索引的数据集中的压缩元素，请参考压缩元素指南。\nMap Map 转换将用户定义的映射函数应用于 DataSet 的每个元素。它实现了一对一的映射，也就是说，函数必须准确地返回一个元素。\n下面的代码将一个由整数对组成的 DataSet 转化为一个由整数组成的 DataSet。\nval intPairs: DataSet[(Int, Int)] = // [...] val intSums = intPairs.map { pair =\u0026gt; pair._1 + pair._2 } FlatMap FlatMap 转换在 DataSet 的每个元素上应用了一个用户定义的 flat-map 函数。这种映射函数的变体可以为每个输入元素返回任意多个结果元素（包括没有）。\n下面的代码将一个文本行的 DataSet 转换为一个单词的 DataSet。\nval textLines: DataSet[String] = // [...] val words = textLines.flatMap { _.split(\u0026#34; \u0026#34;) } MapPartition MapPartition 在一次函数调用中转换一个并行分区。map-partition 函数以 Iterable 的形式获取分区，并可以产生任意数量的结果值。每个分区中元素的数量取决于平行度和之前的操作。\n下面的代码将文本行的 DataSet 转换为每个分区的计数 DataSet。\nval textLines: DataSet[String] = // [...] // Some is required because the return value must be a Collection. // There is an implicit conversion from Option to a Collection. val counts = texLines.mapPartition { in =\u0026gt; Some(in.size) } Filter 过滤器转换将用户定义的过滤器函数应用于 DataSet 的每个元素，并且只保留那些函数返回为真的元素。\n以下代码从数据集中删除所有小于零的整数。\nval intNumbers: DataSet[Int] = // [...] val naturalNumbers = intNumbers.filter { _ \u0026gt; 0 } 重要：系统假设函数不会修改应用谓词的元素。违反这个假设会导致错误的结果。\n元组数据集的投影(Projection) Project 转换删除或移动 Tuple DataSet 的 Tuple 字段。project(int...) 方法通过其索引选择应该保留的 Tuple 字段，并定义它们在输出 Tuple 中的顺序。\n投影(Projection)不需要定义用户函数。\n下面的代码显示了在 DataSet 上应用 Project 转换的不同方法。\nDataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; in = // [...] // converts Tuple3\u0026lt;Integer, Double, String\u0026gt; into Tuple2\u0026lt;String, Integer\u0026gt; DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out = in.project(2,0); # scala Not supported. 分组数据集上的变换 reduce 操作可以对分组的数据集进行操作。指定用于分组的键可以通过多种方式进行。\n 键表达式 键选择器函数 一个或多个字段位置键（仅限元组数据集）。 case 类字段(仅 case 类)  请看一下 reduce 的例子，看看如何指定分组键。\n换算分组数据集 应用于分组数据集的 Reduce 转换，使用用户定义的 Reduce 函数将每个分组换算为一个元素。对于每一组输入元素，一个 Reduce 函数将成对的元素连续组合成一个元素，直到每组只剩下一个元素。\n请注意，对于一个 ReduceFunction，返回对象的键字段应该与输入值相匹配。这是因为 reduce 是隐式可组合的，当传递给 reduce 运算符时，从 combine 运算符发出的对象又是按键分组的。\n在按键表达式分组的数据集上进行 Reduce 操作 键表达式指定了 DataSet 中每个元素的一个或多个字段。每个键表达式都是一个公共字段的名称或一个 getter 方法。点号可以用来深入到对象中。键表达式 \u0026quot;*\u0026quot; 可以选择所有字段。下面的代码展示了如何使用键表达式对 POJO 数据集进行分组，并使用 reduce 函数对其进行换算。\n// some ordinary POJO class WC(val word: String, val count: Int) { def this() { this(null, -1) } // [...] } val words: DataSet[WC] = // [...] val wordCounts = words.groupBy(\u0026#34;word\u0026#34;).reduce { (w1, w2) =\u0026gt; new WC(w1.word, w1.count + w2.count) } 对按键选择器分组的数据集进行换算 键选择器函数从数据集的每个元素中提取一个键值。提取的键值用于对 DataSet 进行分组。下面的代码展示了如何使用键选择器函数对 POJO 数据集进行分组，并使用 reduce 函数对其进行换算。\n// some ordinary POJO class WC(val word: String, val count: Int) { def this() { this(null, -1) } // [...] } val words: DataSet[WC] = // [...] val wordCounts = words.groupBy { _.word } reduce { (w1, w2) =\u0026gt; new WC(w1.word, w1.count + w2.count) } 对按字段位置键分组的数据集进行换算（仅元组数据集） 字段位置键指定了一个 Tuple DataSet 的一个或多个字段，这些字段被用作分组键。下面的代码显示了如何使用字段位置键和应用 reduce 函数。\nval tuples = DataSet[(String, Int, Double)] = // [...] // group on the first and second Tuple field val reducedTuples = tuples.groupBy(0, 1).reduce { ... } 对按 case 类字段分组的数据集进行换算 当使用 Case Classes 时，你也可以使用字段的名称来指定分组键。\ncase class MyClass(val a: String, b: Int, c: Double) val tuples = DataSet[MyClass] = // [...] // group on the first and second field val reducedTuples = tuples.groupBy(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;).reduce { ... } 在分组数据集上进行分组换算 应用在分组 DataSet 上的 GroupReduce 转换，会对每个组调用用户定义的 group-reduce 函数。这与 Reduce 之间的区别在于，用户定义的函数可以一次性获得整个组。该函数是在一个组的所有元素上用一个 Iterable 调用的，并且可以返回任意数量的结果元素。\n在按字段位置键分组的数据集上进行分组 Reduce(只适用于元组数据集) 下面的代码显示了如何从一个按 Integer 分组的 DataSet 中删除重复的字符串。\nval input: DataSet[(Int, String)] = // [...] val output = input.groupBy(0).reduceGroup { (in, out: Collector[(Int, String)]) =\u0026gt; in.toSet foreach (out.collect) } 对按键表达式、键选择器函数或 case 类字段分组的数据集进行分组换算 类似于 Reduce 变换中的键表达式、键选择器函数和 case 类字段的工作。\n对排序组进行 GroupReduce 一个 group-reduce 函数使用一个 Iterable 访问一个组的元素。可选地，Iterable 可以按照指定的顺序输出一个组的元素。在许多情况下，这有助于降低用户定义的 group-reduce 函数的复杂性，并提高其效率。\n下面的代码显示了另一个例子，如何在一个由整数分组并按 String 排序的 DataSet 中删除重复的 String。\nval input: DataSet[(Int, String)] = // [...] val output = input.groupBy(0).sortGroup(1, Order.ASCENDING).reduceGroup { (in, out: Collector[(Int, String)]) =\u0026gt; var prev: (Int, String) = null for (t \u0026lt;- in) { if (prev == null || prev != t) out.collect(t) prev = t } } 注意：如果在 reduce 操作之前，使用运算符的基于排序的执行策略建立了分组，那么 GroupSort 通常是免费的。\n可组合的 GroupReduceFunctions 与 reduce 函数不同，group-reduce 函数是不可隐式组合的。为了使一个分组换算函数可以组合，它必须实现 GroupCombineFunction 接口。\n重要：GroupCombineFunction 接口的通用输入和输出类型必须等于 GroupReduceFunction 的通用输入类型，如下例所示。\n// Combinable GroupReduceFunction that computes two sums. class MyCombinableGroupReducer extends GroupReduceFunction[(String, Int), String] with GroupCombineFunction[(String, Int), (String, Int)] { override def reduce( in: java.lang.Iterable[(String, Int)], out: Collector[String]): Unit = { val r: (String, Int) = in.iterator.asScala.reduce( (a,b) =\u0026gt; (a._1, a._2 + b._2) ) // concat key and sum and emit  out.collect (r._1 + \u0026#34;-\u0026#34; + r._2) } override def combine( in: java.lang.Iterable[(String, Int)], out: Collector[(String, Int)]): Unit = { val r: (String, Int) = in.iterator.asScala.reduce( (a,b) =\u0026gt; (a._1, a._2 + b._2) ) // emit tuple with key and sum  out.collect(r) } } 在分组数据集上进行分组合并 GroupCombine 变换是可组合的 GroupReduceFunction 中 combine 步骤的泛化形式。与此相反，GroupReduce 函数中的 combine 步骤只允许从输入类型 I 到输出类型 I 的组合。这是因为 GroupReduce 函数中的 reduce 步骤期望输入类型 I。\n在某些应用中，希望在执行额外的转换（例如减少数据大小）之前，将一个数据集合并成中间格式。这可以通过一个 CombineGroup 转换来实现，而且成本很低。\n注意：对分组数据集的 GroupCombine 是在内存中以贪婪的策略执行的，它可能不会一次处理所有数据，而是分多个步骤进行。它也是在各个分区上执行的，而不像 GroupReduce 变换那样进行数据交换。这可能会导致部分结果。\n下面的例子演示了如何使用 CombineGroup 变换来实现另一种 WordCount。\nval input: DataSet[String] = [..] // The words received as input  val combinedWords: DataSet[(String, Int)] = input .groupBy(0) .combineGroup { (words, out: Collector[(String, Int)]) =\u0026gt; var key: String = null var count = 0 for (word \u0026lt;- words) { key = word count += 1 } out.collect((key, count)) } val output: DataSet[(String, Int)] = combinedWords .groupBy(0) .reduceGroup { (words, out: Collector[(String, Int)]) =\u0026gt; var key: String = null var sum = 0 for ((word, sum) \u0026lt;- words) { key = word sum += count } out.collect((key, sum)) } 上面的另一种 WordCount 实现演示了 GroupCombine 如何在执行 GroupReduce 转换之前组合单词。上面的例子只是一个概念证明。请注意，组合步骤如何改变 DataSet 的类型，通常在执行 GroupReduce 之前需要进行额外的 Map 转换。\n在分组元组数据集上进行聚合 有一些常用的聚合操作是经常使用的。Aggregate 转换提供了以下内置的聚合函数。\n Sum, Min, Max.  Aggregate 变换只能应用在 Tuple 数据集上，并且只支持字段位置键进行分组。\n下面的代码显示了如何在按字段位置键分组的数据集上应用\u0026quot;聚合\u0026quot;变换。\nval input: DataSet[(Int, String, Double)] = // [...] val output = input.groupBy(1).aggregate(SUM, 0).and(MIN, 2) 要在一个 DataSet 上应用多个聚合，必须在第一个聚合之后使用 .and() 函数，也就是说 .aggregary(SUM, 0).and(MIN, 2) 会产生原始 DataSet 的字段 0 和字段 2 的最小值之和。与此相反，.aggregary(SUM，0).aggregary(MIN，2) 将在一个聚合上应用一个聚合。在给定的示例中，它将在计算字段 0 与字段 1 分组后产生字段 2 的最小值。\n注意：聚合函数集将在未来得到扩展。\n对分组元组数据集的 MinBy / MaxBy 函数 MinBy (MaxBy) 转换为每组元组选择一个元组。被选择的元组是一个或多个指定字段的值是最小（最大）的元组。用于比较的字段必须是有效的关键字段，即可比较的字段。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。\n下面的代码显示了如何从 DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; 中选择具有相同 String 值的每组元组的 Integer 和 Double 字段最小值的元组。\nval input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input .groupBy(1) // group DataSet on second field  .minBy(0, 2) // select tuple with minimum values for first and third field. 换算整个数据集 Reduce 转换将用户定义的 reduce 函数应用于一个数据集的所有元素。随后，reduce 函数将元素对组合成一个元素，直到只剩下一个元素。\n下面的代码显示了如何对一个整数数据集的所有元素进行求和。\nval intNumbers = env.fromElements(1,2,3) val sum = intNumbers.reduce (_ + _) 使用 Reduce 转换换算一个完整的 DataSet 意味着最后的 Reduce 操作不能并行完成。然而，reduce 函数是可以自动组合的，因此 Reduce 转换不会限制大多数用例的可扩展性。\n对整个数据集进行分组换算 GroupReduce 转换将用户定义的 group-reduce 函数应用于 DataSet 的所有元素。group-reduce 可以遍历 DataSet 的所有元素，并返回任意数量的结果元素。\n下面的示例展示了如何在一个完整的 DataSet 上应用 GroupReduce 转换。\nval input: DataSet[Int] = // [...] val output = input.reduceGroup(new MyGroupReducer()) 注意：如果 group-reduce 函数不可组合，那么在一个完整的 DataSet 上的 GroupReduce 转换不能并行完成。因此，这可能是一个非常耗费计算的操作。请参阅上面的\u0026quot;可组合的 GroupReduceFunctions\u0026quot; 部分，了解如何实现可组合的 group-reduce 函数。\n在完整的数据集上进行分组合并(GroupCombine) 在一个完整的 DataSet 上的 GroupCombine 的工作原理类似于在一个分组的 DataSet 上的 GroupCombine。在所有节点上对数据进行分区，然后以贪婪的方式进行合并（即只有适合内存的数据才会一次性合并）。\n在完整的 Tuple 数据集上进行聚合 有一些常用的聚合操作是经常使用的。Aggregate 转换提供了以下内置的聚合函数。\n Sum, Min, 和 Max.  Aggregate 变换只能应用于 Tuple 数据集。\n下面的代码显示了如何在一个完整的数据集上应用聚合转换。\nval input: DataSet[(Int, String, Double)] = // [...] val output = input.aggregate(SUM, 0).and(MIN, 2) 注意：扩展支持的聚合函数集是我们的路线图。\n在完整的元组数据集上实现 MinBy / MaxBy MinBy (MaxBy) 转换从一个元组数据集中选择一个元组。被选择的元组是一个或多个指定字段的值是最小（最大）的元组。用于比较的字段必须是有效的键字段，即可比较的字段。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。\n以下代码显示了如何从 DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; 中选择具有 Integer 和 Double 字段最大值的元组。\nval input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input .maxBy(0, 2) // select tuple with maximum values for first and third field. Distinct Distinct 转换计算源 DataSet 中不同元素的 DataSet。下面的代码从 DataSet 中删除所有重复的元素。\nval input: DataSet[(Int, String, Double)] = // [...] val output = input.distinct() 也可以使用以下方法改变 DataSet 中元素的区分方式。\n 一个或多个字段位置键（仅元组数据集）。 一个键选择器函数，或 一个键表达式  用字段位置键去重(Distinct) val input: DataSet[(Int, Double, String)] = // [...] val output = input.distinct(0,2) 用 KeySelector 函数去重(Distinct) val input: DataSet[Int] = // [...] val output = input.distinct {x =\u0026gt; Math.abs(x)} 用键表达式去重(Distinct) // some ordinary POJO case class CustomType(aName : String, aNumber : Int) { } val input: DataSet[CustomType] = // [...] val output = input.distinct(\u0026#34;aName\u0026#34;, \u0026#34;aNumber\u0026#34;) 也可以用通配符表示使用所有字段:\n// some ordinary POJO val input: DataSet[CustomType] = // [...] val output = input.distinct(\u0026#34;_\u0026#34;) Join Join 转换将两个 DataSets 连接成一个 DataSet。两个数据集的元素在一个或多个键上进行连接(join)，这些键可以通过使用\n 键选择器函数 一个或多个字段位置键（仅限 Tuple DataSet）。 case 类字段  有几种不同的方法来执行 Join 转换，如下所示。\n默认的 Join (Join into Tuple2) 默认的 Join 变换会产生一个新的 Tuple DataSet，它有两个字段。每个元组在第一个元组字段中持有第一个输入 DataSet 的 join 元素，在第二个字段中持有第二个输入 DataSet 的匹配元素。\n下面的代码显示了一个使用字段位置键的默认 Join 转换。\nval input1: DataSet[(Int, String)] = // [...] val input2: DataSet[(Double, Int)] = // [...] val result = input1.join(input2).where(0).equalTo(1) 用 Join 函数连接 Join 转换也可以调用用户定义的 join 函数来处理连接(joining)元组。join 函数接收第一个输入 DataSet 的一个元素和第二个输入 DataSet 的一个元素，并准确返回一个元素。\n下面的代码使用键选择器函数执行了一个带有自定义 java 对象的 DataSet 和一个 Tuple DataSet 的连接，并展示了如何使用用户定义的连接(join)函数。\ncase class Rating(name: String, category: String, points: Int) val ratings: DataSet[Ratings] = // [...] val weights: DataSet[(String, Double)] = // [...]  val weightedRatings = ratings.join(weights).where(\u0026#34;category\u0026#34;).equalTo(0) { (rating, weight) =\u0026gt; (rating.name, rating.points * weight._2) } 用 Flat-Join 函数连接 类似于 Map 和 FlatMap，FlatJoin 的行为方式与 Join 相同，但它不是返回一个元素，而是可以返回（收集）、零个、一个或多个元素。\ncase class Rating(name: String, category: String, points: Int) val ratings: DataSet[Ratings] = // [...] val weights: DataSet[(String, Double)] = // [...]  val weightedRatings = ratings.join(weights).where(\u0026#34;category\u0026#34;).equalTo(0) { (rating, weight, out: Collector[(String, Double)]) =\u0026gt; if (weight._2 \u0026gt; 0.1) out.collect(rating.name, rating.points * weight._2) } 用 Projection (Java Only) 连接 Join 变换可以使用投影(projection)构造结果元组，如下所示:\nDataSet\u0026lt;Tuple3\u0026lt;Integer, Byte, String\u0026gt;\u0026gt; input1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple4\u0026lt;Integer, String, Double, Byte\u0026gt;\u0026gt; result = input1.join(input2) // key definition on first DataSet using a field position key  .where(0) // key definition of second DataSet using a field position key  .equalTo(0) // select and reorder fields of matching tuples  .projectFirst(0,2).projectSecond(1).projectFirst(1); // scala Not supported. 用数据集大小提示 Join 为了引导优化器选择正确的执行策略，你可以提示要连接(join)的 DataSet 的大小，如下所示:\nval input1: DataSet[(Int, String)] = // [...] val input2: DataSet[(Int, String)] = // [...]  // hint that the second DataSet is very small val result1 = input1.joinWithTiny(input2).where(0).equalTo(0) // hint that the second DataSet is very large val result1 = input1.joinWithHuge(input2).where(0).equalTo(0) Join 算法提示 Flink 运行时可以以各种方式执行连接(join)。每一种可能的方式在不同的情况下都会优于其他方式。系统会尝试自动选择一种合理的方式，但也允许你手动选择一种策略，以防你想强制执行特定的连接(join)方式。\nval input1: DataSet[SomeType] = // [...] val input2: DataSet[AnotherType] = // [...]  // hint that the second DataSet is very small val result1 = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST).where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;) 有以下提示:\n  OPTIMIZER_CHOOSES: 相当于完全不给提示，让系统来选择。\n  BROADCAST_HASH_FIRST：广播第一个输入，并据此建立一个哈希表，由第二个输入探测。如果第一个输入的数据非常小，这是一个很好的策略。\n  BROADCAST_HASH_SECOND: 广播第二个输入，并从中建立一个哈希表，由第一个输入探测。如果第二个输入非常小，是一个很好的策略。\n  REPARTITION_HASH_FIRST：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第一个输入建立一个哈希表。如果第一个输入比第二个输入小，但两个输入都很大，这个策略就很好。注意：如果无法估计大小，也无法重新使用已有的分区和排序，系统就会使用这个默认的后备策略。\n  REPARTITION_HASH_SECOND：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第二个输入建立一个哈希表。如果第二个输入比第一个输入小，但两个输入仍然很大，这个策略就很好。\n  REPARTITION_SORT_MERGE：系统对每个输入进行分区（洗牌）（除非输入已经分区），并对每个输入进行排序（除非已经排序）。通过对排序后的输入进行流式合并来连接(join)这些输入。如果一个或两个输入都已经被排序，这个策略就很好。\n  外连接 OuterJoin 转换在两个数据集上执行左、右或全外连接。外连接与常规（内连接）类似，创建所有键值相等的元素对。此外，如果在另一侧没有找到匹配的键，\u0026ldquo;外侧\u0026quot;的记录（左、右，或者在完全的情况下两者都有）将被保留。匹配的一对元素（或一个元素和另一个输入的空值）被交给 JoinFunction 将这对元素变成一个元素，或交给 FlatJoinFunction 将这对元素变成任意多个（包括无）元素。\n两个 DataSets 的元素都是在一个或多个键上连接的，这些键可以通过使用\n 键选择器函数 一个或多个字段位置键（仅限 Tuple DataSet）。 case 类字段  OuterJoins 只支持 Java 和 Scala DataSet API。\n用 Join 函数进行外连接 OuterJoin 转换调用一个用户定义的 join 函数来处理连接元组。join 函数接收第一个输入 DataSet 的一个元素和第二个输入 DataSet 的一个元素，并准确地返回一个元素。根据外连接的类型（左、右、全），连接函数的两个输入元素中可以有一个是空的。\n下面的代码使用键选择器函数执行 DataSet 与自定义 java 对象和 Tuple DataSet 的左外连接，并展示了如何使用用户定义的连接函数。\ncase class Rating(name: String, category: String, points: Int) val movies: DataSet[(String, String)] = // [...] val ratings: DataSet[Ratings] = // [...]  val moviesWithPoints = movies.leftOuterJoin(ratings).where(0).equalTo(\u0026#34;name\u0026#34;) { (movie, rating) =\u0026gt; (movie._1, if (rating == null) -1 else rating.points) } 使用 Flat-Join 函数进行外连接 类似于 Map 和 FlatMap，一个带有 flat-join 函数的 OuterJoin 的行为与带有 join 函数的 OuterJoin 相同，但它不是返回一个元素，而是可以返回（收集）、零个、一个或多个元素。\nNot supported. Join 算法提示 Flink 运行时可以以各种方式执行外连接。每一种可能的方式在不同的情况下都会优于其他方式。系统试图自动选择一种合理的方式，但允许你手动选择一种策略，以防你想强制执行特定的外连接方式。\nval input1: DataSet[SomeType] = // [...] val input2: DataSet[AnotherType] = // [...]  // hint that the second DataSet is very small val result1 = input1.leftOuterJoin(input2, JoinHint.REPARTITION_SORT_MERGE).where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;) val result2 = input1.rightOuterJoin(input2, JoinHint.BROADCAST_HASH_FIRST).where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;) 有以下提示:\n  OPTIMIZER_CHOOSES: 相当于完全不给提示，让系统来选择。\n  BROADCAST_HASH_FIRST：广播第一个输入，并据此建立一个哈希表，由第二个输入探测。如果第一个输入的数据非常小，这是一个很好的策略。\n  BROADCAST_HASH_SECOND: 广播第二个输入，并从中建立一个哈希表，由第一个输入探测。如果第二个输入非常小，是一个很好的策略。\n  REPARTITION_HASH_FIRST：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第一个输入建立一个哈希表。如果第一个输入比第二个输入小，但两个输入仍然很大，这个策略就很好。\n  REPARTITION_HASH_SECOND：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第二个输入建立一个哈希表。如果第二个输入比第一个输入小，但两个输入仍然很大，这个策略就很好。\n  REPARTITION_SORT_MERGE：系统对每个输入进行分区（洗牌）（除非输入已经分区），并对每个输入进行排序（除非已经排序）。通过对排序后的输入进行流式合并来连接(join)这些输入。如果一个或两个输入都已经被排序，这个策略就很好。\n  注意：目前还不是所有的外连接类型都支持所有的执行策略。\n  LeftOuterJoin 支持:\n OPTIMIZER_CHOOSES BROADCAST_HASH_SECOND REPARTITION_HASH_SECOND REPARTITION_SORT_MERGE    RightOuterJoin 支持:\n OPTIMIZER_CHOOSES BROADCAST_HASH_FIRST REPARTITION_HASH_FIRST REPARTITION_SORT_MERGE    FullOuterJoin 支持:\n OPTIMIZER_CHOOSES REPARTITION_SORT_MERGE    Cross Cross 变换将两个 DataSets 组合成一个 DataSet。它建立了两个输入数据集元素的所有 pairwise 组合，即建立了一个笛卡尔积。Cross 变换要么在每对元素上调用用户定义的 cross 函数，要么输出一个 Tuple2。这两种模式如下所示。\n注意：Cross 是一个潜在的计算密集型操作，甚至可以挑战大型计算集群。\n使用用户定义函数进行交叉运算 Cross 变换可以调用一个用户定义的 cross 函数。cross 函数接收第一个输入的一个元素和第二个输入的一个元素，并正好返回一个结果元素。\n下面的代码展示了如何使用 cross 函数对两个 DataSets 进行交叉变换。\ncase class Coord(id: Int, x: Int, y: Int) val coords1: DataSet[Coord] = // [...] val coords2: DataSet[Coord] = // [...]  val distances = coords1.cross(coords2) { (c1, c2) =\u0026gt; val dist = sqrt(pow(c1.x - c2.x, 2) + pow(c1.y - c2.y, 2)) (c1.id, c2.id, dist) } 用数据集大小提示交叉 为了引导优化器选择正确的执行策略，你可以提示要交叉的 DataSet 的大小，如下所示。\nval input1: DataSet[(Int, String)] = // [...] val input2: DataSet[(Int, String)] = // [...]  // hint that the second DataSet is very small val result1 = input1.crossWithTiny(input2) // hint that the second DataSet is very large val result1 = input1.crossWithHuge(input2) CoGroup CoGroup 转换联合(jointly)处理两个 DataSets 的组。两个 DataSets 根据定义的键进行分组，共享同一键的两个 DataSets 的组被一起交给用户定义的共组(co-group)函数。如果对于一个特定的键来说，只有一个 DataSet 有一个组，那么 co-group 函数就会和这个组以及一个空组一起被调用。共组(co-group)函数可以分别迭代两个组的元素，并返回任意数量的结果元素。\n与 Reduce、GroupReduce 和 Join 类似，可以使用不同的键选择器方法来定义键。\n数据集上的 CoGroup val iVals: DataSet[(String, Int)] = // [...] val dVals: DataSet[(String, Double)] = // [...]  val output = iVals.coGroup(dVals).where(0).equalTo(0) { (iVals, dVals, out: Collector[Double]) =\u0026gt; val ints = iVals map { _._2 } toSet for (dVal \u0026lt;- dVals) { for (i \u0026lt;- ints) { out.collect(dVal._2 * i) } } } Union 产生两个 DataSets 的联合(union)，这两个 DataSets 必须是同一类型。两个以上 DataSets 的联合(union)可以通过多个联合(union)调用来实现，如下所示。\nval vals1: DataSet[(String, Int)] = // [...] val vals2: DataSet[(String, Int)] = // [...] val vals3: DataSet[(String, Int)] = // [...]  val unioned = vals1.union(vals2).union(vals3) Rebalance 均匀地重新平衡 DataSet 的并行分区，以消除数据倾斜。\nval in: DataSet[String] = // [...] // rebalance DataSet and apply a Map transformation. val out = in.rebalance().map { ... } Hash-Partition 在给定的键上对 DataSet 进行散列分割。键可以被指定为位置键、表达式键和键选择器函数（关于如何指定键，请参见 Reduce 示例）。\nval in: DataSet[(String, Int)] = // [...] // hash-partition DataSet by String value and apply a MapPartition transformation. val out = in.partitionByHash(0).mapPartition { ... } Range-Partition 在给定的键上 Range-partitions 一个 DataSet。键可以被指定为位置键、表达式键和键选择器函数（关于如何指定键，请参见 Reduce 示例）。\nval in: DataSet[(String, Int)] = // [...] // range-partition DataSet by String value and apply a MapPartition transformation. val out = in.partitionByRange(0).mapPartition { ... } Sort Partition 按照指定的顺序，在指定的字段上对 DataSet 的所有分区进行本地排序。字段可以被指定为字段表达式或字段位置（关于如何指定键，请参阅 Reduce 示例）。通过链式 sortPartition() 调用，可以在多个字段上对分区进行排序。\nval in: DataSet[(String, Int)] = // [...] // Locally sort partitions in ascending order on the second String field and // in descending order on the first String field. // Apply a MapPartition transformation on the sorted partitions. val out = in.sortPartition(1, Order.ASCENDING) .sortPartition(0, Order.DESCENDING) .mapPartition { ... } First-n 返回一个 DataSet 的前 n 个（任意）元素。First-n 可以应用于一个常规的 DataSet、一个分组的 DataSet 或一个分组排序的 DataSet。分组键可以被指定为键选择器函数或字段位置键（关于如何指定键，请参见 Reduce 示例）。\nval in: DataSet[(String, Int)] = // [...] // Return the first five (arbitrary) elements of the DataSet val out1 = in.first(5) // Return the first two (arbitrary) elements of each String group val out2 = in.groupBy(0).first(2) // Return the first three elements of each String group ordered by the Integer field val out3 = in.groupBy(0).sortGroup(1, Order.ASCENDING).first(3) 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/","tags":["Flink","Flink 官方文档","DataSet API"],"title":"Dataset 变换"},{"categories":["Flink"],"contents":"DROP 语句 DROP 语句用于从当前或指定的目录中删除一个注册的表/视图/函数。\nFlink SQL 目前支持以下 DROP 语句。\n DROP TABLE DROP DATABASE DROP VIEW DROP FUNCTION  运行一个 DROP 语句 DROP 语句可以用 TableEnvironment 的 executeSql()方法执行，也可以在 SQL CLI 中执行。executeSql()方法对于一个成功的 DROP 操作会返回\u0026rsquo;OK'，否则会抛出一个异常。\n下面的例子展示了如何在 TableEnvironment 和 SQL CLI 中运行 DROP 语句。\nval settings = EnvironmentSettings.newInstance()... val tableEnv = TableEnvironment.create(settings) // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) // a string array: [\u0026#34;Orders\u0026#34;] val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print()  // drop \u0026#34;Orders\u0026#34; table from catalog tableEnv.executeSql(\u0026#34;DROP TABLE Orders\u0026#34;) // an empty string array val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() DROP TABLE DROPTABLE[IFEXISTS][catalog_name.][db_name.]table_name删除一个给定表名的表。如果要删除的表不存在，则抛出一个异常。\nIF EXISTS\n如果该表不存在，就不会发生任何事情。\nDROP DATABASE DROPDATABASE[IFEXISTS][catalog_name.]db_name[(RESTRICT|CASCADE)]删除一个给定数据库名称的数据库，如果要删除的数据库不存在，会产生异常。如果要删除的数据库不存在，则抛出一个异常。\nIF EXISTS\n如果数据库不存在，则不会发生任何事情。\nRESTRICT\n丢弃非空数据库会触发异常。默认为启用。\nCASCADE\n丢弃一个非空的数据库也会丢弃所有相关的表和函数。\nDROP VIEW DROP[TEMPORARY]VIEW[IFEXISTS][catalog_name.][db_name.]view_name丢弃一个有目录和数据库命名空间的视图。如果要删除的视图不存在，则会产生一个异常。\nTEMPORARY\n删除具有目录和数据库命名空间的临时视图。\nIF EXISTS\n如果视图不存在，则不会发生任何事情。\n维护依赖关系 Flink 没有通过 CASCADE/RESTRICT 关键字来维护视图的依赖关系，当前的方式是当用户试图在诸如视图的底层表被删除的情况下使用视图时，会产生延迟错误消息。\nDROP FUNCTION DROP[TEMPORARY|TEMPORARYSYSTEM]FUNCTION[IFEXISTS][catalog_name.][db_name.]function_name;删除一个有目录和数据库命名空间的目录函数。如果要放弃的函数不存在，则会产生一个异常。\nTEMPORARY\n丢弃具有目录和数据库命名空间的临时目录功能。\nTEMPORARY SYSTEM\n删除没有命名空间的临时系统函数。\nIF EXISTS\n如果函数不存在，就不会发生任何事情。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/drop.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","SQL"],"title":"Drop 语句"},{"categories":["Flink"],"contents":"EXPLAIN 语句 EXPLAIN 语句用于解释一个查询或 INSERT 语句的逻辑和优化查询计划。\n运行 EXPLAIN 语句 EXPLAIN 语句可以用 TableEnvironment 的 executeSql() 方法执行，也可以在 SQL CLI 中执行。executeSql() 方法在 EXPLAIN 操作成功后返回解释结果，否则将抛出一个异常。\n下面的例子展示了如何在 TableEnvironment 和 SQL CLI 中运行 EXPLAIN 语句。\nval env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // register a table named \u0026#34;Orders\u0026#34; tEnv.executeSql(\u0026#34;CREATE TABLE MyTable1 (count bigint, work VARCHAR(256) WITH (...)\u0026#34;) tEnv.executeSql(\u0026#34;CREATE TABLE MyTable2 (count bigint, work VARCHAR(256) WITH (...)\u0026#34;) // explain SELECT statement through TableEnvironment.explainSql() val explanation = tEnv.explainSql( \u0026#34;SELECT count, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT count, word FROM MyTable2\u0026#34;) println(explanation) // explain SELECT statement through TableEnvironment.executeSql() val tableResult = tEnv.executeSql( \u0026#34;EXPLAIN PLAN FOR \u0026#34; + \u0026#34;SELECT count, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT count, word FROM MyTable2\u0026#34;) tableResult.print() EXPLAIN 的结果是：\n== Abstract Syntax Tree == LogicalUnion(all=[true]) LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')]) FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word]) FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word]) == Optimized Logical Plan == DataStreamUnion(all=[true], union all=[count, word]) DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')]) TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word]) TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word]) == Physical Execution Plan == Stage 1 : Data Source content : collect elements with CollectionInputFormat Stage 2 : Data Source content : collect elements with CollectionInputFormat Stage 3 : Operator content : from: (count, word) ship_strategy : REBALANCE Stage 4 : Operator content : where: (LIKE(word, _UTF-16LE'F%')), select: (count, word) ship_strategy : FORWARD Stage 5 : Operator content : from: (count, word) ship_strategy : REBALANCE 语法 EXPLAINPLANFOR\u0026lt;query_statement_or_insert_statement\u0026gt;关于查询语法，请参考查询页面。关于 INSERT，请参考 INSERT 页面。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","SQL"],"title":"Explan 语句"},{"categories":["Flink"],"contents":"Flink DataSet API 编程指南 Flink 中的数据集程序是对数据集实现转换（如过滤、映射、加入、分组）的常规程序。数据集最初是从某些来源创建的（例如，通过读取文件，或从本地集合中创建）。结果通过汇返回，例如可以将数据写入（分布式）文件，或标准输出（例如命令行终端）。Flink 程序可以在各种环境下运行，独立运行，或者嵌入其他程序中。执行可以发生在本地 JVM 中，也可以发生在许多机器的集群中。\n请参考 DataStream API 概述，了解 Flink API 的基本概念。该概述是针对 DataStream API 的，但这两个 API 的基本概念是一样的。\n为了创建你自己的 Flink DataSet 程序，我们鼓励你从 Flink 程序的骨架开始，并逐步添加你自己的转换。其余部分作为附加操作和高级功能的参考。\n程序示例 下面的程序是一个完整的、可以使用的 WordCount 的例子，你可以复制和粘贴代码在本地运行。你可以复制和粘贴代码在本地运行它。你只需要在你的项目中加入正确的 Flink 的库（参见与 Flink 的链接部分）并指定导入。然后你就可以开始了\nimport org.apache.flink.api.scala._ object WordCount { def main(args: Array[String]) { val env = ExecutionEnvironment.getExecutionEnvironment val text = env.fromElements( \u0026#34;Who\u0026#39;s there?\u0026#34;, \u0026#34;I think I hear them. Stand, ho! Who\u0026#39;s there?\u0026#34;) val counts = text.flatMap { _.toLowerCase.split(\u0026#34;\\\\W+\u0026#34;) filter { _.nonEmpty } } .map { (_, 1) } .groupBy(0) .sum(1) counts.print() } } DataSet 转换 数据转换将一个或多个 DataSet 转换为一个新的 DataSet。程序可以将多个转换组合成复杂的集合。\n本节简要介绍了可用的转换。转换文档中有所有变换的完整描述和示例。\n Map  接受一个元素，产生一个元素。\ndata.map { x =\u0026gt; x.toInt }  FlatMap  接受一个元素并产生零、一个或多个元素。\ndata.flatMap { str =\u0026gt; str.split(\u0026#34; \u0026#34;) }  MapPartition  在一个函数调用中转换一个并行分区。该函数以\u0026quot;迭代器\u0026quot;的形式获取分区，并可产生任意数量的结果值。每个分区的元素数量取决于平行度和之前的操作。\ndata.mapPartition { in =\u0026gt; in map { (_, 1) } }  Filter  对每个元素进行布尔函数评估，并保留那些函数返回真的元素。 重要：系统假设函数不会修改应用谓词的元素。违反这个假设会导致错误的结果。\ndata.filter { _ \u0026gt; 1000 }  Reduce  通过重复将两个元素合并为一个元素，将一组元素合并为一个元素。换算可以应用于一个完整的数据集，也可以应用于一个分组的数据集。\ndata.reduce { _ + _ }  ReduceGroup  将一组元素合并成一个或多个元素。ReduceGroup 可以应用在一个完整的数据集上，也可以应用在一个分组的数据集上。\ndata.reduceGroup { elements =\u0026gt; elements.sum }  Aggregate  将一组值聚合成一个值。Aggregation 函数可以被认为是内置的 reduce 函数。Aggregate 可以应用于一个完整的数据集，也可以应用于一个分组的数据集。\nval input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input.aggregate(SUM, 0).aggregate(MIN, 2) 你也可以使用简写语法来进行 minimum, maximum 和 sum 的聚合。\nval input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input.sum(0).min(2)  Distinct  返回数据集的不同元素。它从输入的 DataSet 中删除元素的所有字段或字段子集的重复条目。\ndata.distinct()  Join  通过创建所有键值相等的元素对来连接两个数据集。可以选择使用 JoinFunction 将一对元素变成一个元素，或者使用 FlatJoinFunction 将一对元素变成任意多个（包括无）元素。请参阅键部分了解如何定义连接键。\n// In this case tuple fields are used as keys. \u0026#34;0\u0026#34; is the join field on the first tuple // \u0026#34;1\u0026#34; is the join field on the second tuple. val result = input1.join(input2).where(0).equalTo(1) 你可以通过 Join Hints 指定运行时执行连接的方式。这些提示描述了连接是通过分区还是广播进行的，以及它是使用基于排序还是基于散列的算法。请参考转换指南，了解可能的提示列表和示例。 如果没有指定提示，系统将尝试对输入大小进行估计，并根据这些估计选择最佳策略。\n// This executes a join by broadcasting the first data set // using a hash table for the broadcast data val result = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST) .where(0).equalTo(1) 请注意，连接转换只适用于等价连接。其他的连接类型需要使用 OuterJoin 或 CoGroup 来表达。\n OuterJoin  在两个数据集上执行左联接、右联接或完全外联接。外联接与常规（内联接）类似，创建所有键值相同的元素对。此外，如果在另一侧没有找到匹配的键，\u0026ldquo;外侧\u0026quot;的记录（左、右或全联接时两者都有）将被保留。匹配的元素对（或一个元素和另一个输入的 null 值）被交给 JoinFunction 将这对元素变成单个元素，或交给 FlatJoinFunction 将这对元素变成任意多个（包括无）元素。请参阅键部分，了解如何定义连接键。\nval joined = left.leftOuterJoin(right).where(0).equalTo(1) { (left, right) =\u0026gt; val a = if (left == null) \u0026#34;none\u0026#34; else left._1 (a, right) }  CoGroup  减少操作的二维变体。在一个或多个字段上对每个输入进行分组，然后将分组合并。每一对组都会调用转换函数。请参阅键部分，了解如何定义 coGroup 键。\ndata1.coGroup(data2).where(0).equalTo(1)  Cross  建立两个输入的笛卡尔乘积（交叉乘积），创建所有元素对。可选择使用交叉函数将一对元素变成一个单一元素。\nval data1: DataSet[Int] = // [...] val data2: DataSet[String] = // [...] val result: DataSet[(Int, String)] = data1.cross(data2) 注意：Cross 可能是一个非常耗费计算的操作，甚至可以挑战大型计算集群！建议使用 crossWithTiny() 和 crossWithHuge() 来提示系统数据集的大小。\n  Union\n  产生两个数据集的并集。\n  data.union(data2)  Rebalance  均匀地重新平衡数据集的并行分区，以消除数据倾斜。只有类似于 Map 的变换才可以跟随重新平衡(rebalance)变换。\nval data1: DataSet[Int] = // [...] val result: DataSet[(Int, String)] = data1.rebalance().map(...)  Hash-Partition  在给定的键上对数据集进行散列分区。键可以被指定为位置键、表达式键和键选择函数。\nval in: DataSet[(Int, String)] = // [...] val result = in.partitionByHash(0).mapPartition { ... }  Range-Partition  在给定的键上按照范围分割数据集。键可以被指定为位置键、表达式键和键选择函数。\nval in: DataSet[(Int, String)] = // [...] val result = in.partitionByRange(0).mapPartition { ... }  自定义分区  使用自定义的 Partitioner 函数，根据键将记录分配到特定的分区。键可以指定为位置键、表达式键和键选择函数。 注意：此方法仅适用于单个字段键。\nval in: DataSet[(Int, String)] = // [...] val result = in .partitionCustom(partitioner, key).mapPartition { ... }  Sort Partition  按照指定的顺序对数据集的所有分区进行本地排序。字段可以指定为元组位置或字段表达式。对多个字段的排序是通过链式 sortPartition() 调用完成的。\nval in: DataSet[(Int, String)] = // [...] val result = in.sortPartition(1, Order.ASCENDING).mapPartition { ... }  First-n  返回一个数据集的前 n 个（任意）元素。First-n 可以应用于一个常规数据集、一个分组数据集或一个分组排序数据集。分组键可以指定为键选择函数、元组位置或 case 类字段。\nval in: DataSet[(Int, String)] = // [...] // regular data set val result1 = in.first(3) // grouped data set val result2 = in.groupBy(0).first(3) // grouped-sorted data set val result3 = in.groupBy(0).sortGroup(1, Order.ASCENDING).first(3) 以下转换可用于元组的数据集。\n MinBy / MaxBy  从一组元组中选择一个元组，这些元组的一个或多个字段的值是最小的（最大的）。用于比较的字段必须是有效的键字段，即可比较。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。MinBy (MaxBy)可以应用于一个完整的数据集或一个分组数据集。\nval in: DataSet[(Int, Double, String)] = // [...] // a data set with a single tuple with minimum values for the Int and String fields. val out: DataSet[(Int, Double, String)] = in.minBy(0, 2) // a data set with one tuple for each group with the minimum value for the Double field. val out2: DataSet[(Int, Double, String)] = in.groupBy(2) .minBy(1) 通过匿名模式匹配从 tuple、case 类和集合中提取，比如下面。\nval data: DataSet[(Int, String, Double)] = // [...] data.map { case (id, name, temperature) =\u0026gt; // [...] } 不受 API 开箱即用的支持。要使用这个功能，你应该使用 Scala API 扩展。\n变换的并行度可以通过 setParallelism(int) 来定义，而 name(String) 可以给变换指定一个自定义的名称，这对调试很有帮助。数据源和数据接收器也是如此。\nwithParameters(Configuration) 传递 Configuration 对象，这些对象可以从用户函数里面的 open() 方法访问。\n指定键 一些转换（join、coGroup、groupBy）需要在元素集合上定义一个键。其他转换（Reduce、GroupReduce、Aggregate）允许在应用之前将数据按键分组。\n一个 DataSet 被分组为：\nDataSet\u0026lt;...\u0026gt; input = // [...] DataSet\u0026lt;...\u0026gt; reduced = input .groupBy(/*define key here*/) .reduceGroup(/*do something*/); Flink 的数据模型不是基于键值对的。因此，你不需要将数据集类型物理地打包成键和值。键是\u0026quot;虚拟的\u0026rdquo;：它们被定义为实际数据上的函数，以指导分组操作符。\n为元组定义键 最简单的情况是对 Tuple 的一个或多个字段进行分组。\nval input: DataSet[(Int, String, Long)] = // [...] val keyed = input.groupBy(0) 元组在第一个字段（整数类型的字段）上进行分组。\nval input: DataSet[(Int, String, Long)] = // [...] val grouped = input.groupBy(0,1) 在这里，我们将元组放在一个由第一个字段和第二个字段组成的复合键上。\n关于嵌套 Tuple 的说明。如果你的 DataSet 有一个嵌套的元组，比如：\nDataSet\u0026lt;Tuple3\u0026lt;Tuple2\u0026lt;Integer, Float\u0026gt;,String,Long\u0026gt;\u0026gt; ds; 指定 groupBy(0) 将使系统使用完整的 Tuple2 作为键（以 Integer 和 Float 为键）。如果要\u0026quot;导航\u0026quot;到嵌套的 Tuple2 中，就必须使用字段表达式键，下面将对其进行说明。\n使用字段表达式定义键 你可以使用基于字符串的字段表达式来引用嵌套的字段，并为分组、排序、连接(join)或 coGrouping 定义键。\n字段表达式可以非常容易地选择（嵌套的）复合类型中的字段，如 Tuple 和 POJO 类型。\n在下面的例子中，我们有一个有两个字段 \u0026ldquo;word\u0026rdquo; 和 \u0026ldquo;count\u0026rdquo; 的 WC POJO。要按字段 word 进行分组，我们只需将其名称传递给 groupBy() 函数。\n// some ordinary POJO (Plain old Java Object) class WC(var word: String, var count: Int) { def this() { this(\u0026#34;\u0026#34;, 0L) } } val words: DataSet[WC] = // [...] val wordCounts = words.groupBy(\u0026#34;word\u0026#34;) // or, as a case class, which is less typing case class WC(word: String, count: Int) val words: DataSet[WC] = // [...] val wordCounts = words.groupBy(\u0026#34;word\u0026#34;) 字段表达式语法   通过字段名选择 POJO 字段。例如 \u0026ldquo;user\u0026rdquo; 指的是 POJO 类型的 \u0026ldquo;user\u0026rdquo; 字段。\n  通过 1-offset 字段名或 0-offset 字段索引来选择 Tuple 字段。例如 \u0026ldquo;_1\u0026rdquo; 和 \u0026ldquo;5\u0026rdquo; 分别指 Scala Tuple 类型的第一和第六字段。\n  你可以在 POJO 和 Tuple 中选择嵌套字段。例如 \u0026ldquo;user.zip\u0026rdquo; 指的是 POJO 的 \u0026ldquo;zip\u0026rdquo; 字段，它存储在 POJO 类型的 \u0026ldquo;user\u0026rdquo; 字段中。支持 POJO 和 Tuple 的任意嵌套和混合，如 \u0026ldquo;_2.user.zip\u0026rdquo; 或 \u0026ldquo;user._4.1.zip\u0026rdquo;。\n你可以使用 \u0026ldquo;_\u0026rdquo; 通配符表达式选择完整的类型。这也适用于不是 Tuple 或 POJO 类型的类型。\n字段表达式示例:\nclass WC(var complex: ComplexNestedClass, var count: Int) { def this() { this(null, 0) } } class ComplexNestedClass( var someNumber: Int, someFloat: Float, word: (Long, Long, String), hadoopCitizen: IntWritable) { def this() { this(0, 0, (0, 0, \u0026#34;\u0026#34;), new IntWritable(0)) } } 这些都是上面例子代码的有效字段表达式。\n  \u0026ldquo;count\u0026rdquo;: WC 类中的计数字段\n  \u0026ldquo;complex\u0026rdquo;: 递归选择 POJO 类型 ComplexNestedClass 的 complex 字段的所有字段。\n  \u0026ldquo;complex.word._3\u0026rdquo;: 选择嵌套的 Tuple3 的最后一个字段。\n  \u0026ldquo;complex.hadoopCitizen\u0026rdquo;: 选择 Hadoop IntWritable 类型。\n  使用键选择函数定义键 另一种定义键的方法是\u0026quot;键选择器\u0026quot;函数。键选择器函数将一个元素作为输入，并返回该元素的键。键可以是任何类型的，并且可以从确定性计算中得到。\n下面的例子显示了一个简单返回对象字段的键选择函数。\n// some ordinary case class case class WC(word: String, count: Int) val words: DataSet[WC] = // [...] val keyed = words.groupBy( _.word ) 数据源 数据源创建初始数据集，例如从文件或 Java 集合中创建。创建数据集的一般机制是在 InputFormat 后面抽象出来的。Flink 自带了几种内置的格式来从常见的文件格式创建数据集。其中许多格式在 ExecutionEnvironment 上有快捷方法。\n基于文件的:\n  readTextFile(path) / TextInputFormat - 读取文件并以字符串形式返回。\n  readTextFileWithValue(path) / TextValueInputFormat - 以行的方式读取文件并以 StringValues 的形式返回。StringValues 是可变字符串。\n  readCsvFile(path) / CsvInputFormat - 解析以逗号（或其他字符）分隔的文件。返回一个由 tuple、case 类对象或 POJOs 组成的 DataSet。支持基本的 java 类型及其对应的 Value 类型作为字段类型。\n  readFileOfPrimitives(path, delimiter) / PrimitiveInputFormat - 使用给定的定界符，解析新行（或其他字符序列）定界的基元数据类型的文件，如 String 或 Integer。\n  readSequenceFile(Key, Value, path) / SequenceFileInputFormat - 创建一个 JobConf 并从指定的路径读取文件，文件类型为 SequenceFileInputFormat，Key 类和 Value 类，并以 Tuple2\u0026lt;Key, Value\u0026gt; 的形式返回。\n  基于集合的:\n  fromCollection(Iterable) - 从一个 Iterable 创建一个数据集。Iterable 返回的所有元素必须是相同的类型。\n  fromCollection(Iterator) - 从一个 Iterator 创建一个数据集。该类指定了迭代器返回的元素的数据类型。\n  fromElements(elements: _*) - 从给定的对象序列中创建一个数据集。所有对象必须是相同的类型。\n  fromParallelCollection(SplittableIterator) - 从迭代器中并行创建一个数据集。该类指定了迭代器返回的元素的数据类型。\n  generateSequence(from, to) - 在给定的区间内并行生成数字序列。\n  通用的:\n  readFile(inputFormat, path) / FileInputFormat - 接受一个文件输入格式。\n  createInput(inputFormat) / InputFormat - 接受一个通用的输入格式。\n  示例:\nval env = ExecutionEnvironment.getExecutionEnvironment // read text file from local files system val localLines = env.readTextFile(\u0026#34;file:///path/to/my/textfile\u0026#34;) // read text file from an HDFS running at nnHost:nnPort val hdfsLines = env.readTextFile(\u0026#34;hdfs://nnHost:nnPort/path/to/my/textfile\u0026#34;) // read a CSV file with three fields val csvInput = env.readCsvFile[(Int, String, Double)](\u0026#34;hdfs:///the/CSV/file\u0026#34;) // read a CSV file with five fields, taking only two of them val csvInput = env.readCsvFile[(String, Double)]( \u0026#34;hdfs:///the/CSV/file\u0026#34;, includedFields = Array(0, 3)) // take the first and the fourth field  // CSV input can also be used with Case Classes case class MyCaseClass(str: String, dbl: Double) val csvInput = env.readCsvFile[MyCaseClass]( \u0026#34;hdfs:///the/CSV/file\u0026#34;, includedFields = Array(0, 3)) // take the first and the fourth field  // read a CSV file with three fields into a POJO (Person) with corresponding fields val csvInput = env.readCsvFile[Person]( \u0026#34;hdfs:///the/CSV/file\u0026#34;, pojoFields = Array(\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;zipcode\u0026#34;)) // create a set from some given elements val values = env.fromElements(\u0026#34;Foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;foobar\u0026#34;, \u0026#34;fubar\u0026#34;) // generate a number sequence val numbers = env.generateSequence(1, 10000000) // read a file from the specified path of type SequenceFileInputFormat val tuples = env.createInput(HadoopInputs.readSequenceFile(classOf[IntWritable], classOf[Text], \u0026#34;hdfs://nnHost:nnPort/path/to/file\u0026#34;)) 配置 CSV 解析 Flink 为 CSV 解析提供了许多配置选项。\n  lineDelimiter: 字符串指定单个记录的定界符。默认的行定界符是新行字符 '/n'。\n  fieldDelimiter: 字符串指定分隔记录字段的定界符。默认的字段定界符是逗号字符 ','。\n  includeFields: Array[Int] 定义从输入文件中读取哪些字段（以及忽略哪些字段）。默认情况下，前 n 个字段（由 type() 调用中的类型数定义）会被解析。\n  pojoFields: Array[String] 指定 POJO 的字段，这些字段被映射到 CSV 字段。CSV 字段的解析器会根据 POJO 字段的类型和顺序自动初始化。\n  parseQuotedStrings: 启用引号字符串解析的字符。如果字符串字段的第一个字符是引号字符，那么字符串将被解析为引号字符串（前导或尾部的空白不被修剪）。引号字符串中的字段定界符会被忽略。如果引号字符串字段的最后一个字符不是引号字符，则引号字符串解析失败。如果启用了引号字符串解析，且字段的第一个字符不是引号字符串，则该字符串将被解析为未引号字符串。默认情况下，引号字符串解析被禁用。\n  ignoreComments: 字符串指定一个注解前缀。所有以指定注解前缀开始的行都不会被解析和忽略。默认情况下，没有行被忽略。\n  lenient：布尔值，启用宽松解析。也就是说，不能正确解析的行会被忽略。默认情况下，禁用宽松解析，无效行会引发异常。\n  ignoreFirstLine: Boolean 配置 InputFormat 忽略输入文件的第一行。默认情况下，没有行被忽略。\n  Input Path 的递归遍历 对于基于文件的输入，当输入路径是一个目录时，默认情况下不会枚举嵌套文件。取而代之的是，只读取基础目录内的文件，而忽略嵌套文件。嵌套文件的递归枚举可以通过 recursive.file.enumeration 配置参数启用，就像下面的例子。\n// enable recursive enumeration of nested input files val env = ExecutionEnvironment.getExecutionEnvironment // create a configuration object val parameters = new Configuration // set the recursive enumeration parameter parameters.setBoolean(\u0026#34;recursive.file.enumeration\u0026#34;, true) // pass the configuration to the data source env.readTextFile(\u0026#34;file:///path/with.nested/files\u0026#34;).withParameters(parameters) 读取压缩文件 Flink 目前支持输入文件的透明解压，如果这些文件被标记为适当的文件扩展名。特别是，这意味着无需进一步配置输入格式，任何 FileInputFormat 都支持压缩，包括自定义输入格式。请注意，压缩文件可能不会被并行读取，从而影响作业的可扩展性。\n下表列出了当前支持的压缩方法。\n   压缩方法 文件后缀 并行性     DEFLATE .deflate no   GZip .gz, .gzip no   Bzip2 .bz2 no   XZ .xz no    数据接收器 数据接收器消费 DataSet 并用于存储或返回它们。数据接收器的操作是用 OutputFormat 来描述的。Flink 带有各种内置的输出格式，这些格式被封装在对 DataSet 的操作后面。\n writeAsText() / TextOutputFormat \u0026ndash;将元素逐行写成 Strings。通过调用每个元素的 toString() 方法获得字符串。 writeAsCsv(...) / CsvOutputFormat - 将元组写成逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的 toString() 方法。 print() / printToErr() - 在标准输出/标准错误流上打印每个元素的 toString() 值。 write() / FileOutputFormat - 用于自定义文件输出的方法和基类。支持自定义对象到字节的转换。 output()/ OutputFormat - 最通用的输出方法，用于非基于文件的数据接收器（如将结果存储在数据库中）。  一个 DataSet 可以被输入到多个操作中。程序可以写入或打印一个数据集，同时还可以对其进行额外的转换。\n示例\n标准数据接收器方法:\n// text data val textData: DataSet[String] = // [...]  // write DataSet to a file on the local file system textData.writeAsText(\u0026#34;file:///my/result/on/localFS\u0026#34;) // write DataSet to a file on an HDFS with a namenode running at nnHost:nnPort textData.writeAsText(\u0026#34;hdfs://nnHost:nnPort/my/result/on/localFS\u0026#34;) // write DataSet to a file and overwrite the file if it exists textData.writeAsText(\u0026#34;file:///my/result/on/localFS\u0026#34;, WriteMode.OVERWRITE) // tuples as lines with pipe as the separator \u0026#34;a|b|c\u0026#34; val values: DataSet[(String, Int, Double)] = // [...] values.writeAsCsv(\u0026#34;file:///path/to/the/result/file\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;|\u0026#34;) // this writes tuples in the text formatting \u0026#34;(a, b, c)\u0026#34;, rather than as CSV lines values.writeAsText(\u0026#34;file:///path/to/the/result/file\u0026#34;) // this writes values as strings using a user-defined formatting values map { tuple =\u0026gt; tuple._1 + \u0026#34; - \u0026#34; + tuple._2 } .writeAsText(\u0026#34;file:///path/to/the/result/file\u0026#34;) 本地排序输出 数据接收器的输出可以使用元组字段位置或字段表达式对指定字段按指定顺序进行本地排序。这适用于每一种输出格式。\n下面的示例展示了如何使用该功能。\nval tData: DataSet[(Int, String, Double)] = // [...] val pData: DataSet[(BookPojo, Double)] = // [...] val sData: DataSet[String] = // [...]  // sort output on String field in ascending order tData.sortPartition(1, Order.ASCENDING).print() // sort output on Double field in descending and Int field in ascending order tData.sortPartition(2, Order.DESCENDING).sortPartition(0, Order.ASCENDING).print() // sort output on the \u0026#34;author\u0026#34; field of nested BookPojo in descending order pData.sortPartition(\u0026#34;_1.author\u0026#34;, Order.DESCENDING).writeAsText(...) // sort output on the full tuple in ascending order tData.sortPartition(\u0026#34;_\u0026#34;, Order.ASCENDING).writeAsCsv(...) // sort atomic type (String) output in descending order sData.sortPartition(\u0026#34;_\u0026#34;, Order.DESCENDING).writeAsText(...) 目前还不支持全局排序输出。\n迭代运算符 迭代在 Flink 程序中实现了循环。迭代运算符封装了程序的一部分，并反复执行，将一次迭代的结果（部分解）反馈到下一次迭代中。Flink 中的迭代有两种类型。BulkIteration 和 DeltaIteration。\n本节提供了如何使用这两种运算符的快速示例。查看迭代介绍页面可以获得更详细的介绍。\n批量迭代 要创建一个 BulkIteration，调用迭代开始的 DataSet 的 iterate(int) 方法，同时指定一个 step 函数。step 函数获取当前迭代的输入 DataSet，并且必须返回一个新的 DataSet。迭代调用的参数是最大的迭代次数，迭代过后要停止。\n还有 iterateWithTermination(int) 函数，接受 step 函数，返回两个 DataSets。迭代步骤的结果和一个终止标准。一旦终止准则 DataSet 为空，就会停止迭代。\n下面的例子是迭代估计数字 Pi。目标是计算随机点的数量，这些随机点落入单位圆中。在每一次迭代中，都会挑选一个随机点。如果这个点位于单位圆内，我们就递增计数。然后，Pi 的估计值是所得到的计数除以迭代次数乘以 4。\nval env = ExecutionEnvironment.getExecutionEnvironment() // Create initial DataSet val initial = env.fromElements(0) val count = initial.iterate(10000) { iterationInput: DataSet[Int] =\u0026gt; val result = iterationInput.map { i =\u0026gt; val x = Math.random() val y = Math.random() i + (if (x * x + y * y \u0026lt; 1) 1 else 0) } result } val result = count map { c =\u0026gt; c / 10000.0 * 4 } result.print() env.execute(\u0026#34;Iterative Pi Example\u0026#34;) 你也可以查看 K-Means 的例子，它使用 BulkIteration 来聚类一组未标记的点。\n增量迭代 增量迭代利用了某些算法在每次迭代中不改变解的每个数据点的事实。\n除了在每次迭代中反馈的部分解（称为 workset），delta 迭代还保持着跨迭代的状态（称为解集），可以通过 delta 更新。迭代计算的结果是最后一次迭代后的状态。关于 delta 迭代的基本原理，请参考迭代简介。\n定义 DeltaIteration 与定义 BulkIteration 类似。对于 delta 迭代，两个数据集构成了每次迭代的输入（工作集和解集），并且在每次迭代中产生两个数据集作为结果（新工作集，解集 delta）。\n要创建一个 DeltaIteration 在初始解集上调用 iterateDelta(initialWorkset，maxIterations，key)。step 函数需要两个参数。(solutionSet, workset), 并且必须返回两个值: (solutionSetDelta, newWorkset).\n下面是一个 delta 迭代语法的例子。\n// read the initial data sets val initialSolutionSet: DataSet[(Long, Double)] = // [...]  val initialWorkset: DataSet[(Long, Double)] = // [...]  val maxIterations = 100 val keyPosition = 0 val result = initialSolutionSet.iterateDelta(initialWorkset, maxIterations, Array(keyPosition)) { (solution, workset) =\u0026gt; val candidateUpdates = workset.groupBy(1).reduceGroup(new ComputeCandidateChanges()) val deltas = candidateUpdates.join(solution).where(0).equalTo(0)(new CompareChangesToCurrent()) val nextWorkset = deltas.filter(new FilterByThreshold()) (deltas, nextWorkset) } result.writeAsCsv(outputPath) env.execute() 在函数中对数据对象进行操作 Flink 的运行时以 Java 对象的形式与用户函数交换数据。函数从运行时接收输入对象作为方法参数，并返回输出对象作为结果。因为这些对象是由用户函数和运行时代码访问的，所以理解和遵循用户代码如何访问，即读取和修改这些对象的规则是非常重要的。\n用户函数以常规方法参数（如 MapFunction）或通过 Iterable 参数（如 GroupReduceFunction）从 Flink 的运行时接收对象。我们把运行时传递给用户函数的对象称为输入对象。用户函数可以将对象作为方法返回值（像 MapFunction）或通过 Collector（像 FlatMapFunction）发射给 Flink 运行时。我们将用户函数向运行时发射的对象称为输出对象。\nFlink 的 DataSet API 具有两种模式，它们在 Flink 的运行时如何创建或重用输入对象方面有所不同。这种行为会影响用户函数如何与输入和输出对象交互的保证和约束。下面的章节定义了这些规则，并给出了编写安全用户函数代码的编码指南。\n禁用对象重用(DEFAULT) 默认情况下，Flink 在禁用对象重用模式下运行。这种模式可以保证函数在函数调用中总是接收新的输入对象。对象重用禁用模式能提供更好的保证，使用起来也更安全。但是，它有一定的处理开销，可能会引起较高的 Java 垃圾收集活动。下表解释了在禁用对象重用模式下，用户函数如何访问输入和输出对象。\n   操作 保证和限制     读取输入对象 在一个方法调用中，保证输入对象的值不会改变。这包括由 Iterable 服务的对象。例如，在 List 或 Map 中收集由 Iterable 服务的输入对象是安全的。请注意，在方法调用离开后，对象可能会被修改。跨函数调用记忆对象是不安全的。   修改输入对象 你可以修改输入对象。   发射输入对象 你可以发射输入对象。输入对象的值可能在发射后发生变化。读取发射后的输入对象是不安全的。   读取输出对象 给予收集器的对象或作为方法结果返回的对象可能已经改变了其值。读取输出对象是不安全的。   修改输出对象 你可以在对象被发射后对其进行修改，然后再次发射。    对象重用禁用（默认）模式的编码准则。\n 不要跨方法调用记忆和读取输入对象。 不要在发出对象后读取对象。  启用对象重用 在启用对象重用模式下，Flink 的运行时会尽量减少对象实例化的数量。这可以提高性能，并且可以减少 Java 垃圾收集的压力。通过调用 ExecutionConfig.enableObjectReuse() 激活对象重用启用模式。下表解释了在启用对象重用模式下，用户函数如何访问输入和输出对象。\n   操作 保证和限制     读取作为常规方法参数接收的输入对象 作为常规方法参数接收的输入对象在一次函数调用中不被修改。对象可能在方法调用结束后被修改。跨函数调用记忆对象是不安全的。   读取从 Iterable 参数中接收到的输入对象 从 Iterable 中接收到的输入对象只在调用 next()方法之前有效。一个 Iterable 或 Iterator 可以多次服务于同一个对象实例。记住从 Iterable 接收的输入对象是不安全的，例如，把它们放在 List 或 Map 中。   修改输入对象 除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(reuse)的输入对象外，你不得修改输入对象。   发射输入对象 除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(重用)的输入对象外，你不得发射输入对象。   读取输出对象 一个被交给 Collector 或作为方法结果返回的对象可能已经改变了它的值。读取输出对象是不安全的。   修改输出对象 你可以修改一个输出对象并再次发出它。    启用对象重用的编码准则。\n 不记忆从 Iterable 接收的输入对象。 不记忆和读取跨方法调用的输入对象。 除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(reuse)的输入对象外，不要修改或发出输入对象。 为了减少对象实例化，你总是可以发出一个专门的输出对象，这个对象被反复修改，但从不读取。  调试 在分布式集群中的大型数据集上运行数据分析程序之前，最好确保所实现的算法能够按照预期的方式运行。因此，实现数据分析程序通常是一个检查结果、调试和改进的渐进过程。\nFlink 提供了一些不错的功能，通过支持 IDE 内的本地调试、注入测试数据和收集结果数据，大大简化了数据分析程序的开发过程。本节给大家一些提示，如何简化 Flink 程序的开发。\n本地执行环境 LocalEnvironment 在它创建的同一个 JVM 进程中启动 Flink 系统。如果你从 IDE 中启动 LocalEnvironment，你可以在代码中设置断点，轻松调试你的程序。\nLocalEnvironment 的创建和使用方法如下。\nval env = ExecutionEnvironment.createLocalEnvironment() val lines = env.readTextFile(pathToTextFile) // build your program  env.execute() 收集数据源和接收器 为分析程序提供输入并检查其输出，如果通过创建输入文件和读取输出文件来完成，是很麻烦的。Flink 具有特殊的数据源和接收器，这些数据源和接收器由 Java 集合支持，以方便测试。一旦程序经过测试，源和接收器可以很容易地被从 HDFS 等外部数据存储中读取/写入的源和接收器所替代。\n集合数据源可以使用以下方式。\nval env = ExecutionEnvironment.createLocalEnvironment() // Create a DataSet from a list of elements val myInts = env.fromElements(1, 2, 3, 4, 5) // Create a DataSet from any Collection val data: Seq[(String, Int)] = ... val myTuples = env.fromCollection(data) // Create a DataSet from an Iterator val longIt: Iterator[Long] = ... val myLongs = env.fromCollection(longIt) 注：目前，集合数据源要求数据类型和迭代器实现 Serializable。此外，集合数据源不能并行执行（ parallelism = 1）。\n语义注解 语义注解可以用来给 Flink 提供关于函数行为的提示。它们告诉系统，函数读取并评估了函数输入的哪些字段，以及它将哪些字段从输入转发到输出，而没有进行修改。语义注解是加快执行速度的有力手段，因为它们允许系统推理出在多个操作中重复使用排序顺序或分区的问题。使用语义注解最终可能会使程序免于不必要的数据洗牌或不必要的排序，并显著提高程序的性能。\n注意：语义注解的使用是可选的。然而，在提供语义注解时，保守地使用语义注解是绝对关键的! 不正确的语义注解将导致 Flink 对你的程序做出不正确的假设，并可能最终导致不正确的结果。如果一个操作符的行为不是明确可预测的，就不应该提供注解。请仔细阅读文档。\n目前支持以下语义注解。\n转发字段注解\n转发字段信息声明了未被修改的输入字段被函数转发到输出中的同一位置或另一位置。该信息被优化器用来推断数据属性（如排序或分区）是否被函数保留。对于对输入元素组进行操作的函数，如 GroupReduce、GroupCombine、CoGroup 和 MapPartition，所有被定义为转发字段的字段必须总是从同一个输入元素联合转发。由组智函数发出的每个元素的转发字段可能来源于函数的输入组的不同元素。\n字段转发信息使用字段表达式来指定。在输出中转发到同一位置的字段可以通过其位置来指定。指定的位置必须对输入和输出的数据类型有效，并具有相同的类型。例如字符串 \u0026ldquo;f2 \u0026ldquo;声明 Java 输入元组的第三个字段总是等于输出元组中的第三个字段。\n将输入中的源字段和输出中的目标字段指定为字段表达式，就可以声明未修改的字段转发到输出中的另一个位置。字符串 \u0026quot;f0-\u0026gt;f2\u0026quot; 表示将 Java 输入元组的第一个字段不变的复制到 Java 输出元组的第三个字段。通配符表达式 * 可以用来指代整个输入或输出类型，即 \u0026quot;f0-\u0026gt;*\u0026quot; 表示一个函数的输出总是等于其 Java 输入元组的第一个字段。\n多个转发字段可以在一个字符串中用分号隔开声明为 \u0026quot;f0; f2-\u0026gt;f1; f3-\u0026gt;f2\u0026quot;，也可以在单独的字符串中声明为 \u0026ldquo;f0\u0026rdquo;、\u0026ldquo;f2-\u0026gt;f1\u0026rdquo;、\u0026ldquo;f3-\u0026gt;f2\u0026rdquo;。当指定转发字段时，不要求所有的转发字段都声明，但所有的声明必须正确。\n转发字段信息可以通过在函数类定义上附加 Java 注解来声明，或者在调用 DataSet 上的函数后将其作为操作符参数传递，如下图所示。\n函数类注解\n @ForwardedFields 用于单输入的函数，如 Map 和 Reduce。 @ForwardedFieldsFirst 代表有两个输入的函数的第一个输入，如 Join 和 CoGroup。 @ForwardedFieldsSecond 代表有两个输入的函数的第二个输入，如 Join 和 CoGroup。  操作符参数\n data.map(myMapFnc).withForwardedFields() 用于单输入的函数，如 Map 和 Reduce。 data1.join(data2).where().equalTo().with(myJoinFnc).withForwardFieldsFirst() 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。 data1.join(data2).where().equalTo().with(myJoinFnc).withForwardFieldsSecond() 用于有两个输入的函数的第二个输入，如 Join 和 CoGroup。  请注意，不可能覆盖通过操作符参数指定为类注解的字段前向信息。\n例子：在函数的第二个输入端，如 Join 和 CoGroup，请注意不能覆盖通过运算符参数指定的类注解的字段前向信息。\n下面的例子显示了如何使用函数类注解来声明转发的字段信息。\n@ForwardedFields(\u0026#34;_1-\u0026gt;_3\u0026#34;) class MyMap extends MapFunction[(Int, Int), (String, Int, Int)]{ def map(value: (Int, Int)): (String, Int, Int) = { return (\u0026#34;foo\u0026#34;, value._2 / 2, value._1) } } 非转发字段\n非转发字段信息声明了所有在函数输出中不保留在同一位置的字段。所有其他字段的值都被认为保留在输出的同一位置。因此，非转发字段信息与转发字段信息是相反的。分组运算符（如 GroupReduce、GroupCombine、CoGroup 和 MapPartition）的非转发字段信息必须满足与转发字段信息相同的要求。\n重要：非转发字段信息的规范是可选的。但是如果使用，必须指定 ALL! 非转发字段，因为所有其他字段都被认为是原地转发的。将一个转发字段声明为非转发字段是安全的。\n非转发字段被指定为字段表达式的列表。这个列表既可以是由分号分隔的字段表达式组成的单个字符串，也可以是多个字符串。例如 \u0026ldquo;f1; f3\u0026rdquo; 和 \u0026ldquo;f1\u0026rdquo;、\u0026ldquo;f3\u0026rdquo; 都声明 Java 元组的第二个和第四个字段不保留在原地，其他所有字段都保留在原地。非前向字段信息只能为输入和输出类型相同的函数指定。\n非转发字段信息是作为函数类注解使用以下注解来指定的。\n @NonForwardedFields 用于单个输入函数，如 Map 和 Reduce。 @NonForwardedFieldsFirst 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。 @NonForwardedFieldsSecond 用于函数的第二个输入，如 Join 和 CoGroup。  例子\n下面的例子显示了如何声明非转发字段信息。\n@NonForwardedFields(\u0026#34;_2\u0026#34;) // second field is not forwarded class MyMap extends MapFunction[(Int, Int), (Int, Int)]{ def map(value: (Int, Int)): (Int, Int) = { return (value._1, value._2 / 2) } } 读取字段\n读取字段信息声明了所有被函数访问和评估的字段，也就是说，所有被函数用来计算结果的字段。例如，在条件语句中被评估的字段或用于计算的字段必须在指定读取字段信息时被标记为读取。仅仅是未经修改就转发到输出而不评估其值的字段，或者根本没有被访问的字段都不被认为是读。\n重要：读取字段信息的指定是可选的。但是如果使用，必须指定 ALL! 读取字段。将一个非读字段声明为读字段是安全的。\n读取字段被指定为字段表达式的列表。这个列表可以是一个由分号分隔的字段表达式组成的单个字符串，也可以是多个字符串。例如 \u0026ldquo;f1; f3\u0026rdquo; 和 \u0026ldquo;f1\u0026rdquo;、\u0026ldquo;f3\u0026rdquo; 都声明 Java 元组的第二和第四字段被函数读取和评估。\n读取字段信息是以函数类注解的形式指定的，使用以下注解。\n @ReadFields 用于单输入函数，如 Map 和 Reduce。 @ReadFieldsFirst 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。 @ReadFieldsSecond 用于有两个输入的函数的第二个输入，如 Join 和 CoGroup。  示例：\n下面的例子显示了如何声明读取字段信息。\n@ReadFields(\u0026#34;_1; _4\u0026#34;) // _1 and _4 are read and evaluated by the function. class MyMap extends MapFunction[(Int, Int, Int, Int), (Int, Int)]{ def map(value: (Int, Int, Int, Int)): (Int, Int) = { if (value._1 == 42) { return (value._1, value._2) } else { return (value._4 + 10, value._2) } } } 广播变量 广播变量允许你在操作的常规输入之外，将一个数据集提供给操作的所有并行实例。这对辅助数据集或数据依赖性参数化很有用。然后，该数据集将作为一个集合在操作者处被访问。\n 广播：广播集通过 withBroadcastSet(DataSet，String) 按名称注册，并通过 访问方式：通过目标操作者处的 getRuntimeContext().getBroadcastVariable(String) 访问。  // 1. The DataSet to be broadcast val toBroadcast = env.fromElements(1, 2, 3) val data = env.fromElements(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) data.map(new RichMapFunction[String, String]() { var broadcastSet: Traversable[String] = null override def open(config: Configuration): Unit = { // 3. Access the broadcast DataSet as a Collection  broadcastSet = getRuntimeContext().getBroadcastVariable[String](\u0026#34;broadcastSetName\u0026#34;).asScala } def map(in: String): String = { ... } }).withBroadcastSet(toBroadcast, \u0026#34;broadcastSetName\u0026#34;) // 2. Broadcast the DataSet 在注册和访问广播数据集时，确保名称（前面例子中的 broadcastSetName）匹配。关于完整的示例程序，可以看一下 KMeans 算法。\n注意：由于广播变量的内容在每个节点上都保存在内存中，所以它不应该变得太大。对于像标量值这样简单的东西，你可以简单地将参数作为函数闭包的一部分，或者使用 withParameters(...) 方法来传递配置。\n分布式缓存 Flink 提供了一个类似于 Apache Hadoop 的分布式缓存，以使用户函数的并行实例可以在本地访问文件。该功能可用于共享包含静态外部数据的文件，如字典或机器学习的回归模型。\n缓存的工作原理如下。程序在其 ExecutionEnvironment 中以特定的名称将本地或远程文件系统（如 HDFS 或 S3）的文件或目录注册为缓存文件。当程序执行时，Flink 会自动将该文件或目录复制到所有工作者的本地文件系统中。用户函数可以查找指定名称下的文件或目录，并从工作者的本地文件系统中访问它。\n分布式缓存的使用方法如下。\n在 ExecutionEnvironment 中注册文件或目录。\nval env = ExecutionEnvironment.getExecutionEnvironment // register a file from HDFS env.registerCachedFile(\u0026#34;hdfs:///path/to/your/file\u0026#34;, \u0026#34;hdfsFile\u0026#34;) // register a local executable file (script, executable, ...) env.registerCachedFile(\u0026#34;file:///path/to/exec/file\u0026#34;, \u0026#34;localExecFile\u0026#34;, true) // define your program and execute ... val input: DataSet[String] = ... val result: DataSet[Integer] = input.map(new MyMapper()) ... env.execute() 在一个用户函数（这里是 MapFunction）中访问缓存文件。该函数必须扩展一个 RichFunction 类，因为它需要访问 RuntimeContext。\n// extend a RichFunction to have access to the RuntimeContext class MyMapper extends RichMapFunction[String, Int] { override def open(config: Configuration): Unit = { // access cached file via RuntimeContext and DistributedCache  val myFile: File = getRuntimeContext.getDistributedCache.getFile(\u0026#34;hdfsFile\u0026#34;) // read the file (or navigate the directory)  ... } override def map(value: String): Int = { // use content of cached file  ... } } 向函数传递参数 可以使用构造函数或 withParameters(Configuration) 方法将参数传递给函数。参数会被序列化为函数对象的一部分，并传送给所有并行任务实例。\n通过构造函数\nval toFilter = env.fromElements(1, 2, 3) toFilter.filter(new MyFilter(2)) class MyFilter(limit: Int) extends FilterFunction[Int] { override def filter(value: Int): Boolean = { value \u0026gt; limit } } 通过 withParameters(配置)\n本方法以一个 Configuration 对象作为参数，它将被传递给富函数的 open() 方法。配置对象是一个从 String 键到不同值类型的 Map。\nval toFilter = env.fromElements(1, 2, 3) val c = new Configuration() c.setInteger(\u0026#34;limit\u0026#34;, 2) toFilter.filter(new RichFilterFunction[Int]() { var limit = 0 override def open(config: Configuration): Unit = { limit = config.getInteger(\u0026#34;limit\u0026#34;, 0) } def filter(in: Int): Boolean = { in \u0026gt; limit } }).withParameters(c) 在全局范围内通过 ExecutionConfig\nFlink 还允许将自定义配置值传递到环境的 ExecutionConfig 接口。由于执行配置可以在所有（丰富的）用户函数中访问，因此自定义配置将在所有函数中全局可用。\n设置一个自定义的全局配置：\nval env = ExecutionEnvironment.getExecutionEnvironment val conf = new Configuration() conf.setString(\u0026#34;mykey\u0026#34;, \u0026#34;myvalue\u0026#34;) env.getConfig.setGlobalJobParameters(conf) 请注意，你也可以传递一个扩展 ExecutionConfig.GlobalJobParameters 类的自定义类作为全局作业参数给执行配置。该接口允许实现 Map\u0026lt;String, String\u0026gt; toMap() 方法，该方法将在 web 前端显示来自配置的值。\n从全局配置中访问值\n全局工作参数中的对象在系统中的很多地方都可以访问。所有实现 RichFunction 接口的用户函数都可以通过运行时上下文访问。\npublic static final class Tokenizer extends RichFlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { private String mykey; @Override public void open(Configuration parameters) throws Exception { super.open(parameters); ExecutionConfig.GlobalJobParameters globalParams = getRuntimeContext().getExecutionConfig().getGlobalJobParameters(); Configuration globConf = (Configuration) globalParams; mykey = globConf.getString(\u0026#34;mykey\u0026#34;, null); } // ... more here ... 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/","tags":["Flink","Flink 官方文档","DataSet API","Guide"],"title":"Flink Dataset API 编程指南"},{"categories":["Flink"],"contents":"Hadoop 兼容性测试版 Flink 与 Apache Hadoop MapReduce 接口兼容，因此允许重用为 Hadoop MapReduce 实现的代码。\n您可以:\n 在 Flink 程序中使用 Hadoop 的可写数据类型。 使用任何 Hadoop InputFormat 作为数据源。 使用任何 Hadoop 输出格式作为数据接收器。 将 Hadoop Mapper 用作 FlatMapFunction。 使用 Hadoop Reducer 作为 GroupReduceFunction。  本文档展示了如何将现有的 Hadoop MapReduce 代码与 Flink 一起使用。从 Hadoop 支持的文件系统读取代码，请参考连接到其他系统指南。\n项目配置 对 Hadoop 输入/输出格式的支持是 flink-java 和 flink-scala Maven 模块的一部分，这些模块在编写 Flink 作业时总是需要的。这些代码位于 org.apache.flink.api.java.hadoop 和 org.apache.flink.api.scala.hadoop 中的 mapred 和 mapreduce API 的附加子包中。\n对 Hadoop Mappers 和 Reducers 的支持包含在 flink-hadoop-compatibility Maven 模块中。这段代码位于 org.apache.flink.hadoopcompatibility 包中。\n如果您想重用 Mappers 和 Reducers，请在 pom.xml 中添加以下依赖关系。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-hadoop-compatibility_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 另请参见如何配置 hadoop 依赖关系。\n使用 Hadoop 输入格式 要使用 Flink 的 Hadoop InputFormats，必须先使用 HadoopInputs 实用程序类的 readHadoopFile 或 createHadoopInput 来包装格式。前者用于从 FileInputFormat 派生的输入格式，而后者必须用于通用的输入格式。通过使用 ExecutionEnvironmen#createInput，产生的 InputFormat 可以用来创建数据源。\n生成的 DataSet 包含 2 个元组，其中第一个字段是键，第二个字段是从 Hadoop InputFormat 中检索的值。\n下面的示例展示了如何使用 Hadoop 的 TextInputFormat。\nval env = ExecutionEnvironment.getExecutionEnvironment val input: DataSet[(LongWritable, Text)] = env.createInput(HadoopInputs.readHadoopFile( new TextInputFormat, classOf[LongWritable], classOf[Text], textPath)) // Do something with the data. [...] 使用 Hadoop 输出格式 Flink 为 Hadoop OutputFormat 提供了一个兼容性封装器，它支持任何实现 org.apache.hadoop.mapred.OutputFormat 或扩展 org.apache.hadoop.mapreduce.OutputFormat 的类。OutputFormat 包装器希望它的输入数据是一个包含2个key和value的 DataSet。这些数据将由 Hadoop OutputFormat 处理。\n下面的示例展示了如何使用 Hadoop 的 TextOutputFormat。\n// Obtain your result to emit. val hadoopResult: DataSet[(Text, IntWritable)] = [...] val hadoopOF = new HadoopOutputFormat[Text,IntWritable]( new TextOutputFormat[Text, IntWritable], new JobConf) hadoopOF.getJobConf.set(\u0026#34;mapred.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;) FileOutputFormat.setOutputPath(hadoopOF.getJobConf, new Path(resultPath)) hadoopResult.output(hadoopOF) 使用 Hadoop Mappers 和 Reducers Hadoop Mappers 在语义上等同于 Flink 的 FlatMapFunctions，Hadoop Reducers 等同于 Flink 的 GroupReduceFunctions。Flink 为 Hadoop MapReduce 的 Mapper 和 Reducer 接口的实现提供了封装器，也就是说，你可以在常规的 Flink 程序中重用你的 Hadoop Mapper 和 Reducer。目前，只支持 Hadoop 的 mapred API（org.apache.hadoop.mapred）的 Mapper 和 Reduce 接口。\n包装器将一个 DataSet\u0026lt;Tuple2\u0026lt;KEYIN,VALUEIN\u0026gt; 作为输入，并产生一个 DataSet\u0026lt;Tuple2\u0026lt;KEYOUT,VALUEOUT\u0026gt; 作为输出，其中 KEYIN 和 KEYOUT 是键，VALUEIN 和 VALUEOUT 是 Hadoop 函数处理的 Hadoop 键值对的值。对于 Reducers，Flink 提供了一个包装器，用于带（HadoopReduceCombineFunction）和不带 Combiner（HadoopReduceFunction）的 GroupReduceFunction。包装器接受一个可选的 JobConf 对象来配置 Hadoop Mapper 或 Reducer。\nFlink 的函数包装器有:\n sorg.apache.flink.hadoopcompatibility.mapred.HadoopMapFunction, sorg.apache.flink.hadoopcompatibility.mapred.HadoopReduceFunction, 和 sorg.apache.flink.hadoopcompatibility.mapred.HadoopReduceCombineFunction.  并可作为常规的 Flink FlatMapFunctions 或 GroupReduceFunctions 使用。\n下面的例子展示了如何使用 Hadoop Mapper 和 Reducer 函数:\n// Obtain data to process somehow. DataSet\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; text = [...] DataSet\u0026lt;Tuple2\u0026lt;Text, LongWritable\u0026gt;\u0026gt; result = text // use Hadoop Mapper (Tokenizer) as MapFunction  .flatMap(new HadoopMapFunction\u0026lt;LongWritable, Text, Text, LongWritable\u0026gt;( new Tokenizer() )) .groupBy(0) // use Hadoop Reducer (Counter) as Reduce- and CombineFunction  .reduceGroup(new HadoopReduceCombineFunction\u0026lt;Text, LongWritable, Text, LongWritable\u0026gt;( new Counter(), new Counter() )); 请注意：Reducer 包装器工作在 Flink 的 groupBy() 操作所定义的组上。它不考虑您在 JobConf 中设置的任何自定义分区器、排序或分组比较器。\n完整的 Hadoop WordCount 示例 下面的示例展示了使用 Hadoop 数据类型、Input-和 OutputFormats 以及 Mapper 和 Reducer 实现的完整 WordCount 实现。\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // Set up the Hadoop TextInputFormat. Job job = Job.getInstance(); HadoopInputFormat\u0026lt;LongWritable, Text\u0026gt; hadoopIF = new HadoopInputFormat\u0026lt;LongWritable, Text\u0026gt;( new TextInputFormat(), LongWritable.class, Text.class, job ); TextInputFormat.addInputPath(job, new Path(inputPath)); // Read data using the Hadoop TextInputFormat. DataSet\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; text = env.createInput(hadoopIF); DataSet\u0026lt;Tuple2\u0026lt;Text, LongWritable\u0026gt;\u0026gt; result = text // use Hadoop Mapper (Tokenizer) as MapFunction  .flatMap(new HadoopMapFunction\u0026lt;LongWritable, Text, Text, LongWritable\u0026gt;( new Tokenizer() )) .groupBy(0) // use Hadoop Reducer (Counter) as Reduce- and CombineFunction  .reduceGroup(new HadoopReduceCombineFunction\u0026lt;Text, LongWritable, Text, LongWritable\u0026gt;( new Counter(), new Counter() )); // Set up the Hadoop TextOutputFormat. HadoopOutputFormat\u0026lt;Text, LongWritable\u0026gt; hadoopOF = new HadoopOutputFormat\u0026lt;Text, LongWritable\u0026gt;( new TextOutputFormat\u0026lt;Text, LongWritable\u0026gt;(), job ); hadoopOF.getConfiguration().set(\u0026#34;mapreduce.output.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;); TextOutputFormat.setOutputPath(job, new Path(outputPath)); // Emit data using the Hadoop TextOutputFormat. result.output(hadoopOF); // Execute Program env.execute(\u0026#34;Hadoop WordCount\u0026#34;); 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/hadoop_compatibility.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/","tags":["Flink","Flink 官方文档","DataSet API"],"title":"Hadoop 的兼容性"},{"categories":["Flink"],"contents":"INSERT 语句 INSERT 语句用于向表中添加行。\n运行 INSERT 语句 单条 INSERT 语句可以通过 TableEnvironment 的 executeSql() 方法执行，也可以在 SQL CLI 中执行。INSERT 语句的 executeSql() 方法会立即提交一个 Flink 作业，并返回一个与提交的作业相关联的 TableResult 实例。多个 INSERT 语句可以通过 StatementSet 的 addInsertSql() 方法执行，StatementSet 可以由 TableEnvironment.createStatementSet() 方法创建。addInsertSql() 方法是一种懒惰的执行方式，它们只有在调用 StatementSet.execute() 时才会被执行。\n下面的例子展示了如何在 TableEnvironment 中运行一条 INSERT 语句，以及在 SQL CLI 中，在 StatementSet 中运行多条 INSERT 语句。\nFlink SQL\u0026gt; CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...); Flink SQL\u0026gt; SHOW TABLES; Orders RubberOrders Flink SQL\u0026gt; INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;; [INFO] Submitting SQL update statement to the cluster... [INFO] Table update statement has been successfully submitted to the cluster: val settings = EnvironmentSettings.newInstance()... val tEnv = TableEnvironment.create(settings) // register a source table named \u0026#34;Orders\u0026#34; and a sink table named \u0026#34;RubberOrders\u0026#34; tEnv.executeSql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) tEnv.executeSql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)\u0026#34;) // run a single INSERT query on the registered source table and emit the result to registered sink table val tableResult1 = tEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // get job status through TableResult println(tableResult1.getJobClient().get().getJobStatus()) //---------------------------------------------------------------------------- // register another sink table named \u0026#34;GlassOrders\u0026#34; for multiple INSERT queries tEnv.executeSql(\u0026#34;CREATE TABLE GlassOrders(product VARCHAR, amount INT) WITH (...)\u0026#34;) // run multiple INSERT queries on the registered source table and emit the result to registered sink tables val stmtSet = tEnv.createStatementSet() // only single INSERT query can be accepted by `addInsertSql` method stmtSet.addInsertSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) stmtSet.addInsertSql( \u0026#34;INSERT INTO GlassOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Glass%\u0026#39;\u0026#34;) // execute all statements together val tableResult2 = stmtSet.execute() // get job status through TableResult println(tableResult2.getJobClient().get().getJobStatus()) Insert from select queries 查询结果可以通过使用插入子句插入到表中。\n语法 INSERT{INTO|OVERWRITE}[catalog_name.][db_name.]table_name[PARTITIONpart_spec]select_statementpart_spec:(part_col_name1=val1[,part_col_name2=val2,...])OVERWRITE\nINSERT OVERWRITE 将覆盖表或分区中的任何现有数据。否则，将追加新数据。\nPARTITION\nPARTITION 子句应包含本次插入的静态分区列。\n例子 -- Creates a partitioned table CREATETABLEcountry_page_view(userSTRING,cntINT,dateSTRING,countrySTRING)PARTITIONEDBY(date,country)WITH(...)-- Appends rows into the static partition (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) INSERTINTOcountry_page_viewPARTITION(date=\u0026#39;2019-8-30\u0026#39;,country=\u0026#39;China\u0026#39;)SELECTuser,cntFROMpage_view_source;-- Appends rows into partition (date, country), where date is static partition with value \u0026#39;2019-8-30\u0026#39;, -- country is dynamic partition whose value is dynamic determined by each row. INSERTINTOcountry_page_viewPARTITION(date=\u0026#39;2019-8-30\u0026#39;)SELECTuser,cnt,countryFROMpage_view_source;-- Overwrites rows into static partition (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) INSERTOVERWRITEcountry_page_viewPARTITION(date=\u0026#39;2019-8-30\u0026#39;,country=\u0026#39;China\u0026#39;)SELECTuser,cntFROMpage_view_source;-- Overwrites rows into partition (date, country), where date is static partition with value \u0026#39;2019-8-30\u0026#39;, -- country is dynamic partition whose value is dynamic determined by each row. INSERTOVERWRITEcountry_page_viewPARTITION(date=\u0026#39;2019-8-30\u0026#39;)SELECTuser,cnt,countryFROMpage_view_source;Insert values into tables INSERT\u0026hellip;VALUES 语句可以用来直接从 SQL 中向表中插入数据。\n语法 INSERT{INTO|OVERWRITE}[catalog_name.][db_name.]table_nameVALUESvalues_row[,values_row...]values_row::(val1[,val2,...])OVERWRITE\nINSERT OVERWRITE 将覆盖表中的任何现有数据。否则，将追加新数据。\n例子 CREATETABLEstudents(nameSTRING,ageINT,gpaDECIMAL(3,2))WITH(...);INSERTINTOstudentsVALUES(\u0026#39;fred flintstone\u0026#39;,35,1.28),(\u0026#39;barney rubble\u0026#39;,32,2.32);原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/insert.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","SQL"],"title":"Insert 语句"},{"categories":["Flink"],"contents":"Java Lambda 表达式 Java 8 引入了一些新的语言功能，旨在实现更快、更清晰的编码。其中最重要的功能是所谓的\u0026quot;Lambda 表达式\u0026quot;，它打开了函数式编程的大门。Lambda 表达式允许以一种直接的方式实现和传递函数，而无需声明额外的（匿名）类。\n注意 Flink 支持对 Java API 的所有操作符使用 lambda 表达式，但是，每当 lambda 表达式使用 Java 属的时候，你需要明确地声明类型信息。\n本文档展示了如何使用 lambda 表达式并描述了当前的限制。关于 Flink API 的一般介绍，请参考 DataSteam API 概述。\n例子和限制 下面的例子说明了如何实现一个简单的内联 map() 函数，该函数使用 lambda 表达式对其输入进行平方化。map() 函数的输入 i 和输出参数的类型不需要声明，因为它们是由 Java 编译器推断的。\nenv.fromElements(1, 2, 3) // returns the squared i .map(i -\u0026gt; i*i) .print(); Flink 可以从方法签名 OUT map(IN value) 的实现中自动提取结果类型信息，因为 OUT 不是通用的，而是 Integer。\n遗憾的是，像 flatMap() 这样签名为 void flatMap(IN value, Collector\u0026lt;OUT\u0026gt; out) 的函数被 Java 编译器编译成 void flatMap(IN value, Collector out)。这使得 Flink 无法自动推断输出类型的类型信息。\nFlink 很可能会抛出一个类似下面的异常。\norg.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of 'Collector' are missing. In many cases lambda methods don't provide enough information for automatic type extraction when Java generics are involved. An easy workaround is to use an (anonymous) class instead that implements the 'org.apache.flink.api.common.functions.FlatMapFunction' interface. Otherwise the type has to be specified explicitly using type information. 在这种情况下，需要明确指定类型信息，否则输出将被视为类型为 Object，导致序列化效率低下。\nimport org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.api.java.DataSet; import org.apache.flink.util.Collector; DataSet\u0026lt;Integer\u0026gt; input = env.fromElements(1, 2, 3); // collector type must be declared input.flatMap((Integer number, Collector\u0026lt;String\u0026gt; out) -\u0026gt; { StringBuilder builder = new StringBuilder(); for(int i = 0; i \u0026lt; number; i++) { builder.append(\u0026#34;a\u0026#34;); out.collect(builder.toString()); } }) // provide type information explicitly .returns(Types.STRING) // prints \u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;aa\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;aa\u0026#34;, \u0026#34;aaa\u0026#34; .print(); 当使用具有通用返回类型的 map() 函数时，也会出现类似的问题。在下面的例子中，一个方法签名 Tuple2\u0026lt;Integer, Integer\u0026gt; map(Integer value) 被擦除为 Tuple2 map(Integer value)。\nimport org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.tuple.Tuple2; env.fromElements(1, 2, 3) .map(i -\u0026gt; Tuple2.of(i, i)) // no information about fields of Tuple2  .print(); 一般来说，这些问题可以通过多种方式解决。\nimport org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.api.java.tuple.Tuple2; // use the explicit \u0026#34;.returns(...)\u0026#34; env.fromElements(1, 2, 3) .map(i -\u0026gt; Tuple2.of(i, i)) .returns(Types.TUPLE(Types.INT, Types.INT)) .print(); // use a class instead env.fromElements(1, 2, 3) .map(new MyTuple2Mapper()) .print(); public static class MyTuple2Mapper extends MapFunction\u0026lt;Integer, Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;Integer, Integer\u0026gt; map(Integer i) { return Tuple2.of(i, i); } } // use an anonymous class instead env.fromElements(1, 2, 3) .map(new MapFunction\u0026lt;Integer, Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;Integer, Integer\u0026gt; map(Integer i) { return Tuple2.of(i, i); } }) .print(); // or in this example use a tuple subclass instead env.fromElements(1, 2, 3) .map(i -\u0026gt; new DoubleTuple(i, i)) .print(); public static class DoubleTuple extends Tuple2\u0026lt;Integer, Integer\u0026gt; { public DoubleTuple(int f0, int f1) { this.f0 = f0; this.f1 = f1; } } 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/java_lambdas.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"Java Lambda 表达式"},{"categories":["Flink"],"contents":"窗口连接(Join) 窗口连接(window join)将两个流的元素连接起来，这两个流有一个共同的键，并且位于同一个窗口中。这些窗口可以通过使用窗口分配器来定义，并对两个流的元素进行评估。\n然后，来自双方的元素被传递到一个用户定义的 JoinFunction 或 FlatJoinFunction 中，用户可以发出符合加入标准的结果。\n一般的用法可以归纳为以下几点。\nstream.join(otherStream) .where(\u0026lt;KeySelector\u0026gt;) .equalTo(\u0026lt;KeySelector\u0026gt;) .window(\u0026lt;WindowAssigner\u0026gt;) .apply(\u0026lt;JoinFunction\u0026gt;) 关于语义的一些说明:\n 两个流中元素的成对组合的创建就像一个内部连接，这意味着一个流中的元素如果没有另一个流中的相应元素与之连接，就不会发出。 那些被加入的元素将以各自窗口中最大的时间戳作为它们的时间戳。例如，一个以 [5, 10) 为边界的窗口将导致加入的元素以9作为它们的时间戳。  在下面的章节中，我们将使用一些示例性的场景来概述不同类型的窗口连接是如何进行的。\n滚动窗口连接 当执行滚动窗口连接时，所有具有共同的键和共同的滚动窗口的元素都会以成对组合的方式进行连接，并传递给 JoinFunction 或 FlatJoinFunction。因为这表现得像一个内连接，所以一个流的元素如果在其滚动窗口中没有来自另一个流的元素，就不会被发出去！这就是为什么我们要把一个流的元素加入到滚动窗口中。\n如图所示，我们定义了一个大小为2毫秒的滚动窗口，其结果是 [0,1]，[2,3]，\u0026hellip;形式的窗口。图中显示了每个窗口中所有元素的配对组合，这些元素将被传递给 JoinFunction。请注意，在翻滚窗口 [6,7] 中，没有任何元素发出，因为绿色流中没有元素存在，要与橙色元素⑥和⑦连接。\nimport org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream.join(greenStream) .where(elem =\u0026gt; /* select key */) .equalTo(elem =\u0026gt; /* select key */) .window(TumblingEventTimeWindows.of(Time.milliseconds(2))) .apply { (e1, e2) =\u0026gt; e1 + \u0026#34;,\u0026#34; + e2 } 滑动窗连接 在执行滑动窗口连接时，所有具有共同键和共同滑动窗口的元素都会以成对组合的方式连接，并传递给 JoinFunction 或 FlatJoinFunction。一个流的元素如果在当前的滑动窗口中没有来自另一个流的元素，则不会被发出! 请注意，有些元素可能在一个滑动窗口中被加入，但在另一个滑动窗口中却没有!\n在这个例子中，我们使用的是大小为两毫秒的滑动窗口，并将它们滑动一毫秒，结果是滑动窗口 [-1，0]，[0，1]，[1，2]，[2，3]，\u0026hellip;。x轴下面的加入元素就是每个滑动窗口传递给 JoinFunction 的元素。这里你也可以看到，例如橙色的②与绿色的③在窗口 [2,3] 中连接，但与窗口 [1,2] 中的任何元素都没有连接。\nimport org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream.join(greenStream) .where(elem =\u0026gt; /* select key */) .equalTo(elem =\u0026gt; /* select key */) .window(SlidingEventTimeWindows.of(Time.milliseconds(2) /* size */, Time.milliseconds(1) /* slide */)) .apply { (e1, e2) =\u0026gt; e1 + \u0026#34;,\u0026#34; + e2 } 会议窗口连接 当执行会话窗口连接时，所有具有相同键的元素，当\u0026quot;组合\u0026quot;满足会话标准时，将以成对组合的方式连接，并传递给 JoinFunction 或 FlatJoinFunction。同样，这也是执行内部连接，所以如果有一个会话窗口只包含来自一个流的元素，就不会有输出。\n在这里，我们定义了一个会话窗口加入，其中每个会话被至少1ms的间隙所分割。有三个会话，在前两个会话中，两个流中的加入元素都会传递给 JoinFunction。在第三个会话中，绿色流中没有元素，所以⑧和⑨没有加入！在第三个会话中，绿色流中没有元素。\nimport org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows; import org.apache.flink.streaming.api.windowing.time.Time; ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream.join(greenStream) .where(elem =\u0026gt; /* select key */) .equalTo(elem =\u0026gt; /* select key */) .window(EventTimeSessionWindows.withGap(Time.milliseconds(1))) .apply { (e1, e2) =\u0026gt; e1 + \u0026#34;,\u0026#34; + e2 } 间隔连接 间隔连接将两个流的元素（我们暂且称它们为A和B）用一个共同的键连接起来，流B中的元素的时间戳与流A中元素的时间戳处于一个相对的时间间隔。\n这也可以更正式地表达为 b.timestamp∈[a.timestamp + lowerBound; a.timestamp + upperBound] 或 a.timestamp + lowerBound \u0026lt;= b.timestamp \u0026lt;= a.timestamp + upperBound。\n其中a和b是A和B的元素，它们有一个共同的键。下界和上界都可以是负的或正的，只要下界总是小于或等于上界。区间连接目前只执行内连接。\n当一对元素传递给 ProcessJoinFunction 时，它们将被分配为两个元素中较大的时间戳（可以通过 ProcessJoinFunction.Context 访问）。\n注意：间隔连接目前只支持事件时间。\n在上面的例子中，我们将两个流\u0026quot;橙色\u0026quot;和\u0026quot;绿色\u0026quot;连接起来，下界为-2毫秒，上界为+1毫秒。默认情况下，这些边界是包容的，但可以应用 .lowerBoundExclusive() 和 .upperBoundExclusive 来改变行为。\n再次使用更正式的符号，这将被翻译为:\norangeElem.ts + lowerBound \u0026lt;= greenElem.ts \u0026lt;= orangeElem.ts + upperBound as indicated by the triangles.\nimport org.apache.flink.streaming.api.functions.co.ProcessJoinFunction; import org.apache.flink.streaming.api.windowing.time.Time; ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream .keyBy(elem =\u0026gt; /* select key */) .intervalJoin(greenStream.keyBy(elem =\u0026gt; /* select key */)) .between(Time.milliseconds(-2), Time.milliseconds(1)) .process(new ProcessJoinFunction[Integer, Integer, String] { override def processElement(left: Integer, right: Integer, ctx: ProcessJoinFunction[Integer, Integer, String]#Context, out: Collector[String]): Unit = { out.collect(left + \u0026#34;,\u0026#34; + right); } }); }); 原文连接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/joining.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-joining/","tags":["Flink","Flink 官方文档","DataStream API","Operators","Joining"],"title":"Joining"},{"categories":["Flink"],"contents":"操作符 操作符将一个或多个 DataStream 转换为一个新的 DataStream。程序可以将多个变换组合成复杂的数据流拓扑。\n本节给出了基本变换的描述，应用这些变换后的有效物理分区，以及对 Flink 的操作符链的见解。\nDataStream 转换  Map  DataStream → DataStream\n接受一个元素并产生一个元素。一个将输入流的值翻倍的 map 函数:\ndataStream.map { x =\u0026gt; x * 2 }  FlatMap  DataStream → DataStream\n接受一个元素并产生零个、一个或多个元素。一个将句子分割成单词的 flatMap 函数:\ndataStream.flatMap { str =\u0026gt; str.split(\u0026#34; \u0026#34;) }  Filter  DataStream → DataStream\n评估每个元素的布尔函数，并保留那些函数返回值为真的元素。一个过滤掉零值的 filter:\ndataStream.filter { _ != 0 }  KeyBy  DataStream → KeyedStream\n在逻辑上将一个流划分为互斥的分区，每个分区包含相同键的元素。在内部，这是通过哈希分区实现的。关于如何指定键，请参见 keys。这个转换会返回一个 KeyedStream:\ndataStream.keyBy(\u0026#34;someKey\u0026#34;) // Key by field \u0026#34;someKey\u0026#34; dataStream.keyBy(0) // Key by the first element of a Tuple  Reduce  KeyedStream → DataStream\n在 keyed 数据流上进行\u0026quot;滚动\u0026quot;换算(reduce)。将当前元素与最后一个换算的值合并，并发出新的值。\n一个创建部分和(sum)流的 reduce 函数:\nkeyedStream.reduce { _ + _ }  Fold  KeyedStream → DataStream\n在一个带有初始值的 keyed 数据流上进行\u0026quot;滚动\u0026quot;折叠。将当前元素与最后一个折叠的值结合起来，并发出新的值。\n一个折叠函数，当应用于序列(1,2,3,4,5)时，发出序列 \u0026ldquo;start-1\u0026rdquo;、\u0026ldquo;start-1-2\u0026rdquo;、\u0026ldquo;start-1-2-3\u0026rdquo;、\u0026hellip;\nval result: DataStream[String] = keyedStream.fold(\u0026#34;start\u0026#34;)((str, i) =\u0026gt; { str + \u0026#34;-\u0026#34; + i })  Aggregations  KeyedStream → DataStream\n在 keyed 数据流上进行滚动聚合。min 和 minBy 的区别在于 min 返回最小值，而 minBy 则返回该字段中具有最小值的元素（max 和 maxBy 也一样）。\nkeyedStream.sum(0) keyedStream.sum(\u0026#34;key\u0026#34;) keyedStream.min(0) keyedStream.min(\u0026#34;key\u0026#34;) keyedStream.max(0) keyedStream.max(\u0026#34;key\u0026#34;) keyedStream.minBy(0) keyedStream.minBy(\u0026#34;key\u0026#34;) keyedStream.maxBy(0) keyedStream.maxBy(\u0026#34;key\u0026#34;)  Window  KeyedStream → WindowedStream\n可以在已经分区的 KeyedStream 上定义 Window。窗口根据一些特征（例如，最近5秒内到达的数据）对每个键中的数据进行分组。关于窗口的描述，请参见窗口。\ndataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data  WindowAll  DataStream → AllWindowedStream\n可以在常规的 DataStream 上定义窗口。窗口根据一些特征（例如，在过去5秒内到达的数据）对所有流事件进行分组。关于窗口的完整描述，请参见窗口。\n警告：在许多情况下，这是一个非并行的转换。所有的记录将被收集在 windowAll 操作符的一个任务(task)中。\ndataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data  Window Apply  WindowedStream → DataStream\nAllWindowedStream → DataStream\n将一般函数应用于整个窗口。下面是一个手动求和窗口元素的函数。\n注意：如果您使用的是 windowAll 转换，您需要使用 AllWindowFunction 来代替。\nwindowedStream.apply { WindowFunction } // applying an AllWindowFunction on non-keyed window stream allWindowedStream.apply { AllWindowFunction }  Window Reduce  WindowedStream → DataStream\n对窗口应用函数式的 reduce  函数，并返回换算后的值:\nwindowedStream.reduce { _ + _ }  Window Fold  WindowedStream → DataStream\n对窗口应用功能 fold 函数并返回折叠后的值。示例函数应用于序列 (1,2,3,4,5) 时，将序列折叠成字符串 \u0026ldquo;start-1-2-3-4-5\u0026rdquo;:\nval result: DataStream[String] = windowedStream.fold(\u0026#34;start\u0026#34;, (str, i) =\u0026gt; { str + \u0026#34;-\u0026#34; + i })  窗口上的聚合  WindowedStream → DataStream\n聚合一个窗口的内容。min 和 minBy 的区别在于 min 返回最小值，而 minBy 返回在该字段中具有最小值的元素（max 和 maxBy 相同）。\nwindowedStream.sum(0) windowedStream.sum(\u0026#34;key\u0026#34;) windowedStream.min(0) windowedStream.min(\u0026#34;key\u0026#34;) windowedStream.max(0) windowedStream.max(\u0026#34;key\u0026#34;) windowedStream.minBy(0) windowedStream.minBy(\u0026#34;key\u0026#34;) windowedStream.maxBy(0) windowedStream.maxBy(\u0026#34;key\u0026#34;)  Union  DataStream* → DataStream\n联合两个或多个数据流，创建一个新的流，包含所有流的所有元素。注意：如果你把一个数据流和它自己联合起来，你将在生成的数据流中得到每个元素两次。\ndataStream.union(otherStream1, otherStream2, ...)  Window Join  DataStream,DataStream → DataStream\n在一个给定的键和一个公共窗口上连接(join)两个数据流。\ndataStream.join(otherStream) .where(\u0026lt;key selector\u0026gt;).equalTo(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply { ... }  Window CoGroup  DataStream,DataStream → DataStream\n在一个给定的键和一个共同的窗口上将两个数据流串联(Cogroups)起来。\ndataStream.coGroup(otherStream) .where(0).equalTo(1) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply {}  Connect  DataStream,DataStream → ConnectedStreams\n\u0026ldquo;连接\u0026rdquo;(connect)两个数据流，保留其类型，允许两个数据流之间共享状态。\nsomeStream : DataStream[Int] = ... otherStream : DataStream[String] = ... val connectedStreams = someStream.connect(otherStream)  CoMap, CoFlatMap  ConnectedStreams → DataStream\n类似于连接(connected)数据流上的 map 和 flatMap:\nconnectedStreams.map( (_ : Int) =\u0026gt; true, (_ : String) =\u0026gt; false ) connectedStreams.flatMap( (_ : Int) =\u0026gt; true, (_ : String) =\u0026gt; false )  Split  DataStream → SplitStream\n根据某种标准，将流分成两个或多个流。\nval split = someDataStream.split( (num: Int) =\u0026gt; (num % 2) match { case 0 =\u0026gt; List(\u0026#34;even\u0026#34;) case 1 =\u0026gt; List(\u0026#34;odd\u0026#34;) } )  Select  SplitStream → DataStream\n从分割流中选择一个或多个流。\nval even = split select \u0026#34;even\u0026#34; val odd = split select \u0026#34;odd\u0026#34; val all = split.select(\u0026#34;even\u0026#34;,\u0026#34;odd\u0026#34;)  Iterate  DataStream → IterativeStream → DataStream\n在流(flow)中创建一个\u0026quot;反馈\u0026quot;循环，将一个操作符的输出重定向到之前的某个操作符。这对于定义持续更新模型的算法特别有用。下面的代码从一个流(stream)开始，连续应用迭代体。大于0的元素被送回反馈通道，其余元素被转发到下游。参见迭代的完整描述。\ninitialStream.iterate { iteration =\u0026gt; { val iterationBody = iteration.map {/*do something*/} (iterationBody.filter(_ \u0026gt; 0), iterationBody.filter(_ \u0026lt;= 0)) } } 通过匿名模式匹配从 tuple、case 类和集合中提取，比如下面:\nval data: DataStream[(Int, String, Double)] = // [...] data.map { case (id, name, temperature) =\u0026gt; // [...] } 不受 API 开箱即用的支持。要使用这个功能，你应该使用 Scala API 扩展。\n以下转换可用于 Tuples 的数据流:\n Project  DataStream → DataStream\n从元组中选择一个字段的子集。\nDataStream\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; in = // [...] DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out = in.project(2,0); 物理分区 Flink 还可以通过以下函数对转换后的准确流分区进行低级控制（如果需要）。\n 自定义分区  DataStream → DataStream\n使用用户定义的 Partitioner 为每个元素选择目标任务。\ndataStream.partitionCustom(partitioner, \u0026#34;someKey\u0026#34;) dataStream.partitionCustom(partitioner, 0)  随机分区  DataStream → DataStream\n将元素按照均匀分布随机分区。\ndataStream.shuffle()  Rebalancing (循环分区)  DataStream → DataStream\n对元素进行循环分区，使每个分区的负载相等。在数据倾斜的情况下，对性能优化很有用。\ndataStream.rebalance()  Rescaling  DataStream → DataStream\n将元素，轮回分区到下游操作的子集。如果你想拥有管道，例如，从源的每个并行实例向几个映射器(mappers)的子集扇出，以分配负载，但又不想进行 rebalance() 会引起的完全再平衡，那么这就很有用。这将只需要本地数据传输，而不是通过网络传输数据，这取决于其他配置值，如 TaskManagers 的槽数(slots)。\n上游操作向其发送元素的下游操作子集取决于上游和下游操作的并行程度。例如，如果上游操作的并行度为2，下游操作的并行度为4，那么一个上游操作将向两个下游操作分发元素，而另一个上游操作将向另外两个下游操作分发。另一方面，如果下游操作具有并行度2，而上游操作具有并行度4，那么两个上游操作将分配给一个下游操作，而其他两个上游操作将分配给其他下游操作。\n在不同的并行度不是彼此的倍数的情况下，一个或几个下游操作将从上游操作中获得不同数量的输入。\n请看此图，可以直观地看到上例中的连接(connection)模式。\ndataStream.rescale()  Broadcasting  DataStream → DataStream\n将元素广播到每个分区。\ndataStream.broadcast() 任务链和资源组 链式的两个后续变换意味着将它们共同放置在同一个线程中以获得更好的性能。如果可能的话，Flink 默认会将操作符链起来（例如，两个后续的 map 变换）。如果需要的话，API 提供了对链式操作的精细控制。\n如果你想在整个作业(job)中禁用链，请使用 StreamExecutionEnvironment.disableOperatorChaining()。对于更细粒度的控制，以下函数是可用的。请注意，这些函数只能在 DataStream 转换之后使用，因为它们引用了之前的转换。例如，你可以使用 someStream.map(...).startNewChain()，但你不能使用 someStream.startNewChain()。\n资源组是 Flink 中的一个槽，参见 slots。如果需要，你可以在单独的槽中手动隔离操作符。\n Start new chain  开始一个新的链，从这个操作符开始。两个映射器(mappers)将被连锁，filter 将不会被连锁到第一个映射器(mapper)。\nsomeStream.filter(...).map(...).startNewChain().map(...)  Disable chaining  不将 map 运算符连锁化。\nsomeStream.map(...).disableChaining()  Set slot sharing group  设置操作的槽位共享组。Flink 会将具有相同槽位共享组的操作放入同一个槽位，而将没有槽位共享组的操作保留在其他槽位。这可以用来隔离槽位。如果所有的输入操作都在同一个槽共享组中，槽共享组就会从输入操作中继承。缺省槽共享组的名称是 \u0026ldquo;default\u0026rdquo;，操作可以通过调用 slotSharingGroup(\u0026quot;default\u0026quot;) 来明确地放入这个组。\nsomeStream.filter(...).slotSharingGroup(\u0026#34;name\u0026#34;) 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-operators/","tags":["Flink","Flink 官方文档","DataStream API","Operator"],"title":"Operators"},{"categories":["Flink"],"contents":"ProcessFunction ProcessFunction 是一种低级的流处理操作，可以访问所有（非循环）流应用的基本构件。\n 事件（流元素） 状态（容错，一致，只在 keyed 流上）。 定时器(事件时间和处理时间，仅在 keyed 流上)  ProcessFunction 可以被认为是一个 FlatMapFunction，它可以访问 keyed 状态和定时器。它通过对输入流中收到的每个事件进行调用来处理事件。\n对于容错状态，ProcessFunction 提供了对 Flink 的 keyed 状态的访问，通过 RuntimeContext 访问，类似于其他有状态函数访问 keyed 状态的方式。\n定时器允许应用程序对处理时间和事件时间的变化做出反应。每次调用函数 processElement(...) 都会得到一个 Context 对象，它可以访问元素的事件时间时间戳，以及 TimerService。TimerService 可以用来为未来的事件/处理时间实例注册回调。对于事件时间定时器，当当前水印提前到或超过定时器的时间戳时，就会调用 onTimer(...) 方法；而对于处理时间定时器，当挂钟时间达到指定时间时，就会调用 onTimer(...)。在该调用过程中，所有的状态又会被限定在创建定时器的键上，允许定时器对 keyed 状态进行操作。\n注意：如果你想访问 keyed 状态和定时器，你必须在 keyed 流上应用 ProcessFunction。\nstream.keyBy(...).process(new MyProcessFunction()) 低级连接(join) 为了实现对两个输入的低级操作，应用程序可以使用 CoProcessFunction 或 KeyedCoProcessFunction。该函数与两个不同的输入绑定，并从两个不同的输入中获取对 processElement1(...) 和 processElement2(...) 记录的单独调用。\n实现低级联接(join)通常遵循这种模式。\n 为一个输入（或两个输入）创建一个状态对象。 从其输入中接收到元素时更新状态 在接收到另一个输入的元素后，探测状态并产生加入的结果。  例如，您可能会将客户数据加入到金融交易中，同时为客户数据保留状态。如果你关心在面对失序事件时是否有完整的、确定性的加入，你可以使用一个定时器，当客户数据流的水印已经超过该交易的时间时，你可以为该交易评估并发出加入。\n例子 在下面的例子中，KeyedProcessFunction 维护每个键的计数，并且每当一分钟过去（以事件时间为单位），该键没有更新时，就会发出一个键/计数对。\n 计数、键和最后修改时间戳都存储在一个 ValueState 中，这个 ValueState 隐式地被键所限定。 对于每条记录，KeyedProcessFunction 都会递增计数器并设置最后修改时间戳。 该函数还将在未来一分钟内（以事件时间为单位）安排一次回调。 在每次回调时，它都会将回调的事件时间戳与存储的计数的最后修改时间进行核对，如果两者匹配（即在该分钟内没有进一步的更新发生），则发出 key/count。  注意: 这个简单的例子可以用会话窗口来实现。我们在这里使用 KeyedProcessFunction 来说明它提供的基本模式。\nimport org.apache.flink.api.common.state.ValueState import org.apache.flink.api.common.state.ValueStateDescriptor import org.apache.flink.api.java.tuple.Tuple import org.apache.flink.streaming.api.functions.KeyedProcessFunction import org.apache.flink.util.Collector // the source data stream val stream: DataStream[Tuple2[String, String]] = ... // apply the process function onto a keyed stream val result: DataStream[Tuple2[String, Long]] = stream .keyBy(0) .process(new CountWithTimeoutFunction()) /** * The data type stored in the state */ case class CountWithTimestamp(key: String, count: Long, lastModified: Long) /** * The implementation of the ProcessFunction that maintains the count and timeouts */ class CountWithTimeoutFunction extends KeyedProcessFunction[Tuple, (String, String), (String, Long)] { /** The state that is maintained by this process function */ lazy val state: ValueState[CountWithTimestamp] = getRuntimeContext .getState(new ValueStateDescriptor[CountWithTimestamp](\u0026#34;myState\u0026#34;, classOf[CountWithTimestamp])) override def processElement( value: (String, String), ctx: KeyedProcessFunction[Tuple, (String, String), (String, Long)]#Context, out: Collector[(String, Long)]): Unit = { // initialize or retrieve/update the state  val current: CountWithTimestamp = state.value match { case null =\u0026gt; CountWithTimestamp(value._1, 1, ctx.timestamp) case CountWithTimestamp(key, count, lastModified) =\u0026gt; CountWithTimestamp(key, count + 1, ctx.timestamp) } // write the state back  state.update(current) // schedule the next timer 60 seconds from the current event time  ctx.timerService.registerEventTimeTimer(current.lastModified + 60000) } override def onTimer( timestamp: Long, ctx: KeyedProcessFunction[Tuple, (String, String), (String, Long)]#OnTimerContext, out: Collector[(String, Long)]): Unit = { state.value match { case CountWithTimestamp(key, count, lastModified) if (timestamp == lastModified + 60000) =\u0026gt; out.collect((key, count)) case _ =\u0026gt; } } } 注意：在 Flink 1.4.0 之前，当从处理时间计时器调用时，ProcessFunction.onTimer() 方法将当前处理时间设置为事件时间戳。这种行为非常微妙，可能不会被用户发现。嗯，这是有害的，因为处理时间时间戳是不确定的，而且不与水印对齐。此外，用户实现的逻辑依赖于这个错误的时间戳极有可能是无意中的错误。所以我们决定修复它。升级到 1.4.0 后，使用这个错误的事件时间时间戳的 Flink 作业将失败，用户应该将他们的作业调整为正确的逻辑。\nKeyedProcessFunction KeyedProcessFunction 作为 ProcessFunction 的扩展，在其 onTimer(...) 方法中提供了对定时器键的访问。\noverride def onTimer(timestamp: Long, ctx: OnTimerContext, out: Collector[OUT]): Unit = { var key = ctx.getCurrentKey // ... } 定时器 两种类型的定时器（处理时间和事件时间）都由 TimerService 内部维护，并排队执行。\nTimerService 对每个键和时间戳的定时器进行重复复制，即每个键和时间戳最多只有一个定时器。如果同一个时间戳注册了多个定时器，onTimer() 方法将只被调用一次。\n注意 Flink 同步调用 onTimer() 和 processElement()。因此，用户不必担心状态的并发修改。\n容错性 定时器是容错的，并与应用程序的状态一起检查点。在故障恢复或从保存点启动应用程序时，定时器将被恢复。\n注意: 在恢复之前应该启动的检查点处理时间定时器将立即启动。这可能发生在应用程序从故障中恢复或从保存点启动时。\n注意: 定时器总是异步检查点，除了 RocksDB 后端/与增量快照/与基于堆的定时器的组合（将用FLINK-10026解决）。注意，大量的定时器会增加检查点时间，因为定时器是检查点状态的一部分。请参阅 \u0026ldquo;定时器凝聚 \u0026ldquo;部分，了解如何减少定时器数量的建议。\n定时器凝聚 由于 Flink 对每个键和时间戳只维护一个定时器，您可以通过降低定时器分辨率来凝聚定时器的数量。\n对于1秒的定时器分辨率（事件或处理时间），您可以将目标时间四舍五入到整秒。定时器最多会提前1秒发射，但不会晚于要求的毫秒精度。因此，每个键和秒最多有一个定时器。\nval coalescedTime = ((ctx.timestamp + timeout) / 1000) * 1000 ctx.timerService.registerProcessingTimeTimer(coalescedTime) 由于事件时间定时器只在有水印出现时才会启动，你也可以通过使用当前的水印来安排和凝聚这些定时器与下一个水印。\nval coalescedTime = ctx.timerService.currentWatermark + 1 ctx.timerService.registerEventTimeTimer(coalescedTime) 定时器也可以通过以下方式停止或删除。\n停止一个处理时间的定时器。\nval timestampOfTimerToStop = ... ctx.timerService.deleteProcessingTimeTimer(timestampOfTimerToStop) 停止事件时间定时器。\nval timestampOfTimerToStop = ... ctx.timerService.deleteEventTimeTimer(timestampOfTimerToStop) 注意: 如果没有注册给定时间戳的定时器，停止定时器没有效果。\n原文连接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/process_function.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-process-function/","tags":["Flink","Flink 官方文档"],"title":"Process Function"},{"categories":["Flink"],"contents":"Scala API 扩展 为了在 Scala 和 Java API 之间保持相当程度的一致性，一些允许在 Scala 中进行高级表达的功能被从标准 API 中省略了，包括批处理和流式处理。\n如果你想享受完整的 Scala 体验，你可以选择加入通过隐式转换来增强 Scala API 的扩展。\n要使用所有可用的扩展，您只需为 DataSet API 添加一个简单的导入即可。\nimport org.apache.flink.api.scala.extensions._ 或者 DataStream API:\nimport org.apache.flink.streaming.api.scala.extensions._ 另外，你也可以按顺序导入单个扩展，只使用你喜欢的扩展。\n接受部分函数 通常情况下，DataSet 和 DataStream API 都不接受匿名模式匹配函数来解构 tuple、case 类或集合，比如下面。\nval data: DataSet[(Int, String, Double)] = // [...] data.map { case (id, name, temperature) =\u0026gt; // [...]  // The previous line causes the following compilation error:  // \u0026#34;The argument types of an anonymous function must be fully known. (SLS 8.5)\u0026#34; } 该扩展在 DataSet 和 DataStream Scala API 中引入了新的方法，这些方法在扩展的 API 中具有一对一的对应关系。这些代理方法确实支持匿名模式匹配函数。\nDataSet API  mapWith\t方法和原来的 map (DataSet)  data.mapWith { case (_, value) =\u0026gt; value.toString }  mapPartitionWith 方法和原来的\tmapPartition (DataSet)  data.mapPartitionWith { case head #:: _ =\u0026gt; head }  flatMapWith\t方法和原来的 flatMap (DataSet)  data.flatMapWith { case (_, name, visitTimes) =\u0026gt; visitTimes.map(name -\u0026gt; _) }  filterWith 方法和原来的\tfilter (DataSet)  data.filterWith { case Train(_, isOnTime) =\u0026gt; isOnTime }  reduceWith 方法和原来的\treduce (DataSet, GroupedDataSet)  data.reduceWith { case ((_, amount1), (_, amount2)) =\u0026gt; amount1 + amount2 }  reduceGroupWith\t方法和原来的 reduceGroup (GroupedDataSet)  data.reduceGroupWith { case id #:: value #:: _ =\u0026gt; id -\u0026gt; value }  groupingBy 方法和原来的\tgroupBy (DataSet)  data.groupingBy { case (id, _, _) =\u0026gt; id }  sortGroupWith\t方法和原来的 sortGroup (GroupedDataSet)  grouped.sortGroupWith(Order.ASCENDING) { case House(_, value) =\u0026gt; value }  combineGroupWith 方法和原来的\tcombineGroup (GroupedDataSet)  grouped.combineGroupWith { case header #:: amounts =\u0026gt; amounts.sum }  projecting 方法和原来的\tapply (JoinDataSet, CrossDataSet)  data1.join(data2). whereClause(case (pk, _) =\u0026gt; pk). isEqualTo(case (_, fk) =\u0026gt; fk). projecting { case ((pk, tx), (products, fk)) =\u0026gt; tx -\u0026gt; products } data1.cross(data2).projecting { case ((a, _), (_, b) =\u0026gt; a -\u0026gt; b }  projecting 方法和原来的\tapply (CoGroupDataSet)  data1.coGroup(data2). whereClause(case (pk, _) =\u0026gt; pk). isEqualTo(case (_, fk) =\u0026gt; fk). projecting { case (head1 #:: _, head2 #:: _) =\u0026gt; head1 -\u0026gt; head2 } } DataStream API  mapWith\t方法和原来的 map (DataStream)  data.mapWith { case (_, value) =\u0026gt; value.toString }  flatMapWith\t方法和原来的 flatMap (DataStream)  data.flatMapWith { case (_, name, visits) =\u0026gt; visits.map(name -\u0026gt; _) }  filterWith 方法和原来的\tfilter (DataStream)  data.filterWith { case Train(_, isOnTime) =\u0026gt; isOnTime }  keyingBy 方法和原来的\tkeyBy (DataStream)  data.keyingBy { case (id, _, _) =\u0026gt; id }  mapWith 方法和原来的 map (ConnectedDataStream)  data.mapWith( map1 = case (_, value) =\u0026gt; value.toString, map2 = case (_, _, value, _) =\u0026gt; value + 1 )  flatMapWith\t方法和原来的 flatMap (ConnectedDataStream)  data.flatMapWith( flatMap1 = case (_, json) =\u0026gt; parse(json), flatMap2 = case (_, _, json, _) =\u0026gt; parse(json) )  keyingBy 方法和原来的\tkeyBy (ConnectedDataStream)  data.keyingBy( key1 = case (_, timestamp) =\u0026gt; timestamp, key2 = case (id, _, _) =\u0026gt; id )  reduceWith 方法和原来的 reduce (KeyedStream, WindowedStream)  data.reduceWith { case ((_, sum1), (_, sum2) =\u0026gt; sum1 + sum2 }  foldWith 方法和原来的\tfold (KeyedStream, WindowedStream)  data.foldWith(User(bought = 0)) { case (User(b), (_, items)) =\u0026gt; User(b + items.size) }  applyWith\t方法和原来的 apply (WindowedStream)  data.applyWith(0)( foldFunction = case (sum, amount) =\u0026gt; sum + amount windowFunction = case (k, w, sum) =\u0026gt; // [...] )  projecting 方法和原来的\tapply (JoinedStream)  data1.join(data2). whereClause(case (pk, _) =\u0026gt; pk). isEqualTo(case (_, fk) =\u0026gt; fk). projecting { case ((pk, tx), (products, fk)) =\u0026gt; tx -\u0026gt; products } 关于每个方法的语义的更多信息，请参考 DataSet 和 DataStream API 文档。\n要专门使用这个扩展，可以添加以下导入。\nimport org.apache.flink.api.scala.extensions.acceptPartialFunctions 对于 DataSet 扩展和\nimport org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions 下面的代码段展示了一个最小的例子，说明如何一起使用这些扩展方法（与 DataSet API 一起）。\nobject Main { import org.apache.flink.api.scala.extensions._ case class Point(x: Double, y: Double) def main(args: Array[String]): Unit = { val env = ExecutionEnvironment.getExecutionEnvironment val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6)) ds.filterWith { case Point(x, _) =\u0026gt; x \u0026gt; 1 }.reduceWith { case (Point(x1, y1), (Point(x2, y2))) =\u0026gt; Point(x1 + y1, x2 + y2) }.mapWith { case Point(x, y) =\u0026gt; (x, y) }.flatMapWith { case (x, y) =\u0026gt; Seq(\u0026#34;x\u0026#34; -\u0026gt; x, \u0026#34;y\u0026#34; -\u0026gt; y) }.groupingBy { case (id, value) =\u0026gt; id } } } 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/scala_api_extensions.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"Scala API 扩展"},{"categories":["Flink"],"contents":"SHOW 语句 SHOW 语句用于列出所有目录，或列出当前目录中的所有数据库，或列出当前目录和当前数据库中的所有表/视图，或列出当前目录和当前数据库中的所有函数，包括临时系统函数、系统函数、临时目录函数和目录函数。\nFlink SQL 目前支持以下 SHOW 语句。\n SHOW CATALOGS SHOW DATABASES SHOW TABLES SHOW VIEWS SHOW FUNCTIONS  运行 SHOW 语句 SHOW 语句可以用 TableEnvironment 的 executeSql()方法执行，也可以在 SQL CLI 中执行。executeSql()方法会对成功的 SHOW 操作返回对象，否则会抛出一个异常。\n下面的例子展示了如何在 TableEnvironment 和 SQL CLI 中运行 SHOW 语句。\nFlink SQL\u0026gt; SHOW CATALOGS; default_catalog Flink SQL\u0026gt; SHOW DATABASES; default_database Flink SQL\u0026gt; CREATE TABLE my_table (...) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; SHOW TABLES; my_table Flink SQL\u0026gt; CREATE VIEW my_view AS ...; [INFO] View has been created. Flink SQL\u0026gt; SHOW VIEWS; my_view Flink SQL\u0026gt; SHOW FUNCTIONS; mod sha256 ... val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // show catalogs tEnv.executeSql(\u0026#34;SHOW CATALOGS\u0026#34;).print() // +-----------------+ // | catalog name | // +-----------------+ // | default_catalog | // +-----------------+  // show databases tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print() // +------------------+ // | database name | // +------------------+ // | default_database | // +------------------+  // create a table tEnv.executeSql(\u0026#34;CREATE TABLE my_table (...) WITH (...)\u0026#34;) // show tables tEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() // +------------+ // | table name | // +------------+ // | my_table | // +------------+  // create a view tEnv.executeSql(\u0026#34;CREATE VIEW my_view AS ...\u0026#34;) // show views tEnv.executeSql(\u0026#34;SHOW VIEWS\u0026#34;).print() // +-----------+ // | view name | // +-----------+ // | my_view | // +-----------+  // show functions tEnv.executeSql(\u0026#34;SHOW FUNCTIONS\u0026#34;).print() // +---------------+ // | function name | // +---------------+ // | mod | // | sha256 | // | ... | // +---------------+ SHOW CATALOGS SHOWCATALOGS显示所有目录。\nSHOW DATABASES SHOWDATABASES显示当前目录中的所有数据库。\nSHOW TABLES SHOWTABLES显示当前目录和当前数据库中的所有表。\nSHOW VIEWS SHOWVIEWS显示当前目录和当前数据库中的所有视图。\nSHOW FUNCTIONS SHOWFUNCTIONS显示当前目录和当前数据库中的所有功能，包括临时系统功能、系统功能、临时目录功能和目录功能。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/show.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-show-statements/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","SQL"],"title":"Show 语句"},{"categories":["Flink"],"contents":"SQL 本页介绍了 Flink 支持的 SQL 语言，包括数据定义语言（DDL）、数据操作语言（DML）和查询语言。Flink 的 SQL 支持是基于 Apache Calcite，它实现了 SQL 标准。\n本页列出了目前 Flink SQL 支持的所有语句。\n SELECT (Queries) CREATE TABLE, DATABASE, VIEW, FUNCTION DROP TABLE, DATABASE, VIEW, FUNCTION ALTER TABLE, DATABASE, FUNCTION INSERT SQL HINTS DESCRIBE EXPLAIN USE SHOW  数据类型 请看关于数据类型的专门页面。\n通用类型和(嵌套的)复合类型(例如 POJOs、tuple、行、Scala case 类)也可以是行的字段。\n具有任意嵌套的复合类型的字段可以用值访问函数来访问。\n通用类型被当作一个黑盒子，可以通过用户定义的函数进行传递或处理。\n对于 DDL，我们支持页面数据类型中定义的完整数据类型。\n注意事项: 有些数据类型在 SQL 查询中还不支持（即在投递表达式或字元中）。例如：STRING, BYTES, RAW, WITHOUT TIME ZONE 的 TIME(p), WITH LOCAL TIME ZONE 的 TIME(p), WITHOUT TIME ZONE 的 TIMESTAMP(p), WITH LOCAL TIME ZONE 的 TIMESTAMP(p), ARRAY, MULTISET, ROW。\n保留关键词 虽然并不是每一个 SQL 功能都已实现，但有些字符串组合已经被保留为关键字，供将来使用。如果你想使用下面的一个字符串作为字段名，请确保在它们周围加上反引号（例如value，count）。\nA, ABS, ABSOLUTE, ACTION, ADA, ADD, ADMIN, AFTER, ALL, ALLOCATE, ALLOW, ALTER, ALWAYS, AND, ANY, ARE, ARRAY, AS, ASC, ASENSITIVE, ASSERTION, ASSIGNMENT, ASYMMETRIC, AT, ATOMIC, ATTRIBUTE, ATTRIBUTES, AUTHORIZATION, AVG, BEFORE, BEGIN, BERNOULLI, BETWEEN, BIGINT, BINARY, BIT, BLOB, BOOLEAN, BOTH, BREADTH, BY, BYTES, C, CALL, CALLED, CARDINALITY, CASCADE, CASCADED, CASE, CAST, CATALOG, CATALOG_NAME, CEIL, CEILING, CENTURY, CHAIN, CHAR, CHARACTER, CHARACTERISTICS, CHARACTERS, CHARACTER_LENGTH, CHARACTER_SET_CATALOG, CHARACTER_SET_NAME, CHARACTER_SET_SCHEMA, CHAR_LENGTH, CHECK, CLASS_ORIGIN, CLOB, CLOSE, COALESCE, COBOL, COLLATE, COLLATION, COLLATION_CATALOG, COLLATION_NAME, COLLATION_SCHEMA, COLLECT, COLUMN, COLUMN_NAME, COMMAND_FUNCTION, COMMAND_FUNCTION_CODE, COMMIT, COMMITTED, CONDITION, CONDITION_NUMBER, CONNECT, CONNECTION, CONNECTION_NAME, CONSTRAINT, CONSTRAINTS, CONSTRAINT_CATALOG, CONSTRAINT_NAME, CONSTRAINT_SCHEMA, CONSTRUCTOR, CONTAINS, CONTINUE, CONVERT, CORR, CORRESPONDING, COUNT, COVAR_POP, COVAR_SAMP, CREATE, CROSS, CUBE, CUME_DIST, CURRENT, CURRENT_CATALOG, CURRENT_DATE, CURRENT_DEFAULT_TRANSFORM_GROUP, CURRENT_PATH, CURRENT_ROLE, CURRENT_SCHEMA, CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_TRANSFORM_GROUP_FOR_TYPE, CURRENT_USER, CURSOR, CURSOR_NAME, CYCLE, DATA, DATABASE, DATE, DATETIME_INTERVAL_CODE, DATETIME_INTERVAL_PRECISION, DAY, DEALLOCATE, DEC, DECADE, DECIMAL, DECLARE, DEFAULT, DEFAULTS, DEFERRABLE, DEFERRED, DEFINED, DEFINER, DEGREE, DELETE, DENSE_RANK, DEPTH, DEREF, DERIVED, DESC, DESCRIBE, DESCRIPTION, DESCRIPTOR, DETERMINISTIC, DIAGNOSTICS, DISALLOW, DISCONNECT, DISPATCH, DISTINCT, DOMAIN, DOUBLE, DOW, DOY, DROP, DYNAMIC, DYNAMIC_FUNCTION, DYNAMIC_FUNCTION_CODE, EACH, ELEMENT, ELSE, END, END-EXEC, EPOCH, EQUALS, ESCAPE, EVERY, EXCEPT, EXCEPTION, EXCLUDE, EXCLUDING, EXEC, EXECUTE, EXISTS, EXP, EXPLAIN, EXTEND, EXTERNAL, EXTRACT, FALSE, FETCH, FILTER, FINAL, FIRST, FIRST_VALUE, FLOAT, FLOOR, FOLLOWING, FOR, FOREIGN, FORTRAN, FOUND, FRAC_SECOND, FREE, FROM, FULL, FUNCTION, FUSION, G, GENERAL, GENERATED, GET, GLOBAL, GO, GOTO, GRANT, GRANTED, GROUP, GROUPING, HAVING, HIERARCHY, HOLD, HOUR, IDENTITY, IMMEDIATE, IMPLEMENTATION, IMPORT, IN, INCLUDING, INCREMENT, INDICATOR, INITIALLY, INNER, INOUT, INPUT, INSENSITIVE, INSERT, INSTANCE, INSTANTIABLE, INT, INTEGER, INTERSECT, INTERSECTION, INTERVAL, INTO, INVOKER, IS, ISOLATION, JAVA, JOIN, K, KEY, KEY_MEMBER, KEY_TYPE, LABEL, LANGUAGE, LARGE, LAST, LAST_VALUE, LATERAL, LEADING, LEFT, LENGTH, LEVEL, LIBRARY, LIKE, LIMIT, LN, LOCAL, LOCALTIME, LOCALTIMESTAMP, LOCATOR, LOWER, M, MAP, MATCH, MATCHED, MAX, MAXVALUE, MEMBER, MERGE, MESSAGE_LENGTH, MESSAGE_OCTET_LENGTH, MESSAGE_TEXT, METHOD, MICROSECOND, MILLENNIUM, MIN, MINUTE, MINVALUE, MOD, MODIFIES, MODULE, MONTH, MORE, MULTISET, MUMPS, NAME, NAMES, NATIONAL, NATURAL, NCHAR, NCLOB, NESTING, NEW, NEXT, NO, NONE, NORMALIZE, NORMALIZED, NOT, NULL, NULLABLE, NULLIF, NULLS, NUMBER, NUMERIC, OBJECT, OCTETS, OCTET_LENGTH, OF, OFFSET, OLD, ON, ONLY, OPEN, OPTION, OPTIONS, OR, ORDER, ORDERING, ORDINALITY, OTHERS, OUT, OUTER, OUTPUT, OVER, OVERLAPS, OVERLAY, OVERRIDING, PAD, PARAMETER, PARAMETER_MODE, PARAMETER_NAME, PARAMETER_ORDINAL_POSITION, PARAMETER_SPECIFIC_CATALOG, PARAMETER_SPECIFIC_NAME, PARAMETER_SPECIFIC_SCHEMA, PARTIAL, PARTITION, PASCAL, PASSTHROUGH, PATH, PERCENTILE_CONT, PERCENTILE_DISC, PERCENT_RANK, PLACING, PLAN, PLI, POSITION, POWER, PRECEDING, PRECISION, PREPARE, PRESERVE, PRIMARY, PRIOR, PRIVILEGES, PROCEDURE, PUBLIC, QUARTER, RANGE, RANK, RAW, READ, READS, REAL, RECURSIVE, REF, REFERENCES, REFERENCING, REGR_AVGX, REGR_AVGY, REGR_COUNT, REGR_INTERCEPT, REGR_R2, REGR_SLOPE, REGR_SXX, REGR_SXY, REGR_SYY, RELATIVE, RELEASE, REPEATABLE, RESET, RESTART, RESTRICT, RESULT, RETURN, RETURNED_CARDINALITY, RETURNED_LENGTH, RETURNED_OCTET_LENGTH, RETURNED_SQLSTATE, RETURNS, REVOKE, RIGHT, ROLE, ROLLBACK, ROLLUP, ROUTINE, ROUTINE_CATALOG, ROUTINE_NAME, ROUTINE_SCHEMA, ROW, ROWS, ROW_COUNT, ROW_NUMBER, SAVEPOINT, SCALE, SCHEMA, SCHEMA_NAME, SCOPE, SCOPE_CATALOGS, SCOPE_NAME, SCOPE_SCHEMA, SCROLL, SEARCH, SECOND, SECTION, SECURITY, SELECT, SELF, SENSITIVE, SEQUENCE, SERIALIZABLE, SERVER, SERVER_NAME, SESSION, SESSION_USER, SET, SETS, SIMILAR, SIMPLE, SIZE, SMALLINT, SOME, SOURCE, SPACE, SPECIFIC, SPECIFICTYPE, SPECIFIC_NAME, SQL, SQLEXCEPTION, SQLSTATE, SQLWARNING, SQL_TSI_DAY, SQL_TSI_FRAC_SECOND, SQL_TSI_HOUR, SQL_TSI_MICROSECOND, SQL_TSI_MINUTE, SQL_TSI_MONTH, SQL_TSI_QUARTER, SQL_TSI_SECOND, SQL_TSI_WEEK, SQL_TSI_YEAR, SQRT, START, STATE, STATEMENT, STATIC, STDDEV_POP, STDDEV_SAMP, STREAM, STRING, STRUCTURE, STYLE, SUBCLASS_ORIGIN, SUBMULTISET, SUBSTITUTE, SUBSTRING, SUM, SYMMETRIC, SYSTEM, SYSTEM_USER, TABLE, TABLESAMPLE, TABLE_NAME, TEMPORARY, THEN, TIES, TIME, TIMESTAMP, TIMESTAMPADD, TIMESTAMPDIFF, TIMEZONE_HOUR, TIMEZONE_MINUTE, TINYINT, TO, TOP_LEVEL_COUNT, TRAILING, TRANSACTION, TRANSACTIONS_ACTIVE, TRANSACTIONS_COMMITTED, TRANSACTIONS_ROLLED_BACK, TRANSFORM, TRANSFORMS, TRANSLATE, TRANSLATION, TREAT, TRIGGER, TRIGGER_CATALOG, TRIGGER_NAME, TRIGGER_SCHEMA, TRIM, TRUE, TYPE, UESCAPE, UNBOUNDED, UNCOMMITTED, UNDER, UNION, UNIQUE, UNKNOWN, UNNAMED, UNNEST, UPDATE, UPPER, UPSERT, USAGE, USER, USER_DEFINED_TYPE_CATALOG, USER_DEFINED_TYPE_CODE, USER_DEFINED_TYPE_NAME, USER_DEFINED_TYPE_SCHEMA, USING, VALUE, VALUES, VARBINARY, VARCHAR, VARYING, VAR_POP, VAR_SAMP, VERSION, VIEW, WEEK, WHEN, WHENEVER, WHERE, WIDTH_BUCKET, WINDOW, WITH, WITHIN, WITHOUT, WORK, WRAPPER, WRITE, XML, YEAR, ZONE ","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-sql-overview/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"SQL"},{"categories":["Flink"],"contents":"SQL Client Flink 的 Table \u0026amp; SQL API 使得它可以使用 SQL 语言编写的查询，但是这些查询需要嵌入到一个用 Java 或 Scala 编写的表程序中。而且，这些程序在提交给集群之前需要用构建工具打包。这或多或少限制了 Flink 对 Java/Scala 程序员的使用。\nSQL Client 旨在提供一种简单的方式来编写、调试和提交表程序到 Flink 集群，而不需要任何一行 Java 或 Scala 代码。SQL Client CLI 允许在命令行上从运行的分布式应用中检索和可视化实时结果。\n入门 本节介绍如何从命令行设置和运行第一个 Flink SQL 程序。\nSQL Client 被捆绑在常规的 Flink 发行版中，因此可以开箱即运行。它只需要一个正在运行的 Flink 集群，在那里可以执行表程序。关于设置 Flink 集群的更多信息，请参见集群和部署部分。如果你只是想试用 SQL Client，也可以使用下面的命令用一个 worker 启动一个本地集群。\n./bin/start-cluster.sh 启动 SQL 客户端 CLI SQL Client 脚本也位于 Flink 的二进制目录中。在未来，用户将有两种启动 SQL Client CLI 的可能性，一是通过启动一个嵌入式的独立进程，二是通过连接到一个远程 SQL Client 网关。目前只支持嵌入式模式。你可以通过调用来启动 CLI。\n./bin/sql-client.sh embedded 默认情况下，SQL 客户端将从位于 ./conf/sql-client-defaults.yaml 的环境文件中读取其配置。有关环境文件结构的更多信息，请参见配置部分。\n运行 SQL 查询 一旦启动 CLI，您可以使用 HELP 命令列出所有可用的 SQL 语句。为了验证你的设置和集群连接，你可以输入第一个 SQL 查询，然后按回车键执行。\nSELECT\u0026#39;Hello World\u0026#39;;这个查询不需要表源(table source)，并产生一个单行结果。CLI 将从集群中检索结果，并将其可视化。您可以按Q键关闭结果视图。\nCLI 支持三种模式来维护和可视化结果。\ntable 模式将结果在内存中具体化，并以常规的、分页的表格表示方式将其可视化。可以通过在 CLI 中执行以下命令启用该模式。\nSET execution.result-mode=table; changelog 模式不将结果具体化，而是将由插入(+)和收回(-)组成的连续查询所产生的结果流可视化。\nSET execution.result-mode=changelog; tableau 模式更像是传统的方式，将结果以 tableau 的形式直接显示在屏幕上。显示内容会受到查询执行类型(execute.type)的影响。\nSET execution.result-mode=tableau; 请注意，当您使用此模式进行流式查询时，结果将在控制台中连续打印。如果这个查询的输入数据是有边界的，那么在Flink处理完所有输入数据后，作业就会终止，打印也会自动停止。否则，如果你想终止一个正在运行的查询，在这种情况下只要输入CTRL-C键，作业和打印就会停止。\n你可以使用下面的查询来查看所有的结果模式。\nSELECTname,COUNT(*)AScntFROM(VALUES(\u0026#39;Bob\u0026#39;),(\u0026#39;Alice\u0026#39;),(\u0026#39;Greg\u0026#39;),(\u0026#39;Bob\u0026#39;))ASNameTable(name)GROUPBYname;这个查询执行一个有界字数的例子。\n在 changelog 模式下，可视化的 changelog 应该类似于。\n+ Bob, 1 + Alice, 1 + Greg, 1 - Bob, 1 + Bob, 2 在 table 模式下，可视化的结果表会持续更新，直到表程序结束。\nBob, 2 Alice, 1 Greg, 1 在tableau模式下，如果你在流模式下运行查询，显示的结果将是。\n+-----+----------------------+----------------------+ | +/- | name | cnt | +-----+----------------------+----------------------+ | + | Bob | 1 | | + | Alice | 1 | | + | Greg | 1 | | - | Bob | 1 | | + | Bob | 2 | +-----+----------------------+----------------------+ Received a total of 5 rows 而如果你在批处理模式下运行查询，显示的结果将是。\n+-------+-----+ | name | cnt | +-------+-----+ | Alice | 1 | | Bob | 2 | | Greg | 1 | +-------+-----+ 3 rows in set 在 SQL 查询的原型设计过程中，所有这些结果模式都是有用的。在所有这些模式中，结果都存储在 SQL 客户端的 Java 堆内存中。为了保持CLI界面的响应性，changelog 模式只显示最新的1000个变化。表模式允许浏览更大的结果，这些结果仅受可用主内存和配置的最大行数（max-table-result-rows）的限制。\n注意：在批处理环境中执行的查询，只能使用table或tableau结果模式进行检索。\n在定义了一个查询后，可以将其作为一个长期运行的、分离的 Flink 作业提交给集群。为此，需要使用 INSERT INTO 语句指定存储结果的目标系统。配置部分解释了如何声明 table source 以读取数据，如何声明 table sink 以写入数据，以及如何配置其他表程序属性。\n配置 SQL Client 可以通过以下可选的CLI命令来启动。这些命令将在后面的段落中详细讨论。\n./bin/sql-client.sh embedded --help Mode \u0026quot;embedded\u0026quot; submits Flink jobs from the local machine. Syntax: embedded [OPTIONS] \u0026quot;embedded\u0026quot; mode options: -d,--defaults \u0026lt;environment file\u0026gt; The environment properties with which every new session is initialized. Properties might be overwritten by session properties. -e,--environment \u0026lt;environment file\u0026gt; The environment properties to be imported into the session. It might overwrite default environment properties. -h,--help Show the help message with descriptions of all options. -hist,--history \u0026lt;History file path\u0026gt; The file which you want to save the command history into. If not specified, we will auto-generate one under your user's home directory. -j,--jar \u0026lt;JAR file\u0026gt; A JAR file to be imported into the session. The file might contain user-defined classes needed for the execution of statements such as functions, table sources, or sinks. Can be used multiple times. -l,--library \u0026lt;JAR directory\u0026gt; A JAR file directory with which every new session is initialized. The files might contain user-defined classes needed for the execution of statements such as functions, table sources, or sinks. Can be used multiple times. -pyarch,--pyArchives \u0026lt;arg\u0026gt; Add python archive files for job. The archive files will be extracted to the working directory of python UDF worker. Currently only zip-format is supported. For each archive file, a target directory be specified. If the target directory name is specified, the archive file will be extracted to a name can directory with the specified name. Otherwise, the archive file will be extracted to a directory with the same name of the archive file. The files uploaded via this option are accessible via relative path. '#' could be used as the separator of the archive file path and the target directory name. Comma (',') could be used as the separator to specify multiple archive files. This option can be used to upload the virtual environment, the data files used in Python UDF (e.g.: --pyArchives file:///tmp/py37.zip,file:///tmp/data .zip#data --pyExecutable py37.zip/py37/bin/python). The data files could be accessed in Python UDF, e.g.: f = open('data/data.txt', 'r'). -pyexec,--pyExecutable \u0026lt;arg\u0026gt; Specify the path of the python interpreter used to execute the python UDF worker (e.g.: --pyExecutable /usr/local/bin/python3). The python UDF worker depends on Python 3.5+, Apache Beam (version == 2.19.0), Pip (version \u0026gt;= 7.1.0) and SetupTools (version \u0026gt;= 37.0.0). Please ensure that the specified environment meets the above requirements. -pyfs,--pyFiles \u0026lt;pythonFiles\u0026gt; Attach custom python files for job. These files will be added to the PYTHONPATH of both the local client and the remote python UDF worker. The standard python resource file suffixes such as .py/.egg/.zip or directory are all supported. Comma (',') could be used as the separator to specify multiple files (e.g.: --pyFiles file:///tmp/myresource.zip,hdfs:///$n amenode_address/myresource2.zip). -pyreq,--pyRequirements \u0026lt;arg\u0026gt; Specify a requirements.txt file which defines the third-party dependencies. These dependencies will be installed and added to the PYTHONPATH of the python UDF worker. A directory which contains the installation packages of these dependencies could be specified optionally. Use '#' as the separator if the optional parameter exists (e.g.: --pyRequirements file:///tmp/requirements.txt#file:/// tmp/cached_dir). -s,--session \u0026lt;session identifier\u0026gt; The identifier for a session. 'default' is the default identifier. -u,--update \u0026lt;SQL update statement\u0026gt; Experimental (for testing only!): Instructs the SQL Client to immediately execute the given update statement after starting up. The process is shut down after the statement has been submitted to the cluster and returns an appropriate return code. Currently, this feature is only supported for INSERT INTO statements that declare the target sink table. 环境文件 一个SQL查询需要一个配置环境来执行。所谓的环境文件定义了可用的目录(catalogs)、table source 和 sink、用户定义的函数以及执行和部署所需的其他属性。\n每个环境文件都是一个常规的 YAML 文件。下面是这样一个文件的例子。\n# Define tables here such as sources, sinks, views, or temporal tables.tables:- name:MyTableSourcetype:source-tableupdate-mode:appendconnector:type:filesystempath:\u0026#34;/path/to/something.csv\u0026#34;format:type:csvfields:- name:MyField1data-type:INT- name:MyField2data-type:VARCHARline-delimiter:\u0026#34;\\n\u0026#34;comment-prefix:\u0026#34;#\u0026#34;schema:- name:MyField1data-type:INT- name:MyField2data-type:VARCHAR- name:MyCustomViewtype:viewquery:\u0026#34;SELECT MyField2 FROM MyTableSource\u0026#34;# Define user-defined functions here.functions:- name:myUDFfrom:classclass:foo.bar.AggregateUDFconstructor:- 7.6- false# Define available catalogscatalogs:- name:catalog_1type:hiveproperty-version:1hive-conf-dir:...- name:catalog_2type:hiveproperty-version:1default-database:mydb2hive-conf-dir:...# Properties that change the fundamental execution behavior of a table program.execution:planner: blink # optional:either \u0026#39;blink\u0026#39; (default) or \u0026#39;old\u0026#39;type: streaming # required:execution mode either \u0026#39;batch\u0026#39; or \u0026#39;streaming\u0026#39;result-mode: table # required:either \u0026#39;table\u0026#39; or \u0026#39;changelog\u0026#39;max-table-result-rows: 1000000 # optional:maximum number of maintained rows in# \u0026#39;table\u0026#39; mode (1000000 by default, smaller 1 means unlimited)time-characteristic: event-time # optional:\u0026#39;processing-time\u0026#39;or \u0026#39;event-time\u0026#39; (default)parallelism: 1 # optional:Flink\u0026#39;s parallelism (1 by default)periodic-watermarks-interval: 200 # optional:interval for periodic watermarks (200 ms by default)max-parallelism: 16 # optional:Flink\u0026#39;s maximum parallelism (128 by default)min-idle-state-retention: 0 # optional:table program\u0026#39;s minimum idle state timemax-idle-state-retention: 0 # optional:table program\u0026#39;s maximum idle state timecurrent-catalog: catalog_1 # optional:name of the current catalog of the session (\u0026#39;default_catalog\u0026#39; by default)current-database: mydb1 # optional:name of the current database of the current catalog# (default database of the current catalog by default)restart-strategy: # optional:restart strategytype:fallback # \u0026#34;fallback\u0026#34; to global restart strategy by default# Configuration options for adjusting and tuning table programs.# A full list of options and their default values can be found# on the dedicated \u0026#34;Configuration\u0026#34; page.configuration:table.optimizer.join-reorder-enabled:truetable.exec.spill-compression.enabled:truetable.exec.spill-compression.block-size:128kb# Properties that describe the cluster to which table programs are submitted to.deployment:response-timeout:5000这份配置:\n 定义了一个表源 MyTableSource 的环境，该表源从 CSV 文件中读取。 定义了一个视图 MyCustomView，该视图使用 SQL 查询声明了一个虚拟表。 定义了一个用户定义的函数 myUDF，可以使用类名和两个构造函数参数来实例化。 连接两个 Hive 目录，并使用 catalog_1 作为当前目录，mydb1 作为目录的当前数据库。 在流式模式下使用 blink planner，运行具有事件时间特性和并行度为1的语句。 在 table 结果模式下运行探索性查询。 并通过配置选项围绕连接重排序和溢出进行一些 planner 调整。  根据使用情况，一个配置可以被分割成多个文件。因此，可以为一般目的创建环境文件（使用 --defaults 创建默认环境文件），也可以为每个会话创建环境文件（使用 --environment 创建会话环境文件）。每一个 CLI 会话都会用默认属性和会话属性来初始化。例如，defaults 环境文件可以指定在每个会话中应该可以查询的所有 table source，而 session 环境文件只声明特定的状态保留时间和并行度。在启动CLI应用程序时，可以同时传递默认环境文件和会话环境文件。如果没有指定默认环境文件，SQL Client 会在 Flink 的配置目录下搜索 ./conf/sql-client-defaults.yaml。\n注意：在 CLI 会话中设置的属性（例如使用SET命令）具有最高优先权。\nCLI commands \u0026gt; session environment file \u0026gt; defaults environment file 重启策略 重启策略控制 Flink 作业在发生故障时如何重启。与 Flink 集群的全局重启策略类似，可以在环境文件中声明一个更精细的重启配置。\n支持以下策略。\nexecution:# falls back to the global strategy defined in flink-conf.yamlrestart-strategy:type:fallback# job fails directly and no restart is attemptedrestart-strategy:type:none# attempts a given number of times to restart the jobrestart-strategy:type:fixed-delayattempts: 3 # retries before job is declared as failed (default:Integer.MAX_VALUE)delay: 10000 # delay in ms between retries (default:10s)# attempts as long as the maximum number of failures per time interval is not exceededrestart-strategy:type:failure-ratemax-failures-per-interval: 1 # retries in interval until failing (default:1)failure-rate-interval:60000# measuring interval in ms for failure ratedelay: 10000 # delay in ms between retries (default:10s)依赖性 SQL Client 不需要使用 Maven 或 SBT 设置一个 Java 项目。相反，你可以将依赖关系作为常规的 JAR 文件传递给集群。你可以单独指定每个 JAR 文件（使用 --jar）或定义整个库目录（使用 --library）。对于连接到外部系统（如 Apache Kafka）的连接器和相应的数据格式（如JSON），Flink 提供了现成的 JAR bundles。这些 JAR 文件可以从 Maven 中央仓库为每个版本下载。\n所提供的 SQL JAR 的完整列表和关于如何使用它们的文档可以在连接到外部系统页面上找到。\n下面的例子显示了一个环境文件，它定义了一个从 Apache Kafka 读取 JSON 数据的 table source。\ntables:- name:TaxiRidestype:source-tableupdate-mode:appendconnector:property-version:1type:kafkaversion:\u0026#34;0.11\u0026#34;topic:TaxiRidesstartup-mode:earliest-offsetproperties:bootstrap.servers:localhost:9092group.id:testGroupformat:property-version:1type:jsonschema:\u0026#34;ROW\u0026lt;rideId LONG, lon FLOAT, lat FLOAT, rideTime TIMESTAMP\u0026gt;\u0026#34;schema:- name:rideIddata-type:BIGINT- name:londata-type:FLOAT- name:latdata-type:FLOAT- name:rowTimedata-type:TIMESTAMP(3)rowtime:timestamps:type:\u0026#34;from-field\u0026#34;from:\u0026#34;rideTime\u0026#34;watermarks:type:\u0026#34;periodic-bounded\u0026#34;delay:\u0026#34;60000\u0026#34;- name:procTimedata-type:TIMESTAMP(3)proctime:true由此产生的 TaxiRide 表的模式(schema)包含了 JSON 模式(schema)的大部分字段。此外，它还增加了一个行时间属性 rowTime 和处理时间属性 procTime。\nconnector 和 format 都允许定义一个属性版本（目前是版本1），以便于将来向后兼容。\n用户定义的函数 SQL Client 允许用户创建自定义的、用户定义的函数，以便在 SQL 查询中使用。目前，这些函数被限制在 Java/Scala 类或 Python 文件中以编程方式定义。\n为了提供一个 Java/Scala 用户定义的函数，你需要首先实现和编译一个扩展 ScalarFunction、AggregateFunction 或 TableFunction 的函数类（见用户定义的函数）。然后可以将一个或多个函数打包到 SQL Client 的依赖 JAR 中。\n为了提供一个 Python 用户定义函数，你需要编写一个 Python 函数，并用 pyflink.table.udf.udf 或 pyflink.table.udf.udtf 装饰器来装饰它(见 Python UDFs)。然后可以将一个或多个函数放入一个 Python 文件中。Python 文件和相关的依赖关系需要通过环境文件中的配置 (参见 Python 配置) 或命令行选项 (参见命令行用法) 来指定。\n所有函数在被调用之前必须在环境文件中声明。对于函数列表中的每一项，必须指定\n 注册该函数的名称。 函数的源，使用 from (暂时限制为 class (Java/Scala UDF) 或 python (Python UDF))。  Java/Scala UDF 必须指定:\n class，表示函数的完全限定类名和一个可选的实例化构造参数列表。  Python 的 UDF 必须指定:\n fully-qualified-name: 表示完全限定的名称，即函数的\u0026quot;[模块名].[对象名]\u0026quot;。  functions:- name: java_udf # required:name of the functionfrom: class # required:source of the functionclass: ... # required:fully qualified class name of the functionconstructor: # optional:constructor parameters of the function class- ... # optional:a literal parameter with implicit type- class: ... # optional:full class name of the parameterconstructor: # optional:constructor parameters of the parameter\u0026#39;s class- type: ... # optional:type of the literal parametervalue: ... # optional:value of the literal parameter- name: python_udf # required:name of the functionfrom: python # required:source of the function fully-qualified-name: ... # required:fully qualified class name of the function 对于 Java/Scala UDF，请确保指定参数的顺序和类型严格匹配你的函数类的一个构造函数。\n构造函数参数 根据用户定义的函数，在 SQL 语句中使用它之前，可能需要对实现进行参数化。\n如前面的例子所示，在声明用户定义函数时，可以通过以下三种方式之一使用构造函数参数来配置类。\n一个隐含类型的字面值。SQL Client 会根据字面值本身自动推导出类型。目前，这里只支持 BOOLEAN、INT、DOUBLE 和 VARCHAR 的值。如果自动推导没有达到预期的效果（例如，你需要一个 VARCHAR false），请使用显式类型代替。\n- true # -\u0026gt; BOOLEAN (case sensitive) - 42 # -\u0026gt; INT - 1234.222 # -\u0026gt; DOUBLE - foo # -\u0026gt; VARCHAR 一个具有明确类型的字面值。明确声明参数的 type 和 value 属性，以保证类型安全。\n- type: DECIMAL value: 11111111111111111 下表说明了支持的 Java 参数类型和相应的 SQL 类型字符串。\n   Java type SQL type     java.math.BigDecimal DECIMAL   java.lang.Boolean BOOLEAN   java.lang.Byte TINYINT   java.lang.Double DOUBLE   java.lang.Float REAL, FLOAT   java.lang.Integer INTEGER, INT   java.lang.Long BIGINT   java.lang.Short SMALLINT   java.lang.String VARCHAR    目前还不支持更多的类型（如 TIMESTAMP 或 ARRAY）、原语类型和 null。\n一个（嵌套的）类实例。除了字面值，你还可以通过指定 class 和 constructor 函数属性，为构造函数参数创建（嵌套）类实例。这个过程可以递归执行，直到所有的构造参数都用字面值表示。\n- class:foo.bar.paramClassconstructor:- StarryName- class:java.lang.Integerconstructor:- class:java.lang.Stringconstructor:- type:VARCHARvalue:3Catalogs Catalogs 可以定义为一组 YAML 属性，在启动 SQL Client 时自动注册到环境中。\n用户可以在 SQL CLI 中指定要使用哪个目录(catalog)作为当前目录(catalog)，以及要使用该目录(catalog)的哪个数据库作为当前数据库。\ncatalogs:- name:catalog_1type:hiveproperty-version:1default-database:mydb2hive-conf-dir:\u0026lt;path of Hive conf directory\u0026gt;- name:catalog_2type:hiveproperty-version:1hive-conf-dir:\u0026lt;path of Hive conf directory\u0026gt;execution:...current-catalog:catalog_1current-database:mydb1关于目录的更多信息，请参见目录。\n分离式 SQL 查询 为了定义端到端 SQL 管道，SQL 的 INSERT INTO 语句可以用于向 Flink 集群提交长期运行的、分离的查询。这些查询将其结果产生到外部系统中，而不是 SQL Client。这允许处理更高的并行性和更大的数据量。CLI 本身对提交后的分离查询没有任何控制。\nINSERTINTOMyTableSinkSELECT*FROMMyTableSource表接收器 MyTableSink 必须在环境文件中声明。有关支持的外部系统及其配置的更多信息，请参见连接页面。下面是一个 Apache Kafka 表接收器的例子。\ntables:- name:MyTableSinktype:sink-tableupdate-mode:appendconnector:property-version:1type:kafkaversion:\u0026#34;0.11\u0026#34;topic:OutputTopicproperties:bootstrap.servers:localhost:9092group.id:testGroupformat:property-version:1type:jsonderive-schema:trueschema:- name:rideIddata-type:BIGINT- name:londata-type:FLOAT- name:latdata-type:FLOAT- name:rideTimedata-type:TIMESTAMP(3)SQL 客户端确保语句成功提交到集群。一旦提交查询，CLI 将显示有关 Flink 作业的信息。\n[INFO] Table update statement has been successfully submitted to the cluster: Cluster ID: StandaloneClusterId Job ID: 6f922fe5cba87406ff23ae4a7bb79044 Web interface: http://localhost:8081 注意: SQL 客户端在提交后不会跟踪正在运行的 Flink 作业的状态。CLI 进程可以在提交后被关闭，而不影响分离查询。Flink 的重启策略照顾到了容错。可以使用 Flink 的 Web 界面、命令行或 REST API 来取消查询。\nSQL 视图 视图允许从 SQL 查询中定义虚拟表。视图定义会被立即解析和验证。然而，实际的执行发生在提交一般的 INSERT INTO 或 SELECT 语句期间访问视图时。\n视图可以在环境文件中定义，也可以在 CLI 会话中定义。\n下面的例子显示了如何在一个文件中定义多个视图。视图是按照它们在环境文件中定义的顺序注册的。支持诸如视图 A 依赖于视图 B 依赖于视图 C 的引用链。\ntables:- name:MyTableSource# ...- name:MyRestrictedViewtype:viewquery:\u0026#34;SELECT MyField2 FROM MyTableSource\u0026#34;- name:MyComplexViewtype:viewquery:\u0026gt;SELECT MyField2 + 42, CAST(MyField1 AS VARCHAR) FROM MyTableSource WHERE MyField2 \u0026gt; 200与 table source 和 sink 类似，会话环境文件中定义的视图具有最高优先级。\n视图也可以在 CLI 会话中使用 CREATE VIEW 语句创建。\nCREATEVIEWMyNewViewASSELECTMyField2FROMMyTableSource;在 CLI 会话中创建的视图也可以使用 DROP VIEW 语句再次删除。\nDROPVIEWMyNewView;注意: CLI 中视图的定义仅限于上述语法。在未来的版本中，将支持为视图定义模式或在表名中转义空格。\n临时表 临时表允许对变化的历史表进行（参数化的）查看，该表在特定的时间点返回一个表的内容。这对于将一个表与另一个表在特定时间戳的内容连接起来特别有用。更多信息可以在临时表连接页面中找到。\n下面的示例展示了如何定义一个临时表 SourceTemporalTable。\ntables:# Define the table source (or view) that contains updates to a temporal table- name:HistorySourcetype:source-tableupdate-mode:appendconnector:# ...format:# ...schema:- name:integerFielddata-type:INT- name:stringFielddata-type:STRING- name:rowtimeFielddata-type:TIMESTAMP(3)rowtime:timestamps:type:from-fieldfrom:rowtimeFieldwatermarks:type:from-source# Define a temporal table over the changing history table with time attribute and primary key- name:SourceTemporalTabletype:temporal-tablehistory-table:HistorySourceprimary-key:integerFieldtime-attribute:rowtimeField # could also be a proctime field如示例中所示，table source、视图和临时表的定义可以相互混合。它们按照在环境文件中定义的顺序进行注册。例如，一个临时表可以引用一个视图，该视图可以依赖于另一个视图或 table source。\n限制和未来 目前的 SQL Client 只支持嵌入式模式。未来，社区计划通过提供基于 REST 的 SQL Client 网关来扩展其功能，详见 FLIP-24 和 FLIP-91。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-sql-client/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"SQL 客户端"},{"categories":["Flink"],"contents":"SQL 提示 SQL 提示可以与 SQL 语句一起使用，以改变执行计划。本章解释了如何使用提示来强制各种方法。\n一般来说，一个提示可以用来。\n强制执行计划器：没有完美的计划器，所以实现提示让用户更好地控制执行是有意义的。 Append meta data(或统计)：一些统计，比如\u0026quot;扫描的表索引\u0026quot;和 \u0026ldquo;一些 shuffle 键的 skew info\u0026rdquo;，对于查询来说是有些动态的，用提示来配置它们会非常方便，因为我们从 planner 得到的规划元数据往往不是那么准确。 运算符资源约束：对于很多情况，我们会给执行运算符一个默认的资源配置，比如最小并行或管理内存（耗费资源的 UDF）或特殊的资源需求（GPU 或 SSD 磁盘）等等，用提示对每个查询（而不是 Job）的资源进行配置会非常灵活。\n动态表选项 动态表选项允许动态指定或覆盖表选项，与 SQL DDL 或连接 API 定义的静态表选项不同，这些选项可以在每个查询中的每个表范围内灵活指定。\n因此，它非常适用于交互式终端的临时查询，例如，在 SQL-CLI 中，只需添加一个动态选项 /*+ OPTIONS('csv.ignore-parse-errors'='true') */，就可以指定忽略 CSV 源的解析错误。\n注意：动态表选项默认是禁止使用的，因为它可能会改变查询的语义。您需要将配置选项 table.dynamic-table-options.enabled 显式地设置为 true（默认为 false），有关如何设置配置选项的详细信息，请参阅配置。\n语法 为了不破坏 SQL 的兼容性，我们使用 Oracle 风格的 SQL 提示语法。\ntable_path /*+ OPTIONS(key=val [, key=val]*) */ key: stringLiteral val: stringLiteral 例子 CREATETABLEkafka_table1(idBIGINT,nameSTRING,ageINT)WITH(...);CREATETABLEkafka_table2(idBIGINT,nameSTRING,ageINT)WITH(...);-- override table options in query source selectid,namefromkafka_table1/*+ OPTIONS(\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;earliest-offset\u0026#39;) */;-- override table options in join select*fromkafka_table1/*+ OPTIONS(\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;earliest-offset\u0026#39;) */t1joinkafka_table2/*+ OPTIONS(\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;earliest-offset\u0026#39;) */t2ont1.id=t2.id;-- override table options for INSERT target table insertintokafka_table1/*+ OPTIONS(\u0026#39;sink.partitioner\u0026#39;=\u0026#39;round-robin\u0026#39;) */select*fromkafka_table2;原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/hints.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-sql-hints/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","SQL"],"title":"SQL 提示"},{"categories":["Flink"],"contents":"Table API 表 API 是一个统一的关系型 API，用于流处理和批处理。Table API 查询可以在批处理或流处理输入上运行，无需修改。Table API 是 SQL 语言的超级集，是专门为 Apache Flink 工作而设计的。Table API 是 Scala、Java 和 Python 的语言集成 API。Table API 查询不是像 SQL 那样以字符串值的方式指定查询，而是以语言嵌入的方式在 Java、Scala 或 Python 中定义查询，并支持自动完成和语法验证等 IDE。\nTable API 与 Flink 的 SQL 集成共享许多概念和部分 API。请看通用概念和 API，了解如何注册表或创建表对象。流概念页面讨论了流的具体概念，如动态表和时间属性。\n下面的例子假设一个名为 Orders 的注册表具有属性（a, b, c, rowtime）。rowtime 字段在流式中是一个逻辑时间属性，在批处理中是一个常规的时间戳字段。\n概述和示例 Table API 可用于 Scala、Java 和 Python。Scala Table API 利用的是 Scala 表达式，Java Table API 既支持 Expression DSL，也支持解析并转换为等价表达式的字符串，Python Table API 目前只支持解析并转换为等价表达式的字符串。\n下面的例子显示了 Scala、Java 和 Python Table API 之间的差异。表程序是在批处理环境中执行的。它扫描 Orders 表，按字段 a 分组，并计算每组的结果行。\n通过导入 org.apache.flink.table.api._、org.apache.flink.api.scala._ 和 org.apache.flink.table.api.bridge.scala._（用于桥接到/来自 DataStream）来启用 Scala Table API。\n下面的例子展示了 Scala Table API 程序是如何构造的。表字段使用 Scala 的字符串插值，使用美元字符（$）引用。\nimport org.apache.flink.api.scala._ import org.apache.flink.table.api._ import org.apache.flink.table.api.bridge.scala._ // environment configuration val env = ExecutionEnvironment.getExecutionEnvironment val tEnv = BatchTableEnvironment.create(env) // register Orders table in table environment // ...  // specify table program val orders = tEnv.from(\u0026#34;Orders\u0026#34;) // schema (a, b, c, rowtime)  val result = orders .groupBy($\u0026#34;a\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.count as \u0026#34;cnt\u0026#34;) .toDataSet[Row] // conversion to DataSet  .print() 下一个例子显示了一个更复杂的 Table API 程序。该程序再次扫描 Orders 表，过滤空值，对类型为 String 的字段 a 进行归一化处理，并为每个小时和产品 a 计算平均计费金额 b。它过滤空值，对类型为 String 的字段 a 进行标准化，并为每个小时和产品 a 计算平均账单金额 b。\n// environment configuration // ...  // specify table program val orders: Table = tEnv.from(\u0026#34;Orders\u0026#34;) // schema (a, b, c, rowtime)  val result: Table = orders .filter($\u0026#34;a\u0026#34;.isNotNull \u0026amp;\u0026amp; $\u0026#34;b\u0026#34;.isNotNull \u0026amp;\u0026amp; $\u0026#34;c\u0026#34;.isNotNull) .select($\u0026#34;a\u0026#34;.lowerCase() as \u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;rowtime\u0026#34;) .window(Tumble over 1.hour on $\u0026#34;rowtime\u0026#34; as \u0026#34;hourlyWindow\u0026#34;) .groupBy($\u0026#34;hourlyWindow\u0026#34;, $\u0026#34;a\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;hourlyWindow\u0026#34;.end as \u0026#34;hour\u0026#34;, $\u0026#34;b\u0026#34;.avg as \u0026#34;avgBillingAmount\u0026#34;) 由于表 API 是针对批处理和流数据的统一 API，所以这两个示例程序都可以在批处理和流输入上执行，而不需要对表程序本身进行任何修改。在这两种情况下，考虑到流式记录不会迟到，程序会产生相同的结果（详见流概念）。\n操作 表 API 支持以下操作。请注意，并不是所有的操作都能在批处理和流式处理中使用，它们都有相应的标签。\nScan, Projection 和 Filter  From(Batch/Streaming)  类似于 SQL 查询中的 FROM 子句。执行对注册的表的扫描。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;)  Values(Batch/Streaming)  类似于 SQL 查询中的 VALUES 子句。从提供的行中产生一个内联表。\n你可以使用 row(...) 表达式来创建复合行。\nval table = tEnv.fromValues( row(1, \u0026#34;ABC\u0026#34;), row(2L, \u0026#34;ABCDE\u0026#34;) ) 将产生一个模式(schema)如下的表。\nroot |-- f0: BIGINT NOT NULL // original types INT and BIGINT are generalized to BIGINT |-- f1: VARCHAR(5) NOT NULL // original types CHAR(3) and CHAR(5) are generalized // to VARCHAR(5). VARCHAR is used instead of CHAR so that // no padding is applied 该方法将从输入的表达式中自动得出类型，如果某个位置的类型不同，该方法将尝试为所有类型找到共同的超级类型。如果某个位置的类型不同，方法将尝试为所有类型找到一个共同的超级类型。如果一个共同的超级类型不存在，将抛出一个异常。\n您也可以明确地指定请求的类型。这可能对分配更多的通用类型（如 DECIMAL）或为列命名很有帮助。\nval table = tEnv.fromValues( DataTypes.ROW( DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.DECIMAL(10, 2)), DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()) ), row(1, \u0026#34;ABC\u0026#34;), row(2L, \u0026#34;ABCDE\u0026#34;) ) 将产生一个模式(schema)如下的表。\nroot |-- id: DECIMAL(10, 2) |-- name: STRING  Select(Batch/Streaming)  类似于 SQL SELECT 语句。执行选择操作。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.select($\u0026#34;a\u0026#34;, $\u0026#34;c\u0026#34; as \u0026#34;d\u0026#34;) 你可以使用星号（*）作为通配符，选择表中所有的列。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.select($\u0026#34;*\u0026#34;)  As(Batch/Streaming)  重新命名字段。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;).as(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;, \u0026#34;t\u0026#34;)  Where / Filter(Batch/Streaming)  类似于 SQL WHERE 子句。过滤掉没有通过过滤谓词的记录。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.filter($\u0026#34;a\u0026#34; % 2 === 0) 或:\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.where($\u0026#34;b\u0026#34; === \u0026#34;red\u0026#34;) 列操作  AddColumns(Batch/Streaming)  执行字段添加操作。如果添加的字段已经存在，它将抛出一个异常。\nval orders = tableEnv.from(\u0026#34;Orders\u0026#34;); val result = orders.addColumns(concat($\u0026#34;c\u0026#34;, \u0026#34;Sunny\u0026#34;))  AddOrReplaceColumns(Batch/Streaming)  执行字段添加操作。如果添加的列名与现有的列名相同，那么现有的字段将被替换。此外，如果添加的字段名与现有字段名重复，则使用最后一个字段名。\nval orders = tableEnv.from(\u0026#34;Orders\u0026#34;); val result = orders.addOrReplaceColumns(concat($\u0026#34;c\u0026#34;, \u0026#34;Sunny\u0026#34;) as \u0026#34;desc\u0026#34;)  DropColumns(Batch/Streaming)  执行字段删除操作。字段表达式应该是字段引用表达式，并且只能删除现有的字段。\nval orders = tableEnv.from(\u0026#34;Orders\u0026#34;); val result = orders.dropColumns($\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;)  RenameColumns(Batch/Streaming)  执行字段重命名操作。字段表达式应为别名表达式，且只能对现有字段进行重命名。\nval orders = tableEnv.from(\u0026#34;Orders\u0026#34;); val result = orders.renameColumns($\u0026#34;b\u0026#34; as \u0026#34;b2\u0026#34;, $\u0026#34;c\u0026#34; as \u0026#34;c2\u0026#34;) 聚合(Aggregations)  GroupBy 聚合(Batch/Streaming/Result Updating)  类似于 SQL 的 GROUP BY 子句。将分组键上的行与下面的运行聚合操作符进行分组，以分组方式聚合行。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.groupBy($\u0026#34;a\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum().as(\u0026#34;d\u0026#34;)) 注意：对于流式查询，计算查询结果所需的状态可能会无限增长，这取决于聚合的类型和不同分组键的数量。请提供有效的保留时间间隔的查询配置，以防止状态大小过大。详见查询配置。\n GroupBy 窗口聚合(Batch/Streaming)  在分组窗口和可能的一个或多个分组键上对一个表进行分组和聚合。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result: Table = orders .window(Tumble over 5.minutes on $\u0026#34;rowtime\u0026#34; as \u0026#34;w\u0026#34;) // define window  .groupBy($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;) // group by key and window  .select($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;.start, $\u0026#34;w\u0026#34;.end, $\u0026#34;w\u0026#34;.rowtime, $\u0026#34;b\u0026#34;.sum as \u0026#34;d\u0026#34;) // access window properties and aggregate  Over 窗口聚合(Streaming)  类似于 SQL OVER 子句。根据前后记录的窗口(范围)，为每条记录计算 OVER 窗口汇总。更多细节请参见 over 窗口部分。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result: Table = orders // define window  .window( Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_RANGE following CURRENT_RANGE as \u0026#34;w\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.avg over $\u0026#34;w\u0026#34;, $\u0026#34;b\u0026#34;.max().over($\u0026#34;w\u0026#34;), $\u0026#34;b\u0026#34;.min().over($\u0026#34;w\u0026#34;)) // sliding aggregate 注意：所有的聚合必须定义在同一个窗口上，即相同的分区、排序和范围。目前，只支持对 CURRENT ROW 范围的 PRECEDING（UNBOUNDED 和 bounded）窗口。还不支持带 FOLLOWING 的范围。ORDER BY 必须在单个时间属性上指定。\n Distinct 聚合(Batch Streaming/Result Updating)  类似于 SQL 的 DISTINCT AGGREGATION 子句，如 COUNT(DISTINCT a)。Distinct 聚合声明一个聚合函数（内置的或用户定义的）只应用在不同的输入值上，Distinct 可以应用于 GroupBy 聚合，GroupBy 窗口聚合和 Over 窗口聚合。Distinct 可以应用于 GroupBy 聚合、GroupBy 窗口聚合和 Over 窗口聚合。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;); // Distinct aggregation on group by val groupByDistinctResult = orders .groupBy($\u0026#34;a\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum.distinct as \u0026#34;d\u0026#34;) // Distinct aggregation on time window group by val groupByWindowDistinctResult = orders .window(Tumble over 5.minutes on $\u0026#34;rowtime\u0026#34; as \u0026#34;w\u0026#34;).groupBy($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum.distinct as \u0026#34;d\u0026#34;) // Distinct aggregation on over window val result = orders .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_RANGE as $\u0026#34;w\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.avg.distinct over $\u0026#34;w\u0026#34;, $\u0026#34;b\u0026#34;.max over $\u0026#34;w\u0026#34;, $\u0026#34;b\u0026#34;.min over $\u0026#34;w\u0026#34;) 用户自定义的聚合函数也可以与 DISTINCT 修饰符一起使用。如果只计算不同值的聚合结果，只需在聚合函数中添加 distinct 修饰符即可。\nval orders: Table = tEnv.from(\u0026#34;Orders\u0026#34;); // Use distinct aggregation for user-defined aggregate functions val myUdagg = new MyUdagg(); orders.groupBy($\u0026#34;users\u0026#34;).select($\u0026#34;users\u0026#34;, myUdagg.distinct($\u0026#34;points\u0026#34;) as \u0026#34;myDistinctResult\u0026#34;); 注意：对于流式查询，计算查询结果所需的状态可能会根据不同字段的数量而无限增长。请提供一个有效的保留时间间隔的查询配置，以防止过大的状态大小。详情请看查询配置。\n Distinct(Batch Streaming/Result Updating)  类似于 SQL 的 DISTINCT 子句。返回具有不同值组合的记录。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.distinct() 注意：对于流式查询，计算查询结果所需的状态可能会根据不同字段的数量而无限增长。请提供一个有效的保留时间间隔的查询配置，以防止过大的状态大小。如果启用了状态清洗功能，Distinct 必须发出消息，以防止下游操作者过早地驱逐状态，从而使 Distinct 包含结果更新。详见查询配置。\nJoins  Inner Join(Batch/Streaming)  类似于 SQL JOIN 子句。连接两个表。两个表必须有不同的字段名，并且必须通过 join 操作符或使用 where 或 filter 操作符定义至少一个平等连接谓词。\nval left = ds1.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = ds2.toTable(tableEnv, $\u0026#34;d\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;) val result = left.join(right).where($\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) 注意：对于流式查询，计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供一个具有有效保留时间间隔的查询配置，以防止状态大小过大。详情请看查询配置。\n Outer Join(Batch/Streaming/Result Updating)  类似于 SQL LEFT/RIGHT/FULL OUTER JOIN 子句。连接两个表。两个表必须有不同的字段名，并且必须定义至少一个平等连接谓词。\nval left = tableEnv.fromDataSet(ds1, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = tableEnv.fromDataSet(ds2, $\u0026#34;d\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;) val leftOuterResult = left.leftOuterJoin(right, $\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) val rightOuterResult = left.rightOuterJoin(right, $\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) val fullOuterResult = left.fullOuterJoin(right, $\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) 注意：对于流式查询，计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供一个具有有效保留时间间隔的查询配置，以防止状态大小过大。详情请看查询配置。\n Interval Join(Batch/Streaming)  注：区间连接是常规连接的一个子集，可以用流式处理。\n一个区间连接至少需要一个等价连接谓词和一个连接条件，以限制双方的时间。这样的条件可以由两个合适的范围谓词（\u0026lt;，\u0026lt;=，\u0026gt;=，\u0026gt;）或一个比较两个输入表的相同类型的时间属性（即处理时间或事件时间）的单一平等谓词来定义。\n例如，以下谓词是有效的区间连接条件。\n$\u0026#34;ltime\u0026#34; === $\u0026#34;rtime\u0026#34; $\u0026#34;ltime\u0026#34; \u0026gt;= $\u0026#34;rtime\u0026#34; \u0026amp;\u0026amp; $\u0026#34;ltime\u0026#34; \u0026lt; $\u0026#34;rtime\u0026#34; + 10.minutes val left = ds1.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;, $\u0026#34;ltime\u0026#34;.rowtime) val right = ds2.toTable(tableEnv, $\u0026#34;d\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;, $\u0026#34;rtime\u0026#34;.rowtime) val result = left.join(right) .where($\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34; \u0026amp;\u0026amp; $\u0026#34;ltime\u0026#34; \u0026gt;= $\u0026#34;rtime\u0026#34; - 5.minutes \u0026amp;\u0026amp; $\u0026#34;ltime\u0026#34; \u0026lt; $\u0026#34;rtime\u0026#34; + 10.minutes) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;ltime\u0026#34;)  Inner Join with Table Function (UDTF)(Batch/Streaming)  用表格函数的结果连接一个表格。左表（外表）的每条记录都与相应的表函数调用所产生的所有记录合并。如果左（外）表的表函数调用返回的结果是空的，则放弃该表的某行。\n// instantiate User-Defined Table Function val split: TableFunction[_] = new MySplitUDTF() // join val result: Table = table .joinLateral(split($\u0026#34;c\u0026#34;) as (\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;s\u0026#34;, $\u0026#34;t\u0026#34;, $\u0026#34;v\u0026#34;)  Left Outer Join with Table Function (UDTF)(Batch/Streaming)  用表格函数的结果连接一个表格。左表（外表）的每一行都与相应的表函数调用所产生的所有行相连接。如果表函数调用返回的结果为空，则保留相应的外侧行，并将结果用空值填充。\n注意：目前，表函数左外侧连接的谓词只能是空或字面为真。\n// instantiate User-Defined Table Function val split: TableFunction[_] = new MySplitUDTF() // join val result: Table = table .leftOuterJoinLateral(split($\u0026#34;c\u0026#34;) as (\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;s\u0026#34;, $\u0026#34;t\u0026#34;, $\u0026#34;v\u0026#34;)  Join with Temporal Table(Streaming)  时间表是跟踪其随时间变化的表。\n时间表函数提供了对时间表在特定时间点的状态的访问。用时态表函数连接表的语法与带表函数的内部连接中的语法相同。\n目前只支持与时态表的内联接。\nval ratesHistory = tableEnv.from(\u0026#34;RatesHistory\u0026#34;) // register temporal table function with a time attribute and primary key val rates = ratesHistory.createTemporalTableFunction($\u0026#34;r_proctime\u0026#34;, $\u0026#34;r_currency\u0026#34;) // join with \u0026#34;Orders\u0026#34; based on the time attribute and key val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders .joinLateral(rates($\u0026#34;o_rowtime\u0026#34;), $\u0026#34;r_currency\u0026#34; === $\u0026#34;o_currency\u0026#34;) 更多信息请查看更详细的时间表概念说明。\n集合操作  Union(Batch)  类似于 SQL UNION 子句。将两个表联合起来，去除重复记录，两个表必须有相同的字段类型。\nval left = ds1.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = ds2.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val result = left.union(right)  UnionAll(Batch/Streaming)  类似于 SQL UNION ALL 子句。联合两个表，两个表的字段类型必须相同。\nval left = ds1.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = ds2.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val result = left.unionAll(right)  Intersect(Batch)  类似于 SQL INTERSECT 子句。Intersect 子句返回的是两个表中都存在的记录。如果一条记录在一个表或两个表中存在一次以上，则只返回一次，即结果表没有重复记录。两个表的字段类型必须相同。\nval left = ds1.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = ds2.toTable(tableEnv, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;, $\u0026#34;g\u0026#34;) val result = left.intersect(right)  IntersectAll(Batch)  类似于 SQL 的 INTERSECT ALL 子句。IntersectAll 子句返回两个表中都存在的记录。如果一条记录在两张表中都存在一次以上，那么就会按照它在两张表中存在的次数来返回，也就是说，得到的表可能有重复的记录。两个表的字段类型必须相同。\nval left = ds1.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = ds2.toTable(tableEnv, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;, $\u0026#34;g\u0026#34;) val result = left.intersectAll(right)  Minus(Batch)  类似于 SQL EXCEPT 子句。Minus 返回左表中不存在于右表中的记录。左表中的重复记录只返回一次，即删除重复记录。两个表的字段类型必须相同。\nval left = ds1.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = ds2.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val result = left.minus(right)  MinusAll(Batch)  类似于 SQL EXCEPT ALL 子句。MinusAll 子句返回右表中不存在的记录。一条记录在左表中出现 n 次，在右表中出现 m 次，则返回(n - m)次，即删除右表中存在的重复记录。两个表的字段类型必须相同。\nval left = ds1.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = ds2.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val result = left.minusAll(right)  In(Batch/Streaming)  类似于 SQL 的 IN 子句。如果一个表达式存在于给定的表子查询中，In 子句返回 true。子查询表必须由一列组成。该列必须与表达式具有相同的数据类型。\nval left = ds1.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = ds2.toTable(tableEnv, $\u0026#34;a\u0026#34;) val result = left.select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;).where($\u0026#34;a\u0026#34;.in(right)) 注意：对于流式查询，该操作被重写为加入和分组操作。计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供有效的保留时间间隔的查询配置，以防止状态大小过大。详情请看查询配置。\nOrderBy, Offset 和 Fetch  Order By(Batch)  类似于 SQL ORDER BY 子句。返回所有平行分区的全局排序记录。\nval in = ds.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val result = in.orderBy($\u0026#34;a\u0026#34;.asc)  Offset 和 Fetch(Batch)  类似于 SQL 的 OFFSET 和 FETCH 子句。Offset 和 Fetch 限制了排序结果中返回的记录数量。Offset 和 Fetch 在技术上是 Order By 操作符的一部分，因此必须在它前面。\nval in = ds.toTable(tableEnv, $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) // returns the first 5 records from the sorted result val result1: Table = in.orderBy($\u0026#34;a\u0026#34;.asc).fetch(5) // skips the first 3 records and returns all following records from the sorted result val result2: Table = in.orderBy($\u0026#34;a\u0026#34;.asc).offset(3) // skips the first 10 records and returns the next 5 records from the sorted result val result3: Table = in.orderBy($\u0026#34;a\u0026#34;.asc).offset(10).fetch(5) Insert  Insert Into(Batch/Streaming)  类似于 SQL 查询中的 INSERT INTO 子句，该方法执行插入到一个注册的输出表中。executeInsert() 方法将立即提交一个执行插入操作的 Flink 作业。\n输出表必须在 TableEnvironment 中注册（见连接器表）。此外，注册表的模式必须与查询的模式相匹配。\nval orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) orders.executeInsert(\u0026#34;OutOrders\u0026#34;) Group 窗口 分组窗口根据时间或行数间隔将分组行汇总成有限的组，每组评估一次汇总函数。对于批处理表来说，窗口是按时间间隔对记录进行分组的便捷捷径。\n窗口是使用 window(w: GroupWindow)子句定义的，并且需要一个别名，这个别名是使用 as 子句指定的。为了通过窗口对表进行分组，必须在 groupBy(\u0026hellip;)子句中像常规分组属性一样引用窗口别名。下面的例子展示了如何在表上定义窗口聚合。\nval table = input .window([w: GroupWindow] as $\u0026#34;w\u0026#34;) // define window with alias w  .groupBy($\u0026#34;w\u0026#34;) // group the table by window w  .select($\u0026#34;b\u0026#34;.sum) // aggregate 在流式环境中，只有当窗口聚合除窗口外还对一个或多个属性进行分组时，才能并行计算，即 groupBy(\u0026hellip;)子句引用了一个窗口别名和至少一个附加属性。仅引用窗口别名的 groupBy(\u0026hellip;) 子句（如上面的例子）只能由单个非并行任务来评估。下面的示例显示了如何定义具有附加分组属性的窗口聚合。\nval table = input .window([w: GroupWindow] as $\u0026#34;w\u0026#34;) // define window with alias w  .groupBy($\u0026#34;w\u0026#34;, $\u0026#34;a\u0026#34;) // group the table by attribute a and window w  .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum) // aggregate 窗口属性，如时间窗口的开始、结束或行时间戳，可以在选择语句中作为窗口别名的属性，分别添加为 w.start、w.end 和 w.rowtime。窗口开始时间和行时间时间戳是包含的窗口下界和上界。相反，窗口结束时间戳是专属的上层窗口边界。例如一个从下午 2 点开始的 30 分钟的翻滚窗口，其起始时间戳为 14:00:00.000，行时时间戳为 14:29:59.999，结束时间戳为 14:30:00.000。\nval table = input .window([w: GroupWindow] as $\u0026#34;w\u0026#34;) // define window with alias w  .groupBy($\u0026#34;w\u0026#34;, $\u0026#34;a\u0026#34;) // group the table by attribute a and window w  .select($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;.start, $\u0026#34;w\u0026#34;.end, $\u0026#34;w\u0026#34;.rowtime, $\u0026#34;b\u0026#34;.count) // aggregate and add window start, end, and rowtime timestamps 窗口参数定义了如何将行映射到窗口。Window 不是一个用户可以实现的接口。相反，Table API 提供了一组具有特定语义的预定义 Window 类，它们被翻译成底层的 DataStream 或 DataSet 操作。下面列出了支持的窗口定义。\n滚动窗口 滚动窗口将行分配到固定长度的非重叠的连续窗口。例如，5 分钟的滚动窗口以 5 分钟的间隔将行分组。滚动窗口可以在事件时间、处理时间或行数上定义。\n滚动窗口可以通过使用 Tumble 类来定义，具体如下。\n   方法 描述     over 定义窗口的长度，可以是时间或行数间隔。   on 要对其进行分组（时间间隔）或排序（行数）的时间属性。对于批处理查询，这可能是任何 Long 或 Timestamp 属性。对于流式查询，这必须是一个声明的事件时间或处理时间时间属性。   as 为窗口指定一个别名。该别名用于在下面的 groupBy()子句中引用窗口，并在 select()子句中选择窗口属性，如窗口开始、结束或行时间戳。    // Tumbling Event-time Window .window(Tumble over 10.minutes on $\u0026#34;rowtime\u0026#34; as $\u0026#34;w\u0026#34;) // Tumbling Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble over 10.minutes on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) // Tumbling Row-count Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble over 10.rows on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) Slide (滑动窗口) 滑动窗口有一个固定的尺寸，并按指定的滑动间隔滑动，如果滑动间隔小于窗口尺寸，滑动窗口就会重叠。如果滑动间隔小于窗口大小，滑动窗口就会重叠。因此，行可以分配给多个窗口。例如，一个 15 分钟大小和 5 分钟滑动间隔的滑动窗口将每行分配到 3 个不同的 15 分钟大小的窗口，这些窗口以 5 分钟的间隔进行评估。滑动窗口可以在事件时间、处理时间或行数上定义。\n滑动窗口是通过使用 Slide 类来定义的，具体如下。\n   方法 描述     over 定义窗口的长度，可以是时间或行数间隔。   every 定义滑动间隔，可以是时间间隔或行数间隔。缩放间隔的类型必须与尺寸间隔相同。   on 要对其进行分组（时间间隔）或排序（行数）的时间属性。对于批处理查询，这可能是任何 Long 或 Timestamp 属性。对于流式查询，这必须是一个声明的事件时间或处理时间时间属性。   as 为窗口指定一个别名。该别名用于在下面的 groupBy()子句中引用窗口，并在 select()子句中选择窗口属性，如窗口开始、结束或行时间戳。    // Sliding Event-time Window .window(Slide over 10.minutes every 5.minutes on $\u0026#34;rowtime\u0026#34; as $\u0026#34;w\u0026#34;) // Sliding Processing-time window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide over 10.minutes every 5.minutes on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) // Sliding Row-count window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide over 10.rows every 5.rows on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) Session (会话窗口) 会话窗口没有固定的大小，但它们的界限是由不活动的时间间隔来定义的，也就是说，如果在定义的间隙期没有事件出现，会话窗口就会被关闭。例如，有 30 分钟间隔的会话窗口在 30 分钟不活动后观察到一行时开始（否则该行将被添加到现有的窗口中），如果在 30 分钟内没有行被添加，则关闭。会话窗口可以在事件时间或处理时间工作。\n通过使用 Session 类定义会话窗口，如下所示。\n   方法 描述     withGap 将两个窗口之间的间隔定义为时间间隔。   on 要对其进行分组（时间间隔）或排序（行数）的时间属性。对于批处理查询，这可能是任何 Long 或 Timestamp 属性。对于流式查询，这必须是一个声明的事件时间或处理时间时间属性。   as 为窗口指定一个别名。该别名用于在下面的 groupBy()子句中引用窗口，并在 select()子句中选择窗口属性，如窗口开始、结束或行时间戳。    // Session Event-time Window .window(Session withGap 10.minutes on $\u0026#34;rowtime\u0026#34; as $\u0026#34;w\u0026#34;) // Session Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Session withGap 10.minutes on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) Over 窗口 Over 窗口聚合是从标准 SQL（over 子句）中得知的，并在查询的 SELECT 子句中定义。与组窗口不同的是，组窗口是在 GROUP BY 子句中指定的，over 窗口不折叠行。相反，over 窗口聚合计算的是每条输入行在其相邻行的范围内的聚合。\nOver 窗口是使用 window(w: OverWindow*)子句来定义的(在 Python API 中使用 over_window(*OverWindow))，并且在 select()方法中通过别名来引用。下面的例子展示了如何在表上定义一个 over 窗口聚合。\nval table = input .window([w: OverWindow] as $\u0026#34;w\u0026#34;) // define over window with alias w  .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum over $\u0026#34;w\u0026#34;, $\u0026#34;c\u0026#34;.min over $\u0026#34;w\u0026#34;) // aggregate over the over window w OverWindow 定义了计算汇总的行的范围。OverWindow 不是一个用户可以实现的接口。相反，Table API 提供了 Over 类来配置 over 窗口的属性。Over 窗口可以在事件时间或处理时间上定义，也可以在指定为时间间隔或行数的范围上定义。支持的 over 窗口定义是以 Over（和其他类）上的方法暴露出来的，下面列出了这些方法。\n   方法 Required 描述     partitionBy Optional 定义输入的一个或多个属性的分区。每个分区都被单独排序，聚合函数被分别应用到每个分区。注意：在流环境中，只有当窗口包含 partitionBy 子句时，才能并行计算 over window aggregates。如果没有 partitionBy(\u0026hellip;)，流就会被一个单一的、非并行的任务处理。   orderBy Required 定义每个分区中行的顺序，从而定义聚合函数应用到行的顺序。注意：对于流式查询，这必须是一个声明的事件时间或处理时间时间属性。目前，只支持单个排序属性。   preceding Optional 定义包含在窗口中并在当前行之前的行的间隔。这个间隔可以指定为时间间隔或行数间隔。有边界的窗口用间隔的大小来指定，例如，时间间隔为 10.分钟，行数间隔为 10.行。无边界的窗口用一个常数来指定，例如，时间间隔为 UNBOUNDED_RANGE，行数间隔为 UNBOUNDED_ROW。未绑定的窗口从分区的第一行开始。如果省略了前面的子句，UNBOUNDED_RANGE 和 CURRENT_RANGE 被用作窗口的默认前后。   following Optional 定义包含在窗口中并跟随当前行的行的窗口间隔。这个间隔必须与前一个间隔的单位（时间或行数）相同。目前，不支持在当前行之后添加行的窗口。您可以指定两个常量中的一个。CURRENT_ROW 将窗口的上界设置为当前行。CURRENT_RANGE 将窗口的上界设置为当前行的排序键，也就是说，所有与当前行具有相同排序键的行都包含在窗口中。如果省略下面的子句，时间间隔窗口的上界定义为 CURRENT_RANGE，行数间隔窗口的上界定义为 CURRENT_ROW。   as Required 为 over 窗口指定一个别名。该别名用于在下面的 select()子句中引用 over 窗口。    注意：目前，在同一个 select()调用中，所有的聚合函数都必须在同一个 over 窗口中计算。\nUnbounded Over Windows // Unbounded Event-time over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_RANGE as \u0026#34;w\u0026#34;) // Unbounded Processing-time over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding UNBOUNDED_RANGE as \u0026#34;w\u0026#34;) // Unbounded Event-time Row-count over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_ROW as \u0026#34;w\u0026#34;) // Unbounded Processing-time Row-count over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding UNBOUNDED_ROW as \u0026#34;w\u0026#34;) Bounded Over Windows // Bounded Event-time over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding 1.minutes as \u0026#34;w\u0026#34;) // Bounded Processing-time over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding 1.minutes as \u0026#34;w\u0026#34;) // Bounded Event-time Row-count over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding 10.rows as \u0026#34;w\u0026#34;) // Bounded Processing-time Row-count over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding 10.rows as \u0026#34;w\u0026#34;) 基于行的操作 基于行的操作产生多列的输出。\n Map(Batch/Streaming)  使用用户定义的标量函数或内置的标量函数执行 map 操作。如果输出类型是复合类型，则输出将被扁平化。\nclass MyMapFunction extends ScalarFunction { def eval(a: String): Row = { Row.of(a, \u0026#34;pre-\u0026#34; + a) } override def getResultType(signature: Array[Class[_]]): TypeInformation[_] = Types.ROW(Types.STRING, Types.STRING) } val func = new MyMapFunction() val table = input .map(func($\u0026#34;c\u0026#34;)).as(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)  FlatMap(Batch/Streaming)  用表格函数执行 flatMap 操作。\nclass MyFlatMapFunction extends TableFunction[Row] { def eval(str: String): Unit = { if (str.contains(\u0026#34;#\u0026#34;)) { str.split(\u0026#34;#\u0026#34;).foreach({ s =\u0026gt; val row = new Row(2) row.setField(0, s) row.setField(1, s.length) collect(row) }) } } override def getResultType: TypeInformation[Row] = { Types.ROW(Types.STRING, Types.INT) } } val func = new MyFlatMapFunction val table = input .flatMap(func($\u0026#34;c\u0026#34;)).as(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)  Aggregate(Batch/Streaming/Result Updating)  用一个聚合函数执行一个聚合操作。必须用 select 语句关闭\u0026quot;聚合\u0026quot;，select 语句不支持聚合函数。如果输出类型是复合类型，聚合的输出将被扁平化。\ncase class MyMinMaxAcc(var min: Int, var max: Int) class MyMinMax extends AggregateFunction[Row, MyMinMaxAcc] { def accumulate(acc: MyMinMaxAcc, value: Int): Unit = { if (value \u0026lt; acc.min) { acc.min = value } if (value \u0026gt; acc.max) { acc.max = value } } override def createAccumulator(): MyMinMaxAcc = MyMinMaxAcc(0, 0) def resetAccumulator(acc: MyMinMaxAcc): Unit = { acc.min = 0 acc.max = 0 } override def getValue(acc: MyMinMaxAcc): Row = { Row.of(Integer.valueOf(acc.min), Integer.valueOf(acc.max)) } override def getResultType: TypeInformation[Row] = { new RowTypeInfo(Types.INT, Types.INT) } } val myAggFunc = new MyMinMax val table = input .groupBy($\u0026#34;key\u0026#34;) .aggregate(myAggFunc($\u0026#34;a\u0026#34;) as (\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) .select($\u0026#34;key\u0026#34;, $\u0026#34;x\u0026#34;, $\u0026#34;y\u0026#34;)  Group 窗口聚合(Batch/Streaming)  在分组窗口和可能的一个或多个分组键上对一个表进行分组和聚合。你必须用 select 语句关闭\u0026quot;聚合\u0026quot;。而且选择语句不支持 \u0026ldquo;*\u0026rdquo; 或聚合函数。\nval myAggFunc = new MyMinMax val table = input .window(Tumble over 5.minutes on $\u0026#34;rowtime\u0026#34; as \u0026#34;w\u0026#34;) // define window  .groupBy($\u0026#34;key\u0026#34;, $\u0026#34;w\u0026#34;) // group by key and window  .aggregate(myAggFunc($\u0026#34;a\u0026#34;) as (\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) .select($\u0026#34;key\u0026#34;, $\u0026#34;x\u0026#34;, $\u0026#34;y\u0026#34;, $\u0026#34;w\u0026#34;.start, $\u0026#34;w\u0026#34;.end) // access window properties and aggregate results  FlatAggregate(Streaming/Result Updating)  类似于 GroupBy 聚合。将分组键上的行与下面的运行表聚合运算符进行分组，将行进行分组。与 AggregateFunction 的不同之处在于，TableAggregateFunction 可以为一个组返回 0 条或多条记录。你必须用 select 语句关闭 \u0026ldquo;flatAggregate\u0026rdquo;。而 select 语句不支持聚合函数。\n不使用 emitValue 输出结果，还可以使用 emitUpdateWithRetract 方法。与 emitValue 不同的是，emitUpdateWithRetract 用于输出已经更新的值。这个方法以回缩模式增量输出数据，也就是说，一旦有更新，我们必须在发送新的更新记录之前回缩旧的记录。如果在表聚合函数中定义了 emitUpdateWithRetract 方法，那么 emitUpdateWithRetract 方法将优先于 emitValue 方法使用，因为该方法被视为比 emitValue 更有效，因为它可以增量输出值。详见表聚合函数。\nimport java.lang.{Integer =\u0026gt; JInteger} import org.apache.flink.table.api.Types import org.apache.flink.table.functions.TableAggregateFunction /** * Accumulator for top2. */ class Top2Accum { var first: JInteger = _ var second: JInteger = _ } /** * The top2 user-defined table aggregate function. */ class Top2 extends TableAggregateFunction[JTuple2[JInteger, JInteger], Top2Accum] { override def createAccumulator(): Top2Accum = { val acc = new Top2Accum acc.first = Int.MinValue acc.second = Int.MinValue acc } def accumulate(acc: Top2Accum, v: Int) { if (v \u0026gt; acc.first) { acc.second = acc.first acc.first = v } else if (v \u0026gt; acc.second) { acc.second = v } } def merge(acc: Top2Accum, its: JIterable[Top2Accum]): Unit = { val iter = its.iterator() while (iter.hasNext) { val top2 = iter.next() accumulate(acc, top2.first) accumulate(acc, top2.second) } } def emitValue(acc: Top2Accum, out: Collector[JTuple2[JInteger, JInteger]]): Unit = { // emit the value and rank  if (acc.first != Int.MinValue) { out.collect(JTuple2.of(acc.first, 1)) } if (acc.second != Int.MinValue) { out.collect(JTuple2.of(acc.second, 2)) } } } val top2 = new Top2 val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders .groupBy($\u0026#34;key\u0026#34;) .flatAggregate(top2($\u0026#34;a\u0026#34;) as ($\u0026#34;v\u0026#34;, $\u0026#34;rank\u0026#34;)) .select($\u0026#34;key\u0026#34;, $\u0026#34;v\u0026#34;, $\u0026#34;rank\u0026#34;) 注意：对于流式查询，计算查询结果所需的状态可能会无限增长，这取决于聚合的类型和不同分组键的数量。请提供具有有效保留时间间隔的查询配置，以防止状态大小过大。详情请参见查询配置。\n Group Window FlatAggregate(Streaming)  在分组窗口和可能一个或多个分组键上对一个表进行分组和聚合。你必须用 select 语句关闭 \u0026ldquo;flatAggregate\u0026rdquo;。而 select 语句不支持聚合函数。\nval top2 = new Top2 val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders .window(Tumble over 5.minutes on $\u0026#34;rowtime\u0026#34; as \u0026#34;w\u0026#34;) // define window  .groupBy($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;) // group by key and window  .flatAggregate(top2($\u0026#34;b\u0026#34;) as ($\u0026#34;v\u0026#34;, $\u0026#34;rank\u0026#34;)) .select($\u0026#34;a\u0026#34;, w.start, $\u0026#34;w\u0026#34;.end, $\u0026#34;w\u0026#34;.rowtime, $\u0026#34;v\u0026#34;, $\u0026#34;rank\u0026#34;) // access window properties and aggregate results 数据类型 请看关于数据类型的专门页面。\n通用类型和(嵌套的)复合类型(例如 POJOs、tuple、行、Scala case 类)也可以是行的字段。\n具有任意嵌套的复合类型的字段可以用值访问函数来访问。\n通用类型被视为一个黑盒，可以通过用户定义的函数进行传递或处理。\n表达式语法 前面几节中的一些操作符都期望有一个或多个表达式。表达式可以使用内嵌的 Scala DSL 或作为字符串来指定。请参考上面的例子来了解如何指定表达式。\n这是表达式的 EBNF 语法。\nexpressionList = expression , { \u0026quot;,\u0026quot; , expression } ; expression = overConstant | alias ; alias = logic | ( logic , \u0026quot;as\u0026quot; , fieldReference ) | ( logic , \u0026quot;as\u0026quot; , \u0026quot;(\u0026quot; , fieldReference , { \u0026quot;,\u0026quot; , fieldReference } , \u0026quot;)\u0026quot; ) ; logic = comparison , [ ( \u0026quot;\u0026amp;\u0026amp;\u0026quot; | \u0026quot;||\u0026quot; ) , comparison ] ; comparison = term , [ ( \u0026quot;=\u0026quot; | \u0026quot;==\u0026quot; | \u0026quot;===\u0026quot; | \u0026quot;!=\u0026quot; | \u0026quot;!==\u0026quot; | \u0026quot;\u0026gt;\u0026quot; | \u0026quot;\u0026gt;=\u0026quot; | \u0026quot;\u0026lt;\u0026quot; | \u0026quot;\u0026lt;=\u0026quot; ) , term ] ; term = product , [ ( \u0026quot;+\u0026quot; | \u0026quot;-\u0026quot; ) , product ] ; product = unary , [ ( \u0026quot;*\u0026quot; | \u0026quot;/\u0026quot; | \u0026quot;%\u0026quot;) , unary ] ; unary = [ \u0026quot;!\u0026quot; | \u0026quot;-\u0026quot; | \u0026quot;+\u0026quot; ] , composite ; composite = over | suffixed | nullLiteral | prefixed | atom ; suffixed = interval | suffixAs | suffixCast | suffixIf | suffixDistinct | suffixFunctionCall | timeIndicator ; prefixed = prefixAs | prefixCast | prefixIf | prefixDistinct | prefixFunctionCall ; interval = timeInterval | rowInterval ; timeInterval = composite , \u0026quot;.\u0026quot; , (\u0026quot;year\u0026quot; | \u0026quot;years\u0026quot; | \u0026quot;quarter\u0026quot; | \u0026quot;quarters\u0026quot; | \u0026quot;month\u0026quot; | \u0026quot;months\u0026quot; | \u0026quot;week\u0026quot; | \u0026quot;weeks\u0026quot; | \u0026quot;day\u0026quot; | \u0026quot;days\u0026quot; | \u0026quot;hour\u0026quot; | \u0026quot;hours\u0026quot; | \u0026quot;minute\u0026quot; | \u0026quot;minutes\u0026quot; | \u0026quot;second\u0026quot; | \u0026quot;seconds\u0026quot; | \u0026quot;milli\u0026quot; | \u0026quot;millis\u0026quot;) ; rowInterval = composite , \u0026quot;.\u0026quot; , \u0026quot;rows\u0026quot; ; suffixCast = composite , \u0026quot;.cast(\u0026quot; , dataType , \u0026quot;)\u0026quot; ; prefixCast = \u0026quot;cast(\u0026quot; , expression , dataType , \u0026quot;)\u0026quot; ; dataType = \u0026quot;BYTE\u0026quot; | \u0026quot;SHORT\u0026quot; | \u0026quot;INT\u0026quot; | \u0026quot;LONG\u0026quot; | \u0026quot;FLOAT\u0026quot; | \u0026quot;DOUBLE\u0026quot; | \u0026quot;BOOLEAN\u0026quot; | \u0026quot;STRING\u0026quot; | \u0026quot;DECIMAL\u0026quot; | \u0026quot;SQL_DATE\u0026quot; | \u0026quot;SQL_TIME\u0026quot; | \u0026quot;SQL_TIMESTAMP\u0026quot; | \u0026quot;INTERVAL_MONTHS\u0026quot; | \u0026quot;INTERVAL_MILLIS\u0026quot; | ( \u0026quot;MAP\u0026quot; , \u0026quot;(\u0026quot; , dataType , \u0026quot;,\u0026quot; , dataType , \u0026quot;)\u0026quot; ) | ( \u0026quot;PRIMITIVE_ARRAY\u0026quot; , \u0026quot;(\u0026quot; , dataType , \u0026quot;)\u0026quot; ) | ( \u0026quot;OBJECT_ARRAY\u0026quot; , \u0026quot;(\u0026quot; , dataType , \u0026quot;)\u0026quot; ) ; suffixAs = composite , \u0026quot;.as(\u0026quot; , fieldReference , \u0026quot;)\u0026quot; ; prefixAs = \u0026quot;as(\u0026quot; , expression, fieldReference , \u0026quot;)\u0026quot; ; suffixIf = composite , \u0026quot;.?(\u0026quot; , expression , \u0026quot;,\u0026quot; , expression , \u0026quot;)\u0026quot; ; prefixIf = \u0026quot;?(\u0026quot; , expression , \u0026quot;,\u0026quot; , expression , \u0026quot;,\u0026quot; , expression , \u0026quot;)\u0026quot; ; suffixDistinct = composite , \u0026quot;distinct.()\u0026quot; ; prefixDistinct = functionIdentifier , \u0026quot;.distinct\u0026quot; , [ \u0026quot;(\u0026quot; , [ expression , { \u0026quot;,\u0026quot; , expression } ] , \u0026quot;)\u0026quot; ] ; suffixFunctionCall = composite , \u0026quot;.\u0026quot; , functionIdentifier , [ \u0026quot;(\u0026quot; , [ expression , { \u0026quot;,\u0026quot; , expression } ] , \u0026quot;)\u0026quot; ] ; prefixFunctionCall = functionIdentifier , [ \u0026quot;(\u0026quot; , [ expression , { \u0026quot;,\u0026quot; , expression } ] , \u0026quot;)\u0026quot; ] ; atom = ( \u0026quot;(\u0026quot; , expression , \u0026quot;)\u0026quot; ) | literal | fieldReference ; fieldReference = \u0026quot;*\u0026quot; | identifier ; nullLiteral = \u0026quot;nullOf(\u0026quot; , dataType , \u0026quot;)\u0026quot; ; timeIntervalUnit = \u0026quot;YEAR\u0026quot; | \u0026quot;YEAR_TO_MONTH\u0026quot; | \u0026quot;MONTH\u0026quot; | \u0026quot;QUARTER\u0026quot; | \u0026quot;WEEK\u0026quot; | \u0026quot;DAY\u0026quot; | \u0026quot;DAY_TO_HOUR\u0026quot; | \u0026quot;DAY_TO_MINUTE\u0026quot; | \u0026quot;DAY_TO_SECOND\u0026quot; | \u0026quot;HOUR\u0026quot; | \u0026quot;HOUR_TO_MINUTE\u0026quot; | \u0026quot;HOUR_TO_SECOND\u0026quot; | \u0026quot;MINUTE\u0026quot; | \u0026quot;MINUTE_TO_SECOND\u0026quot; | \u0026quot;SECOND\u0026quot; ; timePointUnit = \u0026quot;YEAR\u0026quot; | \u0026quot;MONTH\u0026quot; | \u0026quot;DAY\u0026quot; | \u0026quot;HOUR\u0026quot; | \u0026quot;MINUTE\u0026quot; | \u0026quot;SECOND\u0026quot; | \u0026quot;QUARTER\u0026quot; | \u0026quot;WEEK\u0026quot; | \u0026quot;MILLISECOND\u0026quot; | \u0026quot;MICROSECOND\u0026quot; ; over = composite , \u0026quot;over\u0026quot; , fieldReference ; overConstant = \u0026quot;current_row\u0026quot; | \u0026quot;current_range\u0026quot; | \u0026quot;unbounded_row\u0026quot; | \u0026quot;unbounded_row\u0026quot; ; timeIndicator = fieldReference , \u0026quot;.\u0026quot; , ( \u0026quot;proctime\u0026quot; | \u0026quot;rowtime\u0026quot; ) ; 字符。在这里，literal 是一个有效的 Java 字元。字符串的字元可以使用单引号或双引号来指定。复制引号进行转义（例如 \u0026lsquo;It\u0026rsquo;s me.\u0026rsquo; 或 \u0026ldquo;I \u0026ldquo;\u0026ldquo;like \u0026ldquo;dog.\u0026quot;）。\n空符。空符必须有一个类型。使用 nullOf(type)(例如 nullOf(INT))来创建一个空值。\n字段引用。fieldReference 指定数据中的一列（如果使用 *，则指定所有列），functionIdentifier 指定一个支持的标量函数。列名和函数名遵循 Java 标识符语法。\n函数调用。作为字符串指定的表达式也可以使用前缀符号代替后缀符号来调用运算符和函数。\n小数。如果需要处理精确的数值或大的小数，Table API 也支持 Java 的 BigDecimal 类型。在 Scala Table API 中，小数可以通过 BigDecimal(\u0026ldquo;123456\u0026rdquo;)来定义，而在 Java 中，可以通过附加一个 \u0026ldquo;p\u0026rdquo; 来表示精确，例如 123456p。\n时间表示法。为了处理时间值，Table API 支持 Java SQL 的 Date, Time 和 Timestamp 类型。在 Scala Table API 中，可以通过使用 java.sql.Date.valueOf(\u0026ldquo;2016-06-27\u0026rdquo;)、java.sql.Time.valueOf(\u0026ldquo;10:10:42\u0026rdquo;) 或 java.sql.Timestamp.valueOf(\u0026ldquo;2016-06-27 10:10:42.123\u0026rdquo;) 来定义字符。Java 和 Scala Table API 还支持调用 \u0026ldquo;2016-06-27\u0026rdquo;.toDate()、\u0026ldquo;10:10:42\u0026rdquo;.toTime() 和 \u0026ldquo;2016-06-27 10:10:42.123\u0026rdquo;.toTimestamp() 来将 Strings 转换为时间类型。注意：由于 Java 的时态 SQL 类型依赖于时区，请确保 Flink 客户端和所有的 TaskManagers 使用相同的时区。\n时间间隔。时间间隔可以用月数（Types.INTERVAL_MONTHS）或毫秒数（Types.INTERVAL_MILLIS）表示。同一类型的时间间隔可以加减(例如：1.小时+10.分钟)。可以将毫秒的时间间隔加到时间点上（如 \u0026ldquo;2016-08-10\u0026rdquo;.toDate + 5.days）。\nScala 表达式。Scala 表达式使用隐式转换。因此，确保在你的程序中添加通配符 import org.apache.flink.table.api._。如果一个字词没有被当作表达式，可以使用.toExpr 如 3.toExpr 来强制转换一个字词。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-table-api/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"Table API"},{"categories":["Flink"],"contents":"Table API 和 SQL Apache Flink 具有两个关系型 API - Table API 和 SQL - 用于统一的流和批处理。Table API 是 Scala 和 Java 的语言集成查询 API，它允许用非常直观的方式从关系运算符（如选择、过滤和连接）组成查询。Flink 的 SQL 支持是基于 Apache Calcite，它实现了 SQL 标准。无论输入是批处理输入（DataSet）还是流输入（DataStream），在任一接口中指定的查询都具有相同的语义，并指定相同的结果。\n表 API 和 SQL 接口与 Flink 的 DataStream 和 DataSet API 紧密集成。你可以很容易地在所有 API 和建立在 API 基础上的库之间切换。例如，您可以使用 CEP 库 从 DataStream 中提取模式，随后使用 Table API 来分析模式，或者您可能会在预处理数据上运行 Gelly 图算法之前，使用 SQL 查询扫描、过滤和聚合一个批处理表。\n请注意，Table API 和 SQL 的功能还不完善，正在积极开发中。并非所有的操作都被 [Table API, SQL] 和 [stream, batch] 输入的每个组合所支持。\n依赖结构 从 Flink 1.9 开始，Flink 为评估 Table \u0026amp; SQL API 程序提供了两种不同的规划器实现：Blink planner 和 Flink 1.9 之前的旧 planner。Planner 负责将关系运算符转化为可执行的、优化的 Flink 作业。这两种 planner 都有不同的优化规则和运行时类。它们在支持的功能集上也可能有所不同。\n注意: 对于生产用例，我们推荐 blink planner，它从 1.11 开始成为默认 planner。\n所有的 Table API 和 SQL 组件都捆绑在 flink-table 或 flink-table-blink Maven 构件中。\n以下是与大多数项目相关的依赖关系。\n flink-table-common: 一个通用模块，用于通过自定义函数、格式等扩展表生态系统。 flink-table-api-java: 使用 Java 编程语言的纯表程序的 Table \u0026amp; SQL API（处于早期开发阶段，不推荐！）。 flink-table-api-scala: Table 和 SQL API，用于使用 Java 编程语言的纯表程序（处于早期开发阶段，不推荐）。 flink-table-api-java-bridge: 使用 Java 编程语言支持 DataStream/DataSet API 的 Table \u0026amp; SQL API。 flink-table-api-scala-bridge: 使用 Scala 编程语言，支持 DataStream/DataSet API 的表和 SQL API。 flink-table-planner: 表程序 planner 和运行时。这是在 1.9 版本之前 Flink 唯一的 planner。从 Flink 1.11 开始，不再推荐使用它。 flink-table-planner-link: 新的 Blink 计划器，从 Flink 1.11 开始成为默认的。 flink-table-runtim-blink: 新的 Blink 运行时。 flink-table-uber: 将上面的 API 模块加上旧的规划器打包成一个适用于大多数 Table \u0026amp; SQL API 使用案例的发行版。uber JAR 文件 flink-table-*.jar 默认位于 Flink 版本的 /lib 目录下。 flink-table-uber-blink: 将上面的 API 模块加上 Blink 的特定模块打包成一个适用于大多数 Table \u0026amp; SQL API 用例的发行版。uber JAR 文件 flink-table-blink-*.jar 默认位于 Flink 版本的 /lib 目录下。  关于如何在表程序中切换新旧 Blink planner，请参见通用 API 页面。\n表程序依赖 根据目标编程语言的不同，您需要将 Java 或 Scala API 添加到项目中，以便使用 Table API 和 SQL 来定义管道。\n\u0026lt;!-- Either... --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- or... --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-scala-bridge_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 此外，如果你想在 IDE 中本地运行 Table API 和 SQL 程序，你必须添加以下一组模块，这取决于你想使用的计划器。\n\u0026lt;!-- Either... (for the old planner that was available before Flink 1.9) --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-planner_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- or.. (for the new Blink planner) --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-planner-blink_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 在内部，表生态系统的部分内容是在 Scala 中实现的。因此，请确保为批处理和流应用添加以下依赖关系。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-streaming-scala_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 扩展依赖性 如果你想实现与 Kafka 交互的自定义格式或一组用户定义的函数，下面的依赖就足够了，可以用于 SQL 客户端的 JAR 文件。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-common\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 目前，该模块包括以下扩展点：\n SerializationSchemaFactory DeserializationSchemaFactory ScalarFunction TableFunction AggregateFunction  下一步怎么走？  概念与通用 API: Table API 和 SQL 的共享概念和 API。 数据类型: 列出了预先定义的数据类型及其属性。 流概念: 表 API 或 SQL 的流特定文档，如时间属性的配置和更新结果的处理。 连接到外部系统: 可用的连接器和格式，用于向外部系统读写数据。 Table API。支持的操作和表 API 的 API。 SQL。支持 SQL 的操作和语法。 内置函数: 表 API 和 SQL 中支持的函数。 SQL 客户端: 玩转 Flink SQL，并向集群提交表格程序，无需编程知识。  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-table-api-and-sql/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"Table API 和 SQL"},{"categories":["Flink"],"contents":"USE 语句 USE 语句用于设置当前数据库或目录。\n运行 USE 语句 USE 语句可以通过 TableEnvironment 的 executeSql() 方法执行，也可以在 SQL CLI 中执行。executeSql() 方法会对一个成功的 USE 操作返回 \u0026lsquo;OK\u0026rsquo;， 否则会抛出一个异常。\n下面的例子展示了如何在 TableEnvironment 和 SQL CLI 中运行一条 USE 语句。\nval env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // create a catalog tEnv.executeSql(\u0026#34;CREATE CATALOG cat1 WITH (...)\u0026#34;) tEnv.executeSql(\u0026#34;SHOW CATALOGS\u0026#34;).print() // +-----------------+ // | catalog name | // +-----------------+ // | default_catalog | // | cat1 | // +-----------------+  // change default catalog tEnv.executeSql(\u0026#34;USE CATALOG cat1\u0026#34;) tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print() // databases are empty // +---------------+ // | database name | // +---------------+ // +---------------+  // create a database tEnv.executeSql(\u0026#34;CREATE DATABASE db1 WITH (...)\u0026#34;) tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print() // +---------------+ // | database name | // +---------------+ // | db1 | // +---------------+  // change default database tEnv.executeSql(\u0026#34;USE db1\u0026#34;) USE CATLOAG USECATALOGcatalog_name设置当前目录。所有没有明确指定目录的后续命令将使用这个目录。如果所提供的目录不存在，则会抛出一个异常。默认的当前目录是default_catalog。\nUSE USE[catalog_name.]database_name设置当前数据库。所有没有明确指定数据库的后续命令将使用这个数据库。如果提供的数据库不存在，则会抛出一个异常。默认的当前数据库是default_database。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/use.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-use-statements/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","SQL"],"title":"Use 语句"},{"categories":["Flink"],"contents":"时间表 时间表代表了对变化表的（参数化）视图的概念，该视图返回一个表在特定时间点的内容。\n变化表可以是跟踪变化的变化历史表（如数据库变化日志），也可以是将变化具体化的变化维度表（如数据库表）。\n对于变化的历史表，Flink 可以跟踪变化，并允许在查询中的某个时间点访问表的内容。在 Flink 中，这种表用 Temporal Table Function 来表示。\n对于变化的维度表，Flink 允许在查询内的处理时间点访问表的内容。在 Flink 中，这种表是由一个 Temporal Table 来表示的。\n动机 与不断变化的历史表相关联 假设我们有以下表格 RatesHistory。\nSELECT*FROMRatesHistory;rowtimecurrencyrate=====================09:00USDollar10209:00Euro11409:00Yen110:45Euro11611:15Euro11911:49Pounds108RatesHistory 代表了一个不断增长的对日元（汇率为 1）的货币汇率附加表。例如，从 09:00 到 10:45，欧元对日元的汇率是 114，从 10:45 到 11:15 是 116。从 10:45 到 11:15 是 116。\n考虑到我们希望输出 10:58 时的所有当前汇率，我们将需要以下 SQL 查询来计算结果表。\nSELECT*FROMRatesHistoryASrWHEREr.rowtime=(SELECTMAX(rowtime)FROMRatesHistoryASr2WHEREr2.currency=r.currencyANDr2.rowtime\u0026lt;=TIME\u0026#39;10:58\u0026#39;);相关子查询确定相应货币的最大时间低于或等于期望时间。外层查询列出具有最大时间戳的汇率。\n下表显示了这种计算的结果。在我们的例子中，10:45 的欧元更新被考虑在内，然而，11:15 的欧元更新和新输入的英镑在 10:58 的表格中没有被考虑。\nrowtime currency rate ======= ======== ====== 09:00 US Dollar 102 09:00 Yen 1 10:45 Euro 116 Temporal Tables 的概念旨在简化此类查询，加快其执行速度，并减少 Flink 的状态使用。Temporal Table 是一个关于 append-only 表的参数化视图，它将 append-only 表的行解释为表的 changelog，并提供该表在特定时间点的版本。将 append-only 表解释为变更日志需要指定一个主键属性和一个时间戳属性。主键决定哪些行会被覆盖，时间戳决定行的有效时间。\n在上面的例子中，currency 是 RatesHistory 表的主键，rowtime 是时间戳属性。\n在 Flink 中，这是由一个 Temporal Table Function 来表示的。\n与变化的维度表相关联 另一方面，有些用例需要加入一个不断变化的维度表，而这个表是一个外部数据库表。\n让我们假设 LatestRates 是一张表（例如存储在），它是以最新的速率来物化的。LatestRates 是物化的历史 RatesHistory。那么 LatestRates 表在时间 10:58 时的内容将是。\n10:58\u0026gt;SELECT*FROMLatestRates;currencyrate==============USDollar102Yen1Euro11612:00 时 LatestRates 表的内容将是：\n12:00\u0026gt;SELECT*FROMLatestRates;currencyrate==============USDollar102Yen1Euro119Pounds108在 Flink 中，这用一个时间表来表示。\n时间表函数 为了访问时态表中的数据，必须传递一个时间属性，该属性决定了将返回的表的版本。Flink 使用表函数的 SQL 语法来提供一种表达方式。\n一旦定义好，一个时态表函数就会接受一个单一的时间参数 timeAttribute，并返回一组行。这个集合包含了所有现有主键相对于给定时间属性的最新版本的行。\n假设我们基于 RatesHistory 表定义了一个时态表函数 Rates(timeAttribute)，我们可以用下面的方式查询这样的函数。\nSELECT*FROMRates(\u0026#39;10:15\u0026#39;);rowtimecurrencyrate=====================09:00USDollar10209:00Euro11409:00Yen1SELECT*FROMRates(\u0026#39;11:00\u0026#39;);rowtimecurrencyrate=====================09:00USDollar10210:45Euro11609:00Yen1每次查询 Rates(timeAttribute) 都会返回给定时间属性的 Rates 的状态。\n注意：目前，Flink 不支持直接查询带有恒定时间属性参数的时态表函数。目前，时态表函数只能用于连接。上面的例子是用来提供对函数 Rates(timeAttribute) 返回内容的直观认识。\n关于如何使用时态表进行联接的更多信息，还请参见关于连续查询的连接页面。\n定义时态表函数 下面的代码片段说明了如何从一个仅有追加的表创建一个时态表函数。\n// Get the stream and table environments. val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = StreamTableEnvironment.create(env) // Provide a static data set of the rates history table. val ratesHistoryData = new mutable.MutableList[(String, Long)] ratesHistoryData.+=((\u0026#34;US Dollar\u0026#34;, 102L)) ratesHistoryData.+=((\u0026#34;Euro\u0026#34;, 114L)) ratesHistoryData.+=((\u0026#34;Yen\u0026#34;, 1L)) ratesHistoryData.+=((\u0026#34;Euro\u0026#34;, 116L)) ratesHistoryData.+=((\u0026#34;Euro\u0026#34;, 119L)) // Create and register an example table using above data set. // In the real setup, you should replace this with your own table. val ratesHistory = env .fromCollection(ratesHistoryData) .toTable(tEnv, \u0026#39;r_currency, \u0026#39;r_rate, \u0026#39;r_proctime.proctime) tEnv.createTemporaryView(\u0026#34;RatesHistory\u0026#34;, ratesHistory) // Create and register TemporalTableFunction. // Define \u0026#34;r_proctime\u0026#34; as the time attribute and \u0026#34;r_currency\u0026#34; as the primary key. val rates = ratesHistory.createTemporalTableFunction($\u0026#34;r_proctime\u0026#34;, $\u0026#34;r_currency\u0026#34;) // \u0026lt;==== (1) tEnv.registerFunction(\u0026#34;Rates\u0026#34;, rates) // \u0026lt;==== (2) 第(1)行创建了一个 Rates 时态表函数，这使得我们可以使用 Table API 中的函数 Rates。\n第(2)行在我们的表环境中以 Rates 的名义注册这个函数，这使得我们可以在 SQL 中使用 Rates 函数。\n时态表 注意: 这只在 Blink planner 中支持。\n为了访问时间表中的数据，目前必须定义一个 LookupableTableSource 的 TableSource。Flink 使用 SQL:2011 中提出的 SQL 语法 FOR SYSTEM_TIME AS OF 来查询时间表。\n假设我们定义了一个名为 LatestRates 的时态表，我们可以用下面的方式查询这样的表。\nSELECT*FROMLatestRatesFORSYSTEM_TIMEASOFTIME\u0026#39;10:15\u0026#39;;currencyrate==============USDollar102Euro114Yen1SELECT*FROMLatestRatesFORSYSTEM_TIMEASOFTIME\u0026#39;11:00\u0026#39;;currencyrate==============USDollar102Euro116Yen1注意：目前，Flink 不支持直接查询时间恒定的时态表。目前，时态表只能用在 join 中。上面的例子是用来提供一个直观的时间表 LatestRates 返回的内容。\n更多关于如何使用时态表进行连接的信息，请参见关于连续查询的连接页面。\n定义时态表 // Get the stream and table environments. val env = StreamExecutionEnvironment.getExecutionEnvironment val settings = EnvironmentSettings.newInstance().build() val tEnv = StreamTableEnvironment.create(env, settings) // or val tEnv = TableEnvironment.create(settings)  // Define an HBase table with DDL, then we can use it as a temporal table in sql // Column \u0026#39;currency\u0026#39; is the rowKey in HBase table tEnv.executeSql( s\u0026#34;\u0026#34;\u0026#34; |CREATE TABLE LatestRates ( | currency STRING, | fam1 ROW\u0026lt;rate DOUBLE\u0026gt; |) WITH ( | \u0026#39;connector\u0026#39; = \u0026#39;hbase-1.4\u0026#39;, | \u0026#39;table-name\u0026#39; = \u0026#39;Rates\u0026#39;, | \u0026#39;zookeeper.quorum\u0026#39; = \u0026#39;localhost:2181\u0026#39; |) |\u0026#34;\u0026#34;\u0026#34;.stripMargin) 也请参见如何定义 LookupableTableSource 的页面。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-temporal-tables/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"临时表"},{"categories":["Flink"],"contents":"Side Output 除了 DataStream 操作产生的主流(main stream)外，还可以产生任意数量的附加侧输出结果流。结果流中的数据类型不必与主流中的数据类型相匹配，不同侧输出的类型也可以不同。当您要分割数据流时，这种操作非常有用，通常您必须复制数据流，然后从每个数据流中过滤掉您不想要的数据。\n在使用侧输出时，首先需要定义一个 OutputTag，用来识别侧输出流。\nval outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) 请注意 OutputTag 是如何根据侧输出流所包含的元素类型进行类型化的。\n可以通过以下函数向侧输出发送数据。\n ProcessFunction KeyedProcessFunction CoProcessFunction KeyedCoProcessFunction ProcessWindowFunction ProcessAllWindowFunction  你可以使用 Context 参数（在上面的函数中暴露给用户）向一个由 OutputTag 标识的侧输出发送数据。下面是一个从 ProcessFunction 中发射侧输出数据的例子。\nval input: DataStream[Int] = ... val outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) val mainDataStream = input .process(new ProcessFunction[Int, Int] { override def processElement( value: Int, ctx: ProcessFunction[Int, Int]#Context, out: Collector[Int]): Unit = { // emit data to regular output  out.collect(value) // emit data to side output  ctx.output(outputTag, \u0026#34;sideout-\u0026#34; + String.valueOf(value)) } }) 为了检索侧输出流，你可以在 DataStream 操作的结果上使用 getSideOutput(OutputTag)。这将给你一个 DataStream，它的类型是侧输出流的结果。\nval outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) val mainDataStream = ... val sideOutputStream: DataStream[String] = mainDataStream.getSideOutput(outputTag) 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/side_output.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/","tags":["Flink","Flink 官方文档","DataStream API","Side Outputs"],"title":"侧输出"},{"categories":["Flink"],"contents":"函数 Flink Table API \u0026amp; SQL 使用户能够通过函数进行数据转换。\n函数的类型 Flink 中的函数有两个维度来分类。\n一个维度是系统（或内置）函数 v.s. 目录函数。系统函数没有命名空间，可以只用名字来引用。目录函数属于目录和数据库，因此它们有目录和数据库的命名空间，它们可以用完全/部分限定名（catalog.db.func 或 db.func）或者只用函数名来引用。\n另一个维度是临时函数 v.s. 持久化函数。临时函数是不稳定的，只存在于一个会话的生命周期内，它们总是由用户创建的。而持久性函数则是在会话的生命周期内存在的，它们要么是由系统提供的，要么是在目录中持久存在的。\n这两个维度给 Flink 用户提供了4类函数。\n 临时系统函数 系统函数 临时目录函数 目录函数  引用函数 在 Flink 中，用户有两种引用函数的方式 - 精确引用函数或模棱两可的引用函数。\n精确的函数引用 精确的函数引用使用户能够专门使用目录函数，并且跨目录和跨数据库，例如从 mytable 中选择 mycatalog.mydb.myfunc(x)，从 mytable 中选择 mydb.myfunc(x)。\n这只从 Flink 1.10 开始支持。\n模棱两可的函数引用 在模棱两可的函数引用中，用户只需在 SQL 查询中指定函数名称即可，例如：select myfunc(x) from mytable。\n函数解析顺序 只有当有不同类型但名称相同的函数时，解析顺序才是重要的，比如有三个函数都名为 \u0026ldquo;myfunc\u0026rdquo;，但分别是临时目录、目录和系统函数。如果没有函数名冲突，则函数将被解析为唯一的一个。\n精确的函数引用 因为系统函数没有命名空间，所以 Flink 中的精确函数引用必须指向临时目录函数或目录函数。\n其解析顺序是：\n 临时目录函数 目录函数  含糊不清的函数参考 解析顺序是:\n 临时系统函数 系统函数 临时目录函数，在当前目录和当前数据库中的会话。 目录函数，在当前目录和当前数据库中的会话。  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-functions/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","Function"],"title":"函数"},{"categories":["Flink"],"contents":"动态表 SQL 和关系代数在设计时并没有考虑到流数据。因此，关系代数（和 SQL）和流处理之间几乎没有概念上的差距。\n本页讨论了这些差异，并解释了 Flink 如何在无界数据上实现与常规数据库引擎在有界数据上相同的语义。\n数据流的关系查询 下表比较了传统的关系代数和流处理在输入数据、执行和输出结果方面的情况。\n   关系代数/SQL 流处理     关系（或表）是有界（多）元组的集合。 流是一个无限的元组序列。   在批数据上执行的查询（如关系数据库中的表）可以访问完整的输入数据。 流式查询在启动时不能访问所有的数据，必须\u0026quot;等待\u0026quot;数据流进来。   批量查询在产生一个固定大小的结果后就会终止。 流式查询根据接收到的记录不断地更新其结果，并且永远不会完成。    尽管存在这些差异，但用关系查询和 SQL 处理流并不是不可能的。先进的关系数据库系统提供了一种叫做物化视图的功能。物化视图被定义为一个 SQL 查询，就像一个普通的虚拟视图一样。与虚拟视图不同，物化视图会缓存查询的结果，这样在访问视图时就不需要对查询进行评估。缓存的一个常见挑战是防止缓存提供过时的结果。当其定义查询的基表被修改时，一个物化视图就会过时。急切的视图维护是一种技术，它可以在更新基表时立即更新一个物化视图。\n如果我们考虑以下几点，急切的视图维护和流上的 SQL 查询之间的联系就会变得很明显。\n 数据库表是一个 INSERT、UPDATE 和 DELETE DML 语句流的结果，通常称为 changelog 流。 物化视图被定义为一个 SQL 查询。为了更新视图，查询不断处理视图的基础关系的 changelog 流。 物化视图是流式 SQL 查询的结果。  考虑到这些要点，我们在下一节介绍以下动态表的概念。\n动态表与连续查询 动态表是 Flink 的表 API 和 SQL 支持流数据的核心概念。与代表批处理数据的静态表相比，动态表是随时间变化的。它们可以像静态批处理表一样被查询。查询动态表会产生一个连续查询。一个连续查询永远不会终止，并产生一个动态表作为结果。查询不断地更新它的（动态）结果表，以反映其（动态）输入表的变化。本质上，对动态表的连续查询与定义物化视图的查询非常相似。\n需要注意的是，连续查询的结果在语义上总是等同于在输入表的快照上以批处理模式执行相同查询的结果。\n下图直观地展示了流、动态表和连续查询的关系。\n 一个流被转换为一个动态表。 对动态表进行连续查询，得到一个新的动态表。 产生的动态表又被转换回流。  注意：动态表首先是一个逻辑概念。动态表在查询执行过程中不一定（完全）实体化。\n在下文中，我们将解释动态表和连续查询的概念，其点击事件流的模式如下。\n[ user: VARCHAR, // the name of the user cTime: TIMESTAMP, // the time when the URL was accessed url: VARCHAR // the URL that was accessed by the user ] 在流上定义一个表 为了用关系查询来处理一个流，必须把它转换成一个表。从概念上讲，流的每一条记录都被解释为对生成的表进行 INSERT 修改。从本质上讲，我们是从一个仅有 INSERT 的 changelog 流建立一个表。\n下图直观地展示了点击事件流（左手边）是如何转换为表（右手边）的。随着更多的点击流记录被插入，生成的表在不断增长。\n注意：一个定义在流上的表在内部是不被实现的。\n连续查询 连续查询是在动态表上进行评估，并生成一个新的动态表作为结果。与批处理查询不同，连续查询永远不会终止，并根据输入表的更新更新其结果表。在任何时间点上，连续查询的结果在语义上等同于在输入表的快照上以批处理模式执行相同查询的结果。\n在下面我们展示了在点击事件流上定义的点击表上的两个查询示例。\n第一个查询是一个简单的 GROUP-BY COUNT 聚合查询。它对用户字段的点击表进行分组，并统计访问的 URL 数量。下图显示了当点击表更新了更多的行时，查询是如何随着时间的推移进行评估的。\n查询开始时，点击表（左侧）为空。当第一条记录被插入到 clicks 表中时，查询开始计算结果表。插入第一行 [Mary, ./home] 后，结果表（右侧，顶部）由一条行 [Mary, 1] 组成。当第二条记录 [Bob, ./cart] 插入点击表后，查询更新结果表，插入一条新的记录 [Bob, 1]。第三条记录 [Mary, ./prod?id=1] 产生对已经计算好的结果行的更新，这样 [Mary, 1] 就更新为 [Mary, 2]。最后，查询将第三条记录 [Liz，1] 插入到结果表中，这时第四条记录被追加到点击表中。\n第二个查询与第一个查询类似，但将点击表除了用户属性也分组在一个小时滚动窗口上，然后再统计 URL 的数量（窗口等基于时间的计算是基于特殊的时间属性，后面会讨论）。同样，图中显示了不同时间点的输入和输出，以直观地显示动态表的变化性质。\n和以前一样，输入表的点击率在左边显示。查询每隔一小时持续计算结果并更新结果表。clicks 表包含四条记录，时间戳（cTime）在 12:00:00 和 12:59:59 之间。查询从这个输入中计算出两条结果行（每个用户一条），并将它们追加到结果表中。对于 13:00:00 和 13:59:59 之间的下一个窗口，点击表包含三条记录，结果是另外两条记录被追加到结果表中。随着时间的推移，更多的行被追加到点击表中，结果表会被更新。\n更新和追加查询 虽然这两个例子查询看起来很相似（都是计算一个分组计数合计），但它们在一个重要方面有所不同。\n 第一个查询更新了之前发出的结果，即定义结果表的 changelog 流包含了 INSERT 和 UPDATE 变化。 第二个查询只对结果表进行追加，即结果表的 changelog 流只包含 insert 更改。  查询产生的是只追加表还是更新表有一定的影响。\n 产生更新变化的查询通常要维护更多的状态（见下节）。 将仅有附录的表转换为流与更新表的转换是不同的（参见表到流的转换部分）。  查询限制 许多（但不是全部）语义有效的查询可以作为流上的连续查询来评估。有些查询的计算成本太高，要么是由于它们需要维护的状态大小，要么是由于计算更新太贵。\n 状态大小。连续查询是在无边界的流上进行评估的，通常应该运行数周或数月。因此，一个连续查询处理的数据总量可能非常大。必须更新之前发出的结果的查询需要维护所有发出的行，以便能够更新它们。例如，第一个示例查询需要存储每个用户的 URL 计数，以便能够增加计数，并在输入表收到新行时发出新结果。如果只跟踪注册用户，需要维护的计数数量可能不会太高。但是，如果非注册用户被分配了一个唯一的用户名，那么需要维护的次数会随着时间的推移而增加，最终可能会导致查询失败。  SELECTuser,COUNT(url)FROMclicksGROUPBYuser; 计算更新。有些查询需要重新计算和更新大部分发出的结果行，即使只增加或更新一条输入记录。显然，这种查询并不适合作为连续查询来执行。一个例子是下面的查询，它根据最后一次点击的时间为每个用户计算一个 rank。只要点击表收到一条新的记录，该用户的 lastAction 就会被更新，必须计算新的 rank。但是由于两行不能有相同的 rank，所以所有排名较低的行也需要更新。  SELECTuser,RANK()OVER(ORDERBYlastLogin)FROM(SELECTuser,MAX(cTime)ASlastActionFROMclicksGROUPBYuser);查询配置页面讨论了控制连续查询执行的参数。有些参数可用于以维护状态的大小来换取结果的准确性。\n表到流的转换 动态表可以像普通的数据库表一样，通过 INSERT、UPDATE 和 DELETE 的修改不断地进行修改。它可能是一张只有一行的表，不断地更新，也可能是一张只有插入的表，没有 UPDATE 和 DELETE 的修改，或者是介于两者之间的任何表。\n当把动态表转换为流或写入外部系统时，需要对这些变化进行编码。Flink 的表 API 和 SQL 支持三种方式来编码动态表的变化。\n  只添加流。一个只被 INSERT 修改的动态表，可以通过发出插入的行来转换成流。\n  撤回流。缩回流是指有两种消息的流，即添加消息和缩回消息。通过将 INSERT 变更编码为添加消息，将 DELETE 变更编码为回撤消息，将 UPDATE 变更编码为更新（上一条）行的回撤消息和更新（新一条）行的添加消息，就可以将一张动态表转换为回撤流。下图直观地展示了动态表转换为回撤流的过程。\n   Upsert 流。Upsert 流是一个有两种消息类型的流，即 upsert 消息和删除消息。一个动态表被转换为 upsert 流需要一个（可能是复合的）唯一键。通过将 INSERT 和 UPDATE 更改编码为 upsert 消息，将 DELETE 更改编码为 delete 消息，将具有唯一键的动态表转换为流。消耗流的操作者需要知道唯一键属性，以便正确应用消息。与 retract 流的主要区别在于 update 变更用一条消息进行编码，因此效率更高。下图直观地展示了动态表转换为 update 流的过程。  将动态表转换为 DataStream 的 API 在通用概念页面上讨论。请注意，在将动态表转换为 DataStream 时，只支持追加和收回流。在 TableSources 和 TableSinks 页面上讨论了将动态表发射到外部系统的 TableSink 接口。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/dynamic_tables.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-dynamic-tables/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"动态表"},{"categories":["Flink"],"contents":"注意：可查询状态的客户端 API 目前处于不断发展的状态，对所提供接口的稳定性不做保证。在即将到来的 Flink 版本中，客户端的 API 很可能会有突破性的变化。\n简而言之，这个功能将 Flink 的 managed keyed (partitioned) state（参见 Working with State）暴露给外界，并允许用户从 Flink 外部查询作业的状态。对于某些场景来说，可查询状态消除了与外部系统（如键值存储）进行分布式操作/交易的需求，而这往往是实践中的瓶颈。此外，该功能对于调试目的可能特别有用。\n注意事项: 当查询一个状态对象时，该对象是在没有任何同步或复制的情况下从一个并发线程访问的。这是一个设计上的选择，因为上述任何一种情况都会导致作业延迟的增加，这是我们想要避免的。因为任何使用 Java 堆空间的状态后端，如 MemoryStateBackend 或 FsStateBackend，在检索值时都不会使用副本，而是直接引用存储的值，所以读-修改-写模式是不安全的，可能会导致可查询状态服务器因并发修改而失败。RocksDBStateBackend 则可以避免这些问题。\n架构 在展示如何使用可查询状态之前，先简单介绍一下构成它的实体。Queryable State 功能由三个主要实体组成。\n QueryableStateClient，它（可能）运行在 Flink 集群之外，并提交用户查询。 QueryableStateClientProxy，它运行在每个 TaskManager 上（即 Flink 集群内部），负责接收客户端的查询，代表他从负责的 TaskManager 中获取所请求的状态，并将其返回给客户端，以及 QueryableStateServer，它运行在每个 TaskManager 上，负责为本地存储的状态提供服务。  客户端连接到其中一个代理，并发送一个与特定键 k 相关联的状态的请求。正如在使用状态中所述，keyed state 被组织在键组(Key Groups)中，每个 TaskManager 都被分配了一些这样的键组(Key Groups)。为了发现哪个 TaskManager 负责持有 k 的键组，代理将询问 JobManager。根据答案，代理将查询运行在该 TaskManager 上的 QueryableStateServer，以获取与 k 相关联的状态，并将响应转发到客户端。\n激活可查询状态 要在 Flink 集群上启用可查询状态，你需要做以下工作。\n 将 flink-queryable-state-runtime_2.11-1.11.0.jar 从 Flink 发行版的 opt/ 文件夹中复制到 lib/ 文件夹中。 设置属性 queryable-state.enable 为 true。请参阅配置文档了解详情和附加参数。  要验证您的群集是否在启用可查询状态后运行，请检查任何 TaskManager 的日志中的行。\u0026ldquo;Started the Queryable State Proxy Server @ \u0026hellip;\u0026quot;。\n使状态可查询 现在你已经在集群上激活了可查询状态，现在是时候看看如何使用它了。为了使一个状态对外界可见，它需要通过使用以下方式明确地成为可查询状态。\n QueryableStateStream, 一个方便的对象，它作为一个接收器(sink)，并把它的传入值作为可查询的状态提供，或者是 stateDescriptor.setQueryable(String queryableStateName) 方法，使得状态描述符所代表的 keyed state，可以查询。  下面的章节将解释这两种方法的使用。\n可查询的状态流 在 KeyedStream 上调用 .asQueryableState(stateName, stateDescriptor) 会返回一个 QueryableStateStream，它将其值作为可查询状态提供。根据状态的类型，asQueryableState() 方法有以下几种变体。\n// ValueState QueryableStateStream asQueryableState( String queryableStateName, ValueStateDescriptor stateDescriptor) // Shortcut for explicit ValueStateDescriptor variant QueryableStateStream asQueryableState(String queryableStateName) // FoldingState QueryableStateStream asQueryableState( String queryableStateName, FoldingStateDescriptor stateDescriptor) // ReducingState QueryableStateStream asQueryableState( String queryableStateName, ReducingStateDescriptor stateDescriptor) 注意：没有可查询的 ListState 接收器，因为这会导致一个不断增长的列表，可能无法清理，因此最终会消耗过多的内存。\n返回的 QueryableStateStream 可以被看作是一个接收器(sink)，不能被进一步转换。在内部，一个 QueryableStateStream 会被翻译成一个操作符，它使用所有传入的记录来更新可查询状态实例。更新逻辑是由 asQueryableState 调用中提供的 StateDescriptor 的类型暗示的。在像下面这样的程序中，keyed stream 的所有记录将通过 ValueState.update(value) 来更新状态实例:\nstream.keyBy(0).asQueryableState(\u0026#34;query-name\u0026#34;) 这就像 Scala API 的 flatMapWithState 一样。\n管理的 Keyed State 通过 StateDescriptor.setQueryable(String queryableStateName) 使相应的状态描述符成为可查询的状态，可以使操作符的托管键控状态(参见使用 Managed Keyed State))成为可查询的状态，如下面的例子。\nValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, // the state name  TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {})); // type information  descriptor.setQueryable(\u0026#34;query-name\u0026#34;); // queryable state name 注意：queryableStateName 参数可以任意选择，并且只用于查询。它不一定要与状态本身的名称相同。\n这个变体对于哪种类型的状态可以被查询没有限制。这意味着它可以用于任何 ValueState、ReduceState、ListState、MapState、AggregatingState 以及目前已被废弃的 FoldingState。\n查询状态 到目前为止，你已经设置了你的集群以可查询的状态运行，并且你已经将你的（部分）状态声明为可查询。现在是时候看看如何查询这个状态了。\n为此，你可以使用 QueryableStateClient 辅助类。它可以在 flink-queryable-state-client jar 中找到，它必须和 flink-core 一起被显式地包含在项目的 pom.xml 中作为依赖，如下所示。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-queryable-state-client-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 更多的内容，可以查看如何设置 Flink 程序。\nQueryableStateClient 会将你的查询提交给内部代理，然后代理会处理你的查询并返回最终结果。初始化客户端的唯一要求是提供一个有效的 TaskManager 主机名（记住每个 TaskManager 上都有一个可查询状态代理运行）和代理监听的端口。更多关于如何配置代理和状态服务器端口的信息请参见配置部分。\nQueryableStateClient client = new QueryableStateClient(tmHostname, proxyPort) 客户端准备好后，要查询一个类型为 V 的状态，与类型为 K 的键相关联，可以使用该方法。\nCompletableFuture\u0026lt;S\u0026gt; getKvState( JobID jobId, String queryableStateName, K key, TypeInformation\u0026lt;K\u0026gt; keyTypeInfo, StateDescriptor\u0026lt;S, V\u0026gt; stateDescriptor) 以上返回一个 CompletableFuture，最终持有 ID 为 jobID 的作业的 queryableStateName 所标识的可查询状态实例的状态值。key 是你对其状态感兴趣的键，keyTypeInfo 将告诉 Flink 如何序列化/解序列化它。最后，stateDescriptor 包含了关于所请求的状态的必要信息，即它的类型（Value、Reduce 等）和如何序列化/解序列化它的必要信息。\n细心的读者会注意到，返回的 future 包含一个 S 类型的值，即一个包含实际值的 State 对象。这可以是 Flink 支持的任何一种状态类型。ValueState，ReduceState，ListState，MapState，AggregatingState，以及目前已经废弃的 FoldingState。\n注意：这些状态对象不允许对包含的状态进行修改。您可以使用它们来获取状态的实际值，例如使用 valueState.get()，或者迭代包含的 \u0026lt;K，V\u0026gt; 条目，例如使用 mapState.entry()，但您不能修改它们。举个例子，在返回的列表状态上调用 add() 方法会抛出一个 UnsupportedOperationException。\n注意：客户端是异步的，可以被多个线程共享。在未使用时需要通过 QueryableStateClient.shutdown() 来关闭它，以释放资源。\n例子 下面的例子扩展了 CountWindowAverage 的例子(请看使用 Managed Keyed State)，使其可查询，并展示了如何查询这个值。\npublic class CountWindowAverage extends RichFlatMapFunction\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { private transient ValueState\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; sum; // a tuple containing the count and the sum  @Override public void flatMap(Tuple2\u0026lt;Long, Long\u0026gt; input, Collector\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; out) throws Exception { Tuple2\u0026lt;Long, Long\u0026gt; currentSum = sum.value(); currentSum.f0 += 1; currentSum.f1 += input.f1; sum.update(currentSum); if (currentSum.f0 \u0026gt;= 2) { out.collect(new Tuple2\u0026lt;\u0026gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); } } @Override public void open(Configuration config) { ValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, // the state name  TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {})); // type information  descriptor.setQueryable(\u0026#34;query-name\u0026#34;); sum = getRuntimeContext().getState(descriptor); } } Once used in a job, you can retrieve the job ID and then query any key’s current state from this operator: QueryableStateClient client = new QueryableStateClient(tmHostname, proxyPort); // the state descriptor of the state to be fetched. ValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {})); CompletableFuture\u0026lt;ValueState\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; resultFuture = client.getKvState(jobId, \u0026#34;query-name\u0026#34;, key, BasicTypeInfo.LONG_TYPE_INFO, descriptor); // now handle the returned value resultFuture.thenAccept(response -\u0026gt; { try { Tuple2\u0026lt;Long, Long\u0026gt; res = response.get(); } catch (Exception e) { e.printStackTrace(); } }); 配置 以下配置参数会影响可查询状态服务器和客户端的行为，它们被定义在 QueryableStateOptions 中。\n状态服务器  queryable-state.server.ports：可查询状态服务器的服务器端口范围。如果在同一台机器上运行多个 task manager，这对避免端口冲突很有用。指定的范围可以是：一个端口: \u0026ldquo;9123\u0026rdquo;，一个端口范围: \u0026ldquo;50100-50200\u0026rdquo;，或者一个范围和或点的列表: \u0026ldquo;50100-50200,50300-50400,51234\u0026rdquo;。默认端口为 9067。 queryable-state.server.network-threads: 接收状态服务器传入请求的网络（事件循环）线程数（0 =\u0026gt; #slots）。 queryable-state.server.query-threads: 为状态服务器处理/服务传入请求的线程数（0 =\u0026gt; #slots）。  代理  queryable-state.proxy.ports：可查询状态代理服务器的端口范围。如果在同一台机器上运行多个 task manager，这对避免端口冲突很有用。指定的范围可以是：一个端口: \u0026ldquo;9123\u0026rdquo;，一个端口范围: \u0026ldquo;50100-50200\u0026rdquo;，或者一个范围和或点的列表: \u0026ldquo;50100-50200,50300-50400,51234\u0026rdquo;。默认端口为 9069。 queryable-state.proxy.network-threads：为客户端代理接收传入请求的网络（事件循环）线程数（0 =\u0026gt; #slots）。 queryable-state.proxy.query-threads：为客户端代理处理/服务传入请求的线程数（0 =\u0026gt; #slots）。  限制条件  可查询状态的生命周期与任务的生命周期绑定，例如，任务在启动时注册可查询状态，在处置时取消注册。在未来的版本中，我们希望将其解耦，以便在任务完成后允许查询，并通过状态复制加快恢复速度。 关于可用 KvState 的通知是通过一个简单的告诉发生的。将来应该改进这个功能，使其更加强大，包括询问和确认。 服务器和客户端会跟踪查询的统计数据。目前默认情况下，这些数据是被禁用的，因为它们不会暴露在任何地方。一旦有更好的支持通过 Metrics 系统发布这些数字，我们应该启用统计。  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/queryable_state.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"可查询状态"},{"categories":["Flink"],"contents":"处理应用程序参数 几乎所有的 Flink 应用，包括批处理和流式应用，都依赖于外部配置参数，它们用于指定输入和输出源（如路径或地址）、系统参数（并行性、运行时配置）和应用特定参数（通常在用户函数中使用）。它们用于指定输入和输出源（如路径或地址）、系统参数（并行性、运行时配置）和应用程序特定参数（通常在用户函数中使用）。\nFlink 提供了一个名为 ParameterTool 的简单工具，为解决这些问题提供一些基本的工具。请注意，你不一定要使用这里描述的 ParameterTool。其他框架如 Commons CLI和argparse4j 也能很好地与 Flink 一起工作。\n将你的配置值导入 ParameterTool 之中\nParameterTool 提供了一组预定义的静态方法来读取配置。该工具内部期待的是一个 Map\u0026lt;String，String\u0026gt;，所以很容易将其与自己的配置风格整合在一起。\n从 .properties 文件中\n下面的方法将读取一个属性文件并提供键/值对。\nString propertiesFilePath = \u0026#34;/home/sam/flink/myjob.properties\u0026#34;; ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFilePath); File propertiesFile = new File(propertiesFilePath); ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFile); InputStream propertiesFileInputStream = new FileInputStream(file); ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFileInputStream); 从命令行参数来看\n这就允许从命令行中获取 --input hdfs://mydata --elements 42 这样的参数。\npublic static void main(String[] args) { ParameterTool parameter = ParameterTool.fromArgs(args); // .. regular code .. 从系统属性\n当启动 JVM 时，你可以将系统属性传递给它。-Dinput=hdfs://mydata。你也可以从这些系统属性中初始化 ParameterTool。\nParameterTool parameter = ParameterTool.fromSystemProperties(); 在 Flink 程序中使用参数\n现在我们已经从某个地方得到了参数（见上文），我们可以以各种方式使用它们。\n直接从 ParameterTool 中使用\nParameterTool 本身有访问值的方法。\nParameterTool parameters = // ... parameter.getRequired(\u0026#34;input\u0026#34;); parameter.get(\u0026#34;output\u0026#34;, \u0026#34;myDefaultValue\u0026#34;); parameter.getLong(\u0026#34;expectedCount\u0026#34;, -1L); parameter.getNumberOfParameters() // .. there are more methods available. 你可以在客户端提交应用程序的 main() 方法中直接使用这些方法的返回值。例如，你可以这样设置一个操作符的并行性。\nParameterTool parameters = ParameterTool.fromArgs(args); int parallelism = parameters.get(\u0026#34;mapParallelism\u0026#34;, 2); DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; counts = text.flatMap(new Tokenizer()).setParallelism(parallelism); 由于 ParameterTool 是可序列化的，所以你可以把它传递给函数本身。\nParameterTool parameters = ParameterTool.fromArgs(args); DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; counts = text.flatMap(new Tokenizer(parameters)); 然后在函数内部使用它从命令行获取值。\n全局注册参数\n在 ExecutionConfig 中注册为全局作业参数的参数可以作为配置值从 JobManager Web 界面和用户定义的所有功能中访问。\n全局注册参数。\nParameterTool parameters = ParameterTool.fromArgs(args); // set up the execution environment final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(parameters); 在任何丰富的用户功能中访问它们。\npublic static final class Tokenizer extends RichFlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String value, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { ParameterTool parameters = (ParameterTool) getRuntimeContext().getExecutionConfig().getGlobalJobParameters(); parameters.getRequired(\u0026#34;input\u0026#34;); // .. do more .. 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/application_parameters.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-handling-application-parameters/","tags":["Flink","Flink 官方文档","DataStream API","Parameters"],"title":"处理应用程序参数"},{"categories":["Flink"],"contents":"实验特性 本节介绍 DataStream API 中的实验性功能。实验性功能仍在不断发展，可能是不稳定的、不完整的，或者在未来的版本中会有很大的变化。\n将预先分割的数据流重新解释为 keyed 流 我们可以将一个预分区的数据流重新解释为一个 keyed 流，以避免洗牌。\n警告：重新解释的数据流必须已经被预分区了，其方式与 Flink 的 keyBy 在洗牌中对数据的分区方式完全相同，即键组分配。\n一个用例是两个作业之间的物化洗牌：第一个作业执行 keyBy 洗牌，并将每个输出物化为一个分区。第二个作业有源，对于每个并行实例，从第一个作业创建的相应分区中读取。现在可以将这些源重新解释为 keyed 流，例如应用窗口化。请注意，这个技巧使得第二个作业的并行性很尴尬，这对细粒度的恢复方案很有帮助。\n这个重新解释的功能是通过 DataStreamUtils 暴露的。\nstatic \u0026lt;T, K\u0026gt; KeyedStream\u0026lt;T, K\u0026gt; reinterpretAsKeyedStream( DataStream\u0026lt;T\u0026gt; stream, KeySelector\u0026lt;T, K\u0026gt; keySelector, TypeInformation\u0026lt;K\u0026gt; typeInfo) 给定一个基流(base stream)、一个键选择器和类型信息，该方法从基流创建一个 keyed 流。\n代码示例:\nval env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1) val source = ... new DataStreamUtils(source).reinterpretAsKeyedStream((in) =\u0026gt; in) .timeWindow(Time.seconds(1)) .reduce((a, b) =\u0026gt; a + b) .addSink(new DiscardingSink[Int]) env.execute() 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/experimental.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-experimental-features/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"实验特性"},{"categories":["Flink"],"contents":"在本节中，您将了解如何在实践中使用广播状态。请参考 Stateful Stream Processing 来了解有状态流处理背后的概念。\n提供的 API 为了展示所提供的 API，我们将在介绍它们的全部功能之前先举一个例子。作为我们的运行示例，我们将使用这样的情况：我们有一个不同颜色和形状的对象流，我们希望找到相同颜色的对象对，并遵循特定的模式，例如，一个矩形和一个三角形。我们假设有趣的模式集会随着时间的推移而演变。\n在这个例子中，第一个流将包含具有 Color 和 Shape 属性的 Item 类型的元素。另一个流将包含 Rules。\n从 Items 流开始，我们只需要按 Color keyBy，因为我们想要相同颜色的对。这将确保相同颜色的元素最终会出现在同一个物理机上。\n// key the items by color KeyedStream\u0026lt;Item, Color\u0026gt; colorPartitionedStream = itemStream .keyBy(new KeySelector\u0026lt;Item, Color\u0026gt;(){...}); 继续讨论规则，包含规则的流应该被广播到所有下游任务，这些任务应该将它们存储在本地，以便它们可以根据所有传入的项目评估它们。下面的代码段将i)广播规则流，ii)使用提供的 MapStateDescriptor，它将创建规则将被存储的广播状态。\n// a map descriptor to store the name of the rule (string) and the rule itself. MapStateDescriptor\u0026lt;String, Rule\u0026gt; ruleStateDescriptor = new MapStateDescriptor\u0026lt;\u0026gt;( \u0026#34;RulesBroadcastState\u0026#34;, BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint\u0026lt;Rule\u0026gt;() {})); // broadcast the rules and create the broadcast state BroadcastStream\u0026lt;Rule\u0026gt; ruleBroadcastStream = ruleStream .broadcast(ruleStateDescriptor); 最后，为了根据从 Item 流传入的元素来评估 Rules，我们需要。\n 连接(connect)两个流，并且 指定我们的匹配检测逻辑。  将一个流（keyed or non-keyed）与 BroadcastStream 连接起来，可以通过在非广播流上调用 connect() 来完成，并将 BroadcastStream 作为一个参数。这将返回一个 BroadcastConnectedStream，我们可以在这个 Stream 上调用一个特殊类型的 CoProcessFunction 来处理。该函数将包含我们的匹配逻辑。该函数的具体类型取决于非广播流的类型。\n 如果它是 keyed，那么这个函数就是 KeyedBroadcastProcessFunction。 如果是 non-keyed,，那么该函数就是一个 BroadcastProcessFunction。  鉴于我们的非广播流是 keyed 的，下面的代码段包含了上述调用。\n注意： 连接(connect)应该被调用在非广播流上， 以 BroadcastStream 作为参数。\nDataStream\u0026lt;String\u0026gt; output = colorPartitionedStream .connect(ruleBroadcastStream) .process( // type arguments in our KeyedBroadcastProcessFunction represent:  // 1. the key of the keyed stream  // 2. the type of elements in the non-broadcast side  // 3. the type of elements in the broadcast side  // 4. the type of the result, here a string  new KeyedBroadcastProcessFunction\u0026lt;Color, Item, Rule, String\u0026gt;() { // my matching logic  } ); BroadcastProcessFunction 和 KeyedBroadcastProcessFunction 与 CoProcessFunction 一样，这些函数有两个处理方法要实现；processBroadcastElement() 负责处理广播流中的传入元素，processElement() 用于处理非广播流。这些方法的完整签名如下:\npublic abstract class BroadcastProcessFunction\u0026lt;IN1, IN2, OUT\u0026gt; extends BaseBroadcastProcessFunction { public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; public abstract void processBroadcastElement(IN2 value, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; } public abstract class KeyedBroadcastProcessFunction\u0026lt;KS, IN1, IN2, OUT\u0026gt; { public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; public abstract void processBroadcastElement(IN2 value, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; public void onTimer(long timestamp, OnTimerContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; } 首先需要注意的是，这两个函数在处理广播端元素时都需要实现 processBroadcastElement() 方法，在处理非广播端元素时需要实现 processElement() 方法。\n这两个方法在提供的上下文中有所不同。非广播侧有一个 ReadOnlyContext，而广播侧有一个 Context。\n这两个上下文（以下枚举中的 ctx）:\n 提供对广播状态的访问：ctx.getBroadcastState(MapStateDescriptor\u0026lt;K, V\u0026gt; stateDescriptor)。 允许查询元素的时间戳：ctx.timestamp()。 获取当前水印：ctx.currentWatermark()。 获取当前处理时间：ctx.currentProcessingTime()，以及 将元素发射到侧输出：ctx.output(OutputTag\u0026lt;X\u0026gt; outputTag, X value)。  getBroadcastState() 中的 stateDescriptor 应该和上面的 .broadcast(ruleStateDescriptor) 中的 stateDescriptor 是一样的。\n区别在于各自对广播状态的访问类型。广播端对其有读写访问权，而非广播端则只有读的访问权（因此才有这些名字）。原因是在 Flink 中，不存在跨任务通信。所以，为了保证广播状态中的内容在我们操作符的所有并行实例中都是相同的，我们只给广播侧读写访问权，而广播侧在所有任务中看到的元素都是相同的，并且我们要求该侧每个传入元素的计算在所有任务中都是相同的。忽略这个规则会打破状态的一致性保证，导致结果不一致，而且往往难以调试。\n注意 processBroadcastElement() 中实现的逻辑必须在所有并行实例中具有相同的确定性行为!\n最后，由于 KeyedBroadcastProcessFunction 是在 keyed stream 上运行的，它暴露了一些 BroadcastProcessFunction 无法实现的功能。那就是\n processElement() 方法中的 ReadOnlyContext 允许访问 Flink 的底层定时器服务，它允许注册事件和/或处理时间定时器。当一个定时器发射时， onTimer() (如上所示)被调用一个 OnTimerContext，它暴露了与 ReadOnlyContext 相同的功能，再加上   能够询问发射的定时器是事件还是处理时间, 和 来查询与定时器相关联的键。  processBroadcastElement() 方法中的 Context 包含 applyToKeyedState(StateDescriptor\u0026lt;S, VS\u0026gt; stateDescriptor, KeyedStateFunction\u0026lt;KS, S\u0026gt; function) 方法。这允许注册一个 KeyedStateFunction，以应用于与提供的 stateDescriptor 相关联的所有键的所有状态。  注意。注册定时器只能在 KeyedBroadcastProcessFunction 的 processElement() 处进行，而且只能在那里进行。在 processBroadcastElement() 方法中是不可能的，因为没有键与广播元素相关联。\n回到我们原来的例子，我们的 KeyedBroadcastProcessFunction 可以是如下的样子。\nnew KeyedBroadcastProcessFunction\u0026lt;Color, Item, Rule, String\u0026gt;() { // store partial matches, i.e. first elements of the pair waiting for their second element  // we keep a list as we may have many first elements waiting  private final MapStateDescriptor\u0026lt;String, List\u0026lt;Item\u0026gt;\u0026gt; mapStateDesc = new MapStateDescriptor\u0026lt;\u0026gt;( \u0026#34;items\u0026#34;, BasicTypeInfo.STRING_TYPE_INFO, new ListTypeInfo\u0026lt;\u0026gt;(Item.class)); // identical to our ruleStateDescriptor above  private final MapStateDescriptor\u0026lt;String, Rule\u0026gt; ruleStateDescriptor = new MapStateDescriptor\u0026lt;\u0026gt;( \u0026#34;RulesBroadcastState\u0026#34;, BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint\u0026lt;Rule\u0026gt;() {})); @Override public void processBroadcastElement(Rule value, Context ctx, Collector\u0026lt;String\u0026gt; out) throws Exception { ctx.getBroadcastState(ruleStateDescriptor).put(value.name, value); } @Override public void processElement(Item value, ReadOnlyContext ctx, Collector\u0026lt;String\u0026gt; out) throws Exception { final MapState\u0026lt;String, List\u0026lt;Item\u0026gt;\u0026gt; state = getRuntimeContext().getMapState(mapStateDesc); final Shape shape = value.getShape(); for (Map.Entry\u0026lt;String, Rule\u0026gt; entry : ctx.getBroadcastState(ruleStateDescriptor).immutableEntries()) { final String ruleName = entry.getKey(); final Rule rule = entry.getValue(); List\u0026lt;Item\u0026gt; stored = state.get(ruleName); if (stored == null) { stored = new ArrayList\u0026lt;\u0026gt;(); } if (shape == rule.second \u0026amp;\u0026amp; !stored.isEmpty()) { for (Item i : stored) { out.collect(\u0026#34;MATCH: \u0026#34; + i + \u0026#34; - \u0026#34; + value); } stored.clear(); } // there is no else{} to cover if rule.first == rule.second  if (shape.equals(rule.first)) { stored.add(value); } if (stored.isEmpty()) { state.remove(ruleName); } else { state.put(ruleName, stored); } } } } 重要的考虑因素 在介绍完提供的 API 之后，本节重点介绍使用广播状态时需要注意的重要事项。这些事项是\n  没有跨任务通信。如前所述，这就是为什么只有 (Keyed)-BroadcastProcessFunction 的广播端可以修改广播状态的内容的原因。此外，用户必须确保所有的任务对每一个传入元素都以同样的方式修改广播状态的内容。否则，不同的任务可能有不同的内容，导致结果不一致。\n  不同任务的广播状态中事件的顺序可能不同。虽然广播流的元素保证了所有元素将（最终）进入所有下游任务，但元素可能会以不同的顺序到达每个任务。因此，每个传入元素的状态更新必须不依赖于传入事件的顺序。\n  所有的任务都会对其广播状态进行 checkpoint。虽然当 checkpoint 发生时，所有任务的广播状态中都有相同的元素（checkpoint 屏障不会超过元素），但所有任务都会 checkpoint 他们的广播状态，而不仅仅是其中一个。这是一个设计决定，以避免在还原过程中让所有任务从同一个文件中读取（从而避免热点），尽管它的代价是将检查点状态的大小增加了p的系数（=并行性）。Flink 保证在恢复/缩放时，不会有重复和丢失的数据。在以相同或更小的并行度进行恢复时，每个任务读取其检查点状态。扩容后，每个任务读取自己的状态，其余任务（p_new-p_old）以循环的方式读取之前任务的检查点。\n  没有 RocksDB 状态后端。广播状态在运行时保存在内存中，内存供应也应相应进行。这对所有的操作符状态都适用。\n  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-the-broadcast-state-pattern/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"广播状态模式"},{"categories":["Flink"],"contents":"Batch 示例 下面的示例程序展示了 Flink 的不同应用，从简单的单词计数到图形算法。这些代码样本说明了 Flink 的 DataSet API 的使用。\n以下和更多例子的完整源代码可以在 Flink 源码库的 flink-examples-batch 模块中找到。\n运行一个例子 为了运行一个 Flink 实例，我们假设你有一个正在运行的 Flink 实例。导航中的 \u0026ldquo;Quickstart\u0026rdquo; 和 \u0026ldquo;Setup\u0026rdquo; 选项卡描述了启动 Flink 的各种方法。\n最简单的方法是运行 ./bin/start-cluster.sh，默认情况下，它用一个 JobManager 和一个 TaskManager 启动一个本地集群。\nFlink 的每个二进制版本都包含一个例子目录，其中有本页每个例子的 jar 文件。\n要运行 WordCount 示例，请发出以下命令。\n./bin/flink run ./examples/batch/WordCount.jar 其他的例子也可以用类似的方式启动。\n请注意，许多例子在运行时没有传递任何参数，而是使用内置的数据。要使用真实数据运行 WordCount，你必须传递数据的路径。\n./bin/flink run ./examples/batch/WordCount.jar --input /path/to/some/text/data --output /path/to/result 请注意，非本地文件系统需要一个模式前缀，如 hdfs://。\nWordCount WordCount 是大数据处理系统中的 \u0026ldquo;Hello World\u0026rdquo;。它计算文本集合中的单词频率。该算法分两步工作。首先，文本被分割成单个单词。第二，对单词进行分组和计数。\nval env = ExecutionEnvironment.getExecutionEnvironment // get input data val text = env.readTextFile(\u0026#34;/path/to/file\u0026#34;) val counts = text.flatMap { _.toLowerCase.split(\u0026#34;\\\\W+\u0026#34;) filter { _.nonEmpty } } .map { (_, 1) } .groupBy(0) .sum(1) counts.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) WordCount 的例子实现了上面描述的算法，输入参数：--input \u0026lt;path\u0026gt; --output \u0026lt;path\u0026gt;。作为测试数据，任何文本文件都可以。\n页面排名 PageRank 算法计算由链接定义的图中页面的\u0026quot;重要性\u0026quot;，这些链接从一个页面指向另一个页面。它是一种迭代图算法，这意味着它反复应用相同的计算。在每一次迭代中，每个页面将其当前的排名分布在所有的邻居上，并计算其新的排名，作为它从邻居那里得到的排名的累加和。PageRank 算法是由 Google 搜索引擎推广的，它利用网页的重要性来对搜索查询的结果进行排名。\n在这个简单的例子中，PageRank 的实现方式是批量迭代和固定的迭代次数。\n// User-defined types case class Link(sourceId: Long, targetId: Long) case class Page(pageId: Long, rank: Double) case class AdjacencyList(sourceId: Long, targetIds: Array[Long]) // set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment // read the pages and initial ranks by parsing a CSV file val pages = env.readCsvFile[Page](pagesInputPath) // the links are encoded as an adjacency list: (page-id, Array(neighbor-ids)) val links = env.readCsvFile[Link](linksInputPath) // assign initial ranks to pages val pagesWithRanks = pages.map(p =\u0026gt; Page(p, 1.0 / numPages)) // build adjacency list from link input val adjacencyLists = links // initialize lists  .map(e =\u0026gt; AdjacencyList(e.sourceId, Array(e.targetId))) // concatenate lists  .groupBy(\u0026#34;sourceId\u0026#34;).reduce { (l1, l2) =\u0026gt; AdjacencyList(l1.sourceId, l1.targetIds ++ l2.targetIds) } // start iteration val finalRanks = pagesWithRanks.iterateWithTermination(maxIterations) { currentRanks =\u0026gt; val newRanks = currentRanks // distribute ranks to target pages  .join(adjacencyLists).where(\u0026#34;pageId\u0026#34;).equalTo(\u0026#34;sourceId\u0026#34;) { (page, adjacent, out: Collector[Page]) =\u0026gt; for (targetId \u0026lt;- adjacent.targetIds) { out.collect(Page(targetId, page.rank / adjacent.targetIds.length)) } } // collect ranks and sum them up  .groupBy(\u0026#34;pageId\u0026#34;).aggregate(SUM, \u0026#34;rank\u0026#34;) // apply dampening factor  .map { p =\u0026gt; Page(p.pageId, (p.rank * DAMPENING_FACTOR) + ((1 - DAMPENING_FACTOR) / numPages)) } // terminate if no rank update was significant  val termination = currentRanks.join(newRanks).where(\u0026#34;pageId\u0026#34;).equalTo(\u0026#34;pageId\u0026#34;) { (current, next, out: Collector[Int]) =\u0026gt; // check for significant update  if (math.abs(current.rank - next.rank) \u0026gt; EPSILON) out.collect(1) } (newRanks, termination) } val result = finalRanks // emit result result.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) PageRank 程序实现了上述示例。它需要以下参数才能运行。--pages \u0026lt;path\u0026gt; --links \u0026lt;path\u0026gt; --output \u0026lt;path\u0026gt; --numPages \u0026lt;n\u0026gt; --iterations \u0026lt;n\u0026gt;。\n输入文件是纯文本文件，必须按以下格式进行。\n 页数用一个（长）ID 表示，用换行字符分隔。  例如 \u0026ldquo;1/n2/n12/n42/n63/n\u0026rdquo; 给出了 5 个 ID 为 1、2、12、42 和 63 的页面。   链接用页面 ID 对表示，用空格分隔。链接用换行符分隔。  例如 \u0026ldquo;1 2\\n2 12\\n1 12\\n42 63\\n\u0026rdquo; 给出了四个(定向)链接(1)-\u0026gt;(2)，(2)-\u0026gt;(12)，(1)-\u0026gt;(12)和(42)-\u0026gt;(63)。    对于这个简单的实现，要求每个页面至少有一个入站链接和一个出站链接（一个页面可以指向自己）。\n连接的组件 Connected Components 算法通过给同一连接部分中的所有顶点分配相同的组件 ID，来识别较大图中相互连接的部分。与 PageRank 类似，Connected Components 是一种迭代算法。在每一步中，每个顶点将其当前的组件 ID 传播给所有的邻居。如果一个顶点接受来自邻居的组件 ID，如果它小于自己的组件 ID。\n本实现使用增量迭代。没有改变组件 ID 的顶点不参与下一步。这产生了更好的性能，因为后面的迭代通常只处理一些离群的顶点。\n// set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment // read vertex and edge data // assign the initial components (equal to the vertex id) val vertices = getVerticesDataSet(env).map { id =\u0026gt; (id, id) } // undirected edges by emitting for each input edge the input edges itself and an inverted // version val edges = getEdgesDataSet(env).flatMap { edge =\u0026gt; Seq(edge, (edge._2, edge._1)) } // open a delta iteration val verticesWithComponents = vertices.iterateDelta(vertices, maxIterations, Array(0)) { (s, ws) =\u0026gt; // apply the step logic: join with the edges  val allNeighbors = ws.join(edges).where(0).equalTo(0) { (vertex, edge) =\u0026gt; (edge._2, vertex._2) } // select the minimum neighbor  val minNeighbors = allNeighbors.groupBy(0).min(1) // update if the component of the candidate is smaller  val updatedComponents = minNeighbors.join(s).where(0).equalTo(0) { (newVertex, oldVertex, out: Collector[(Long, Long)]) =\u0026gt; if (newVertex._2 \u0026lt; oldVertex._2) out.collect(newVertex) } // delta and new workset are identical  (updatedComponents, updatedComponents) } verticesWithComponents.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) ConnectedComponents 程序实现了上面的例子。它需要以下参数才能运行: --vertices \u0026lt;path\u0026gt; --edges \u0026lt;path\u0026gt; --output \u0026lt;path\u0026gt; --iterations \u0026lt;n\u0026gt;。\n输入文件是纯文本文件，必须按如下格式编写。\n 顶点用 ID 表示，并用换行符隔开。  例如 \u0026ldquo;1/n2/n12/n42/n63/n\u0026rdquo; 给出了五个顶点，分别是(1)、(2)、(12)、(42)和(63)。   边缘用一对顶点 ID 表示，这些顶点 ID 用空格字符分隔。边缘用换行符隔开。  例如，\u0026ldquo;1 2/n2 12/n1 12/n42 63/n\u0026rdquo; 给出了四个(非直接)联系(1)-(2)、(2)-(12)、(1)-(12)和(42)-(63)。    原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/examples.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/","tags":["Flink","Flink 官方文档","DataSet API"],"title":"批处理例子"},{"categories":["Flink"],"contents":"注：这描述了新的数据源 API ，作为 FLIP-27 的一部分在 Flink 1.11 中引入。这个新的 API 目前处于 BETA 状态。 大多数现有的源连接器还没有（截至 Flink 1.11 ）使用这个新的 API 实现，而是使用以前的 API ，基于 SourceFunction 。 本页介绍了 Flink 的数据源 API 及其背后的概念和架构。如果你对 Flink 中的数据源是如何工作的，或者你想实现一个新的数据源，请阅读本页面。\n如果您正在寻找预定义的源连接器，请查看连接器文档。\n数据源概念 核心部件\n一个数据源有三个核心组件。Split、SplitEnumerator 和 SourceReader。\n  Split 是数据源所消耗的一部分数据，就像一个文件或一个日志分区。Split 是源分配工作和并行读取数据的粒度。\n  SourceReader 请求 Split 并进行处理，例如读取 Split 所代表的文件或日志分区。SourceReader 在 SourceOperators 的 Task Manager 上并行运行，并产生事件/记录的并行流。\n  SplitEnumerator 生成 Split 并将它们分配给 SourceReader 。它作为单个实例在任务管理器上运行，负责维护待处理的 Split 的积压，并以平衡的方式将它们分配给读者。\n  Source 类是将上述三个组件联系在一起的 API 入口点。\n统一的跨流和批处理\n数据源 API 以统一的方式支持无界流源和有界批处理源。\n这两种情况的区别很小：在有界/批处理的情况下，枚举器生成一组固定的 split ，而且每个 split 必然是有限的。在无界流的情况下，这两种情况中的一种是不正确的（ split 不是有限的，或者枚举器不断产生新的 split ）。\n例子 下面是一些简化的概念性例子，以说明在流式和批处理情况下，数据源组件如何交互。\n请注意，这并不能准确地描述 Kafka 和 File 源的实现是如何工作的；部分内容是简化的，用于说明目的。\n绑定的文件源\n源有一个要读取的目录的 URI/路径，以及一个定义如何解析文件的格式。\n Split 是一个文件，或者一个文件的一个区域（如果数据格式支持分割文件）。 SplitEnumerator 列出了给定目录路径下的所有文件。它将 Split 分配给下一个请求 Split 的读者。一旦所有的 Split 都分配完毕，它就会用 NoMoreSplits 来响应请求。 SourceReader 请求一个 Split ，并读取被分配的 Split （文件或文件区域），并使用给定的格式进行解析。如果它没有得到另一个 Split ，而是得到一个 NoMoreSplits 消息，它就结束了。  非绑定流文件源\n这个源的工作方式和上面描述的一样，除了 SplitEnumerator 从不响应 NoMoreSplits ，而是周期性地列出给定 URI/Path 下的内容以检查新文件。一旦发现新文件，它就会为它们生成新的 Splits ，并可以将它们分配给可用的 SourceReaders。\n无界流 Kafka 源\n该源有一个 Kafka Topic （或 Topic 列表或 Topic regex ）和一个 Deserializer 来解析记录。\n 一个 Split 就是一个 Kafka Topic 分区。 SplitEnumerator 连接到 brokers ，以列出所有涉及订阅的主题分区。枚举器可以选择重复这个操作来发现新添加的主题/分区。 SourceReader 使用 KafkaConsumer 读取分配的 split （主题分区），并使用提供的 Deserializer 反序列化记录。分割(Topic Partitions) 没有终点，所以读取器永远不会到达数据的终点。  绑定的 Kafka 源\n和上面一样，只是每个 Split （主题分区）有一个定义的结束偏移量。一旦 SourceReader 达到一个 Split 的结束偏移量，它就会完成该 Split 。一旦所有分配的 Split 结束， SourceReader 就结束了。\n数据源 API 本节介绍了 FLIP-27 中新引入的 Source API 的主要接口，并为开发者提供了 Source 开发的技巧。\nSource Source API 是一个工厂风格的接口，用于创建以下组件。\n Split Enumerator 源读取器 分离式序列器 枚举器检查点序列器  除此之外， Source 还提供了源的边界属性，这样 Flink 可以选择合适的模式来运行 Flink 作业。\nSource 的实现应该是可序列化的，因为 Source 实例在运行时被序列化并上传到 Flink 集群。\nSplitEnumerator SplitEnumerator 有望成为 Source 的\u0026quot;大脑\u0026quot;。SplitEnumerator 的典型实现会做以下工作。\n SourceReader 注册处理 SourceReader 失败处理  当 SourceReader 失败时，将调用 addSplitsBack() 方法。SplitEnumerator 应该收回未被失败的 SourceReader 承认的分割分配。   SourceEvent 处理  SourceEvents 是在 SplitEnumerator 和 SourceReader 之间发送的自定义事件。实现可以利用这种机制来进行复杂的协调。   分割发现和分配  SplitEnumerator 可以根据各种事件将 split 分配给 SourceReaders ，包括发现新的 split 、新的 SourceReader 注册、 SourceReader 失败等。    SplitEnumerator 可以借助 SplitEnumeratorContext 完成上述工作， SplitEnumeratorContext 是在创建或恢复 SplitEnumerator 时提供给 Source 的。SplitEnumeratorContext 允许 SplitEnumerator 检索读取器的必要信息并执行协调动作。Source 实现应该将 SplitEnumeratorContext 传递给 SplitEnumerator 实例。\n虽然 SplitEnumerator 实现可以通过只在它的方法被调用时采取协调动作的被动方式很好地工作，但一些 SplitEnumerator 实现可能希望主动采取行动。例如，一个 SplitEnumerator 可能希望定期运行 split discovery ，并将新的 split 分配给 SourceReaders 。这样的实现可能会发现调用 Async() 方法 SplitEnumeratorContext 很方便。下面的代码片段展示了 SplitEnumerator 实现如何在不维护自己的线程的情况下实现这一点。\nclass MySplitEnumerator implements SplitEnumerator\u0026lt;MySplit\u0026gt; { private final long DISCOVER_INTERVAL = 60_000L; /** * A method to discover the splits. */ private List\u0026lt;MySplit\u0026gt; discoverSplits() {...} @Override public void start() { ... enumContext.callAsync(this::discoverSplits, splits -\u0026gt; { Map\u0026lt;Integer, List\u0026lt;MockSourceSplit\u0026gt;\u0026gt; assignments = new HashMap\u0026lt;\u0026gt;(); int parallelism = enumContext.currentParallelism(); for (MockSourceSplit split : splits) { int owner = split.splitId().hashCode() % parallelism; assignments.computeIfAbsent(owner, new ArrayList\u0026lt;\u0026gt;()).add(split); } enumContext.assignSplits(new SplitsAssignment\u0026lt;\u0026gt;(assignments)); }, 0L, DISCOVER_INTERVAL); ... } ... } SourceReader SourceReader 是一个运行在 Task Manager 中的组件，用于消耗来自 Splits 的记录。\nSourceReader 暴露了一个基于拉的消费接口。一个 Flink 任务在循环中不断调用 pollNext(ReaderOutput) 来轮询 SourceReader 的记录。pollNext(ReaderOutput) 方法的返回值表示源阅读器的状态。\n MORE_AVAILABLE - SourceReader 立即有更多的记录可用。 NOTHING_AVAILABLE - SourceReader 此时没有更多的记录可用，但将来可能会有更多的记录。 END_OF_INPUT - SourceReader 已经用完了所有的记录，达到了数据的终点。这意味着 SourceReader 可以被关闭。  为了保证性能，会给 pollNext(ReaderOutput) 方法提供一个 ReaderOutput ，所以如果有必要， SourceReader 可以在一次调用 pollNext() 的过程中发出多条记录。例如，有时外部系统的工作粒度是块。一个块可能包含多条记录，但源码只能在块的边界处进行检查点。在这种情况下， SourceReader 可以一次将一个块中的所有记录排放到 ReaderOutput 。但是，除非必要， SourceReader 的实现应该避免在一次 pollNext(ReaderOutput) 的调用中发射多条记录。这是因为从 SourceReader 中进行轮询的任务线程是在事件循环中工作的，不能阻塞。\nSourceReader 的所有状态都应该维护在 SourceSplits 里面，这些状态在 snapshotState() 调用时返回。这样做可以在需要时将 SourceSplits 重新分配给其他 SourceReaders 。\n在创建 SourceReader 时，会向 Source 提供一个 SourceReaderContext 。预计 Source 将把上下文传递给 SourceReader 实例。SourceReader 可以通过 SourceReaderContext 向其 SplitEnumerator 发送 SourceEvent 。Source 的一个典型的设计模式是让 SourceReaders 向 SplitEnumerator 报告它们的本地信息， SplitEnumerator 有一个全局视图来做决策。\nSourceReader API 是一个低级的 API ，它允许用户手动处理 split ，并有自己的线程模型来获取和交接记录。为了方便 SourceReader 的实现， Flink 提供了一个 SourceReaderBase 类，大大减少了编写 SourceReader 的工作量。强烈建议连接器开发人员利用 SourceReaderBase ，而不是从头开始编写 SourceReaders 。更多细节请查看 Split Reader API 部分。\n使用 Source 为了从 Source 创建 DataStream ，需要将 Source 传递给 StreamExecutionEnvironment。例如:\nval env = StreamExecutionEnvironment.getExecutionEnvironment() val mySource = new MySource(...) val stream = env.fromSource( mySource, WatermarkStrategy.noWatermarks(), \u0026#34;MySourceName\u0026#34;) ... Split 读取器 API 核心的 SourceReader API 是完全异步的，需要实现者手动管理异步拆分读取。然而，在实践中，大多数 Source 使用执行阻塞操作，比如在客户端（例如 KafkaConsumer ）上阻塞 poll() 调用，或者在分布式文件系统（ HDFS ， S3 ，\u0026hellip;）上阻塞 I/O 操作。为了与异步的 Source API 兼容，这些阻塞（同步）操作需要发生在单独的线程中，线程将数据交给异步部分的阅读器。\nSplitReader 是用于简单的基于同步读取/轮询的源码实现的高级 API ，比如文件读取、 Kafka 等。\n核心是 SourceReaderBase 类，它接收一个 SplitReader 并创建运行 SplitReader 的 fetcher 线程，支持不同的消费线程模型。\nSplitReader SplitReader API 只有三个方法。\n 一个阻塞获取方法，返回一个 RecordsWithSplitIds 。 一种非阻塞方法，用于处理拆分变化。 一个非阻塞的唤醒方法，用于唤醒阻塞的获取操作。  SplitReader 只专注于从外部系统读取记录，因此比 SourceReader 简单得多。详情请查看该类的 Java 文档。\nSourceReaderBase SourceReader 的实现很常见，它做了以下工作。\n 拥有一个线程池，以阻塞的方式从外部系统的分割处获取数据。 处理内部获取线程和其他方法调用之间的同步，如 pollNext(ReaderOutput) 。 维护每个 split 的水印，以便进行水印对齐。 维护每个分身的状态，以便检查点。  为了减少编写一个新的 SourceReader 的工作， Flink 提供了一个 SourceReaderBase 类作为 SourceReader 的基础实现。SourceReaderBase 开箱即完成了上述所有工作。如果要编写一个新的 SourceReader ，只需要让 SourceReader 实现继承 SourceReaderBase ，填充一些方法，然后实现一个高级的 SplitReader 就可以了。\nSplitFetcherManager SourceReaderBase 支持一些开箱即用的线程模型，这取决于与之合作的 SplitFetcherManager 的行为。SplitFetcherManager 帮助创建和维护一个 SplitFetcher 池，每个 SplitFetcher 用一个 SplitReader 来获取。它还决定了如何将 split 分配给每个 split fetcher 。\n举个例子，如下图所示，一个 SplitFetcherManager 可能有固定数量的线程，每个线程从分配给 SourceReader 的一些 split 中获取。\n下面的代码片段实现了这个线程模型。\n/** * A SplitFetcherManager that has a fixed size of split fetchers and assign splits * to the split fetchers based on the hash code of split IDs. */ public class FixedSizeSplitFetcherManager\u0026lt;E, SplitT extends SourceSplit\u0026gt; extends SplitFetcherManager\u0026lt;E, SplitT\u0026gt; { private final int numFetchers; public FixedSizeSplitFetcherManager( int numFetchers, FutureNotifier futureNotifier, FutureCompletingBlockingQueue\u0026lt;RecordsWithSplitIds\u0026lt;E\u0026gt;\u0026gt; elementsQueue, Supplier\u0026lt;SplitReader\u0026lt;E, SplitT\u0026gt;\u0026gt; splitReaderSupplier) { super(futureNotifier, elementsQueue, splitReaderSupplier); this.numFetchers = numFetchers; // Create numFetchers split fetchers.  for (int i = 0; i \u0026lt; numFetchers; i++) { startFetcher(createSplitFetcher()); } } @Override public void addSplits(List\u0026lt;SplitT\u0026gt; splitsToAdd) { // Group splits by their owner fetchers.  Map\u0026lt;Integer, List\u0026lt;SplitT\u0026gt;\u0026gt; splitsByFetcherIndex = new HashMap\u0026lt;\u0026gt;(); splitsToAdd.forEach(split -\u0026gt; { int ownerFetcherIndex = split.hashCode() % numFetchers; splitsByFetcherIndex .computeIfAbsent(ownerFetcherIndex, s -\u0026gt; new ArrayList\u0026lt;\u0026gt;()) .add(split); }); // Assign the splits to their owner fetcher.  splitsByFetcherIndex.forEach((fetcherIndex, splitsForFetcher) -\u0026gt; { fetchers.get(fetcherIndex).addSplits(splitsForFetcher); }); } } 而使用这个线程模型的 SourceReader 可以创建如下。\npublic class FixedFetcherSizeSourceReader\u0026lt;E, T, SplitT extends SourceSplit, SplitStateT\u0026gt; extends SourceReaderBase\u0026lt;E, T, SplitT, SplitStateT\u0026gt; { public FixedFetcherSizeSourceReader( FutureNotifier futureNotifier, FutureCompletingBlockingQueue\u0026lt;RecordsWithSplitIds\u0026lt;E\u0026gt;\u0026gt; elementsQueue, Supplier\u0026lt;SplitReader\u0026lt;E, SplitT\u0026gt;\u0026gt; splitFetcherSupplier, RecordEmitter\u0026lt;E, T, SplitStateT\u0026gt; recordEmitter, Configuration config, SourceReaderContext context) { super( futureNotifier, elementsQueue, new FixedSizeSplitFetcherManager\u0026lt;\u0026gt;( config.getInteger(SourceConfig.NUM_FETCHERS), futureNotifier, elementsQueue, splitFetcherSupplier), recordEmitter, config, context); } @Override protected void onSplitFinished(Collection\u0026lt;String\u0026gt; finishedSplitIds) { // Do something in the callback for the finished splits.  } @Override protected SplitStateT initializedState(SplitT split) { ... } @Override protected SplitT toSplitType(String splitId, SplitStateT splitState) { ... } } 显然， SourceReader 的实现也可以在 SplitFetcherManager 和 SourceReaderBase 之上轻松实现自己的线程模型。\n事件时间和水印 事件时间分配和水印生成作为数据源的一部分发生。离开源读取器的事件流具有事件时间戳，并且（在流执行期间）包含水印。有关事件时间和水印的介绍，请参见及时流处理。\n重要事项 基于传统 SourceFunction 的应用程序通常会在后面单独的步骤中通过 stream.assignTimestampsAndWatermarks(WatermarkStrategy) 生成时间戳和水印。这个函数不应该被用于新的源，因为时间戳将被分配，并且它将覆盖之前的分割感知水印。\nAPI 在 DataStream API 创建期间， WatermarkStrategy 被传递给 Source，并创建 TimestampAssigner 和 WatermarkGenerator。\nenvironment.fromSource( Source\u0026lt;OUT, ?, ?\u0026gt; source, WatermarkStrategy\u0026lt;OUT\u0026gt; timestampsAndWatermarks, String sourceName) TimestampAssigner 和 WatermarkGenerator 作为 ReaderOutput(或 SourceOutput) 的一部分透明地运行，因此源码实现者不必实现任何时间戳提取和水印生成代码。\n事件时间戳 事件时间戳的分配有两个步骤。\n  SourceReader 可以通过调用 SourceOutput.collect(event, timestamp) 将源记录时间戳附加到事件上。这只与基于记录且有时间戳的数据源有关，如 Kafka 、 Kinesis 、 Pulsar 或 Pravega 。不基于记录且有时间戳的数据源（如文件）没有源记录时间戳。这一步是源连接器实现的一部分，而不是由使用源的应用程序参数化。\n  由应用程序配置的 TimestampAssigner 分配最终的时间戳。TimestampAssigner 看到原始源记录时间戳和事件。分配者可以使用源记录时间戳或访问事件的一个字段获得最终的事件时间戳。\n  这种两步法允许用户同时引用源系统的时间戳和事件数据中的时间戳作为事件时间戳。\n注意：当使用没有源记录时间戳的数据源（如文件），并选择源记录时间戳作为最终的事件时间戳时，事件将得到一个默认的时间戳，等于 LONG_MIN （=-9,223,372,036,854,775,808 ）。\n水印生成 水印生成器仅在流式执行期间激活。批量执行会停用水印生成器；下面描述的所有相关操作都将成为有效的无操作。\n数据源 API 支持每次拆分单独运行水印生成器。这使得 Flink 可以单独观察每个分体的事件时间进度，这对于正确处理事件时间偏斜和防止空闲分区拖累整个应用的事件时间进度非常重要。\n当使用 Split Reader API 实现一个源连接器时，会自动处理这个问题。所有基于 Split Reader API 的实现都具有开箱即用的 split-aware 水印。\n对于一个低级别的 SourceReader API 的实现来说，要使用 split-aware 水印的生成，该实现必须将不同的 split 事件输出到不同的输出中： Split-local SourceOutputs 。分割本地输出可以通过 createOutputForSplit(splitId) 和 releaseOutputForSplit(splitId) 方法在主 ReaderOutput 上创建和释放。详情请参考该类和方法的 JavaDocs 。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/sources.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-data-sources/","tags":["Flink","Flink 官方文档","DataStream API","Data Sources"],"title":"数据源"},{"categories":["Flink"],"contents":"Zipping 数据集中的元素 在某些算法中，人们可能需要为数据集元素分配唯一的标识符。本文档介绍了如何将 DataSetUtils 用于该目的。\n使用密集索引进行 Zip zipWithIndex 给元素分配连续的标签，接收一个数据集作为输入，并返回一个新的（唯一id，初始值）2-tuples的数据集。这个过程需要两次传递，先计数再给元素贴标签，而且由于计数的同步性，不能采用流水线方式。备选的 zipWithUniqueId 以流水线的方式工作，当唯一的标签已经足够时，首选 zip。例如，下面的代码。\nimport org.apache.flink.api.scala._ val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val input: DataSet[String] = env.fromElements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;) val result: DataSet[(Long, String)] = input.zipWithIndex result.writeAsCsv(resultPath, \u0026#34;\\n\u0026#34;, \u0026#34;,\u0026#34;) env.execute() 可以得到元组: (0,G), (1,H), (2,A), (3,B), (4,C), (5,D), (6,E), (7,F)\n带有唯一标识符的 Zip 在许多情况下，人们可能不需要分配连续的标签，zipWithUniqueId 以流水线的方式工作，加快了标签分配过程。该方法接收一个数据集作为输入，并返回一个由（唯一id，初始值）2-tuples组成的新数据集。例如，下面的代码。\nimport org.apache.flink.api.scala._ val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val input: DataSet[String] = env.fromElements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;) val result: DataSet[(Long, String)] = input.zipWithUniqueId result.writeAsCsv(resultPath, \u0026#34;\\n\u0026#34;, \u0026#34;,\u0026#34;) env.execute() 可以得到元组: (0,G), (1,A), (2,H), (3,B), (5,C), (7,D), (9,E), (11,F)\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/zip_elements_guide.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/","tags":["Flink","Flink 官方文档","DataSet API"],"title":"数据集中的 zipping 元素"},{"categories":["Flink"],"contents":"时间属性 Flink 能够根据不同的时间概念来处理流数据。\n 处理时间是指正在执行相应操作的机器的系统时间（也称为\u0026quot;挂钟时间\u0026quot;）。 事件时间指的是基于时间戳对流媒体数据的处理，时间戳附加在每一行上。时间戳可以编码事件发生的时间。 摄取时间是事件进入 Flink 的时间；在内部，它的处理方式与事件时间类似。  关于 Flink 中时间处理的更多信息，请参见关于事件时间和水印的介绍。\n本页解释了如何在 Flink 的表 API 和 SQL 中为基于时间的操作定义时间属性。\n时间属性介绍 Table API 和 SQL 中的窗口等基于时间的操作都需要时间概念及其来源的信息。因此，表可以提供逻辑时间属性，用于指示时间和在表程序中访问相应的时间戳。\n时间属性可以成为每个表模式的一部分。它们是在从 CREATE TABLE DDL 或 DataStream 创建表时定义的，或者是在使用 TableSource 时预先定义的。一旦在开始时定义了时间属性，它就可以作为一个字段被引用，并且可以在基于时间的操作中使用。\n只要时间属性没有被修改，只是从查询的一个部分转发到另一个部分，它仍然是一个有效的时间属性。时间属性的行为就像常规的时间戳一样，可以被访问进行计算。如果在计算中使用了时间属性，它将被具体化并成为常规时间戳。常规时间戳不与 Flink 的时间和水印系统合作，因此不能再用于基于时间的操作。\n表程序要求已经为流环境指定了相应的时间特征。\nval env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime) // default  // alternatively: // env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime) // env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) 处理时间 处理时间允许表程序根据本地机器的时间产生结果。它是最简单的时间概念，但不提供确定性。它既不需要提取时间戳，也不需要生成水印。\n有三种方法可以定义处理时间属性。\n在创建表 DDL 中定义 处理时间属性是在创建表 DDL 中使用系统 PROCTIME()函数定义为计算列。关于计算列的更多信息请参见 CREATE TABLE DDL。\nCREATETABLEuser_actions(user_nameSTRING,dataSTRING,user_action_timeASPROCTIME()-- declare an additional field as a processing time attribute )WITH(...);SELECTTUMBLE_START(user_action_time,INTERVAL\u0026#39;10\u0026#39;MINUTE),COUNT(DISTINCTuser_name)FROMuser_actionsGROUPBYTUMBLE(user_action_time,INTERVAL\u0026#39;10\u0026#39;MINUTE);在 DataStream-to-Table 转换期间 处理时间属性是在模式定义过程中用 .proctime 属性定义的。时间属性只能通过一个额外的逻辑字段来扩展物理模式。因此，它只能在模式定义的最后定义。\nval stream: DataStream[(String, String)] = ... // declare an additional logical field as a processing time attribute val table = tEnv.fromDataStream(stream, $\u0026#34;UserActionTimestamp\u0026#34;, $\u0026#34;user_name\u0026#34;, $\u0026#34;data\u0026#34;, $\u0026#34;user_action_time\u0026#34;.proctime) val windowedTable = table.window(Tumble over 10.minutes on $\u0026#34;user_action_time\u0026#34; as \u0026#34;userActionWindow\u0026#34;) 使用 Table Source 处理时间属性由实现 DefinedProctimeAttribute 接口的 TableSource 定义。逻辑时间属性附加到由 TableSource 的返回类型定义的物理模式中。\n// define a table source with a processing attribute class UserActionSource extends StreamTableSource[Row] with DefinedProctimeAttribute { override def getReturnType = { val names = Array[String](\u0026#34;user_name\u0026#34; , \u0026#34;data\u0026#34;) val types = Array[TypeInformation[_]](Types.STRING, Types.STRING) Types.ROW(names, types) } override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = { // create stream \tval stream = ... stream } override def getProctimeAttribute = { // field with this name will be appended as a third field \t\u0026#34;user_action_time\u0026#34; } } // register table source tEnv.registerTableSource(\u0026#34;user_actions\u0026#34;, new UserActionSource) val windowedTable = tEnv .from(\u0026#34;user_actions\u0026#34;) .window(Tumble over 10.minutes on $\u0026#34;user_action_time\u0026#34; as \u0026#34;userActionWindow\u0026#34;) 事件时间 事件时间允许表格程序根据每条记录中包含的时间产生结果。这使得即使在事件失序或事件迟到的情况下，也能得到一致的结果。当从持久存储中读取记录时，它还能保证表程序的结果可重放。\n此外，事件时间允许在批处理和流环境中对表程序进行统一的语法。流式环境中的时间属性可以是批处理环境中记录的常规字段。\n为了处理失序事件，区分流式环境中事件的准时和迟到，Flink 需要从事件中提取时间戳，并在时间上做出某种进展（所谓的水印）。\n事件时间属性既可以在创建表 DDL 中定义，也可以在 DataStream 到表的转换过程中定义，或者使用 TableSource 定义。\n在创建表 DDL 中定义 事件时间属性是在 CREATE TABLE DDL 中使用 WATERMARK 语句定义的。水印语句在现有的事件时间字段上定义了一个水印生成表达式，将事件时间字段标记为事件时间属性。关于水印语句和水印策略的更多信息，请参见 CREATE TABLE DDL。\nCREATETABLEuser_actions(user_nameSTRING,dataSTRING,user_action_timeTIMESTAMP(3),-- declare user_action_time as event time attribute and use 5 seconds delayed watermark strategy WATERMARKFORuser_action_timeASuser_action_time-INTERVAL\u0026#39;5\u0026#39;SECOND)WITH(...);SELECTTUMBLE_START(user_action_time,INTERVAL\u0026#39;10\u0026#39;MINUTE),COUNT(DISTINCTuser_name)FROMuser_actionsGROUPBYTUMBLE(user_action_time,INTERVAL\u0026#39;10\u0026#39;MINUTE);在 DataStream-to-Table 转换期间 事件时间属性是在模式定义期间用 .rowtime 属性定义的。时间戳和水印必须在被转换的 DataStream 中被分配。\n在将 DataStream 转换为表时，有两种方法可以定义时间属性。根据指定的.rowtime 字段名是否存在于 DataStream 的模式中，时间戳字段要么是\n 作为一个新的字段添加到模式中，或 替换一个现有的字段。  无论哪种情况，事件时间戳字段都将持有 DataStream 事件时间戳的值。\n// Option 1:  // extract timestamp and assign watermarks based on knowledge of the stream val stream: DataStream[(String, String)] = inputStream.assignTimestampsAndWatermarks(...) // declare an additional logical field as an event time attribute val table = tEnv.fromDataStream(stream, $\u0026#34;user_name\u0026#34;, $\u0026#34;data\u0026#34;, $\u0026#34;user_action_time\u0026#34;.rowtime) // Option 2:  // extract timestamp from first field, and assign watermarks based on knowledge of the stream val stream: DataStream[(Long, String, String)] = inputStream.assignTimestampsAndWatermarks(...) // the first field has been used for timestamp extraction, and is no longer necessary // replace first field with a logical event time attribute val table = tEnv.fromDataStream(stream, $\u0026#34;user_action_time\u0026#34;.rowtime, $\u0026#34;user_name\u0026#34;, $\u0026#34;data\u0026#34;) // Usage:  val windowedTable = table.window(Tumble over 10.minutes on $\u0026#34;user_action_time\u0026#34; as \u0026#34;userActionWindow\u0026#34;) 使用 TableSource 事件时间属性由一个实现 DefinedRowtimeAttributes 接口的 TableSource 定义。getRowtimeAttributeDescriptors() 方法返回一个 RowtimeAttributeDescriptor 列表，用于描述时间属性的最终名称，一个用于导出属性值的时间戳提取器，以及与属性相关的水印策略。\n请确保 getDataStream() 方法返回的 DataStream 与定义的时间属性一致。只有当定义了 StreamRecordTimestamp 时间戳提取器时，才会考虑 DataStream 的时间戳（由 TimestampAssigner 分配的时间戳）。只有定义了 PreserveWatermarks 水印策略，DataStream 的水印才会被保留。否则，只有 TableSource 的 rowtime 属性的值是相关的。\n// define a table source with a rowtime attribute class UserActionSource extends StreamTableSource[Row] with DefinedRowtimeAttributes { override def getReturnType = { val names = Array[String](\u0026#34;user_name\u0026#34; , \u0026#34;data\u0026#34;, \u0026#34;user_action_time\u0026#34;) val types = Array[TypeInformation[_]](Types.STRING, Types.STRING, Types.LONG) Types.ROW(names, types) } override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = { // create stream \t// ... \t// assign watermarks based on the \u0026#34;user_action_time\u0026#34; attribute \tval stream = inputStream.assignTimestampsAndWatermarks(...) stream } override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = { // Mark the \u0026#34;user_action_time\u0026#34; attribute as event-time attribute. \t// We create one attribute descriptor of \u0026#34;user_action_time\u0026#34;. \tval rowtimeAttrDescr = new RowtimeAttributeDescriptor( \u0026#34;user_action_time\u0026#34;, new ExistingField(\u0026#34;user_action_time\u0026#34;), new AscendingTimestamps) val listRowtimeAttrDescr = Collections.singletonList(rowtimeAttrDescr) listRowtimeAttrDescr } } // register the table source tEnv.registerTableSource(\u0026#34;user_actions\u0026#34;, new UserActionSource) val windowedTable = tEnv .from(\u0026#34;user_actions\u0026#34;) .window(Tumble over 10.minutes on $\u0026#34;user_action_time\u0026#34; as \u0026#34;userActionWindow\u0026#34;) 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-time-attributes/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"时间属性"},{"categories":["Flink"],"contents":"本地执行 Flink 可以在一台机器上运行，甚至在一台 Java 虚拟机中运行。这使得用户可以在本地测试和调试 Flink 程序。本节将对本地执行机制进行概述。\n本地环境和执行器允许你在本地 Java 虚拟机中运行 Flink 程序，或者作为现有程序的一部分在任何 JVM 中运行。大多数例子可以通过简单地点击 IDE 的\u0026quot;运行\u0026quot;按钮在本地启动。\nFlink 中支持两种不同的本地执行。LocalExecutionEnvironment 是启动完整的 Flink 运行时，包括一个 JobManager 和一个 TaskManager。这些包括内存管理和所有在集群模式下执行的内部算法。\nCollectionEnvironment 是在 Java 集合上执行 Flink 程序。这种模式不会启动完整的 Flink 运行时，所以执行的开销非常低，而且是轻量级的。例如，一个 DataSet.map() 转换将通过将 map() 函数应用于 Java 列表中的所有元素来执行。\n调试 如果你在本地运行 Flink 程序，你也可以像其他 Java 程序一样调试你的程序。你可以使用 System.out.println() 来写出一些内部变量，也可以使用调试器。可以在 map()、reduce() 和其他所有方法中设置断点。也请参考 Java API 文档中的调试部分，了解 Java API 中的测试和本地调试工具的指南。\nMaven 依赖 如果你是在 Maven 项目中开发程序，你必须使用这个依赖关系添加 flink-clients 模块。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-clients_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 本地环境 LocalEnvironment 是 Flink 程序本地执行的一个句柄。使用它可以在本地 JVM 中运行程序\u0026ndash;独立或嵌入其他程序中。\n本地环境是通过 ExecutionEnvironment.createLocalEnvironment() 方法实例化的。默认情况下，它将使用与你的机器有多少 CPU 核（硬件上下文）一样多的本地线程来执行。您也可以指定所需的并行度。本地环境可以配置为使用 enableLogging()/disableLogging() 将日志记录到控制台。\n在大多数情况下，调用 ExecutionEnvironment.getExecutionEnvironment() 是更好的方法。当程序在本地（命令行接口之外）启动时，该方法会返回一个 LocalEnvironment，当程序被命令行接口调用时，该方法会返回一个预配置的集群执行环境。\npublic static void main(String[] args) throws Exception { ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(); DataSet\u0026lt;String\u0026gt; data = env.readTextFile(\u0026#34;file:///path/to/file\u0026#34;); data .filter(new FilterFunction\u0026lt;String\u0026gt;() { public boolean filter(String value) { return value.startsWith(\u0026#34;http://\u0026#34;); } }) .writeAsText(\u0026#34;file:///path/to/result\u0026#34;); JobExecutionResult res = env.execute(); } 在执行结束后返回的 JobExecutionResult 对象，包含了程序运行时间和累加器结果。\nLocalEnvironment 还允许向 Flink 传递自定义配置值。\nConfiguration conf = new Configuration(); conf.setFloat(ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY, 0.5f); final ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(conf); 注意：本地执行环境不启动任何 Web 前端来监控执行。\n收集环境 使用 CollectionEnvironment 在 Java 集合上执行是一种执行 Flink 程序的低开销方法。这种模式的典型用例是自动测试、调试和代码重用。\n用户可以使用为批处理而实现的算法，也可以用于交互性更强的情况。Flink 程序的一个稍微改变的变体可以用于 Java 应用服务器中处理传入的请求。\n基于集合执行的骨架:\npublic static void main(String[] args) throws Exception { // initialize a new Collection-based execution environment  final ExecutionEnvironment env = new CollectionEnvironment(); DataSet\u0026lt;User\u0026gt; users = env.fromCollection( /* get elements from a Java Collection */); /* Data Set transformations ... */ // retrieve the resulting Tuple2 elements into a ArrayList.  Collection\u0026lt;...\u0026gt; result = new ArrayList\u0026lt;...\u0026gt;(); resultDataSet.output(new LocalCollectionOutputFormat\u0026lt;...\u0026gt;(result)); // kick off execution.  env.execute(); // Do some work with the resulting ArrayList (=Collection).  for(... t : result) { System.err.println(\u0026#34;Result = \u0026#34;+t); } } flink-examples-batch 模块包含一个完整的例子，叫做 CollectionExecutionExample。\n请注意，基于集合的 Flink 程序的执行只可能在小数据上执行，小数据适合 JVM 堆。在集合上的执行不是多线程的，只使用一个线程。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/local_execution.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-local-execution/","tags":["Flink","Flink 官方文档","DataSet API"],"title":"本地执行"},{"categories":["Flink"],"contents":"查询 SELECT 语句和 VALUES 语句是用 TableEnvironment 的 sqlQuery()方法指定的。该方法将 SELECT 语句（或 VALUES 语句）的结果作为一个表返回。表可以在后续的 SQL 和 Table API 查询中使用，可以转换为 DataSet 或 DataStream，也可以写入 TableSink。SQL 和 Table API 查询可以无缝混合，并进行整体优化，转化为一个程序。\n为了在 SQL 查询中访问一个表，必须在 TableEnvironment 中注册。表可以从 TableSource、Table、CREATE TABLE 语句、DataStream 或 DataSet 中注册。另外，用户也可以在 TableEnvironment 中注册目录来指定数据源的位置。\n为了方便起见，Table.toString()会自动在其 TableEnvironment 中以唯一的名称注册表，并返回名称。所以，Table 对象可以直接内联到 SQL 查询中，如下例所示。\n注意：包含不支持的 SQL 特性的查询会导致 TableException。批量表和流式表的 SQL 支持的功能在下面的章节中列出。\n指定查询 下面的例子显示了如何在注册表和内联表上指定 SQL 查询。\nval env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) // read a DataStream from an external source val ds: DataStream[(Long, String, Integer)] = env.addSource(...) // SQL query with an inlined (unregistered) table val table = ds.toTable(tableEnv, $\u0026#34;user\u0026#34;, $\u0026#34;product\u0026#34;, $\u0026#34;amount\u0026#34;) val result = tableEnv.sqlQuery( s\u0026#34;SELECT SUM(amount) FROM $tableWHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // SQL query with a registered table // register the DataStream under the name \u0026#34;Orders\u0026#34; tableEnv.createTemporaryView(\u0026#34;Orders\u0026#34;, ds, $\u0026#34;user\u0026#34;, $\u0026#34;product\u0026#34;, $\u0026#34;amount\u0026#34;) // run a SQL query on the Table and retrieve the result as a new Table val result2 = tableEnv.sqlQuery( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // create and register a TableSink val schema = new Schema() .field(\u0026#34;product\u0026#34;, DataTypes.STRING()) .field(\u0026#34;amount\u0026#34;, DataTypes.INT()) tableEnv.connect(new FileSystem(\u0026#34;/path/to/file\u0026#34;)) .withFormat(...) .withSchema(schema) .createTemporaryTable(\u0026#34;RubberOrders\u0026#34;) // run an INSERT SQL on the Table and emit the result to the TableSink tableEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) 执行查询 可以通过 TableEnvironment.executeSql()方法执行 SELECT 语句或 VALUES 语句，将内容收集到本地。该方法将 SELECT 语句（或 VALUES 语句）的结果作为 TableResult 返回。与 SELECT 语句类似，可以使用 Table.execute()方法执行 Table 对象，将查询的内容收集到本地客户端。TableResult.collect()方法返回一个可关闭的行迭代器。除非收集完所有的结果数据，否则选择作业不会结束。我们应该通过 CloseableIterator#close()方法主动关闭作业，避免资源泄露。我们也可以通过 TableResult.print()方法将选择结果打印到客户端控制台。TableResult 中的结果数据只能被访问一次。因此，collect()和 print()不能相继被调用。\n对于流式作业，TableResult.collect()方法或 TableResult.print()方法可以保证端到端的精确一次记录传递。这需要启用检查点机制。默认情况下，检查点机制是被禁用的。要启用检查点，我们可以通过 TableConfig 设置检查点属性（详见检查点配置）。所以一条结果记录只有在其对应的检查点完成后才能被客户端访问。\n注意事项 对于流媒体模式，现在只支持只追加查询。\nval env = StreamExecutionEnvironment.getExecutionEnvironment() val tableEnv = StreamTableEnvironment.create(env, settings) // enable checkpointing tableEnv.getConfig.getConfiguration.set( ExecutionCheckpointingOptions.CHECKPOINTING_MODE, CheckpointingMode.EXACTLY_ONCE) tableEnv.getConfig.getConfiguration.set( ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL, Duration.ofSeconds(10)) tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) // execute SELECT statement val tableResult1 = tableEnv.executeSql(\u0026#34;SELECT * FROM Orders\u0026#34;) val it = tableResult1.collect() try while (it.hasNext) { val row = it.next // handle row } finally it.close() // close the iterator to avoid resource leak  // execute Table val tableResult2 = tableEnv.sqlQuery(\u0026#34;SELECT * FROM Orders\u0026#34;).execute() tableResult2.print() 语法 Flink 使用 Apache Calcite 解析 SQL，它支持标准的 ANSI SQL。\n下面的 BNF-语法描述了在批处理和流式查询中支持的 SQL 特性的超集。操作部分显示了支持的特性的例子，并指出哪些特性只支持批处理或流式查询。\nquery: values | { select | selectWithoutFrom | query UNION [ ALL ] query | query EXCEPT query | query INTERSECT query } [ ORDER BY orderItem [, orderItem ]* ] [ LIMIT { count | ALL } ] [ OFFSET start { ROW | ROWS } ] [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ONLY] orderItem: expression [ ASC | DESC ] select: SELECT [ ALL | DISTINCT ] { * | projectItem [, projectItem ]* } FROM tableExpression [ WHERE booleanExpression ] [ GROUP BY { groupItem [, groupItem ]* } ] [ HAVING booleanExpression ] [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ] selectWithoutFrom: SELECT [ ALL | DISTINCT ] { * | projectItem [, projectItem ]* } projectItem: expression [ [ AS ] columnAlias ] | tableAlias . * tableExpression: tableReference [, tableReference ]* | tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ] joinCondition: ON booleanExpression | USING '(' column [, column ]* ')' tableReference: tablePrimary [ matchRecognize ] [ [ AS ] alias [ '(' columnAlias [, columnAlias ]* ')' ] ] tablePrimary: [ TABLE ] [ [ catalogName . ] schemaName . ] tableName [ dynamicTableOptions ] | LATERAL TABLE '(' functionName '(' expression [, expression ]* ')' ')' | UNNEST '(' expression ')' dynamicTableOptions: /*+ OPTIONS(key=val [, key=val]*) */ key: stringLiteral val: stringLiteral values: VALUES expression [, expression ]* groupItem: expression | '(' ')' | '(' expression [, expression ]* ')' | CUBE '(' expression [, expression ]* ')' | ROLLUP '(' expression [, expression ]* ')' | GROUPING SETS '(' groupItem [, groupItem ]* ')' windowRef: windowName | windowSpec windowSpec: [ windowName ] '(' [ ORDER BY orderItem [, orderItem ]* ] [ PARTITION BY expression [, expression ]* ] [ RANGE numericOrIntervalExpression {PRECEDING} | ROWS numericExpression {PRECEDING} ] ')' matchRecognize: MATCH_RECOGNIZE '(' [ PARTITION BY expression [, expression ]* ] [ ORDER BY orderItem [, orderItem ]* ] [ MEASURES measureColumn [, measureColumn ]* ] [ ONE ROW PER MATCH ] [ AFTER MATCH ( SKIP TO NEXT ROW | SKIP PAST LAST ROW | SKIP TO FIRST variable | SKIP TO LAST variable | SKIP TO variable ) ] PATTERN '(' pattern ')' [ WITHIN intervalLiteral ] DEFINE variable AS condition [, variable AS condition ]* ')' measureColumn: expression AS alias pattern: patternTerm [ '|' patternTerm ]* patternTerm: patternFactor [ patternFactor ]* patternFactor: variable [ patternQuantifier ] patternQuantifier: '*' | '*?' | '+' | '+?' | '?' | '??' | '{' { [ minRepeat ], [ maxRepeat ] } '}' ['?'] | '{' repeat '}' Flink SQL 对标识符（表名、属性名、函数名）使用了类似 Java 的词汇策略。\n无论标识符是否被引用，它们的大小写都会被保留。 之后，标识符会被大小写敏感地匹配。 与 Java 不同的是，回标允许标识符包含非字母数字字符（例如：\u0026ldquo;SELECT a ASmy fieldFROM t\u0026rdquo;）。 字符串必须用单引号括起来（例如，SELECT \u0026lsquo;Hello World\u0026rsquo;）。重复一个单引号进行转义（例如，SELECT \u0026lsquo;It\u0026rsquo;s me.'）。字符串中支持 Unicode 字符。如果需要明确的 unicode 码点，请使用以下语法。\n使用反斜杠（\\）作为转义字符（默认）。SELECT U\u0026amp;'\\263A\u0026rsquo; 使用自定义转义字符。SELECT U\u0026amp;'#263A' UESCAPE \u0026lsquo;#'。\nOperations Scan, Projection 和 Filter  Scan / Select / As(Batch/Streaming)  SELECT*FROMOrdersSELECTa,cASdFROMOrders Where / Filter(Batch/Streaming)  SELECT*FROMOrdersWHEREb=\u0026#39;red\u0026#39;SELECT*FROMOrdersWHEREa%2=0 用户定义标量函数 (Scalar UDF)(Batch/Streaming)  UDF 必须在 TableEnvironment 中注册。关于如何指定和注册标量 UDF 的详细信息，请参见 UDF 文档。\nSELECTPRETTY_PRINT(user)FROMOrders聚合  GroupBy 聚合(Batch/Streaming/Result Updating)  注意：流表上的 GroupBy 会产生更新结果。详情请参见动态表流概念页面。\nSELECTa,SUM(b)asdFROMOrdersGROUPBYa GroupBy 窗口聚合(Batch/Streaming)  使用分组窗口来计算每个组的单一结果行。更多细节请参见分组窗口部分。\nSELECTuser,SUM(amount)FROMOrdersGROUPBYTUMBLE(rowtime,INTERVAL\u0026#39;1\u0026#39;DAY),user Over 窗口聚合(Streaming)  注意：所有的聚合必须定义在同一个窗口上，即相同的分区、排序和范围。目前，只支持对 CURRENT ROW 范围的 PRECEDING（UNBOUNDED 和 bounded）窗口。还不支持带 FOLLOWING 的范围。ORDER BY 必须在单个时间属性上指定。\nSELECTCOUNT(amount)OVER(PARTITIONBYuserORDERBYproctimeROWSBETWEEN2PRECEDINGANDCURRENTROW)FROMOrdersSELECTCOUNT(amount)OVERw,SUM(amount)OVERwFROMOrdersWINDOWwAS(PARTITIONBYuserORDERBYproctimeROWSBETWEEN2PRECEDINGANDCURRENTROW) Distinct(Batch/Streaming/Result Updating)  SELECTDISTINCTusersFROMOrders注意：对于流式查询，计算查询结果所需的状态可能会根据不同字段的数量而无限增长。请提供一个有效的保留时间间隔的查询配置，以防止过大的状态大小。详情请看查询配置。\n Grouping sets, Rollup, Cube(Batch/Streaming/Result Updating)  SELECTSUM(amount)FROMOrdersGROUPBYGROUPINGSETS((user),(product))注：流式模式分组集、Rollup 和 Cube 仅在 Blink 计划器中支持。\n Having(Batch/Streaming)  SELECTSUM(amount)FROMOrdersGROUPBYusersHAVINGSUM(amount)\u0026gt;50 用户定义聚合函数 (UDAGG)(Batch/Streaming)  UDAGG 必须在 TableEnvironment 中注册。关于如何指定和注册 UDAGG 的细节，请参见 UDF 文档。\nSELECTMyAggregate(amount)FROMOrdersGROUPBYusersJoins  Inner Equi-join(Batch/Streaming)  目前，只支持等价连接，即至少有一个带有平等谓词的共轭条件的连接，不支持任意的交叉连接或θ连接。不支持任意的交叉连接或θ连接。\n注意：连接的顺序没有被优化。表的连接顺序是按照 FROM 子句中指定的顺序进行的。确保指定表的顺序不会产生交叉连接（笛卡尔乘积），因为交叉连接不支持，会导致查询失败。\nSELECT*FROMOrdersINNERJOINProductONOrders.productId=Product.id注意：对于流式查询，计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供一个具有有效保留时间间隔的查询配置，以防止状态大小过大。详情请看查询配置。\n Outer Equi-join(Batch/Streaming/Result Updating)  目前，只支持 equi-joins 连接，即至少有一个带有平等谓词的共轭条件的连接，不支持任意的交叉连接或θ连接。不支持任意的交叉连接或θ连接。\n注意：连接的顺序没有被优化。表的连接顺序是按照 FROM 子句中指定的顺序进行的。确保指定表的顺序不会产生交叉连接（笛卡尔乘积），因为交叉连接不支持，会导致查询失败。\nSELECT*FROMOrdersLEFTJOINProductONOrders.productId=Product.idSELECT*FROMOrdersRIGHTJOINProductONOrders.productId=Product.idSELECT*FROMOrdersFULLOUTERJOINProductONOrders.productId=Product.id注意：对于流式查询，计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供一个具有有效保留时间间隔的查询配置，以防止状态大小过大。详情请看查询配置。\n Interval Join(Batch/Streaming)  注：区间连接是常规连接的一个子集，可以用流式处理。\n一个区间连接至少需要一个等价连接谓词和一个连接条件，以限制双方的时间。这样的条件可以由两个合适的范围谓词（\u0026lt;，\u0026lt;=，\u0026gt;=，\u0026gt;）、一个 BETWEEN 谓词或一个比较两个输入表的相同类型的时间属性（即处理时间或事件时间）的单一平等谓词来定义。\n例如，以下谓词是有效的区间连接条件。\n ltime = rtime ltime \u0026gt;= rtime AND ltime \u0026lt; rtime + INTERVAL \u0026lsquo;10\u0026rsquo; MINUTE ltime BETWEEN rtime - INTERVAL \u0026lsquo;10\u0026rsquo; SECOND AND rtime + INTERVAL \u0026lsquo;5\u0026rsquo; SECOND  SELECT*FROMOrderso,ShipmentssWHEREo.id=s.orderIdANDo.ordertimeBETWEENs.shiptime-INTERVAL\u0026#39;4\u0026#39;HOURANDs.shiptime上面的例子中，如果在收到订单 4 小时后才发货，那么就会将所有的订单与其对应的货物加入。\n 将数组扩展为关系(Batch/Streaming)  还不支持 Unnesting With ORDINALITY。\nSELECTusers,tagFROMOrdersCROSSJOINUNNEST(tags)ASt(tag) Join with Table Function (UDTF)(Batch/Streaming)  用表格函数的结果连接一个表格。左表（外表）的每一行都与表函数的相应调用所产生的所有行相连接。\n用户定义表函数（UDTF）必须在之前注册。关于如何指定和注册 UDTF 的细节，请参见 UDF 文档。\nInner Join\n左表（外表）的一行，如果它的表函数调用返回一个空的结果，就会被删除。\nSELECTusers,tagFROMOrders,LATERALTABLE(unnest_udtf(tags))tAStagLeft Outer Join\n如果表函数调用返回的结果为空，则保留相应的外行，并将结果用空值填充。\nSELECTusers,tagFROMOrdersLEFTJOINLATERALTABLE(unnest_udtf(tags))tAStagONTRUE注意：目前，只有字面意义上的 \u0026ldquo;TRUE \u0026ldquo;被支持为针对横向表的左外连接的谓词。\n Join with Temporal Table Function(Streaming)  时间表是跟踪随时间变化的表。\n时间表函数提供了对时间表在特定时间点的状态的访问。使用时态表函数连接表的语法与使用表函数连接相同。\n注意：目前只支持与时态表的内部连接。\n假设 Rates 是一个时间表函数，连接可以用 SQL 表达如下。\nSELECTo_amount,r_rateFROMOrders,LATERALTABLE(Rates(o_proctime))WHEREr_currency=o_currency更多信息请查看更详细的时间表概念说明。\n Join with Temporal Table(Batch/Streaming)  时间表是跟踪随时间变化的表。时间表提供了对时间表在特定时间点的版本的访问。\n只支持与处理时间的时态表进行内联和左联。\n下面的例子假设 LatestRates 是一个时间表，它是以最新的速率来具体化的。\nSELECTo.amout,o.currency,r.rate,o.amount*r.rateFROMOrdersASoJOINLatestRatesFORSYSTEM_TIMEASOFo.proctimeASrONr.currency=o.currency更多信息请查看更详细的时间表概念描述。\n仅支持 Blink 计划器。\n集合运算  Union(Batch)  SELECT*FROM((SELECTuserFROMOrdersWHEREa%2=0)UNION(SELECTuserFROMOrdersWHEREb=0)) UnionAll(Batch/Streaming)  SELECT*FROM((SELECTuserFROMOrdersWHEREa%2=0)UNIONALL(SELECTuserFROMOrdersWHEREb=0)) Intersect / Except(Batch)  SELECT*FROM((SELECTuserFROMOrdersWHEREa%2=0)INTERSECT(SELECTuserFROMOrdersWHEREb=0))SELECT*FROM((SELECTuserFROMOrdersWHEREa%2=0)EXCEPT(SELECTuserFROMOrdersWHEREb=0)) In(Batch/Streaming)  如果给定表的子查询中存在表达式，则返回 true。子查询表必须由一列组成。该列必须与表达式具有相同的数据类型。\nSELECTuser,amountFROMOrdersWHEREproductIN(SELECTproductFROMNewProducts)注意：对于流式查询，该操作被重写为加入和分组操作。计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供有效的保留时间间隔的查询配置，以防止状态大小过大。详情请看查询配置。\n Exists(Batch/Streaming)  如果子查询至少返回一条记录，则返回 true。只有当操作可以被重写成联接和分组操作时才支持。\nSELECTuser,amountFROMOrdersWHEREproductEXISTS(SELECTproductFROMNewProducts)注意：对于流式查询，该操作被重写为加入和分组操作。计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供有效的保留时间间隔的查询配置，以防止状态大小过大。详情请看查询配置。\nOrderBy 和 Limit  Order By  批量流注：流查询的结果必须主要按升序时间属性进行排序。支持其他排序属性。\nSELECT*FROMOrdersORDERBYorderTime Limit(Batch)  注意：LIMIT 子句需要一个 ORDER BY 子句。\nSELECT*FROMOrdersORDERBYorderTimeLIMIT3Top-N 注意 Top-N 只在 Blink planner 中支持。\nTop-N 查询要求按列排序的 N 个最小或最大的值。最小值和最大值集都被认为是 Top-N 查询。当需要从批处理/流处理表中只显示 N 条最底层或最上层的记录时，Top-N 查询非常有用。这个结果集可以用于进一步分析。\nFlink 使用 OVER 窗口子句和过滤条件的组合来表达 Top-N 查询。借助 OVER window PARTITION BY 子句的强大功能，Flink 还支持每组 Top-N。例如，每个类别中实时销售量最大的前五个产品。对于批处理表和流处理表的 SQL，都支持 Top-N 查询。\n下面是 TOP-N 语句的语法。\nSELECT[column_list]FROM(SELECT[column_list],ROW_NUMBER()OVER([PARTITIONBYcol1[,col2...]]ORDERBYcol1[asc|desc][,col2[asc|desc]...])ASrownumFROMtable_name)WHERErownum\u0026lt;=N[ANDconditions]参数说明:\n ROW_NUMBER()。根据分区内行的顺序，给每一行分配一个唯一的、连续的数字，从 1 开始。目前，我们只支持 ROW_NUMBER 作为 over window 函数。在未来，我们将支持 RANK()和 DENSE_RANK()。 PARTITION BY col1[，col2\u0026hellip;]。指定分区列。每个分区将有一个 Top-N 的结果。 ORDER BY col1[asc|desc][，col2[asc|desc]\u0026hellip;]：指定排序列。指定排序列。不同列的排序方向可以不同。 WHERE rownum \u0026lt;= N：为了让 Flink 识别这个查询是 Top-N 查询，需要 rownum \u0026lt;= N。N 代表将保留 N 条最小或最大的记录。 [AND 条件]。在 where 子句中可以自由添加其他条件，但其他条件只能与 rownum \u0026lt;= N 使用 AND 连接组合。  流模式下的注意点: TopN 查询是结果更新。Flink SQL 会根据顺序键对输入的数据流进行排序，所以如果前 N 条记录发生了变化，变化后的记录会作为回撤/更新记录发送到下游。建议使用支持更新的存储作为 Top-N 查询的汇。另外，如果 Top N 记录需要存储在外部存储中，结果表应该与 Top-N 查询的唯一键相同。\nTop-N 查询的唯一键是分区列和 rownum 列的组合。Top-N 查询也可以得出上游的唯一键。以下面的工作为例，假设 product_id 是 ShopSales 的唯一键，那么 Top-N 查询的唯一键是[category，rownum]和[product_id]。\n下面的例子展示了如何在流表上使用 Top-N 指定 SQL 查询。这个例子是为了得到我们上面提到的 \u0026ldquo;每个类别实时销量最大的前五个产品\u0026rdquo;。\nval env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = TableEnvironment.getTableEnvironment(env) // read a DataStream from an external source val ds: DataStream[(String, String, String, Long)] = env.addSource(...) // register the DataStream under the name \u0026#34;ShopSales\u0026#34; tableEnv.createTemporaryView(\u0026#34;ShopSales\u0026#34;, ds, $\u0026#34;product_id\u0026#34;, $\u0026#34;category\u0026#34;, $\u0026#34;product_name\u0026#34;, $\u0026#34;sales\u0026#34;) // select top-5 products per category which have the maximum sales. val result1 = tableEnv.sqlQuery( \u0026#34;\u0026#34;\u0026#34; |SELECT * |FROM ( | SELECT *, | ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) as row_num | FROM ShopSales) |WHERE row_num \u0026lt;= 5 \u0026#34;\u0026#34;\u0026#34;.stripMargin) 无排名输出优化 如上所述，rownum 字段将作为唯一键的一个字段写入结果表，这可能导致很多记录被写入结果表。例如，当排名 9 的记录（比如产品-1001）更新，其排名升级为 1 时，排名 1~9 的所有记录都会作为更新消息输出到结果表。如果结果表接收的数据过多，就会成为 SQL 作业的瓶颈。\n优化的方式是在 Top-N 查询的外侧 SELECT 子句中省略 rownum 字段。这样做是合理的，因为 Top N 记录的数量通常不多，因此消费者可以自己快速排序。如果没有 rownum 字段，在上面的例子中，只需要将改变的记录（product-1001）发送到下游，这样可以减少很多结果表的 IO。\n下面的例子展示了如何用这种方式优化上面的 Top-N 例子。\nval env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = TableEnvironment.getTableEnvironment(env) // read a DataStream from an external source val ds: DataStream[(String, String, String, Long)] = env.addSource(...) // register the DataStream under the name \u0026#34;ShopSales\u0026#34; tableEnv.createTemporaryView(\u0026#34;ShopSales\u0026#34;, ds, $\u0026#34;product_id\u0026#34;, $\u0026#34;category\u0026#34;, $\u0026#34;product_name\u0026#34;, $\u0026#34;sales\u0026#34;) // select top-5 products per category which have the maximum sales. val result1 = tableEnv.sqlQuery( \u0026#34;\u0026#34;\u0026#34; |SELECT product_id, category, product_name, sales -- omit row_num field in the output |FROM ( | SELECT *, | ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) as row_num | FROM ShopSales) |WHERE row_num \u0026lt;= 5 \u0026#34;\u0026#34;\u0026#34;.stripMargin) 流模式下的注意点: 为了将上述查询输出到外部存储中，并得到正确的结果，外部存储必须与 Top-N 查询具有相同的唯一键，在上面的示例查询中，如果 product_id 是查询的唯一键，那么外部表也应该以 product_id 作为唯一键。在上面的示例查询中，如果 product_id 是查询的唯一键，那么外部表也应该以 product_id 作为唯一键。\n重复数据删除 注意 重复数据删除只在 Blink planner 中支持。\n重复数据删除就是删除一组列上重复的行，只保留第一条或最后一条。在某些情况下，上游 ETL 作业并不是端到端完全对接的，这可能会导致在故障切换时，sink 中有重复的记录。但是，重复的记录会影响到下游分析作业（如 SUM、COUNT）的正确性。所以在进一步分析之前需要进行重复数据删除。\nFlink 使用 ROW_NUMBER()来删除重复记录，就像 Top-N 查询的方式一样。理论上，重复数据删除是 Top-N 的一个特例，N 为 1，按处理时间或事件时间排序。\n下面是重复数据删除语句的语法。\nSELECT[column_list]FROM(SELECT[column_list],ROW_NUMBER()OVER([PARTITIONBYcol1[,col2...]]ORDERBYtime_attr[asc|desc])ASrownumFROMtable_name)WHERErownum=1参数说明:\n ROW_NUMBER()。为每一行指定一个唯一的、连续的编号，从 1 开始。 PARTITION BY col1[，col2\u0026hellip;]: 指定分区列，即重复复制键。 ORDER BY time_attr[asc|desc]。指定排序列，必须是时间属性。目前只支持 proctime 属性。未来将支持 Rowtime 属性。用 ASC 排序表示保留第一行，用 DESC 排序表示保留最后一行。 WHERE rownum = 1：为了让 Flink 识别这个查询是重复数据删除，需要 rownum = 1。  下面的例子展示了如何在流表上指定使用重复数据删除的 SQL 查询。\nval env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = TableEnvironment.getTableEnvironment(env) // read a DataStream from an external source val ds: DataStream[(String, String, String, Int)] = env.addSource(...) // register the DataStream under the name \u0026#34;Orders\u0026#34; tableEnv.createTemporaryView(\u0026#34;Orders\u0026#34;, ds, $\u0026#34;order_id\u0026#34;, $\u0026#34;user\u0026#34;, $\u0026#34;product\u0026#34;, $\u0026#34;number\u0026#34;, $\u0026#34;proctime\u0026#34;.proctime) // remove duplicate rows on order_id and keep the first occurrence row, // because there shouldn\u0026#39;t be two orders with the same order_id. val result1 = tableEnv.sqlQuery( \u0026#34;\u0026#34;\u0026#34; |SELECT order_id, user, product, number |FROM ( | SELECT *, | ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY proctime DESC) as row_num | FROM Orders) |WHERE row_num = 1 \u0026#34;\u0026#34;\u0026#34;.stripMargin) Group Windows 组窗口是在 SQL 查询的 GROUP BY 子句中定义的。就像使用常规的 GROUP BY 子句的查询一样，使用包含组窗口函数的 GROUP BY 子句的查询是为每个组计算一条结果行。在批处理表和流式表上的 SQL 支持以下组窗口函数。\n   分组窗口函数 描述     TUMBLE(time_attr, interval) 定义一个滚动时间窗口。滚动时间窗口将行分配到具有固定持续时间（间隔）的非重叠的连续窗口。例如，一个 5 分钟的时间窗口可以将行以 5 分钟的间隔进行分组。滚动窗口可以在事件时间（流+批次）或处理时间（流）上定义。   HOP(time_attr, interval, interval) 定义一个跳转时间窗口（在表 API 中称为滑动窗口）。跳跃时间窗口有一个固定的持续时间（第二个间隔参数），并按指定的跳跃间隔（第一个间隔参数）进行跳转。如果跳转间隔小于窗口大小，则跳转窗口是重叠的。因此，可以将行分配到多个窗口。例如，15 分钟大小的跳转窗口和 5 分钟的跳转间隔将每行分配给 3 个 15 分钟大小的不同窗口，这些窗口以 5 分钟的间隔进行评估。滚动窗口可以在事件时间（流+批处理）或处理时间（流）上定义。   SESSION(time_attr, interval) 定义一个会话时间窗口。会话时间窗口没有固定的持续时间，但其边界由不活动的时间间隔定义，即如果在定义的间隙期内没有事件出现，则会话窗口关闭。例如，有 30 分钟间隙的会话窗口在 30 分钟不活动后观察到一行时开始（否则该行将被添加到现有的窗口中），如果在 30 分钟内没有行被添加，则关闭。会话窗口可以在事件时间（流+批处理）或处理时间（流）上工作。    时间属性 对于流表的 SQL 查询，组窗口函数的 time_attr 参数必须引用一个有效的时间属性，该属性指定行的处理时间或事件时间。请参阅时间属性的文档，了解如何定义时间属性。\n对于批处理表上的 SQL，组窗口函数的 time_attr 参数必须是类型为 TIMESTAMP 的属性。\n选择组窗口的开始和结束时间戳 可以通过以下辅助功能选择组窗口的开始和结束时间戳以及时间属性。\n   Auxiliary 函数 Description     TUMBLE_START(time_attr, interval),HOP_START(time_attr, interval, interval),SESSION_START(time_attr, interval) 返回对应的滚动、跳跃或会话窗口的包容下界的时间戳。   TUMBLE_END(time_attr, interval),HOP_END(time_attr, interval, interval),SESSION_END(time_attr, interval) 返回对应的翻滚、跳跃或会话窗口的专属上界的时间戳。注意：在后续的基于时间的操作中，如区间连接和分组窗口或 over 窗口聚合中，不能将专属上界时间戳作为行时间属性使用。   TUMBLE_ROWTIME(time_attr, interval),HOP_ROWTIME(time_attr, interval, interval),SESSION_ROWTIME(time_attr, interval) 返回对应的翻滚、跳跃或会话窗口的包容上界的时间戳。产生的属性是一个行时间属性，可以用于后续的基于时间的操作，如区间连接和分组窗口或窗口聚合。   TUMBLE_PROCTIME(time_attr, interval),HOP_PROCTIME(time_attr, interval, interval),SESSION_PROCTIME(time_attr, interval) 返回一个 proctime 属性，该属性可用于后续基于时间的操作，如区间连接和分组窗口或过窗口聚合。    注意：在调用辅助函数时，必须使用与 GROUP BY 子句中的组窗口函数完全相同的参数。\n下面的例子展示了如何在流式表上使用组窗口指定 SQL 查询。\nval env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) // read a DataStream from an external source val ds: DataStream[(Long, String, Int)] = env.addSource(...) // register the DataStream under the name \u0026#34;Orders\u0026#34; tableEnv.createTemporaryView(\u0026#34;Orders\u0026#34;, ds, $\u0026#34;user\u0026#34;, $\u0026#34;product\u0026#34;, $\u0026#34;amount\u0026#34;, $\u0026#34;proctime\u0026#34;.proctime, $\u0026#34;rowtime\u0026#34;.rowtime) // compute SUM(amount) per day (in event-time) val result1 = tableEnv.sqlQuery( \u0026#34;\u0026#34;\u0026#34; |SELECT | user, | TUMBLE_START(rowtime, INTERVAL \u0026#39;1\u0026#39; DAY) as wStart, | SUM(amount) | FROM Orders | GROUP BY TUMBLE(rowtime, INTERVAL \u0026#39;1\u0026#39; DAY), user \u0026#34;\u0026#34;\u0026#34;.stripMargin) // compute SUM(amount) per day (in processing-time) val result2 = tableEnv.sqlQuery( \u0026#34;SELECT user, SUM(amount) FROM Orders GROUP BY TUMBLE(proctime, INTERVAL \u0026#39;1\u0026#39; DAY), user\u0026#34;) // compute every hour the SUM(amount) of the last 24 hours in event-time val result3 = tableEnv.sqlQuery( \u0026#34;SELECT product, SUM(amount) FROM Orders GROUP BY HOP(rowtime, INTERVAL \u0026#39;1\u0026#39; HOUR, INTERVAL \u0026#39;1\u0026#39; DAY), product\u0026#34;) // compute SUM(amount) per session with 12 hour inactivity gap (in event-time) val result4 = tableEnv.sqlQuery( \u0026#34;\u0026#34;\u0026#34; |SELECT | user, | SESSION_START(rowtime, INTERVAL \u0026#39;12\u0026#39; HOUR) AS sStart, | SESSION_END(rowtime, INTERVAL \u0026#39;12\u0026#39; HOUR) AS sEnd, | SUM(amount) | FROM Orders | GROUP BY SESSION(rowtime(), INTERVAL \u0026#39;12\u0026#39; HOUR), user \u0026#34;\u0026#34;\u0026#34;.stripMargin) 模式识别  MATCH_RECOGNIZE(Streaming)  根据 MATCH_RECOGNIZE ISO 标准在流表中搜索给定模式。这使得在 SQL 查询中表达复杂事件处理（CEP）逻辑成为可能。\n更详细的描述，请参见检测表中模式的专门页面。\nSELECTT.aid,T.bid,T.cidFROMMyTableMATCH_RECOGNIZE(PARTITIONBYuseridORDERBYproctimeMEASURESA.idASaid,B.idASbid,C.idAScidPATTERN(ABC)DEFINEAASname=\u0026#39;a\u0026#39;,BASname=\u0026#39;b\u0026#39;,CASname=\u0026#39;c\u0026#39;)AST","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-queries/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"查询"},{"categories":["Flink"],"contents":"查询配置 表 API 和 SQL 查询具有相同的语义，无论其输入是有限的行集还是无限制的表变化流。在许多情况下，对流输入的连续查询能够计算出与离线计算结果相同的准确结果。然而，对于一些连续查询，你必须限制它们所维持的状态的大小，以避免在摄取无约束的输入流时耗尽存储。这取决于输入数据的特性和查询本身是否需要限制状态大小，以及它是否和如何影响计算结果的准确性。\nFlink 的 Table API 和 SQL 接口提供了参数来调整连续查询的准确性和资源消耗。这些参数是通过 TableConfig 对象指定的，可以从 TableEnvironment 中获得。\nval env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) // obtain query configuration from TableEnvironment val tConfig: TableConfig = tableEnv.getConfig // set query parameters tConfig.setIdleStateRetentionTime(Time.hours(12), Time.hours(24)) // define query val result: Table = ??? // create TableSink val sink: TableSink[Row] = ??? // register TableSink tableEnv.registerTableSink( \u0026#34;outputTable\u0026#34;, // table name  Array[String](...), // field names  Array[TypeInformation[_]](...), // field types  sink) // table sink  // emit result Table via a TableSink result.executeInsert(\u0026#34;outputTable\u0026#34;) // convert result Table into a DataStream[Row] val stream: DataStream[Row] = result.toAppendStream[Row] 下面我们介绍 TableConfig 的参数，以及它们如何影响查询的准确性和资源消耗。\n闲置状态保留时间 许多查询在一个或多个键属性上聚合或连接记录。当这样的查询在一个流上执行时，连续查询需要收集记录或维护每个键的部分结果。如果输入流的键域是不断变化的，即活跃的键值是随着时间的推移而变化的，那么随着观察到越来越多不同的键，连续查询会积累越来越多的状态。然而，往往键在一段时间后就会变得不活跃，其相应的状态也就变得陈旧无用。\n例如下面的查询计算每节课的点击次数。\nSELECTsessionId,COUNT(*)FROMclicksGROUPBYsessionId;sessionId 属性被用作分组键，连续查询会对它观察到的每个 sessionId 进行计数。sessionId 属性是随着时间的推移而不断变化的，sessionId 值只有在会话结束之前才是有效的，即在有限的时间内。然而，连续查询无法知道 sessionId 的这一属性，它期望每个 sessionId 值都能在任何时间点出现。它为每一个观察到的 sessionId 值维持一个计数。因此，随着观察到的 sessionId 值越来越多，查询的总状态大小也在不断增加。\n闲置状态保留时间参数定义了一个键的状态在被移除之前不被更新的保留时间。对于前面的示例查询，只要在配置的时间段内没有更新，sessionId 的计数就会被删除。\n通过删除一个键的状态，连续查询就会完全忘记它以前见过这个键。如果处理一条带有键的记录，其状态在之前已经被删除，则该记录将被视为带有相应键的第一条记录。对于上面的例子来说，这意味着一个 sessionId 的计数将重新开始为 0。\n有两个参数可以配置空闲状态保留时间。\n 最小空闲状态保留时间定义了一个非活动键的状态在被移除之前至少保留多长时间。 最大空闲状态保留时间定义了非活动键的状态在被删除前最多保留多长时间。  参数指定如下:\nval tConfig: TableConfig = ??? // set idle state retention time: min = 12 hours, max = 24 hours tConfig.setIdleStateRetentionTime(Time.hours(12), Time.hours(24)) 清理状态需要额外的记账，对于 minTime 和 maxTime 的较大差异，记账成本较低。minTime 和 maxTime 之间的差异必须至少为 5 分钟。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/query_configuration.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-query-configuration/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"查询配置"},{"categories":["Flink"],"contents":"Flink 中的每一个函数和操作符都可以是有状态的（详情请看使用状态）。有状态的函数在单个元素/事件的处理过程中存储数据，使得状态成为任何类型的更复杂操作的关键构建模块。\n为了使状态具有容错性，Flink 需要对状态进行 checkpoint。检查点允许 Flink 恢复流中的状态和位置，使应用程序具有与无故障执行相同的语义。\n关于流式容错的文档详细描述了 Flink 的流式容错机制背后的技术。\n前提条件 Flink 的检查点机制与流和状态的持久存储交互。一般来说，它需要:\n 一个能在一定时间内重放记录(replay records)的持久（或耐用）数据源。这种源的例子是持久性消息队列（如 Apache Kafka、RabbitMQ、Amazon Kinesis、Google PubSub）或文件系统（如 HDFS、S3、GFS、NFS、Ceph\u0026hellip;）。 状态的持久性存储，通常是一个分布式文件系统（如 HDFS、S3、GFS、NFS、Ceph\u0026hellip;）。  启用和配置检查点 默认情况下，检查点被禁用。要启用检查点，在 StreamExecutionEnvironment 上调用 enableCheckpointing(n)，其中 n 是检查点间隔，单位为毫秒。\n检查点的其他参数包括:\n  exactly-once vs. at-least-once：你可以选择向 enableCheckpointing(n) 方法传递一个模式，以便在两个保证级别之间进行选择。对于大多数应用来说，exactly-once 是比较好的。At-least-once 可能适用于某些超低延迟（持续几毫秒）的应用。\n  检查点超时。如果一个正在进行中的检查点没有完成，那么它被中止的时间。\n  检查点之间的最小时间。为了确保流应用在检查点之间有一定的进度，可以定义检查点之间需要经过多少时间。例如，如果这个值设置为5000，那么下一个检查点将在上一个检查点完成后不早于5秒开始，无论检查点持续时间和检查点间隔如何。请注意，这意味着检查点间隔永远不会小于这个参数。\n  通过定义\u0026quot;检查点之间的时间\u0026quot;(time between checkpoints)通常比检查点间隔更容易配置应用程序，因为\u0026quot;检查点之间的时间\u0026quot;不容易受到检查点有时可能比平均时间长的事实的影响（例如，如果目标存储系统暂时缓慢）。\n请注意，这个值也意味着并发检查点的数量为1。\n 并发检查点的数量。默认情况下，当一个检查点仍在进行时，系统不会触发另一个检查点。这可以确保拓扑不会在检查点上花费太多时间，而使处理流的工作没有进展。可以允许多个重叠的检查点，这对于那些有一定处理延迟（例如因为函数调用外部服务，需要一些时间来响应），但仍然希望做非常频繁的检查点（100s毫秒），以便在故障时重新处理很少的管道来说是很有意思的。  当定义了检查点之间的最小时间时，不能使用这个选项。\n  外部化检查点。您可以配置周期性检查点，使其在外部持久化。外部化的检查点会将它们的元数据写入持久化存储中，当作业失败时不会自动清理。这样一来，如果你的工作失败了，你身边就会有一个检查点来恢复。关于外部化检查点的部署说明中有更多细节。\n  Fail/checkpoint 错误时继续执行任务。这决定了如果在执行任务的检查点过程中出现错误，任务是否会失败。这是默认行为。另外，当禁用该功能时，任务将简单地拒绝向检查点协调器提供检查点并继续运行。\n  更喜欢用于恢复的检查点。这决定了即使有更近的保存点可用时，任务是否会回退到最新的检查点，以减少恢复时间。\n  不对齐的检查点。你可以启用不对齐的检查点，以大大减少背压下的检查点时间。仅适用于精确的一次检查点，且并发检查点数量为1。\n  val env = StreamExecutionEnvironment.getExecutionEnvironment() // start a checkpoint every 1000 ms env.enableCheckpointing(1000) // 高级选项:  // 设置模式为 exactly-once (这是默认的) env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // make sure 500 ms of progress happen between checkpoints env.getCheckpointConfig.setMinPauseBetweenCheckpoints(500) // checkpoints have to complete within one minute, or are discarded env.getCheckpointConfig.setCheckpointTimeout(60000) // prevent the tasks from failing if an error happens in their checkpointing, the checkpoint will just be declined. env.getCheckpointConfig.setFailTasksOnCheckpointingErrors(false) // allow only one checkpoint to be in progress at the same time env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // enables the experimental unaligned checkpoints env.getCheckpointConfig.enableUnalignedCheckpoints() 相关配置选项 更多的参数和/或默认值可以通过 conf/flink-conf.yaml 来设置（参见配置的完整指南）。\n   键 默认值 类型 描述     state.backend (none) String 用于存储和 checkpoint 状态的状态后端。   state.backend.async true Boolean 状态后端是否应该在可能的情况下使用异步快照方法的选项，可配置。有些状态后端可能不支持异步快照，或者只支持异步快照，而忽略这个选项。   state.backend.fs.memory-threshold 20 kb MemorySize 状态数据文件的最小尺寸。小于这个大小的所有状态块都内嵌存储在根检查点元数据文件中。该配置的最大内存阈值为1MB。   state.backend.fs.write-buffer-size 4096 Integer 写入文件系统的检查点流的默认写缓冲区大小。实际的写缓冲区大小是由这个选项和选项 \u0026lsquo;state.backend.fs.memory-threshold\u0026rsquo; 的最大值决定的。   state.backend.incremental false Boolean 如果可能，状态后端是否应该创建增量检查点。对于增量检查点，只存储与前一个检查点的差异，而不是完整的检查点状态。一旦启用，在 Web UI 中显示的状态大小或从 rest API 中获取的状态大小只代表 delta 检查点大小，而不是完整的检查点大小。一些状态后端可能不支持增量检查点而忽略这个选项。   state.backend.local-recovery false Boolean 这个选项可以配置这个状态后端的本地恢复。默认情况下，本地恢复是被停用的。本地恢复目前只覆盖 keyed state 后端。目前，MemoryStateBackend 不支持本地恢复，忽略此选项。   state.checkpoints.dir (none) String 在 Flink 支持的文件系统中，用于存储检查点数据文件和元数据的默认目录。该存储路径必须可以从所有参与进程/节点（即所有 TaskManager 和 JobManager）访问。   state.checkpoints.num-retained 1 Integer 保留已完成的检查点的最大数量。   state.savepoints.dir (none) String 保存点的默认目录。由将保存点写入文件系统的状态后端（MemoryStateBackend, FsStateBackend, RocksDBStateBackend）使用。   taskmanager.state.local.root-dirs (none) String 配置参数，定义本地恢复中存储基于文件的状态的根目录。本地恢复目前只覆盖 keyed state 后端。目前，MemoryStateBackend 不支持本地恢复，忽略这个选项。    选择状态后端 Flink 的检查点机制在定时器和有状态的操作符中存储所有状态的一致快照，包括连接器、窗口和任何用户定义的状态。检查点存储的位置（例如，JobManager内存、文件系统、数据库）取决于配置的状态后端。\n默认情况下，状态保存在 TaskManager 的内存中，检查点保存在 JobManager 的内存中。为了正确地持久化大状态，Flink 支持各种方法在其他状态后端存储和检查点状态。状态后端的选择可以通过 StreamExecutionEnvironment.setStateBackend(...) 进行配置。\n有关可用的状态后端以及作业范围(job-wide)和集群范围(cluster-wide)配置选项的更多细节，请参见状态后端。\n迭代作业中的状态检查点 Flink 目前只为没有迭代的作业提供处理保证。在迭代作业上启用检查点会导致异常。为了在迭代程序上强制检查点，用户需要在启用检查点时设置一个特殊标志：env.enableCheckpointing(interval, CheckpointingMode.EXACTLY_ONCE, force = true)。\n请注意，循环边缘中飞行中的记录（以及与之相关的状态变化）将在失败时丢失。\n重新启动策略 Flink 支持不同的重启策略，这些策略可以控制作业(job)在发生故障时如何重启。更多信息，请参阅重启策略。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-checkpointing/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"检查点"},{"categories":["Flink"],"contents":"检测表格中的模式 搜索一组事件模式是一个常见的用例，特别是在数据流的情况下。Flink 自带复杂事件处理（CEP）库，可以在事件流中进行模式检测。此外，Flink 的 SQL API 提供了一种关系型的查询表达方式，有大量的内置函数和基于规则的优化，可以开箱即用。\n2016 年 12 月，国际标准化组织（ISO）发布了新版本的 SQL 标准，其中包括 SQL 中的行模式识别（ISO/IEC TR 19075-5:2016）。它允许 Flink 使用 MATCH_RECOGNIZE 子句整合 CEP 和 SQL API，用于 SQL 中的复杂事件处理。\nMATCH_RECOGNIZE 子句可以实现以下任务。\n对使用 partition by 和 order by 子句的数据进行逻辑分区和排序。 使用 PATTERN 子句定义要寻找的行的模式。这些模式使用类似于正则表达式的语法。 行模式变量的逻辑成分在 DEFINE 子句中指定。 在 MEASURES 子句中定义措施，这些措施是在 SQL 查询的其他部分中可用的表达式。 下面的例子说明了基本模式识别的语法。\nSELECTT.aid,T.bid,T.cidFROMMyTableMATCH_RECOGNIZE(PARTITIONBYuseridORDERBYproctimeMEASURESA.idASaid,B.idASbid,C.idAScidPATTERN(ABC)DEFINEAASname=\u0026#39;a\u0026#39;,BASname=\u0026#39;b\u0026#39;,CASname=\u0026#39;c\u0026#39;)AST本页将更详细地解释每个关键字，并将说明更复杂的例子。\n注意 Flink 对 MATCH_RECOGNIZE 子句的实现是完整标准的一个子集。只有那些在下面的章节中记录的功能得到了支持。根据社区反馈，可能会支持更多的功能，也请看一下已知的限制。\n介绍和示例 安装指南 模式识别功能内部使用了 Apache Flink 的 CEP 库。为了能够使用 MATCH_RECOGNIZE 子句，需要将该库作为一个依赖项添加到你的 Maven 项目中。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-cep_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 另外，你也可以将依赖关系添加到集群 classpath 中（更多信息请参见依赖关系部分）。\n如果你想在 SQL 客户端中使用 MATCH_RECOGNIZE 子句，你不需要做任何事情，因为所有的依赖关系都是默认的。\nSQL 语义 每个 MATCH_RECOGNIZE 查询都由以下子句组成。\nPARTITION BY - 定义表的逻辑分区；类似于 GROUP BY 操作。\nMEASURES - 定义子句的输出；类似于 SELECT 子句。 ONE ROW PER MATCH - 输出模式，定义每次匹配应该产生多少行。 AFTER MATCH SKIP\u0026ndash;指定下一个匹配应该从哪里开始；这也是控制一个事件可以属于多少个不同匹配的方法。 PATTERN - 允许使用类似于正则表达式的语法来构建搜索的模式。 DEFINE - 这一部分定义了模式变量必须满足的条件。 注意 目前，MATCH_RECOGNIZE 子句只能应用于追加表。此外，它也总是产生一个追加表。\n例子 在我们的例子中，我们假设已经注册了一个 Ticker 表。该表包含股票在某一特定时间点的价格。\n该表的模式如下：\nTicker |-- symbol: String # symbol of the stock |-- price: Long # price of the stock |-- tax: Long # tax liability of the stock |-- rowtime: TimeIndicatorTypeInfo(rowtime) # point in time when the change to those values happened 为了简化，我们只考虑单只股票 ACME 的传入数据。一个行情可以类似于下表，其中行是连续追加的。\nsymbol rowtime price tax ====== ==================== ======= ======= 'ACME' '01-Apr-11 10:00:00' 12 1 'ACME' '01-Apr-11 10:00:01' 17 2 'ACME' '01-Apr-11 10:00:02' 19 1 'ACME' '01-Apr-11 10:00:03' 21 3 'ACME' '01-Apr-11 10:00:04' 25 2 'ACME' '01-Apr-11 10:00:05' 18 1 'ACME' '01-Apr-11 10:00:06' 15 1 'ACME' '01-Apr-11 10:00:07' 14 2 'ACME' '01-Apr-11 10:00:08' 24 2 'ACME' '01-Apr-11 10:00:09' 25 2 'ACME' '01-Apr-11 10:00:10' 19 1 现在的任务是寻找单一行情的价格不断下降的时期。为此，可以写一个类似的查询。\nSELECT*FROMTickerMATCH_RECOGNIZE(PARTITIONBYsymbolORDERBYrowtimeMEASURESSTART_ROW.rowtimeASstart_tstamp,LAST(PRICE_DOWN.rowtime)ASbottom_tstamp,LAST(PRICE_UP.rowtime)ASend_tstampONEROWPERMATCHAFTERMATCHSKIPTOLASTPRICE_UPPATTERN(START_ROWPRICE_DOWN+PRICE_UP)DEFINEPRICE_DOWNAS(LAST(PRICE_DOWN.price,1)ISNULLANDPRICE_DOWN.price\u0026lt;START_ROW.price)ORPRICE_DOWN.price\u0026lt;LAST(PRICE_DOWN.price,1),PRICE_UPASPRICE_UP.price\u0026gt;LAST(PRICE_DOWN.price,1))MR;该查询按符号列对 Ticker 表进行分区，并按行时间属性进行排序。\nPATTERN 子句指定我们感兴趣的模式是以 START_ROW 事件为起点，然后是一个或多个 PRICE_DOWN 事件，最后是 PRICE_UP 事件。如果能找到这样的模式，下一个模式匹配将在最后一个 PRICE_UP 事件中寻找，如 AFTER MATCH SKIP TO LAST 子句所示。\nDEFINE 子句指定了 PRICE_DOWN 和 PRICE_UP 事件需要满足的条件。虽然 START_ROW 模式变量并不存在，但它有一个隐含的条件，这个条件总是被评估为 TRUE。\n模式变量 PRICE_DOWN 被定义为价格小于满足 PRICE_DOWN 条件的最后一行的价格。对于初始情况或者没有满足 PRICE_DOWN 条件的最后一行，这一行的价格应该小于模式中前一行的价格（由 START_ROW 引用）。\n模式变量 PRICE_UP 被定义为价格大于满足 PRICE_DOWN 条件的最后一行的价格的行。\n该查询为股票价格连续下跌的每个时期产生一条汇总行。\n输出行的具体表示方法在查询的 MEASURES 部分定义。输出行的数量由 ONE ROW PER MATCH 输出模式定义。\n symbol start_tstamp bottom_tstamp end_tstamp ========= ================== ================== ================== ACME 01-APR-11 10:00:04 01-APR-11 10:00:07 01-APR-11 10:00:08 结果一行描述了从 01-APR-11 10:00:04 开始的价格下降期，在 01-APR-11 10:00:07 达到最低价，在 01-APR-11 10:00:08 再次上涨。\n分割 可以在分区数据中寻找模式，例如，单个股票或特定用户的趋势。这可以使用 partition by 子句来表达。该子句类似于使用 GROUP BY 进行聚合。\n注意 强烈建议对输入的数据进行分区，否则 MATCH_RECOGNIZE 子句将被翻译成一个非平行操作符，以确保全局排序。\n事件的顺序 Apache Flink 允许根据时间来搜索模式；无论是处理时间还是事件时间。\n在事件时间的情况下，事件在被传递到内部模式状态机之前会被排序。因此，产生的输出将是正确的，不管行被附加到表中的顺序如何。相反，模式是按照每行包含的时间所指定的顺序来评估的。\nMATCH_RECOGNIZE 子句假设时间属性以升序作为 ORDER BY 子句的第一个参数。\n对于 Ticker 表的例子，像 ORDER BY rowtime ASC, price DESC 这样的定义是有效的，但是 ORDER BY price, rowtime 或者 ORDER BY rowtime DESC, price ASC 是无效的。\n定义和测量 DEFINE 和 MEASURES 关键字的含义类似于简单 SQL 查询中的 WHERE 和 SELECT 子句。\nMEASURES 子句定义了匹配模式的输出中会包含哪些内容。它可以投射列和定义评估的表达式。产生的行数取决于输出模式的设置。\nDEFINE 子句指定了行必须满足的条件，以便将其分类到相应的模式变量。如果没有为模式变量定义条件，那么将使用一个默认条件，该条件对每条记录的评价为真。\n关于这些子句中可以使用的表达式的更详细解释，请看事件流导航部分。\n聚合 聚合可以在 DEFINE 和 MEASURES 子句中使用。同时支持内置和自定义的用户定义函数。\n聚合函数被应用于映射到匹配的行的每个子集。为了了解这些子集是如何被评估的，请看一下事件流导航部分。\n下面这个例子的任务是找到一个股票平均价格不低于某个阈值的最长时间段。它显示了 MATCH_RECOGNIZE 可以如何通过聚合来表达。这个任务可以用下面的查询来执行。\nSELECT*FROMTickerMATCH_RECOGNIZE(PARTITIONBYsymbolORDERBYrowtimeMEASURESFIRST(A.rowtime)ASstart_tstamp,LAST(A.rowtime)ASend_tstamp,AVG(A.price)ASavgPriceONEROWPERMATCHAFTERMATCHSKIPPASTLASTROWPATTERN(A+B)DEFINEAASAVG(A.price)\u0026lt;15)MR;给定这个查询和以下输入值：\nsymbol rowtime price tax ====== ==================== ======= ======= 'ACME' '01-Apr-11 10:00:00' 12 1 'ACME' '01-Apr-11 10:00:01' 17 2 'ACME' '01-Apr-11 10:00:02' 13 1 'ACME' '01-Apr-11 10:00:03' 16 3 'ACME' '01-Apr-11 10:00:04' 25 2 'ACME' '01-Apr-11 10:00:05' 2 1 'ACME' '01-Apr-11 10:00:06' 4 1 'ACME' '01-Apr-11 10:00:07' 10 2 'ACME' '01-Apr-11 10:00:08' 15 2 'ACME' '01-Apr-11 10:00:09' 25 2 'ACME' '01-Apr-11 10:00:10' 25 1 'ACME' '01-Apr-11 10:00:11' 30 1 只要事件的平均价格不超过 15，查询就会将事件累积为模式变量 A 的一部分。例如，这样的超限事件发生在 01-4-11 10:00:04。接下来的时期在 01-4-11 10:00:11 再次超过 15 的平均价格。因此，所述查询的结果将是：。\n symbol start_tstamp end_tstamp avgPrice ========= ================== ================== ============ ACME 01-APR-11 10:00:00 01-APR-11 10:00:03 14.5 ACME 01-APR-11 10:00:05 01-APR-11 10:00:10 13.5 注意 聚合可以应用于表达式，但只有当它们引用一个单一的模式变量时才可以。因此 SUM(A.price * A.tax)是有效的，但是 AVG(A.price * B.tax)不是。\n注意不支持 DISTINCT 聚合。\n定义一个模式 MATCH_RECOGNIZE 子句允许用户在事件流中搜索模式，使用一种强大的、富有表现力的语法，这种语法与广泛使用的正则表达式语法有些相似。\n每个模式都是由基本的构件构成的，称为模式变量，可以对其应用运算符（量化符和其他修饰符）。整个模式必须用括号括起来。\n一个模式的例子可以是这样的。\nPATTERN (A B+ C* D) 我们可以使用以下操作符。\n并集 \u0026ndash; 像(A B)这样的模式意味着 A 和 B 之间的相邻性是严格的，因此，中间不能有没有映射到 A 或 B 的行。 定量符\u0026ndash;修改可以映射到模式变量的行数。\n* — 0 or more rows + — 1 or more rows ? — 0 or 1 rows { n } — exactly n rows (n \u0026gt; 0) { n, } — n or more rows (n ≥ 0) { n, m } — between n and m (inclusive) rows (0 ≤ n ≤ m, 0 \u0026lt; m) { , m } — between 0 and m (inclusive) rows (m \u0026gt; 0) 注意 不支持可能产生空匹配的模式。这类模式的例子有 PATTERN (A*)、PATTERN (A?B*)、PATTERN (A{0,} B{0,} C*)等。\n贪婪和不情愿的量化器 每个量化器可以是贪婪的（默认行为）或勉强的。贪婪的量化器试图匹配尽可能多的记录，而不情愿的量化器试图匹配尽可能少的记录。\n为了说明两者的区别，我们可以查看下面的示例，在这个示例中，一个贪婪的量化器被应用于 B 变量。\nSELECT*FROMTickerMATCH_RECOGNIZE(PARTITIONBYsymbolORDERBYrowtimeMEASURESC.priceASlastPriceONEROWPERMATCHAFTERMATCHSKIPPASTLASTROWPATTERN(AB*C)DEFINEAASA.price\u0026gt;10,BASB.price\u0026lt;15,CASC.price\u0026gt;12)鉴于我们有以下输入。\n symbol tax price rowtime ======= ===== ======== ===================== XYZ 1 10 2018-09-17 10:00:02 XYZ 2 11 2018-09-17 10:00:03 XYZ 1 12 2018-09-17 10:00:04 XYZ 2 13 2018-09-17 10:00:05 XYZ 1 14 2018-09-17 10:00:06 XYZ 2 16 2018-09-17 10:00:07 上述模式将产生以下输出。\n symbol lastPrice ======== =========== XYZ 16 同样的查询，将 B* 修改为 B* 吗，即 B*应该是不愿意的，会产生。\n symbol lastPrice ======== =========== XYZ 13 XYZ 16 模式变量 B 只匹配到价格为 12 的行，而不是吞掉价格为 12、13、14 的行。\n注意 对于一个模式的最后一个变量，不可能使用贪婪的量化符。因此，像（A B*）这样的模式是不允许的。这可以通过引入一个人为的状态（如 C）来轻松解决，这个状态具有 B 的否定条件，所以你可以使用这样的查询。\nPATTERN (A B* C) DEFINE A AS condA(), B AS condB(), C AS NOT condB() 注意 目前不支持可选的勉强量化符(A??或 A{0,1}?)。\n时间限制 特别是对于流式使用案例，通常要求一个模式在给定的时间内完成。这允许限制 Flink 必须在内部维护的整体状态大小，即使在贪婪的量化器的情况下。\n因此，Flink SQL 支持额外的（非标准 SQL）WITHIN 子句来定义模式的时间约束。该子句可以定义在 PATTERN 子句之后，并以毫秒为间隔进行解析。\n如果一个潜在匹配的第一个事件和最后一个事件之间的时间长于给定的值，这样的匹配将不会被追加到结果表中。\n注意 一般鼓励使用 within 子句，因为它有助于 Flink 进行有效的内存管理。一旦达到阈值，底层状态可以被修剪。\n注意 然而，WITHIN 子句不是 SQL 标准的一部分。推荐的处理时间限制的方式可能会在未来发生变化。\n在下面的查询示例中说明了 WITHIN 子句的使用。\nSELECT*FROMTickerMATCH_RECOGNIZE(PARTITIONBYsymbolORDERBYrowtimeMEASURESC.rowtimeASdropTime,A.price-C.priceASdropDiffONEROWPERMATCHAFTERMATCHSKIPPASTLASTROWPATTERN(AB*C)WITHININTERVAL\u0026#39;1\u0026#39;HOURDEFINEBASB.price\u0026gt;A.price-10CASC.price\u0026lt;A.price-10)查询检测到在 1 小时的时间间隔内发生的价格下跌 10。\n假设该查询用于分析以下行情数据。\nsymbol rowtime price tax ====== ==================== ======= ======= 'ACME' '01-Apr-11 10:00:00' 20 1 'ACME' '01-Apr-11 10:20:00' 17 2 'ACME' '01-Apr-11 10:40:00' 18 1 'ACME' '01-Apr-11 11:00:00' 11 3 'ACME' '01-Apr-11 11:20:00' 14 2 'ACME' '01-Apr-11 11:40:00' 9 1 'ACME' '01-Apr-11 12:00:00' 15 1 'ACME' '01-Apr-11 12:20:00' 14 2 'ACME' '01-Apr-11 12:40:00' 24 2 'ACME' '01-Apr-11 13:00:00' 1 2 'ACME' '01-Apr-11 13:20:00' 19 1 查询将产生以下结果。\nsymbol dropTime dropDiff ====== ==================== ============= 'ACME' '01-Apr-11 13:00:00' 14 结果行表示价格从 15（在 4 月 1 日 12:00:00）下降到 1（在 4 月 1 日 13:00:00）。dropDiff 列包含了价格差。\n请注意，即使价格也以更高的数值下降，例如，下降 11（在 01-Apr-11 10:00:00 和 01-Apr-11 11:40:00 之间），这两个事件之间的时间差大于 1 小时。因此，它们不会产生匹配。\n输出模式 输出模式描述了每找到一个匹配的记录应该发出多少行。SQL 标准描述了两种模式。\nALL ROWS PER MATCH ONE ROW PER MATCH. 目前，唯一支持的输出模式是 ONE ROW PER MATCH，对于每一个找到的匹配项，总会产生一个输出汇总行。\n输出行的模式将是[分区列]+[措施列]按该特定顺序的连接。\n下面的例子显示了一个定义为查询的输出。\nSELECT*FROMTickerMATCH_RECOGNIZE(PARTITIONBYsymbolORDERBYrowtimeMEASURESFIRST(A.price)ASstartPrice,LAST(A.price)AStopPrice,B.priceASlastPriceONEROWPERMATCHPATTERN(A+B)DEFINEAASLAST(A.price,1)ISNULLORA.price\u0026gt;LAST(A.price,1),BASB.price\u0026lt;LAST(A.price))对于以下输入行：\n symbol tax price rowtime ======== ===== ======== ===================== XYZ 1 10 2018-09-17 10:00:02 XYZ 2 12 2018-09-17 10:00:03 XYZ 1 13 2018-09-17 10:00:04 XYZ 2 11 2018-09-17 10:00:05 查询将产生以下输出。\n symbol startPrice topPrice lastPrice ======== ============ ========== =========== XYZ 10 13 11 模式识别是按符号列进行分区的。尽管在 MEASURES 子句中没有明确提到，但在结果的开头会添加分区列。\n模式导航 DEFINE 和 MEASURES 子句允许在（可能）匹配模式的行列表中进行导航。\n本节将讨论这种用于声明条件或产生输出结果的导航。\n模式变量引用 模式变量引用允许引用映射到 DEFINE 或 MEASURES 子句中特定模式变量的一组行。\n例如，表达式 A.price 描述了迄今为止映射到 A 的一组行，再加上当前行，如果我们尝试将当前行与 A 进行匹配。如果 DEFINE/MEASURES 子句中的表达式需要单行（例如 A.price 或 A.price\u0026gt;10），则选择属于相应集合的最后一个值。\n如果没有指定模式变量（例如 SUM(price)），表达式会引用默认的模式变量*，它引用模式中的所有变量。换句话说，它创建了一个迄今为止映射到任何变量的所有行加上当前行的列表。\n例子\n要想了解更透彻的例子，可以看看下面的模式和相应的条件。\nPATTERN (A B+) DEFINE A AS A.price \u0026gt; 10, B AS B.price \u0026gt; A.price AND SUM(price) \u0026lt; 100 AND SUM(B.price) \u0026lt; 80 下表描述了如何评估每个传入事件的这些条件。\n该表由以下几栏组成：\n# - the row identifier that uniquely identifies an incoming row in the lists [A.price]/[B.price]/[price]. price - the price of the incoming row. [A.price]/[B.price]/[price] - describe lists of rows which are used in the DEFINE clause to evaluate conditions. Classifier - the classifier of the current row which indicates the pattern variable the row is mapped to. A.price/B.price/SUM(price)/SUM(B.price) - describes the result after those expressions have been evaluated. #\tprice\tClassifier\t[A.price]\t[B.price]\t[price]\tA.price\tB.price\tSUM(price)\tSUM(B.price) #1\t10\t-\u0026gt; A\t#1\t-\t-\t10\t-\t-\t- #2\t15\t-\u0026gt; B\t#1\t#2\t#1, #2\t10\t15\t25\t15 #3\t20\t-\u0026gt; B\t#1\t#2, #3\t#1, #2, #3\t10\t20\t45\t35 #4\t31\t-\u0026gt; B\t#1\t#2, #3, #4\t#1, #2, #3, #4\t10\t31\t76\t66 #5\t35\t#1\t#2, #3, #4, #5\t#1, #2, #3, #4, #5\t10\t35\t111\t101 从表中可以看出，第一行被映射到模式变量 A，随后的行被映射到模式变量 B，但是最后一行不满足 B 的条件，因为所有映射行的 SUM(价格)和 B 中所有行的总和超过了指定的阈值。\n逻辑偏移 逻辑偏移可以在映射到特定模式变量的事件中进行导航。这可以用两个相应的函数来表示。\n偏移函数 描述 LAST(variable.field, n) 返回事件中被映射到变量第 n 个最后元素的字段的值。从映射到的最后一个元素开始计算。\nFIRST(variable.field, n) 返回事件中被映射到变量第 n 个元素的字段值。从映射到的第一个元素开始计算。\n示例\n为了更透彻的举例，可以看看下面的模式和相应的条件。\nPATTERN (A B+) DEFINE A AS A.price \u0026gt; 10, B AS (LAST(B.price, 1) IS NULL OR B.price \u0026gt; LAST(B.price, 1)) AND (LAST(B.price, 2) IS NULL OR B.price \u0026gt; 2 * LAST(B.price, 2)) 下表描述了如何评估每个传入事件的这些条件。\n该表由以下几栏组成：\nprice - the price of the incoming row. Classifier - the classifier of the current row which indicates the pattern variable the row is mapped to. LAST(B.price, 1)/LAST(B.price, 2) - describes the result after those expressions have been evaluated. price\tClassifier\tLAST(B.price, 1)\tLAST(B.price, 2)\tComment 10\t-\u0026gt; A\t15\t-\u0026gt; B\tnull\tnull\tNotice that LAST(A.price, 1) is null because there is still nothing mapped to B. 20\t-\u0026gt; B\t15\tnull\t31\t-\u0026gt; B\t20\t15\t35\t31\t20\tNot mapped because 35 \u0026lt; 2 * 20. 使用默认的模式变量与逻辑偏移量也可能是有意义的。\n在这种情况下，偏移量会考虑到目前为止映射的所有行。\nPATTERN (A B? C) DEFINE B AS B.price \u0026lt; 20, C AS LAST(price, 1) \u0026lt; C.price price\tClassifier\tLAST(price, 1)\tComment 10\t-\u0026gt; A\t15\t-\u0026gt; B\t20\t-\u0026gt; C\t15\tLAST(price, 1) is evaluated as the price of the row mapped to the B variable. 如果第二行没有映射到 B 变量，我们会有以下结果。\nprice\tClassifier\tLAST(price, 1)\tComment 10\t-\u0026gt; A\t20\t-\u0026gt; C\t10\tLAST(price, 1) is evaluated as the price of the row mapped to the A variable. 也可以在 first/last 函数的第一个参数中使用多个模式变量引用。这样，就可以写一个访问多列的表达式。但是，所有这些表达式必须使用同一个模式变量。换句话说，LAST/FIRST 函数的值必须在单行中计算。\n因此，可以使用 LAST(A.price * A.tax)，但不允许使用 LAST(A.price * B.tax)这样的表达式。\n匹配后策略 AFTER MATCH SKIP 子句指定了在找到完整匹配后，在哪里开始一个新的匹配过程。\n有四种不同的策略。\nSKIP PAST LAST ROW - 在当前匹配的最后一行之后的下一行恢复模式匹配。 SKIP TO NEXT ROW - 从匹配起始行后的下一行开始继续搜索新的匹配。 SKIP TO LAST 变量\u0026ndash;在映射到指定模式变量的最后一行恢复模式匹配。 SKIP TO FIRST 变量\u0026ndash;在被映射到指定模式变量的第一行恢复模式匹配。 这也是一种指定一个事件可以属于多少个匹配的方式。例如，使用 SKIP PAST LAST ROW 策略，每个事件最多只能属于一个匹配。\n例子\n为了更好地理解这些策略之间的差异，可以看一下下面的例子。\n对于以下输入行。\n symbol tax price rowtime ======== ===== ======= ===================== XYZ 1 7 2018-09-17 10:00:01 XYZ 2 9 2018-09-17 10:00:02 XYZ 1 10 2018-09-17 10:00:03 XYZ 2 5 2018-09-17 10:00:04 XYZ 2 17 2018-09-17 10:00:05 XYZ 2 14 2018-09-17 10:00:06 我们用不同的策略评估以下查询。\nSELECT*FROMTickerMATCH_RECOGNIZE(PARTITIONBYsymbolORDERBYrowtimeMEASURESSUM(A.price)ASsumPrice,FIRST(rowtime)ASstartTime,LAST(rowtime)ASendTimeONEROWPERMATCH[AFTERMATCHSTRATEGY]PATTERN(A+C)DEFINEAASSUM(A.price)\u0026lt;30)查询返回映射到 A 的所有行的价格总和，以及整体匹配的第一个和最后一个时间戳。\n根据使用的 AFTER MATCH 策略，查询会产生不同的结果。\nAFTER MATCH SKIP PAST ROW(跳过最后一行)\n symbol sumPrice startTime endTime ======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:06 第一个结果与 1 号，2 号，3 号，4 号行相匹配。\n第二个结果与#5, #6 行相匹配。\n匹配后跳转到下一行。\n symbol sumPrice startTime endTime ======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 24 2018-09-17 10:00:02 2018-09-17 10:00:05 XYZ 15 2018-09-17 10:00:03 2018-09-17 10:00:05 XYZ 22 2018-09-17 10:00:04 2018-09-17 10:00:06 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:06 同样，第一个结果对 1 号、2 号、3 号、4 号行进行匹配。\n与之前的策略相比，接下来的匹配中又包含了 2 号行的匹配。因此，第二个结果与行#2，#3，#4，#5 相匹配。\n第三个结果与 3 号，4 号，5 号行相匹配。\n第四个结果与行#4，#5，#6 相匹配。\n最后一个结果与行#5，#6 匹配。\n匹配后跳转到最后一行。\n symbol sumPrice startTime endTime ======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 15 2018-09-17 10:00:03 2018-09-17 10:00:05 XYZ 22 2018-09-17 10:00:04 2018-09-17 10:00:06 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:06 同样，第一个结果针对 1 号、2 号、3 号、4 号行进行匹配。\n与之前的策略相比，接下来的匹配只包括 3 号行（映射到 A 行），再次进行匹配。因此，第二个结果与行#3，#4，#5 相匹配。\n第三个结果与#4，#5，#6 行相匹配。\n最后一个结果与行#5,#6 匹配，因此第三个结果与行#4,#5,#6 匹配。\n匹配后跳转到第一行 A。\n这个组合会产生一个运行时异常，因为我们总是试图在上一个比赛开始的地方开始一个新的比赛。这将产生一个无限循环，因此是被禁止的。\n我们必须记住，在使用 SKIP TO FIRST/LAST 变量策略的情况下，有可能没有记录映射到该变量上（例如模式 A*）。在这种情况下，将抛出一个运行时异常，因为标准要求有一条有效的记录来继续匹配。\n时间属性 为了在 MATCH_RECOGNIZE 之上应用一些后续的查询，可能需要使用时间属性。为了选择这些属性，有两个函数可用。\n功能描述 MATCH_ROWTIME() 返回被映射到给定模式的最后一行的时间戳。\n所得到的属性是一个 rowtime 属性，它可以被用于后续的基于时间的操作，如区间连接和组窗口或窗口聚合。\nMATCH_PROCTIME() 返回一个 proctime 属性，该属性可用于后续基于时间的操作，如区间连接和组窗口或窗口聚合。\n控制内存消耗 在编写 MATCH_RECOGNIZE 查询时，内存消耗是一个重要的考虑因素，因为潜在的匹配空间是以类似广度优先的方式建立的。考虑到这一点，必须确保模式能够完成。最好是有合理数量的行映射到匹配中，因为它们必须适应内存。\n例如，模式不能有一个没有上限的量化器，接受每一行。这样的模式可以是这样的。\nPATTERN (A B+ C) DEFINE A as A.price \u0026gt; 10, C as C.price \u0026gt; 20 该查询将把每一条进入的记录映射到 B 变量上，因此永远不会结束。这个查询可以通过否定 C 的条件来解决。\nPATTERN (A B+ C) DEFINE A as A.price \u0026gt; 10, B as B.price \u0026lt;= 20, C as C.price \u0026gt; 20 或者通过使用勉强的定量器。\nPATTERN (A B+? C) DEFINE A as A.price \u0026gt; 10, C as C.price \u0026gt; 20 注意 请注意，MATCH_RECOGNIZE 子句不使用配置的状态保留时间。人们可能希望使用 WITHIN 子句来达到这个目的。\n已知限制 Flink 对 MATCH_RECOGNIZE 子句的实现是一项持续的努力，目前还不支持 SQL 标准的一些功能。\n不支持的功能包括\n模式表达式。 模式组\u0026ndash;这意味着，例如量化符不能应用于模式的子序列。因此，（A (B C)+）不是有效的模式。 改变\u0026ndash;像 PATTERN((A B | C D) E)这样的模式，这意味着在寻找 E 行之前必须先找到一个子序列 A B 或 C D。 PERMUTE 运算符\u0026ndash;相当于它所应用的所有变量的排列组合，例如 PATTERN(PERMUTE (A, B, C))=PATTERN(A B C | A C B | B A C | B A C | C B A | C B A)。 锚 - ^, $，表示一个分区的开始/结束，这些在流媒体环境中没有意义，将不被支持。 排除 - PATTERN ({- A -} B) 意味着 A 将被查找，但不会参与输出。这只对 ALL ROWS PER MATCH 模式有效。 不情愿的可选量化符\u0026ndash;PATTERN A?? 只支持贪婪的可选量化符。 ALL ROWS PER MATCH 输出模式\u0026ndash;它为每一条参与创建发现匹配的记录产生一条输出行。这也意味着。 MEASURES 子句唯一支持的语义是 FINAL。 CLASSIFIER 函数，该函数返回某行被映射到的模式变量，目前还不支持。 SUBSET - 允许创建模式变量的逻辑组，并在 DEFINE 和 MEASURES 子句中使用这些组。 物理偏移\u0026ndash;PREV/NEXT，它索引所有看到的事件，而不是只索引那些被映射到模式变量的事件（如逻辑偏移情况）。 提取时间属性\u0026ndash;目前没有可能为后续基于时间的操作获取时间属性。 MATCH_RECOGNIZE 只支持 SQL。在 Table API 中没有等价物。 聚合。 不支持不同的聚合。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/match_recognize.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-detecting-patterns-in-tables/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"检测表中的模式"},{"categories":["Flink"],"contents":"概念和通用 API Table API 和 SQL 被集成在一个联合 API 中。这个 API 的核心概念是一个 Table，作为查询的输入和输出。本文档介绍了具有 Table API 和 SQL 查询的程序的常用结构，如何注册 Table，如何查询 Table，如何发出 Table。\n两种 Planners 的主要区别  Blink 将批处理作业视为流式作业的一种特殊情况。因此，也不支持 Table 和 DataSet 之间的转换，批处理作业不会被翻译成 DateSet 程序，而是翻译成 DataStream 程序，和流作业一样。 Blink 计划器不支持 BatchTableSource，请使用有界的 StreamTableSource 代替。 旧计划器和 Blink 计划器的 FilterableTableSource 的实现是不兼容的。旧的规划者会将 PlannerExpressions 推送到 FilterableTableSource 中，而 Blink 规划者会将 Expressions 推送下去。 基于字符串的键值配置选项(详情请看配置文档)只用于 Blink 规划器。 PlannerConfig 在两个规划器中的实现(CalciteConfig)是不同的。 Blink 规划师将在 TableEnvironment 和 StreamTableEnvironment 上把多个汇优化成一个 DAG。旧的规划器总是会将每个汇优化成一个新的 DAG，其中所有的 DAG 是相互独立的。 现在老的计划器不支持目录统计，而 Blink 计划器支持。  Table API 和 SQL 程序的结构 所有用于批处理和流处理的 Table API 和 SQL 程序都遵循相同的模式。下面的代码示例显示了 Table API 和 SQL 程序的共同结构。\n// create a TableEnvironment for specific planner batch or streaming val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  // create a Table tableEnv.connect(...).createTemporaryTable(\u0026#34;table1\u0026#34;) // register an output Table tableEnv.connect(...).createTemporaryTable(\u0026#34;outputTable\u0026#34;) // create a Table from a Table API query val tapiResult = tableEnv.from(\u0026#34;table1\u0026#34;).select(...) // create a Table from a SQL query val sqlResult = tableEnv.sqlQuery(\u0026#34;SELECT ... FROM table1 ...\u0026#34;) // emit a Table API result Table to a TableSink, same for SQL result val tableResult = tapiResult.executeInsert(\u0026#34;outputTable\u0026#34;) tableResult... 注意：表 API 和 SQL 查询可以很容易地与 DataStream 或 DataSet 程序集成并嵌入其中。请查看与 DataStream 和 DataSet API 的集成部分，了解如何将 DataStream 和 DataSets 转换为表，反之亦然。\n创建一个 TableEnvironment TableEnvironment 是 Table API 和 SQL 集成的核心概念。它负责\n 在内部目录(catalog)中注册一个 Table 登记目录(catalog) 加载可插拔模块 执行 SQL 查询 注册一个用户定义的（标量、表或聚合）函数 将 DataStream 或 DataSet 转换为 Table 持有对 ExecutionEnvironment 或 StreamExecutionEnvironment 的引用。  一个 Table 总是绑定在一个特定的 TableEnvironment 上。在同一个查询中，不可能将不同 TableEnvironments 的表组合起来，例如，将它们连接或联合起来。\n通过调用静态的 BatchTableEnvironment.create() 或 StreamTableEnvironment.create() 方法创建一个 TableEnvironment，其中包含一个 StreamExecutionEnvironment 或 ExecutionEnvironment 和一个可选的 TableConfig。TableConfig 可以用来配置 TableEnvironment 或自定义查询优化和翻译过程（参见 Query Optimization）。\n确保选择与你的编程语言相匹配的特定规划器 BatchTableEnvironment/StreamTableEnvironment。\n如果这两个规划器 jar 都在 classpath 上（默认行为），你应该明确设置在当前程序中使用哪个规划器。\n// ********************** // FLINK STREAMING QUERY // ********************** import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.EnvironmentSettings import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment val fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build() val fsEnv = StreamExecutionEnvironment.getExecutionEnvironment val fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings) // or val fsTableEnv = TableEnvironment.create(fsSettings)  // ****************** // FLINK BATCH QUERY // ****************** import org.apache.flink.api.scala.ExecutionEnvironment import org.apache.flink.table.api.bridge.scala.BatchTableEnvironment val fbEnv = ExecutionEnvironment.getExecutionEnvironment val fbTableEnv = BatchTableEnvironment.create(fbEnv) // ********************** // BLINK STREAMING QUERY // ********************** import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.EnvironmentSettings import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment val bsEnv = StreamExecutionEnvironment.getExecutionEnvironment val bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build() val bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings) // or val bsTableEnv = TableEnvironment.create(bsSettings)  // ****************** // BLINK BATCH QUERY // ****************** import org.apache.flink.table.api.{EnvironmentSettings, TableEnvironment} val bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build() val bbTableEnv = TableEnvironment.create(bbSettings) 注意：如果在 /lib 目录下只有一个 planner jar，可以使用 AnyPlanner(python 的 use_any_planner) 来创建特定的环境设置。\n在目录(Catalog)中创建表 一个 TableEnvironment 维护着一个表的目录图，这些表是用一个标识符创建的。每个标识符由 3 部分组成：目录名、数据库名和对象名。如果没有指定目录或数据库，将使用当前的默认值（参见Table 标识符展开部分的例子）。\n表可以是虚拟的（VIEWS）或常规的（TABLES）。VIEWS 可以从现有的 Table 对象创建，通常是 Table API 或 SQL 查询的结果。TABLES 描述外部数据，如文件、数据库表或消息队列。\n临时表与永久表 表可以是临时的，与单个 Flink 会话的生命周期挂钩，也可以是永久的，在多个 Flink 会话和集群中可见。\n永久表需要一个目录（如 Hive Metastore）来维护表的元数据。一旦创建了永久表，它对连接到目录的任何 Flink 会话都是可见的，并将继续存在，直到表被显式放弃。\n另一方面，临时表总是存储在内存中，并且只在它们创建的 Flink 会话的持续时间内存在。这些表对其他会话不可见。它们不绑定到任何目录或数据库，但可以在一个目录或数据库的命名空间中创建。如果相应的数据库被删除，临时表不会被删除。\nShadowing 可以用与现有永久表相同的标识符登记一个临时表。只要临时表存在，临时表就会对永久表产生遮盖，使永久表无法访问。所有使用该标识符的查询都将针对临时表执行。\n这可能对实验很有用。它允许首先对临时表运行完全相同的查询，例如，只有一个数据子集，或者数据被混淆了。一旦验证了查询的正确性，就可以针对真正的生产表运行。\n创建一个 Table 虚拟表 表 API 对象对应于 SQL 术语中的 VIEW（虚拟表）。它封装了一个逻辑查询计划。它可以在一个目录中创建，具体如下。\n// get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  // table is the result of a simple projection query val projTable: Table = tableEnv.from(\u0026#34;X\u0026#34;).select(...) // register the Table projTable as table \u0026#34;projectedTable\u0026#34; tableEnv.createTemporaryView(\u0026#34;projectedTable\u0026#34;, projTable) 注意：Table 对象与关系型数据库系统中的 VIEW 类似，即定义 Table 的查询不进行优化，但当另一个查询引用注册的 Table 时，会被内联。如果多个查询引用同一个注册表，则会对每个引用查询进行内联，并执行多次，即注册表的结果不会被共享。\n连接器表 也可以从连接器声明中创建一个关系型数据库中已知的 TABLE。连接器描述的是存储表数据的外部系统。这里可以声明 Apacha Kafka 或普通文件系统等存储系统。\nDDL tableEnvironment .connect(...) .withFormat(...) .withSchema(...) .inAppendMode() .createTemporaryTable(\u0026#34;MyTable\u0026#34;) 扩展 Table 标识符 表总是用目录(catalog)、数据库、表名三部分组成的标识符进行注册。\n用户可以将其中的一个目录和一个数据库设置为\u0026quot;当前目录\u0026quot;和\u0026quot;当前数据库\u0026quot;。其中，上述 3 部分标识符中的前两部分可以选择，如果不提供，则引用当前目录和当前数据库。用户可以通过表 API 或 SQL 切换当前目录和当前数据库。\n标识符遵循 SQL 的要求，这意味着它们可以用反引号符(`)进行转义。\n// get a TableEnvironment val tEnv: TableEnvironment = ...; tEnv.useCatalog(\u0026#34;custom_catalog\u0026#34;) tEnv.useDatabase(\u0026#34;custom_database\u0026#34;) val table: Table = ...; // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;custom_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;exampleView\u0026#34;, table) // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;other_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;other_database.exampleView\u0026#34;, table) // register the view named \u0026#39;example.View\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;custom_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;`example.View`\u0026#34;, table) // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;other_catalog\u0026#39; // in the database named \u0026#39;other_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;other_catalog.other_database.exampleView\u0026#34;, table) 查询一个 Table Table API Table API 是 Scala 和 Java 的语言集成查询 API。与 SQL 不同的是，查询不是指定为 Strings，而是在宿主语言中一步步组成。\n该 API 基于 Table 类，它表示一个表（流式或批处理），并提供了应用关系操作的方法。这些方法返回一个新的 Table 对象，该对象表示对输入的 Table 应用关系操作的结果。有些关系操作由多个方法调用组成，如 table.groupBy(...).select()，其中 groupBy(...) 指定表的分组，select(...) 是表的分组上的投影。\nTable API 文档描述了流式表和批处理表上支持的所有 Table API 操作。\n下面的示例显示了一个简单的 Table API 聚合查询。\n// get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  // register Orders table  // scan registered Orders table val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) // compute revenue for all customers from France val revenue = orders .filter($\u0026#34;cCountry\u0026#34; === \u0026#34;FRANCE\u0026#34;) .groupBy($\u0026#34;cID\u0026#34;, $\u0026#34;cName\u0026#34;) .select($\u0026#34;cID\u0026#34;, $\u0026#34;cName\u0026#34;, $\u0026#34;revenue\u0026#34;.sum AS \u0026#34;revSum\u0026#34;) // emit or convert Table // execute query 注意：Scala Table API 使用以美元符号（$）开头的 Scala 字符串插值来引用 Table 的属性。Table API 使用 Scala implicits。请确保导入\n org.apache.flink.table.api._ - 用于隐式表达式转换 org.apache.flink.api.scala._ 和 org.apache.flink.table.api.bridge.scala._，如果你想从 DataStream 转换到 DataStream。  SQL Flink 的 SQL 集成是基于 Apache Calcite，它实现了 SQL 标准。SQL 查询被指定为常规 Strings。\nSQL 文档描述了 Flink 对流和批处理表的 SQL 支持。\n下面的例子展示了如何指定一个查询并将结果以表的形式返回。\n// get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  // register Orders table  // compute revenue for all customers from France val revenue = tableEnv.sqlQuery(\u0026#34;\u0026#34;\u0026#34; |SELECT cID, cName, SUM(revenue) AS revSum |FROM Orders |WHERE cCountry = \u0026#39;FRANCE\u0026#39; |GROUP BY cID, cName \u0026#34;\u0026#34;\u0026#34;.stripMargin) // emit or convert Table // execute query 下面的示例显示了如何指定一个更新查询，将其结果插入到注册表中。\n// get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  // register \u0026#34;Orders\u0026#34; table // register \u0026#34;RevenueFrance\u0026#34; output table  // compute revenue for all customers from France and emit to \u0026#34;RevenueFrance\u0026#34; tableEnv.executeSql(\u0026#34;\u0026#34;\u0026#34; |INSERT INTO RevenueFrance |SELECT cID, cName, SUM(revenue) AS revSum |FROM Orders |WHERE cCountry = \u0026#39;FRANCE\u0026#39; |GROUP BY cID, cName \u0026#34;\u0026#34;\u0026#34;.stripMargin) 混合 Table API 和 SQL 表 API 和 SQL 查询可以很容易地混合，因为两者都返回 Table 对象。\n 可以在 SQL 查询返回的 Table 对象上定义 Table API 查询。 通过在 TableEnvironment 中注册生成的 Table并在 SQL 查询的 FROM 子句中引用它，可以在 Table API 查询的结果上定义一个 SQL 查询。  发出一个表 一个 Table 是通过将其写入 TableSink 而发出的。TableSink 是一个通用接口，它支持多种文件格式（如 CSV、Apache Parquet、Apache Avro）、存储系统（如 JDBC、Apache HBase、Apache Cassandra、Elasticsearch）或消息系统（如 Apache Kafka、RabbitMQ）。\n批量表只能写入 BatchTableSink，而流式表则需要 AppendStreamTableSink、RetractStreamTableSink 或 UpsertStreamTableSink。\n请参阅有关 Table Sources \u0026amp; Sink 的文档，以了解可用的 Sink 的详细信息以及如何实现自定义 TableSink 的说明。\nTable.executeInsert(String tableName) 方法将 Table 排放到一个注册的 TableSink 中。该方法通过名称从目录中查找 TableSink，并验证 Table 的模式与 TableSink 的模式是否相同。\n下面的示例展示了如何发射 Table。\n// get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  // create an output Table val schema = new Schema() .field(\u0026#34;a\u0026#34;, DataTypes.INT()) .field(\u0026#34;b\u0026#34;, DataTypes.STRING()) .field(\u0026#34;c\u0026#34;, DataTypes.LONG()) tableEnv.connect(new FileSystem(\u0026#34;/path/to/file\u0026#34;)) .withFormat(new Csv().fieldDelimiter(\u0026#39;|\u0026#39;).deriveSchema()) .withSchema(schema) .createTemporaryTable(\u0026#34;CsvSinkTable\u0026#34;) // compute a result Table using Table API operators and/or SQL queries val result: Table = ... // emit the result Table to the registered TableSink result.executeInsert(\u0026#34;CsvSinkTable\u0026#34;) 翻译和执行查询 两个规划器翻译和执行查询的行为是不同的。\n Blink 计划器  表 API 和 SQL 查询无论其输入是流式还是批处理，都会被翻译成 DataStream 程序。一个查询在内部表示为一个逻辑查询计划，并分两个阶段进行翻译。\n 逻辑计划的优化。 翻译成 DataStream 程序。  Table API 或 SQL 查询被翻译时:\n TableEnvironment.executeSql() 被调用。这个方法用于执行给定的语句，一旦这个方法被调用，sql 查询就会立即被翻译。 Table.executeInsert() 被调用。该方法用于将表的内容插入到给定的 sink 路径中，一旦调用该方法，Table API 立即被翻译。 调用 Table.execute()。该方法用于将表内容收集到本地客户端，一旦调用该方法，Table API 立即被翻译。 StatementSet.execute() 被调用。一个 Table（通过 StatementSet.addInsert() 向 sink 发出）或一个 INSERT 语句（通过 StatementSet.addInsertSql() 指定）将首先在 StatementSet 中被缓冲。一旦 StatementSet.execute() 被调用，它们就会被翻译。所有接收器将被优化成一个 DAG。 当一个表被转换为 DataStream 时，它就会被翻译（参见与 DataStream 和 DataSet API 的集成）。一旦翻译完毕，它就是一个常规的 DataStream 程序，并在调用 StreamExecutionEnvironment.execut()时被执行。 注意: 从 1.11 版本开始，sqlUpdate() 方法和 insertInto() 方法已被废弃。如果 Table 程序是由这两个方法构建的，我们必须使用 StreamTableEnvironment.execution() 方法代替 StreamExecutionEnvironment.execution() 方法来执行。  与 DataStream 和 DataSet API 的集成 两种流上的计划器都可以与 DataStream API 集成，只有老的计划器可以与 DataSet API 集成，批处理的 Blink 计划器不能与两者结合。只有旧的计划器可以与 DataSet API 集成，批处理的 Blink 计划器不能与两者结合。注：下面讨论的 DataSet API 只适用于批处理的旧版规划器。\nTable API 和 SQL 查询可以很容易地与 DataStream 和 DataSet 程序集成并嵌入其中。例如，可以查询一个外部表（例如来自 RDBMS），做一些预处理，如过滤、投影、聚合或加入元数据，然后用 DataStream 或 DataSet API（以及建立在这些 API 之上的任何库，如 CEP 或 Gelly）进一步处理数据。反之，也可以在 DataStream 或 DataSet 程序的结果上应用 Table API 或 SQL 查询。\n这种交互可以通过将 DataStream 或 DataSet 转换为表来实现，反之亦然。在本节中，我们将描述这些转换是如何完成的。\nScala 隐式转换 Scala Table API 为 DataSet、DataStream 和 Table 类提供了隐式转换的功能。这些转换是通过导入包 org.apache.flink.table.api.bridge.scala._ 来实现的，此外还可以导入 org.apache.flink.api.scala._ 来实现 Scala DataStream API。\n从 DataStream 或 DataSet 创建视图 DataStream 或 DataSet 可以作为视图在 TableEnvironment 中注册。由此产生的视图的模式取决于注册的 DataStream 或 DataSet 的数据类型。请查看有关数据类型到表模式的映射部分以了解详情。\n注意：从 DataStream 或 DataSet 创建的视图只能注册为临时视图。\n// get TableEnvironment // registration of a DataSet is equivalent val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  val stream: DataStream[(Long, String)] = ... // register the DataStream as View \u0026#34;myTable\u0026#34; with fields \u0026#34;f0\u0026#34;, \u0026#34;f1\u0026#34; tableEnv.createTemporaryView(\u0026#34;myTable\u0026#34;, stream) // register the DataStream as View \u0026#34;myTable2\u0026#34; with fields \u0026#34;myLong\u0026#34;, \u0026#34;myString\u0026#34; tableEnv.createTemporaryView(\u0026#34;myTable2\u0026#34;, stream, \u0026#39;myLong, \u0026#39;myString) 将 DataStream 或 DataSet 转换为 Table 不需要在 TableEnvironment 中注册一个 DataStream 或 DataSet，也可以直接将其转换为 Table。如果你想在 Table API 查询中使用 Table，这很方便。\n// get TableEnvironment // registration of a DataSet is equivalent val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  val stream: DataStream[(Long, String)] = ... // convert the DataStream into a Table with default fields \u0026#34;_1\u0026#34;, \u0026#34;_2\u0026#34; val table1: Table = tableEnv.fromDataStream(stream) // convert the DataStream into a Table with fields \u0026#34;myLong\u0026#34;, \u0026#34;myString\u0026#34; val table2: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;, $\u0026#34;myString\u0026#34;) 将 Table 转换为 DataStream 或 DataSet Table 可以被转换为 DataStream 或 DataSet。通过这种方式，可以在表 API 或 SQL 查询的结果上运行自定义 DataStream 或 DataSet 程序。\n当将 Table 转换为 DataStream 或 DataSet 时，您需要指定生成的 DataStream 或 DataSet 的数据类型，即表的行要转换为的数据类型。通常，最方便的转换类型是 Row。下面的列表给出了不同选项的功能概述。\n Row：字段按位置映射，字段数量任意，支持 null 值，无类型安全访问。 POJO：字段按名称映射（POJO 字段必须与表字段一样命名），任意数量的字段，支持 null 值，类型安全访问。 Case Class：字段按位置映射，不支持 null 值，类型安全访问。 Tuple：字段按位置映射，限制为 22 个（Scala）或 25 个（Java）字段，不支持 null 值，类型安全访问。 原子类型：表必须有一个字段，不支持空值，类型安全访问。表必须有一个字段，不支持 null 值，类型安全访问。  将 Table 转换为 DataStream 作为流式查询结果的表将被动态更新，即随着查询输入流中新记录的到达而变化。因此，将这种动态查询转换成的 DataStream 需要对表的更新进行编码。\n有两种模式可以将表转换为 DataStream。\n Append 模式。只有当动态 Table 只被 INSERT 修改时，才可以使用这种模式，即只进行追加，之前发出的结果永远不会更新。 收回模式。这种模式可以一直使用。它将 INSERT 和 DELETE 更改用布尔标志编码。  // get TableEnvironment. // registration of a DataSet is equivalent val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  // Table with two fields (String name, Integer age) val table: Table = ... // convert the Table into an append DataStream of Row val dsRow: DataStream[Row] = tableEnv.toAppendStream[Row](table) // convert the Table into an append DataStream of Tuple2[String, Int] val dsTuple: DataStream[(String, Int)] dsTuple = tableEnv.toAppendStream[(String, Int)](table) // convert the Table into a retract DataStream of Row. // A retract stream of type X is a DataStream[(Boolean, X)]. // The boolean field indicates the type of the change. // True is INSERT, false is DELETE. val retractStream: DataStream[(Boolean, Row)] = tableEnv.toRetractStream[Row](table) 注意：关于动态表及其属性的详细讨论在动态表文档中给出。\n注意: 一旦表转换为 DataStream，请使用 StreamExecutionEnvironment.execute() 方法来执行 DataStream 程序。\n将 Table 转换为 DataSet Table 转换为 DataStream 的过程如下:\n// get TableEnvironment // registration of a DataSet is equivalent val tableEnv = BatchTableEnvironment.create(env) // Table with two fields (String name, Integer age) val table: Table = ... // convert the Table into a DataSet of Row val dsRow: DataSet[Row] = tableEnv.toDataSet[Row](table) // convert the Table into a DataSet of Tuple2[String, Int] val dsTuple: DataSet[(String, Int)] = tableEnv.toDataSet[(String, Int)](table) 注意: 一旦 Table 转换为 DataSet，我们必须使用 ExecutionEnvironment.execute 方法来执行 DataSet 程序。\n数据类型到 Table Schema 的映射 Flink 的 DataStream 和 DataSet API 支持非常多样化的类型。复合类型，如 Tuples（内置的 Scala 和 Flink Java tuples）、POJOs、Scala case 类和 Flink 的 Row 类型，允许嵌套具有多个字段的数据结构，这些字段可以在 Table 表达式中访问。其他类型被视为原子类型。在下文中，我们将描述 Table API 如何将这些类型转换为内部行表示，并展示将 DataStream 转换为 Table 的例子。\n数据类型到 Table Schema 的映射可以通过两种方式进行：基于字段位置或基于字段名。\n 基于位置的映射  基于位置的映射可以用来给字段一个更有意义的名字，同时保持字段顺序。这种映射可用于具有定义字段顺序的复合数据类型以及原子类型。复合数据类型如元组、行和 case 类都有这样的字段顺序。然而，POJO 的字段必须根据字段名进行映射（见下一节）。字段可以被投影出来，但不能使用别名作为重命名。\n当定义基于位置的映射时，指定的名称必须不存在于输入数据类型中，否则 API 将假设映射应该基于字段名发生。如果没有指定字段名，则使用复合类型的默认字段名和字段顺序，对于原子类型则使用 f0。\n// get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  val stream: DataStream[(Long, Int)] = ... // convert DataStream into Table with default field names \u0026#34;_1\u0026#34; and \u0026#34;_2\u0026#34; val table: Table = tableEnv.fromDataStream(stream) // convert DataStream into Table with field \u0026#34;myLong\u0026#34; only val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;) // convert DataStream into Table with field names \u0026#34;myLong\u0026#34; and \u0026#34;myInt\u0026#34; val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;, $\u0026#34;myInt\u0026#34;)  基于名称的映射  基于名称的映射可以用于任何数据类型，包括 POJO。它是定义表模式映射的最灵活的方式。映射中的所有字段都是通过名称引用的，并可能使用别名重命名为。字段可以重新排序和投影出来。\n如果没有指定字段名，则使用复合类型的默认字段名和字段顺序，对于原子类型则使用 f0。\n// get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  val stream: DataStream[(Long, Int)] = ... // convert DataStream into Table with default field names \u0026#34;_1\u0026#34; and \u0026#34;_2\u0026#34; val table: Table = tableEnv.fromDataStream(stream) // convert DataStream into Table with field \u0026#34;_2\u0026#34; only val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;) // convert DataStream into Table with swapped fields val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;, $\u0026#34;_1\u0026#34;) // convert DataStream into Table with swapped fields and field names \u0026#34;myInt\u0026#34; and \u0026#34;myLong\u0026#34; val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34; as \u0026#34;myInt\u0026#34;, $\u0026#34;_1\u0026#34; as \u0026#34;myLong\u0026#34;) 原子类型 Flink 将原语（Integer、Double、String）或通用类型（不能分析和分解的类型）视为原子类型。原子类型的 DataStream 或 DataSet 会被转换为具有单一属性的 Table。属性的类型是从原子类型推断出来的，可以指定属性的名称。\n// get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  val stream: DataStream[Long] = ... // convert DataStream into Table with default field name \u0026#34;f0\u0026#34; val table: Table = tableEnv.fromDataStream(stream) // convert DataStream into Table with field name \u0026#34;myLong\u0026#34; val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;) Tuples（Scala 和 Java）和 Case 类（仅 Scala）。 Flink 支持 Scala 的内置元组，并为 Java 提供了自己的元组类。DataStreams 和 DataSets 这两种元组都可以转换为表。通过为所有字段提供名称（基于位置的映射），可以重命名字段。如果没有指定字段名，则使用默认的字段名。如果引用了原始的字段名（对于 Flink Tuples 来说是 f0, f1, \u0026hellip;，对于 Scala Tuples 来说是 _1, _2, \u0026hellip;），API 会假定映射是基于名称而不是基于位置的。基于名称的映射允许重新排序字段和用别名（as）进行投影。\n// get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  val stream: DataStream[(Long, String)] = ... // convert DataStream into Table with renamed default field names \u0026#39;_1, \u0026#39;_2 val table: Table = tableEnv.fromDataStream(stream) // convert DataStream into Table with field names \u0026#34;myLong\u0026#34;, \u0026#34;myString\u0026#34; (position-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;, $\u0026#34;myString\u0026#34;) // convert DataStream into Table with reordered fields \u0026#34;_2\u0026#34;, \u0026#34;_1\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;, $\u0026#34;_1\u0026#34;) // convert DataStream into Table with projected field \u0026#34;_2\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;) // convert DataStream into Table with reordered and aliased fields \u0026#34;myString\u0026#34;, \u0026#34;myLong\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34; as \u0026#34;myString\u0026#34;, $\u0026#34;_1\u0026#34; as \u0026#34;myLong\u0026#34;) // define case class case class Person(name: String, age: Int) val streamCC: DataStream[Person] = ... // convert DataStream into Table with default field names \u0026#39;name, \u0026#39;age val table = tableEnv.fromDataStream(streamCC) // convert DataStream into Table with field names \u0026#39;myName, \u0026#39;myAge (position-based) val table = tableEnv.fromDataStream(streamCC, $\u0026#34;myName\u0026#34;, $\u0026#34;myAge\u0026#34;) // convert DataStream into Table with reordered and aliased fields \u0026#34;myAge\u0026#34;, \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;age\u0026#34; as \u0026#34;myAge\u0026#34;, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) POJO（Java 和 Scala） Flink 支持 POJO 作为复合类型。这里记录了确定 POJO 的规则。\n当将 POJO DataStream 或 DataSet 转换为 Table 而不指定字段名时，会使用原始 POJO 字段的名称。名称映射需要原始名称，不能通过位置来完成。字段可以使用别名（使用 as 关键字）重命名，重新排序，并进行投影。\n// get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  // Person is a POJO with field names \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; val stream: DataStream[Person] = ... // convert DataStream into Table with default field names \u0026#34;age\u0026#34;, \u0026#34;name\u0026#34; (fields are ordered by name!) val table: Table = tableEnv.fromDataStream(stream) // convert DataStream into Table with renamed fields \u0026#34;myAge\u0026#34;, \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;age\u0026#34; as \u0026#34;myAge\u0026#34;, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) // convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34;) // convert DataStream into Table with projected and renamed field \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) Row Row 数据类型支持任意数量的字段和具有 null 值的字段。字段名可以通过 RowTypeInfo 来指定，也可以在将 Row DataStream 或 DataSet 转换为 Table 时指定。Row 类型支持通过位置和名称对字段进行映射。可以通过为所有字段提供名称（基于位置的映射）或单独选择字段进行投影/排序/重命名（基于名称的映射）来重命名字段。\n// get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section  // DataStream of Row with two fields \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; specified in `RowTypeInfo` val stream: DataStream[Row] = ... // convert DataStream into Table with default field names \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34; val table: Table = tableEnv.fromDataStream(stream) // convert DataStream into Table with renamed field names \u0026#34;myName\u0026#34;, \u0026#34;myAge\u0026#34; (position-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myName\u0026#34;, $\u0026#34;myAge\u0026#34;) // convert DataStream into Table with renamed fields \u0026#34;myName\u0026#34;, \u0026#34;myAge\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;, $\u0026#34;age\u0026#34; as \u0026#34;myAge\u0026#34;) // convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34;) // convert DataStream into Table with projected and renamed field \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) 查询优化  Blink 计划器  Apache Flink 利用并扩展了 Apache Calcite 来执行复杂的查询优化。这包括一系列基于规则和成本的优化，如：\n 基于 Apache Calcite 的子查询装饰相关。 投影修剪 分区修剪 过滤器下推 子计划重复复制，避免重复计算。 特殊子查询重写，包括两部分。  将 IN 和 EXISTS 转换为左半连接。 将 NOT IN 和 NOT EXISTS 转换为左反连接。   可选的 join 重新排序  通过 table.optimizer.join-reorder-enabled 启用。    注：IN/EXISTS/NOT IN/NOT EXISTS 目前只支持子查询重写中的连词条件。\n优化器做出智能决策，不仅基于计划，还基于数据源提供的丰富统计数据，以及每个操作符（如 io、cpu、网络和内存）的细粒度成本。\n高级用户可以通过 CalciteConfig 对象提供自定义优化，该对象可以通过调用 TableEnvironment#getConfig#setPlannerConfig 提供给 table 环境。\n解释表 Table API 提供了一种机制来解释计算 Table 的逻辑和优化查询计划。这是通过 Table.explain() 方法或 StatementSet.explain() 方法完成的。Table.explain() 返回一个 Table 的计划。StatementSet.explain() 返回多个接收器的计划。它返回一个描述三个计划的字符串。\n 关系查询的抽象语法树，即未优化的逻辑查询计划。 优化的逻辑查询计划，以及 物理执行计划。  TableEnvironment.explainSql() 和 TableEnvironment.executeSql() 支持执行 EXPLAIN 语句来获取计划，请参考 EXPLAIN 页面。\n下面的代码显示了一个使用 Table.explain() 方法给定 Table 的例子和相应的输出。\nval env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = StreamTableEnvironment.create(env) val table1 = env.fromElements((1, \u0026#34;hello\u0026#34;)).toTable(tEnv, $\u0026#34;count\u0026#34;, $\u0026#34;word\u0026#34;) val table2 = env.fromElements((1, \u0026#34;hello\u0026#34;)).toTable(tEnv, $\u0026#34;count\u0026#34;, $\u0026#34;word\u0026#34;) val table = table1 .where($\u0026#34;word\u0026#34;.like(\u0026#34;F%\u0026#34;)) .unionAll(table2) println(table.explain()) 上述例子的结果是:\n== Abstract Syntax Tree == LogicalUnion(all=[true]) LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')]) FlinkLogicalDataStreamScan(id=[1], fields=[count, word]) FlinkLogicalDataStreamScan(id=[2], fields=[count, word]) == Optimized Logical Plan == DataStreamUnion(all=[true], union all=[count, word]) DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')]) DataStreamScan(id=[1], fields=[count, word]) DataStreamScan(id=[2], fields=[count, word]) == Physical Execution Plan == Stage 1 : Data Source content : collect elements with CollectionInputFormat Stage 2 : Data Source content : collect elements with CollectionInputFormat Stage 3 : Operator content : from: (count, word) ship_strategy : REBALANCE Stage 4 : Operator content : where: (LIKE(word, _UTF-16LE'F%')), select: (count, word) ship_strategy : FORWARD Stage 5 : Operator content : from: (count, word) ship_strategy : REBALANCE 下面的代码显示了使用 StatementSet.explain() 方法进行多重接收器计划的一个例子和相应的输出。\nval settings = EnvironmentSettings.newInstance.useBlinkPlanner.inStreamingMode.build val tEnv = TableEnvironment.create(settings) val schema = new Schema() .field(\u0026#34;count\u0026#34;, DataTypes.INT()) .field(\u0026#34;word\u0026#34;, DataTypes.STRING()) tEnv.connect(new FileSystem(\u0026#34;/source/path1\u0026#34;)) .withFormat(new Csv().deriveSchema()) .withSchema(schema) .createTemporaryTable(\u0026#34;MySource1\u0026#34;) tEnv.connect(new FileSystem(\u0026#34;/source/path2\u0026#34;)) .withFormat(new Csv().deriveSchema()) .withSchema(schema) .createTemporaryTable(\u0026#34;MySource2\u0026#34;) tEnv.connect(new FileSystem(\u0026#34;/sink/path1\u0026#34;)) .withFormat(new Csv().deriveSchema()) .withSchema(schema) .createTemporaryTable(\u0026#34;MySink1\u0026#34;) tEnv.connect(new FileSystem(\u0026#34;/sink/path2\u0026#34;)) .withFormat(new Csv().deriveSchema()) .withSchema(schema) .createTemporaryTable(\u0026#34;MySink2\u0026#34;) val stmtSet = tEnv.createStatementSet() val table1 = tEnv.from(\u0026#34;MySource1\u0026#34;).where($\u0026#34;word\u0026#34;.like(\u0026#34;F%\u0026#34;)) stmtSet.addInsert(\u0026#34;MySink1\u0026#34;, table1) val table2 = table1.unionAll(tEnv.from(\u0026#34;MySource2\u0026#34;)) stmtSet.addInsert(\u0026#34;MySink2\u0026#34;, table2) val explanation = stmtSet.explain() println(explanation) 多重接收器计划的结果是:\n== Abstract Syntax Tree == LogicalLegacySink(name=[MySink1], fields=[count, word]) +- LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')]) +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]]) LogicalLegacySink(name=[MySink2], fields=[count, word]) +- LogicalUnion(all=[true]) :- LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')]) : +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]]) +- LogicalTableScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]]) == Optimized Logical Plan == Calc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')], reuse_id=[1]) +- TableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) LegacySink(name=[MySink1], fields=[count, word]) +- Reused(reference_id=[1]) LegacySink(name=[MySink2], fields=[count, word]) +- Union(all=[true], union=[count, word]) :- Reused(reference_id=[1]) +- TableSourceScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) == Physical Execution Plan == Stage 1 : Data Source content : collect elements with CollectionInputFormat Stage 2 : Operator content : CsvTableSource(read fields: count, word) ship_strategy : REBALANCE Stage 3 : Operator content : SourceConversion(table:Buffer(default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]), fields:(count, word)) ship_strategy : FORWARD Stage 4 : Operator content : Calc(where: (word LIKE _UTF-16LE'F%'), select: (count, word)) ship_strategy : FORWARD Stage 5 : Operator content : SinkConversionToRow ship_strategy : FORWARD Stage 6 : Operator content : Map ship_strategy : FORWARD Stage 8 : Data Source content : collect elements with CollectionInputFormat Stage 9 : Operator content : CsvTableSource(read fields: count, word) ship_strategy : REBALANCE Stage 10 : Operator content : SourceConversion(table:Buffer(default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]), fields:(count, word)) ship_strategy : FORWARD Stage 12 : Operator content : SinkConversionToRow ship_strategy : FORWARD Stage 13 : Operator content : Map ship_strategy : FORWARD Stage 7 : Data Sink content : Sink: CsvTableSink(count, word) ship_strategy : FORWARD Stage 14 : Data Sink content : Sink: CsvTableSink(count, word) ship_strategy : FORWARD 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-concepts-and-common-api/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"概念和通用 API"},{"categories":["Flink"],"contents":"模块测试版 模块允许用户扩展 Flink 的内置对象，比如定义一些行为类似 Flink 内置函数的功能。它们是可插拔的，虽然 Flink 提供了一些预建模块，但用户可以编写自己的模块。\n例如，用户可以定义自己的地理函数，并将其作为内置函数插入 Flink，以便在 Flink SQL 和 Table API 中使用。又比如，用户可以加载一个现成的 Hive 模块，将 Hive 内置函数作为 Flink 内置函数使用。\n模块类型 CoreModule CoreModule 包含了 Flink 的所有系统（内置）功能，并且默认被加载。\nHiveModule HiveModule 作为 Flink 的系统函数，向 SQL 和 Table API 用户提供 Hive 内置函数。Flink 的 Hive 文档提供了设置该模块的全部细节。\n用户定义模块 用户可以通过实现 Module 接口来开发自定义模块。为了在 SQL CLI 中使用自定义模块，用户应该通过实现 ModuleFactory 接口同时开发一个模块和它对应的模块工厂。\n模块工厂定义了一组属性，用于在 SQL CLI 引导时配置模块。属性被传递给发现服务，服务会尝试将属性与模块工厂进行匹配，并实例化一个相应的模块实例。\n命名空间和解析顺序 模块提供的对象被认为是 Flink 系统（内置）对象的一部分；因此，它们没有任何命名空间。\n当有两个同名的对象存在于两个模块中时，Flink 总是将对象引用解析为第一个加载模块中的对象。\n模块 API 装载和卸载模块 用户可以在现有的 Flink 会话中加载和卸载模块。\n Scala  tableEnv.loadModule(\u0026#34;myModule\u0026#34;, new CustomModule()); tableEnv.unloadModule(\u0026#34;myModule\u0026#34;);  YAML  所有使用 YAML 定义的模块都必须提供一个 type 属性来指定类型。以下类型是开箱即用的。\n   Catalog Type Value     CoreModule core   HiveModule hive    modules:- name:coretype:core- name:myhivetype:hive列出可用的模块  Scala  tableEnv.listModules();  SQL  Flink SQL\u0026gt; SHOW MODULES; 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/modules.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-modules/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","Module"],"title":"模块"},{"categories":["Flink"],"contents":"流的概念 Flink 的 Table API和 SQL 支持是批处理和流处理的统一 API。这意味着Table API 和 SQL 查询具有相同的语义，无论其输入是有界批处理输入还是无界流输入。由于关系代数和 SQL 最初是为批处理设计的，所以对无界流输入的关系查询不如对有界批输入的关系查询好理解。\n下面几页解释了 Flink 的关系 API 在流数据上的概念、实际限制和特定流的配置参数。\n下一步该往哪里走？  动态表。描述动态表的概念。 时间属性。解释时间属性，以及在表API和SQL中如何处理时间属性。 连续查询中的连接。连续查询中支持的不同类型的连接。 临时表。描述临时表的概念。 查询配置。列出 Table API 和 SQL 特定配置选项。  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-streaming-concepts/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"流的概念"},{"categories":["Flink"],"contents":"测试 测试是每个软件开发过程中不可缺少的一部分，因此 Apache Flink 提供的工具可以在测试金字塔的多个层次上测试你的应用程序代码。\n测试用户自定义函数 通常，我们可以假设 Flink 在用户定义的函数之外产生正确的结果。因此，建议尽可能用单元测试来测试那些包含主要业务逻辑的类。\n单元测试无状态、Timeless UDFs。 例如，我们来看看下面的无状态 MapFunction。\nclass IncrementMapFunction extends MapFunction[Long, Long] { override def map(record: Long): Long = { record + 1 } } 通过传递合适的参数和验证输出，用你最喜欢的测试框架对这样的函数进行单元测试是非常容易的。\nclass IncrementMapFunctionTest extends FlatSpec with Matchers { \u0026#34;IncrementMapFunction\u0026#34; should \u0026#34;increment values\u0026#34; in { // instantiate your function  val incrementer: IncrementMapFunction = new IncrementMapFunction() // call the methods that you have implemented  incremeter.map(2) should be (3) } } 同样，使用 org.apache.flink.util.Collector 的用户定义函数（例如 FlatMapFunction 或 ProcessFunction）可以通过提供一个模拟对象而不是真实的 Collector 来轻松测试。一个与 IncrementMapFunction 功能相同的 FlatMapFunction 可以进行如下单元测试。\nclass IncrementFlatMapFunctionTest extends FlatSpec with MockFactory { \u0026#34;IncrementFlatMapFunction\u0026#34; should \u0026#34;increment values\u0026#34; in { // instantiate your function  val incrementer : IncrementFlatMapFunction = new IncrementFlatMapFunction() val collector = mock[Collector[Integer]] //verify collector was called with the right output  (collector.collect _).expects(3) // call the methods that you have implemented  flattenFunction.flatMap(2, collector) } } 单元测试 有状态或及时的 UDF 和自定义操作符 测试一个用户定义函数的功能是比较困难的，因为它涉及到测试用户代码和 Flink 运行时之间的交互。为此，Flink 提供了一个所谓的测试线束的集合，它可以用来测试这样的用户定义函数以及自定义操作符。\n OneInputStreamOperatorTestHarness(用于 DataStreams 上的操作符) KeyedOneInputStreamOperatorTestHarness(用于 KeyedStreams 上的操作者) TwoInputStreamOperatorTestHarness (适用于两个 DataStreams 的 ConnectedStreams 操作者) KeyedTwoInputStreamOperatorTestHarness (用于两个 KeyedStream 的 ConnectedStreams 上的操作员)  为了使用测试套件，需要一组额外的依赖关系（测试范围）。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-test-utils_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-runtime_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;classifier\u0026gt;tests\u0026lt;/classifier\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-streaming-java_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;classifier\u0026gt;tests\u0026lt;/classifier\u0026gt; \u0026lt;/dependency\u0026gt; 现在，测试线束可以用来将记录和水印推送到你的用户定义函数或自定义运算符中，控制处理时间，最后对运算符的输出进行断言（包括侧输出）。\nclass StatefulFlatMapFunctionTest extends FlatSpec with Matchers with BeforeAndAfter { private var testHarness: OneInputStreamOperatorTestHarness[Long, Long] = null private var statefulFlatMap: StatefulFlatMapFunction = null before { //instantiate user-defined function  statefulFlatMap = new StatefulFlatMap // wrap user defined function into a the corresponding operator  testHarness = new OneInputStreamOperatorTestHarness[Long, Long](new StreamFlatMap(statefulFlatMap)) // optionally configured the execution environment  testHarness.getExecutionConfig().setAutoWatermarkInterval(50); // open the test harness (will also call open() on RichFunctions)  testHarness.open(); } \u0026#34;StatefulFlatMap\u0026#34; should \u0026#34;do some fancy stuff with timers and state\u0026#34; in { //push (timestamped) elements into the operator (and hence user defined function)  testHarness.processElement(2, 100); //trigger event time timers by advancing the event time of the operator with a watermark  testHarness.processWatermark(100); //trigger proccesign time timers by advancing the processing time of the operator directly  testHarness.setProcessingTime(100); //retrieve list of emitted records for assertions  testHarness.getOutput should contain (3) //retrieve list of records emitted to a specific side output for assertions (ProcessFunction only)  //testHarness.getSideOutput(new OutputTag[Int](\u0026#34;invalidRecords\u0026#34;)) should have size 0  } } KeyedOneInputStreamOperatorTestHarness 和 KeyedTwoInputStreamOperatorTestHarness 是通过额外提供一个包括键类的 TypeInformation 的 KeySelector 来实例化的。\nclass StatefulFlatMapTest extends FlatSpec with Matchers with BeforeAndAfter { private var testHarness: OneInputStreamOperatorTestHarness[String, Long, Long] = null private var statefulFlatMapFunction: FlattenFunction = null before { //instantiate user-defined function  statefulFlatMapFunction = new StateFulFlatMap // wrap user defined function into a the corresponding operator  testHarness = new KeyedOneInputStreamOperatorTestHarness(new StreamFlatMap(statefulFlatMapFunction),new MyStringKeySelector(), Types.STRING()) // open the test harness (will also call open() on RichFunctions)  testHarness.open(); } //tests  } 在 Flink 代码库中还可以找到更多使用这些测试线束的例子，例如。\n org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest 是一个很好的例子，用于测试依赖于处理或事件时间的操作员和用户定义的函数。 org.apache.flink.streaming.api.function.sink.filesystem.LocalStreamingFileSinkTest 展示了如何使用 AbstractStreamOperatorTestHarness 测试自定义的 sink。具体来说，它使用 AbstractStreamOperatorTestHarness.snapshot 和 AbstractStreamOperatorTestHarness.initializeState 来测试它与 Flink 的检查点机制的交互。  注意: AbstractStreamOperatorTestHarness 和它的派生类目前不是公共 API 的一部分，可能会发生变化。\n单元测试 ProcessFunction 鉴于其重要性，除了之前的测试线束可以直接用于测试 ProcessFunction 外，Flink 还提供了一个名为 ProcessFunctionTestHarnesses 的测试线束工厂，可以更方便地进行测试线束实例化。考虑到这个例子。\n注意: 要使用这个测试线束，你还需要引入上一节中提到的依赖关系。\nclass PassThroughProcessFunction extends ProcessFunction[Integer, Integer] { @throws[Exception] override def processElement(value: Integer, ctx: ProcessFunction[Integer, Integer]#Context, out: Collector[Integer]): Unit = { out.collect(value) } } 使用 ProcessFunctionTestHarnesses 对这样的函数进行单元测试是非常容易的，通过传递合适的参数并验证输出。\nclass PassThroughProcessFunctionTest extends FlatSpec with Matchers { \u0026#34;PassThroughProcessFunction\u0026#34; should \u0026#34;forward values\u0026#34; in { //instantiate user-defined function  val processFunction = new PassThroughProcessFunction // wrap user defined function into a the corresponding operator  val harness = ProcessFunctionTestHarnesses.forProcessFunction(processFunction) //push (timestamped) elements into the operator (and hence user defined function)  harness.processElement(1, 10) //retrieve list of emitted records for assertions  harness.extractOutputValues() should contain (1) } } 关于如何使用 ProcessFunctionTestHarnesses 来测试 ProcessFunction 的不同风味，如 KeyedProcessFunction、KeyedCoProcessFunction、BroadcastProcessFunction 等的更多例子，鼓励用户查看 ProcessFunctionTestHarnessesTest。\n测试 Flink 作业 JUnit 规则 MiniClusterWithClientResource Apache Flink 提供了一个名为 MiniClusterWithClientResource 的 JUnit 规则，用于针对本地的、嵌入式的迷你集群测试完整的作业，名为 MiniClusterWithClientResource。\n要使用 MiniClusterWithClientResource，需要一个额外的依赖（测试范围）。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-test-utils_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 让我们以前面几节中同样简单的 MapFunction 为例。\nclass IncrementMapFunction extends MapFunction[Long, Long] { override def map(record: Long): Long = { record + 1 } } 现在可以在本地 Flink 集群中测试使用该 MapFunction 的简单管道，具体如下。\nclass StreamingJobIntegrationTest extends FlatSpec with Matchers with BeforeAndAfter { val flinkCluster = new MiniClusterWithClientResource(new MiniClusterResourceConfiguration.Builder() .setNumberSlotsPerTaskManager(1) .setNumberTaskManagers(1) .build) before { flinkCluster.before() } after { flinkCluster.after() } \u0026#34;IncrementFlatMapFunction pipeline\u0026#34; should \u0026#34;incrementValues\u0026#34; in { val env = StreamExecutionEnvironment.getExecutionEnvironment // configure your test environment  env.setParallelism(2) // values are collected in a static variable  CollectSink.values.clear() // create a stream of custom elements and apply transformations  env.fromElements(1, 21, 22) .map(new IncrementMapFunction()) .addSink(new CollectSink()) // execute  env.execute() // verify your results  CollectSink.values should contain allOf (2, 22, 23) } } // create a testing sink class CollectSink extends SinkFunction[Long] { override def invoke(value: Long): Unit = { synchronized { CollectSink.values.add(value) } } } object CollectSink { // must be static  val values: util.List[Long] = new util.ArrayList() } 关于 MiniClusterWithClientResource 的集成测试的几点说明。\n  为了不把你的整个流水线代码从生产中复制到测试中，请在你的生产代码中使源和汇可插拔，并在你的测试中注入特殊的测试源和测试汇。\n  这里使用了 CollectSink 中的静态变量，因为 Flink 在将所有操作符分布在集群中之前，会将它们序列化。通过静态变量与本地 Flink 迷你集群实例化的运算符进行通信是解决这个问题的一种方法。另外，你可以将数据写到与你的测试汇的临时目录中的文件中。\n  如果你的作业使用事件时间计时器，你可以实现一个自定义的并行源函数来发射水印。\n  建议始终以并行度 \u0026gt;1 的方式在本地测试你的流水线，以识别只有并行执行的流水线才会出现的错误。\n  优先选择 @ClassRule 而不是 @Rule，这样多个测试可以共享同一个 Flink 集群。这样做可以节省大量的时间，因为 Flink 集群的启动和关闭通常会支配实际测试的执行时间。\n  如果你的管道包含自定义状态处理，你可以通过启用检查点并在迷你集群内重新启动作业来测试其正确性。为此，你需要通过从你的管道中的（仅测试的）用户定义函数中抛出一个异常来触发失败。\n  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-testing/","tags":["Flink","Flink 官方文档","DataStream API","testing"],"title":"测试"},{"categories":["Flink"],"contents":"状态后端 Flink 提供了不同的状态后端，指定状态的存储方式和位置。\n状态可以位于 Java 的堆上或离堆(off-heap)。根据你的状态后端，Flink 还可以为应用程序管理状态，这意味着 Flink 处理内存管理（必要时可能会溢出到磁盘），以允许应用程序持有非常大的状态。默认情况下，配置文件 flink-conf.yaml 决定了所有 Flink 作业(job)的状态后端。\n然而，默认的状态后端可以在每个作业(per-job)的基础上被重写，如下所示。\n有关可用的状态后端、其优势、限制和配置参数的更多信息，请参见部署与操作中的相应章节。\nval env = StreamExecutionEnvironment.getExecutionEnvironment() env.setStateBackend(...) 状态后端: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state_backends.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-state-backends/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"状态后端"},{"categories":["Flink"],"contents":"Apache Flink 流媒体应用通常被设计为无限期或长时间运行。与所有长期运行的服务一样，应用程序需要更新以适应不断变化的需求。这对于应用程序所针对的数据模式(data schema)也是一样的，它们会随着应用程序的发展而发展。\n本页提供了关于如何演进状态类型的数据模式(data schema)的概述。当前的限制在不同的类型和状态结构（ValueState、ListState 等）中有所不同。\n请注意，本页面上的信息仅在您使用由 Flink 自己的类型序列化框架生成的状态序列化器时相关。也就是说，在声明你的状态时，所提供的状态描述符并没有被配置为使用特定的 TypeSerializer 或 TypeInformation，在这种情况下，Flink 会推导出状态类型的信息。\nListStateDescriptor\u0026lt;MyPojoType\u0026gt; descriptor = new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;state-name\u0026#34;, MyPojoType.class); checkpointedState = getRuntimeContext().getListState(descriptor); 在底层，状态的模式(schema)是否可以被演化取决于用于读取/写入持久化状态字节的序列化器。简单地说，只有当它的序列化器正确地支持时，一个注册状态的模式才能被演化。这是由 Flink 的类型序列化框架生成的序列化器透明地处理的（当前的支持范围列在下面）。\n如果你打算为你的状态类型实现一个自定义的 TypeSerializer，并想了解如何实现序列化器以支持状态模式演化，请参考自定义状态序列化。那里的文档还涵盖了关于状态序列化器和 Flink 的状态后端之间的相互作用的必要内部细节，以支持状态模式(state schema)演化。\n状态模式的演化 要演化给定状态类型的模式，您需要采取以下步骤。\n 保存你的 Flink 流作业(job)的保存点。 更新您的应用程序中的状态类型（例如，修改您的 Avro 类型模式）。 从保存点恢复作业(job)。当第一次访问状态时，Flink 将评估是否已经改变了状态的模式(schema)，并在必要时迁移状态模式。  迁移状态以适应已更改的模式的过程是自动发生的，并且对每个状态都是独立的。这个过程由 Flink 内部执行，首先检查状态的新序列器是否与之前的序列器有不同的序列化模式，如果有，则用之前的序列器将状态读到对象，再用新的序列器写回字节。\n关于迁移过程的进一步细节不在本文档的范围内，请参考这里。\n支持的模式演化数据类型 目前，模式演化只支持 POJO 和 Avro 类型。因此，如果你关心状态的模式演化，目前建议始终使用 POJO 或 Avro 作为状态数据类型。\n有计划扩展对更多复合类型的支持；更多细节请参考 FLINK-10896。\nPOJO 类型 Flink 支持 POJO 类型的演化模式，基于以下一组规则。\n 字段可以被删除。一旦被删除，在未来的检查点和保存点中，被删除字段的之前值将被丢弃。 可以添加新字段。新字段将被初始化为其类型的默认值，正如 Java 所定义的那样。 已声明的字段类型不能改变。 POJO 类型的类名不能改变，包括类的命名空间。  请注意，POJO 类型状态的模式只能在 Flink 版本大于 1.8.0 的情况下，从以前的保存点恢复时才能进化。当使用比 1.8.0 更老的 Flink 版本进行还原时，模式不能被改变。\nAvro 类型 Flink 完全支持 Avro 类型状态的演变模式，只要模式变化被 Avro 的模式解析规则认为是兼容的。\n一个限制是作为状态类型使用的 Avro 生成的类在恢复作业时不能被重新定位或具有不同的命名空间。\n注意: 不支持键的模式演变。\n举个例子。RocksDB 状态后端依赖于二进制对象的标识，而不是 hashCode 方法实现。对 keys 对象结构的任何改变都可能导致非确定性行为。\n注意: Kryo 不能用于模式演化。\n当使用 Kryo 时，框架没有可能验证是否有任何不兼容的变化。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/schema_evolution.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-state-schema-evolution/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"状态模式的演变"},{"categories":["Flink"],"contents":"本页解释了如何使用 Flink 的 API 与外部数据存储进行异步 I/O。对于不熟悉异步或事件驱动编程的用户来说，一篇关于 Futures 和事件驱动编程的文章可能是有用的准备。\n注：关于异步 I/O 实用程序的设计和实现的细节可以在提案和设计文件 FLIP-12：异步I/O设计和实现中找到。\n异步I/O操作的必要性 在与外部系统交互时（例如用存储在数据库中的数据来丰富流事件时），需要注意与外部系统的通信延迟不会主导流应用的总工作。\n奈何访问外部数据库中的数据，例如在 MapFunction 中，通常意味着同步交互。一个请求被发送到数据库，MapFunction 等待直到收到响应。在许多情况下，这种等待占据了函数的绝大部分时间。\n与数据库的异步交互意味着一个并行函数实例可以同时处理许多请求，并同时接收响应。这样一来，等待时间就可以与发送其他请求和接收响应叠加起来。最起码，等待时间可以摊在多个请求上。这在大多数情况下会导致更高的流吞吐量。\n注意：通过仅仅将 MapFunction 扩展到很高的并行度来提高吞吐量，在某些情况下也是可行的，但通常要付出很高的资源代价：拥有更多的并行 MapFunction 实例意味着更多的任务、线程、Flink 内部网络连接、与数据库的网络连接、缓冲区以及一般的内部记账开销。\n前提条件 如上节所述，要实现对数据库（或键/值存储）的适当异步 I/O，需要向该数据库提供一个支持异步请求的客户端。许多流行的数据库都提供了这样的客户端。\n在没有这样的客户端的情况下，可以尝试通过创建多个客户端，并用线程池处理同步调用，将同步客户端变成有限的并发客户端。然而，这种方法通常比一个合适的异步客户端效率低。\n异步 I/O API Flink 的 Async I/O API 允许用户使用异步请求客户端与数据流。该 API 处理与数据流的集成，以及处理顺序、事件时间、容错等。\n假设自己有一个目标数据库的异步客户端，需要三个部分来实现对数据库的异步 I/O 的流转换。\n 一个 AsyncFunction 的实现，用来调度请求。 一个回调，获取操作结果并将其交给 ResultFuture。 在 DataStream 上应用异步 I/O 操作作为转换。  下面的代码示例说明了基本模式。\n/** * An implementation of the \u0026#39;AsyncFunction\u0026#39; that sends requests and sets the callback. */ class AsyncDatabaseRequest extends AsyncFunction[String, (String, String)] { /** The database specific client that can issue concurrent requests with callbacks */ lazy val client: DatabaseClient = new DatabaseClient(host, post, credentials) /** The context used for the future callbacks */ implicit lazy val executor: ExecutionContext = ExecutionContext.fromExecutor(Executors.directExecutor()) override def asyncInvoke(str: String, resultFuture: ResultFuture[(String, String)]): Unit = { // issue the asynchronous request, receive a future for the result  val resultFutureRequested: Future[String] = client.query(str) // set the callback to be executed once the request by the client is complete  // the callback simply forwards the result to the result future  resultFutureRequested.onSuccess { case result: String =\u0026gt; resultFuture.complete(Iterable((str, result))) } } } // create the original stream val stream: DataStream[String] = ... // apply the async I/O transformation val resultStream: DataStream[(String, String)] = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100) 重要提示：ResultFuture.complete 的第一次调用就完成了。所有后续的完成调用将被忽略。\n以下两个参数控制异步操作。\n  超时: 超时定义了异步请求在被认为失败之前可能需要的时间。这个参数可以防范死机/失败的请求。\n  Capacity（容量）：该参数定义了异步请求在被认为失败之前可能需要的时间。这个参数定义了多少个异步请求可以同时进行。尽管异步I/O方法通常会带来更好的吞吐量，但操作者仍然可以成为流应用的瓶颈。限制并发请求的数量可以确保操作者不会积累越来越多的待处理请求的积压，但一旦容量耗尽，就会触发背压。\n  超时处理 当一个异步 I/O 请求超时时，默认情况下会抛出一个异常并重新启动作业。如果你想处理超时，你可以重写 AsyncFunction#timeout 方法。\n结果的顺序 AsyncFunction 发出的并发请求经常以某种未定义的顺序完成，基于哪个请求先完成。为了控制结果记录以何种顺序发出，Flink 提供了两种模式。\n  Unordered: 异步请求一结束，结果记录就会被发出。在异步 I/O 操作符之后，流中记录的顺序与之前不同。这种模式以处理时间为基本时间特性时，延迟最低，开销最小。使用 AsyncDataStream.unorderedWait(...) 来实现这种模式。\n  Ordered: 在这种情况下，流的顺序被保留下来。结果记录的发出顺序与异步请求被触发的顺序相同（运算符输入记录的顺序）。为了达到这个目的，操作符会缓冲一个结果记录，直到它前面的所有记录都被发出来（或定时发出来）。这通常会在检查点中引入一些额外的延迟和一些开销，因为与无序模式相比，记录或结果在检查点状态下维持的时间更长。使用 AsyncDataStream.orderedWait(...) 来处理这种模式。\n  事件时间 当流媒体应用程序使用事件时间工作时，水印将由异步 I/O 操作符正确处理。具体来说，这意味着两种顺序模式的以下内容。\n 无序的：水印不会超越记录，反之亦然，这意味着水印会建立一个顺序边界。只有在水印之间才会发出无序的记录。发生在某一水印之后的记录，只有在该水印被发射之后才会被发射。而水印则只有在该水印之前的所有输入的结果记录被发出之后才会被发出。  这意味着在有水印的情况下，无序模式会引入一些与有序模式相同的延迟和管理开销。该开销的数量取决于水印的频率。\n 有序的: 水印和记录的顺序被保留下来 就像记录之间的顺序被保留一样 与处理时间相比，开销没有明显变化。  请记住，摄取时间是事件时间的一种特殊情况，其自动生成的水印是基于源处理时间的。\n容错保证 异步 I/O 操作符提供了完全精确的一次容错保证，它将飞行中的异步请求记录存储在检查点中，并在故障恢复时恢复/重新触发请求。它将飞行中的异步请求记录存储在检查点中，并在从故障中恢复时恢复/重新触发请求。\n实现技巧 对于有 Executor（或 Scala 中的 ExecutionContext）用于回调的 Futures 实现，我们建议使用 DirectExecutor，因为回调通常只做最少的工作，而且DirectExecutor 避免了额外的线程间交接开销。回调通常只将结果交给 ResultFuture，后者将其添加到输出缓冲区。从那里开始，包括记录排放和与检查点记账的交互在内的繁重逻辑无论如何都发生在一个专用线程池中。\n可以通过 org.apache.flink.runtime.concurrent.Executors.directExecutor() 或 com.google.common.util.concurrent.MoreExecutors.directExecutor() 获得 DirectExecutor。\n注意事项 AsyncFunction 不叫多线程。\n我们在这里要明确指出的一个常见的困惑是，AsyncFunction 不是以多线程的方式调用的。AsyncFunction 只存在一个实例，并且对于流的各个分区中的每一条记录，它都会被依次调用。除非 asyncInvoke(...) 方法快速返回并依赖于回调（由客户端），否则不会导致正确的异步 I/O。\n例如，以下模式会导致阻塞 asyncInvoke(...) 函数，从而使异步行为无效。\n  使用一个数据库客户端，其查找/查询方法的调用会被阻塞，直到结果被接收回来为止\n  在 asyncInvoke(...) 方法中阻止/等待异步客户端返回的未来型对象。\n  出于一致性的考虑，AsyncFunction 的操作符（AsyncWaitOperator）目前必须位于操作符链的头部。\n由于在 FLINK-13063 问题中给出的原因，我们目前必须打破 AsyncWaitOperator 的操作符链，以防止潜在的一致性问题。这是对以前支持链的行为的改变。需要旧行为并接受潜在的违反一致性保证的用户可以手动实例化并将 AsyncWaitOperator 添加到作业图中，并通过 AsyncWaitOperator#setChainingStrategy(ChainingStrategy.ALWAYS) 将链式策略设置回链式。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-asynchronous-io-for-external-data-access/","tags":["Flink","Flink 官方文档","DataStream API","Operators","IO"],"title":"用于外部数据访问的异步 I/O"},{"categories":["Flink"],"contents":"用户自定义函数 用户自定义函数(UDFs)是扩展点，用于调用常用的逻辑或自定义逻辑，这些逻辑无法在查询中以其他方式表达。\n用户定义函数可以用 JVM 语言（如 Java 或 Scala）或 Python 实现。实现者可以在 UDF 中使用任意的第三方库。本页将重点介绍基于 JVM 的语言。\n概述 目前，Flink 区分了以下几种函数。\n 标量函数将标量值映射到一个新的标量值。 表函数将标量值映射到新的行(row)。 聚合函数将多行的标量值映射到新的标量值。 表聚合函数将多行的标量值映射到新的行上。 异步表函数是针对 table source 执行查找的特殊函数。  注意: 标量函数和表函数已经更新为基于数据类型的新类型系统。聚合函数仍然使用基于 TypeInformation 的旧类型系统。\n下面的示例展示了如何创建一个简单的标量函数，以及如何在表 API 和 SQL 中调用该函数。\n对于 SQL 查询，一个函数必须始终以一个名字注册。对于 Table API，函数可以被注册，也可以直接内联使用。\nimport org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction // define function logic class SubstringFunction extends ScalarFunction { def eval(s: String, begin: Integer, end: Integer): String = { s.substring(begin, end) } } val env = TableEnvironment.create(...) // call function \u0026#34;inline\u0026#34; without registration in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(classOf[SubstringFunction], $\u0026#34;myField\u0026#34;, 5, 12)) // register function env.createTemporarySystemFunction(\u0026#34;SubstringFunction\u0026#34;, classOf[SubstringFunction]) // call registered function in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(\u0026#34;SubstringFunction\u0026#34;, $\u0026#34;myField\u0026#34;, 5, 12)) // call registered function in SQL env.sqlQuery(\u0026#34;SELECT SubstringFunction(myField, 5, 12) FROM MyTable\u0026#34;) 对于交互式会话，也可以在使用或注册函数之前对其进行参数化。在这种情况下，可以使用函数实例代替函数类作为临时函数。\n它要求参数是可序列化的，以便将函数实例运送到集群。\nimport org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction // define parameterizable function logic class SubstringFunction(val endInclusive) extends ScalarFunction { def eval(s: String, begin: Integer, end: Integer): String = { s.substring(endInclusive ? end + 1 : end) } } val env = TableEnvironment.create(...) // call function \u0026#34;inline\u0026#34; without registration in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(new SubstringFunction(true), $\u0026#34;myField\u0026#34;, 5, 12)) // register function env.createTemporarySystemFunction(\u0026#34;SubstringFunction\u0026#34;, new SubstringFunction(true)) 实现指南 注意：本节目前只适用于标量函数和表函数；在集合函数更新到新的类型系统之前，本节只适用于标量函数。\n无论函数的种类如何，所有用户定义的函数都遵循一些基本的实现原则。\n函数类 一个实现类必须从一个可用的基类(例如 org.apache.flink.table.function.ScalarFunction)中扩展出来。\n这个类必须被声明为 public，而不是 abstract，并且应该是全局访问的。因此，不允许使用非静态的内部类或匿名类。\n对于在持久化目录中存储用户定义的函数，该类必须有一个默认的构造函数，并且在运行时必须是可实例化的。\n评估方法 基类提供了一组可以重写的方法，如 open()、close() 或 isDeterministic()。\n然而，除了这些声明的方法外，应用于每个传入记录的主要运行时逻辑必须通过专门的评估方法来实现。\n根据函数种类的不同，评价方法如 eval()、accumulate() 或 retract() 会在运行时被代码生成的操作符调用。\n这些方法必须声明为 public，并接受一组定义明确的参数。\n常规的 JVM 方法调用语义适用。因此，可以\n 实现重载方法，如 eval(Integer) 和 eval(LocalDateTime)。 使用 var-args，如 eval(Integer...)。 使用对象继承，如 eval(Object)，它同时接受 LocalDateTime 和 Integer。 以及上述函数的组合，如 eval(Object...)，它可以接受所有类型的参数。  如果你打算在 Scala 中实现函数，请在使用变量参数时添加 scala.annotation.varargs 注解。此外，建议使用盒状基元（如用 java.lang.Integer 代替 Int）来支持 NULL。\n下面的代码段显示了一个重载函数的示例。\nimport org.apache.flink.table.functions.ScalarFunction import java.lang.Integer import java.lang.Double import scala.annotation.varargs // function with overloaded evaluation methods class SumFunction extends ScalarFunction { def eval(a: Integer, b: Integer): Integer = { a + b } def eval(a: String, b: String): Integer = { Integer.valueOf(a) + Integer.valueOf(b) } @varargs // generate var-args like Java  def eval(d: Double*): Integer = { d.sum.toInt } } 类型推断 表生态系统（类似于 SQL 标准）是一个强类型的 API。因此，函数参数和返回类型都必须映射到数据类型。\n从逻辑的角度来看，规划师需要关于预期类型、精度和规模的信息。从 JVM 的角度来看，规划师需要了解当调用用户定义的函数时，内部数据结构如何被表示为 JVM 对象。\n验证输入参数和推导出函数的参数和结果的数据类型的逻辑被总结在类型推理这个术语下。\nFlink 的用户定义函数实现了自动类型推理提取，通过反射从函数的类和它的评估方法中导出数据类型。如果这种隐式反射提取方法不成功，可以通过用 @DataTypeHint 和 @FunctionHint 注释受影响的参数、类或方法来支持提取过程。更多关于如何注释函数的例子如下所示。\n如果需要更高级的类型推理逻辑，实现者可以在每个用户定义的函数中显式覆盖 getTypeInference() 方法。然而，推荐使用注释方法，因为它将自定义类型推理逻辑保持在受影响的位置附近，并回落到其余实现的默认行为。\n自动类型推断 自动类型推理检查函数的类和评估方法，从而得出函数的参数和结果的数据类型。@DataTypeHint 和 @FunctionHint 注解支持自动提取。\n关于可以隐式映射到数据类型的类的完整列表，请参阅数据类型提取部分。\n@DataTypeHint 在很多情况下，需要支持对函数的参数和返回类型进行在线自动提取。\n下面的示例展示了如何使用数据类型提示。更多信息可以在注解类的文档中找到。\nimport org.apache.flink.table.annotation.DataTypeHint import org.apache.flink.table.annotation.InputGroup import org.apache.flink.table.functions.ScalarFunction import org.apache.flink.types.Row import scala.annotation.varargs // function with overloaded evaluation methods class OverloadedFunction extends ScalarFunction { // no hint required  def eval(a: Long, b: Long): Long = { a + b } // define the precision and scale of a decimal  @DataTypeHint(\u0026#34;DECIMAL(12, 3)\u0026#34;) def eval(double a, double b): BigDecimal = { java.lang.BigDecimal.valueOf(a + b) } // define a nested data type  @DataTypeHint(\u0026#34;ROW\u0026lt;s STRING, t TIMESTAMP(3) WITH LOCAL TIME ZONE\u0026gt;\u0026#34;) def eval(Int i): Row = { Row.of(java.lang.String.valueOf(i), java.time.Instant.ofEpochSecond(i)) } // allow wildcard input and customly serialized output  @DataTypeHint(value = \u0026#34;RAW\u0026#34;, bridgedTo = classOf[java.nio.ByteBuffer]) def eval(@DataTypeHint(inputGroup = InputGroup.ANY) Object o): java.nio.ByteBuffer = { MyUtils.serializeToByteBuffer(o) } } @FunctionHint 在某些场景下，一个评估方法同时处理多种不同的数据类型是可取的。此外，在某些场景中，重载的评估方法有一个共同的结果类型，应该只声明一次。\n@FunctionHint 注解可以提供从参数数据类型到结果数据类型的映射。它可以为输入、累加器和结果数据类型注释整个函数类或评估方法。一个或多个注解可以在一个类的顶部声明，也可以为每个评估方法单独声明，以便重载函数签名。所有的提示参数都是可选的。如果没有定义参数，则使用默认的基于反射的提取方式。在函数类之上定义的提示参数会被所有的评估方法继承。\n下面的例子展示了如何使用函数提示。更多信息可以在注解类的文档中找到。\nimport org.apache.flink.table.annotation.DataTypeHint import org.apache.flink.table.annotation.FunctionHint import org.apache.flink.table.functions.TableFunction import org.apache.flink.types.Row // function with overloaded evaluation methods // but globally defined output type @FunctionHint(output = new DataTypeHint(\u0026#34;ROW\u0026lt;s STRING, i INT\u0026gt;\u0026#34;)) class OverloadedFunction extends TableFunction[Row] { def eval(a: Int, b: Int): Unit = { collect(Row.of(\u0026#34;Sum\u0026#34;, Int.box(a + b))) } // overloading of arguments is still possible  def eval(): Unit = { collect(Row.of(\u0026#34;Empty args\u0026#34;, Int.box(-1))) } } // decouples the type inference from evaluation methods, // the type inference is entirely determined by the function hints @FunctionHint( input = Array(new DataTypeHint(\u0026#34;INT\u0026#34;), new DataTypeHint(\u0026#34;INT\u0026#34;)), output = new DataTypeHint(\u0026#34;INT\u0026#34;) ) @FunctionHint( input = Array(new DataTypeHint(\u0026#34;BIGINT\u0026#34;), new DataTypeHint(\u0026#34;BIGINT\u0026#34;)), output = new DataTypeHint(\u0026#34;BIGINT\u0026#34;) ) @FunctionHint( input = Array(), output = new DataTypeHint(\u0026#34;BOOLEAN\u0026#34;) ) class OverloadedFunction extends TableFunction[AnyRef] { // an implementer just needs to make sure that a method exists  // that can be called by the JVM  @varargs def eval(o: AnyRef*) = { if (o.length == 0) { collect(Boolean.box(false)) } collect(o(0)) } } 自定义类型推断 对于大多数情况下，@DataTypeHint 和 @FunctionHint 应该足以为用户定义的函数建模。然而，通过覆盖 getTypeInference() 中定义的自动类型推理，实现者可以创建任意的函数，这些函数的行为就像内置的系统函数一样。\n下面这个用 Java 实现的例子说明了自定义类型推理逻辑的潜力。它使用一个字符串文字参数来确定一个函数的结果类型。该函数需要两个字符串参数：第一个参数代表要解析的字符串，第二个参数代表目标类型。\nimport org.apache.flink.table.api.DataTypes; import org.apache.flink.table.catalog.DataTypeFactory; import org.apache.flink.table.functions.ScalarFunction; import org.apache.flink.table.types.inference.TypeInference; import org.apache.flink.types.Row; public static class LiteralFunction extends ScalarFunction { public Object eval(String s, String type) { switch (type) { case \u0026#34;INT\u0026#34;: return Integer.valueOf(s); case \u0026#34;DOUBLE\u0026#34;: return Double.valueOf(s); case \u0026#34;STRING\u0026#34;: default: return s; } } // the automatic, reflection-based type inference is disabled and  // replaced by the following logic  @Override public TypeInference getTypeInference(DataTypeFactory typeFactory) { return TypeInference.newBuilder() // specify typed arguments  // parameters will be casted implicitly to those types if necessary  .typedArguments(DataTypes.STRING(), DataTypes.STRING()) // specify a strategy for the result data type of the function  .outputTypeStrategy(callContext -\u0026gt; { if (!callContext.isArgumentLiteral(1) || callContext.isArgumentNull(1)) { throw callContext.newValidationError(\u0026#34;Literal expected for second argument.\u0026#34;); } // return a data type based on a literal  final String literal = callContext.getArgumentValue(1, String.class).orElse(\u0026#34;STRING\u0026#34;); switch (literal) { case \u0026#34;INT\u0026#34;: return Optional.of(DataTypes.INT().notNull()); case \u0026#34;DOUBLE\u0026#34;: return Optional.of(DataTypes.DOUBLE().notNull()); case \u0026#34;STRING\u0026#34;: default: return Optional.of(DataTypes.STRING()); } }) .build(); } } 运行时集成 有时可能需要用户自定义函数在实际工作前获取全局运行时信息或做一些设置/清理工作。用户自定义函数提供了 open() 和 close() 方法，这些方法可以被重写，并提供与 DataStream API 的 RichFunction 中的方法类似的功能。\nopen() 方法在评估方法之前被调用一次。close() 方法在最后一次调用评估方法后调用。\nopen() 方法提供了一个 FunctionContext，该 FunctionContext 包含了用户定义函数执行的上下文信息，如度量组、分布式缓存文件或全局作业参数。\n通过调用 FunctionContext 的相应方法，可以获得以下信息。\n   方法 描述     getMetricGroup() 该并行子任务的度量组。   getCachedFile(name) 分布式缓存文件的本地临时文件副本。   getJobParameter(name, defaultValue) 与给定键相关联的全局作业参数值。    下面的示例片段展示了如何在标量函数中使用 FunctionContext 来访问全局工作参数。\nimport org.apache.flink.table.api._ import org.apache.flink.table.functions.FunctionContext import org.apache.flink.table.functions.ScalarFunction class HashCodeFunction extends ScalarFunction { private var factor: Int = 0 override def open(context: FunctionContext): Unit = { // access the global \u0026#34;hashcode_factor\u0026#34; parameter  // \u0026#34;12\u0026#34; would be the default value if the parameter does not exist  factor = context.getJobParameter(\u0026#34;hashcode_factor\u0026#34;, \u0026#34;12\u0026#34;).toInt } def eval(s: String): Int = { s.hashCode * factor } } val env = TableEnvironment.create(...) // add job parameter env.getConfig.addJobParameter(\u0026#34;hashcode_factor\u0026#34;, \u0026#34;31\u0026#34;) // register the function env.createTemporarySystemFunction(\u0026#34;hashCode\u0026#34;, classOf[HashCodeFunction]) // use the function env.sqlQuery(\u0026#34;SELECT myField, hashCode(myField) FROM MyTable\u0026#34;) 标量函数 用户定义的标量函数可以将零、一或多个标量值映射到一个新的标量值。数据类型一节中列出的任何数据类型都可以作为一个评估方法的参数或返回类型。\n为了定义一个标量函数，必须扩展 org.apache.flink.table.function 中的基类 ScalarFunction，并实现一个或多个名为 eval(...) 的评估方法。\n下面的例子展示了如何定义自己的哈希码函数并在查询中调用它。更多细节请参见实施指南。\nimport org.apache.flink.table.annotation.InputGroup import org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction class HashFunction extends ScalarFunction { // take any data type and return INT  def eval(@DataTypeHint(inputGroup = InputGroup.ANY) o: AnyRef): Int { return o.hashCode(); } } val env = TableEnvironment.create(...) // call function \u0026#34;inline\u0026#34; without registration in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(classOf[HashFunction], $\u0026#34;myField\u0026#34;)) // register function env.createTemporarySystemFunction(\u0026#34;HashFunction\u0026#34;, classOf[HashFunction]) // call registered function in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(\u0026#34;HashFunction\u0026#34;, $\u0026#34;myField\u0026#34;)) // call registered function in SQL env.sqlQuery(\u0026#34;SELECT HashFunction(myField) FROM MyTable\u0026#34;) 如果你打算用 Python 实现或调用函数，请参考 Python Scalar Functions 文档了解更多细节。\n表函数 与用户定义的标量函数类似，用户定义的表函数将零、一个或多个标量值作为输入参数。然而，与标量函数不同的是，它可以返回任意数量的行（或结构化类型）作为输出，而不是单个值。返回的记录可能由一个或多个字段组成。如果一条输出记录只由一个字段组成，则可以省略结构化记录，并发出一个标量值。它将被运行时包装成一个隐式行。\n为了定义一个表函数，必须扩展 org.apache.flink.table.function 中的基类 TableFunction，并实现一个或多个名为 eval(...) 的评估方法。与其他函数类似，输入和输出数据类型也是使用反射自动提取的。这包括类的通用参数 T，用于确定输出数据类型。与标量函数不同的是，评价方法本身不能有返回类型，相反，表函数提供了一个 collect(T) 方法，可以在每个评价方法内调用，用于发出零、一条或多条记录。\n在表 API 中，表函数的使用方法是 .joinLateral(...) 或 .leftOuterJoinLateral(...)。joinLateral 运算符（cross）将外表（运算符左边的表）的每条记录与表值函数产生的所有记录（表值函数在运算符的右边）连接起来。leftOuterJoinLateral 操作符将外表（操作符左边的表）的每一条记录与表值函数产生的所有记录（它在操作符的右边）连接起来，并且保留那些表函数返回空表的外表。\n在 SQL 中，使用 LATERAL TABLE(\u0026lt;TableFunction\u0026gt;) 与 JOIN 或 LEFT JOIN 与 ON TRUE 连接条件。\n下面的示例展示了如何定义自己的拆分函数并在查询中调用它。更多细节请参见《实现指南》。\nimport org.apache.flink.table.annotation.DataTypeHint import org.apache.flink.table.annotation.FunctionHint import org.apache.flink.table.api._ import org.apache.flink.table.functions.TableFunction import org.apache.flink.types.Row @FunctionHint(output = new DataTypeHint(\u0026#34;ROW\u0026lt;word STRING, length INT\u0026gt;\u0026#34;)) class SplitFunction extends TableFunction[Row] { def eval(str: String): Unit = { // use collect(...) to emit a row  str.split(\u0026#34; \u0026#34;).foreach(s =\u0026gt; collect(Row.of(s, Int.box(s.length)))) } } val env = TableEnvironment.create(...) // call function \u0026#34;inline\u0026#34; without registration in Table API env .from(\u0026#34;MyTable\u0026#34;) .joinLateral(call(classOf[SplitFunction], $\u0026#34;myField\u0026#34;) .select($\u0026#34;myField\u0026#34;, $\u0026#34;word\u0026#34;, $\u0026#34;length\u0026#34;) env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(classOf[SplitFunction], $\u0026#34;myField\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;word\u0026#34;, $\u0026#34;length\u0026#34;) // rename fields of the function in Table API env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(classOf[SplitFunction], $\u0026#34;myField\u0026#34;).as(\u0026#34;newWord\u0026#34;, \u0026#34;newLength\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;newWord\u0026#34;, $\u0026#34;newLength\u0026#34;) // register function env.createTemporarySystemFunction(\u0026#34;SplitFunction\u0026#34;, classOf[SplitFunction]) // call registered function in Table API env .from(\u0026#34;MyTable\u0026#34;) .joinLateral(call(\u0026#34;SplitFunction\u0026#34;, $\u0026#34;myField\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;word\u0026#34;, $\u0026#34;length\u0026#34;) env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(\u0026#34;SplitFunction\u0026#34;, $\u0026#34;myField\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;word\u0026#34;, $\u0026#34;length\u0026#34;) // call registered function in SQL env.sqlQuery( \u0026#34;SELECT myField, word, length \u0026#34; + \u0026#34;FROM MyTable, LATERAL TABLE(SplitFunction(myField))\u0026#34;); env.sqlQuery( \u0026#34;SELECT myField, word, length \u0026#34; + \u0026#34;FROM MyTable \u0026#34; + \u0026#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) ON TRUE\u0026#34;) // rename fields of the function in SQL env.sqlQuery( \u0026#34;SELECT myField, newWord, newLength \u0026#34; + \u0026#34;FROM MyTable \u0026#34; + \u0026#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) AS T(newWord, newLength) ON TRUE\u0026#34;) 如果你打算在 Scala 中实现函数，不要将表函数实现为 Scala 对象。Scala 对象是单子，会导致并发问题。\n如果你打算用 Python 实现或调用函数，请参考 Python 表函数文档了解更多细节。\n聚合函数 用户自定义聚合函数（UDAGG）将一个表（一个或多个具有一个或多个属性的行）聚合成一个标量值。\n上图显示了一个聚合的例子。假设你有一个包含饮料数据的表。该表由 id、名称和价格三列和 5 行组成。想象一下，你需要找到表中所有饮料的最高价格，即执行 max() 聚合。你需要对 5 行中的每一行进行检查，结果将是一个单一的数值。\n用户定义的聚合函数是通过扩展 AggregateFunction 类来实现的。AggregateFunction 的工作原理如下。首先，它需要一个累加器，它是存放聚合中间结果的数据结构。通过调用 AggregateFunction 的 createAccumulator() 方法创建一个空的累加器。随后，函数的 accumulate() 方法对每一条输入行进行调用，以更新累加器。一旦所有的行都被处理完毕，函数的 getValue() 方法就会被调用来计算并返回最终结果。\n以下方法是每个 AggregateFunction 必须使用的。\n createAccumulator() accumulate() getValue()  Flink 的类型提取设施可能无法识别复杂的数据类型，例如，如果它们不是基本类型或简单的 POJOs。所以与 ScalarFunction 和 TableFunction 类似，AggregateFunction 提供了指定结果类型（通过 AggregateFunction#getResultType()）和累加器类型（通过 AggregateFunction#getAccumulatorType()）的方法。\n除了上述方法外，还有一些签约方法可以选择实现。这些方法中的一些方法可以让系统更高效地执行查询，而另一些方法则是某些用例所必须的。例如，如果聚合函数应该在会话组窗口的上下文中应用，那么 merge() 方法是强制性的（当观察到有一行 \u0026ldquo;连接 \u0026ldquo;它们时，需要将两个会话窗口的累加器连接起来）。\nAggregateFunction 的以下方法是根据用例需要的。\n retract() 对于有界 OVER 窗口上的聚合是需要的。 merge() 是许多批次聚合和会话窗口聚合所需要的。 resetAccumulator() 是许多批处理聚合所需要的。  AggregateFunction 的所有方法都必须声明为 public，而不是 static，并且命名与上述名称完全一致。方法 createAccumulator、getValue、getResultType 和 getAccumulatorType 是在 AggregateFunction 抽象类中定义的，而其他方法则是合同方法。为了定义一个聚合函数，必须扩展基类 org.apache.flink.table.function.AggregateFunction，并实现一个（或多个）accumulate 方法。方法 accumulate 可以用不同的参数类型重载，并支持变量参数。\n下面给出了 AggregateFunction 所有方法的详细文档。\n/** * Base class for user-defined aggregates and table aggregates. * * @tparam T the type of the aggregation result. * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. */ abstract class UserDefinedAggregateFunction[T, ACC] extends UserDefinedFunction { /** * Creates and init the Accumulator for this (table)aggregate function. * * @return the accumulator with the initial value */ def createAccumulator(): ACC // MANDATORY  /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s result. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s result or null if the result * type should be automatically inferred. */ def getResultType: TypeInformation[T] = null // PRE-DEFINED  /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s accumulator. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s accumulator or null if the * accumulator type should be automatically inferred. */ def getAccumulatorType: TypeInformation[ACC] = null // PRE-DEFINED } /** * Base class for aggregation functions. * * @tparam T the type of the aggregation result * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. * AggregateFunction represents its state using accumulator, thereby the state of the * AggregateFunction must be put into the accumulator. */ abstract class AggregateFunction[T, ACC] extends UserDefinedAggregateFunction[T, ACC] { /** * Processes the input values and update the provided accumulator instance. The method * accumulate can be overloaded with different custom types and arguments. An AggregateFunction * requires at least one accumulate() method. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ def accumulate(accumulator: ACC, [user defined inputs]): Unit // MANDATORY  /** * Retracts the input values from the accumulator instance. The current design assumes the * inputs are the values that have been previously accumulated. The method retract can be * overloaded with different custom types and arguments. This function must be implemented for * datastream bounded over aggregate. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ def retract(accumulator: ACC, [user defined inputs]): Unit // OPTIONAL  /** * Merges a group of accumulator instances into one accumulator instance. This function must be * implemented for datastream session window grouping aggregate and dataset grouping aggregate. * * @param accumulator the accumulator which will keep the merged aggregate results. It should * be noted that the accumulator may contain the previous aggregated * results. Therefore user should not replace or clean this instance in the * custom merge method. * @param its an [[java.lang.Iterable]] pointed to a group of accumulators that will be * merged. */ def merge(accumulator: ACC, its: java.lang.Iterable[ACC]): Unit // OPTIONAL  /** * Called every time when an aggregation result should be materialized. * The returned value could be either an early and incomplete result * (periodically emitted as data arrive) or the final result of the * aggregation. * * @param accumulator the accumulator which contains the current * aggregated results * @return the aggregation result */ def getValue(accumulator: ACC): T // MANDATORY  /** * Resets the accumulator for this [[AggregateFunction]]. This function must be implemented for * dataset grouping aggregate. * * @param accumulator the accumulator which needs to be reset */ def resetAccumulator(accumulator: ACC): Unit // OPTIONAL  /** * Returns true if this AggregateFunction can only be applied in an OVER window. * * @return true if the AggregateFunction requires an OVER window, false otherwise. */ def requiresOver: Boolean = false // PRE-DEFINED } 下面的例子说明了如何进行\n 定义一个 AggregateFunction，用于计算给定列的加权平均值。 在 TableEnvironment 中注册该函数，并且 在查询中使用该函数。  为了计算加权平均值，累加器需要存储所有已积累的数据的加权和和计数。在我们的例子中，我们定义了一个类 WeightedAvgAccum 作为累加器。累积器由 Flink 的检查点机制自动备份，并在故障时恢复，以保证精确的唯一性语义。\n我们 WeightedAvg AggregateFunction 的 accumulate() 方法有三个输入。第一个是 WeightedAvgAccum 累加器，另外两个是用户自定义的输入：输入值 ivalue 和输入的权重 iweight。虽然 retract()、merge() 和 resetAccumulator() 方法对于大多数聚合类型来说并不是强制性的，但我们在下面提供它们作为例子。请注意，我们在 Scala 示例中使用了 Java 基元类型，并定义了 getResultType() 和 getAccumulatorType() 方法，因为 Flink 类型提取对于 Scala 类型并不十分有效。\nimport java.lang.{Long =\u0026gt; JLong, Integer =\u0026gt; JInteger} import org.apache.flink.api.java.tuple.{Tuple1 =\u0026gt; JTuple1} import org.apache.flink.api.java.typeutils.TupleTypeInfo import org.apache.flink.table.api.Types import org.apache.flink.table.functions.AggregateFunction /** * Accumulator for WeightedAvg. */ class WeightedAvgAccum extends JTuple1[JLong, JInteger] { sum = 0L count = 0 } /** * Weighted Average user-defined aggregate function. */ class WeightedAvg extends AggregateFunction[JLong, CountAccumulator] { override def createAccumulator(): WeightedAvgAccum = { new WeightedAvgAccum } override def getValue(acc: WeightedAvgAccum): JLong = { if (acc.count == 0) { null } else { acc.sum / acc.count } } def accumulate(acc: WeightedAvgAccum, iValue: JLong, iWeight: JInteger): Unit = { acc.sum += iValue * iWeight acc.count += iWeight } def retract(acc: WeightedAvgAccum, iValue: JLong, iWeight: JInteger): Unit = { acc.sum -= iValue * iWeight acc.count -= iWeight } def merge(acc: WeightedAvgAccum, it: java.lang.Iterable[WeightedAvgAccum]): Unit = { val iter = it.iterator() while (iter.hasNext) { val a = iter.next() acc.count += a.count acc.sum += a.sum } } def resetAccumulator(acc: WeightedAvgAccum): Unit = { acc.count = 0 acc.sum = 0L } override def getAccumulatorType: TypeInformation[WeightedAvgAccum] = { new TupleTypeInfo(classOf[WeightedAvgAccum], Types.LONG, Types.INT) } override def getResultType: TypeInformation[JLong] = Types.LONG } // register function val tEnv: StreamTableEnvironment = ??? tEnv.registerFunction(\u0026#34;wAvg\u0026#34;, new WeightedAvg()) // use function tEnv.sqlQuery(\u0026#34;SELECT user, wAvg(points, level) AS avgPoints FROM userScores GROUP BY user\u0026#34;) 表聚合函数 用户定义表聚合函数(UDTAGGs)将一个表(具有一个或多个属性的一行或多行)聚合到一个具有多行和多列的结果表。\n上图显示了一个表聚合的例子。假设你有一个包含饮料数据的表。该表由 id、名称和价格三列和 5 行组成。设想你需要找到表中所有饮料中价格最高的前 2 名，即执行 top2() 表聚合。你需要对 5 行中的每一行进行检查，结果将是一个具有前 2 个值的表。\n用户定义的表聚合函数是通过扩展 TableAggregateFunction 类来实现的。TableAggregateFunction 的工作原理如下。首先，它需要一个累加器，它是存放聚合中间结果的数据结构。通过调用 TableAggregateFunction 的 createAccumulator() 方法创建一个空的累加器。随后，对每一条输入行调用函数的 accumulate() 方法来更新累加器。一旦所有的行都被处理完毕，函数的 emitValue() 方法就会被调用来计算并返回最终结果。\n以下方法是每个 TableAggregateFunction 必须使用的。\n createAccumulator() accumulate()  Flink 的类型提取设施可能无法识别复杂的数据类型，例如，如果它们不是基本类型或简单的 POJOs。因此，与 ScalarFunction 和 TableFunction 类似，TableAggregateFunction 提供了指定结果类型（通过 TableAggregateFunction#getResultType()）和累积器类型（通过 TableAggregateFunction#getAccumulatorType()）的方法。\n除了上述方法外，还有一些签约方法可以选择实现。这些方法中的一些方法可以让系统更高效地执行查询，而另一些方法则是某些用例所必须的。例如，如果聚合函数应该在会话组窗口的上下文中应用，那么 merge() 方法是强制性的（当观察到有一条记录\u0026quot;连接\u0026quot;它们时，需要将两个会话窗口的累加器连接起来）。\nTableAggregateFunction 的以下方法是需要的，这取决于用例。\n retract() 对于有界 OVER 窗口上的聚合是需要的。 merge() 是许多批次聚合和会话窗口聚合所需要的。 resetAccumulator() 是许多批处理聚合所需要的。 emitValue() 是批处理和窗口聚合所需要的。  TableAggregateFunction 的以下方法用于提高流作业的性能。\n emitUpdateWithRetract() 用于发射在伸缩模式下更新的值。  对于 emitValue 方法，则是根据累加器来发射完整的数据。以 TopN 为例，emitValue 每次都会发射所有前 n 个值。这可能会给流式作业带来性能问题。为了提高性能，用户也可以实现 emitUpdateWithRetract 方法来提高性能。该方法以回缩模式增量输出数据，即一旦有更新，我们必须在发送新的更新记录之前回缩旧记录。如果在表聚合函数中都定义了该方法，那么该方法将优先于 emitValue 方法使用，因为 emitUpdateWithRetract 被认为比 emitValue 更有效率，因为它可以增量输出值。\nTableAggregateFunction 的所有方法都必须声明为 public，而不是 static，并完全按照上面提到的名字命名。方法 createAccumulator、getResultType 和 getAccumulatorType 是在 TableAggregateFunction 的父抽象类中定义的，而其他方法则是收缩的方法。为了定义一个表聚合函数，必须扩展基类 org.apache.flink.table.function.TableAggregateFunction，并实现一个（或多个）accumulate 方法。积累方法可以用不同的参数类型重载，并支持变量参数。\n下面给出了 TableAggregateFunction 所有方法的详细文档。\n/** * Base class for user-defined aggregates and table aggregates. * * @tparam T the type of the aggregation result. * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. */ abstract class UserDefinedAggregateFunction[T, ACC] extends UserDefinedFunction { /** * Creates and init the Accumulator for this (table)aggregate function. * * @return the accumulator with the initial value */ def createAccumulator(): ACC // MANDATORY  /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s result. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s result or null if the result * type should be automatically inferred. */ def getResultType: TypeInformation[T] = null // PRE-DEFINED  /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s accumulator. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s accumulator or null if the * accumulator type should be automatically inferred. */ def getAccumulatorType: TypeInformation[ACC] = null // PRE-DEFINED } /** * Base class for table aggregation functions. * * @tparam T the type of the aggregation result * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. * TableAggregateFunction represents its state using accumulator, thereby the state of * the TableAggregateFunction must be put into the accumulator. */ abstract class TableAggregateFunction[T, ACC] extends UserDefinedAggregateFunction[T, ACC] { /** * Processes the input values and update the provided accumulator instance. The method * accumulate can be overloaded with different custom types and arguments. A TableAggregateFunction * requires at least one accumulate() method. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ def accumulate(accumulator: ACC, [user defined inputs]): Unit // MANDATORY  /** * Retracts the input values from the accumulator instance. The current design assumes the * inputs are the values that have been previously accumulated. The method retract can be * overloaded with different custom types and arguments. This function must be implemented for * datastream bounded over aggregate. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ def retract(accumulator: ACC, [user defined inputs]): Unit // OPTIONAL  /** * Merges a group of accumulator instances into one accumulator instance. This function must be * implemented for datastream session window grouping aggregate and dataset grouping aggregate. * * @param accumulator the accumulator which will keep the merged aggregate results. It should * be noted that the accumulator may contain the previous aggregated * results. Therefore user should not replace or clean this instance in the * custom merge method. * @param its an [[java.lang.Iterable]] pointed to a group of accumulators that will be * merged. */ def merge(accumulator: ACC, its: java.lang.Iterable[ACC]): Unit // OPTIONAL  /** * Called every time when an aggregation result should be materialized. The returned value * could be either an early and incomplete result (periodically emitted as data arrive) or * the final result of the aggregation. * * @param accumulator the accumulator which contains the current * aggregated results * @param out the collector used to output data */ def emitValue(accumulator: ACC, out: Collector[T]): Unit // OPTIONAL  /** * Called every time when an aggregation result should be materialized. The returned value * could be either an early and incomplete result (periodically emitted as data arrive) or * the final result of the aggregation. * * Different from emitValue, emitUpdateWithRetract is used to emit values that have been updated. * This method outputs data incrementally in retract mode, i.e., once there is an update, we * have to retract old records before sending new updated ones. The emitUpdateWithRetract * method will be used in preference to the emitValue method if both methods are defined in the * table aggregate function, because the method is treated to be more efficient than emitValue * as it can outputvalues incrementally. * * @param accumulator the accumulator which contains the current * aggregated results * @param out the retractable collector used to output data. Use collect method * to output(add) records and use retract method to retract(delete) * records. */ def emitUpdateWithRetract(accumulator: ACC, out: RetractableCollector[T]): Unit // OPTIONAL  /** * Collects a record and forwards it. The collector can output retract messages with the retract * method. Note: only use it in `emitRetractValueIncrementally`. */ trait RetractableCollector[T] extends Collector[T] { /** * Retract a record. * * @param record The record to retract. */ def retract(record: T): Unit } } 下面的例子说明了如何进行\n 定义一个 TableAggregateFunction，用于计算给定列上的前 2 个值。 在 TableEnvironment 中注册该函数，并且 在 Table API 查询中使用该函数(TableAggregateFunction 仅由 Table API 支持)。  为了计算前 2 名的值，累加器需要存储所有已积累的数据中最大的 2 个值。在我们的例子中，我们定义了一个类 Top2Accum 作为累加器。累积器会被 Flink 的检查点机制自动备份，并在故障时恢复，以保证精确的 once 语义。\n我们 Top2 TableAggregateFunction 的 accumulate() 方法有两个输入。第一个是 Top2Accum 累加器，另一个是用户定义的输入：输入值 v，虽然 merge() 方法对于大多数表聚合类型来说不是强制性的，但我们在下面提供它作为例子。请注意，我们在 Scala 示例中使用了 Java 基元类型，并定义了 getResultType() 和 getAccumulatorType() 方法，因为 Flink 类型提取对 Scala 类型的效果并不好。\nimport java.lang.{Integer =\u0026gt; JInteger} import org.apache.flink.table.api.Types import org.apache.flink.table.functions.TableAggregateFunction /** * Accumulator for top2. */ class Top2Accum { var first: JInteger = _ var second: JInteger = _ } /** * The top2 user-defined table aggregate function. */ class Top2 extends TableAggregateFunction[JTuple2[JInteger, JInteger], Top2Accum] { override def createAccumulator(): Top2Accum = { val acc = new Top2Accum acc.first = Int.MinValue acc.second = Int.MinValue acc } def accumulate(acc: Top2Accum, v: Int) { if (v \u0026gt; acc.first) { acc.second = acc.first acc.first = v } else if (v \u0026gt; acc.second) { acc.second = v } } def merge(acc: Top2Accum, its: JIterable[Top2Accum]): Unit = { val iter = its.iterator() while (iter.hasNext) { val top2 = iter.next() accumulate(acc, top2.first) accumulate(acc, top2.second) } } def emitValue(acc: Top2Accum, out: Collector[JTuple2[JInteger, JInteger]]): Unit = { // emit the value and rank  if (acc.first != Int.MinValue) { out.collect(JTuple2.of(acc.first, 1)) } if (acc.second != Int.MinValue) { out.collect(JTuple2.of(acc.second, 2)) } } } // init table val tab = ... // use function tab .groupBy(\u0026#39;key) .flatAggregate(top2(\u0026#39;a) as (\u0026#39;v, \u0026#39;rank)) .select(\u0026#39;key, \u0026#39;v, \u0026#39;rank) 下面的例子展示了如何使用 emitUpdateWithRetract 方法来只发送更新。在我们的例子中，为了只发出更新，累加器同时保留新旧 top2 的值。注意：如果 topN 的 N 很大，那么同时保留新旧值的效率可能很低。解决这种情况的方法之一是在累加方法中把输入的记录存储到累加器中，然后在 emitUpdateWithRetract 中进行计算。\nimport java.lang.{Integer =\u0026gt; JInteger} import org.apache.flink.table.api.Types import org.apache.flink.table.functions.TableAggregateFunction /** * Accumulator for top2. */ class Top2Accum { var first: JInteger = _ var second: JInteger = _ var oldFirst: JInteger = _ var oldSecond: JInteger = _ } /** * The top2 user-defined table aggregate function. */ class Top2 extends TableAggregateFunction[JTuple2[JInteger, JInteger], Top2Accum] { override def createAccumulator(): Top2Accum = { val acc = new Top2Accum acc.first = Int.MinValue acc.second = Int.MinValue acc.oldFirst = Int.MinValue acc.oldSecond = Int.MinValue acc } def accumulate(acc: Top2Accum, v: Int) { if (v \u0026gt; acc.first) { acc.second = acc.first acc.first = v } else if (v \u0026gt; acc.second) { acc.second = v } } def emitUpdateWithRetract( acc: Top2Accum, out: RetractableCollector[JTuple2[JInteger, JInteger]]) : Unit = { if (acc.first != acc.oldFirst) { // if there is an update, retract old value then emit new value.  if (acc.oldFirst != Int.MinValue) { out.retract(JTuple2.of(acc.oldFirst, 1)) } out.collect(JTuple2.of(acc.first, 1)) acc.oldFirst = acc.first } if (acc.second != acc.oldSecond) { // if there is an update, retract old value then emit new value.  if (acc.oldSecond != Int.MinValue) { out.retract(JTuple2.of(acc.oldSecond, 2)) } out.collect(JTuple2.of(acc.second, 2)) acc.oldSecond = acc.second } } } // init table val tab = ... // use function tab .groupBy(\u0026#39;key) .flatAggregate(top2(\u0026#39;a) as (\u0026#39;v, \u0026#39;rank)) .select(\u0026#39;key, \u0026#39;v, \u0026#39;rank) 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-table-api-user-defined-functions/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","Function"],"title":"用户定义函数"},{"categories":["Flink"],"contents":"用户自定义函数 大多数操作符都需要用户定义的函数。本节列出了如何指定这些函数的不同方法。我们还涵盖了累加器，它可以用来深入了解您的 Flink 应用程序。\nLambda 函数 在前面的例子中已经看到，所有的操作符都接受 lambda 函数来描述操作。\nval data: DataSet[String] = // [...] data.filter { _.startsWith(\u0026#34;http://\u0026#34;) } val data: DataSet[Int] = // [...] data.reduce { (i1,i2) =\u0026gt; i1 + i2 } // 或 data.reduce { _ + _ } 富函数(Rich functions) 所有以 lambda 函数作为参数的变换都可以以富函数作为参数。例如，我们可以不使用:\ndata.map { x =\u0026gt; x.toInt } 你可以编写:\nclass MyMapFunction extends RichMapFunction[String, Int] { def map(in: String):Int = { in.toInt } }; 并将该函数传递给 map 转换:\ndata.map(new MyMapFunction()) 丰富的函数也可以定义为匿名类:\ndata.map (new RichMapFunction[String, Int] { def map(in: String):Int = { in.toInt } }) 丰富的函数除了提供用户定义的函数（map、reduce等）外，还提供了四个方法：open、close、getRuntimeContext 和 setRuntimeContext。这些方法可以用于为函数设置参数（参见 Passing Parameters to Functions）、创建和最终确定局部状态、访问广播变量（参见 Broadcast Variables）、访问运行时信息，如累加器和计数器（参见 Accumulators and Counters）以及迭代信息（参见 Iterations）。\n累积器和计数器 累积器是一个简单的构造，有一个加法运算和一个最终的累积结果，在作业结束后就可以使用。\n最直接的累加器是一个计数器，你可以使用 Accumulator.add(V value) 方法对它进行增量。在作业结束时，Flink 将对所有部分结果进行加总（合并）并将结果发送给客户端。累积器在调试期间或如果你快速想了解更多的数据时是很有用的。\nFlink 目前有以下内置的累加器。它们每个都实现了 Accumulator 接口。\n IntCounter、LongCounter 和 DoubleCounter。请看下面一个使用计数器的例子。 直方图。一个离散数量的直方块的直方图实现。在内部，它只是一个从 Integer 到 Integer 的映射。你可以用它来计算值的分布，例如字数程序的每行字数分布。  如何使用累加器:\n首先你必须在用户定义的转换函数中创建一个累加器对象(这里是一个计数器)，在你想使用它的地方。\nprivate IntCounter numLines = new IntCounter(); 其次，你必须注册累加器对象，通常是在富函数的 open() 方法中。在这里你还需要定义名称。\ngetRuntimeContext().addAccumulator(\u0026#34;num-lines\u0026#34;, this.numLines); 现在你可以在运算函数的任何地方使用累加器，包括在 open() 和 close() 方法中。\nthis.numLines.add(1); 整体结果将存储在 JobExecutionResult 对象中，该对象由执行环境的 execute() 方法返回（目前只有在执行等待作业完成的情况下才有效）。\nmyJobExecutionResult.getAccumulatorResult(\u0026#34;num-lines\u0026#34;) 所有的累加器在每个作业中共享一个命名空间。因此你可以在你的工作的不同操作函数中使用同一个累加器。Flink 会在内部合并所有同名的累加器。\n关于累加器和迭代的说明。目前，累加器的结果只有在整个作业结束后才会出现。我们计划在下一次迭代中也能获得上一次迭代的结果。你可以使用 Aggregators 来计算每次迭代的统计数据，并根据这些统计数据来终止迭代。\n自定义累加器:\n要实现你自己的累加器，你只需要编写你的 Accumulator 接口的实现。如果你认为你的自定义累加器应该和Flink一起发布，请随时创建一个pull request。\n你可以选择实现 Accumulator 或 SimpleAccumulator。\nAccumulator\u0026lt;V,R\u0026gt; 是最灵活的。它为要添加的值定义了一个类型 V，为最终结果定义了一个结果类型 R。例如，对于一个直方图，V 是一个数字，R 是一个直方图。 SimpleAccumulator 适用于两种类型都相同的情况，例如计数器。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-user-defined-functions/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"用户定义函数"},{"categories":["Flink"],"contents":"窗口 窗口是处理无限流的核心。窗口将流分割成有限大小的\u0026quot;桶\u0026quot;，我们可以对其应用计算。本文档主要介绍 Flink 中如何进行窗口化，以及程序员如何从其提供的功能中最大限度地受益。\n下面介绍了一个窗口化 Flink 程序的一般结构。第一个片段指的是 keyed 流，而第二个片段指的是 non-keyed 流。正如人们所看到的那样，唯一的区别是 keyed 流的 keyBy(...) 调用和 non-keyed 流的 window(...) 变成了 windowAll(...)。这也将作为本页面其他内容的路线图。\nKeyed 窗口\nstream .keyBy(...) \u0026lt;- keyed 与 non-keyed 窗口的对比 .window(...) \u0026lt;- 必须的: \u0026quot;assigner\u0026quot; [.trigger(...)] \u0026lt;- 可选的: \u0026quot;trigger\u0026quot; (否则使用默认的 trigger) [.evictor(...)] \u0026lt;- 可选的: \u0026quot;evictor\u0026quot; (否则没有 evictor) [.allowedLateness(...)] \u0026lt;- 可选的: \u0026quot;lateness\u0026quot; (否则为零) [.sideOutputLateData(...)] \u0026lt;- 可选的: \u0026quot;output tag\u0026quot; (否则迟到数据无侧输出) .reduce/aggregate/fold/apply() \u0026lt;- 必须的: \u0026quot;function\u0026quot; [.getSideOutput(...)] \u0026lt;- 可选的: \u0026quot;output tag\u0026quot; Non-Keyed 窗口\nstream .windowAll(...) \u0026lt;- 必须的: \u0026quot;assigner\u0026quot; [.trigger(...)] \u0026lt;- 可选的: \u0026quot;trigger\u0026quot; (否则使用默认的 trigger) [.evictor(...)] \u0026lt;- 可选的: \u0026quot;evictor\u0026quot; (否则没有 evictor) [.allowedLateness(...)] \u0026lt;- 可选的: \u0026quot;lateness\u0026quot; (否则为零) [.sideOutputLateData(...)] \u0026lt;- 可选的: \u0026quot;output tag\u0026quot; (否则迟到数据无侧输出) .reduce/aggregate/fold/apply() \u0026lt;- 必须的: \u0026quot;function\u0026quot; [.getSideOutput(...)] \u0026lt;- 可选的: \u0026quot;output tag\u0026quot; 在上面，方括号中的命令([...])是可选的。这表明 Flink 允许你以多种不同的方式定制你的窗口逻辑，以便它最适合你的需求。\n窗口生命周期 简而言之，当第一个应该属于这个窗口的元素到达时，就会创建一个窗口，当时间（事件时间或处理时间）经过(passes)它的结束时间戳加上用户指定的允许延迟时，这个窗口就会被完全移除（见允许延迟）。Flink 只保证对基于时间的窗口进行移除，而不保证对其他类型的窗口，如全局窗口进行移除（见窗口分配器）。例如，基于事件-时间的窗口策略每5分钟创建一个非重叠（或翻滚）的窗口，并且允许的延迟为1分钟，当第一个具有时间戳的元素落入这个区间时，Flink 将为 12:00 和 12:05 之间的区间创建一个新的窗口，当水印通过 12:06 的时间戳时，它将删除它。\n此外，每个窗口将有一个触发器(见触发器)和一个函数(ProcessWindowFunction、ReduceFunction、AggregateFunction或FoldFunction)(见窗口函数)。函数将包含要应用于窗口内容的计算，而触发器则指定了窗口被认为可以应用函数的条件。触发策略可能是\u0026quot;当窗口中的元素数量超过4时\u0026quot;，或者\u0026quot;当水印经过窗口的末端时\u0026quot;。触发器还可以决定在创建和删除窗口之间的任何时间(any time between its creation and removal)清除窗口的内容。在这种情况下，清除只指窗口中的元素，而不是窗口元数据。这意味着新的数据仍然可以被添加到该窗口中。\n除上述之外，您还可以指定一个 Evictor(见 Evictors)，它将能够在触发器触发后以及在函数应用之前和/或之后从窗口中删除元素。\n在下文中，我们将对上述每个组件进行更详细的介绍。我们先从上述代码段中必须的部分开始(参见 Keyed vs Non-Keyed 窗口、窗口分配器和窗口函数)，然后再转向可选部分。\nKeyed 与 Non-Keyed 窗口的对比 首先要指定的是您的流是否应该是 keyed 的。这必须在定义窗口之前完成。使用 keyBy(...) 将把您的无限流分割成逻辑 keyed 流。如果没有调用 keyBy(...)，那么您的流就不是 keyed 流。\n在 keyed 流的情况下，传入事件的任何属性都可以被用作键（更多细节在这里）。拥有一个 keyed 流将允许你的窗口计算由多个任务并行执行，因为每个逻辑 keyed 流可以独立于其他流进行处理。所有指向同一键的元素将被发送到同一个并行任务(task)。\n在 non-keyed 流的情况下，您的原始流不会被分割成多个逻辑流，所有的窗口化逻辑将由一个任务(task)来执行，即并行度为1。\n窗口分配器 在指定流是否是 keyed 流之后，下一步是定义窗口分配器。窗口分配器定义了如何将元素分配给窗口。这是通过在 window(...)（对于 keyed 流）或 windowAll()（对于 non-keyed 流）调用中指定您所选择的 WindowAssigner 来实现的。\nWindowAssigner 负责将每个传入的元素分配给一个或多个窗口。Flink 为最常见的用例提供了预定义的窗口分配器，即滚动窗口、滑动窗口、会话窗口和全局窗口。您也可以通过扩展 WindowAssigner 类来实现自定义窗口分配器。所有内置的窗口分配器（除了全局窗口）都是基于时间将元素分配给窗口，时间可以是处理时间，也可以是事件时间。请查看我们关于事件时间的部分，了解处理时间和事件时间之间的区别，以及时间戳和水印是如何生成的。\n基于时间的窗口有一个开始时间戳（包括）和结束时间戳（不包括），共同描述窗口的大小。在代码中，Flink 在处理基于时间的窗口时使用了 TimeWindow，它有查询开始和结束时间戳的方法，还有一个额外的方法 maxTimestamp()，可以返回给定窗口的最大允许时间戳。\n在下文中，我们将展示 Flink 的预定义窗口分配器是如何工作的，以及如何在 DataStream 程序中使用它们。下图直观地展示了每个分配器的工作情况。紫色的圆圈代表流的元素，这些元素被某个键（在本例中是用户1、用户2和用户3）分割。x轴显示的是时间的进度。\n滚动窗口 滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口。滚动窗口有一个固定的大小，并且不重叠。例如，如果你指定了一个大小为5分钟的滚动窗口，那么当前的窗口将被评估，并且每隔5分钟就会启动一个新的窗口，如下图所示。\n以下代码片段展示了如何使用滚动窗口。\nval input: DataStream[T] = ... // tumbling event-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // tumbling processing-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // daily tumbling event-time windows offset by -8 hours. input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) 时间间隔可以使用 Time.milliseconds(x), Time.seconds(x), Time.minutes(x) 等中的一种来指定。\n如最后一个例子所示，滚动窗口分配器还可以采用一个可选的偏移量(offset)参数，用于改变窗口的对齐方式。例如，在没有偏移量的情况下，每小时的滚动窗口与纪元对齐，也就是说，你会得到诸如 1:00:00.000 - 1:59:59.999，2:00:00.000 - 2:59:59.999 等窗口。如果你想改变这一点，你可以给出一个偏移量。例如，如果偏移量为15分钟，您将得到 1:15:00.000 - 2:14:59.999，2:15:00.000 - 3:14:59.999 等。偏移量的一个重要用途是调整窗口到 UTC-0 以外的时区。例如，在中国，你必须指定一个 Time.hours(-8) 的偏移量。\n滑动窗口 滑动窗口分配器将元素分配给固定长度的窗口。与滚动窗口分配器类似，窗口的大小由窗口大小(window size)参数配置。一个额外的窗口滑动(window slide)参数控制滑动窗口的启动频率。因此，如果滑动窗口的滑块小于窗口大小，滑动窗口可以重叠。在这种情况下，元素被分配到多个窗口。\n例如，你可以有10分钟大小的窗口，滑动5分钟。这样，每隔5分钟就会有一个窗口，包含过去10分钟内到达的事件，如下图所示。\n以下代码片段展示了如何使用滑动窗口。\nval input: DataStream[T] = ... // sliding event-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // sliding processing-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // sliding processing-time windows offset by -8 hours input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) 时间间隔可以通过使用 Time.milliseconds(x), Time.seconds(x), Time.minutes(x) 等中的一个来指定。\n如上一个例子所示，滑动窗口分配器还可以采取一个可选的偏移量(offset)参数，用于改变窗口的对齐方式。例如，在没有偏移量的情况下，每小时滑动30分钟的窗口与纪元对齐，也就是说，你将得到 1:00:00.000 - 1:59:59.999，1:30:00.000 - 2:29:59.999 等窗口。如果你想改变这一点，你可以给出一个偏移量。例如，如果偏移量为15分钟，您将得到 1:15:00.000 - 2:14:59.999，1:45:00.000 - 2:44:59.999 等。偏移量的一个重要用途是调整窗口到 UTC-0 以外的时区。例如，在中国，你必须指定一个 Time.hours(-8) 的偏移。\n会话窗口 会话窗口分配器按活动的会话对元素进行分组。与滚动窗口和滑动窗口不同，会话窗口不重叠，也没有固定的开始和结束时间。相反，当会话窗口在一定时间内没有接收到元素时，也就是在不活动的间隙发生时，会话窗口就会关闭。会话窗口分配器可以配置一个静态的会话间隙(session gap)，也可以配置一个会话间隙提取函数，该函数定义了多长时间的不活动期。当这个时间段(period)到期(expires)时，当前会话关闭，后续元素被分配到一个新的会话窗口。\n以下代码片段展示了如何使用会话窗口。\nval input: DataStream[T] = ... // event-time session windows with static gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // event-time session windows with dynamic gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(EventTimeSessionWindows.withDynamicGap(new SessionWindowTimeGapExtractor[String] { override def extract(element: String): Long = { // determine and return session gap  } })) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // processing-time session windows with static gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // processing-time session windows with dynamic gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(DynamicProcessingTimeSessionWindows.withDynamicGap(new SessionWindowTimeGapExtractor[String] { override def extract(element: String): Long = { // determine and return session gap  } })) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) 静态间隙可以通过使用 Time.milliseconds(x), Time.seconds(x), Time.minutes(x) 等之一来指定。\n动态间隙可以通过实现 SessionWindowTimeGapExtractor 接口来指定。\n注意: 由于会话窗口没有固定的开始和结束，所以它们的评估方式与滚动和滑动窗口不同。在内部，会话窗口操作符为每个到达的记录创建一个新的窗口，如果它们彼此之间的距离比定义的间隙更近，就会将窗口合并在一起。为了能够合并，会话窗口操作符需要一个合并触发器和一个合并窗口函数，如 ReduceFunction、AggregateFunction 或 ProcessWindowFunction(FoldFunction 不能合并)。\n全局窗口 全局窗口分配器将具有相同键的所有元素分配到同一个全局窗口。只有当你还指定了一个自定义触发器时，这种窗口方案才有用。否则，任何计算都不会被执行，因为全局窗口没有一个自然的终点，我们可以在那里处理聚集的元素。\n下面的代码片段展示了如何使用全局窗口。\nval input: DataStream[T] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(GlobalWindows.create()) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) 窗口函数 在定义了窗口分配器之后，我们需要指定我们要对这些窗口中的每一个窗口进行的计算。这是窗口函数的责任，一旦系统确定一个窗口准备好进行处理，它就会用来处理每个（可能是 keyed 的）窗口的元素（关于 Flink 如何确定窗口准备好，请参见触发器）。\n窗口函数可以是 ReduceFunction、AggregateFunction、FoldFunction 或 ProcessWindowFunction 中的一种。前两个可以更有效地执行（见状态大小部分），因为 Flink 可以在每个窗口到达时增量地聚合元素。ProcessWindowFunction 可以为一个窗口中包含的所有元素获取一个 Iterable，以及关于元素所属窗口的附加元信息。\n带有 ProcessWindowFunction 的窗口化转换不能像其他情况一样高效执行，因为 Flink 在调用函数之前必须在内部缓冲一个窗口的所有元素。通过将 ProcessWindowFunction 与 ReduceFunction、AggregateFunction 或 FoldFunction 结合起来，既可以得到窗口元素的增量聚合，也可以得到 ProcessWindowFunction 接收到的额外的窗口元数据，从而缓解这种情况。我们将查看这些变体的每个例子。\nReduceFunction ReduceFunction 指定了如何将输入的两个元素组合起来以产生相同类型的输出元素。Flink 使用 ReduceFunction 来增量聚合一个窗口的元素。\nReduceFunction 可以这样定义和使用。\nval input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce { (v1, v2) =\u0026gt; (v1._1, v1._2 + v2._2) } 上面的例子把一个窗口中所有元素的元组的第二个字段相加起来。\nAggregateFunction AggregateFunction 是 ReduceFunction 的通用版本，它有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型是输入流中元素的类型，AggregateFunction 有一个方法用于将一个输入元素添加到累加器中。该接口还有创建一个初始累加器、将两个累加器合并成一个累加器以及从一个累加器中提取一个输出（类型为 OUT）的方法。我们将在下面的例子中看到这些方法是如何工作的。\n和 ReduceFunction 一样，Flink 会在窗口的输入元素到达时，对它们进行增量聚合。\nAggregateFunction 可以这样定义和使用。\n/** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */ class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] { override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2) } val input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .aggregate(new AverageAggregate) 上面的例子是计算窗口中元素的第二个字段的平均值。\nFoldFunction FoldFunction 指定了窗口的输入元素如何与输出类型的元素相结合。对于添加到窗口的每个元素和当前的输出值，都会递增地调用 FoldFunction。第一个元素与输出类型的预定义初始值相结合。\n可以这样定义和使用 FoldFunction。\nval input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .fold(\u0026#34;\u0026#34;) { (acc, v) =\u0026gt; acc + v._2 } 上面的例子将所有输入的 Long 值追加到一个初始的空字符串中。\n注意 fold() 不能用于会话窗口或其他可合并窗口。\nProcessWindowFunction ProcessWindowFunction 得到一个包含窗口所有元素的 Iterable，以及一个可以访问时间和状态信息的 Context 对象，这使得它能够提供比其他窗口函数更多的灵活性。这是以性能和资源消耗为代价的，因为元素不能增量聚合，而是需要在内部缓冲，直到窗口被认为可以处理为止。\nProcessWindowFunction 的签名如下。\nabstract class ProcessWindowFunction[IN, OUT, KEY, W \u0026lt;: Window] extends Function { /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ def process( key: KEY, context: Context, elements: Iterable[IN], out: Collector[OUT]) /** * The context holding window metadata */ abstract class Context { /** * Returns the window that is being evaluated. */ def window: W /** * Returns the current processing time. */ def currentProcessingTime: Long /** * Returns the current event-time watermark. */ def currentWatermark: Long /** * State accessor for per-key and per-window state. */ def windowState: KeyedStateStore /** * State accessor for per-key global state. */ def globalState: KeyedStateStore } } 注意 key 参数是通过为 keyBy() 调用指定的 KeySelector 提取的键。如果是元组索引键或字符串字段引用，这个键的类型总是 Tuple，你必须手动将其转换为一个正确大小的元组来提取键字段。\nProcessWindowFunction 可以这样定义和使用。\nval input: DataStream[(String, Long)] = ... input .keyBy(_._1) .timeWindow(Time.minutes(5)) .process(new MyProcessWindowFunction()) /* ... */ class MyProcessWindowFunction extends ProcessWindowFunction[(String, Long), String, String, TimeWindow] { def process(key: String, context: Context, input: Iterable[(String, Long)], out: Collector[String]) = { var count = 0L for (in \u0026lt;- input) { count = count + 1 } out.collect(s\u0026#34;Window ${context.window}count: $count\u0026#34;) } } 这个例子显示了一个 ProcessWindowFunction，它可以计算一个窗口中的元素。此外，窗口函数还将窗口的信息添加到输出中。\n注意，使用 ProcessWindowFunction 进行简单的聚合，如 count，效率相当低。下一节将展示如何将 ReduceFunction 或 AggregateFunction 与 ProcessWindowFunction 结合起来，以获得增量聚合和 ProcessWindowFunction 的附加信息。\n具有增量聚合功能的 ProcessWindowFunction ProcessWindowFunction 可以与 ReduceFunction、AggregateFunction 或 FoldFunction 相结合，以在元素到达窗口时进行增量聚合。当窗口关闭时，ProcessWindowFunction 将被提供聚合的结果。这使得它可以增量计算窗口，同时可以访问 ProcessWindowFunction 的附加窗口元信息。\n注意 您也可以使用 legacy WindowFunction 代替 ProcessWindowFunction 进行增量窗口聚合。\n使用 ReduceFunction 进行增量窗口聚合 下面的例子展示了如何将增量 ReduceFunction 与 ProcessWindowFunction 相结合，以返回窗口中最小的事件以及窗口的开始时间。\nval input: DataStream[SensorReading] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .timeWindow(\u0026lt;duration\u0026gt;) .reduce( (r1: SensorReading, r2: SensorReading) =\u0026gt; { if (r1.value \u0026gt; r2.value) r2 else r1 }, ( key: String, context: ProcessWindowFunction[_, _, _, TimeWindow]#Context, minReadings: Iterable[SensorReading], out: Collector[(Long, SensorReading)] ) =\u0026gt; { val min = minReadings.iterator.next() out.collect((context.window.getStart, min)) } ) 用 AggregateFunction 进行增量窗口聚合 下面的例子展示了如何将增量的 AggregateFunction 与 ProcessWindowFunction 结合起来，计算平均值，同时将键和窗口与平均值一起发出。\nval input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .timeWindow(\u0026lt;duration\u0026gt;) .aggregate(new AverageAggregate(), new MyProcessWindowFunction()) // Function definitions  /** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */ class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] { override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2) } class MyProcessWindowFunction extends ProcessWindowFunction[Double, (String, Double), String, TimeWindow] { def process(key: String, context: Context, averages: Iterable[Double], out: Collector[(String, Double)]) = { val average = averages.iterator.next() out.collect((key, average)) } } 用 FoldFunction 进行增量窗口聚合 下面的例子展示了如何将增量式 FoldFunction 与 ProcessWindowFunction 相结合，以提取窗口中的事件数量，并返回窗口的键和结束时间。\nval input: DataStream[SensorReading] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .timeWindow(\u0026lt;duration\u0026gt;) .fold ( (\u0026#34;\u0026#34;, 0L, 0), (acc: (String, Long, Int), r: SensorReading) =\u0026gt; { (\u0026#34;\u0026#34;, 0L, acc._3 + 1) }, ( key: String, window: TimeWindow, counts: Iterable[(String, Long, Int)], out: Collector[(String, Long, Int)] ) =\u0026gt; { val count = counts.iterator.next() out.collect((key, window.getEnd, count._3)) } ) 在 ProcessWindowFunction 中使用 per-窗口状态 除了访问 keyed 状态（任何富函数都可以），ProcessWindowFunction 还可以使用 keyed 状态，该状态的作用域是函数当前正在处理的窗口。在这种情况下，理解每个窗口状态所指的窗口是什么很重要。这里涉及到不同的\u0026quot;窗口\u0026quot;。\n 窗口是在指定窗口操作时定义的。这可能是1小时的滚动窗口或者2小时的滑动窗口，滑动1小时。 一个给定的键的定义窗口的实际实例。这可能是 12: 00 到 13: 00 的时间窗口，用户 ID xyz. 这是基于窗口定义的，会有很多窗口，基于作业当前正在处理的键的数量，基于事件属于什么时间段。  每个窗口的状态与这两者中的后一种挂钩。意思是说，如果我们处理了1000个不同键的事件，并且所有键的事件当前都属于 [12:00，13:00) 时间窗口，那么将有1000个窗口实例，每个窗口都有自己的键的per-窗口状态。\nprocess() 调用接收到的 Context 对象上有两个方法允许访问这两种类型的状态。\n globalState()，允许访问不在窗口范围内的 keyed 状态。 windowState()，它允许访问同样作用于窗口的 keyed 状态。  如果你预计同一窗口会有多次发射，那么这个功能是很有帮助的，因为当你对晚到的数据有晚发射的情况，或者当你有一个自定义的触发器，做投机性的早期发射时，可能会发生这种情况。在这种情况下，你会在每个窗口状态下存储之前的发射信息或发射次数。\n当使用窗口状态时，重要的是当窗口被清除时也要清理该状态。这应该发生在 clear() 方法中。\nWindowFunction(Legacy) 在一些可以使用 ProcessWindowFunction 的地方，你也可以使用 WindowFunction。这是 ProcessWindowFunction 的旧版本，它提供的上下文信息较少，而且没有一些先进的功能，比如每个窗口的 keyed 状态。这个接口在某些时候会被废弃。\nWindowFunction 的签名如下。\ntrait WindowFunction[IN, OUT, KEY, W \u0026lt;: Window] extends Function with Serializable { /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param window The window that is being evaluated. * @param input The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ def apply(key: KEY, window: W, input: Iterable[IN], out: Collector[OUT]) } 可以这样使用。\nval input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .apply(new MyWindowFunction()) 触发器 触发器决定一个窗口（由窗口分配器形成）何时可以被窗口函数处理。每个 WindowAssigner 都有一个默认的触发器。如果默认的触发器不符合你的需求，你可以使用 trigger(...) 指定一个自定义的触发器。\n触发器接口有五个方法，允许 Trigger 对不同的事件做出反应。\n onElement() 方法对每个添加到窗口的元素都会被调用。 onEventTime() 方法在注册的事件时间定时器启动时被调用。 onProcessingTime() 方法在注册的处理时间计时器启动时被调用。 onMerge() 方法与有状态的触发器相关，当两个触发器的对应窗口合并时，例如使用会话窗口时，就会合并两个触发器的状态。 最后 clear() 方法在删除相应窗口时执行任何需要的操作。  关于以上方法有两点需要注意。\n1）前三个方法通过返回一个 TriggerResult 来决定如何对其调用事件采取行动。动作可以是以下之一。\n CONTINUE：什么也不做。 FIRE：触发计算。 PURGE：清除窗口中的元素，以及 FIRE_AND_PURGE：触发计算，之后清除窗口中的元素。  这些方法中的任何一种都可以用来注册处理时间或事件时间的定时器，以备将来的操作。  Fire 和 Purge 一旦触发器确定一个窗口可以处理，它就会发射，即返回 FIRE 或 FIRE_AND_PURGE。这是窗口操作者发出当前窗口结果的信号。给定一个带有 ProcessWindowFunction 的窗口，所有的元素都会被传递给 ProcessWindowFunction（可能是在将它们传递给 evictor 之后）。带有 ReduceFunction、AggregateFunction 或 FoldFunction 的窗口只是简单地发出它们急切的聚合结果。\n当一个触发器发射时，它可以是 FIRE 或 FIRE_AND_PURGE。FIRE 保留窗口的内容，而 FIRE_AND_PURGE 则删除其内容。默认情况下，预先实现的触发器只是 FIRE 而不清除窗口状态。\n注意 Purging 将简单地删除窗口的内容，并将完整地保留任何关于窗口和任何触发状态的潜在元信息。\n窗口分配器的默认触发器 WindowAssigner 的默认触发器适合于许多用例。例如，所有的事件时间窗口分配器都有一个 EventTimeTrigger 作为默认触发器。这个触发器仅仅是在水印通过窗口结束后就会触发。\n注意：GlobalWindow 的默认触发器是 NeverTrigger，它永远不会触发。因此，在使用 GlobalWindow 时，您必须定义一个自定义的触发器。\n注意：通过使用 trigger() 指定一个触发器，您将覆盖一个 WindowAssigner 的默认触发器。例如，如果你为 TumblingEventTimeWindows 指定了一个 CountTrigger，你将不再获得基于时间进度的窗口启动，而只能通过计数来获得。现在，如果你想同时基于时间和计数做出反应，你必须编写自己的自定义触发器。\n内置和自定义触发器 Flink 内置了一些触发器。\n 前面已经提到过的, EventTimeTrigger 会根据水印测量的事件时间的进展而触发。 处理时间触发器（ProcessingTimeTrigger）基于处理时间而触发。 CountTrigger 在一个窗口中的元素数量超过给定的限制时触发。 PurgingTrigger 将另一个触发器作为参数，并将其转换为一个清洗触发器。  如果你需要实现一个自定义的触发器，你应该查看抽象的 Trigger 类。请注意，API 仍在不断发展，可能会在 Flink 的未来版本中改变。\nEvictors Flink 的窗口模型允许在 WindowAssigner 和 Trigger 之外指定一个可选的 Evictor。这可以通过 evictor(...) 方法来完成（如本文开头所示）。Evictor 能够在触发器触发后和应用窗口函数之前和/或之后从窗口中移除元素。要做到这一点，Evictor 接口有两个方法。\n/** * Optionally evicts elements. Called before windowing function. * * @param elements The elements currently in the pane. * @param size The current number of elements in the pane. * @param window The {@link Window} * @param evictorContext The context for the Evictor */ void evictBefore(Iterable\u0026lt;TimestampedValue\u0026lt;T\u0026gt;\u0026gt; elements, int size, W window, EvictorContext evictorContext); /** * Optionally evicts elements. Called after windowing function. * * @param elements The elements currently in the pane. * @param size The current number of elements in the pane. * @param window The {@link Window} * @param evictorContext The context for the Evictor */ void evictAfter(Iterable\u0026lt;TimestampedValue\u0026lt;T\u0026gt;\u0026gt; elements, int size, W window, EvictorContext evictorContext); evictBefore() 包含在窗口函数之前应用的驱逐逻辑，而 evictAfter() 包含在窗口函数之后应用的逻辑。在应用窗口函数之前被驱逐的元素将不会被它处理。\nFlink 自带了三个预先实现的驱逐器。这三个是:\n CountEvictor：从窗口中保留最多用户指定数量的元素，并从窗口缓冲区开始丢弃剩余的元素。 DeltaEvictor：取 DeltaFunction 和阈值，计算窗口缓冲区中最后一个元素和剩余元素之间的 delta，并删除 delta 大于或等于阈值的元素。 TimeEvictor：以毫秒为单位的时间间隔作为参数，对于一个给定的窗口，它在其元素中找到最大的时间戳 max_ts，并删除所有时间戳小于 max_ts - interval 的元素。  默认情况下，所有预先实现的 evictor 都会在 window 函数之前应用其逻辑。\n注意: 指定一个 evictor 可以防止任何预聚集，因为一个窗口的所有元素都必须在应用计算之前传递给 evictor。\n注意 Flink 不保证窗口内元素的顺序。这意味着，虽然 evictor 可以从窗口的开头移除元素，但这些元素不一定是最先或最后到达的。\n允许的延迟 当使用事件时间窗口时，可能会发生元素迟到的情况，也就是说，Flink 用来跟踪事件时间进度的水印已经超过了元素所属窗口的结束时间戳。关于 Flink 如何处理事件时间，请参见事件时间，尤其是迟到元素。\n默认情况下，当水印超过窗口的结束时间时，晚期元素就会被删除。然而，Flink 允许为窗口操作者指定一个最大允许延迟。允许延迟指定了元素在被丢弃之前可以迟到多少时间，其默认值为0。 在水印通过窗口结束后但在其通过窗口结束前加上允许延迟之前到达的元素，仍然会被添加到窗口中。根据所使用的触发器，一个迟到但未被丢弃的元素可能会导致窗口再次启动。EventTimeTrigger 就属于这种情况。\n为了使这个工作，Flink 会保持窗口的状态，直到它们的允许延迟过期。一旦发生这种情况，Flink 就会删除窗口并删除其状态，这一点在窗口生命周期部分也有描述。\n默认情况下，允许的延迟被设置为0，也就是说，到达水印后面的元素将被丢弃。\n您可以像这样指定允许的延迟。\nval input: DataStream[T] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .allowedLateness(\u0026lt;time\u0026gt;) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) 注意 当使用 GlobalWindows 窗口分配器时，由于全局窗口的结束时间戳是 Long.MAX_VALUE，因此没有数据被认为是迟到数据。\n作为侧输出获取迟到数据 使用 Flink 的侧输出功能，你可以得到一个被丢弃的迟到数据流。\n首先，你需要在窗口化的数据流上使用 sideOutputLateData(OutputTag) 来指定你要获取迟到的数据。然后，你就可以在窗口化操作的结果上得到侧输出流。\nval lateOutputTag = OutputTag[T](\u0026#34;late-data\u0026#34;) val input: DataStream[T] = ... val result = input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .allowedLateness(\u0026lt;time\u0026gt;) .sideOutputLateData(lateOutputTag) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) val lateStream = result.getSideOutput(lateOutputTag) 迟到元素的考虑 当指定允许的延迟大于0时，在水印通过窗口结束后，窗口及其内容将被保留。在这些情况下，当一个迟到但未被丢弃的元素到达时，它可能会触发窗口的另一次发射。这些发射被称为晚期发射，因为它们是由晚期事件触发的，与主发射相反，主发射是窗口的第一次发射。在会话窗口的情况下，迟发可能会进一步导致窗口的合并，因为它们可能会\u0026quot;弥合\u0026quot;两个已经存在的、未合并的窗口之间的差距。\n注意：你应该意识到，晚点发射的元素应该被视为之前计算的更新结果，也就是说，你的数据流将包含同一计算的多个结果。根据你的应用，你需要考虑到这些重复的结果，或者对它们进行重复复制。\n处理窗口结果 窗口化操作的结果又是一个 DataStream，在结果元素中没有保留任何关于窗口化操作的信息，所以如果你想保留窗口的元信息，你必须在你的 ProcessWindowFunction 的结果元素中手动编码这些信息。在结果元素上设置的唯一相关信息是元素的时间戳。这被设置为处理过的窗口的最大允许时间戳，也就是结束时间戳-1，因为窗口结束时间戳是独占的。注意，这对事件时间窗口和处理时间窗口都是如此，即在窗口化操作后元素总是有一个时间戳，但这个时间戳可以是事件时间时间戳，也可以是处理时间时间戳。对于处理时间窗口来说，这没有特别的影响，但是对于事件时间窗口来说，加上水印与窗口的交互方式，使得连续的窗口化操作具有相同的窗口大小。我们将在看完水印如何与窗口交互后再谈这个问题。\n水印和窗口的交互 在继续本节之前，你可能想看看我们关于事件时间和水印的章节。\n当水印到达窗口操作符时，会触发两件事。\n 水印会触发计算所有窗口的最大时间戳（就是结束时间戳-1）小于新水印的窗口。 水印被转发到下游的操作中  直观地说，水印会\u0026quot;冲掉\u0026quot;任何在下游操作中被认为是晚期的窗口，一旦它们收到该水印。\n连续的窗口操作 如前所述，计算窗口化结果的时间戳的方式以及水印与窗口的交互方式允许将连续的窗口化操作串在一起。当你想进行两个连续的窗口化操作时，如果你想使用不同的键，但仍然希望来自同一个上游窗口的元素最终出现在同一个下游窗口中，这就很有用。考虑这个例子。\nval input: DataStream[Int] = ... val resultsPerKey = input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .reduce(new Summer()) val globalResults = resultsPerKey .windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) .process(new TopKWindowFunction()) 在这个例子中，第一次操作的时间窗口 [0，5) 的结果也会在随后的窗口操作中最终出现在时间窗口 [0，5)。这样就可以计算每个键的和，然后在第二个操作中计算同一窗口内的 top-k 元素。\n有用的状态大小考虑 窗口可以在很长一段时间内（如几天、几周或几个月）被定义，因此会积累非常大的状态。在估算窗口计算的存储需求时，有几个规则需要牢记。\n  Flink 为每个元素所属的窗口创建一个副本。鉴于此，翻滚窗口为每个元素保留一个副本（一个元素正好属于一个窗口，除非它被后期丢弃）。相比之下，滑动窗口会给每个元素创建若干个，这一点在窗口分配器部分有解释。因此，大小为1天，滑动1秒的滑动窗口可能不是一个好主意。\n  ReduceFunction、AggregateFunction 和 FoldFunction 可以显著降低存储要求，因为它们热衷于聚合元素，每个窗口只存储一个值。相比之下，仅仅使用 ProcessWindowFunction 就需要累积所有元素。\n  使用 Evictor 可以防止任何预聚集，因为一个窗口的所有元素都必须在应用计算之前通过 evictor（见 Evictor）。\n  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-windows/","tags":["Flink","Flink 官方文档","DataStream API","Operators","Windows"],"title":"窗口"},{"categories":["Flink"],"contents":"本页面的目标是为需要使用自定义状态序列化的用户提供指导，涵盖了如何提供自定义状态序列化器，以及实现允许状态模式演化的序列化器的指南和最佳实践。\n如果你只是简单地使用 Flink 自带的序列化器，这个页面是不相关的，可以忽略。\n使用自定义状态序列化器 当注册一个 managed operator 或 keyed state时，需要一个 StateDescriptor 来指定状态的名称，以及状态的类型信息。类型信息被 Flink 的类型序列化框架用来为状态创建合适的序列化器。\n也可以完全绕过这一点，让 Flink 使用自己的自定义序列化器来序列化被管理的状态，只需用自己的 TypeSerializer 实现直接实例化 StateDescriptor 即可。\nclass CustomTypeSerializer extends TypeSerializer[(String, Integer)] {...} val descriptor = new ListStateDescriptor[(String, Integer)]( \u0026#34;state-name\u0026#34;, new CustomTypeSerializer) ) checkpointedState = getRuntimeContext.getListState(descriptor) 状态序列化器和模式演进 本节解释了与状态序列化和模式演进相关的面向用户的抽象，以及关于 Flink 如何与这些抽象交互的必要内部细节。\n当从保存点恢复时，Flink 允许改变用于读取和写入先前注册状态的序列化器，因此用户不会被锁定在任何特定的序列化模式上。当状态被还原时，将为该状态注册一个新的序列化器（即在还原作业中用于访问状态的 StateDescriptor 所附带的序列化器）。这个新的序列化器可能与之前的序列化器的模式不同。因此，在实现状态序列化器时，除了读取/写入数据的基本逻辑外，另一个需要注意的重要问题是未来如何改变序列化模式。\n说到 schema，在这里，这个术语可以互换，指的是状态类型的数据模型和状态类型的序列化二进制格式。一般来说，模式，可以为少数情况而改变。\n 状态类型的数据模式发生了变化，即从 POJO 中增加或删除一个作为状态的字段。 一般来说，数据模式发生变化后，需要升级序列器的序列化格式。 序列器的配置发生了变化。  为了让新的执行有状态的写入模式的信息，并检测模式是否发生了变化，在对操作符的状态进行保存点时，需要将状态序列器的快照和状态字节一起写入。这就是抽象出来的一个 TypeSerializerSnapshot，在下一小节解释。\nTypeSerializerSnapshot 抽象 public interface TypeSerializerSnapshot\u0026lt;T\u0026gt; { int getCurrentVersion(); void writeSnapshot(DataOuputView out) throws IOException; void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException; TypeSerializerSchemaCompatibility\u0026lt;T\u0026gt; resolveSchemaCompatibility(TypeSerializer\u0026lt;T\u0026gt; newSerializer); TypeSerializer\u0026lt;T\u0026gt; restoreSerializer(); } public abstract class TypeSerializer\u0026lt;T\u0026gt; { // ...  public abstract TypeSerializerSnapshot\u0026lt;T\u0026gt; snapshotConfiguration(); } 序列器的 TypeSerializerSnapshot 是一个时间点信息，它作为状态序列器的写模式的唯一真理来源，以及还原一个序列器所必须的任何额外信息，这些信息将与给定的时间点相同。关于在还原时应该写入和读取什么作为序列器快照的逻辑是在 writeSnapshot和readSnapshot 方法中定义的。\n请注意，快照本身的写模式也可能需要随着时间的推移而改变（例如，当你希望在快照中添加更多关于序列器的信息时）。为了方便，快照是有版本的，在 getCurrentVersion 方法中定义了当前的版本号。在还原时，当从保存点读取序列器快照时，将向 readSnapshot 方法提供写入快照的模式的版本，以便读取实现可以处理不同的版本。\n在还原时，检测新的序列器的模式是否改变的逻辑应该在 resolveSchemaCompatibility 方法中实现。当之前的注册状态在还原执行的操作符中再次注册新的序列化器时，新的序列化器会通过这个方法提供给之前序列化器的快照。该方法返回一个代表兼容性解决结果的 TypeSerializerSchemaCompatibility，它可以是以下之一。\n TypeSerializerSchemaCompatibility.compatibleAsIs()：这个结果标志着新的序列化器是兼容的，这意味着新的序列化器与之前的序列化器具有相同的模式。有可能在resolveSchemaCompatibility方法中重新配置了新的序列化器，使其兼容。 TypeSerializerSchemaCompatibility.compatibleAfterMigration()：这个结果标志着新的序列化器具有不同的序列化模式，可以从旧的模式迁移，使用之前的序列化器（识别旧的模式）将字节读入状态对象，然后用新的序列化器（识别新的模式）将对象重新写回字节。 TypeSerializerSchemaCompatibility.incompatible()：这个结果标志着新的序列化器有不同的序列化模式，但不可能从旧模式迁移。  最后一点细节是在需要迁移的情况下，如何获得之前的序列化器。序列化器的 TypeSerializerSnapshot 的另一个重要作用是，它可以作为一个工厂来恢复以前的序列化器。更具体地说，TypeSerializerSnapshot 应该实现 restoreSerializer 方法来实例化一个序列化器实例，该实例能够识别之前序列化器的模式和配置，因此可以安全地读取之前序列化器写入的数据。\nFlink 如何与 TypeSerializer 和 TypeSerializerSnapshot 抽象进行交互 总结一下，本节总结了 Flink，或者更具体地说，状态后端如何与抽象进行交互。根据状态后端的不同，交互略有不同，但这与状态序列化器及其序列化器快照的实现是正交的。\n离堆状态后端(如 RocksDBStateBackend)\n 用具有模式A的状态序列器注册新的状态。   注册的 TypeSerializer 用于在每次状态访问时读取/写入状态。 状态被写入模式A中。  拍摄一个保存点   序列器快照是通过 TypeSerializer#snapshotConfiguration 方法提取的。 序列器快照被写入保存点，以及已经序列化的状态字节（模式A）。  恢复的执行用新的状态序列化器重新访问恢复的状态字节，新的状态序列化器具有模式B。   前一个状态序列器的快照被还原。 状态字节在还原时不被反序列化，只被加载回状态后端（因此，仍在模式A中）。 接收到新的序列化器后，通过 TypeSerializer#resolveSchemaCompatibility 提供给被还原的前一个序列化器的快照，检查模式是否兼容。  将后端中的状态字节从模式A迁移到模式B。   如果兼容性决议反映模式已经改变，并且可以进行迁移，则进行模式迁移。通过 TypeSerializerSnapshot#restoreSerializer()，将从序列化器快照中获取之前识别模式A的状态序列化器，并用于反序列化状态字节到对象，进而用新的序列化器再次重写，识别模式B，完成迁移。在继续处理之前，所有访问状态的条目全部迁移完毕。 如果解析信号为不兼容，则状态访问失败，出现异常。  堆状态后端（如 MemoryStateBackend、FsStateBackend）:\n 用具有模式A的状态序列器注册新的状态。   注册的 TypeSerializer 由状态后端维护。  拍摄一个保存点，将所有状态用模式A序列化。   序列器快照是通过 TypeSerializer#snapshotConfiguration 方法提取的。 序列化器快照被写入保存点。 现在状态对象被序列化到保存点，写入模式A中。  在还原时，将状态反序列化为堆中的对象。   前一个状态序列器的快照被恢复。 通过 TypeSerializerSnapshot#restoreSerializer() 从序列化器快照中获取之前的序列化器，该序列化器识别模式A，用于将状态字节反序列化为对象。 从现在开始，所有的状态都已经被反序列化了。  恢复后的执行用新的状态序列化器重新访问以前的状态，新的状态序列化器具有模式B。   在接收到新的序列化器后，通过 TypeSerializer#resolveSchemaCompatibility 提供给恢复之前序列化器的快照，以检查模式的兼容性。 如果兼容性检查发出需要迁移的信号，在这种情况下什么都不会发生，因为对于堆后端来说，所有的状态已经被反序列化为对象。 如果解析信号为不兼容，则状态访问失败，出现异常。  再拍摄一个保存点，将所有状态用模式B序列化。   与步骤2.相同，但现在状态字节都在模式B中。  预先定义方便的 TypeSerializerSnapshot 类 Flink 提供了两个抽象的基础 TypeSerializerSnapshot 类，可以用于典型场景。SimpleTypeSerializerSnapshot 和 CompositeTypeSerializerSnapshot。\n提供这些预定义快照作为其序列化器快照的序列化器必须始终有自己独立的子类实现。这与不在不同的序列化器之间共享快照类的最佳实践相对应，这将在下一节中得到更详尽的解释。\n实现 SimpleTypeSerializerSnapshot SimpleTypeSerializerSnapshot 是为没有任何状态或配置的序列化器准备的，本质上意味着序列化器的序列化模式完全由序列化器的类来定义。\n当使用 SimpleTypeSerializerSnapshot 作为你的序列化器的快照类时，兼容性解决只有2种可能的结果。\n TypeSerializerSchemaCompatibility.compatibleAsIs()，如果新的序列化器类保持相同，或 TypeSerializerSchemaCompatibility.incompatible()，如果新的序列化器类与之前的序列化器类不同。  下面以 Flink 的 IntSerializer 为例，介绍 SimpleTypeSerializerSnapshot 的使用方法。\npublic class IntSerializerSnapshot extends SimpleTypeSerializerSnapshot\u0026lt;Integer\u0026gt; { public IntSerializerSnapshot() { super(() -\u0026gt; IntSerializer.INSTANCE); } } IntSerializer 没有状态或配置。序列化格式完全由序列化器类自己定义，只能由另一个 IntSerializer 读取。因此，它适合 SimpleTypeSerializerSnapshot 的使用情况。\nSimpleTypeSerializerSnapshot 的基础超级构造函数期望得到一个相应序列器实例的 Supplier，不管快照当前是在还原还是在快照期间写入。该 Supplier 用于创建还原序列化器，以及类型检查，以验证新序列化器是否属于相同的预期序列化器类。\n实现 CompositeTypeSerializerSnapshot CompositeTypeSerializerSnapshot 是为那些依赖于多个嵌套序列化器的序列化器而设计的。\n在进一步解释之前，我们将依赖于多个嵌套序列化器的序列化器称为此上下文中的\u0026quot;外部\u0026quot;序列化器。这方面的例子可以是 MapSerializer、ListSerializer、GenericArraySerializer 等。例如，考虑 MapSerializer \u0026ndash;键和值序列化器将是嵌套序列化器，而MapSerializer本身是 \u0026ldquo;外部 \u0026ldquo;序列化器。\n在这种情况下，外层序列化器的快照也应该包含嵌套序列化器的快照，这样就可以独立检查嵌套序列化器的兼容性。在解决外层序列化器的兼容性时，需要考虑每个嵌套序列化器的兼容性。\n提供 CompositeTypeSerializerSnapshot 是为了协助实现这类复合序列器的快照。它处理嵌套序列化器快照的读写，以及考虑到所有嵌套序列化器的兼容性，解析最终的兼容性结果。\n下面以 Flink 的 MapSerializer 为例，介绍如何使用 CompositeTypeSerializerSnapshot。\npublic class MapSerializerSnapshot\u0026lt;K, V\u0026gt; extends CompositeTypeSerializerSnapshot\u0026lt;Map\u0026lt;K, V\u0026gt;, MapSerializer\u0026gt; { private static final int CURRENT_VERSION = 1; public MapSerializerSnapshot() { super(MapSerializer.class); } public MapSerializerSnapshot(MapSerializer\u0026lt;K, V\u0026gt; mapSerializer) { super(mapSerializer); } @Override public int getCurrentOuterSnapshotVersion() { return CURRENT_VERSION; } @Override protected MapSerializer createOuterSerializerWithNestedSerializers(TypeSerializer\u0026lt;?\u0026gt;[] nestedSerializers) { TypeSerializer\u0026lt;K\u0026gt; keySerializer = (TypeSerializer\u0026lt;K\u0026gt;) nestedSerializers[0]; TypeSerializer\u0026lt;V\u0026gt; valueSerializer = (TypeSerializer\u0026lt;V\u0026gt;) nestedSerializers[1]; return new MapSerializer\u0026lt;\u0026gt;(keySerializer, valueSerializer); } @Override protected TypeSerializer\u0026lt;?\u0026gt;[] getNestedSerializers(MapSerializer outerSerializer) { return new TypeSerializer\u0026lt;?\u0026gt;[] { outerSerializer.getKeySerializer(), outerSerializer.getValueSerializer() }; } } 当实现一个新的序列器快照作为 CompositeTypeSerializerSnapshot 的子类时，必须实现以下三个方法。\n #getCurrentOuterSnapshotVersion()。该方法定义了当前外部序列化器快照的序列化二进制格式的版本。 #getNestedSerializers(TypeSerializer)。给定外部序列化器，返回其嵌套的序列化器。 #createOuterSerializerWithNestedSerializers(TypeSerializer[])。给定嵌套的序列化器，创建一个外部序列化器的实例。  上面的例子是一个 CompositeTypeSerializerSnapshot，除了嵌套的序列化器的快照外，没有额外的信息需要快照。因此，可以预期其外部快照版本永远不需要上报。然而，其他一些序列化器，包含一些额外的静态配置，需要和嵌套的组件序列化器一起持久化。一个例子是 Flink 的 GenericArraySerializer，除了嵌套的元素序列化器之外，它还包含了数组元素类型的类作为配置。\n在这些情况下，需要在 CompositeTypeSerializerSnapshot 上实现另外三个方法。\n #writeOuterSnapshot(DataOutputView)：定义如何写入外部快照信息。 #readOuterSnapshot(int, DataInputView, ClassLoader)：定义如何读取外部快照信息。 #resolveOuterSchemaCompatibility(TypeSerializer)：根据外部快照信息检查兼容性。  默认情况下，CompositeTypeSerializerSnapshot 假设没有任何外部快照信息可读/可写，因此上述方法的默认实现为空。如果子类有外部快照信息，那么这三个方法必须全部实现。\n下面以 Flink 的 GenericArraySerializer 为例，说明 CompositeTypeSerializerSnapshot 如何用于确实有外部快照信息的复合序列器快照。\npublic final class GenericArraySerializerSnapshot\u0026lt;C\u0026gt; extends CompositeTypeSerializerSnapshot\u0026lt;C[], GenericArraySerializer\u0026gt; { private static final int CURRENT_VERSION = 1; private Class\u0026lt;C\u0026gt; componentClass; public GenericArraySerializerSnapshot() { super(GenericArraySerializer.class); } public GenericArraySerializerSnapshot(GenericArraySerializer\u0026lt;C\u0026gt; genericArraySerializer) { super(genericArraySerializer); this.componentClass = genericArraySerializer.getComponentClass(); } @Override protected int getCurrentOuterSnapshotVersion() { return CURRENT_VERSION; } @Override protected void writeOuterSnapshot(DataOutputView out) throws IOException { out.writeUTF(componentClass.getName()); } @Override protected void readOuterSnapshot(int readOuterSnapshotVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException { this.componentClass = InstantiationUtil.resolveClassByName(in, userCodeClassLoader); } @Override protected boolean resolveOuterSchemaCompatibility(GenericArraySerializer newSerializer) { return (this.componentClass == newSerializer.getComponentClass()) ? OuterSchemaCompatibility.COMPATIBLE_AS_IS : OuterSchemaCompatibility.INCOMPATIBLE; } @Override protected GenericArraySerializer createOuterSerializerWithNestedSerializers(TypeSerializer\u0026lt;?\u0026gt;[] nestedSerializers) { TypeSerializer\u0026lt;C\u0026gt; componentSerializer = (TypeSerializer\u0026lt;C\u0026gt;) nestedSerializers[0]; return new GenericArraySerializer\u0026lt;\u0026gt;(componentClass, componentSerializer); } @Override protected TypeSerializer\u0026lt;?\u0026gt;[] getNestedSerializers(GenericArraySerializer outerSerializer) { return new TypeSerializer\u0026lt;?\u0026gt;[] { outerSerializer.getComponentSerializer() }; } } 在上面的代码片段中，有两个重要的事情需要注意。首先，由于这个 CompositeTypeSerializerSnapshot 实现的外快照信息是作为快照的一部分写入的，所以每当外快照信息的序列化格式发生变化时，由 getCurrentOuterSnapshotVersion() 定义的外快照版本必须被上调。\n其次，请注意我们在写组件类时避免使用 Java 序列化，只写类名，在读回快照时动态加载。避免使用 Java 序列化来编写序列化器快照的内容，总的来说是一个很好的做法。关于这方面的更多细节将在下一节介绍。\n实施说明和最佳实践  Flink 通过将序列器快照实例化，恢复序列器快照，其类名为  序列器的快照，是注册状态如何被序列化的唯一真实来源，是读取保存点中状态的入口。为了能够恢复和访问以前的状态，必须能够恢复以前状态序列化器的快照。\nFlink 通过首先实例化 TypeSerializerSnapshot 与其类名（与快照字节一起写入）来恢复序列器快照。因此，为了避免受到意外的类名更改或实例化失败， TypeSerializerSnapshot 类应该。\n 避免被实现为匿名类或嵌套类。 有一个公共的空值构造函数用于实例化。  避免在不同的序列化器之间共享同一个 TypeSerializerSnapshot 类。  由于模式兼容性检查要通过序列化器快照，让多个序列化器返回同一个 TypeSerializerSnapshot 类作为它们的快照，会使 TypeSerializerSnapshot#resolveSchemaCompatibility 和 TypeSerializerSnapshot#restoreSerializer() 方法的实现变得复杂。\n这也将是一个不好的分离关注点，一个单一序列化器的序列化模式、配置以及如何恢复它，应该合并在自己专门的TypeSerializerSnapshot类中。\n避免使用 Java 序列化来制作序列化器快照内容  在编写持久化的序列化器快照的内容时，完全不应该使用 Java 序列化。例如，一个序列化器需要持久化一个目标类型的类作为其快照的一部分。关于类的信息应该通过写入类名来持久化，而不是直接使用 Java 将类序列化。在读取快照时，会读取类名，并通过名称来动态加载类。\n这种做法保证了序列化器快照总是可以安全读取。在上面的例子中，如果类型类是使用 Java 序列化来持久化的，一旦类的实现发生了变化，根据 Java 序列化的具体规定，快照可能不再可读，不再二进制兼容。\n从 Flink 1.7 之前的废弃序列化快照 API 迁移 本节是一个从 Flink 1.7 之前存在的序列化器和序列化器快照的 API 迁移指南。\n在 Flink 1.7 之前，序列化器快照是以 TypeSerializerConfigSnapshot 的形式实现的（现在已经被废弃了，将来最终会被移除，完全被新的 TypeSerializerSnapshot 接口取代）。此外，序列化器模式兼容性检查的责任住在 TypeSerializer 内部，在 TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) 方法中实现。\n新旧抽象之间的另一个主要区别是，被废弃的 TypeSerializerConfigSnapshot 不具备实例化之前的序列化器的能力。因此，在你的序列化器仍然返回 TypeSerializerConfigSnapshot 的子类作为它的快照的情况下，序列化器实例本身将总是使用 Java 序列化写入 savepoints，以便在还原时可以使用以前的序列化器。这是很不可取的，因为还原作业是否成功，很容易受到前一个序列化器类的可用性的影响，或者说，一般来说，序列化器实例是否可以在还原时使用 Java 序列化读回。这意味着你的状态只能使用同一个序列化器，一旦你想升级序列化器类或进行模式迁移，可能会出现问题。\n为了面向未来，并能灵活地迁移你的状态序列器和模式，强烈建议从旧的抽象中迁移。做到这一点的步骤如下。\n 实现 TypeSerializerSnapshot 的新子类。这将是你的序列化器的新快照。 在 TypeSerializer#snapshotConfiguration() 方法中返回新的 TypeSerializerSnapshot 作为你的 serializer 快照。 从 Flink 1.7 之前存在的保存点恢复作业，然后再取一个保存点。注意，在这一步，旧的序列化器的 TypeSerializerConfigSnapshot 必须仍然存在于 classpath 中，并且不能删除 TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) 方法的实现。这个过程的目的是将旧保存点中写的 TypeSerializerConfigSnapshot 替换为序列化器新实现的 TypeSerializerSnapshot。 一旦你有一个用 Flink 1.7 拍摄的保存点，保存点将包含 TypeSerializerSnapshot 作为状态序列化器快照，序列化器实例将不再写入保存点中。在这一点上，现在可以安全地删除旧抽象的所有实现（从序列化器中删除旧的 TypeSerializerConfigSnapshot 实现，因为将作为 TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot)）。  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-custom-serialization-for-managed-state/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"自定义序列化管理状态"},{"categories":["Flink"],"contents":"DESCRIBE 语句 DESCRIBE 语句用于描述表或视图的模式。\n运行一个DESCRIBE语句 DESCRIBE语句可以用TableEnvironment的executeSql()方法执行，也可以在SQL CLI中执行。executeSql()方法对于一个成功的DESCRIBE操作会返回给定表的模式，否则会抛出一个异常。\n下面的例子展示了如何在TableEnvironment和SQL CLI中运行DESCRIBE语句。\nval settings = EnvironmentSettings.newInstance()... val tableEnv = TableEnvironment.create(settings) // register a table named \u0026#34;Orders\u0026#34;  tableEnv.executeSql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `user` BIGINT NOT NULl,\u0026#34; + \u0026#34; product VARCHAR(32),\u0026#34; + \u0026#34; amount INT,\u0026#34; + \u0026#34; ts TIMESTAMP(3),\u0026#34; + \u0026#34; ptime AS PROCTIME(),\u0026#34; + \u0026#34; PRIMARY KEY(`user`) NOT ENFORCED,\u0026#34; + \u0026#34; WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; SECONDS\u0026#34; + \u0026#34;) with (...)\u0026#34;) // print the schema tableEnv.executeSql(\u0026#34;DESCRIBE Orders\u0026#34;).print() Flink SQL\u0026gt; CREATE TABLE Orders ( \u0026gt; `user` BIGINT NOT NULl, \u0026gt; product VARCHAR(32), \u0026gt; amount INT, \u0026gt; ts TIMESTAMP(3), \u0026gt; ptime AS PROCTIME(), \u0026gt; PRIMARY KEY(`user`) NOT ENFORCED, \u0026gt; WATERMARK FOR ts AS ts - INTERVAL '1' SECONDS \u0026gt; ) with ( \u0026gt; ... \u0026gt; ); [INFO] Table has been created. Flink SQL\u0026gt; DESCRIBE Orders; root |-- user: BIGINT NOT NULL |-- product: VARCHAR(32) |-- amount: INT |-- ts: TIMESTAMP(3) *ROWTIME* |-- ptime: TIMESTAMP(3) NOT NULL *PROCTIME* AS PROCTIME() |-- WATERMARK FOR ts AS `ts` - INTERVAL '1' SECOND |-- CONSTRAINT PK_3599338 PRIMARY KEY (user) 上述例子的结果是：\n+---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | name | type | null | key | computed column | watermark | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL '1' SECOND | | ptime | TIMESTAMP(3) NOT NULL *PROCTIME* | false | | PROCTIME() | | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ 5 rows in set 语法 DESCRIBE[catalog_name.][db_name.]table_name原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/describe.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-run-a-describe-statement/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL","SQL"],"title":"运行 Describe 语句"},{"categories":["Flink"],"contents":"连续查询中的 Join 在批处理数据时，连接是一种常见的、好理解的操作，用来连接两个关系的行。然而，在动态表上的连接的语义就不那么明显了，甚至是混乱的。\n正因为如此，有几种方法可以使用 Table API 或 SQL 实际执行连接。\n关于语法的更多信息，请查看 Table API 和 SQL 中的连接部分。\n常规连接 常规联接是最通用的联接类型，联接输入的任何一条新记录或变化都是可见的，并影响整个联接结果。例如，如果左边有一条新记录，它将与右边所有以前和将来的记录一起连接。\nSELECT*FROMOrdersINNERJOINProductONOrders.productId=Product.id这些语义允许任何形式的更新（插入、更新、删除）输入表。\n然而，这种操作有一个重要的含义：它需要将 join 输入的双方永远保持在 Flink 的状态中。因此，如果一个或两个输入表持续增长，资源使用量也会无限增长。\n区间连接 区间联接是由联接谓词定义的，它检查输入记录的时间属性是否在一定的时间限制内，即时间窗口。\nSELECT*FROMOrderso,ShipmentssWHEREo.id=s.orderIdANDo.ordertimeBETWEENs.shiptime-INTERVAL\u0026#39;4\u0026#39;HOURANDs.shiptime与普通的 join 操作相比，这种 join 只支持带有时间属性的 append-only 表。由于时间属性是准单调递增的，所以 Flink 可以在不影响结果正确性的情况下，从其状态中删除旧值。\n用临时表函数进行联接 使用时态表函数的连接，将一个只附加表（左输入/探针侧）与一个时态表（右输入/建立侧）连接起来，即一个随时间变化的表，并跟踪其变化。关于临时表的更多信息，请查看相应页面。\n下面的例子显示了一个只附加表 Orders，它应该与不断变化的货币汇率表 RatesHistory 连接。\nOrders 是一个只附加表，表示给定金额和给定货币的付款。例如在 10:15 有一个金额为 2 欧元的订单。\nSELECT*FROMOrders;rowtimeamountcurrency======================10:152Euro10:301USDollar10:3250Yen10:523Euro11:045USDollarRatesHistory 代表了一个不断变化的对日元（汇率为 1）的货币汇率附加表。例如，从 09:00 到 10:45，欧元对日元的汇率是 114，从 10:45 到 11:15 是 116。从 10:45 到 11:15 是 116。\nSELECT*FROMRatesHistory;rowtimecurrencyrate=====================09:00USDollar10209:00Euro11409:00Yen110:45Euro11611:15Euro11911:49Pounds108我们想计算所有订单的金额，并将其换算成一种通用货币（日元）。\n例如，我们想使用给定行时间(114)的适当换算率换算以下订单。\nrowtime amount currency ======= ====== ========= 10:15 2 Euro 如果不使用临时表的概念，就需要写一个类似的查询。\nSELECTSUM(o.amount*r.rate)ASamountFROMOrdersASo,RatesHistoryASrWHEREr.currency=o.currencyANDr.rowtime=(SELECTMAX(rowtime)FROMRatesHistoryASr2WHEREr2.currency=o.currencyANDr2.rowtime\u0026lt;=o.rowtime);在临时表函数 Rates over RatesHistory 的帮助下，我们可以将这样的查询用 SQL 表达为:\nSELECTo.amount*r.rateASamountFROMOrdersASo,LATERALTABLE(Rates(o.rowtime))ASrWHEREr.currency=o.currency来自探针侧的每条记录将与构建侧表在探针侧记录的相关时间属性时的版本连接。为了支持更新（覆盖）构建侧表的先前值，表必须定义一个主键。\n在我们的例子中，来自 Orders 的每条记录将与 Rates 的版本在时间 o.rowtime 连接。货币字段之前已经被定义为 Rates 的主键，在我们的例子中用来连接两个表。如果查询使用的是处理时间的概念，那么在执行操作时，新添加的订单将始终与 Rates 的最新版本连接。\n与常规的连接不同，这意味着如果在构建端有新的记录，不会影响之前的连接结果。这又使得 Flink 可以限制必须保留在状态中的元素数量。\n与区间联接相比，时间表联接并没有定义一个时间窗口，在这个时间窗口的范围内，记录将被加入。来自探针侧的记录总是在时间属性指定的时间与构建侧的版本进行连接。因此，构建侧的记录可能是任意的旧记录。随着时间的流逝，记录（对于给定的主键）以前的和不再需要的版本将从状态中删除。\n这样的行为使得时间表连接成为用关系术语来表达流丰富的一个很好的候选。\n使用方法 定义了临时表函数之后，我们就可以开始使用它了。时间表函数的使用方法可以和普通表函数的使用方法一样。\n下面的代码片段解决了我们的动机问题，即从订单表中转换货币。\nSELECTSUM(o_amount*r_rate)ASamountFROMOrders,LATERALTABLE(Rates(o_proctime))WHEREr_currency=o_currency// scala val result = orders .joinLateral(rates(\u0026#39;o_proctime), \u0026#39;r_currency === \u0026#39;o_currency) .select((\u0026#39;o_amount * \u0026#39;r_rate).sum as \u0026#39;amount) 注意：在查询配置中定义的状态保留还没有实现时序连接。这意味着计算查询结果所需的状态可能会根据历史表的不同主键的数量而无限增长。\n处理时间的 Temporal 连接 有了处理时间时间属性，就不可能将过去的时间属性作为参数传递给时序表函数。根据定义，它总是当前的时间戳。因此，对处理时间时间表函数的调用将始终返回底层表的最新已知版本，底层历史表的任何更新也将立即覆盖当前值。\n只有构建侧记录的最新版本（相对于定义的主键）才会保存在状态中。构建侧的更新不会对之前发出的连接结果产生影响。\n我们可以把处理时的时空联接看成一个简单的 HashMap\u0026lt;K，V\u0026gt;，它存储了来自构建侧的所有记录。当来自构建侧的新记录与之前的某个记录具有相同的键时，旧的值只是简单地被覆盖。来自探针侧的每条记录总是根据 HashMap 的最近/当前状态进行评估。\n事件时间的 Temporal 连接 有了事件时间属性（即行时间属性），就可以将过去的时间属性传递给时间表函数。这样就可以在一个共同的时间点上连接两个表。\n与处理时间的时空连接相比，时空表不仅保留了状态下构建方记录的最新版本（相对于定义的主键），而且还存储了自上次水印以来的所有版本（通过时间来识别）。\n例如，一个事件时间时间戳为 12:30:00 的传入行被追加到探针侧表中，根据临时表的概念，它与构建侧表中时间为 12:30:00 的版本连接。因此，传入的行只与时间戳小于或等于 12:30:00 的行连接，并根据主键应用更新，直到这个时间点。\n根据事件时间的定义，水印允许联接操作在时间上向前移动，并丢弃不再需要的构建表的版本，因为预计不会有时间戳较低或相等的传入行。\n用时间表进行联接 带时态表的连接将一个任意表（左输入/探针侧）与一个时态表（右输入/建立侧）连接起来，即一个随时间变化的外部维度表。关于时间表的详细信息，请查看相应页面。\n注意: 用户不能使用任意表作为时间表，而是需要使用一个由 LookupableTableSource 支持的表。一个 LookupableTableSource 只能作为一个时态表用于时态连接。有关如何定义 LookupableTableSource 的详细信息，请参见页面。\n下面的示例显示了一个 Orders 流，它应该与不断变化的货币汇率表 LatestRates 进行连接。\nLatestRates 是一个维度表，它是以最新的汇率来实现的。在时间 10:15、10:30、10:52，LatestRates 的内容如下。\n10:15\u0026gt;SELECT*FROMLatestRates;currencyrate==============USDollar102Euro114Yen110:30\u0026gt;SELECT*FROMLatestRates;currencyrate==============USDollar102Euro114Yen110:52\u0026gt;SELECT*FROMLatestRates;currencyrate==============USDollar102Euro116\u0026lt;====changedfrom114to116Yen1时间 10:15 和 10:30 的 LastestRates 内容相等。欧元汇率在 10:52 从 114 变为 116。\n订单是一个只附加的表，表示给定金额和给定货币的支付。例如在 10:15 有一个金额为 2 欧元的订单。\nSELECT*FROMOrders;amountcurrency===============2Euro\u0026lt;==arrivedattime10:151USDollar\u0026lt;==arrivedattime10:302Euro\u0026lt;==arrivedattime10:52我们想计算所有订单的金额，并将其兑换成一种通用货币（日元）。\n例如，我们想使用 LatestRates 中的最新汇率来转换以下订单。结果将是：\namount currency rate amout*rate ====== ========= ======= ============ 2 Euro 114 228 \u0026lt;== arrived at time 10:15 1 US Dollar 102 102 \u0026lt;== arrived at time 10:30 2 Euro 116 232 \u0026lt;== arrived at time 10:52 在时间表连接的帮助下，我们可以将这样的查询用 SQL 表达为。\nSELECTo.amout,o.currency,r.rate,o.amount*r.rateFROMOrdersASoJOINLatestRatesFORSYSTEM_TIMEASOFo.proctimeASrONr.currency=o.currency来自探针侧的每一条记录都将与构建侧表的当前版本相连接。在我们的例子中，查询使用的是处理时间的概念，所以在执行操作时，新追加的订单将始终与最新版本的 LatestRates 连接。需要注意的是，结果并不是处理时间的确定性。\n与常规联接相比，尽管构建端发生了变化，但时态表联接之前的结果不会受到影响。另外，时态表连接操作符非常轻量级，不保留任何状态。\n与区间联接相比，时态表联接不定义记录联接的时间窗口。在处理时，来自探针侧的记录总是与构建侧的最新版本连接。因此，构建侧的记录可能是任意旧的。\n时态表函数连接和时态表连接的动机都是一样的，但在 SQL 语法和运行时的实现上却有所不同。\n 时间表函数 join 的 SQL 语法是 join UDTF，而时间表 join 使用 SQL:2011 中引入的标准时间表语法。 时态表函数 join 的实现实际上是将两个流连接起来并保持状态，而时态表 join 只是接收唯一的输入流，并根据记录中的键查找外部数据库。 时态表函数联接通常用于联接变更日志流，而时态表联接通常用于联接外部表（即维表）。  这样的行为使得时态表连接成为用关系术语来表达流丰富的一个很好的候选。\n在未来，时态表连接将支持时态表函数连接的特性，即支持时态连接 changelog 流。\n使用方法 时间表连接的语法如下。\nSELECT[column_list]FROMtable1[AS\u0026lt;alias1\u0026gt;][LEFT]JOINtable2FORSYSTEM_TIMEASOFtable1.proctime[AS\u0026lt;alias2\u0026gt;]ONtable1.column-name1=table2.column-name1目前只支持 INNER JOIN 和 LEFT JOIN。在 temporal 表后应跟上 FOR SYSTEM_TIME AS OF table1.proctime。proctime 是 table1 的处理时间属性。这意味着它在处理时间对时间表进行快照，当从左表连接每一条记录时，它就会对时间表进行快照。\n例如，在定义了 temporal 表之后，我们可以按以下方式使用。\nSELECTSUM(o_amount*r_rate)ASamountFROMOrdersJOINLatestRatesFORSYSTEM_TIMEASOFo_proctimeONr_currency=o_currency注意: 这只在 Blink 计划器中支持。\n注意: 目前只在 SQL 中支持，在 Table API 中还不支持。\n注意: Flink 目前不支持事件时间的表连接。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-join-in-continuous-queries/","tags":["Flink","Flink 官方文档","Table API \u0026 SQL"],"title":"连续查询中的 Join"},{"categories":["Flink"],"contents":"迭代 迭代算法出现在数据分析的许多领域，如机器学习或图形分析。为了实现大数据的承诺，从数据中提取有意义的信息，此类算法至关重要。随着人们对在非常大的数据集上运行这类算法的兴趣越来越大，就需要以大规模并行的方式执行迭代。\nFlink 程序通过定义一个步骤函数并将其嵌入到一个特殊的迭代运算符中来实现迭代算法。这个运算符有两个变体。Iterate 和 Delta Iterate。这两个运算符都是在当前的迭代状态上反复调用步骤函数，直到达到某个终止条件。\n在这里，我们提供了这两个操作符变体的背景，并概述了它们的用法。编程指南解释了如何在 Scala 和 Java 中实现这些操作符。我们还通过 Flink 的图处理 API Gelly 支持以顶点为中心的迭代和集和应用迭代。\n下表提供了这两种运算符的概述:\n    Iterate Delta Iterate     Iteration 输入 Partial Solution Workset and Solution Set   Step 函数 Arbitrary Data Flows Arbitrary Data Flows   State Update Next partial solution Next workset,Changes to solution set   Iteration Result Last partial solution Solution set state after last iteration   Termination Maximum number of iterations (default),Custom aggregator convergence Maximum number of iterations or empty workset (default),Custom aggregator convergence    Iterate Operator 迭代运算符涵盖了简单的迭代形式：在每一次迭代中，step 函数都会消耗整个输入（上一次迭代的结果，或初始数据集），并计算出下一个版本的部分解（如 map, reduce, join 等）。\n 迭代输入。第一次迭代的初始输入，来自数据源或之前的运算符。 step 函数。步骤函数将在每次迭代中执行。它是一个任意的数据流，由 map、reduce、join 等运算符组成，取决于你手头的具体任务。 下一个部分解决方案。在每次迭代中，步骤函数的输出将被反馈到下一次迭代中。 迭代结果。上一次迭代的输出会被写入数据接收器，或者作为后续运算符的输入。  有多个选项可以指定迭代的终止条件。\n 最大迭代次数。没有任何进一步的条件，迭代将被执行这么多次。 自定义聚合器收敛。迭代允许指定自定义聚合器和收敛标准，比如对发出的记录数量进行加总（聚合器），如果这个数字为零就终止（收敛标准）。  你也可以用伪代码来思考迭代操作符。\nIterationState state = getInitialState(); while (!terminationCriterion()) { state = step(state); } setFinalState(state); 详情和代码示例请参见编程指南。\n例子: 数字递增 在下面的例子中，我们对一组数字进行迭代递增。\n 迭代输入。初始输入是从数据源读取的，由5个单字段记录组成（整数1至5）。 step 函数。步进函数是一个单一的 map 运算符，它将整数字段从i递增到i+1。它将被应用于输入的每一条记录。 下一个部分解。step 函数的输出将是 map 运算符的输出，也就是整数递增的记录。 迭代结果。经过十次迭代，初始数字将被递增十倍，结果是整数11到15。  // 1st 2nd 10th map(1) -\u0026gt; 2 map(2) -\u0026gt; 3 ... map(10) -\u0026gt; 11 map(2) -\u0026gt; 3 map(3) -\u0026gt; 4 ... map(11) -\u0026gt; 12 map(3) -\u0026gt; 4 map(4) -\u0026gt; 5 ... map(12) -\u0026gt; 13 map(4) -\u0026gt; 5 map(5) -\u0026gt; 6 ... map(13) -\u0026gt; 14 map(5) -\u0026gt; 6 map(6) -\u0026gt; 7 ... map(14) -\u0026gt; 15 请注意，1、2和4可以是任意的数据流。\n增量迭代运算符 delta 迭代算子涵盖了增量迭代的情况。增量迭代有选择地修改其解的元素，并对解进行演化，而不是完全重新计算。\n在适用的情况下，这将导致更高效的算法，因为在每次迭代中，并不是解集中的每个元素都会改变。这样就可以把注意力集中在解的热点部分，而对冷点部分不加处理。通常情况下，大部分解的冷却速度比较快，后面的迭代只对一小部分数据进行操作。\n 迭代输入。从数据源或以前的运算符中读取初始工作集和解决方案集，作为第一次迭代的输入。 step 函数。在每次迭代中，步骤函数将被执行。它是一个任意的数据流，由 map、reduce、join 等运算符组成，取决于你手头的具体任务。 下一个工作集/更新解决方案集。下一个工作集驱动迭代计算，并将反馈到下一个迭代中。此外，解决方案集将被更新并隐式转发（它不需要被重建）。这两个数据集都可以通过步长函数的不同运算符进行更新。 迭代结果。最后一次迭代后，解集被写入数据接收器，或作为下面运算符的输入。  delta 迭代的默认终止条件由空工作集收敛准则和最大迭代次数指定。当产生的下一个工作集为空或达到最大迭代次数时，迭代将终止。也可以指定一个自定义的聚合器和收敛准则。\n你也可以用伪代码来思考迭代操作符。\nIterationState workset = getInitialState(); IterationState solution = getInitialSolution(); while (!terminationCriterion()) { (delta, workset) = step(workset, solution); solution.update(delta) } setFinalState(solution); 详情和代码示例请参见编程指南。\n例子: 在图中传播最小值 在下面的例子中，每个顶点都有一个ID和一个着色。每个顶点将把它的顶点ID传播给邻近的顶点。目标是给子图中的每个顶点分配最小的ID。如果一个接收到的ID比当前的ID小，它就会改变成接收到ID的顶点的颜色。这在社区分析或连接组件计算中可以找到一个应用。\n初始输入被设定为工作集和解决方案集。在上图中，颜色直观地显示了解决方案集的演变。随着每次迭代，最小ID的颜色在各自的子图中蔓延。同时，每一次迭代，工作量（交换和比较顶点ID）都在减少。这对应于工作集的大小递减，在三次迭代后，工作集从所有七个顶点变为零，此时迭代终止。重要的观察是，下半子图在上半子图之前收敛，而delta迭代能够用工作集抽象捕捉到这一点。\n在上子图中，ID 1（橙色）是最小ID。在第一次迭代中，它将被传播到顶点2，随后它的颜色将变为橙色。顶点3和4将收到ID 2（黄色）作为它们当前的最小ID，并改变为黄色。因为顶点1的颜色在第一次迭代中没有改变，所以在下一个工作集中可以跳过它。\n在下层子图中，ID 5（青色）是最小ID。下层子图的所有顶点都会在第一次迭代中收到它。同样，我们可以在下一个工作集中跳过没有变化的顶点（顶点5）。\n在第2次迭代中，工作集大小已经从7个元素减少到5个元素（顶点2、3、4、6和7）。这些都是迭代的一部分，并进一步传播它们当前的最小ID。在这次迭代之后，下半部分子图已经收敛了（图的冷部分），因为它在工作集中没有元素，而上半部分则需要对剩下的两个工作集元素（顶点3和4）进行进一步的迭代（图的热部分）。\n当第3次迭代后工作集为空时，迭代终止。\nSuperstep 同步 我们将迭代操作符的步骤函数的每次执行称为单次迭代。在并行设置中，步骤函数的多个实例在迭代状态的不同分区上并行评估。在许多设置中，在所有并行实例上对步骤函数的一次评估形成一个所谓的超级步骤，这也是同步的粒度。因此，一个迭代的所有并行任务都需要完成 superstep，才会初始化下一个 superstep。终止标准也将在 superstep 障碍处进行评估。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-iterations/","tags":["Flink","Flink 官方文档","DataSet API"],"title":"迭代"},{"categories":["Flink"],"contents":"Flink 程序可以在许多机器组成的集群上分布式运行。有两种方法可以将程序发送到集群上执行。\n命令行接口 命令行界面让您可以将打包的程序（JAR）提交到集群（或单机设置）。\n详情请参考命令行接口文档。\n远程环境 远程环境可以让你直接在集群上执行 Flink Java 程序。远程环境指向你要执行程序的集群。\nMaven 依赖 如果你是以 Maven 项目的形式开发程序，你必须使用这个依赖关系添加 flink-clients 模块。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-clients_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 例子 以下说明了 RemoteEnvironment 的使用。\npublic static void main(String[] args) throws Exception { ExecutionEnvironment env = ExecutionEnvironment .createRemoteEnvironment(\u0026#34;flink-jobmanager\u0026#34;, 8081, \u0026#34;/home/user/udfs.jar\u0026#34;); DataSet\u0026lt;String\u0026gt; data = env.readTextFile(\u0026#34;hdfs://path/to/file\u0026#34;); data .filter(new FilterFunction\u0026lt;String\u0026gt;() { public boolean filter(String value) { return value.startsWith(\u0026#34;http://\u0026#34;); } }) .writeAsText(\u0026#34;hdfs://path/to/result\u0026#34;); env.execute(); } 请注意，该程序包含自定义用户代码，因此需要一个包含代码类的 JAR 文件。远程环境的构造函数使用 JAR 文件的路径。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/cluster_execution.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-cluster-execution/","tags":["Flink","Flink 官方文档","Cluster Execution"],"title":"集群 Execution"},{"categories":["Flink"],"contents":"项目配置 每个 Flink 应用都依赖于一组 Flink 库。最起码，应用程序依赖于 Flink APIs。许多应用还依赖于某些连接器库（如 Kafka、Cassandra 等）。当运行 Flink 应用时（无论是在分布式部署中，还是在 IDE 中进行测试），Flink 运行时库也必须是可用的。\nFlink 核心和应用依赖性 与大多数运行用户定义应用的系统一样，Flink 中的依赖和库有两大类。\n Flink 核心依赖。Flink 本身由一组运行系统所需的类和依赖关系组成，例如协调、网络、检查点、故障转移、API、操作（如窗口化）、资源管理等。所有这些类和依赖项的集合构成了 Flink 运行时的核心，在 Flink 应用启动时必须存在。  这些核心类和依赖项被打包在 flink-dist jar 中。它们是 Flink 的 lib 文件夹的一部分，也是基本的 Flink 容器镜像的一部分。把这些依赖关系想象成类似于 Java 的核心库（rt.jar，charsets.jar 等），其中包含了 String 和 List 等类。\nFlink Core Dependencies 不包含任何连接器或库（CEP、SQL、ML 等），以避免默认情况下 classpath 中的依赖关系和类数量过多。事实上，我们尽量让核心依赖关系保持纤细，以保持默认 classpath 小，避免依赖冲突。\n 用户应用依赖是指特定用户应用所需要的所有连接器、格式或库。  用户应用程序通常被打包成一个应用程序 jar，其中包含了应用程序代码和所需的连接器和库依赖。\n用户应用依赖关系明确不包括 Flink DataStream API 和运行时依赖关系，因为这些已经是 Flink 核心依赖关系的一部分。\n设置一个项目: 基本依赖性 每一个 Flink 应用都需要最低限度的 API 依赖关系，来进行开发。\n当手动设置项目时，你需要为 Java/Scala API 添加以下依赖关系（这里用 Maven 语法表示，但同样的依赖关系也适用于其他构建工具（Gradle、SBT 等）。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-streaming-scala_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 重要：请注意，所有这些依赖关系的范围都被设置为 provided。这意味着它们需要被编译，但它们不应该被打包到项目的应用程序 jar 文件中\u0026ndash;这些依赖是 Flink 核心依赖，在任何设置中都是可用的。\n强烈建议将这些依赖关系保持在 provid 的作用域内。如果它们没有被设置为 provided，最好的情况是生成的 JAR 变得过大，因为它也包含了所有 Flink 核心依赖。最坏的情况是，添加到应用程序的 jar 文件中的 Flink 核心依赖与你自己的一些依赖版本发生冲突（通常通过倒类加载来避免）。\n关于 IntelliJ 的说明：要使应用程序在 IntelliJ IDEA 中运行，就必须在运行配置中勾选 Include dependencies with \u0026ldquo;Provided\u0026rdquo; scope box。如果这个选项不可用（可能是由于使用了旧的 IntelliJ IDEA 版本），那么一个简单的变通方法是创建一个调用应用程序 main() 方法的测试。\n添加连接器和库依赖性 大多数应用都需要特定的连接器或库来运行，例如与 Kafka、Cassandra 等的连接器。这些连接器不是 Flink 核心依赖的一部分，必须作为依赖关系添加到应用程序中。\n下面是一个将 Kafka 的连接器作为依赖项添加的例子（Maven 语法）。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-kafka_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 我们建议将应用程序代码和所有需要的依赖关系打包成一个带有依赖关系的 jar，我们称之为应用 jar。应用 jar 可以提交给一个已经运行的 Flink 集群，或者添加到 Flink 应用容器镜像中。\n从 Java 项目模板或 Scala 项目模板创建的项目被配置为在运行 mvn clean package 时自动将应用依赖关系包含到应用 jar 中。对于没有从这些模板中设置的项目，我们建议添加 Maven Shade Plugin（如下文附录中所列）来构建包含所有所需依赖项的应用 jar。\n重要的是。为了让 Maven（和其他构建工具）正确地将依赖关系打包到应用 jar 中，这些应用依赖关系必须在编译范围中指定（与核心依赖关系不同，后者必须在提供的范围中指定）。\nScala 版本 Scala 版本(2.11, 2.12 等)彼此之间不是二进制兼容的。因此，Flink for Scala 2.11 不能用于使用 Scala 2.12 的应用程序。\n所有的 Flink 依赖性都是以 Scala 版本为后缀的，例如 flink-streaming-scala_2.11。\n只使用 Java 的开发者可以选择任何 Scala 版本，Scala 开发者需要选择与其应用的 Scala 版本相匹配的 Scala 版本。\n请参考构建指南，了解如何为特定的 Scala 版本构建 Flink。\nHadoop 依赖性 一般规则：永远不需要直接将 Hadoop 依赖关系添加到您的应用程序中。唯一的例外是当使用现有的 Hadoop 输入/输出格式和 Flink 的 Hadoop 兼容性包装时。\n如果您想将 Flink 与 Hadoop 一起使用，您需要有一个包含 Hadoop 依赖的 Flink 设置，而不是将 Hadoop 添加为应用程序依赖。详情请参考 Hadoop 设置指南。\n这种设计主要有两个原因。\n  一些 Hadoop 交互发生在 Flink 的核心中，可能是在用户应用启动之前，例如为检查点设置 HDFS，通过 Hadoop 的 Kerberos 令牌进行认证，或者在 YARN 上进行部署。\n  Flink 的倒类加载方法将许多过渡性依赖从核心依赖中隐藏起来。这不仅适用于 Flink 自身的核心依赖，也适用于 Hadoop 在设置中存在的依赖。这样一来，应用程序可以使用相同依赖的不同版本，而不会遇到依赖冲突（相信我们，这是一个大问题，因为 Hadoop 的依赖树是巨大的）。\n  如果你在 IDE 内部的测试或开发过程中需要 Hadoop 依赖关系（例如用于 HDFS 访问），请将这些依赖关系配置成类似于要测试或提供的依赖关系的范围。\nMaven 快速入门 所需\n唯一的要求是工作中的 Maven 3.0.4（或更高）和 Java 8.x 的安装。\n创建项目\n使用以下命令之一来创建项目。\n 使用 Maven 原型  $ mvn archetype:generate \\  -DarchetypeGroupId=org.apache.flink \\  -DarchetypeArtifactId=flink-quickstart-java \\  -DarchetypeVersion=1.11.0 这可以让你为新创建的项目命名，它将交互式地要求你提供 groupId、artifactId 和包名。\n 运行快速启动脚本  $ curl https://flink.apache.org/q/quickstart.sh | bash -s 1.11.0 我们建议您将该项目导入到您的 IDE 中进行开发和测试。IntelliJ IDEA 支持开箱即用的 Maven 项目。如果您使用 Eclipse，m2e 插件允许导入 Maven 项目。有些 Eclipse 捆绑包默认包含该插件，有些则需要您手动安装。\n请注意：Java 默认的 JVM 堆大小对 Flink 来说可能太小。你必须手动增加它。在 Eclipse 中，选择 Run Configurations -\u0026gt; Arguments，并在 VM Arguments 框中写下 -Xmx800m。在 IntelliJ IDEA 中推荐的改变 JVM 选项的方法是来自 Help | Edit Custom VM Options 菜单。详情请看这篇文章。\n构建项目 如果你想构建/打包你的项目，进入你的项目目录并运行 \u0026ldquo;mvn clean package\u0026rdquo; 命令。你会发现一个 JAR 文件，其中包含了你的应用程序，加上你可能已经添加的连接器和库作为应用程序的依赖关系：target/\u0026lt;artifact-id\u0026gt;-\u0026lt;version\u0026gt;.jar。\n注意：如果您使用与 StreamingJob 不同的类作为应用程序的主类/入口点，我们建议您相应地更改 pom.xml 文件中的 mainClass 设置。这样，Flink 就可以从 JAR 文件中运行应用程序，而不需要额外指定主类。\nGradle 需求\n唯一的要求是工作的 Gradle 3.x（或更高）和 Java 8.x 安装。\n创建项目\n使用以下命令之一来创建一个项目。\n Gradle 例子  build.gradle\nbuildscript { repositories { jcenter() // this applies only to the Gradle \u0026#39;Shadow\u0026#39; plugin } dependencies { classpath \u0026#39;com.github.jengelman.gradle.plugins:shadow:2.0.4\u0026#39; } } plugins { id \u0026#39;java\u0026#39; id \u0026#39;application\u0026#39; // shadow plugin to produce fat JARs id \u0026#39;com.github.johnrengelman.shadow\u0026#39; version \u0026#39;2.0.4\u0026#39; } // artifact properties group = \u0026#39;org.myorg.quickstart\u0026#39; version = \u0026#39;0.1-SNAPSHOT\u0026#39; mainClassName = \u0026#39;org.myorg.quickstart.StreamingJob\u0026#39; description = \u0026#34;\u0026#34;\u0026#34;Flink Quickstart Job\u0026#34;\u0026#34;\u0026#34; ext { javaVersion = \u0026#39;1.8\u0026#39; flinkVersion = \u0026#39;1.11.0\u0026#39; scalaBinaryVersion = \u0026#39;2.11\u0026#39; slf4jVersion = \u0026#39;1.7.15\u0026#39; log4jVersion = \u0026#39;2.12.1\u0026#39; } sourceCompatibility = javaVersion targetCompatibility = javaVersion tasks.withType(JavaCompile) { options.encoding = \u0026#39;UTF-8\u0026#39; } applicationDefaultJvmArgs = [\u0026#34;-Dlog4j.configurationFile=log4j2.properties\u0026#34;] task wrapper(type: Wrapper) { gradleVersion = \u0026#39;3.1\u0026#39; } // declare where to find the dependencies of your project repositories { mavenCentral() maven { url \u0026#34;https://repository.apache.org/content/repositories/snapshots/\u0026#34; } } // NOTE: We cannot use \u0026#34;compileOnly\u0026#34; or \u0026#34;shadow\u0026#34; configurations since then we could not run code // in the IDE or with \u0026#34;gradle run\u0026#34;. We also cannot exclude transitive dependencies from the // shadowJar yet (see https://github.com/johnrengelman/shadow/issues/159). // -\u0026gt; Explicitly define the // libraries we want to be included in the \u0026#34;flinkShadowJar\u0026#34; configuration! configurations { flinkShadowJar // dependencies which go into the shadowJar // always exclude these (also from transitive dependencies) since they are provided by Flink flinkShadowJar.exclude group: \u0026#39;org.apache.flink\u0026#39;, module: \u0026#39;force-shading\u0026#39; flinkShadowJar.exclude group: \u0026#39;com.google.code.findbugs\u0026#39;, module: \u0026#39;jsr305\u0026#39; flinkShadowJar.exclude group: \u0026#39;org.slf4j\u0026#39; flinkShadowJar.exclude group: \u0026#39;org.apache.logging.log4j\u0026#39; } // declare the dependencies for your production and test code dependencies { // -------------------------------------------------------------- // Compile-time dependencies that should NOT be part of the // shadow jar and are provided in the lib folder of Flink // -------------------------------------------------------------- compile \u0026#34;org.apache.flink:flink-streaming-java_${scalaBinaryVersion}:${flinkVersion}\u0026#34; // -------------------------------------------------------------- // Dependencies that should be part of the shadow jar, e.g. // connectors. These must be in the flinkShadowJar configuration! // -------------------------------------------------------------- //flinkShadowJar \u0026#34;org.apache.flink:flink-connector-kafka-0.11_${scalaBinaryVersion}:${flinkVersion}\u0026#34; compile \u0026#34;org.apache.logging.log4j:log4j-api:${log4jVersion}\u0026#34; compile \u0026#34;org.apache.logging.log4j:log4j-core:${log4jVersion}\u0026#34; compile \u0026#34;org.apache.logging.log4j:log4j-slf4j-impl:${log4jVersion}\u0026#34; compile \u0026#34;org.slf4j:slf4j-log4j12:${slf4jVersion}\u0026#34; // Add test dependencies here. // testCompile \u0026#34;junit:junit:4.12\u0026#34; } // make compileOnly dependencies available for tests: sourceSets { main.compileClasspath += configurations.flinkShadowJar main.runtimeClasspath += configurations.flinkShadowJar test.compileClasspath += configurations.flinkShadowJar test.runtimeClasspath += configurations.flinkShadowJar javadoc.classpath += configurations.flinkShadowJar } run.classpath = sourceSets.main.runtimeClasspath jar { manifest { attributes \u0026#39;Built-By\u0026#39;: System.getProperty(\u0026#39;user.name\u0026#39;), \u0026#39;Build-Jdk\u0026#39;: System.getProperty(\u0026#39;java.version\u0026#39;) } } shadowJar { configurations = [project.configurations.flinkShadowJar] } setting.gradle\nrootProject.name = \u0026#39;quickstart\u0026#39; 这允许你为你新创建的项目命名，它将交互式地询问你项目的名称、组织（也用于包名）、项目版本、Scala 和 Flink。它将交互式地要求你提供项目名称、组织（也用于包名）、项目版本、Scala 和 Flink 版本。\n 运行快速启动脚本  bash -c \u0026#34;$(curl https://flink.apache.org/q/gradle-quickstart.sh)\u0026#34; -- 1.11.0 2.11 我们建议你将这个项目导入到你的 IDE 中进行开发和测试。IntelliJ IDEA 在安装 Gradle 插件后，支持 Gradle 项目。Eclipse 通过 Eclipse Buildship 插件来实现（确保在导入向导的最后一步指定 Gradle 版本\u0026gt;=3.0，影子插件需要它）。你也可以使用 Gradle 的 IDE 集成来从 Gradle 创建项目文件。\n请注意：Java 默认的 JVM 堆大小对 Flink 来说可能太小。你必须手动增加它。在 Eclipse 中，选择 Run Configurations -\u0026gt; Arguments，并在 VM Arguments 框中写下 -Xmx800m。在 IntelliJ IDEA 中推荐的改变 JVM 选项的方法是来自 Help | Edit Custom VM Options 菜单。详情请看这篇文章。\n构建项目 如果你想构建/打包你的项目，去你的项目目录下运行 \u0026ldquo;gradle clean shadowJar\u0026rdquo; 命令，你会发现一个 JAR 文件，其中包含了你的应用程序，以及你可能已经添加到应用程序中作为依赖的连接器和库：build/libs/\u0026lt;project-name\u0026gt;-\u0026lt;version\u0026gt;-all.jar。\n注意：如果你使用与 StreamingJob 不同的类作为应用程序的主类/入口点，我们建议你相应地更改 build.gradle 文件中的 mainClassName 设置。这样，Flink 就可以从 JAR 文件中运行应用程序，而无需额外指定主类。\nSBT 创建项目 您可以通过以下两种方法中的任何一种来构建一个新项目。\n 使用 sbt 模板  $ sbt new tillrohrmann/flink-project.g8  运行快速启动脚本  $ bash \u0026lt;(curl https://flink.apache.org/q/sbt-quickstart.sh) 这将在指定的项目目录下创建一个 Flink 项目。\n构建项目 为了建立你的项目，你只需要发出 sbt clean assembly 命令。这将在 target/scala_your-major-scala-version/ 目录下创建 fat-jar your-project-name-assembly-0.1-SNAPSHOT.jar。\n运行项目\n为了运行你的项目，你必须发出 sbt 运行命令。\n默认情况下，这将在 sbt 运行的同一个 JVM 中运行你的工作。为了在不同的 JVM 中运行你的工作，请在 build.sbt 中添加以下行。\nfork in run := true IntelliJ 我们推荐您使用 IntelliJ 进行 Flink 作业开发。为了开始，您必须将新创建的项目导入到 IntelliJ 中。您可以通过 File -\u0026gt; New -\u0026gt; Project from Existing Sources\u0026hellip;然后选择您的项目目录。IntelliJ 会自动检测 build.sbt 文件，并设置好一切。\n为了运行 Flink 作业，建议选择 mainRunner 模块作为运行/调试配置的 classpath。这将确保所有被设置为提供的依赖关系在执行时都是可用的。您可以通过 Run -\u0026gt; Edit Configurations\u0026hellip;配置 Run/Debug 配置，然后从 Use classpath of module dropbox 中选择 mainRunner。\nEclipse 为了将新创建的项目导入到 Eclipse 中，首先必须为其创建 Eclipse 项目文件。这些项目文件可以通过 sbteclipse 插件来创建。在 PROJECT_DIR/project/plugins.sbt 文件中添加以下一行。\naddSbtPlugin(\u0026quot;com.typeafe.sbteclipse\u0026quot; % \u0026quot;sbteclipse-plugin\u0026quot; % \u0026quot;4.0.0\u0026quot;) 在 sbt 中使用下面的命令来创建 Eclipse 项目文件\n\u0026gt; eclipse 现在你可以通过 File-\u0026gt;Import\u0026hellip;-\u0026gt;Existing Projects into Workspace 导入 Eclipse，然后选择项目目录。\n附录: 用依赖关系构建 Jar 的模板 要构建一个包含声明的连接器和库所需的所有依赖关系的应用程序 JAR，可以使用以下 shade 插件定义。\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;artifactSet\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;com.google.code.findbugs:jsr305\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;org.slf4j:*\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;log4j:*\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/artifactSet\u0026gt; \u0026lt;filters\u0026gt; \u0026lt;filter\u0026gt; \u0026lt;!-- Do not copy the signatures in the META-INF folder. Otherwise, this might cause SecurityExceptions when using the JAR. --\u0026gt; \u0026lt;artifact\u0026gt;*:*\u0026lt;/artifact\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.SF\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.DSA\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.RSA\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/filters\u0026gt; \u0026lt;transformers\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\u0026#34;\u0026gt; \u0026lt;mainClass\u0026gt;my.programs.main.clazz\u0026lt;/mainClass\u0026gt; \u0026lt;/transformer\u0026gt; \u0026lt;/transformers\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-22-project-configuration/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"项目配置"},{"categories":["programming"],"contents":"Event Time 在本节中，您将学习如何编写时间感知(time-aware)的 Flink 程序。请看一下及时流处理，了解及时流处理背后的概念。\n关于如何在 Flink 程序中使用时间的信息请参考 windowing 和 ProcessFunction。\n使用事件时间处理的先决条件是设置正确的时间特性(time characteristic)。该设置定义了数据流源的行为（例如，它们是否会分配时间戳），以及像 KeyedStream.timeWindow(Time.seconds(30)) 这样的窗口操作应该使用什么时间概念。\n你可以使用 StreamExecutionEnvironment.setStreamTimeCharacteristic() 设置时间特性:\nval env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val stream: DataStream[MyEvent] = env.addSource(new FlinkKafkaConsumer[MyEvent](topic, schema, props)) stream .keyBy( _.getUser ) .timeWindow(Time.hours(1)) .reduce( (a, b) =\u0026gt; a.add(b) ) .addSink(...) 需要注意的是，为了在事件时间(event time)中运行这个例子，程序需要使用直接为数据定义事件时间并自己发射水印的源，或者程序必须在源之后注入一个时间戳分配器(Timestamp Assigner)与水印生成器(Watermark Generator)。这些函数描述了如何访问事件时间戳，以及事件流表现出何种程度的无序性。\n下一步该怎么走？  生成水印。展示了如何编写时间戳分配器和水印生成器，这些都是事件时间(event-time)感知 Flink 应用所需要的。 内置的水印生成器。概述了内置的水印生成器。 调试窗口和事件时间：展示如何调试事件时间 Flink 应用程序中围绕水印和时间戳的问题。  ","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-event-time/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"Event Time"},{"categories":["programming"],"contents":"Flink DataStream API 编程指南 Flink 中的 DataStream 程序是对数据流实现转换的常规程序（如过滤、更新状态、定义窗口、聚合）。数据流最初是由各种源（如消息队列、套接字流、文件）创建的。结果通过接收器(sink)返回，例如可以将数据写入文件，或标准输出（例如命令行终端）。Flink 程序可以在各种环境下运行，独立运行，或者嵌入到其他程序中。执行可以发生在本地 JVM 中，也可以发生在许多机器的集群中。\n为了创建你自己的 Flink DataStream 程序，我们鼓励你从一个 Flink 程序的骨架开始，并逐步添加你自己的流转换。其余部分作为额外操作和高级功能的参考。\n什么是 DataStream？ DataStream API 的名字来自于特殊的 DataStream 类，它用于表示 Flink 程序中的数据集合。你可以把它们看作是不可改变的数据集合，可以包含重复的数据。这些数据既可以是有限的，也可以是无边界的，你用来处理它们的 API 是一样的。\nDataStream 在用法上与普通的 Java Collection 类似，但在一些关键方面却有很大不同。它们是不可改变的，这意味着一旦它们被创建，你就不能添加或删除元素。你也不能简单地检查里面的元素，而只能使用 DataStream API 操作对它们进行操作，这也被称为转换。\n你可以通过在 Flink 程序中添加一个源来创建一个初始的 DataStream。然后你可以从中派生新的流，并通过使用 API 方法，如 map、filter 等来组合它们。\nFlink 程序的骨架 Flink 程序看起来就像转换 DataStream 的普通程序。每个程序由相同的基本部分组成。\n 获取一个执行环境 加载/创建初始数据。 指定该数据的转换。 指定计算结果的位置。 触发程序执行  现在我们将对其中的每一个步骤进行概述，更多细节请参考相关章节。注意，Scala DataStream API 的所有核心类都可以在 org.apache.flink.stream.api.scala 中找到。\nStreamExecutionEnvironment 是所有 Flink 程序的基础。你可以使用 StreamExecutionEnvironment 上的这些静态方法获得一个。\ngetExecutionEnvironment() createLocalEnvironment() createRemoteEnvironment(host: String, port: Int, jarFiles: String*) 通常情况下，你只需要使用 getExecutionEnvironment()，因为这将根据上下文做正确的事情：如果你在 IDE 里面执行你的程序，或者作为一个普通的 Java 程序，它将创建一个本地环境，在你的本地机器上执行你的程序。如果你从你的程序中创建了一个 JAR 文件，并通过命令行调用它，Flink 集群管理器将执行你的主方法，并且 getExecutionEnvironment() 将返回一个在集群上执行你的程序的执行环境。\n对于指定数据源，执行环境有几种方法可以使用不同的方法从文件中读取数据：你可以只是逐行读取，作为 CSV 文件，或者使用任何其他提供的数据源。如果只是将文本文件作为一个行的序列来读取，你可以使用。\nval env = StreamExecutionEnvironment.getExecutionEnvironment() val text: DataStream[String] = env.readTextFile(\u0026#34;file:///path/to/file\u0026#34;) 这将为您提供一个 DataStream，然后您可以在其上应用转换来创建新的派生 DataStream。\n你可以通过调用 DataStream 上的方法和转换函数来应用转换。例如，一个 map 转换看起来像这样。\nval input: DataSet[String] = ... val mapped = input.map { x =\u0026gt; x.toInt } 这将通过将原始集合中的每一个字符串转换为一个 Integer 来创建一个新的 DataStream。\n一旦你有了一个包含最终结果的 DataStream，你就可以通过创建一个接收器(sink)将其写入外部系统。这些只是创建接收器的一些示例方法。\nwriteAsText(path: String) print() 一旦你指定了完整的程序，你需要通过调用 StreamExecutionEnvironment 上的 execution() 来触发程序的执行。根据 ExecutionEnvironment 的类型，将在你的本地机器上触发执行，或者将你的程序提交到集群上执行。\nexecute() 方法将等待作业完成，然后返回一个 JobExecutionResult，这个包含执行时间和累加器结果。\n如果你不想等待作业完成，你可以在 StreamExecutionEnvironment 上调用 executeAysnc() 来触发异步作业执行。它将返回一个 JobClient，你可以用它与刚刚提交的作业进行通信。例如，下面是如何通过使用 executeAsync() 来实现 execute() 的语义。\nfinal JobClient jobClient = env.executeAsync(); final JobExecutionResult jobExecutionResult = jobClient.getJobExecutionResult(userClassloader).get(); 最后这部分关于程序执行的内容对于理解 Flink 操作何时以及如何执行至关重要。所有的 Flink 程序都是懒惰地执行的。当程序的主方法被执行时，数据加载和转换不会直接发生。相反，每个操作都被创建并添加到一个数据流图(dataflow graph)中。当执行环境上的 execute() 调用明确触发执行时，这些操作才会被实际执行。程序是在本地执行还是在集群上执行，取决于执行环境的类型\n惰性求值可以让您构建复杂的程序，Flink 作为一个整体规划的单元来执行。\n示例程序 下面的程序是一个完整的，工作的流媒体窗口单词计数应用程序的例子，它可以在5秒的窗口中计算来自 Web Socket 的单词。你可以复制和粘贴代码在本地运行它。\nimport org.apache.flink.streaming.api.scala._ import org.apache.flink.streaming.api.windowing.time.Time object WindowWordCount { def main(args: Array[String]) { val env = StreamExecutionEnvironment.getExecutionEnvironment val text = env.socketTextStream(\u0026#34;localhost\u0026#34;, 9999) val counts = text.flatMap { _.toLowerCase.split(\u0026#34;\\\\W+\u0026#34;) filter { _.nonEmpty } } .map { (_, 1) } .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1) counts.print() env.execute(\u0026#34;Window Stream WordCount\u0026#34;) } } To run the example program, start the input stream with netcat first from a terminal:\nnc -lk 9999 只需输入一些单词，按回车键输入一个新单词。这些词将被输入到单词计数程序中。如果你想看到大于1的计数，请在5秒内反复输入同一个单词（如果你打字没那么快，请从5秒开始增加窗口大小☺）。\n数据源 源是你的程序读取其输入的地方。你可以通过使用 StreamExecutionEnvironment.addSource(sourceFunction) 将一个源附加到你的程序中。Flink 提供了许多预先实现的 SourceFunction，但是你可以通过实现非并行源的 SourceFunction，或者实现并行源的 ParallelSourceFunction 接口或扩展 RichParallelSourceFunction 来编写自己的自定义源。\n有几种预定义的流源(stream sources)可以从 StreamExecutionEnvironment 中访问。\n基于文件的。\n  readTextFile(path) - 逐行读取文本文件，即遵循 TextInputFormat 规范的文件，并将其作为字符串返回。\n  readFile(fileInputFormat, path) - 根据指定的文件输入格式读取（一次）文件。\n  readFile(fileInputFormat, path, watchType, interval, pathFilter) - 这是前面两个方法内部调用的方法。它根据给定的 fileInputFormat 读取路径中的文件。根据所提供的 watchType，这个源可能会周期性地监视(每隔 interval 毫秒)路径中的新数据(FileProcessingMode.PROCESS_CONTINUOUSLY)，或者处理一次当前路径中的数据并退出(FileProcessingMode.PROCESS_ONCE)。使用 pathFilter，用户可以进一步排除被处理的文件。\n  实现:\n在底层下，Flink 将文件读取过程分成两个子任务(sub-tasks)，即目录监控和数据读取。这些子任务中的每一个都是由一个单独的实体实现的。监控由一个单一的、非并行（并行度=1）的任务实现，而读取则由多个任务(task)并行运行。后者的并行度等于作业的并行度(job parallelism)。单个监控任务的作用是扫描目录（根据 watchType 的不同，定期或只扫描一次），找到要处理的文件，将其分割，并将这些分割的文件分配给下游的读取器。读取器是那些将读取实际数据的东西。每个分片只能由一个读取器读取，而一个读取器可以读取多个分片，一个接一个。\n重要提示:\n  如果 watchType 被设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，当一个文件被修改时，它的内容会被完全重新处理。这可能会打破\u0026quot;精确地一次\u0026quot;(exactly-once)的语义，因为在文件末尾追加数据会导致其所有内容被重新处理。\n  如果 watchType 被设置为 FileProcessingMode.PROCESS_ONCE，那么源就会对路径扫描一次并退出，而不会等待读取器完成对文件内容的读取。当然，读取器会继续读取，直到读取完所有文件内容。关闭源会导致在这之后不再有检查点。这可能会导致节点故障后的恢复速度变慢，因为作业(job)将从最后一个检查点开始恢复读取。\n  基于 Socket 的:\n socketTextStream - 从套接字读取。元素可以用定界符分开。  基于集合的:\n  fromCollection(Seq) - 从 Java Java.util.Collection 中创建数据流。集合中的所有元素必须是相同的类型。\n  fromCollection(Iterator) - 从迭代器中创建一个数据流。该类指定迭代器返回的元素的数据类型。\n  fromElements(elements: _*) - 从给定的对象序列中创建一个数据流。所有对象必须是相同的类型。\n  fromParallelCollection(SplittableIterator) - 从迭代器中并行创建数据流。该类指定了迭代器返回的元素的数据类型。\n  generateSequence(from, to) - 在给定的区间内并行生成数字序列。\n  自定义的:\n addSource - 附加一个新的源函数。例如，要从 Apache Kafka 读取数据，你可以使用 addSource(new FlinkKafkaConsumer010\u0026lt;\u0026gt;(...))。更多细节请参见连接器。  数据流转换 请参阅 operators 以了解可用的流转换的概述。\n数据接收器 数据接收器消耗 DataStream，并将其转发到文件、套接字、外部系统或打印。Flink 带有各种内置的输出格式，这些格式被封装在 DataStream 的操作后面。\n  writeAsText() / TextOutputFormat - 将元素逐行写入字符串。这些字符串是通过调用每个元素的 toString() 方法获得的。\n  writeAsCsv(...) / CsvOutputFormat - 将元组写成逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的 toString() 方法。\n  print() / printToErr() - 将每个元素的 toString() 值打印在标准输出/标准错误流上。可以选择提供一个前缀(msg)，这个前缀被添加到输出中。这可以帮助区分不同的 print 调用。如果并行度大于1，输出也将被预置为产生输出的任务(task)的标识符。\n  writeUsingOutputFormat() / FileOutputFormat - 用于自定义文件输出的方法和基类。支持自定义对象到字节的转换。\n  writeToSocket - 根据 SerializationSchema 将元素写入 socket。\n  addSink - 调用一个自定义的 sink 函数。Flink 捆绑了连接其他系统（如 Apache Kafka）的连接器，这些连接器被实现为 sink 函数。\n  请注意，DataStream 上的 write*() 方法主要是为了调试的目的。它们不参与 Flink 的检查点，这意味着这些函数通常具有最多一次(at-least-once)的语义。数据冲洗到目标系统取决于 OutputFormat 的实现。这意味着并非所有发送到 OutputFormat 的元素都会立即在目标系统中显示出来。另外，在失败的情况下，这些记录可能会丢失。\n为了可靠地、精确地一次性将流传送到文件系统中，请使用 flink-connector-filesystem。此外，通过 .addSink(...) 方法的自定义实现可以参与 Flink 的检查点，以实现精确的一次语义。\n迭代 迭代流程序实现了一个步骤函数，并将其嵌入到 IterativeStream 中。由于 DataStream 程序可能永远不会结束，所以没有最大的迭代次数。相反，你需要指定流的哪一部分被馈入到迭代中，哪一部分使用 split 转换或 filter 转发到下游。在这里，我们展示了一个迭代的例子，其中主体（重复计算的部分）是一个简单的 map 转换，而反馈回来的元素是通过使用 filter 转发到下游的元素来区分的。\nval iteratedStream = someDataStream.iterate( iteration =\u0026gt; { val iterationBody = iteration.map(/* this is executed many times */) (iterationBody.filter(/* one part of the stream */), iterationBody.filter(/* some other part of the stream */)) }) 例如，这里的程序是从一系列整数中连续减去1，直到它们达到零。\nval someIntegers: DataStream[Long] = env.generateSequence(0, 1000) val iteratedStream = someIntegers.iterate( iteration =\u0026gt; { val minusOne = iteration.map( v =\u0026gt; v - 1) val stillGreaterThanZero = minusOne.filter (_ \u0026gt; 0) val lessThanZero = minusOne.filter(_ \u0026lt;= 0) (stillGreaterThanZero, lessThanZero) } ) 执行参数 StreamExecutionEnvironment 包含了 ExecutionConfig，它允许为运行时设置作业特定(job specific)的配置值。\n请参考执行配置，了解大多数参数的解释。这些参数专门与 DataStream API 有关。\n setAutoWatermarkInterval(long milliseconds): 设置自动发射水印的时间间隔。你可以通过 long getAutoWatermarkInterval() 来获取当前值。  容错 状态和检查点介绍了如何启用和配置 Flink 的检查点机制。\n控制延迟 默认情况下，元素不会在网络上逐一传输（会造成不必要的网络流量），而是被缓冲。缓冲区（实际在机器之间传输）的大小可以在 Flink 配置文件中设置。虽然这种方法有利于优化吞吐量，但当传入的数据流速度不够快时，会造成延迟问题。为了控制吞吐量和延迟，你可以在执行环境上（或者在单个 operator 上）使用 env.setBufferTimeout(timeoutMillis) 来设置缓冲区填满的最大等待时间。过了这个时间，即使缓冲区没有满，也会自动发送。该超时的默认值为 100 ms。\n使用方法:\nval env: LocalStreamEnvironment = StreamExecutionEnvironment.createLocalEnvironment env.setBufferTimeout(timeoutMillis) env.generateSequence(1,10).map(myMap).setBufferTimeout(timeoutMillis) 为了最大限度地提高吞吐量，设置 setBufferTimeout(-1)，这将消除超时，缓冲区只有在满时才会被刷新。为了最大限度地减少延迟，将超时设置为接近0的值（例如5或10毫秒）。应该避免缓冲区超时为0，因为它会导致严重的性能下降。\n调试 在分布式集群中运行一个流程序之前，最好先确保实现的算法能够按照预期的方式运行。因此，实现数据分析程序通常是一个检查结果、调试和改进的渐进过程。\nFlink 提供了一些功能，通过支持 IDE 内的本地调试、测试数据的注入和结果数据的收集，大大简化了数据分析程序的开发过程。本节给出一些提示，如何简化 Flink 程序的开发。\n本地执行环境 LocalStreamEnvironment 在它创建的同一个 JVM 进程中启动 Flink 系统。如果你从 IDE 中启动 LocalEnvironment，你可以在代码中设置断点，轻松调试你的程序。\nLocalEnvironment 的创建和使用方法如下。\nval env = StreamExecutionEnvironment.createLocalEnvironment() val lines = env.addSource(/* some source */) // build your program  env.execute() 收集数据源 Flink 提供了特殊的数据源，这些数据源由 Java 集合支持，以方便测试。一旦程序被测试，源和接收器就可以很容易地被从外部系统读取/写入的源和接收器所替代。\n集合数据源的使用方法如下。\nval env = StreamExecutionEnvironment.createLocalEnvironment() // 从元素列表中创建一个 DataStream val myInts = env.fromElements(1, 2, 3, 4, 5) // 从任何集合中创建一个 DataStream val data: Seq[(String, Int)] = ... val myTuples = env.fromCollection(data) // 从迭代器中创建一个 DataStream val longIt: Iterator[Long] = ... val myLongs = env.fromCollection(longIt) 注：目前，集合数据源要求数据类型和迭代器实现 Serializable。此外，集合数据源不能并行执行( parallelism = 1)。\n迭代器数据接收器 Flink 还提供了一个收集 DataStream 结果的接收器(sink)，用于测试和调试目的。它的使用方法如下。\nimport org.apache.flink.streaming.experimental.DataStreamUtils import scala.collection.JavaConverters.asScalaIteratorConverter val myResult: DataStream[(String, Int)] = ... val myOutput: Iterator[(String, Int)] = DataStreamUtils.collect(myResult.javaStream).asScala 注意：flink-streaming-contrib 模块从 Flink 1.5.0 中移除。它的类被移到 flink-streaming-java 和 flink-streaming-scala 中。\n下一步怎么走？  运算符: 规范可用的流式运算符。 事件时间: 介绍 Flink 的时间概念。 状态和容错: 解释如何开发有状态的应用。 连接器: 描述可用的输入和输出连接器。  ","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-flink-datastream-api-programming-guide/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"Flink Datastream API 编程指南"},{"categories":["programming"],"contents":"生成水印 在本节中，您将了解 Flink 提供的 API，用于处理事件时间时间戳和水印。关于事件时间、处理时间和摄取时间的介绍，请参考事件时间的介绍。\n水印策略介绍 为了使用事件时间，Flink 需要知道事件的时间戳，这意味着流中的每个元素都需要分配其事件时间戳(event timestamp)。这通常是通过使用 TimestampAssigner 从元素中的某个字段访问/提取时间戳(timestamp)来完成的。\n时间戳分配与生成水印是同步进行的，水印告诉系统事件时间的进展。你可以通过指定一个 WatermarkGenerator 来配置。\nFlink API 期望一个 WatermarkStrategy，其中包含一个 TimestampAssigner 和 WatermarkGenerator。一些常见的策略作为 WatermarkStrategy 上的静态方法是开箱即用的，但用户也可以在需要时建立自己的策略。\n为了完整起见，这里是接口:\npublic interface WatermarkStrategy\u0026lt;T\u0026gt; extends TimestampAssignerSupplier\u0026lt;T\u0026gt;, WatermarkGeneratorSupplier\u0026lt;T\u0026gt;{ /** * Instantiates a {@link TimestampAssigner} for assigning timestamps according to this * strategy. */ @Override TimestampAssigner\u0026lt;T\u0026gt; createTimestampAssigner(TimestampAssignerSupplier.Context context); /** * Instantiates a WatermarkGenerator that generates watermarks according to this strategy. */ @Override WatermarkGenerator\u0026lt;T\u0026gt; createWatermarkGenerator(WatermarkGeneratorSupplier.Context context); } 如前所述，你通常不会自己实现这个接口，而是使用 WatermarkStrategy 上的静态帮助方法来实现常见的水印策略，或者将自定义的 TimestampAssigner 与 WatermarkGenerator 捆绑在一起。例如，要使用有界无序水印和 lambda 函数作为时间戳分配器，你可以使用这个方法。\nWatermarkStrategy .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(20)) .withTimestampAssigner(new SerializableTimestampAssigner[(Long, String)] { override def extractTimestamp(element: (Long, String), recordTimestamp: Long): Long = element._1 }) (在这里使用 Scala Lambdas 目前是行不通的，因为 Scala 很笨，很难支持这个。#fus)\n指定一个 TimestampAssigner 是可选的，在大多数情况下，你其实并不想指定一个。例如，当使用 Kafka 或 Kinesis 时，你会直接从 Kafka/Kinesis 记录中获取时间戳。\n我们将在后面的 Writing WatermarkGenerator中查看 WatermarkGenerator 接口。\n注意：时间戳和水印都被指定为自 1970-01-01T00:00:00Z 的 Java 纪元以来的毫秒。\n使用水印策略 在 Flink 应用中，有两个地方可以使用 WatermarkStrategy。1）直接在源上使用，2）在非源操作后使用。\n第一个选项是比较好的，因为它允许源在水印逻辑中利用关于碎片/分区/分割的知识。源通常可以更精细地跟踪水印，源产生的整体水印也会更准确。直接在源上指定 WatermarkStrategy 通常意味着你必须使用源的特定接口/请参阅 Watermark Strategies 和 Kafka Connector，以了解在 Kafka Connector 上如何工作，以及关于每个分区水印如何工作的更多细节。\n第二个选项（在任意操作后设置 WatermarkStrategy）只应在不能直接在源上设置策略时使用。\nval env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val stream: DataStream[MyEvent] = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter()) val withTimestampsAndWatermarks: DataStream[MyEvent] = stream .filter( _.severity == WARNING ) .assignTimestampsAndWatermarks(\u0026lt;watermark strategy\u0026gt;) withTimestampsAndWatermarks .keyBy( _.getGroup ) .timeWindow(Time.seconds(10)) .reduce( (a, b) =\u0026gt; a.add(b) ) .addSink(...) 以这种方式使用 WatermarkStrategy，可以获取一个流并生成一个带有时间戳元素和水印的新流。如果原始流已经有时间戳和/或水印了，时间戳分配器就会覆盖它们。\n处理闲置源 如果其中一个输入分割/分区/碎片在一段时间内没有携带事件，这意味着 WatermarkGenerator 也没有得到任何新的信息来作为水印的基础。我们称之为空闲输入或空闲源。这是一个问题，因为有可能发生你的一些分区仍然携带事件。在这种情况下，水印将被保留下来，因为它是作为所有不同的并行水印的最小值计算的。\n为了处理这个问题，你可以使用 WatermarkStrategy 来检测空闲，并将一个输入标记为空闲。WatermarkStrategy 为此提供了一个方便的助手。\nWatermarkStrategy .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(20)) .withIdleness(Duration.ofMinutes(1)) 编写水印生成器 时间戳分配器(TimestampAssigner)是一个从事件中提取字段的简单函数，因此我们不需要详细研究它们。而 WatermarkGenerator 的编写就比较复杂了，我们将在接下来的两节中看如何做。这就是 WatermarkGenerator 的接口。\n/** * The {@code WatermarkGenerator} generates watermarks either based on events or * periodically (in a fixed interval). * * \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Note:\u0026lt;/b\u0026gt; This WatermarkGenerator subsumes the previous distinction between the * {@code AssignerWithPunctuatedWatermarks} and the {@code AssignerWithPeriodicWatermarks}. */ @Public public interface WatermarkGenerator\u0026lt;T\u0026gt; { /** * Called for every event, allows the watermark generator to examine and remember the * event timestamps, or to emit a watermark based on the event itself. */ void onEvent(T event, long eventTimestamp, WatermarkOutput output); /** * Called periodically, and might emit a new watermark, or not. * * \u0026lt;p\u0026gt;The interval in which this method is called and Watermarks are generated * depends on {@link ExecutionConfig#getAutoWatermarkInterval()}. */ void onPeriodicEmit(WatermarkOutput output); } 有两种不同风格的水印生成器：周期性和打点式。\n周期性生成器通常通过 onEvent() 观察到传入的事件，然后当框架调用 onPeriodicEmit() 时，发射水印。\n标点式生成器会观察 onEvent() 中的事件，并等待流中携带水印信息的特殊标记事件或标点。当它看到这些事件之一时，就会立即发出一个水印。通常，标点生成器不会从 onPeriodicEmit() 发出水印。\n接下来我们将看看如何实现每种样式的生成器。\n编写周期性水印生成器 周期性生成器观察流事件并周期性地生成水印（可能取决于流元素，或者纯粹基于处理时间）。\n生成水印的间隔（每n毫秒）通过 ExecutionConfig.setAutoWatermarkInterval(...) 来定义。每次都会调用生成器的 onPeriodicEmit() 方法，如果返回的水印是非空的，并且大于前一个水印，就会发出一个新的水印。\n这里我们展示了两个使用周期性水印生成器的简单例子。请注意，Flink 提供了 BoundedOutfOrdernessWatermarks，这是一个 WatermarkGenerator，它的工作原理与下面所示的 BoundedOutfOrdernessGenerator 类似。你可以在这里阅读关于如何使用它。\n/** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */ class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] { val maxOutOfOrderness = 3500L // 3.5 seconds  var currentMaxTimestamp: Long = _ override def onEvent(element: MyEvent, eventTimestamp: Long): Unit = { currentMaxTimestamp = max(eventTimestamp, currentMaxTimestamp) } override def onPeriodicEmit(): Unit = { // emit the watermark as current highest timestamp minus the out-of-orderness bound  output.emitWatermark(new Watermark(currentMaxTimestamp - maxOutOfOrderness - 1)); } } /** * This generator generates watermarks that are lagging behind processing time by a fixed amount. * It assumes that elements arrive in Flink after a bounded delay. */ class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] { val maxTimeLag = 5000L // 5 seconds  override def onEvent(element: MyEvent, eventTimestamp: Long): Unit = { // don\u0026#39;t need to do anything because we work on processing time  } override def onPeriodicEmit(): Unit = { output.emitWatermark(new Watermark(System.currentTimeMillis() - maxTimeLag)); } } 编写一个标点水印生成器 标点水印生成器将观察事件流，每当它看到一个携带水印信息的特殊元素时，就会发出一个水印。\n这就是如何实现一个标点水印生成器，每当一个事件表明它携带某个标记时，它就会发射一个水印。\nclass PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] { override def onEvent(element: MyEvent, eventTimestamp: Long): Unit = { if (event.hasWatermarkMarker()) { output.emitWatermark(new Watermark(event.getWatermarkTimestamp())) } } override def onPeriodicEmit(): Unit = { // don\u0026#39;t need to do anything because we emit in reaction to events above  } } 注：可以对每个事件生成一个水印。然而，由于每个水印都会引起下游的一些计算，因此过多的水印会降低性能。\n水印策略和 Kafka 连接器 当使用 Apache Kafka 作为数据源时，每个 Kafka 分区可能有一个简单的事件时间模式（升序时间戳或有界失序）。然而，当消耗来自 Kafka 的流时，多个分区经常会被并行消耗，交织来自分区的事件，并破坏每个分区的模式（这是 Kafka 的消费者客户端的固有工作方式）。\n在这种情况下，你可以使用 Flink 的 Kafka-partition-aware 水印生成功能。使用该功能，在 Kafka 消费者内部，按 Kafka 分区生成水印，每个分区水印的合并方式与流洗牌的水印合并方式相同。\n例如，如果每个 Kafka 分区的事件时间戳是严格的升序，那么用升序时间戳水印生成器生成每个分区的水印，会得到完美的整体水印。请注意，我们在示例中并没有提供 TimestampAssigner，而是使用 Kafka 记录本身的时间戳。\n下面的插图展示了如何使用 per-Kafka-partition 水印生成器，以及在这种情况下水印如何通过流式数据流传播。\nval kafkaSource = new FlinkKafkaConsumer[MyType](\u0026#34;myTopic\u0026#34;, schema, props) kafkaSource.assignTimestampsAndWatermarks( WatermarkStrategy .forBoundedOutOfOrderness(Duration.ofSeconds(20))) val stream: DataStream[MyType] = env.addSource(kafkaSource) 运算符如何处理水印 作为一般规则，运算符(operator)在向下游转发一个给定的水印之前，需要对其进行完全处理。例如，WindowOperator 将首先评估所有应该被发射的窗口，只有在产生所有由水印触发的输出之后，水印本身才会被发送到下游。换句话说，所有因发生水印而产生的元素将在水印之前被发射。\n同样的规则也适用于 TwoInputStreamOperator。然而，在这种情况下，运算符的当前水印被定义为其两个输入的最小值。\n这种行为的细节由 OneInputStreamOperator#processWatermark、TwoInputStreamOperator#processWatermark1 和 TwoInputStreamOperator#processWatermark2 方法的实现来定义。\n废弃的 AssignerWithPeriodicWatermarks 和 AssignerWithPunctuatedWatermarks 方法 在引入当前的 WatermarkStrategy、TimestampAssigner 和 WatermarkGenerator 抽象之前，Flink 使用了 AssignerWithPeriodicWatermarks 和 AssignerWithPeriodicWatermarks。你仍然会在 API 中看到它们，但建议使用新的接口，因为它们提供了更清晰的分离关注点，也统一了水印生成的周期和标点样式。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-generating-watermarks/","tags":["Flink","Flink 官方文档","DataStream API","Event Time"],"title":"Generating Watermarks"},{"categories":["Julia"],"contents":"Julia REPL Julia 在 julia 可执行文件中内置了一个功能齐全的交互式命令行 REPL（read-eval-print loop）。除了允许快速、简单地评估 Julia 语句外，它还具有可搜索的历史记录、tab-补全、许多有用的键绑定以及专门的帮助和 shell 模式。REPL 可以通过简单地调用 julia 而不使用参数或双击可执行文件来启动。\n$ julia _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type \u0026#34;?\u0026#34; for help, \u0026#34;]?\u0026#34; for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.5.0 (2020-08-01) _/ |\\__\u0026#39;_|_|_|\\__\u0026#39;_| | Official https://julialang.org/ release |__/ | julia\u0026gt; 要退出交互式会话，请键入 ^D - 控制键与 d 键一起在空行上键入，或键入 exit() 后跟回车或回车键。REPL 会用一个横幅和 julia\u0026gt; 提示来欢迎您。\n不同的提示模式 朱利安模式 REPL 有四种主要的操作模式。第一种也是最常见的是 Julian 提示。这是默认的操作模式；每个新行都以 julia\u0026gt; 开始。在这里您可以输入 Julia 表达式。在输入完整的表达式后点击回车或回车将评估该条目并显示最后一个表达式的结果。\njulia\u0026gt; string(1 + 2) \u0026#34;3\u0026#34; 在交互式工作中，有许多独特的有用功能。除了显示结果之外，REPL 还将结果绑定到变量 ans 上。行上的分号可以作为一个标志来抑制显示结果。\njulia\u0026gt; string(3 * 4); julia\u0026gt; ans \u0026#34;12\u0026#34; 在 Julia 模式下，REPL 支持称为提示粘贴(prompt pasting)的东西。当把以 julia\u0026gt; 开头的文本粘贴到 REPL 中时，这个功能会被激活。在这种情况下，只有以 julia\u0026gt; 开头的表达式会被解析，其他表达式会被删除。这使得您可以粘贴从 REPL 会话中复制的代码块，而无需清除提示和输出。这个功能默认是启用的，但可以通过 REPL.enable_promptpaste(::Bool) 来禁用或启用。如果启用了，您可以直接将本段上面的代码块粘贴到 REPL 中试试。这个功能在标准的 Windows 命令提示符上不起作用，因为它在检测粘贴发生时的局限性。\nusing REPL REPL.enable_promptpaste(false) # 禁用 prompt pasting REPL.enable_promptpaste(true) # 启用 prompt pasting 对象在 REPL 中使用带有特定 IOContext 的 show 函数进行打印。特别是，:limit 属性被设置为 true。其他属性可以在某些 show 方法中接收一个默认值，如果它还没有被设置，比如 :compact。作为实验性功能，可以通过 Base.active_repl.options.iocontext 字典来指定 REPL 使用的属性（将值关联到属性）。例如:\njulia\u0026gt; rand(2, 2) 2×2 Array{Float64,2}: 0.8833 0.329197 0.719708 0.59114 julia\u0026gt; show(IOContext(stdout, :compact =\u0026gt; false), \u0026#34;text/plain\u0026#34;, rand(2, 2)) 0.43540323669187075 0.15759787870609387 0.2540832269192739 0.4597637838786053 julia\u0026gt; Base.active_repl.options.iocontext[:compact] = false; julia\u0026gt; rand(2, 2) 2×2 Array{Float64,2}: 0.2083967319174056 0.13330606013126012 0.6244375177790158 0.9777957560761545 为了在启动时自动定义这个字典的值，可以使用 ~/.julia/config/startup.jl 文件中的 atreplinit 函数，例如:\natreplinit() do repl repl.options.iocontext[:compact] = false end 帮助模式 当光标在行首时，可以通过键入 ? 来将提示变为帮助模式。Julia 将尝试打印帮助模式下输入的任何内容的帮助或文档。\njulia\u0026gt; ? # upon typing ?, the prompt changes (in place) to: help?\u0026gt; help?\u0026gt; string search: string String Cstring Cwstring RevString randstring bytestring SubString string(xs...) Create a string from any values using the print function. 也可以查询宏、类型和变量:\nhelp?\u0026gt; @time @time A macro to execute an expression, printing the time it took to execute, the number of allocations, and the total number of bytes its execution caused to be allocated, before returning the value of the expression. See also @timev, @timed, @elapsed, and @allocated. help?\u0026gt; Int32 search: Int32 UInt32 Int32 \u0026lt;: Signed 32-bit signed integer type. 按行首的退格键可以退出帮助模式。\nShell 模式 就像帮助模式对于快速访问文档很有用一样，另一个常见的任务是使用系统 shell 来执行系统命令。就像 ? 进入帮助模式时一样, 在行首按下分号(;)会进入 shell 模式。而且可以在行首按退格键退出。\njulia\u0026gt; ; # upon typing ;, the prompt changes (in place) to: shell\u0026gt; shell\u0026gt; echo hello hello 注意:\n对于 Windows 用户来说，Julia 的 shell 模式不暴露 windows shell 命令。因此，这将会失败:\njulia\u0026gt; ; # upon typing ;, the prompt changes (in place) to: shell\u0026gt; shell\u0026gt; dir ERROR: IOError: could not spawn `dir`: no such file or directory (ENOENT) Stacktrace! ....... 不过，你可以像这样访问 PowerShell:\njulia\u0026gt; ; # upon typing ;, the prompt changes (in place) to: shell\u0026gt; shell\u0026gt; powershell Windows PowerShell Copyright (C) Microsoft Corporation. All rights reserved. PS C:\\Users\\elm\u0026gt; \u0026hellip;而且对 cmd.exe 的访问是这样的（见 dir 命令）:\njulia\u0026gt; ; # upon typing ;, the prompt changes (in place) to: shell\u0026gt; shell\u0026gt; cmd Microsoft Windows [version 10.0.17763.973] (c) 2018 Microsoft Corporation. All rights reserved. C:\\Users\\elm\u0026gt;dir Volume in drive C has no label Volume Serial Number is 1643-0CD7 Directory of C:\\Users\\elm 29/01/2020 22:15 \u0026lt;DIR\u0026gt; . 29/01/2020 22:15 \u0026lt;DIR\u0026gt; .. 02/02/2020 08:06 \u0026lt;DIR\u0026gt; .atom 搜索模式 在上述所有模式中，执行的行数都会被保存到历史文件中，可以进行搜索。要在以前的历史记录中进行增量搜索，输入 ^R\u0026ndash;控制键和 r 键。提示符将变为(reverse-i-search):，当你输入搜索查询时，搜索查询将出现在引号中。当你输入更多的内容时，与查询相匹配的最新结果会动态地更新到冒号的右边。如果要使用相同的查询找到一个较早的结果，只需再次输入 ^R。\n就像 ^R 是反向搜索一样，^S 是正向搜索，并提示(i-search):。两者可以相互结合使用，分别在上一个或下一个匹配结果中移动。\n键绑定 Julia REPL 大量使用了键绑定。上面已经介绍了几个控制键绑定（^D 用于退出，^R 和 ^S 用于搜索），但还有更多。除了控制键，还有元键绑定。这些因平台不同而变化较大，但大多数终端默认使用 alt- 或 option- 按住键发送元键（也可以配置成这样），或者按 Esc 键，然后按键。\n   Keybinding Description     Program control    ^D 退出(当缓存为空时)   ^C 中断或取消   ^L 清理控制台屏幕   Return/Enter, ^J 新行，如果完成了就执行   meta-Return/Enter 插入新行而不执行   ? 或 ; 进入帮助或shell模式(当在行的开头时)   ^R, ^S 递增式历史检索，如上所述    自定义键绑定 Julia 的 REPL 键绑定可以通过向 REPL.setup_interface 传递一个字典来完全定制用户的偏好。这个字典的键可以是字符或字符串。键 \u0026lsquo;*\u0026rsquo; 指的是默认操作。控制加字符x的绑定用\u0026quot;^x\u0026quot;表示。Meta 加x可以写成 \u0026ldquo;\\M-x\u0026rdquo; 或 \u0026ldquo;\\ex\u0026rdquo;，Control 加 x 可以写成 \u0026ldquo;\\C-x\u0026rdquo; 或 \u0026ldquo;^x\u0026rdquo;。自定义 keymap 的值必须是 nothing(表示输入应该被忽略)或接受签名的函数(PromptState, AbstractREPL, Char)。REPL.setup_interface 函数必须在 REPL 初始化之前被调用，通过在 atreplinit 注册操作。例如，要绑定上下方向键来移动历史记录而不需要前缀搜索，可以在 ~/.julia/config/startup.jl 中放入以下代码。\nimport REPL import REPL.LineEdit const mykeys = Dict{Any,Any}( # Up Arrow \u0026#34;\\e[A\u0026#34; =\u0026gt; (s,o...)-\u0026gt;(LineEdit.edit_move_up(s) || LineEdit.history_prev(s, LineEdit.mode(s).hist)), # Down Arrow \u0026#34;\\e[B\u0026#34; =\u0026gt; (s,o...)-\u0026gt;(LineEdit.edit_move_down(s) || LineEdit.history_next(s, LineEdit.mode(s).hist)) ) function customize_keys(repl) repl.interface = REPL.setup_interface(repl; extra_repl_keymap = mykeys) end atreplinit(customize_keys) 用户应该参考 LineEdit.jl 来发现键输入的可用操作。\nTab 补全 在 REPL 的 Julian 和帮助模式下，可以输入函数或类型的前几个字符，然后按tab键，得到一个所有匹配的列表:\njulia\u0026gt; stri[TAB] stride strides string strip julia\u0026gt; Stri[TAB] StridedArray StridedMatrix StridedVecOrMat StridedVector String tab 键也可以用来用它们的 Unicode 等价物替代 LaTeX 数学符号，并获得 LaTeX 匹配列表。\njulia\u0026gt; \\pi[TAB] julia\u0026gt; π π = 3.1415926535897... julia\u0026gt; e\\_1[TAB] = [1,0] julia\u0026gt; e₁ = [1,0] 2-element Array{Int64,1}: 1 0 julia\u0026gt; e\\^1[TAB] = [1 0] julia\u0026gt; e¹ = [1 0] 1×2 Array{Int64,2}: 1 0 julia\u0026gt; \\sqrt[TAB]2 # √ is equivalent to the sqrt function julia\u0026gt; √2 1.4142135623730951 julia\u0026gt; \\hbar[TAB](h) = h / 2\\pi[TAB] julia\u0026gt; ħ(h) = h / 2π ħ (generic function with 1 method) julia\u0026gt; \\h[TAB] \\hat \\hermitconjmatrix \\hkswarow \\hrectangle \\hatapprox \\hexagon \\hookleftarrow \\hrectangleblack \\hbar \\hexagonblack \\hookrightarrow \\hslash \\heartsuit \\hksearow \\house \\hspace julia\u0026gt; α=\u0026#34;\\alpha[TAB]\u0026#34; # LaTeX completion also works in strings julia\u0026gt; α=\u0026#34;α\u0026#34; 完整的tab-补全列表可以在手册的 Unicode 输入部分找到。\n路径补全适用于字符串和 julia 的 shell 模式:\njulia\u0026gt; path=\u0026#34;/[TAB]\u0026#34; .dockerenv .juliabox/ boot/ etc/ lib/ media/ opt/ root/ sbin/ sys/ usr/ .dockerinit bin/ dev/ home/ lib64/ mnt/ proc/ run/ srv/ tmp/ var/ shell\u0026gt; /[TAB] .dockerenv .juliabox/ boot/ etc/ lib/ media/ opt/ root/ sbin/ sys/ usr/ .dockerinit bin/ dev/ home/ lib64/ mnt/ proc/ run/ srv/ tmp/ var/ Tab 补全可以帮助调查匹配输入参数的可用方法。\njulia\u0026gt; max([TAB] # All methods are displayed, not shown here due to size of the list julia\u0026gt; max([1, 2], [TAB] # All methods where `Vector{Int}` matches as first argument max(x, y) in Base at operators.jl:215 max(a, b, c, xs...) in Base at operators.jl:281 julia\u0026gt; max([1, 2], max(1, 2), [TAB] # All methods matching the arguments. max(x, y) in Base at operators.jl:215 max(a, b, c, xs...) in Base at operators.jl:281 关键词也显示在 ; 后面的建议方法中，见下面一行，其中 limit 和 keepempty 是关键词参数:\njulia\u0026gt; split(\u0026#34;1 1 1\u0026#34;, [TAB] split(str::AbstractString; limit, keepempty) in Base at strings/util.jl:302 split(str::T, splitter; limit, keepempty) where T\u0026lt;:AbstractString in Base at strings/util.jl:277 方法的补全使用类型推断，因此即使参数是函数输出的，也能看到参数是否匹配。函数需要类型稳定，完成才能够删除不匹配的方法。\nTab 补全也可以帮助补全字段:\njulia\u0026gt; import UUIDs julia\u0026gt; UUIDs.uuid[TAB] uuid1 uuid4 uuid_version 也可以补全函数输出的字段:\njulia\u0026gt; split(\u0026quot;\u0026quot;,\u0026quot;\u0026quot;)[1].[TAB] lastindex offset string 函数输出的字段完成采用类型推断，只有在函数类型稳定的情况下，它才能建议字段。\n字典键也可以用 tab 补全:\njulia\u0026gt; foo = Dict(\u0026#34;qwer1\u0026#34;=\u0026gt;1, \u0026#34;qwer2\u0026#34;=\u0026gt;2, \u0026#34;asdf\u0026#34;=\u0026gt;3) Dict{String,Int64} with 3 entries: \u0026#34;qwer2\u0026#34; =\u0026gt; 2 \u0026#34;asdf\u0026#34; =\u0026gt; 3 \u0026#34;qwer1\u0026#34; =\u0026gt; 1 julia\u0026gt; foo[\u0026#34;q[TAB] \u0026#34;qwer1\u0026#34; \u0026#34;qwer2\u0026#34; julia\u0026gt; foo[\u0026#34;qwer 自定义颜色 Julia 和 REPL 所使用的颜色也是可以自定义的。要改变 Julia 提示符的颜色，您可以在您的 ~/.julia/config/startup.jl 文件中添加以下内容，该文件应放在您的主目录中。\nfunction customize_colors(repl) repl.prompt_color = Base.text_colors[:cyan] end atreplinit(customize_colors) 可用的颜色键可以通过在 REPL 的帮助模式下输入 Base.text_colors 来查看。此外，对于支持 256 色的终端来说，整数 0 到 255 可以用作颜色键。\n也可以通过在上面的 customize_colors 函数中设置 repl 的相应字段（分别为 help_color、shell_color、input_color 和 answer_color）来改变帮助和 shell 提示符以及输入和回答文字的颜色。对于后两者，要确保 envcolors 字段也设置为 false。\n也可以通过使用 Base.text_colors[:bold] 作为颜色来应用粗体格式。例如，要用粗体字打印答案，可以使用下面的 ~/.julia/config/startup.jl:\nfunction customize_colors(repl) repl.envcolors = false repl.answer_color = Base.text_colors[:bold] end atreplinit(customize_colors) 你也可以通过设置适当的环境变量来定制用于渲染警告和信息消息的颜色。例如，要分别用洋红色、黄色和青色来渲染错误、警告和信息消息，你可以在 ~/.julia/config/startup.jl 文件中添加以下内容:\nENV[\u0026#34;JULIA_ERROR_COLOR\u0026#34;] = :magenta ENV[\u0026#34;JULIA_WARN_COLOR\u0026#34;] = :yellow ENV[\u0026#34;JULIA_INFO_COLOR\u0026#34;] = :cyan TerminalMenus TerminalMenus 是 Julia REPL 的一个子模块，可以在终端中实现小型、低配的交互式菜单。\n例子 import REPL using REPL.TerminalMenus options = [\u0026#34;apple\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;grape\u0026#34;, \u0026#34;strawberry\u0026#34;, \u0026#34;blueberry\u0026#34;, \u0026#34;peach\u0026#34;, \u0026#34;lemon\u0026#34;, \u0026#34;lime\u0026#34;] RadioMenu RadioMenu 允许用户从列表中选择一个选项。request 函数显示交互式菜单并返回所选选项的索引。如果用户按 \u0026lsquo;q\u0026rsquo; 或 ctrl-c，request 将返回 -1。\n# `pagesize` is the number of items to be displayed at a time. # The UI will scroll if the number of options is greater # than the `pagesize` menu = RadioMenu(options, pagesize=4) # `request` displays the menu and returns the index after the # user has selected a choice choice = request(\u0026#34;Choose your favorite fruit:\u0026#34;, menu) if choice != -1 println(\u0026#34;Your favorite fruit is \u0026#34;, options[choice], \u0026#34;!\u0026#34;) else println(\u0026#34;Menu canceled.\u0026#34;) end 输出:\nChoose your favorite fruit: ^ grape strawberry \u0026gt; blueberry v peach Your favorite fruit is blueberry! MultiSelectMenu 多重选择菜单（MultiSelectMenu）允许用户从一个列表中选择许多选择。\n# here we use the default `pagesize` 10 menu = MultiSelectMenu(options) # `request` returns a `Set` of selected indices # if the menu us canceled (ctrl-c or q), return an empty set choices = request(\u0026#34;Select the fruits you like:\u0026#34;, menu) if length(choices) \u0026gt; 0 println(\u0026#34;You like the following fruits:\u0026#34;) for i in choices println(\u0026#34; - \u0026#34;, options[i]) end else println(\u0026#34;Menu canceled.\u0026#34;) end 输出:\nSelect the fruits you like: [press: d=done, a=all, n=none] [ ] apple \u0026gt; [X] orange [X] grape [ ] strawberry [ ] blueberry [X] peach [ ] lemon [ ] lime You like the following fruits: - orange - grape - peach Customization / Configuration 所有的界面定制都是通过关键字 TerminalMenus.config() 函数完成的。\n参数  charset::Symbol=:na: 要使用的ui字符(:ascii 或 :unicode); 被其他参数覆盖。 cursor::Char='\u0026gt;'|'→': 光标使用的字符。 up_arrow::Char='^'|'↑': 用于向上箭头的字符。 down_arrow::Char='v'|'↓': 用于向下箭头的字符。 checked::String=\u0026quot;[X]\u0026quot;|\u0026quot;✓\u0026quot;：用于检查的字符串。 unchecked::String=\u0026quot;[]\u0026quot;|\u0026quot;⬚\u0026quot;)：用于未选中的字符串。 scroll::Symbol=:na: 如果 :wrap，则将光标环绕在顶部和底部，如果 :nowrap 则不环绕光标。 supress_output::Bool=false。用于测试。如果为真，菜单不会被打印到控制台。 ctrl_c_interrupt::Bool=true: 如果为假，在 ^C 时返回空，如果为真，在 ^C 时抛出 InterruptException()。  例子 julia\u0026gt; menu = MultiSelectMenu(options, pagesize=5); julia\u0026gt; request(menu) # ASCII is used by default [press: d=done, a=all, n=none] [ ] apple [X] orange [ ] grape \u0026gt; [X] strawberry v [ ] blueberry Set([4, 2]) julia\u0026gt; TerminalMenus.config(charset=:unicode) julia\u0026gt; request(menu) [press: d=done, a=all, n=none] ⬚ apple ✓ orange ⬚ grape → ✓ strawberry ↓ ⬚ blueberry Set([4, 2]) julia\u0026gt; TerminalMenus.config(checked=\u0026#34;YEP!\u0026#34;, unchecked=\u0026#34;NOPE\u0026#34;, cursor=\u0026#39;⧐\u0026#39;) julia\u0026gt; request(menu) [press: d=done, a=all, n=none] NOPE apple YEP! orange NOPE grape ⧐ YEP! strawberry ↓ NOPE blueberry Set([4, 2]) 参考 Base.atreplinit\n atreplinit(f)\n 注册一个单参数函数，在交互式会话中，在 REPL 接口初始化之前被调用；这对自定义接口很有用。f 的参数是 REPL 对象。这个函数应该在 ~/.julia/config/startup.jl 初始化文件中调用。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-repl-in-julia/","tags":["Julia","Julia 官方文档"],"title":"Julia 中的 REPL"},{"categories":["Flink"],"contents":"使用状态 在本节中，您将了解 Flink 为编写有状态程序提供的 API。请看 Stateful Stream Processing 来了解有状态流处理背后的概念。\nKeyed DataStream 如果要使用 keyed state，首先需要在 DataStream 上指定一个键，这个键应该用来分隔(partition)状态（也包括流中的记录本身）。你可以在 DataStream 上使用 keyBy(KeySelector) 指定一个键。这将产生一个 KeyedDataStream，然后允许使用 keyed state 的操作。\n键选择函数将一条记录作为输入，并返回该记录的键。键可以是任何类型的，并且必须从确定性计算中导出。\nFlink 的数据模型不是基于键值对的。因此，您不需要将数据集类型物理地打包成键和值。键是\u0026quot;虚拟\u0026quot;的：它们被定义为实际数据上的函数，以指导分组操作符。\n下面的例子显示了一个键选择函数，它只是返回对象的字段。\n// 普通的 case 类 case class WC(word: String, count: Int) val words: DataStream[WC] = // [...] val keyed = words.keyBy( _.word ) 元组键和表达式键 Flink 还有两种定义键的方法：元组键和表达式键。有了它，你可以使用元组字段索引或表达式来指定键，用于选择对象的字段。我们今天不推荐使用这些，但你可以参考 DataStream 的 Javadoc 来了解它们。严格来说，使用 KeySelector 函数更胜一筹：使用 Java lambdas，它们很容易使用，而且它们在运行时的开销可能更少。\n使用 Keyed State keyed State 接口提供了对不同类型的状态的访问，这些状态的作用域都是当前输入元素的键。这意味着，这种类型的状态只能在 KeyedStream 上使用，它可以通过 stream.keyBy(...) 来创建。\n现在，我们将首先看看不同类型的状态有哪些，然后我们会看看如何在程序中使用它们。可用的状态原语有:\n  ValueState\u0026lt;T\u0026gt;：它保留了一个可更新和检索的值（如上所述，作用域为输入元素的键，因此操作符所看到的每个键都可能有一个值）。这个值可以使用 update(T) 来设置，也可以使用 T value() 来检索。\n  ListState\u0026lt;T\u0026gt;：这保存了一个元素列表。你可以在所有当前存储的元素上追加元素和检索一个 Iterable。使用 add(T) 或 addAll(List\u0026lt;T\u0026gt;) 添加元素，可以使用 Iterable\u0026lt;T\u0026gt; get() 检索 Iterable。你也可以用 update(List\u0026lt;T\u0026gt;) 覆盖现有的列表。\n  ReducingState\u0026lt;T\u0026gt;: 这保留了一个单一的值，代表所有添加到状态的值的集合。该接口类似于 ListState，但使用 add(T) 添加的元素会使用指定的 ReduceFunction 被化简成一个总计。\n  AggregatingState\u0026lt;IN，OUT\u0026gt;：这保留了一个单一的值，代表所有添加到状态的值的聚合。与 ReducingState 相反，aggregate 类型可能与添加到状态中的元素类型不同。接口与 ListState 相同，但使用 add(IN) 添加的元素会使用指定的 AggregateFunction 进行聚合。\n  MapState\u0026lt;UK, UV\u0026gt;: 它保存了一个映射列表。你可以将键值对放入状态中，并在所有当前存储的映射上检索一个 Iterable。使用 put(UK, UV) 或 putAll(Map\u0026lt;UK, UV\u0026gt;) 可以添加映射。与用户键相关联的值可以使用 get(UK) 来检索。可以分别使用 entries()、keys() 和 values() 检索映射、键和值的可迭代视图。你也可以使用 isEmpty() 来检查这个映射是否包含任何键值映射。\n  所有类型的状态也都有一个方法 clear()，可以清除当前活动键的状态，也就是输入元素的键。\n需要注意的是，这些状态对象只用于带状态的接口。状态不一定存储在里面，而可能驻留在磁盘或其他地方。第二件要记住的事情是，你从状态中得到的值取决于输入元素的键。因此，如果所涉及的键不同，你在用户函数的一次调用中得到的值可能与另一次调用中的值不同。\n为了得到一个状态句柄，你必须创建一个 StateDescriptor。这里面包含了状态的名称(我们稍后会看到，你可以创建多个状态，而且它们必须有独特的名称，这样你才能引用它们)，状态所拥有的值的类型，可能还有一个用户指定的函数，比如 ReduceFunction。根据你要检索的状态类型，你可以创建一个 ValueStateDescriptor、一个 ListStateDescriptor、一个 ReducingStateDescriptor 或一个 MapStateDescriptor。\n状态是使用 RuntimeContext 访问的，所以只有在富函数(rich functions)中才有可能。请看这里了解相关信息，但我们也会很快看到一个例子。RichFunction 中可用的 RuntimeContext 有这些方法来访问状态。\n ValueStategetState(ValueStateDescriptor) ReducingStategetReducingState(ReducingStateDescriptor) ListStategetListState(ListStateDescriptor) AggregatingState\u0026lt;IN, OUT\u0026gt; getAggregatingState(AggregatingStateDescriptor\u0026lt;IN, ACC, OUT\u0026gt;) MapState\u0026lt;UK, UV\u0026gt; getMapState(MapStateDescriptor\u0026lt;UK, UV\u0026gt;)  这是一个 FlatMapFunction 的例子，它展示了所有的部分是如何结合在一起的。\nclass CountWindowAverage extends RichFlatMapFunction[(Long, Long), (Long, Long)] { private var sum: ValueState[(Long, Long)] = _ override def flatMap(input: (Long, Long), out: Collector[(Long, Long)]): Unit = { // 访问状态值  val tmpCurrentSum = sum.value // 如果之前没有使用过，则为 null。  val currentSum = if (tmpCurrentSum != null) { tmpCurrentSum } else { (0L, 0L) } // 更新次数  val newSum = (currentSum._1 + 1, currentSum._2 + input._2) // 更新状态  sum.update(newSum) // 如果计数达到2，则发出平均数，并清除状态。  if (newSum._1 \u0026gt;= 2) { out.collect((input._1, newSum._2 / newSum._1)) sum.clear() } } override def open(parameters: Configuration): Unit = { sum = getRuntimeContext.getState( new ValueStateDescriptor[(Long, Long)](\u0026#34;average\u0026#34;, createTypeInformation[(Long, Long)]) ) } } object ExampleCountWindowAverage extends App { val env = StreamExecutionEnvironment.getExecutionEnvironment env.fromCollection(List( (1L, 3L), (1L, 5L), (1L, 7L), (1L, 4L), (1L, 2L) )).keyBy(_._1) .flatMap(new CountWindowAverage()) .print() // the printed output will be (1,4) and (1,5)  env.execute(\u0026#34;ExampleKeyedState\u0026#34;) } 这个例子实现了一个穷人的计数窗口。我们用第一个字段对元组进行 keyed 操作（在本例中，所有元组都有相同的键 1）。该函数将计数和运行的总和存储在一个 ValueState 中。一旦计数达到 2，它就会发出平均数并清除状态，这样我们就可以从 0 开始。注意，如果我们在第一个字段中的元组具有不同的值，那么这将为每个不同的输入键保持不同的状态值。\n状态存活时间(TTL) 可以为任何类型的 keyed state 分配一个生存时间（TTL）。如果配置了 TTL，并且状态值已经过期，存储的值将在尽力的基础上进行清理，这将在下面详细讨论。\n所有状态集合类型都支持每个条目的 TTL。这意味着列表元素和映射条目独立过期。\n为了使用状态 TTL，必须首先建立一个 StateTtlConfig 配置对象。然后可以通过传递配置在任何状态描述符中启用 TTL 功能。\nimport org.apache.flink.api.common.state.StateTtlConfig import org.apache.flink.api.common.state.ValueStateDescriptor import org.apache.flink.api.common.time.Time val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build val stateDescriptor = new ValueStateDescriptor[String](\u0026#34;text state\u0026#34;, classOf[String]) stateDescriptor.enableTimeToLive(ttlConfig) 配置有几个选项需要考虑。\nnewBuilder 方法的第一个参数是强制性的，它是存活的时间值。\n更新类型配置状态 TTL 何时被刷新（默认为 OnCreateAndWrite）。\n StateTtlConfig.UpdateType.OnCreateAndWrite - 仅在创建和写入访问时才会出现 StateTtlConfig.UpdateType.OnReadAndWrite - 也是在读的时候。  状态可见性配置如果过期值尚未清理，是否在读取访问时返回（默认为 NeverReturnExpired）。\n StateTtlConfig.StateVisibility.NeverReturnExpired - 过期值永不返回 StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp - 如果仍然可用则返回。  在 NeverReturnExpired 的情况下，过期状态就像不存在一样，即使它仍然必须被删除。这个选项对于数据在 TTL 之后必须严格地成为不可读的访问状态的用例是很有用的，例如处理隐私敏感数据的应用程序。\n另一个选项 ReturnExpiredIfNotCleanedUp 允许在清理之前返回过期状态。\n注意:\n  状态后端存储最后一次修改的时间戳和用户值，这意味着启用该功能会增加状态存储的消耗。Heap 状态后端在内存中存储了一个额外的 Java 对象，该对象有一个对用户状态对象的引用和一个原始的长值。RocksDB 状态后端每存储一个值、列表项或映射项增加8个字节。\n  目前只支持参考处理时间的 TTL。\n  试图使用启用 TTL 的描述符来恢复之前没有配置 TTL 的状态，或者反之，将导致兼容性失败和 StateMigrationException。\n  TTL 配置不是检查点或保存点的一部分，而是 Flink 在当前运行的作业中如何处理的一种方式。\n  带 TTL 的映射状态目前只有在用户值序列化器能够处理 null 值的情况下才支持 null 用户值。如果序列化器不支持空值，可以用 NullableSerializer 包装，代价是在序列化形式中多出一个字节。\n  过期状态的清理 默认情况下，过期的值会在读取时显式删除，如 ValueState#value，如果配置的状态后台支持，则会定期在后台进行垃圾回收。后台清理可以在 StateTtlConfig 中禁用。\nimport org.apache.flink.api.common.state.StateTtlConfig val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .disableCleanupInBackground .build 如果想对后台的一些特殊清理进行更精细的控制，可以按照下面的描述单独配置。目前，堆状态后台依靠增量清理，RocksDB 后台使用压实过滤器进行后台清理。\n全快照中的清理 此外，您可以在拍摄完整状态快照的瞬间激活清理，这将减少其大小。在当前的实现下，本地状态不会被清理，但在从上一个快照恢复的情况下，它将不包括删除的过期状态。可以在 StateTtlConfig 中进行配置。\nimport org.apache.flink.api.common.state.StateTtlConfig import org.apache.flink.api.common.time.Time val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupFullSnapshot .build 此选项不适用于 RocksDB 状态后端的增量检查点。\n注意:\n 对于现有的作业，这个清理策略可以在 StateTtlConfig 中随时激活或停用，例如从保存点重新启动后。  增量清理 另一种选择是逐步触发一些状态条目的清理。触发器可以是每次状态访问或/和每次记录处理的回调。如果这种清理策略对某些状态是激活的，存储后端就会为这个状态的所有条目保留一个惰性的全局迭代器。每次触发增量清理时，迭代器都会被提前。对遍历过的状态条目进行检查，对过期的条目进行清理。\n这个功能可以在 StateTtlConfig 中配置。\nimport org.apache.flink.api.common.state.StateTtlConfig val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupIncrementally(10, true) .build 这个策略有两个参数。第一个是每次清理触发的检查状态条目数。它总是在每次状态访问时触发。第二个参数定义是否在每次记录处理中额外触发清理。堆后端默认的后台清理每次记录处理检查5个条目而不进行清理。\n注意:\n 如果没有发生对状态的访问或者没有处理记录，过期状态将持续存在。 增量清理所花费的时间会增加记录处理的延迟。 目前，增量清理只在堆状态后端实现。对 RocksDB 的设置不会有影响。 如果堆状态后端与同步快照一起使用，全局迭代器在迭代的时候会保留所有键的副本，因为它的具体实现不支持并发修改。那么启用这个功能会增加内存消耗。异步快照则不存在这个问题。 对于现有的作业，这个清理策略可以在 StateTtlConfig 中随时激活或停用，例如从保存点重新启动后。  RocksDB 压缩过程中的清理 如果使用 RocksDB 状态后端，将调用 Flink 特定的压实过滤器进行后台清理。RocksDB 会定期运行异步压实来合并状态更新，减少存储量。Flink 压实过滤器通过 TTL 检查状态条目的过期时间戳，排除过期值。\n这个功能可以在 StateTtlConfig 中配置。\nimport org.apache.flink.api.common.state.StateTtlConfig val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupInRocksdbCompactFilter(1000) .build RocksDB 压实过滤器在处理一定数量的状态条目后，每次都会从 Flink 中查询当前的时间戳，用于检查过期情况，你可以改变它，并传递自定义值给 StateTtlConfig.newBuilder(...).cleanupInRocksdbCompactFilter(long queryTimeAfterNumEntries) 方法。更频繁地更新时间戳可以提高清理速度，但由于它使用了来自本地代码的 JNI 调用，因此降低了压缩性能。RocksDB 后台默认的清理方式是每次处理1000个条目后查询当前时间戳。\n你可以通过激活 FlinkCompactionFilter 的调试级别来激活 RocksDB 过滤器原生代码的调试日志。\nlog4j.logger.org.rocksdb.FlinkCompactionFilter=DEBUG 注意:\n 在压实过程中调用 TTL 过滤器会使其速度减慢。TTL 过滤器必须解析最后一次访问的时间戳，并检查每个被压缩的键的存储状态条目的到期时间。如果是集合状态类型(list 或 map)，每个存储元素的检查也会被调用。 如果该功能用于具有非固定字节长度元素的列表状态，则原生 TTL 过滤器必须额外调用每个至少第一个元素已过期的状态条目中元素在 JNI 上的 Flink java 类型序列化器，以确定下一个未过期元素的偏移。 对于现有的作业，这种清理策略可以在 StateTtlConfig 中随时激活或停用，例如从保存点重新启动后。  Scala DataStream API 中的状态 除了上面描述的接口外，Scala API 还为 KeyedStream 上具有单个 ValueState 的有状态 map() 或 flatMap() 函数提供了快捷方式。用户函数在 Option 中获取 ValueState 的当前值，并且必须返回一个更新的值，该值将用于更新状态。\nval stream: DataStream[(String, Int)] = ... val counts: DataStream[(String, Int)] = stream .keyBy(_._1) .mapWithState((in: (String, Int), count: Option[Int]) =\u0026gt; count match { case Some(c) =\u0026gt; ( (in._1, c), Some(c + in._2) ) case None =\u0026gt; ( (in._1, 0), Some(in._2) ) }) Operator State Operator State（或 non-keyed state）是指绑定到一个并行操作符实例的状态。Kafka 连接器是 Flink 中使用 Operator State 的一个很好的激励例子。Kafka 消费者的每个并行实例都维护着一个主题分区和偏移的映射作为其 Operator State。\nOperator State 接口支持在并行操作符实例之间重新分配状态，当并行性发生变化时。有不同的方案来进行这种重新分配。\n在典型的有状态的 Flink 应用中，你不需要操作符状态。它主要是一种特殊类型的状态，用于源/接收器实现和你没有键的情况下，可以通过它来分隔状态。\n广播状态 Broadcast State 是 Operator State 的一种特殊类型。引入它是为了支持这样的用例：一个流的记录(records)需要被广播到所有下游任务，它们被用来在所有子任务中保持相同的状态。然后在处理第二个流的记录时可以访问这个状态。作为一个广播状态可以自然出现的例子，我们可以想象一个低吞吐量的流，其中包含一组规则，我们希望对来自另一个流的所有元素进行评估。考虑到上述类型的用例，广播状态与其余运算符状态的不同之处在于。\n 它有一个 map 格式。 它只适用于有广播流和非广播流作为输入的特定操作符，以及 这样的操作符可以拥有多个不同名称的广播状态。  使用 Operator State 要使用运算符状态，有状态函数可以实现 CheckpointedFunction 接口。\nCheckpointedFunction CheckpointedFunction 接口提供了对不同重分配方案的 non-keyed 的访问。它需要实现两个方法。\nvoid snapshotState(FunctionSnapshotContext context) throws Exception; void initializeState(FunctionInitializationContext context) throws Exception; 每当需要执行一个检查点时，就会调用 snapshotState()。与之对应的 initializeState()，在每次用户定义的函数被初始化时都会被调用，不管是在函数首次初始化时，还是在函数实际从早期的检查点恢复时。鉴于此，initializeState() 不仅是初始化不同类型状态的地方，也是包含状态恢复逻辑的地方。\n目前，支持列表式操作符状态。状态有望成为一个可序列化对象的 List，彼此独立，因此在重新缩放时有资格重新分配。换句话说，这些对象是 non-keyed state 可以重新分配的最细粒度。根据状态访问方法的不同，定义了以下重分布方案。\n  均分重分配: 每个操作符都会返回一个状态元素列表。整个状态在逻辑上是所有列表的连接(concatenation)。在还原/再分配时，列表被平均分成有多少个并行操作符就有多少个子列表。每个操作符都会得到一个子列表，这个子列表可以是空的，也可以包含一个或多个元素。举个例子，如果在并行度为1的情况下，一个操作符的检查点状态包含元素1和元素2，当把并行度增加到2时，元素1可能最终进入操作符实例0，而元素2将进入操作符实例1。\n  联盟再分配。每个操作符都会返回一个状态元素列表。整个状态在逻辑上是所有 List 的连接(concatenation)。在还原/再分配时，每个操作符都会得到完整的状态元素列表。如果你的列表可能有很高的基数(cardinality)，请不要使用这个功能。检查点元数据将为每个列表条目存储一个偏移，这可能会导致 RPC 帧大小或内存外错误。\n  下面是一个有状态的 SinkFunction 的例子，它使用 CheckpointedFunction 来缓冲元素，然后再将它们发送到外界。它演示了基本的均分重分配列表状态。\nclass BufferingSink(threshold: Int = 0) extends SinkFunction[(String, Int)] with CheckpointedFunction { @transient private var checkpointedState: ListState[(String, Int)] = _ private val bufferedElements = ListBuffer[(String, Int)]() override def invoke(value: (String, Int), context: Context): Unit = { bufferedElements += value if (bufferedElements.size == threshold) { for (element \u0026lt;- bufferedElements) { // send it to the sink  } bufferedElements.clear() } } override def snapshotState(context: FunctionSnapshotContext): Unit = { checkpointedState.clear() for (element \u0026lt;- bufferedElements) { checkpointedState.add(element) } } override def initializeState(context: FunctionInitializationContext): Unit = { val descriptor = new ListStateDescriptor[(String, Int)]( \u0026#34;buffered-elements\u0026#34;, TypeInformation.of(new TypeHint[(String, Int)]() {}) ) checkpointedState = context.getOperatorStateStore.getListState(descriptor) if(context.isRestored) { for(element \u0026lt;- checkpointedState.get()) { bufferedElements += element } } } } initializeState 方法的参数是一个 FunctionInitializationContext。它用于初始化 non-keyed \u0026ldquo;容器\u0026rdquo;。这些容器是一个 ListState 类型的容器，在检查点时，non-keyed 对象将被存储在那里。\n请注意如何初始化状态，类似于 keyed state，用一个 StateDescriptor 来初始化，这个 StateDescriptor 包含了状态名称和状态所持有的值的类型信息。\nval descriptor = new ListStateDescriptor[(String, Long)]( \u0026#34;buffered-elements\u0026#34;, TypeInformation.of(new TypeHint[(String, Long)]() {}) ) checkpointedState = context.getOperatorStateStore.getListState(descriptor) 状态访问方法的命名约定包含其重分配模式，然后是其状态结构。例如，如果要在还原时使用 union 重分配方案的列表状态，则使用 getUnionListState(descriptor) 访问状态。如果方法名中不包含重分配模式，例如 getListState(descriptor)，则仅仅意味着将使用基本的均分重分配方案。\n在初始化容器后，我们使用上下文的 isRestored() 方法来检查是否在故障后恢复。如果为真，即我们正在恢复，则应用还原逻辑。\n如修改后的 BufferingSink 的代码所示，在状态初始化过程中恢复的这个 ListState 被保存在一个类变量中，以便将来在 snapshotState() 中使用。在那里，ListState 会被清除掉之前检查点所包含的所有对象，然后用我们要检查点的新对象来填充。\n顺便说一下， keyed state 也可以在 initializeState() 方法中初始化。这可以使用提供的 FunctionInitializationContext 来完成。\n有状态的源函数 与其他操作符相比，有状态的源需要更多的小心。为了使状态和输出集合的更新是原子性的（对于失败/恢复时的精确一次性语义来说是必需的），用户需要从源的上下文中获得一个锁。\nclass CounterSource extends RichParallelSourceFunction[Long] with CheckpointedFunction { @volatile private var isRunning = true private var offset = 0L private var state: ListState[Long] = _ override def run(ctx: SourceFunction.SourceContext[Long]): Unit = { val lock = ctx.getCheckpointLock while (isRunning) { // output and state update are atomic  lock.synchronized({ ctx.collect(offset) offset += 1 }) } } override def cancel(): Unit = isRunning = false override def initializeState(context: FunctionInitializationContext): Unit = { state = context.getOperatorStateStore.getListState( new ListStateDescriptor[Long](\u0026#34;state\u0026#34;, classOf[Long])) for (l \u0026lt;- state.get().asScala) { offset = l } } override def snapshotState(context: FunctionSnapshotContext): Unit = { state.clear() state.add(offset) } } 一些运算符可能需要检查点被 Flink 完全承认时的信息来与外界沟通。在这种情况下，请参见 org.apache.flink.runtime.state.CheckpointListener 接口。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/","tags":["Flink","Flink 官方文档","DataStream API","State"],"title":"使用状态"},{"categories":["programming"],"contents":"内置水印生成器 正如在 Generating Watermarks 一文中所描述的，Flink 提供了抽象，允许程序员分配自己的时间戳和发射自己的水印。更具体地说，可以通过实现 WatermarkGenerator 接口来实现。\n为了进一步简化此类任务的编程工作，Flink 自带了一些预先实现的时间戳分配器。本节提供了它们的列表。除了它们的开箱即用的功能外，它们的实现可以作为自定义实现的范例。\n单调地增加时间戳 周期性水印生成的最简单的特殊情况是当给定源任务(task)看到的时间戳以升序出现时。在这种情况下，当前的时间戳总是可以作为水印，因为不会有更早的时间戳到达。\n请注意，只需要每个并行数据源任务的时间戳是升序的。例如，如果在一个特定的设置中，一个 Kafka 分区被一个并行数据源实例读取，那么只需要在每个 Kafka 分区中时间戳是升序的。每当并行流被洗牌、联合、连接(connected)或合并时，Flink 的水印合并机制都会生成正确的水印。\nWatermarkStrategy.forMonotonousTimestamps() 固定的延迟量 周期性水印生成的另一个例子是，当水印滞后于流中看到的最大（事件时间）时间戳的固定时间量时。这种情况涵盖了预先知道流中可能遇到的最大延迟的场景，例如，当创建一个包含时间戳分布在固定时间段内的元素的自定义源进行测试时。对于这些情况，Flink 提供了 BoundedOutOfOrdernessWatermarks 生成器，它以 maxOutOfOrderness 作为参数，即在计算给定窗口的最终结果时，一个元素在被忽略之前允许迟到的最大时间。Lateness 对应于 t - t_w 的结果，其中 t 是一个元素的（事件-时间）时间戳，t_w 是之前的水印。如果 lateness \u0026gt; 0，那么该元素被认为是迟到的，并且默认情况下，在计算其对应窗口的作业结果时被忽略。请参阅关于允许延迟的文档，以获得更多关于处理迟到元素的信息。\nWatermarkStrategy .forBoundedOutOfOrderness(Duration.ofSeconds(10)) ","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-built-in-watermark-generators/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"内置的水印生成器"},{"categories":["Flink"],"contents":"状态和容错性 在本节中，您将了解 Flink 为编写有状态程序提供的 API。请看一下 Stateful Stream Processing，了解有状态流处理背后的概念。\n下一步怎么走？  使用状态。展示如何在 Flink 应用中使用状态，并解释不同类型的状态。 广播状态模式。解释如何连接一个广播流和一个非广播流，并使用状态在它们之间交换信息。 检查点。描述了如何启用和配置检查点以实现容错。 可查询状态。说明如何在运行时从 Flink 外部访问状态。 状态模式演化：介绍如何在运行时从外部访问状态。展示了状态类型的模式如何演化。 管理状态的自定义序列化。讨论如何实现自定义序列化，特别是针对模式演化。  ","permalink":"https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/","tags":["Flink","Flink 官方文档","DataStream API"],"title":"状态和容错性"},{"categories":["programming"],"contents":"Flink 是一个分布式系统，为了执行流式应用，需要对计算资源进行有效的分配和管理。它集成了所有常见的集群资源管理器，如 Hadoop YARN、Apache Mesos 和 Kubernetes，但也可以设置为独立集群甚至作为库运行。\n本节包含 Flink 架构的概述，并描述了其主要组件如何交互执行应用程序并从故障中恢复。\nFlink 集群的解剖 Flink 运行时由两种类型的进程组成：一个 JobManager 和一个或多个 TaskManagers。\n客户端不是运行时和程序执行的一部分，而是用来准备并向 JobManager 发送数据流。之后，客户端可以断开连接（分离模式），或者保持连接以接收进度报告（附加模式）。客户端既可以作为触发执行的 Java/Scala 程序的一部分运行，也可以在命令行进程 ./bin/flink run \u0026hellip;中运行。\nJobManager 和 TaskManagers 可以以各种方式启动：直接在机器上作为一个独立的集群，在容器中，或由 YARN 或 Mesos 等资源框架管理。TaskManagers 连接到 JobManagers，宣布自己可用，并被分配工作。\nJobManager JobManager 有一些与协调 Flink 应用的分布式执行有关的职责：它决定何时安排下一个任务（或一组任务），对已完成的任务或执行失败作出反应，协调检查点，并协调失败时的恢复等。这个过程由三个不同的组件组成。\n 资源管理器(ResourceManager)  ResourceManager 负责 Flink 集群中的资源去/分配和供应\u0026ndash;它管理任务槽(task slots)，任务槽是 Flink 集群中资源调度的单位（见 TaskManagers）。Flink 针对不同的环境和资源提供者（如 YARN、Mesos、Kubernetes 和独立部署）实现了多个 ResourceManagers。在独立设置中，ResourceManager 只能分配可用的 TaskManagers 的槽位，不能自行启动新的 TaskManagers。\n Dispatcher  Dispatcher 提供了一个 REST 接口来提交 Flink 应用执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 来提供作业执行的信息。\n JobMaster  一个 JobMaster 负责管理一个 JobGraph 的执行。在一个 Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。\n总是至少有一个 JobManager。一个高可用性设置可能有多个 JobManagers，其中一个总是领导者，其他的是备用的（见高可用性（HA））。\nTaskManagers 任务管理器（TaskManagers）（也叫 worker）执行数据流的任务，并缓冲和交换数据流。\n必须始终有至少一个TaskManager。TaskManager中资源调度的最小单位是一个任务槽。一个任务管理器中任务槽的数量表示并发处理任务的数量。请注意，一个任务槽中可以执行多个操作者（参见Tasks 和 Operator 链）。\nTasks 和 Operator Chains 对于分布式执行，Flink 将操作者的子任务链成任务。每个任务由一个线程执行。将运算符一起链入任务是一种有用的优化：它减少了线程到线程的交接和缓冲的开销，增加了整体的吞吐量，同时降低了延迟。链锁行为可以配置，详情请看chaining 文档。\n下图中的示例数据流是以五个子任务，也就是五个并行线程来执行的。\n任务槽和资源(Task Slots 和 Resources) 每个 worker（TaskManager）都是一个 JVM 进程，可以在单独的线程中执行一个或多个子任务。为了控制一个任务管理器接受多少任务，它有所谓的任务槽（至少一个）。\n每个任务槽代表任务管理器的一个固定的资源子集。例如，一个有三个槽的任务管理器，将把其管理内存的1/3奉献给每个槽。槽位资源意味着一个子任务不会与其他任务的子任务争夺管理内存，而是拥有一定量的预留管理内存。需要注意的是，这里并没有发生 CPU 隔离，目前插槽只是将任务的管理内存分开。\n通过调整任务槽的数量，用户可以定义子任务之间的隔离方式。每个任务管理器有一个插槽意味着每个任务组都在一个单独的 JVM 中运行（例如可以在一个单独的容器中启动）。拥有多个插槽意味着更多的子任务共享同一个 JVM。同一 JVM 中的任务共享 TCP 连接（通过多路复用）和心跳消息。它们还可以共享数据集和数据结构，从而减少每个任务的开销。\n默认情况下，Flink 允许子任务共享槽，即使它们是不同任务的子任务，只要它们来自同一个作业。其结果是，一个槽可以容纳整个作业的流水线。允许这种槽位共享有两个主要好处。\n  一个 Flink 集群需要的任务槽数量正好与作业中使用的最高并行度相同。不需要计算一个程序总共包含多少个任务（具有不同的并行度）。\n  更容易获得更好的资源利用率。如果没有槽位共享，非密集型的 source/map() 子任务和资源密集型的 window 子任务一样，会阻塞很多资源。有了槽位共享，在我们的例子中，将基础并行度从2个增加到6个，就会产生槽位资源的充分利用，同时确保重度子任务在 TaskManager 中公平分配。\n  Flink 应用执行 Flink 应用程序是任何从其 main() 方法中生成一个或多个 Flink 作业的用户程序。这些作业的执行可以发生在本地 JVM（LocalEnvironment）中，也可以发生在多台机器的远程集群设置（RemoteEnvironment）中。对于每个程序，ExecutionEnvironment 提供了控制作业执行的方法（例如设置并行性）和与外界交互的方法（参见 Anatomy of a Flink Program）。\nFlink 应用的作业可以提交到一个长期运行的 Flink 会话集群、一个专门的 Flink 作业集群或一个 Flink 应用集群。这些选项之间的区别主要与集群的生命周期和资源隔离保证有关。\nFlink 会话集群   集群生命周期：在 Flink 会话集群中，客户端连接到一个预先存在的、长期运行的集群，可以接受多个作业提交。即使在所有作业完成后，集群（和 JobManager）将继续运行，直到会话被手动停止。因此，一个 Flink 会话集群的寿命不受任何 Flink 作业寿命的约束。\n  资源隔离。TaskManager 插槽由 ResourceManager 在作业提交时分配，作业完成后释放。因为所有作业都共享同一个集群，所以对集群资源有一定的竞争\u0026ndash;比如提交作业阶段的网络带宽。这种共享设置的一个限制是，如果一个任务管理器崩溃，那么所有在这个任务管理器上有任务运行的作业都会失败；同样，如果在作业管理器上发生一些致命的错误，也会影响集群中运行的所有作业。\n  其他考虑因素：拥有一个预先存在的集群，可以节省大量申请资源和启动 TaskManagers 的时间。这在作业的执行时间非常短，高启动时间会对端到端的用户体验产生负面影响的场景中非常重要\u0026ndash;就像对短查询的交互式分析一样，希望作业能够利用现有资源快速执行计算。\n  注：以前，Flink 会话集群也被称为会话模式下的 Flink 集群。\nFlink 作业集群   集群生命周期：在 Flink Job Cluster 中，可用的集群管理器（如 YARN 或 Kubernetes）为每个提交的作业旋转一个集群，这个集群只对该作业可用。在这里，客户端首先向集群管理器请求资源来启动 JobManager，并将作业提交给运行在这个进程内部的 Dispatcher。然后根据作业的资源需求，懒惰地分配 TaskManager。作业完成后，Flink Job Cluster 就会被拆掉。\n  资源隔离：JobManager 的致命错误只影响该 Flink Job Cluster 中运行的一个作业。\n  其他考虑因素：由于 ResourceManager 需要申请并等待外部资源管理组件来启动 TaskManager 进程并分配资源，因此 Flink Job Cluster 更适合运行时间长、稳定性要求高、对启动时间较长不敏感的大型作业。\n注：以前，Flink Job Cluster 也被称为作业（或每作业）模式下的 Flink Cluster。\nFlink 应用集群(Flink Application Cluster)   集群生命周期：Flink 应用集群是一个专用的 Flink 集群，它只执行来自一个 Flink 应用的作业，并且 main() 方法运行在集群上而不是客户端上。作业提交是一个一步到位的过程：你不需要先启动一个 Flink 集群，然后向现有的集群会话提交作业，而是将你的应用逻辑和依赖关系打包成一个可执行的作业 JAR，集群入口点(ApplicationClusterEntryPoint)负责调用 main() 方法来提取作业图。这样你就可以像在 Kubernetes 上部署其他应用一样部署 Flink 应用，例如。因此，Flink Application Cluster 的寿命与 Flink Application 的寿命是绑定的。\n  资源隔离：在 Flink Application Cluster 中，ResourceManager 和 Dispatcher 的范围是单一的 Flink Application，这比 Flink Session Cluster 提供了更好的分离关注点。\n  注：Flink Job Cluster 可以看作是 Flink Application Cluster 的 \u0026ldquo;run-on-client\u0026rdquo; 替代品。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-20-flink-architecture/","tags":["Flink","architecture"],"title":"Flink 的架构"},{"categories":["programming"],"contents":"介绍 及时流处理是有状态流处理的一种扩展，其中时间在计算中起着一定的作用。其中，当你做时间序列分析时，当做基于特定时间段（通常称为窗口）的聚合时，或者当你做事件处理时，事件发生的时间很重要时，都是这种情况。\n在下面的章节中，我们将着重介绍一些您在使用及时 Flink 应用时应该考虑的主题。\n时间的概念：事件时间和处理时间 当在流程序中提到时间时（例如定义窗口），可以提到不同的时间概念。\n 处理时间。处理时间指的是正在执行相应操作的机器的系统时间。  当流程序在处理时间上运行时，所有基于时间的操作(如时间窗口)将使用运行各操作的机器的系统时钟。一个小时的处理时间窗口将包括在系统时钟指示整小时的时间之间到达特定操作者的所有记录。例如，如果一个应用程序在上午9:15开始运行，则第一个小时处理时间窗口将包括上午9:15到10:00之间处理的事件，下一个窗口将包括上午10:00到11:00之间处理的事件，以此类推。\n处理时间是最简单的时间概念，不需要流和机器之间的协调。它提供了最好的性能和最低的延迟。然而，在分布式和异步环境中，处理时间并不能提供确定性，因为它很容易受到记录到达系统的速度（例如从消息队列）、记录在系统内部的操作员之间流动的速度以及中断（计划性的或其他）的影响。\n 事件时间。事件时间是指每个事件在其生产设备上发生的时间。这个时间通常在记录进入 Flink 之前就被嵌入到记录中，该事件时间戳可以从每个记录中提取出来。在事件时间中，时间的进展取决于数据，而不是任何挂钟。事件时间程序必须指定如何生成事件时间水印，这是事件时间中信号进度的机制。这个水印机制将在后面的章节中描述，下面。  在一个完美的世界里，事件时间处理将产生完全一致和确定的结果，不管事件何时到达，或它们的顺序如何。然而，除非已知事件是按顺序到达的（通过时间戳），否则事件时间处理在等待失序事件时就会产生一些延迟。由于只能在有限的时间内等待，这就对事件时间应用的确定性提出了限制。\n假设所有的数据都已经到达，事件时间操作将按照预期的方式进行，即使在处理失序或迟到的事件时，或者在重新处理历史数据时，也能产生正确和一致的结果。例如，每小时事件时间窗口将包含所有携带事件时间戳的记录，这些记录属于该小时，无论它们到达的顺序如何，也无论它们何时被处理。更多信息请参见\u0026ldquo;迟到事件\u0026rdquo;一节）。\n需要注意的是，有时事件时间程序在实时处理实时数据时，会使用一些处理时间操作来保证其及时进行。\n事件时间和水印 注：Flink 实现了 Dataflow 模型中的许多技术。对于事件时间和水印的介绍，可以看看下面的文章。\n Tyler Akidau 的 Streaming 101。 数据流模型论文  一个支持事件时间的流处理器需要一种方法来测量事件时间的进度。例如，当事件时间已经超过一小时结束时，需要通知建立小时窗口的窗口操作员，以便操作员可以关闭正在进行的窗口。\n事件时间的进展可以独立于处理时间(由挂钟测量)。例如，在一个程序中，操作者的当前事件时间可能略微落后于处理时间(考虑到接收事件的延迟)，而两者以相同的速度进行。另一方面，另一个流程序可能通过快进一些已经缓冲在 Kafka 主题（或另一个消息队列）中的历史数据，只用几秒钟的处理时间就可以完成几周的事件时间的进展。\nFlink 中衡量事件时间进度的机制是水印。水印作为数据流的一部分流动，并携带一个时间戳 t，一个 Watermark(t) 声明该数据流中的事件时间已经达到了时间 t，也就是说该数据流中不应该再有时间戳 t'\u0026lt;=t 的元素（即事件的时间戳大于或等于水印）。\n下图显示了一个带有（逻辑）时间戳的事件流，以及水印的内联流。在这个例子中，事件是按顺序排列的（相对于它们的时间戳），这意味着水印只是流中的周期性标记。\n水印对于无序流来说是至关重要的，如下图所示，在这种情况下，事件不是按照时间戳来排序的。一般来说，水印是一种声明，即在流中的那一点上，所有事件在某个时间戳之前都应该已经到达。一旦水印到达操作者，操作者可以将其内部事件时间时钟提前到水印的值。\n请注意，事件时间是由新创建的流元素（或元素）从产生它们的事件或触发创建这些元素的水印中继承的。\n并行流中的水印 水印是在源函数处或直接在源函数后生成的。源函数的每个并行子任务通常都会独立地生成其水印。这些水印定义了该特定并行源的事件时间。\n当水印流经流程序时，它们会在它们到达的操作符处提前事件时间。每当一个操作者提前其事件时间时，它就会在下游为其后续操作者生成一个新的水印。\n有些运算符会消耗多个输入流；例如，一个联合，或者在 keyBy(...) 或 partition(...) 函数之后的运算符。这种运算符的当前事件时间是其输入流事件时间的最小值。当它的输入流更新它们的事件时间时，该运算符也会更新。\n下图显示了事件和水印在并行流中流动的例子，以及运算符跟踪事件时间的例子。\n延时 某些元素有可能会违反水印条件，也就是说，即使在水印(t)发生后，也会有更多时间戳 t'\u0026lt;=t 的元素发生。事实上，在许多现实世界的设置中，某些元素可以任意延迟，这使得无法指定某个事件时间戳的所有元素在什么时候发生。此外，即使延迟时间可以被限定，但延迟水印的时间过长往往是不可取的，因为它对事件时间窗口的评估造成过多的延迟。\n出于这个原因，流媒体程序可能会显式地期望一些迟到的元素。晚期元素是指在系统的事件时间时钟（由水印发出的信号）已经过了晚期元素的时间戳之后到达的元素。有关如何在事件时间窗口中处理迟到元素的更多信息，请参见允许的延时。\n窗口 聚合事件（如计数、求和）在流上的工作方式与批处理中的工作方式不同。例如，不可能对一个流中的所有元素进行计数，因为流一般是无限的（无边界的）。相反，流上的聚合（计数、求和等）是由窗口来限定范围的，比如 \u0026ldquo;过去5分钟的计数\u0026rdquo;，或者\u0026quot;过去100个元素的总和\u0026quot;。\n窗口可以是时间驱动的（例如：每30秒），也可以是数据驱动的（例如：每100个元素）。人们通常会区分不同类型的窗口，如翻滚窗口（无重叠）、滑动窗口（有重叠）和会话窗口（以不活动的间隙为点）。\n请查看这篇博客文章，了解更多的窗口示例，或查看 DataStream API 的窗口文档。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-20-timely-stream-processing/","tags":["Flink","stream"],"title":"及时的流处理"},{"categories":["programming"],"contents":"什么是状态？ 虽然数据流中的许多操作一次只看一个单独的事件（例如事件分析器），但有些操作会记住多个事件的信息（例如窗口 operator ）。这些操作被称为有状态操作。\n一些有状态操作的例子:\n 当一个应用程序搜索某些事件模式时，状态将存储到目前为止遇到的事件序列。 当按分钟/小时/天聚合事件时，状态会保存待聚合的事件。 当在数据点流上训练机器学习模型时，状态会保存模型参数的当前版本。 当需要管理历史数据时，状态可以有效访问过去发生的事件。  Flink 需要了解状态，以便使用检查点和保存点使其具有容错性。\n关于状态的知识还允许重新缩放 Flink 应用，这意味着 Flink 负责在并行实例之间重新分配状态。\n可查询状态允许你在运行时从 Flink 外部访问状态。\n在处理状态时，阅读一下 Flink 的状态后端可能也很有用。Flink 提供了不同的状态后端，指定了状态的存储方式和位置。\nKeyed State Keyed state 被维护在可以被认为是一个嵌入式键/值存储中。该状态严格地与有状态 operator 读取的流一起被分割和分配。因此，对键/值状态的访问只有在 keyed streams 上，即在 keyed/分区数据交换之后才有可能，并且仅限于与当前事件的键相关联的值。将流和状态的键对齐，可以确保所有的状态更新都是本地操作，保证了一致性，而没有事务开销。这种对齐方式还允许 Flink 透明地重新分配状态和调整流分区。\nKeyed State 被进一步组织成所谓的 Key Groups。Key Groups 是 Flink 可以重新分配 Keyed State 的原子单位；Key Groups 的数量正好与定义的最大并行度相同。在执行过程中，keyed operator 的每个并行实例都与一个或多个 Key Groups 的键一起工作。\n状态持久化 Flink 使用流重放(stream replay)和检查点(checkpointing)的组合来实现容错。一个检查点标记了每个输入流中的一个特定点以及每个 operator 的相应状态。通过恢复运算符的状态，从检查点开始重放记录，可以从检查点恢复流数据流，同时保持一致性（精确的一次处理语义）。\n检查点间隔是用恢复时间（需要重放的记录数量）来交换执行过程中容错的开销的一种手段。\n容错机制不断地绘制分布式流数据流的快照。对于状态较小的流媒体应用，这些快照非常轻量级，可以频繁地绘制，而不会对性能产生太大的影响。流应用的状态存储在一个可配置的地方，通常是在一个分布式文件系统中。\n在程序失败的情况下（由于机器、网络或软件故障），Flink会停止分布式流数据流。然后系统会重新启动 operator ，并将其重置到最新的成功检查点。输入流被重置到状态快照的点。作为重新启动的并行数据流的一部分处理的任何记录都保证不影响之前的检查点状态。\n注: 默认情况下，检查点被禁用。有关如何启用和配置检查点的详细信息，请参见检查点。\n注: 为了实现这种机制的完全保证，数据流源（如消息队列或 broker）需要能够将数据流倒退到一个定义的最近点。Apache Kafka 具有这种能力，Flink 的 Kafka 连接器利用了这一点。参见数据源和接收器的容错保证，了解更多关于 Flink 连接器提供的保证的信息。\n注: 因为 Flink 的检查点是通过分布式快照实现的，所以我们互换使用快照和检查点这两个词。通常我们也使用术语快照来表示检查点或保存点。\n检查点 Flink 容错机制的核心部分是绘制分布式数据流和 operator 状态的一致快照。这些快照作为一致的检查点，系统在发生故障时可以回退。Flink 绘制这些快照的机制在 \u0026ldquo;Lightweight Asynchronous Snapshots for Distributed Dataflows\u0026rdquo; 中描述。它的灵感来自于分布式快照的标准 Chandy-Lamport 算法，并专门为 Flink 的执行模型量身定做。\n请记住，所有与检查点有关的事情都可以异步完成。检查点屏障不按锁步走，操作可以异步快照其状态。\n自 Flink 1.11 以来，检查点可以在有或没有对齐的情况下进行。在本节中，我们先介绍对齐的检查点。\n屏障 Flink 的分布式快照中的一个核心元素是流屏障。这些屏障被注入到数据流中，并作为数据流的一部分与记录一起流动。屏障永远不会超越记录，它们严格按照线路流动。屏障将数据流中的记录分为进入当前快照的记录集和进入下一个快照的记录。每个屏障都带有其记录被推到前面的快照的ID。屏障不会中断数据流的流动，因此非常轻量级。不同快照的多个屏障可以同时出现在流中，这意味着不同的快照可以同时发生。\n流屏障是在流源处注入并行数据流。快照n的屏障被注入的点（我们称它为 Sₙ）是源流中快照覆盖数据的位置。例如，在 Apache Kafka 中，这个位置将是分区中最后一条记录的偏移。这个位置 Sₙ 被报告给检查点协调器（Flink 的 JobManager）。\n然后，屏障就会流向下游。当一个中间 operator 从它的所有输入流中接收到一个快照n的屏障时，它就会向它的所有输出流中发出一个快照n的屏障。一旦一个汇 operator （流 DAG 的末端）从它的所有输入流中接收到屏障n，它就会向检查点协调器确认该快照n。在所有的接收器(sink)确认了一个快照之后，它就被认为完成了。\n一旦快照n完成后，作业再也不会向源头询问 Sₙ 之前的记录，因为此时这些记录（以及它们的子孙记录）将通过整个数据流拓扑。\n接收多个输入流的 operator 需要将输入流对准快照屏障。上图就说明了这一点。\n 一旦 operator 从一个输入流中接收到快照屏障n，它就不能再处理该流的任何记录，直到它也从其他输入中接收到屏障n。否则，它就会把属于快照n的记录和属于快照n+1的记录混在一起。 一旦最后一个流收到了屏障n， operator 就会发出所有的待发记录，然后自己发出快照n的屏障。 它快照状态并恢复处理所有输入流的记录，在处理流的记录之前，先处理输入缓冲区的记录。 最后， operator 将状态异步写入状态后端。  需要注意的是，所有具有多个输入的 operator 和洗牌后的 operator 在消耗多个上游子任务的输出流时，都需要进行对齐。\n快照 Operator State 当 operator 包含任何形式的状态时，这个状态也必须是快照的一部分。\nOperator 在从输入流接收到所有快照屏障后，在向输出流发出屏障之前，在这个时间点快照其状态。这时，所有来自屏障之前的记录对状态的更新都已经进行了，而没有依赖于屏障之后的记录的更新。由于快照的状态可能很大，所以它被存储在一个可配置的状态后端。默认情况下，这是 JobManager 的内存，但对于生产使用，应该配置一个分布式的可靠存储（如 HDFS）。状态存储完毕后， operator 确认检查点，向输出流发出快照屏障，然后继续进行。\n现在产生的快照包含。\n 对于每个并行流数据源，当快照开始时，流中的偏移量/位置。 对于每个 operator，一个指向作为快照的一部分存储的状态的指针。  恢复 这种机制下的恢复是直接的。系统会重新部署整个分布式数据流，并给每个 operator 提供快照的状态，作为检查点 k 的一部分。 来源被设置为从位置 Sₖ 开始读取数据流。例如在 Apache Kafka 中，这意味着告诉消费者从偏移量 Sₖ 开始获取。\n如果状态是增量快照的，则运算符从最新的完整快照的状态开始，然后对该状态应用一系列增量快照更新。\n更多信息请参见重启策略。\n不对齐检查点 从 Flink 1.11 开始，检查点也可以在不对齐的情况下进行。基本思路是，只要飞行中的数据成为 operator 状态的一部分，检查点就可以覆盖所有飞行中的数据。\n请注意，这种方法实际上更接近 Chandy-Lamport 算法 ，但 Flink 仍然在源中插入屏障，以避免超载检查点协调器。\n该图描述了一个 operator 如何处理不对齐的检查点屏障。\n operator 对输入缓冲区中存储的第一个屏障作出反应。 它立即将屏障转发给下游 operator ，将其添加到输出缓冲区的末尾。 operator 将所有被超越的记录标记为异步存储，并创建自己状态的快照。  因此， operator 只短暂地停止对输入的处理以标记缓冲区，转发屏障，并创建其他状态的快照。\n不对齐的检查点确保屏障以最快的速度到达汇流排。它特别适合于至少有一条缓慢移动的数据路径的应用，在这种应用中，对齐时间可能达到数小时。然而，由于它增加了额外的I/O压力，所以当状态后端的I/O是瓶颈时，它并没有帮助。关于其他的局限性，请参见运维中更深入的讨论。\n请注意，保存点将始终是对齐的。\n未对齐的恢复 operator 首先恢复飞行中的数据，然后才开始处理来自上游 operator 在不结盟检查点的任何数据。除此之外，它执行的步骤与恢复对齐检查点时相同。\n状态后端 键/值索引的具体数据结构取决于所选择的状态后端。一种状态后端将数据存储在内存中的哈希图中，另一种状态后端使用 RocksDB 作为键/值存储。除了定义持有状态的数据结构外，状态后端还实现了对键/值状态进行时间点快照的逻辑，并将该快照作为检查点的一部分进行存储。状态后端可以在不改变应用逻辑的情况下进行配置。\n保存点 所有使用检查点的程序都可以从保存点(savepoint)恢复执行。保存点允许在不丢失任何状态的情况下同时更新你的程序和 Flink 集群。\n保存点是手动触发的检查点，它采取程序的快照并将其写入状态后端。它们依靠常规的检查点机制来实现。\n保存点与检查点类似，只是它们是由用户触发的，当新的检查点完成后，它们不会自动失效。\n完全一次与至少一次 对齐步骤可能会给流媒体程序增加延迟。通常，这种额外的延迟是在几毫秒的数量级，但我们已经看到一些异常值的延迟明显增加的情况。对于要求所有记录持续超低延迟（几毫秒）的应用，Flink 有一个开关，可以在检查点期间跳过流对齐。只要 operator 从每个输入中看到检查点屏障，检查点快照仍然会被绘制。\n当跳过对齐时， operator 会继续处理所有的输入，甚至在一些检查点 n 的检查点屏障到达后， operator 也会继续处理。这样一来， operator 也会在检查点 n 的状态快照被采集之前处理属于检查点 n+1 的元素。在还原时，这些记录将作为重复发生，因为它们都包含在检查点 n 的状态快照中，并将在检查点 n 之后作为数据的一部分重放。\n注意对齐只发生在有多个前辈的 operator （连接）以及有多个发送者的 operator （流重新分区/洗牌后）。正因为如此，即使在至少一次的模式下，只有令人尴尬的并行流操作(map(), flatMap(), filter(), \u0026hellip;)的数据流实际上也会给出精确的一次保证。\n批量程序中的状态和容错能力 Flink 执行批处理程序是流程序的一种特殊情况，其中流是有界的（元素数量有限）。一个 DataSet 在内部被当作一个数据流。因此，上述概念适用于批处理程序的方式与适用于流程序的方式相同，但有一些小的例外。\n  批处理程序的容错不使用检查点。恢复是通过完全重放流发生的。这是可能的，因为输入是有界的。这将成本更多地推向恢复，但使常规处理更便宜，因为它避免了检查点。\n  DataSet API 中的有状态操作使用简化的内存内/核心外数据结构，而不是键/值索引。\n  DataSet API 引入了特殊的同步（基于superstep的）迭代，只有在有界流上才能实现。详情请查看迭代文档。\n  ","permalink":"https://ohmyweekly.github.io/notes/2020-08-20-stateful-stream-processing/","tags":["Flink","stateful"],"title":"有状态的流处理"},{"categories":["programming"],"contents":"Flink Application Cluster\nFlink 应用集群是一个专用的 Flink 集群，它只执行一个 Flink 应用的 Flink 作业。Flink 集群的寿命与 Flink 应用的寿命绑定。\nFlink Job Cluster\nFlink Job Cluster 是一个专用的 Flink Cluster，它只执行一个 Flink Job。Flink Cluster 的寿命与 Flink Job 的寿命绑定。\nFlink Cluster\n一个分布式系统由（通常）一个 JobManager 和一个或多个 Flink TaskManager 进程组成。\nEvent\n事件是关于应用程序所模拟的域的状态变化的声明。事件可以是流或批处理应用程序的输入和/或输出。事件是特殊类型的记录。\nExecutionGraph\n参见物理图(Physical Graph)\nFunction\n函数由用户实现，封装了 Flink 程序的应用逻辑。大多数 Functions 都由相应的 Operator 封装。\nInstance\n术语 instance 用于描述运行时特定类型（通常是 Operator 或 Function）的具体实例。由于 Apache Flink 大部分是用 Java 编写的，所以对应于 Java 中的 Instance 或 Object 的定义。在 Apache Flink 的上下文中，并行实例这个术语也经常被用来强调同一个 Operator 或 Function 类型的多个实例在并行运行。\nFlink Application\nFlink 应用程序是一个 Java 应用程序，它从 main() 方法(或通过其他方式)提交一个或多个 Flink 作业。提交作业通常是通过调用执行环境上的 execute() 来完成的。\n应用程序的作业可以提交到一个长期运行的 Flink 会话集群，也可以提交到一个专门的 Flink 应用集群，或者提交到一个 Flink 作业集群。\nFlink Job\nFlink Job 是指在 Flink 应用中通过调用 execute() 来创建和提交的逻辑图（也常称为数据流图）的运行时表示。\nJobGraph\n参见逻辑图(Logical Graph)\nFlink JobManager\nJobManager 是 Flink 集群的协调器。它包含了三个不同的组件：Flink 资源管理器、Flink 调度器和每个运行的 Flink 作业 一个 Flink JobMaster。\nFlink JobMaster\nJobMasters 是运行在 JobManager 中的组件之一。一个 JobMaster 负责监督单个作业的 Tasks 的执行情况。\nLogical Graph\n逻辑图是一个有向图，其中节点是 Operators，边缘定义了 operator 的输入/输出关系，并对应数据流或数据集。逻辑图是通过从 Flink 应用程序提交作业来创建的。\n逻辑图也常被称为数据流图。\nManaged State\nManaged State 描述的是已经在框架中注册的应用状态。对于托管状态，Apache Flink 将负责处理持久性和重新缩放等问题。\nOperator\n逻辑图的节点。Operator 执行某种操作，通常由 Function 执行。源和接收器是数据摄入和数据输出的特殊 Operator。\nOperator Chain\n一个 Operator 链由两个或多个连续的 Operator 组成，中间没有任何重新分区。同一 Operator 链内的 operattor 直接相互转发记录，而不需要经过序列化或 Flink 的网络栈。\nPartition\n分区是整个数据流或数据集的一个独立子集。通过将每条记录分配到一个或多个分区，将数据流或数据集划分为多个分区。数据流或数据集的分区在运行时由Tasks消耗。改变数据流或数据集分区方式的转换通常称为重新分区。\nPhysical Graph\n物理图是翻译逻辑图的结果，以便在分布式运行时执行。节点是Tasks，边缘表示输入/输出关系或数据流或数据集的分区。\nRecord\n记录是数据集或数据流的组成元素。Operator 和 Functions 接收记录作为输入，并发出记录作为输出。\nFlink Session Cluster\n一个长期运行的 Flink Cluster，它接受多个 Flink Job 的执行。该 Flink Cluster 的寿命不受任何 Flink Job 寿命的约束。以前，Flink Session Cluster 也被称为会话模式下的 Flink Cluster。与 Flink Application Cluster 比较。\nState Backend\n对于流处理程序来说，Flink Job 的状态后端决定了它的状态如何存储在每个 TaskManager 上（TaskManager 的 Java 堆或（嵌入式）RocksDB），以及它在检查点时的写入位置（JobManager 的 Java 堆或 Filesystem）。\nSub-Task\n子任务( Sub-Task)是指负责处理数据流的一个分区的任务(Task)。术语\u0026quot;子任务\u0026quot;强调同一 Operator 或 Operator 链有多个并行的 Task。\nTask\n物理图的节点。Task 是工作的基本单位，由 Flink 的运行时执行。任务正好封装了一个 Operator 或 Operator 链 的一个并行实例。\nFlink TaskManager\nTaskManager 是 Flink Cluster 的工作进程。Tasks 被安排给 TaskManagers 执行。它们相互通信，在后续的 Task 之间交换数据。\nTransformation\n变换应用于一个或多个数据流或数据集，并产生一个或多个输出数据流或数据集。变换可能会在每条记录的基础上改变数据流或数据集，但也可能只改变其分区或执行聚合。Operator 或 Functions是 Flink 的 API 的 \u0026ldquo;物理\u0026quot;部分，而变换只是一个 API 概念。具体来说，大多数变换是由某些 Operator 实现的。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-20-glossary/","tags":["Flink","architecture"],"title":"术语表"},{"categories":["programming"],"contents":"概念 实践培训解释了作为 Flink 的 API 基础的有状态和及时流处理的基本概念，并提供了这些机制如何在应用中使用的例子。有状态的流处理是在数据管道和ETL的背景下介绍的，并在容错部分进一步发展。在\u0026ldquo;流分析\u0026rdquo;一节中介绍了及时的流处理。\n本概念深度部分提供了对 Flink 的架构和运行时如何实现这些概念的更深入的理解。\nFlink 的 API Flink 为开发流式/批量应用提供了不同层次的抽象。\n  最底层的抽象只是提供有状态和及时的流处理。它通过 Process Function 嵌入到 DataStream API 中。它允许用户自由处理来自一个或多个流的事件，并提供一致的、容错的状态。此外，用户还可以注册事件时间和处理时间的回调，使程序可以实现复杂的计算。\n  在实际应用中，很多应用程序并不需要上述的低级抽象，而是可以针对 Core APIs 进行编程：DataStream API（有界/无界流）和 DataSet API（有界数据集）。这些流畅的 API 为数据处理提供了常见的构件，比如各种形式的用户指定的转换、连接、聚合、窗口、状态等。在这些 API 中处理的数据类型在各自的编程语言中被表示为类。\n  低级 Process Function 与 DataStream API 相集成，因此可以根据需要使用低级抽象。DataSet API 提供了关于有界数据集的附加原语，如循环/迭代。\n Table API 是以表为中心的声明式 DSL，它可能是动态变化的表（当表示流时）。Table API 遵循（扩展的）关系模型。表有一个附加的模式（类似于关系数据库中的表），API 提供了可比较的操作，如select、project、join、group-by、aggregation 等。Table API 程序声明式地定义了应该做什么逻辑操作，而不是具体规定操作的代码是怎样的。虽然 Table API 可以通过各种类型的用户定义函数进行扩展，但它的表现力不如 Core API，使用起来更简洁（写的代码更少）。此外，Table API 程序在执行前还要经过一个优化器，应用优化规则。  人们可以在表和 DataStream/DataSet 之间无缝转换，允许程序将 Table API 与 DataStream 和 DataSet API 混合使用。\n Flink 提供的最高级抽象是 SQL。这个抽象在语义和表现形式上都与 Table API 相似，但将程序表示为 SQL 查询表达式。SQL 抽象与 Table API 紧密交互，SQL 查询可以在 Table API 中定义的表上执行。  ","permalink":"https://ohmyweekly.github.io/notes/2020-08-20-concepts-overview/","tags":["Flink","概念"],"title":"概念"},{"categories":["programming"],"contents":"本次培训的重点是广泛地介绍 DataStream API，使你能够开始编写流式应用程序。\n什么可以被流式化？ Flink 的 DataStream API(Java 和 Scala)可以让你流化任何可以序列化的东西。Flink 自己的序列化器用于:\n 基本类型，即 String, Long, Integer, Boolean, Array 复合类型。Tuples, POJOs 和 Scala case classes  而 Flink 又回到了 Kryo 的其他类型。也可以在 Flink 中使用其他序列化器。特别是 Avro，得到了很好的支持。\nJava 元组 和 POJO Flink 的本地序列化器可以有效地操作元组和 POJO。\n元组\n对于 Java，Flink 定义了自己的 Tuple0 到 Tuple25 类型。\nTuple2\u0026lt;String, Integer\u0026gt; person = Tuple2.of(\u0026#34;Fred\u0026#34;, 35); // zero based index! String name = person.f0; Integer age = person.f1; POJO\n如果满足以下条件，Flink 将数据类型识别为 POJO 类型（并允许\u0026quot;按名称\u0026quot;字段引用）。\n 类是公共的和独立的（没有非静态的内部类）。 该类有一个公共的无参数构造函数。 类（以及所有超级类）中的所有非静态、非瞬态字段要么是公共的（而且是非最终的），要么有公共的 getter- 和 setter- 方法，这些方法遵循 Java beans 中 getter 和 setter 的命名约定。  例如:\npublic class Person { public String name; public Integer age; public Person() {}; public Person(String name, Integer age) { . . . }; } Person person = new Person(\u0026#34;Fred Flintstone\u0026#34;, 35); Flink 的序列化器支持 POJO 类型的模式进化。\nScala 元组和 case class 这些工作就像你期望的那样。\n一个完整的例子 这个例子将一个关于人的记录流作为输入，并将其过滤为只包括成年人。\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.api.common.functions.FilterFunction; public class Example { public static void main(String[] args) throws Exception { final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Person\u0026gt; flintstones = env.fromElements( new Person(\u0026#34;Fred\u0026#34;, 35), new Person(\u0026#34;Wilma\u0026#34;, 35), new Person(\u0026#34;Pebbles\u0026#34;, 2)); DataStream\u0026lt;Person\u0026gt; adults = flintstones.filter(new FilterFunction\u0026lt;Person\u0026gt;() { @Override public boolean filter(Person person) throws Exception { return person.age \u0026gt;= 18; } }); adults.print(); env.execute(); } public static class Person { public String name; public Integer age; public Person() {}; public Person(String name, Integer age) { this.name = name; this.age = age; }; public String toString() { return this.name.toString() + \u0026#34;: age \u0026#34; + this.age.toString(); }; } } 流执行环境 每个 Flink 应用都需要一个执行环境，本例中的 env。流式应用需要使用一个 StreamExecutionEnvironment。\n在你的应用程序中进行的 DataStream API 调用建立了一个作业图(job graph)，这个作业图被附加到 StreamExecutionEnvironment 上。当调用 env.execute() 时，这个图会被打包并发送给 JobManager，JobManager 将作业并行化，并将它的片断分配给 Task Manager 执行。你的作业的每个并行片断将在一个任务槽(task slot)中执行。\n注意，如果你不调用 execute()，你的应用程序将不会被运行。\n这种分布式运行时取决于你的应用程序是可序列化的。它还要求所有的依赖关系都能在集群中的每个节点上使用。\n基本的流源 上面的例子使用 env.fromElements(...) 构造了一个 DataStream[Person]。这是一种方便的方法，可以将一个简单的流组合起来，用于原型或测试。StreamExecutionEnvironment 上还有一个 fromCollection(Collection) 方法。所以，你可以用这个方法来代替。\nval people: List[Person] = new ArrayList\u0026lt;Person\u0026gt;(); people.add(new Person(\u0026#34;Fred\u0026#34;, 35)); people.add(new Person(\u0026#34;Wilma\u0026#34;, 35)); people.add(new Person(\u0026#34;Pebbles\u0026#34;, 2)); val flintstones: DataStream[Person] = env.fromCollection(people); 另一种方便的方法是在原型开发时将一些数据导入流中，使用 socket:\nval lines: DataStream[String] = env.socketTextStream(\u0026#34;localhost\u0026#34;, 9999) 或从文件中读取:\nval lines: DataStream[String] = env.readTextFile(\u0026#34;file:///path\u0026#34;); 在实际应用中，最常用的数据源是那些支持低延迟、高吞吐量并行读取并结合倒带和重放的数据源\u0026ndash;这是高性能和容错的先决条件\u0026ndash;如 Apache Kafka、Kinesis 和各种文件系统。REST API 和数据库也经常被用于流的丰富。\n基本的流式接收器 上面的例子使用 adults.print() 将其结果打印到 task manager 的日志中（当在 IDE 中运行时，它将出现在你的 IDE 的控制台中）。这将在流的每个元素上调用 toString()。\n输出结果看起来像这样：\n1\u0026gt; Fred: age 35 2\u0026gt; Wilma: age 35 其中 1\u0026gt; 和 2\u0026gt; 表示哪个子任务（即线程）产生的输出。\n在生产中，常用的接收器括 StreamingFileSink、各种数据库和一些 pub-sub 系统。\n调试 在生产中，你的应用程序将在远程集群或一组容器中运行。而如果它失败了，它将会远程失败。JobManager 和 TaskManager 日志对调试此类故障非常有帮助，但在 IDE 内部进行本地调试要容易得多，Flink 支持这一点。你可以设置断点，检查本地变量，并逐步检查你的代码。你也可以步入 Flink 的代码，如果你好奇 Flink 是如何工作的，这可以是一个很好的方式来了解它的内部结构。\n实践 在这一点上，你知道了足够的知识，可以开始编码和运行一个简单的 DataStream 应用程序。克隆 flink-training repo，按照 README 中的说明操作后，进行第一个练习。过滤一个流（Ride Cleansing）。\n进一步阅读  Flink序列化调优第一卷：选择你的序列化器\u0026ndash;如果你可以的话 Flink 程序的解剖 数据源 数据接收器 DataStream 连接器  ","permalink":"https://ohmyweekly.github.io/notes/2020-08-19-intro-to-the-datastream-api/","tags":["Flink","Training"],"title":"DataStream API 介绍"},{"categories":["programming"],"contents":"Process Functions 介绍 ProcessFunction 将事件处理与定时器和状态结合起来，使其成为流处理应用的强大构件。这是用 Flink 创建事件驱动应用的基础。它与 RichFlatMapFunction 非常相似，但增加了定时器。\n实例 如果你做过流分析培训中的实战练习，你会记得它使用 TumblingEventTimeWindow 来计算每个司机在每个小时内的小费之和，就像这样:\n// compute the sum of the tips per hour for each driver DataStream\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; hourlyTips = fares .keyBy((TaxiFare fare) -\u0026gt; fare.driverId) .window(TumblingEventTimeWindows.of(Time.hours(1))) .process(new AddTips()); 用 KeyedProcessFunction 做同样的事情是相当直接的，也是很有教育意义的。让我们先把上面的代码替换成这样:\n// compute the sum of the tips per hour for each driver DataStream\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; hourlyTips = fares .keyBy((TaxiFare fare) -\u0026gt; fare.driverId) .process(new PseudoWindow(Time.hours(1))); 在这段代码中，一个名为 PseudoWindow 的 KeyedProcessFunction 被应用于一个 keyed 流，其结果是一个 DataStream\u0026lt;Tuple3\u0026lt;Long，Long，Float\u0026gt;\u0026gt;（就是使用 Flink 内置时间窗口的实现所产生的那种流）。\nPseudoWindow 的整体轮廓是这样的形状:\n// Compute the sum of the tips for each driver in hour-long windows. // The keys are driverIds. public static class PseudoWindow extends KeyedProcessFunction\u0026lt;Long, TaxiFare, Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; { private final long durationMsec; public PseudoWindow(Time duration) { this.durationMsec = duration.toMilliseconds(); } @Override // Called once during initialization.  public void open(Configuration conf) { . . . } @Override // Called as each fare arrives to be processed.  public void processElement( TaxiFare fare, Context ctx, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { . . . } @Override // Called when the current watermark indicates that a window is now complete.  public void onTimer(long timestamp, OnTimerContext context, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { . . . } } 需要注意的事情。\n  ProcessFunctions 有好几种类型\u0026ndash;这是一个 KeyedProcessFunctions，但还有 CoProcessFunctions、BroadcastProcessFunctions 等。\n  KeyedProcessFunction 是 RichFunction的一种。作为一个 RichFunction，它可以访问在管理 keyed state 下工作所需的 open 和 getRuntimeContext 方法。\n  有两个回调要实现：processElement 和 onTimer。processElement 在每次传入事件时被调用；onTimer 在定时器发射时被调用。这些定时器可以是事件时间，也可以是处理时间定时器。processElement 和 onTimer 都提供了一个上下文对象，该对象可以用来与 TimerService 交互（除其他外）。两个回调都还传递了一个可以用来发出结果的 Collector。\n  open() 方法 // Keyed, managed state, with an entry for each window, keyed by the window\u0026#39;s end time. // There is a separate MapState object for each driver. private transient MapState\u0026lt;Long, Float\u0026gt; sumOfTips; @Override public void open(Configuration conf) { MapStateDescriptor\u0026lt;Long, Float\u0026gt; sumDesc = new MapStateDescriptor\u0026lt;\u0026gt;(\u0026#34;sumOfTips\u0026#34;, Long.class, Float.class); sumOfTips = getRuntimeContext().getMapState(sumDesc); } 由于票价事件可能会不按顺序到达，所以有时需要处理一个小时的事件，然后再完成前一个小时的结果计算。事实上，如果水印延迟比窗口长度长得多，那么可能会同时打开许多窗口，而不是只有两个。本实现通过使用 MapState 来支持这一点，MapState 将每个窗口结束的时间戳映射到该窗口的提示之和。\nprocessElement() 方法 public void processElement( TaxiFare fare, Context ctx, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { long eventTime = fare.getEventTime(); TimerService timerService = ctx.timerService(); if (eventTime \u0026lt;= timerService.currentWatermark()) { // This event is late; its window has already been triggered.  } else { // Round up eventTime to the end of the window containing this event.  long endOfWindow = (eventTime - (eventTime % durationMsec) + durationMsec - 1); // Schedule a callback for when the window has been completed.  timerService.registerEventTimeTimer(endOfWindow); // Add this fare\u0026#39;s tip to the running total for that window.  Float sum = sumOfTips.get(endOfWindow); if (sum == null) { sum = 0.0F; } sum += fare.tip; sumOfTips.put(endOfWindow, sum); } } 要考虑的事情:\n  迟到的事件会怎样？在水印后面的事件（即迟到）会被丢弃。如果你想做一些比这更好的事情，可以考虑使用侧输出，这将在下一节解释。\n  这个例子使用了一个 MapState，其中键是时间戳，并为同一个时间戳设置一个 Timer。这是一种常见的模式；它使得在定时器发射时查找相关信息变得简单而高效。\n  onTimer() 方法 public void onTimer( long timestamp, OnTimerContext context, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { long driverId = context.getCurrentKey(); // Look up the result for the hour that just ended.  Float sumOfTips = this.sumOfTips.get(timestamp); Tuple3\u0026lt;Long, Long, Float\u0026gt; result = Tuple3.of(driverId, timestamp, sumOfTips); out.collect(result); this.sumOfTips.remove(timestamp); } 观察:\n  传递给 onTimer 的 OnTimerContext 上下文可以用来确定当前的键。\n  我们的伪窗口是在当前水印到达每个小时结束时被触发的，此时调用 onTimer。这个 onTimer 方法从 sumOfTips 中删除了相关的条目，这样做的效果是无法容纳迟到的事件。这相当于在使用 Flink 的时间窗口时，将 allowLateness 设置为零。\n  性能方面的考虑 Flink 提供了针对 RocksDB 优化的 MapState 和 ListState 类型。在可能的情况下，应该使用这些类型来代替持有某种集合的 ValueState 对象。RocksDB 状态后端可以追加到 ListState，而不需要经过(去)序列化，对于 MapState，每个键/值对都是一个单独的 RocksDB 对象，因此 MapState 可以有效地被访问和更新。\n侧输出 介绍 有几个很好的理由可以让 Flink operator 有一个以上的输出流，比如报告:\n 异常 畸形事件 迟到事件 操作警报，如与外部服务的连接超时。  侧输出是一种方便的方式。除了错误报告，侧输出也是实现流的多路分割的好方法。\n例子 现在，您可以对上一节中被忽略的迟到事件做些什么了。\n一个侧输出通道与一个 OutputTag\u0026lt;T\u0026gt; 相关联。这些标签具有与侧输出的 DataStream 的类型相对应的通用类型，它们有名称。\nprivate static final OutputTag\u0026lt;TaxiFare\u0026gt; lateFares = new OutputTag\u0026lt;TaxiFare\u0026gt;(\u0026#34;lateFares\u0026#34;) {}; 上面展示的是一个静态的 OutputTag\u0026lt;TaxiFare\u0026gt;，它既可以在 PseudoWindow 的 processElement 方法中发出迟到事件时被引用。\nif (eventTime \u0026lt;= timerService.currentWatermark()) { // This event is late; its window has already been triggered.  ctx.output(lateFares, fare); } else { . . . } 并在访问这一侧输出的流时，在作业的 main 方法中输出:\n// compute the sum of the tips per hour for each driver SingleOutputStreamOperator hourlyTips = fares .keyBy((TaxiFare fare) -\u0026gt; fare.driverId) .process(new PseudoWindow(Time.hours(1))); hourlyTips.getSideOutput(lateFares).print(); 或者，您可以使用两个具有相同名称的 OutputTags 来引用同一侧面输出，但如果您这样做，它们必须具有相同的类型。\n结束语 在这个例子中，你已经看到了如何使用 ProcessFunction 来重新实现一个直接的时间窗口。当然，如果 Flink 内置的窗口 API 满足你的需求，无论如何，请继续使用它。但如果你发现自己在考虑用 Flink 的窗口做一些变形，不要害怕推出自己的窗口。\n此外，ProcessFunction 对于计算分析之外的许多其他用例也很有用。下面的实践练习提供了一个完全不同的例子。\nProcessFunction 的另一个常见用例是用于过期的陈旧状态。如果你回想一下 Rides 和 Fares 练习，其中使用 RichCoFlatMapFunction 来计算一个简单的连接，示例解决方案假设 TaxiRides 和 TaxiFares 是完美匹配的，每个 rideId 是一对一的。如果一个事件丢失了，同一乘车 ID 的其他事件将永远保持在状态。这可以替换为一个 KeyedCoProcessFunction 来实现，并且可以使用一个定时器来检测和清除任何陈旧的状态。\n实践 与本节配套的实战练习是 Long Ride Alerts 练习。\n进一步阅读  ProcessFunction 侧输出  ","permalink":"https://ohmyweekly.github.io/notes/2020-08-19-event-driven-applications/","tags":["Flink","Training"],"title":"事件驱动型应用程序"},{"categories":["programming"],"contents":"本次培训的目标和范围 本培训介绍了 Apache Flink，包括足够的内容让你开始编写可扩展的流式 ETL，分析和事件驱动的应用程序，同时省略了很多（最终重要的）细节。本书的重点是为 Flink 管理状态和时间的 API 提供直接的介绍，希望在掌握了这些基础知识后，你能更好地从更详细的参考文档中获取其余需要了解的内容。每一节末尾的链接将引导你到可以学习更多知识的地方。\n具体来说，您将学习:\n 如何实现流数据处理管道 Flink 如何以及为何管理状态 如何使用事件时间来持续计算准确的分析结果？ 如何在连续流上构建事件驱动的应用程序？ Flink 是如何提供具有精确只读语义的容错、有状态的流处理的？  本培训主要介绍四个关键概念：流数据的连续处理、事件时间、有状态的流处理和状态快照。本页介绍了这些概念。\n注: 伴随本培训的是一套实践练习，它将指导您学习如何使用所介绍的概念。每一节的最后都提供了相关练习的链接。\n流处理 流是数据的天然栖息地。无论是来自网络服务器的事件，还是来自股票交易所的交易，或者是来自工厂车间机器的传感器读数，数据都是作为流的一部分被创建的。但当你分析数据时，你可以围绕有界流或无界流组织处理，而你选择哪种范式会产生深远的影响。\n当你处理一个有边界的数据流时，批处理是工作的范式。在这种操作模式下，你可以选择在产生任何结果之前摄取整个数据集，这意味着，例如，可以对数据进行排序，计算全局统计，或产生一个汇总所有输入的最终报告。\n另一方面，流处理涉及无边界的数据流。至少在概念上，输入可能永远不会结束，因此你不得不在数据到达时持续处理数据。\n在 Flink 中，应用程序由流式数据流组成，这些数据流可以通过用户定义的运算符进行转换。这些数据流形成有向图，从一个或多个源开始，到一个或多个 sink 结束。\n通常，程序中的变换(transformation)和数据流(dataflow)中的运算符(operator)之间存在一对一的对应关系。但有时，一个变换可能由多个运算符(operator)组成。\n一个应用程序可能会消耗来自流式源的实时数据，如消息队列或分布式日志，如 Apache Kafka 或 Kinesis。但 Flink 也可以消耗来自各种数据源的有界历史数据。同样，Flink 应用正在产生的结果流也可以被发送到各种各样的系统，这些系统可以作为 sink 连接。\n并行数据流 Flink 中的程序本质上是并行和分布式的。在执行过程中，一个流有一个或多个流分区(stream partitions)，每个运算符(operator)有一个或多个运算符子任务(operator subtasks)。运算符子任务(operator subtasks)相互独立，在不同的线程中执行，也可能在不同的机器或容器上执行。\n运算符符子任务(operator subtasks)的数量就是该特定运算符(operator)的并行度(parallelism)。同一程序的不同运算符可能具有不同的并行度水平。\n流可以在两个运算符之间以一对一（或转发）的模式或以重分发的模式传输数据。\n  一对一的流（例如上图中 Source 和 map() 运算符之间）保留了元素的分区和排序。这意味着 map() 运算符的 subtask[1] 将看到与 Source 运算符的 subtask[1] 所产生的元素顺序相同的元素。\n  重新分发流（如上面 map() 和 keyBy/window 之间，以及 keyBy/window 和 Sink 之间）会改变流的分区。每个运算符子任务(operator subtask)都会根据所选的转换将数据发送到不同的目标子任务。例如 keyBy()（通过散列键来重新分区）、broadcast() 或 rebalance()（随机重新分区）。在重分发交换中，元素之间的排序只在每一对发送和接收子任务中被保留（例如，map() 的 subtask[1] 和 keyBy/window 的 subtask[2]）。因此，例如，上面显示的 keyBy/window 和 Sink 运算符之间的重新分发，引入了关于不同键的聚合结果到达 Sink 的顺序的非确定性。\n  及时的流处理 对于大多数流式应用来说，能够用处理实时数据的相同代码重新处理历史数据是非常有价值的\u0026ndash;无论如何，都能产生确定性的、一致的结果。\n此外，关注事件发生的顺序，而不是事件交付处理的顺序，并且能够推理出一组事件何时（或应该）完成也是至关重要的。例如，考虑电子商务交易，或金融贸易中涉及的一系列事件。\n通过使用记录在数据流中的事件时间戳，而不是使用处理数据的机器的时钟，可以满足这些及时流处理的要求。\n有状态的流处理 Flink 的操作可以是有状态的。这意味着一个事件的处理方式可以取决于之前所有事件的累积效果。状态可以用于一些简单的事情，例如计算每分钟的事件以显示在仪表板上，或者用于一些更复杂的事情，例如计算欺诈检测模型的功能。\n一个 Flink 应用是在分布式集群上并行运行的。一个给定的运算符的各种并行实例将以不同的线程独立执行，一般来说，它们将在不同的机器上运行。\n一个有状态运算符的并行实例集实际上是一个分片的键值存储。每一个并行实例负责处理一组特定键的事件，这些键的状态被保存在本地。\n下图显示了一个作业(Job)，在作业图(job graph)中的前三个运算符上运行的并行度为2，终止于一个并行度为 1 的 sink。第三个运算符是有状态的，你可以看到在第二个和第三个运算符之间发生了一个完全连接的网络洗牌。这是在通过一些键来对流进行分区，这样所有需要一起处理的事件，都会被一起处理。\n状态总是在本地访问，这有助于 Flink 应用实现高吞吐量和低延迟。你可以选择将状态保存在 JVM 堆上，如果状态太大，也可以将其保存在有效组织的磁盘数据结构中。\n通过状态快照进行容错 Flink 能够通过状态快照和流重放的组合，提供容错、精确的一次性语义。这些快照捕获了分布式管道的整个状态，记录了进入输入队列的偏移以及整个作业图(job graph)中因摄取了该点数据而产生的状态。当发生故障时，源会被重放，状态被恢复，并恢复处理。如上所述，这些状态快照是异步捕获的，不会妨碍正在进行的处理。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/learn-flink/\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-19-learn-flink-hands-on-training/","tags":["Flink","Training"],"title":"学习 Flink: 实践培训"},{"categories":["programming"],"contents":"对于 Apache Flink 来说，一个非常常见的用例是实现 ETL（提取、转换、加载）管道，从一个或多个源中获取数据，进行一些转换和/或丰富，然后将结果存储在某个地方。在这一节中，我们将看看如何使用 Flink 的 DataStream API 来实现这种应用。\n请注意，Flink的 Table 和 SQL API很适合许多 ETL 用例。但无论你最终是否直接使用 DataStream API，对这里介绍的基础知识有一个扎实的理解都是有价值的。\n无状态转换 本节介绍了 map() 和 flatmap()，它们是用来实现无状态转换的基本操作。本节中的例子假设你熟悉 flink-training 仓库中的实战练习中使用的出租车乘车数据。\nmap() 在第一个练习中，你过滤了一个打车事件的流，在同一个代码库中，有一个 GeoUtils 类，它提供了一个静态方法 GeoUtils.mapToGridCell(float lon, float lat)，该方法将一个 location (longitude, latitude) 映射到一个网格单元，该单元指的是一个大约100x100米大小的区域。\n现在让我们通过为每个事件添加 startCell 和 endCell 字段来丰富我们的打车对象流。你可以创建一个 EnrichedRide 对象，扩展 TaxiRide，添加这些字段。\npublic static class EnrichedRide extends TaxiRide { public int startCell; public int endCell; public EnrichedRide() {} public EnrichedRide(TaxiRide ride) { this.rideId = ride.rideId; this.isStart = ride.isStart; ... this.startCell = GeoUtils.mapToGridCell(ride.startLon, ride.startLat); this.endCell = GeoUtils.mapToGridCell(ride.endLon, ride.endLat); } public String toString() { return super.toString() + \u0026#34;,\u0026#34; + Integer.toString(this.startCell) + \u0026#34;,\u0026#34; + Integer.toString(this.endCell); } } 然后，您可以创建一个应用程序，将流转化为:\nval rides: DataStream[TaxiRide] = env.addSource(new TaxiRideSource(...)); val enrichedNYCRides: DataStream[EnrichedRide] = rides .filter(new RideCleansingSolution.NYCFilter()) .map(new Enrichment()); enrichedNYCRides.print(); 使用这个 MapFunction:\nclass Enrichment extends MapFunction[TaxiRide, EnrichedRide] { override def map(taxiRide: TaxiRide) { return new EnrichedRide(taxiRide); } } flatmap() MapFunction 只适用于执行一对一的转换：对于每一个进入的流元素，map() 将发出一个转换后的元素。否则，你将需要使用 flatmap():\nval rides: DataStream[TaxiRide] = env.addSource(new TaxiRideSource(...)); val enrichedNYCRides: DataStream[EnrichedRide] = rides .flatMap(new NYCEnrichment()); enrichedNYCRides.print(); 加上一个 FlatMapFunction:\nclass NYCEnrichment extends FlatMapFunction[TaxiRide, EnrichedRide] { override def flatMap(taxiRide: TaxiRide, out: Collector[EnrichedRide]) { val valid: FilterFunction[TaxiRide] = new RideCleansing.NYCFilter(); if (valid.filter(taxiRide)) { out.collect(new EnrichedRide(taxiRide)); } } } 通过这个接口提供的 Collector，flatmap() 方法可以随心所欲地发射许多流元素，包括完全不发射元素。\nKeyed Streams keyBy() 通常，能够围绕一个属性对一个流进行分区是非常有用的，这样所有具有相同属性值的事件就会被归为一组。例如，假设你想找到从每个网格单元开始的最长的出租车乘车时间。从 SQL 查询的角度考虑，这意味着要对 startCell 进行某种 GROUP BY，而在 Flink 中，这是用 keyBy(KeySelector) 来完成的。\nrides .flatMap(new NYCEnrichment()) .keyBy(\u0026#34;startCell\u0026#34;) 每一个 keyBy 都会引起一次网络洗牌，对流进行重新分区。一般来说，这是很昂贵的，因为它涉及到网络通信以及序列化和反序列化。\n在上面的例子中，键是由一个字段名 \u0026ldquo;startCell\u0026rdquo; 指定的。这种键选择的风格有一个缺点，那就是编译器无法推断用于键选择的字段的类型，因此 Flink 会将键值作为元组传递，这可能会很笨拙。最好是使用一个正确类型的 KeySelector，例如:\nrides .flatMap(new NYCEnrichment()) .keyBy( new KeySelector\u0026lt;EnrichedRide, int\u0026gt;() { @Override public int getKey(EnrichedRide enrichedRide) throws Exception { return enrichedRide.startCell; } }) 可以用 lambda 更简洁地表达出来。\nrides .flatMap(new NYCEnrichment()) .keyBy(enrichedRide -\u0026gt; enrichedRide.startCell) Keys are computed KeySelectors 并不局限于从你的事件中提取一个键，相反，它们可以用任何你想要的方式来计算键，只要产生的键是确定性的，并且有有效的 hashCode() 和 equals() 的实现。这个限制排除了生成随机数，或者返回数组或枚举的 KeySelectors，但是你可以使用元组或 POJOs 来生成复合键，例如，只要它们的元素遵循这些相同的规则。\n键必须以确定性的方式产生，因为每当需要它们时，它们就会被重新计算，而不是附加到流记录上。\n例如，我们不是创建一个新的 EnrichedRide 类，该类有一个 startCell 字段，然后我们将其用作键:\nkeyBy(enrichedRide -\u0026gt; enrichedRide.startCell) 相反, 我们可以这样做:\nkeyBy(ride -\u0026gt; GeoUtils.mapToGridCell(ride.startLon, ride.startLat)) Keyed 流的聚合 这段代码为每个 end-of-ride 事件创建一个新的元组流，其中包含 startCell 和持续时间（分钟）。\nimport org.joda.time.Interval; DataStream\u0026lt;Tuple2\u0026lt;Integer, Minutes\u0026gt;\u0026gt; minutesByStartCell = enrichedNYCRides .flatMap(new FlatMapFunction\u0026lt;EnrichedRide, Tuple2\u0026lt;Integer, Minutes\u0026gt;\u0026gt;() { @Override public void flatMap(EnrichedRide ride, Collector\u0026lt;Tuple2\u0026lt;Integer, Minutes\u0026gt;\u0026gt; out) throws Exception { if (!ride.isStart) { Interval rideInterval = new Interval(ride.startTime, ride.endTime); Minutes duration = rideInterval.toDuration().toStandardMinutes(); out.collect(new Tuple2\u0026lt;\u0026gt;(ride.startCell, duration)); } } }); 现在可以产生一个流，其中只包含那些对每个 startCell 来说是有史以来（至此）最长的乘车记录。\n有多种方式可以表达作为键的字段。之前你看到了一个 EnrichedRide POJO 的例子，在这个例子中，要用作键的字段是用它的名字指定的。这个例子涉及到 Tuple2 对象，元组中的索引（从0开始）被用来指定键。\nminutesByStartCell .keyBy(0) // startCell  .maxBy(1) // duration  .print(); 现在，每当持续时间达到一个新的最大值时，输出流就会包含一个针对每个键的记录\u0026ndash;如这里的50797单元格所示。\n... 4\u0026gt; (64549,5M) 4\u0026gt; (46298,18M) 1\u0026gt; (51549,14M) 1\u0026gt; (53043,13M) 1\u0026gt; (56031,22M) 1\u0026gt; (50797,6M) ... 1\u0026gt; (50797,8M) ... 1\u0026gt; (50797,11M) ... 1\u0026gt; (50797,12M) (Implicit) State 这是本次训练中第一个涉及有状态流的例子。虽然状态被透明地处理，但 Flink 必须跟踪每个不同键的最大持续时间。\n每当状态涉及到你的应用时，你应该考虑状态可能会变得多大。每当键空间是无限制的，那么 Flink 需要的状态量也是无限制的。\n当处理流时，一般来说，在有限的窗口上考虑聚合比在整个流上考虑更有意义。\nreduce() 和其他聚合器 上文中使用的 maxBy() 只是 Flink 的 KeyedStreams 上众多聚合函数中的一个例子。还有一个更通用的 reduce() 函数，你可以用它来实现自己的自定义聚合。\n状态转换 为什么 Flink 要参与管理状态？ 你的应用程序当然能够在没有让 Flink 参与管理状态的情况下使用状态\u0026ndash;但 Flink 为它所管理的状态提供了一些引人注目的功能。\n 本地化。Flink 状态被保存在处理它的机器的本地，并且可以以内存速度被访问。 耐用。Flink 状态是容错的，即每隔一段时间就会自动检查一次，一旦失败就会恢复。 纵向可扩展。Flink 状态可以保存在嵌入式 RocksDB 实例中，通过增加更多的本地磁盘来扩展。 横向可扩展。随着集群的增长和收缩，Flink 状态会被重新分配。 可查询。Flink 状态可以通过可查询状态 API 进行外部查询。  在本节中，您将学习如何使用 Flink 的 API 管理 keyed 状态。\nRich 函数 此时你已经看到了 Flink 的几个函数接口，包括 FilterFunction、MapFunction 和 FlatMapFunction。这些都是单一抽象方法模式的例子。\n对于每一个接口，Flink 还提供了一个所谓的\u0026quot;富\u0026quot;变体，例如，RichFlatMapFunction，它有一些额外的方法，包括:\n open(Configuration c) close() getRuntimeContext()  open() 在操作符初始化期间被调用一次。这是一个加载一些静态数据的机会，或者, 例如打开一个外部服务的连接。\ngetRuntimeContext() 提供了对一整套潜在的有趣的东西的访问，但最值得注意的是它是如何创建和访问由 Flink 管理的状态。\n一个带有 Keyed State 的例子 在这个例子中，想象一下，你有一个事件流，你想去掉重复，所以你只保留每个键的第一个事件。这里有一个应用程序可以做到这一点，使用一个名为 Deduplicator 的 RichFlatMapFunction:\nprivate static class Event { public final String key; public final long timestamp; ... } public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.addSource(new EventSource()) .keyBy(e -\u0026gt; e.key) .flatMap(new Deduplicator()) .print(); env.execute(); } 为了达到这个目的，Deduplicator 将需要以某种方式记住，对于每个键来说，是否已经有了该键的事件。它将使用 Flink 的 keyed state 接口来做到这一点。\n当你在使用像这样的 keyed 流时，Flink 将为每个被管理的状态项目维护一个键/值存储。\nFlink 支持几种不同类型的 keyed state，本例使用的是最简单的一种，即 ValueState。这意味着对于每个键，Flink 将存储一个单一的对象\u0026ndash;在本例中，一个类型为 Boolean 的对象。\n我们的 Deduplicator 类有两个方法：open() 和 flatMap()。open 方法通过定义一个 ValueStateDescriptor` 来建立对托管状态的使用。构造函数的参数为这个 keyed state 项指定了一个名称（\u0026ldquo;keyHasBeenSeen\u0026rdquo;），并提供了可用于序列化这些对象的信息（在本例中，Types.BOOLEAN）。\npublic static class Deduplicator extends RichFlatMapFunction\u0026lt;Event, Event\u0026gt; { ValueState\u0026lt;Boolean\u0026gt; keyHasBeenSeen; @Override public void open(Configuration conf) { ValueStateDescriptor\u0026lt;Boolean\u0026gt; desc = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;keyHasBeenSeen\u0026#34;, Types.BOOLEAN); keyHasBeenSeen = getRuntimeContext().getState(desc); } @Override public void flatMap(Event event, Collector\u0026lt;Event\u0026gt; out) throws Exception { if (keyHasBeenSeen.value() == null) { out.collect(event); keyHasBeenSeen.update(true); } } } 当 flatMap 方法调用 keyHasBeenSeen.value() 时，Flink 的运行时会在上下文中查找 key 的这块状态值，只有当它为 null 时，它才会去收集事件到输出。在这种情况下，它还会将 keyHasBeenSeen 更新为 true。\n这种访问和更新 key-partitioned 状态的机制可能看起来相当神奇，因为在我们的 Deduplicator 的实现中，key 并不是显式可见的。当 Flink 的运行时调用我们的 RichFlatMapFunction 的 open 方法时，没有任何事件，因此那一刻上下文中没有 key。但是当它调用 flatMap 方法时，被处理的事件的 key 对运行时来说是可用的，并在幕后用于确定 Flink 的状态后端中的哪个条目被操作。\n当部署到分布式集群时，会有很多这个 Deduplicator 的实例，每个实例将负责整个键空间的一个不相干子集。因此，当你看到一个 ValueState 的单项，如:\nValueState\u0026lt;Boolean\u0026gt; keyHasBeenSeen; 理解这不仅仅是一个单一的布尔值，而是一个分布式的、分片式的、键/值存储。\n清除状态 上面的例子有一个潜在的问题。如果键的空间是无限制的，会发生什么？Flink 是在某个地方为每一个被使用的不同键存储一个布尔的实例。如果有一个有界的键集，那么这将是很好的，但是在键集以无界的方式增长的应用中，有必要为不再需要的键清除状态。这是通过调用状态对象上的 clear() 来实现的，如:\nkeyHasBeenSeen.clear() 例如，你可能想在给定键的一段时间不活动后这样做。当你在事件驱动的应用程序一节中学习 ProcessFunction 时，你将看到如何使用 Timer 来实现这一点。\n此外，还有一个状态存活时间(TTL)选项，你可以用状态描述符来配置，指定什么时候自动清除陈旧键的状态。\nNon-keyed State 也可以在 non-keyed 的上下文中使用托管状态。这有时被称为 operator state。所涉及的接口有些不同，由于用户定义的函数需要 non-keyed state 是不常见的，所以这里不做介绍。这个功能最常用于源和接收器(sink)的实现。\nConnected Streams 有时不是应用这样的预定义变换:\n你希望能够动态地改变变换的某些方面\u0026ndash;通过流的阈值，或规则，或其他参数。Flink 中支持这种模式的是一种叫做连接流(connected streams)的东西，其中一个 operator 有两个输入流，就像这样:\n连接流也可以用来实现流式连接(streaming joins.)。\n例子 在这个例子中，控制流被用来指定必须从 streamOfWords 中过滤掉的单词。一个名为 ControlFunction 的 RichCoFlatMapFunction 被应用到连接的流中来完成这个任务。\npublic static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; control = env.fromElements(\u0026#34;DROP\u0026#34;, \u0026#34;IGNORE\u0026#34;).keyBy(x -\u0026gt; x); DataStream\u0026lt;String\u0026gt; streamOfWords = env.fromElements(\u0026#34;Apache\u0026#34;, \u0026#34;DROP\u0026#34;, \u0026#34;Flink\u0026#34;, \u0026#34;IGNORE\u0026#34;).keyBy(x -\u0026gt; x); control .connect(datastreamOfWords) .flatMap(new ControlFunction()) .print(); env.execute(); } 注意，被连接的两个流必须以兼容的方式进行 keyed。keyBy 的作用是对流的数据进行分区，当 keyed 流连接时，必须以同样的方式进行分区。这样就可以保证两个流中具有相同 key 的事件都会被发送到同一个实例中。那么，这就使得将该键上的两个流连接起来成为可能，例如。\n在这种情况下，两个流的类型都是 DataStream[String]，并且两个流都以字符串为键。如下所示，这个 RichCoFlatMapFunction 在 keyed state 下存储了一个布尔值，而这个布尔值是由两个流共享的。\npublic static class ControlFunction extends RichCoFlatMapFunction\u0026lt;String, String, String\u0026gt; { private ValueState\u0026lt;Boolean\u0026gt; blocked; @Override public void open(Configuration config) { blocked = getRuntimeContext().getState(new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;blocked\u0026#34;, Boolean.class)); } @Override public void flatMap1(String control_value, Collector\u0026lt;String\u0026gt; out) throws Exception { blocked.update(Boolean.TRUE); } @Override public void flatMap2(String data_value, Collector\u0026lt;String\u0026gt; out) throws Exception { if (blocked.value() == null) { out.collect(data_value); } } } RichCoFlatMapFunction 是 FlatMapFunction 的一种，它可以应用于一对连接的流，并且它可以访问富函数接口。这意味着它可以被做成有状态的。\n屏蔽的(blocked)布尔正在被用来记住控制流上提到的键（在这里是单词），这些词被过滤出 streamOfWords 流。这就是 keyed state，它在两个流之间是共享的，这就是为什么两个流要共享同一个键空间。\nflatMap1 和 flatMap2 被 Flink 运行时调用，分别来自两个连接流的元素\u0026ndash;在我们的例子中，来自控制流的元素被传入 flatMap1，来自 streamOfWords 的元素被传入 flatMap2。这是由使用 control.connect(datastreamOfWords) 连接两个流的顺序决定的。\n重要的是要认识到，你无法控制调用 flatMap1 和 flatMap2 回调的顺序。这两个输入流在相互竞争，Flink 运行时将对来自一个流或另一个流的事件的消耗做它想做的事。在时间和/或顺序很重要的情况下，你可能会发现有必要在托管的 Flink 状态下缓冲事件，直到你的应用程序准备好处理它们。(注意：如果你真的很绝望，可以通过使用实现 InputSelectable 接口的自定义 Operator 来对双输入 operator 消耗输入的顺序进行一些有限的控制。)\n实践 与本节配套的实践练习是\u0026ldquo;乘车与票价练习\u0026rdquo;。\n进一步阅读  数据流转换 有状态的流处理  ","permalink":"https://ohmyweekly.github.io/notes/2020-08-19-data-pipelines-and-etl/","tags":["Flink","Training"],"title":"数据管道和 ETL"},{"categories":["programming"],"contents":"Event Time 和 Watermarks 介绍 Flink 明确支持三种不同的时间概念。\n事件时间：事件发生的时间，由产生（或存储）该事件的设备记录的时间\n摄取时间：Flink 在摄取事件时记录的时间戳。\n处理时间：您的管道中的特定 operator 处理事件的时间。\n为了获得可重复的结果，例如，在计算某一天股票在交易的第一个小时内达到的最高价格时，您应该使用事件时间(event time)。这样一来，结果就不会依赖于计算的时间。这种实时应用有时会使用处理时间(processing time)，但这样一来，结果就会由该小时内恰好处理的事件决定，而不是由当时发生的事件决定。基于处理时间的计算分析会导致不一致，并使重新分析历史数据或测试新的实现变得困难。\n使用事件时间 默认情况下，Flink 将使用处理时间(processing time)。要改变这一点，您可以设置时间特性(Time Characteristic)。\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); 如果你想使用事件时间，你还需要提供一个时间戳提取器和水印生成器，Flink 将使用它们来跟踪事件时间的进展。这将在下面的\u0026ldquo;使用水印\u0026rdquo;一节中介绍，但首先我们应该解释一下什么是水印。\n水印 让我们通过一个简单的例子来说明为什么需要水印，以及它们是如何工作的。\n在这个例子中，你有一个带时间戳的事件流，这些事件的到达顺序有些混乱，如下所示。显示的数字是时间戳，表示这些事件实际发生的时间。第一个到达的事件发生在时间 4，随后是更早发生的事件，在时间 2，以此类推。\n··· 23 19 22 24 21 14 17 13 12 15 9 11 7 2 4 → 现在想象一下，你正在尝试创建一个流排序器(stream sorter)。这个应用程序的目的是处理流中的每个事件，并发出一个新的流，其中包含相同的事件，但按时间戳排序。\n一些观察:\n(1)你的流排序器看到的第一个元素是 4， 但你不能马上把它作为排序流的第一个元素释放出来。它可能已经不按顺序到达，而更早的事件可能还没有到达。事实上，你对这个流的未来有一些神一样的知识，你可以看到，你的流排序器至少应该等到 2 到达后再产生任何结果。\n一些缓冲，和一些延迟，是必要的。\n(2)如果你做错了，你可能最终会永远等待。首先，排序器看到了一个来自时间 4 的事件，然后是一个来自时间 2 的事件。一个时间戳小于 2 的事件会不会永远到达？也许会，也许不会。也许不会。你可以永远等待，永远看不到 1。\n最终你必须鼓起勇气，发出 2 作为排序流的开始。\n(3)那么你需要的是某种策略，它定义了对于任何给定的时间戳事件，何时停止等待早期事件的到来。\n这正是水印的作用\u0026ndash;它们定义了何时停止等待早期(earlier)事件。\nFlink 中的事件时间处理依赖于水印生成器，这些水印生成器将特殊的时间戳元素插入到流中，称为水印。时间 t 的水印是一种断言，即到时间 t 为止，流现在（可能）是完整的。\n这个流排序器应该在什么时候停止等待，并推出2开始排序流？当一个时间戳为 2，或更大的水印到达时。\n(4)你可以想象不同的策略来决定如何生成水印。\n每一个事件都是在一些延迟之后到达的，而这些延迟是不同的，所以一些事件的延迟比其他事件更多。一个简单的方法是假设这些延迟被某个最大延迟所约束。Flink 将这种策略称为有界无序水印。很容易想象更复杂的水印方法，但对于大多数应用来说，固定的延迟已经足够好了。\n延迟与完整性 关于水印的另一种思考方式是，水印让你这个流式应用的开发者能够控制延迟和完整性之间的权衡。与批处理不同的是，在批处理中，人们可以在产生任何结果之前完全了解输入，而在流式处理中，你最终必须停止等待看到更多的输入，并产生某种结果。\n你可以积极地配置你的水印，用一个很短的延迟，从而承担在对输入不完全了解的情况下产生结果的风险\u0026ndash;也就是说，一个可能是错误的结果，很快就产生了。或者你可以等待更长时间，并利用对输入流更完整的知识产生结果。\n也可以实现混合解决方案，快速生成初始结果，然后在处理额外（后期）数据时对这些结果进行更新。对于某些应用来说，这是一种很好的方法。\n延迟 迟到的定义是相对于水印而言的。水印(t)声明流在时间t之前是完整的；在这个水印之后的任何事件，如果时间戳 ≤t，则为延迟。\n使用水印 为了执行基于事件时间的事件处理，Flink 需要知道与每个事件相关联的时间，还需要流包含水印。\n实践练习中使用的 Taxi 数据源为你处理了这些细节。但在你自己的应用程序中，你必须自己处理这些事情，通常是通过实现一个类来实现，该类从事件中提取时间戳，并按需生成水印。最简单的方法是使用 WatermarkStrategy:\nDataStream\u0026lt;Event\u0026gt; stream = ... WatermarkStrategy\u0026lt;Event\u0026gt; strategy = WatermarkStrategy .\u0026lt;Event\u0026gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withTimestampAssigner((event, timestamp) -\u0026gt; event.timestamp); DataStream\u0026lt;Event\u0026gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(strategy); 窗口 Flink 具有非常有表现力的窗口语义。\n在本节中，你将学习\n 如何使用窗口来计算无边界流的聚合。 Flink 支持哪些类型的窗口，以及 如何实现一个窗口化聚合的 DataStream 程序？  介绍 在做流处理的时候，自然而然地想要计算流的有界子集的聚合分析，以回答这样的问题。\n 每分钟的页面浏览量 每个用户每周会话数 每个传感器每分钟的最高温度  用 Flink 计算窗口化分析依赖于两个主要的抽象。窗口分配器（Window Assigners）将事件分配给窗口（必要时创建新的窗口对象），窗口函数（Window Functions）应用于分配给窗口的事件。\nFlink 的窗口 API 还有 Triggers 的概念，它决定什么时候调用窗口函数，还有 Evictors，它可以删除窗口中收集的元素。\n在它的基本形式中，你将窗口化应用到像这样的 keyed stream 中。\nstream. .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce|aggregate|process(\u0026lt;window function\u0026gt;) 您也可以对 non-keyed stream 使用窗口化，但请记住，在这种情况下，处理将不会并行进行。\nstream. .windowAll(\u0026lt;window assigner\u0026gt;) .reduce|aggregate|process(\u0026lt;window function\u0026gt;) 窗口分配器 Flink 有几种内置的窗口分配器类型，下面进行说明。\n一些例子说明这些窗口分配器的用途，以及如何指定它们:\n  滚动时间窗口\n  每分钟浏览量\n  TumblingEventTimeWindows.of(Time.minutes(1))\n  滑动时间窗口\n  每10秒计算的每分钟页面浏览量\n  SlidingEventTimeWindows.of(Time.min(1), Time.seconds(10))\n  会话窗口\n  每节课的页面浏览量，其中每节课之间至少有30分钟的间隔。\n  EventTimeSessionWindows.withGap(Time.minutes(30))\n  可以使用 Time.milliseconds(n), Time.seconds(n), Time.minutes(n), Time.hours(n), 和 Time.days(n) 中的一种指定持续时间。\n基于时间的窗口分配器（包括会话窗口）有事件时间(event time)和处理时间(processing time)两种风味。这两种类型的时间窗口之间有显著的权衡。对于处理时间窗口，你必须接受这些限制:\n 不能正确处理历史数据。 不能正确处理失序数据。 结果将是非确定性的。  但具有较低延迟的优势。\n当使用基于计数的窗口时，请记住，这些窗口将不会启动，直到一个批次完成。没有超时和处理部分窗口的选项，尽管你可以用自定义的触发器自己实现这种行为。\n全局窗口分配器将每个事件（用相同的键）分配到同一个全局窗口。只有当你打算使用自定义触发器来做你自己的自定义窗口时，这才是有用的。在许多看似有用的情况下，您最好使用另一节中描述的 ProcessFunction。\n窗口函数 对于如何处理窗口的内容，您有三个基本选项。\n 作为一个批次，使用一个 ProcessWindowFunction，它将被传递一个包含窗口内容的 Iterable。 以增量方式，使用 ReduceFunction 或 AggregateFunction，当每个事件被分配到窗口时被调用。 或两者结合，当窗口被触发时，ReduceFunction 或 AggregateFunction 的预聚集结果被提供给 ProcessWindowFunction。  这里是方法1和3的例子。每个实现都在1分钟的事件时间窗口中从每个传感器中找到峰值值，并产生一个包含(key, end-of-window-timestamp, max_value) 的 Tuples 流。\nProcessWindowFunction 示例 DataStream\u0026lt;SensorReading\u0026gt; input = ... input .keyBy(x -\u0026gt; x.key) .window(TumblingEventTimeWindows.of(Time.minutes(1))) .process(new MyWastefulMax()); public static class MyWastefulMax extends ProcessWindowFunction\u0026lt; SensorReading, // input type  Tuple3\u0026lt;String, Long, Integer\u0026gt;, // output type  String, // key type  TimeWindow\u0026gt; { // window type  @Override public void process( String key, Context context, Iterable\u0026lt;SensorReading\u0026gt; events, Collector\u0026lt;Tuple3\u0026lt;String, Long, Integer\u0026gt;\u0026gt; out) { int max = 0; for (SensorReading event : events) { max = Math.max(event.value, max); } out.collect(Tuple3.of(key, context.window().getEnd(), max)); } } 在这个实现中，有几件事需要注意。\n 所有分配给窗口的事件都必须在 keyed Flink state 下被缓冲，直到窗口被触发。这可能是相当昂贵的。 我们的 ProcessWindowFunction 被传递了一个 Context 对象，其中包含了窗口的信息。它的接口是这样的:  public abstract class Context implements java.io.Serializable { public abstract W window(); public abstract long currentProcessingTime(); public abstract long currentWatermark(); public abstract KeyedStateStore windowState(); public abstract KeyedStateStore globalState(); } windowState 和 globalState 是您可以存储该键的所有窗口的 per-key, per-window, 或全局 per-key 信息的地方。例如，如果您想记录一些关于当前窗口的信息，并在处理后续窗口时使用这些信息，这可能会很有用。\n递增聚合示例 DataStream\u0026lt;SensorReading\u0026gt; input = ... input .keyBy(x -\u0026gt; x.key) .window(TumblingEventTimeWindows.of(Time.minutes(1))) .reduce(new MyReducingMax(), new MyWindowFunction()); private static class MyReducingMax implements ReduceFunction\u0026lt;SensorReading\u0026gt; { public SensorReading reduce(SensorReading r1, SensorReading r2) { return r1.value() \u0026gt; r2.value() ? r1 : r2; } } private static class MyWindowFunction extends ProcessWindowFunction\u0026lt; SensorReading, Tuple3\u0026lt;String, Long, SensorReading\u0026gt;, String, TimeWindow\u0026gt; { @Override public void process( String key, Context context, Iterable\u0026lt;SensorReading\u0026gt; maxReading, Collector\u0026lt;Tuple3\u0026lt;String, Long, SensorReading\u0026gt;\u0026gt; out) { SensorReading max = maxReading.iterator().next(); out.collect(Tuple3.of(key, context.window().getEnd(), max)); } } 请注意，Iterable\u0026lt;SensorReading\u0026gt; 将只包含一个读数\u0026ndash;由 MyReducingMax 计算的 pre-aggregated 最大值。\n迟来的事件 默认情况下，当使用事件时间窗口时，迟到的事件会被丢弃。窗口 API 有两个可选部分可以让您对此有更多的控制。\n您可以使用名为\u0026ldquo;侧输出\u0026rdquo;的机制，安排将被丢弃的事件收集到一个备用的输出流中。下面是一个例子，说明这可能是什么样子的:\nOutputTag\u0026lt;Event\u0026gt; lateTag = new OutputTag\u0026lt;Event\u0026gt;(\u0026#34;late\u0026#34;){}; SingleOutputStreamOperator\u0026lt;Event\u0026gt; result = stream. .keyBy(...) .window(...) .sideOutputLateData(lateTag) .process(...); DataStream\u0026lt;Event\u0026gt; lateStream = result.getSideOutput(lateTag); 您还可以指定允许的延迟时间间隔，在此期间，延迟事件将继续分配给相应的窗口（其状态将被保留）。默认情况下，每个延迟事件都会导致窗口函数再次被调用（有时称为延迟发射）。\n换句话说，水印后面的元素会被丢弃（或发送到侧输出）。\n比如说:\nstream. .keyBy(...) .window(...) .allowedLateness(Time.seconds(10)) .process(...); 当允许的延迟大于零时，只有那些晚到会被丢弃的事件才会被发送到侧输出（如果已经配置了）。\n惊喜 Flink 的 windowing API 的某些方面可能并不像你所期望的那样。基于 flink 用户邮件列表和其他地方的常见问题，这里有一些关于窗口的事实可能会让你感到惊讶。\n滑动窗口会进行复制 滑动窗口分配器可以创建很多窗口对象，并会将每个事件复制到每个相关窗口中。例如，如果你每15分钟有一个长度为24小时的滑动窗口，每个事件将被复制到 4*24=96 个窗口中。\n时间窗口与纪元对齐 仅仅因为你使用了一个小时的处理时间窗口，并且在 12:05 开始运行你的应用程序，并不意味着第一个窗口会在 1:05 关闭。第一个窗口将长达 55 分钟，并在 1:00 关闭。\n但是请注意，滚动窗口和滑动窗口分配器采用一个可选的偏移参数，可以用来改变窗口的对齐方式。详情请参见滚动窗口和滑动窗口。\n窗口可以跟随窗口 例如，这样做是可行的:\nstream .keyBy(t -\u0026gt; t.key) .timeWindow(\u0026lt;time specification\u0026gt;) .reduce(\u0026lt;reduce function\u0026gt;) .timeWindowAll(\u0026lt;same time specification\u0026gt;) .reduce(\u0026lt;same reduce function\u0026gt;) 你可能会期望 Flink 的运行时足够聪明，能够为你做这种并行的预聚合（前提是你使用的是 ReduceFunction 或 AggregateFunction），但事实并非如此。\n之所以这样做的原因是，一个时间窗口产生的事件会根据窗口结束的时间分配时间戳。所以，例如，一个小时长的窗口产生的所有事件都会有标记一个小时结束的时间戳。任何消耗这些事件的后续窗口的持续时间应该与前一个窗口的持续时间相同，或者是其倍数。\n空的时间窗口没有结果 只有当事件被分配到窗口时，才会创建窗口。因此，如果在给定的时间帧内没有事件，就不会报告结果。\n迟来的事件会导致迟来的合并 会话窗口是基于可以合并的窗口的抽象。每个元素最初都被分配到一个新的窗口，之后只要窗口之间的间隙足够小，就会合并。这样一来，一个迟到的事件可以弥合分开两个之前独立的会话的差距，产生迟到的合并。\n实践 与本节配套的实战练习是 Hourly Tips Exercise。\n进一步阅读  及时的流处理 窗口  原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/learn-flink/streaming_analytics.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-19-streaming-analytics/","tags":["Flink","Training"],"title":"流分析"},{"categories":["programming"],"contents":"状态后端 Flink 管理的 keyed state 是一种碎片化的、键/值存储，每项 keyed state 的工作副本都被保存在负责该键的 taskmanager 的本地某处。Operator 的状态也被保存在需要它的机器的本地。Flink 会定期对所有状态进行持久化快照，并将这些快照复制到某个更持久的地方，比如分布式文件系统。\n在发生故障的情况下，Flink 可以恢复你的应用程序的完整状态，并恢复处理，就像什么都没有发生过一样。\nFlink 管理的这种状态被存储在状态后端中。状态后端有两种实现\u0026ndash;一种是基于 RocksDB 的，它是一个嵌入式的键/值存储，将其工作状态保存在磁盘上；另一种是基于堆的状态后端，将其工作状态保存在内存中，在 Java 堆上。这种基于堆的状态后端有两种风味：将其状态快照持久化到分布式文件系统的 FsStateBackend 和使用 JobManager 的堆的 MemoryStateBackend。\n   名称 工作状态 状态备份 快照     RocksDBStateBackend 本地磁盘(tmp dir) 分布式文件系统 完全/增量   支持大于可用内存的状态; 经验法则：比基于堆的后端慢10倍。      FsStateBackend JVM Heap 分布式文件系统 Full   速度快，需要大量堆积; 受制于 GC      MemoryStateBackend JVM Heap JobManager JVM Heap Full   有利于小状态（地方）的测试和实验。       当处理保存在基于堆的状态后端的状态时，访问和更新涉及到在堆上读写对象。但是对于保存在 RocksDBStateBackend 中的对象，访问和更新涉及到序列化和反序列化，因此成本更高。但是使用 RocksDB 可以拥有的状态数量只受限于本地磁盘的大小。还要注意的是，只有 RocksDBStateBackend 能够进行增量快照，这对于有大量缓慢变化的状态的应用来说是一个很大的好处。\n所有这些状态后端都能够进行异步快照，这意味着它们可以在不妨碍正在进行的流处理的情况下进行快照。\n状态快照 定义  快照\u0026ndash;一个通用术语，指的是一个 Flink 作业状态的全局、一致的图像。快照包括进入每个数据源的指针（例如，进入文件或 Kafka 分区的偏移），以及来自每个作业的有状态操作符的状态副本，这些操作符是在处理了所有事件后产生的，直到源中的这些位置。 检查点\u0026ndash;Flink 为了能够从故障中恢复而自动拍摄的快照。检查点可以是增量的，并为快速恢复进行了优化。 外部化检查点\u0026ndash;通常检查点不打算被用户操纵。Flink 只在作业运行时保留n个最近的检查点（n是可配置的），并在作业取消时删除它们。但你也可以配置它们被保留，在这种情况下，你可以手动从它们恢复。 保存点\u0026ndash;由用户（或API调用）手动触发的快照，用于某些操作目的，例如有状态的重新部署/升级/重新缩放操作。保存点始终是完整的，并为操作的灵活性进行了优化。  状态快照是如何工作的？ Flink 使用 Chandy-Lamport 算法的一个变体，称为异步屏障快照。\n当任务管理器被检查点协调器（作业管理器的一部分）指示开始检查点时，它让所有的源记录它们的偏移量，并在它们的流中插入编号的检查点障碍。这些屏障在作业图(job graph)中流动，指示每个检查点前后的流的部分。\n检查点n将包含每个 operator 的状态，这些状态是由于消耗了检查点障碍n之前的每个事件，而没有消耗它之后的任何事件。\n当作业图中的每个 operator 接收到这些障碍之一时，它就会记录其状态。具有两个输入流（如 CoProcessFunction）的 operator 执行屏障对齐，这样快照将反映消耗两个输入流的事件所产生的状态，直到（但不超过）两个屏障。\nFlink 的状态后端使用复制-写机制，允许在异步快照状态的旧版本时，流处理不受阻碍地继续。只有当快照被持久化后，这些旧版本的状态才会被垃圾回收。\n一次性保证 当流处理应用中出现问题时，有可能出现丢失，或者重复的结果。在 Flink 中，根据你对应用的选择和你运行它的集群，这些结果中的任何一种都是可能的。\n Flink 不努力从故障中恢复（最多一次）。 没有任何损失，但您可能会遇到重复的结果（至少一次）。 没有任何东西丢失或重复（精确地一次）。  鉴于 Flink 通过倒带和重放源数据流从故障中恢复，当理想情况被描述为精确一次时，这并不意味着每个事件都将被精确处理一次。相反，它意味着每一个事件都会对 Flink 所管理的状态产生一次确切的影响。\nBarrier 对齐只需要用于提供精确的一次保证。如果你不需要这个，你可以通过配置 Flink 使用 CheckpointingMode.AT_LEAST_ONCE 来获得一些性能，它的效果是禁用屏障对齐。\n精确一次, 端到端 为了实现端到端精确的一次，让源的每个事件精确地影响汇，以下几点必须是真的。\n 你的源必须是可重播的，并且 你的接收器必须是事务性的(或幂等的)  实践 Flink Operations Playground 包括观察故障和恢复的部分。\n进一步阅读  有状态的流处理 状态后端 数据源和接收器的容错保证 启用和配置检查点 检查点 保存点 调整检查点和大状态 监测检查点 任务故障恢复  ","permalink":"https://ohmyweekly.github.io/notes/2020-08-19-fault-tolerance/","tags":["Flink","Training"],"title":"通过状态快照进行容错"},{"categories":["programming"],"contents":"Flink 操作游乐场 在各种环境中部署和操作 Apache Flink 的方法有很多。无论这种多样性如何，Flink 集群的基本构件保持不变，类似的操作原则也适用。\n在这个游乐场上，你将学习如何管理和运行 Flink Jobs。你将看到如何部署和监控应用程序，体验 Flink 如何从 Job 故障中恢复，并执行日常操作任务，如升级和重新缩放。\n这个游乐场的解剖 这个游乐场由一个持久的 Flink Session Cluster 和一个 Kafka Cluster 组成。\n一个 Flink Cluster 总是由一个 JobManager 和一个或多个 Flink TaskManager 组成。JobManager 负责处理 Job 提交，监督 Job 以及资源管理。Flink TaskManager 是 worker 进程，负责执行构成 Flink Job 的实际任务。在这个游戏场中，你将从单个 TaskManager 开始，但以后会扩展到更多的 TaskManager。此外，这个游乐场还带有一个专门的客户端容器，我们使用它来提交 Flink Job，并在以后执行各种操作任务。客户端容器不是 Flink Cluster 本身需要的，只是为了方便使用才包含在里面。\nKafka 集群由一个 Zookeeper 服务器和一个 Kafka Broker 组成。\n当游乐场启动时，一个名为 Flink Event Count 的 Flink Job 将被提交给 JobManager。此外，还会创建两个 Kafka 主题 input 和 output。\n该作业从 input 主题中消耗点击事件(ClickEvent)，每个点击事件(ClickEvent)都有一个时间戳(timestamp)和一个页面(page)。然后按页面对事件进行分组(keyed by)，并在 15 秒的窗口中进行计数。结果被写入 output 主题。\n有6个不同的页面，我们在每个页面和15秒内产生1000个点击事件。因此，Flink 作业的输出应该显示每个页面和窗口有1000个浏览量。\n启动游乐场 游戏场环境的设置只需几步。我们将引导你完成必要的命令，并展示如何验证一切都在正确运行。\n我们假设你的机器上安装了 Docker（1.12+）和 docker-compose（2.1+）。\n所需的配置文件可以在 flink-playgrounds 仓库中找到。检查一下，然后对齐环境。\ngit clone --branch release-1.11 https://github.com/apache/flink-playgrounds.git cd flink-playgrounds/operations-playground docker-compose build docker-compose up -d 之后，你可以用以下命令检查正在运行的 Docker 容器。\ndocker-compose ps Name Command State Ports ----------------------------------------------------------------------------------------------------------------------------- operations-playground_clickevent-generator_1 /docker-entrypoint.sh java ... Up 6123/tcp, 8081/tcp operations-playground_client_1 /docker-entrypoint.sh flin ... Exit 0 operations-playground_jobmanager_1 /docker-entrypoint.sh jobm ... Up 6123/tcp, 0.0.0.0:8081-\u0026gt;8081/tcp operations-playground_kafka_1 start-kafka.sh Up 0.0.0.0:9094-\u0026gt;9094/tcp operations-playground_taskmanager_1 /docker-entrypoint.sh task ... Up 6123/tcp, 8081/tcp operations-playground_zookeeper_1 /bin/sh -c /usr/sbin/sshd ... Up 2181/tcp, 22/tcp, 2888/tcp, 3888/tcp 这表明客户端容器已经成功提交了 Flink Job（Exit 0），所有集群组件以及数据生成器都在运行（Up）。\n你可以通过调用来停止游乐场环境。\ndocker-compose down -v 进入游乐场 在这个游乐场中，有很多东西你可以尝试和检查。在下面的两节中，我们将向你展示如何与 Flink 集群进行交互，并展示 Flink 的一些关键功能。\nFlink WebUI 观察你的 Flink 集群最自然的出发点是在 http://localhost:8081 下暴露的 WebUI。如果一切顺利，你会看到集群最初由一个 TaskManager 组成，并执行一个名为 Click Event Count 的 Job。\nFlink WebUI 包含了很多关于 Flink 集群和它的工作的有用和有趣的信息（JobGraph, Metrics, Checkpointing Statistics, TaskManager Status, \u0026hellip;）。\n日志 JobManager 可以通过 docker-compose 对 JobManager 日志进行跟踪。\ndocker-compose logs -f jobmanager 在初始启动后，你应该主要看到每一个检查点完成的日志信息。\nTaskManager TaskManager 的日志也可以用同样的方式进行 tail。\ndocker-compose logs -f taskmanager 在初始启动后，你应该主要看到每个检查点完成的日志信息。\nFlink CLI Flink CLI 可以在客户端容器中使用。例如，要打印 Flink CLI 的帮助信息，你可以运行以下命令\ndocker-compose run --no-deps client flink --help Flink REST API Flink REST API 通过主机上的 localhost:8081 或客户端容器中的 jobmanager:8081 暴露出来，例如，要列出所有当前正在运行的作业，你可以运行:\ncurl localhost:8081/jobs Kafka Topics 你可以通过运行以下命令来查看写入 Kafka 主题的记录\n//input topic (1000 records/s) docker-compose exec kafka kafka-console-consumer.sh \\  --bootstrap-server localhost:9092 --topic input //output topic (24 records/min) docker-compose exec kafka kafka-console-consumer.sh \\  --bootstrap-server localhost:9092 --topic output Time to Play! 现在你已经学会了如何与 Flink 和 Docker 容器进行交互，让我们来看看一些常见的操作任务，你可以在我们的游乐场上尝试一下。所有这些任务都是相互独立的，即你可以以任何顺序执行它们。大多数任务可以通过 CLI 和 REST API 来执行。\n列出正在运行的 Job  CLI  命令\ndocker-compose run --no-deps client flink list 期望的输出\nWaiting for response... ------------------ Running/Restarting Jobs ------------------- 16.07.2019 16:37:55 : \u0026lt;job-id\u0026gt; : Click Event Count (RUNNING) -------------------------------------------------------------- No scheduled jobs.  REST API  请求\ncurl localhost:8081/jobs 期待的响应(美化了打印)\n{ \u0026#34;jobs\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34; } ] } JobID 在提交时被分配给作业(Job)，并且需要通过 CLI 或 REST API 对作业(Job)执行操作。\n观察故障和恢复 Flink 在(部分)失败下提供了精确的一次处理保证。在这个游乐场中，你可以观察并在一定程度上验证这种行为。\n步骤1：观察输出\n如上所述，在这个游乐场中的事件是这样生成的，每个窗口正好包含一千条记录。因此，为了验证 Flink 是否成功地从 TaskManager 故障中恢复，而没有数据丢失或重复，你可以跟踪 output 主题，并检查恢复后所有的窗口都存在，而且计数是正确的。\n为此，从 output 主题开始读取，并让这个命令运行到恢复后（步骤3）。\ndocker-compose exec kafka kafka-console-consumer.sh \\  --bootstrap-server localhost:9092 --topic output 第二步：引入故障\n为了模拟部分故障，你可以杀死一个 TaskManager，在生产设置中，这可能对应于 TaskManager 进程、TaskManager 机器的丢失，或者仅仅是框架或用户代码抛出的瞬时异常（例如由于暂时不可用）。\ndocker-compose kill taskmanager 几秒钟后，JobManager 会注意到 TaskManager 的丢失，取消受影响的 Job，并立即重新提交它进行恢复。当 Job 被重新启动后，其任务仍处于 SCHEDULED 状态，由紫色的方块表示（见下面的截图）。\n注意：即使作业(Job)的任务(Task)处于 SCHEDULED 状态而不是 RUNNING 状态，作业(Job)的整体状态也会显示为 RUNNING。\n此时，Job 的任务(Task)不能从 SCHEDULED 状态转为 RUNNING 状态，因为没有资源(TaskManager 提供的 TaskSlots）来运行这些任务。在新的 TaskManager 可用之前，Job 将经历一个取消和重新提交的循环。\n同时，数据生成器会不断地将 ClickEvents 推送到 input 主题中。这类似于真正的生产设置，在生产数据的同时，要处理数据的 Job 却宕机了。\n步骤3：恢复\n一旦你重新启动 TaskManager，它就会重新连接到 JobManager。\ndocker-compose up -d taskmanager 当 JobManager 被通知到新的 TaskManager 时，它将恢复中的 Job 的任务(tasks)调度到新的可用 TaskSlots。重新启动后，任务会从故障前最后一次成功的检查点恢复其状态，并切换到 RUNNING 状态。\nJob 将快速处理来自 Kafka 的全部积压输入事件(在故障期间积累的)，并以更高的速度(\u0026gt;24条记录/分钟)产生输出，直到到达流的头部。在输出中，你会看到所有的键(页面)都存在于所有的时间窗口中，而且每个计数都是精确的 1000。由于我们是在\u0026quot;至少一次\u0026quot;模式下使用 FlinkKafkaProducer，所以你有可能会看到一些重复的输出记录。\n注意：大多数生产设置依赖于资源管理器(Kubernetes、Yarn、Mesos)来自动重启失败的进程。\n升级和重新缩放作业 升级 Flink 作业总是涉及两个步骤。首先，用一个保存点优雅地停止 Flink Job。保存点是在一个明确定义的、全局一致的时间点(类似于检查点)上的完整应用状态的一致快照。其次，升级后的 Flink Job 从 Savepoint 开始。在这种情况下，\u0026ldquo;升级\u0026quot;可以意味着不同的事情，包括以下内容:\n 配置的升级（包括作业的并行性）。 对 Job 的拓扑结构进行升级（增加/删除 Operator）。 对 Job 的用户定义的函数进行升级。  在开始升级之前，你可能要开始 tailing output 主题，以观察在升级过程中没有数据丢失或损坏。\ndocker-compose exec kafka kafka-console-consumer.sh \\  --bootstrap-server localhost:9092 --topic output 第一步：停止工作\n要优雅地停止作业，你需要使用 CLI 或 REST API 的 \u0026ldquo;stop\u0026rdquo; 命令。为此，你需要该作业的 JobID，你可以通过列出所有正在运行的 Job 或从 WebUI 中获得。有了 JobID，你就可以继续停止该作业:\n CLI  命令\ndocker-compose run --no-deps client flink stop \u0026lt;job-id\u0026gt; 预期的输出\nSuspending job \u0026quot;\u0026lt;job-id\u0026gt;\u0026quot; with a savepoint. Suspended job \u0026quot;\u0026lt;job-id\u0026gt;\u0026quot; with a savepoint. Savepoint 已经被存储到 flink-conf.yaml 中配置的 state.savepoint.dir 中，它被安装在本地机器的 /tmp/flink-savepoints-directory/ 下。在下一步中，你将需要这个 Savepoint 的路径。在 REST API 的情况下，这个路径已经是响应的一部分，你将需要直接查看文件系统。\n命令\nls -lia /tmp/flink-savepoints-directory 预期的输出\ntotal 0 17 drwxr-xr-x 3 root root 60 17 jul 17:05 . 2 drwxrwxrwt 135 root root 3420 17 jul 17:09 .. 1002 drwxr-xr-x 2 root root 140 17 jul 17:05 savepoint-\u0026lt;short-job-id\u0026gt;-\u0026lt;uuid\u0026gt;  REST API  请求\n# triggering stop curl -X POST localhost:8081/jobs/\u0026lt;job-id\u0026gt;/stop -d \u0026#39;{\u0026#34;drain\u0026#34;: false}\u0026#39; 预期的响应(美化了打印)\n{ \u0026#34;request-id\u0026#34;: \u0026#34;\u0026lt;trigger-id\u0026gt;\u0026#34; } 请求\n# check status of stop action and retrieve savepoint path curl localhost:8081/jobs/\u0026lt;job-id\u0026gt;/savepoints/\u0026lt;trigger-id\u0026gt; 预期的响应(美化了打印)\n{ \u0026#34;status\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;COMPLETED\u0026#34; }, \u0026#34;operation\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;\u0026lt;savepoint-path\u0026gt;\u0026#34; } } 步骤2a: 重启 Job，不做任何改变\n现在你可以从该保存点重新启动升级后的作业(Job)。为了简单起见，你可以在不做任何更改的情况下重新启动它。\n CLI  命令\ndocker-compose run --no-deps client flink run -s \u0026lt;savepoint-path\u0026gt; \\  -d /opt/ClickCountJob.jar \\  --bootstrap.servers kafka:9092 --checkpointing --event-time 预期的输出\nStarting execution of program Job has been submitted with JobID \u0026lt;job-id\u0026gt;  REST API  请求\n# Uploading the JAR from the Client container docker-compose run --no-deps client curl -X POST -H \u0026#34;Expect:\u0026#34; \\  -F \u0026#34;jarfile=@/opt/ClickCountJob.jar\u0026#34; http://jobmanager:8081/jars/upload 预期的响应(美化了打印)\n{ \u0026#34;filename\u0026#34;: \u0026#34;/tmp/flink-web-\u0026lt;uuid\u0026gt;/flink-web-upload/\u0026lt;jar-id\u0026gt;\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } 请求\n# Submitting the Job curl -X POST http://localhost:8081/jars/\u0026lt;jar-id\u0026gt;/run \\  -d \u0026#39;{\u0026#34;programArgs\u0026#34;: \u0026#34;--bootstrap.servers kafka:9092 --checkpointing --event-time\u0026#34;, \u0026#34;savepointPath\u0026#34;: \u0026#34;\u0026lt;savepoint-path\u0026gt;\u0026#34;}\u0026#39; 预期的输出\n{ \u0026#34;jobid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34; } 一旦 Job 再次 RUNNING，你会在 output 主题中看到，当 Job 在处理中断期间积累的积压时，记录以较高的速度产生。此外，你会看到在升级过程中没有丢失任何数据：所有窗口都存在，数量正好是 1000。\n步骤2b: 用不同的并行度重新启动作业（重新缩放）\n另外，你也可以在重新提交时通过传递不同的并行性，从这个保存点重新缩放作业。\n CLI  docker-compose run --no-deps client flink run -p 3 -s \u0026lt;savepoint-path\u0026gt; \\  -d /opt/ClickCountJob.jar \\  --bootstrap.servers kafka:9092 --checkpointing --event-time 预期的输出\nStarting execution of program Job has been submitted with JobID \u0026lt;job-id\u0026gt;  REST API  请求\n# Uploading the JAR from the Client container docker-compose run --no-deps client curl -X POST -H \u0026#34;Expect:\u0026#34; \\  -F \u0026#34;jarfile=@/opt/ClickCountJob.jar\u0026#34; http://jobmanager:8081/jars/upload 预期的响应(美化了打印)\n{ \u0026#34;filename\u0026#34;: \u0026#34;/tmp/flink-web-\u0026lt;uuid\u0026gt;/flink-web-upload/\u0026lt;jar-id\u0026gt;\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } 请求\n# Submitting the Job curl -X POST http://localhost:8081/jars/\u0026lt;jar-id\u0026gt;/run \\  -d \u0026#39;{\u0026#34;parallelism\u0026#34;: 3, \u0026#34;programArgs\u0026#34;: \u0026#34;--bootstrap.servers kafka:9092 --checkpointing --event-time\u0026#34;, \u0026#34;savepointPath\u0026#34;: \u0026#34;\u0026lt;savepoint-path\u0026gt;\u0026#34;}\u0026#39; 预期的响应(美化了打印)\n{ \u0026#34;jobid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34; } 现在，作业(Job)已经被重新提交，但它不会启动，因为没有足够的 TaskSlots 在增加的并行度下执行它（2个可用，需要3个）。使用:\ndocker-compose scale taskmanager=2 你可以在 Flink 集群中添加一个带有两个 TaskSlots 的第二个 TaskManager，它将自动注册到 JobManager 中。添加 TaskManager 后不久，该任务(Job)应该再次开始运行。\n一旦 Job 再次 \u0026ldquo;RUNNING\u0026rdquo;，你会在 output Topic 中看到在重新缩放过程中没有丢失数据：所有的窗口都存在，计数正好是 1000。\n查询作业(Job)的指标 JobManager 通过其 REST API 公开系统和用户指标。\n端点取决于这些指标的范围。可以通过 jobs/\u0026lt;job-id\u0026gt;/metrics 来列出一个作业的范围内的度量。指标的实际值可以通过 get query 参数进行查询。\n请求\ncurl \u0026quot;localhost:8081/jobs/\u0026lt;jod-id\u0026gt;/metrics?get=lastCheckpointSize\u0026quot; 预期的响应(美化了打印; 没有占位符)\n[ { \u0026#34;id\u0026#34;: \u0026#34;lastCheckpointSize\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;9378\u0026#34; } ] REST API 不仅可以用来查询指标，还可以检索运行中的作业状态的详细信息。\n请求\n# find the vertex-id of the vertex of interest curl localhost:8081/jobs/\u0026lt;jod-id\u0026gt; 预期的响应(美化了打印)\n{ \u0026#34;jid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Click Event Count\u0026#34;, \u0026#34;isStoppable\u0026#34;: false, \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066026, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374793, \u0026#34;now\u0026#34;: 1564467440819, \u0026#34;timestamps\u0026#34;: { \u0026#34;CREATED\u0026#34;: 1564467066026, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;SUSPENDED\u0026#34;: 0, \u0026#34;FAILING\u0026#34;: 0, \u0026#34;CANCELLING\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 1564467066126, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;RESTARTING\u0026#34;: 0 }, \u0026#34;vertices\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ClickEvent Source\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066423, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374396, \u0026#34;tasks\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 2, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;metrics\u0026#34;: { \u0026#34;read-bytes\u0026#34;: 0, \u0026#34;read-bytes-complete\u0026#34;: true, \u0026#34;write-bytes\u0026#34;: 5033461, \u0026#34;write-bytes-complete\u0026#34;: true, \u0026#34;read-records\u0026#34;: 0, \u0026#34;read-records-complete\u0026#34;: true, \u0026#34;write-records\u0026#34;: 166351, \u0026#34;write-records-complete\u0026#34;: true } }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Timestamps/Watermarks\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066441, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374378, \u0026#34;tasks\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 2, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;metrics\u0026#34;: { \u0026#34;read-bytes\u0026#34;: 5066280, \u0026#34;read-bytes-complete\u0026#34;: true, \u0026#34;write-bytes\u0026#34;: 5033496, \u0026#34;write-bytes-complete\u0026#34;: true, \u0026#34;read-records\u0026#34;: 166349, \u0026#34;read-records-complete\u0026#34;: true, \u0026#34;write-records\u0026#34;: 166349, \u0026#34;write-records-complete\u0026#34;: true } }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ClickEvent Counter\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066469, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374350, \u0026#34;tasks\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 2, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;metrics\u0026#34;: { \u0026#34;read-bytes\u0026#34;: 5085332, \u0026#34;read-bytes-complete\u0026#34;: true, \u0026#34;write-bytes\u0026#34;: 316, \u0026#34;write-bytes-complete\u0026#34;: true, \u0026#34;read-records\u0026#34;: 166305, \u0026#34;read-records-complete\u0026#34;: true, \u0026#34;write-records\u0026#34;: 6, \u0026#34;write-records-complete\u0026#34;: true } }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ClickEventStatistics Sink\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066476, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374343, \u0026#34;tasks\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 2, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;metrics\u0026#34;: { \u0026#34;read-bytes\u0026#34;: 20668, \u0026#34;read-bytes-complete\u0026#34;: true, \u0026#34;write-bytes\u0026#34;: 0, \u0026#34;write-bytes-complete\u0026#34;: true, \u0026#34;read-records\u0026#34;: 6, \u0026#34;read-records-complete\u0026#34;: true, \u0026#34;write-records\u0026#34;: 0, \u0026#34;write-records-complete\u0026#34;: true } } ], \u0026#34;status-counts\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 4, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;plan\u0026#34;: { \u0026#34;jid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Click Event Count\u0026#34;, \u0026#34;nodes\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;operator\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operator_strategy\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ClickEventStatistics Sink\u0026#34;, \u0026#34;inputs\u0026#34;: [ { \u0026#34;num\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;ship_strategy\u0026#34;: \u0026#34;FORWARD\u0026#34;, \u0026#34;exchange\u0026#34;: \u0026#34;pipelined_bounded\u0026#34; } ], \u0026#34;optimizer_properties\u0026#34;: {} }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;operator\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operator_strategy\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ClickEvent Counter\u0026#34;, \u0026#34;inputs\u0026#34;: [ { \u0026#34;num\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;ship_strategy\u0026#34;: \u0026#34;HASH\u0026#34;, \u0026#34;exchange\u0026#34;: \u0026#34;pipelined_bounded\u0026#34; } ], \u0026#34;optimizer_properties\u0026#34;: {} }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;operator\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operator_strategy\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Timestamps/Watermarks\u0026#34;, \u0026#34;inputs\u0026#34;: [ { \u0026#34;num\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;ship_strategy\u0026#34;: \u0026#34;FORWARD\u0026#34;, \u0026#34;exchange\u0026#34;: \u0026#34;pipelined_bounded\u0026#34; } ], \u0026#34;optimizer_properties\u0026#34;: {} }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;operator\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operator_strategy\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ClickEvent Source\u0026#34;, \u0026#34;optimizer_properties\u0026#34;: {} } ] } } 请查阅 REST API 参考资料，了解可能查询的完整列表，包括如何查询不同作用域的指标（如 TaskManager 指标）。\n变体 你可能已经注意到，Click Event Count 应用程序总是以 --checkpointing 和 --event-time 程序参数启动。通过在 docker-compose.yaml 的客户端容器的命令中省略这些，你可以改变 Job 的行为。\n  --checkpointing 启用了 checkpoint，这是 Flink 的容错机制。如果你在没有它的情况下运行，并通过故障和恢复，你应该会看到数据实际上已经丢失了。\n  --event-time 启用了你的 Job 的事件时间语义。当禁用时，作业将根据挂钟时间而不是 ClickEvent 的时间戳将事件分配给窗口。因此，每个窗口的事件数量将不再是精确的 1000。\n  Click Event Count 应用程序还有另一个选项，默认情况下是关闭的，你可以启用这个选项来探索这个作业在背压下的行为。你可以在 docker-compose.yaml 的客户端容器的命令中添加这个选项。\n --backpressure 在作业中间增加了一个额外的 operator，在偶数分钟内会造成严重的背压（例如，在10:12期间，但在10:13期间不会）。这可以通过检查各种网络指标（如 outputQueueLength 和 outPoolUsage）和/或使用 WebUI 中的背压监控来观察。  ","permalink":"https://ohmyweekly.github.io/notes/2020-08-17-flink-operations-playground/","tags":["Flink","Playground"],"title":"Flink 操作游乐场"},{"categories":["programming"],"contents":"Python API 指南 本演练将快速让你开始构建一个纯 Python Flink 项目。\n关于如何设置 Python 执行环境，请参考 Python Table API 安装指南。\n设置一个 Python 项目 您可以先创建一个 Python 项目，然后按照安装指南安装 PyFlink 包。\n编写一个 Flink Python Table API 程序 Table API 应用程序通过声明一个表环境开始；对于批处理应用程序，可以是 BatchTableEvironment，对于流式应用程序，可以是 StreamTableEnvironment。这作为与 Flink 运行时交互的主要入口点。它可以用来设置执行参数，如重启策略、默认并行度等。表配置允许设置 Table API 的具体配置。\nexec_env = ExecutionEnvironment.get_execution_environment() exec_env.set_parallelism(1) t_config = TableConfig() t_env = BatchTableEnvironment.create(exec_env, t_config) 在创建的表环境中，可以声明 source/sink 表。\nt_env.connect(FileSystem().path(\u0026#39;/tmp/input\u0026#39;)) \\ .with_format(OldCsv() .field(\u0026#39;word\u0026#39;, DataTypes.STRING())) \\ .with_schema(Schema() .field(\u0026#39;word\u0026#39;, DataTypes.STRING())) \\ .create_temporary_table(\u0026#39;mySource\u0026#39;) t_env.connect(FileSystem().path(\u0026#39;/tmp/output\u0026#39;)) \\ .with_format(OldCsv() .field_delimiter(\u0026#39;\\t\u0026#39;) .field(\u0026#39;word\u0026#39;, DataTypes.STRING()) .field(\u0026#39;count\u0026#39;, DataTypes.BIGINT())) \\ .with_schema(Schema() .field(\u0026#39;word\u0026#39;, DataTypes.STRING()) .field(\u0026#39;count\u0026#39;, DataTypes.BIGINT())) \\ .create_temporary_table(\u0026#39;mySink\u0026#39;) 你也可以使用 TableEnvironment.sql_update() 方法来注册 DDL 中定义的 source/sink 表。\nmy_source_ddl = \u0026#34;\u0026#34;\u0026#34; create table mySource ( word VARCHAR ) with ( \u0026#39;connector.type\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;format.type\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;connector.path\u0026#39; = \u0026#39;/tmp/input\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; my_sink_ddl = \u0026#34;\u0026#34;\u0026#34; create table mySink ( word VARCHAR, `count` BIGINT ) with ( \u0026#39;connector.type\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;format.type\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;connector.path\u0026#39; = \u0026#39;/tmp/output\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; t_env.sql_update(my_source_ddl) t_env.sql_update(my_sink_ddl) 这将在执行环境中注册一个名为 mySource 的表和一个名为 mySink 的表。表 mySource 只有一列，即 word，它消耗从文件 /tmp/input 中读取的字符串。表 mySink 有两列，分别是 word 和 count，将数据写入文件 /tmp/output，用 /t 作为字段分隔符。\n现在，你可以创建一个作业(job)，它从表 mySource 中读取输入，预先执行一些转换，并将结果写入表 mySink。\nt_env.from_path(\u0026#39;mySource\u0026#39;) \\ .group_by(\u0026#39;word\u0026#39;) \\ .select(\u0026#39;word, count(1)\u0026#39;) \\ .insert_into(\u0026#39;mySink\u0026#39;) 最后你必须执行实际的 Flink Python Table API 作业。所有的操作，如创建源、转换和 sink 都是懒惰的。只有当 t_env.execute(job_name) 被调用时，作业才会被运行。\nt_env.execute(\u0026#34;tutorial_job\u0026#34;) 到目前为止，完整的代码如下:\nfrom pyflink.dataset import ExecutionEnvironment from pyflink.table import TableConfig, DataTypes, BatchTableEnvironment from pyflink.table.descriptors import Schema, OldCsv, FileSystem exec_env = ExecutionEnvironment.get_execution_environment() exec_env.set_parallelism(1) t_config = TableConfig() t_env = BatchTableEnvironment.create(exec_env, t_config) t_env.connect(FileSystem().path(\u0026#39;/tmp/input\u0026#39;)) \\ .with_format(OldCsv() .field(\u0026#39;word\u0026#39;, DataTypes.STRING())) \\ .with_schema(Schema() .field(\u0026#39;word\u0026#39;, DataTypes.STRING())) \\ .create_temporary_table(\u0026#39;mySource\u0026#39;) t_env.connect(FileSystem().path(\u0026#39;/tmp/output\u0026#39;)) \\ .with_format(OldCsv() .field_delimiter(\u0026#39;\\t\u0026#39;) .field(\u0026#39;word\u0026#39;, DataTypes.STRING()) .field(\u0026#39;count\u0026#39;, DataTypes.BIGINT())) \\ .with_schema(Schema() .field(\u0026#39;word\u0026#39;, DataTypes.STRING()) .field(\u0026#39;count\u0026#39;, DataTypes.BIGINT())) \\ .create_temporary_table(\u0026#39;mySink\u0026#39;) t_env.from_path(\u0026#39;mySource\u0026#39;) \\ .group_by(\u0026#39;word\u0026#39;) \\ .select(\u0026#39;word, count(1)\u0026#39;) \\ .insert_into(\u0026#39;mySink\u0026#39;) t_env.execute(\u0026#34;tutorial_job\u0026#34;) 执行 Flink Python Table API 程序 首先，你需要在 \u0026ldquo;/tmp/input\u0026rdquo; 文件中准备输入数据。你可以选择以下命令行来准备输入数据。\n$ echo -e \u0026#34;flink\\npyflink\\nflink\u0026#34; \u0026gt; /tmp/input 接下来，你可以在命令行上运行这个例子（注意：如果结果文件 \u0026ldquo;/tmp/output\u0026rdquo; 已经存在，你需要在运行这个例子之前删除该文件）。\n$ python WordCount.py 该命令在本地小型集群中构建并运行 Python Table API 程序。你也可以将 Python Table API 程序提交到远程集群，详情可以参考 Job Submission Examples。\n最后，您可以在命令行中看到执行结果。\n$ cat /tmp/output flink\t2 pyflink\t1 这应该可以让你开始编写自己的 Flink Python Table API 程序。要了解更多关于 Python Table API 的信息，你可以参考 Flink Python Table API Docs 了解更多细节。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-17-python-api-tutorial/","tags":["Flink","Python"],"title":"Python API 指南"},{"categories":["programming"],"contents":"Apache Kafka Connector Kafka Consumer val properties = new Properties() properties.setProperty(\u0026#34;bootstrap.servers\u0026#34;, \u0026#34;localhost:9092\u0026#34;) properties.setProperty(\u0026#34;group.id\u0026#34;, \u0026#34;test\u0026#34;) stream = env .addSource(new FlinkKafkaConsumer[String](\u0026#34;topic\u0026#34;, new SimpleStringSchema(), properties)) ","permalink":"https://ohmyweekly.github.io/notes/2020-08-05-connectors-in-flink/","tags":["Flink","Connector"],"title":"Flink 中的 Connectors"},{"categories":["programming"],"contents":"模块 Julia 中的模块是独立的变量工作空间，即它们引入了一个新的全局作用域。它们在语法上是有分界的，在 module Name ... end 里面。模块允许您创建顶层定义（也就是全局变量），而不用担心您的代码与别人的代码一起使用时的名称冲突。在一个模块中，你可以控制哪些来自其他模块的名字是可见的（通过导入），并指定哪些名字是要公开的（通过导出）。\n下面的例子展示了模块的主要功能。这个例子并不是为了运行，而是为了说明问题。\nmodule MyModule using Lib using BigLib: thing1, thing2 import Base.show export MyType, foo struct MyType x end bar(x) = 2x foo(a::MyType) = bar(a.x) + 1 show(io::IO, a::MyType) = print(io, \u0026#34;MyType $(a.x)\u0026#34;) end 需要注意的是，这个样式并不是要在模块的正文中缩进，因为这通常会导致整个文件被缩进。\n这个模块定义了一个 MyType 类型和两个函数。函数 foo 和 MyType 类型是导出的，因此可以导入到其他模块中。函数 bar 是 MyModule 的私有函数。\nusing Lib 语句意味着将有一个名为 Lib 的模块可以根据需要解析名称。当遇到一个全局变量在当前模块中没有定义时，系统会在 Lib 导出的变量中搜索它，如果在那里找到了，就会导入它。这意味着在当前模块内对该全局的所有使用都将解析为该变量在 Lib 中的定义。\nusing BigLib: thing1, thing2 语句，只将标识符 thing1 和 thing2 从模块 BigLib 中带入作用域。如果这些名称指的是函数，那么将不允许向它们添加方法（你只能 \u0026ldquo;使用 \u0026ldquo;它们，而不是扩展它们）。\nimport 关键字支持与 using 相同的语法。import 与 using 的不同之处在于，使用 import 导入的函数可以用新的方法进行扩展。\n在上面的 MyModule 中，我们想给标准的 show 函数添加一个方法，所以我们必须写 import Base.show。只有通过 using 才能看到名字的函数不能被扩展。\n一旦一个变量通过 using 或 import 变得可见，一个模块就不能创建自己的同名变量。导入的变量是只读的，分配给全局变量总是会影响到当前模块所拥有的变量，否则会引发错误。\n模块使用情况概述 要加载一个模块，可以使用两个主要的关键词：using 和 import。要了解它们的区别，请看下面的例子。\nmodule MyModule export x, y x() = \u0026#34;x\u0026#34; y() = \u0026#34;y\u0026#34; p() = \u0026#34;p\u0026#34; end 在这个模块中，我们导出了 x 和 y 函数(用关键字 export)，也有非导出的函数 p，有几种不同的方法可以将 Module 及其内部函数加载到当前的工作空间中。\n   导入命令 带入带作用域中的东西 可用于方法扩展     using MyModule 所有导出的名字(x 和 y), MyModule.x, MyModule.y 和 MyModule.p MyModule.x, MyModule.y 和 MyModule.p   using MyModule: x, p x 和 p    import MyModule MyModule.x, MyModule.y 和 MyModule.p MyModule.x, MyModule.y 和 MyModule.p   import MyModule.x, MyModule.p x 和 p x 和 p   import MyModule: x, p x 和 p x 和 p    模块和文件 文件和文件名大多与模块无关，模块只与模块表达式有关。一个模块可以有多个文件，一个文件可以有多个模块。\nmodule Foo include(\u0026#34;file1.jl\u0026#34;) include(\u0026#34;file2.jl\u0026#34;) end 在不同的模块中包含相同的代码，提供了类似 mixin 的行为。人们可以使用这一点来用不同的基础定义来运行相同的代码，例如，通过使用某些操作符的\u0026quot;安全\u0026quot;版本来测试代码。\nmodule Normal include(\u0026#34;mycode.jl\u0026#34;) end module Testing include(\u0026#34;safe_operators.jl\u0026#34;) include(\u0026#34;mycode.jl\u0026#34;) end 标准模块 There are three important standard modules:\nCore 包含\u0026quot;内置于\u0026quot;语言中的所有功能。 Base 包含几乎在所有情况下都有用的基本功能。 Main 是当 Julia 被启动时的顶级模块和当前模块。\n默认的顶层定义和裸模块 除了 using Base 之外，模块还自动包含 eval和 include 函数的定义，这些函数在该模块的全局作用域内评估表达式/文件。\n如果不想要这些默认的定义，可以使用关键字 baremodule 来代替定义模块（注意： Core 仍然是导入的，如上所述）。以 baremodule 来说，一个标准的模块是这样的。\nbaremodule Mod using Base eval(x) = Core.eval(Mod, x) include(p) = Base.include(Mod, p) ... end 相对和绝对模块路径 给定 using Foo 语句，系统会查询内部的顶层模块表，寻找一个名为 Foo 的模块。如果该模块不存在，系统会尝试 require(:Foo)，这通常会导致从安装的包中加载代码。\n然而，有些模块包含子模块，这意味着你有时需要访问一个非顶层模块。有两种方法可以做到这一点。第一种是使用绝对路径，例如 using Base.Sort。第二种是使用相对路径，这样可以更容易地导入当前模块的子模块或其任何一个外层模块。\nmodule Parent module Utils ... end using .Utils ... end 这里模块 Parent 包含一个子模块 Utils，Parent 中的代码希望 Utils 的内容可见。这可以通过在 using 路径中使用点号来实现。添加更多的前导点号会使模块的层次结构上升。例如，using ..Utils 会在 Parent 的外层模块中查找Utils，而不是在 Parent 本身中查找。\n注意相对导入限定符只在使用和导入语句中有效。\n命名空间杂项 如果一个名字是限定的(例如 Base.sin)，那么即使它没有被导出，也可以被访问。这在调试时往往很有用。它也可以通过使用限定名作为函数名来添加方法。但是，由于会产生语法上的歧义，如果你想给不同模块中的一个函数添加方法，而这个函数的名称只包含符号，例如一个运算符，Base.+，你必须使用 Base.:+ 来引用它。如果运算符的长度超过一个字符，你必须用括号把它括起来，比如 Base.:(==)。\n在导入和导出语句中，宏的名称用 @ 书写，例如 import Mod.@mac。其他模块中的宏可以用 Mod.@mac 或 @Mod.mac 来调用。\n语法 M.x = y 不能用于分配其他模块中的全局，全局分配总是模块-局部的。\n变量名可以通过声明为 global x 来 \u0026ldquo;保留\u0026quot;而不分配给它，这样可以防止加载后初始化的 globals 的名称冲突。\n模块初始化和预编译 大型模块可能需要几秒钟的时间来加载，因为执行一个模块中的所有语句往往需要编译大量的代码。Julia 创建了模块的预编译缓存来减少这个时间。\n当使用 import 或 using 加载模块时，会自动创建并使用增量的预编译模块文件。这将导致它在第一次导入时自动编译。另外，您也可以手动调用 Base.compilecache(modulename)。由此产生的缓存文件将存储在 DEPOT_PATH[1]/compiled/ 中。随后，只要模块的任何依赖关系发生变化，模块就会在 using 或 import 时自动重新编译；依赖关系是指导入的模块、Julia 构建的模块、包含的文件，或者模块文件中 include_dependency(path) 声明的显式依赖关系。\n对于文件依赖，通过检查由 include 加载的文件或由 include_dependency 显式添加的文件的修改时间(mtime)是否保持不变，或者是否等于被截断到最接近秒的修改时间(以适应无法以亚秒级精度复制 mtime 的系统)来确定变化。它还考虑到在 require 中搜索逻辑选择的文件路径是否与创建预编译文件的路径匹配。它还会考虑到已经加载到当前进程中的一组依赖关系，即使这些模块的文件发生变化或消失，也不会重新编译这些模块，以避免在运行系统和预编译缓存之间产生不兼容的情况。\n如果你知道某个模块预编译你的模块是不安全的（例如，出于下面描述的原因之一），你应该在模块文件中加上 __precompile__(false)（通常放在顶部）。这将导致 Base.compilecache 抛出一个错误，并将导致 using / import 直接将其加载到当前进程中而跳过预编译和缓存。这也因此阻止了该模块被任何其他预编译模块导入。\n您可能需要注意创建增量共享库时固有的某些行为，在编写模块时可能需要注意。例如，外部状态不会被保存。为了适应这一点，明确地将任何必须在运行时发生的初始化步骤与可以在编译时发生的步骤分开。为此，Julia 允许您在您的模块中定义一个 __init__() 函数来执行任何必须在运行时发生的初始化步骤。这个函数在编译时不会被调用（--output-*）。实际上，你可以假设它在代码的生命周期中只运行一次。当然，如果有必要的话，你可以手动调用它，但是默认情况下，你可以假设这个函数处理的是本地机器的计算状态，它不需要\u0026ndash;甚至不应该\u0026ndash;在编译后的镜像中捕获。它将在模块被加载到一个进程后被调用，包括如果它被加载到增量编译中(--output-incremental=yes)，但如果它被加载到一个完整的编译进程中，则不会被调用。\n特别是，如果你在一个模块中定义了一个 function __init__()，那么 Julia 将在模块被加载后（例如通过 import、using 或 require）在运行时第一次立即调用 __init__()（也就是说，__init__ 只被调用一次，而且是在模块中的所有语句被执行后才被调用）。因为它是在模块完全导入之后被调用的，所以任何子模块或其它导入的模块都会在外层模块的 __init__ 之前调用它们的 __init__ 函数。\n__init__ 的两个典型用途是调用外部 C 库的运行时初始化函数和初始化涉及外部库返回指针的全局常量。例如，假设我们正在调用一个 C 库 libfoo，它要求我们在运行时调用 foo_init() 初始化函数。假设我们还想定义一个全局常量 foo_data_ptr，用来存放 libfoo 定义的 void *foo_data() 函数的返回值\u0026ndash;这个常量必须在运行时（而不是在编译时）初始化，因为指针地址会随着运行而改变。你可以通过在你的模块中定义下面的 __init__ 函数来实现。\nconst foo_data_ptr = Ref{Ptr{Cvoid}}(0) function __init__() ccall((:foo_init, :libfoo), Cvoid, ()) foo_data_ptr[] = ccall((:foo_data, :libfoo), Ptr{Cvoid}, ()) nothing end 请注意，我们完全可以在函数内部定义一个全局，比如 __init__；这是使用动态语言的优势之一。但是通过在全局作用域内定义一个常量，我们可以确保编译器知道这个类型，并允许它生成更好的优化代码。显然，你的模块中任何其他依赖于 foo_data_ptr 的 globals 也必须在 __init__ 中初始化。\n涉及大多数不是由 ccall 产生的 Julia 对象的常量不需要放在 __init__ 中：它们的定义可以被预编译并从缓存的模块映像中加载。这包括像数组这样复杂的堆分配对象。然而，任何返回原始指针值的例程都必须在运行时调用，以便预编译工作（Ptr 对象将变成空指针，除非它们被隐藏在 isbits 对象中）。这包括 Julia 函数 cfunction 和 pointer 的返回值。\n字典和集合类型，或者一般来说任何依赖于 hash(key) 方法输出的东西，都是比较棘手的情况。在常见的情况下，键是数字、字符串、符号、范围、Expr 或这些类型的组合（通过数组、元组、集合、对等），它们可以安全地进行预编译。然而，对于其他一些关键类型，如 Function 或 DataType 和通用的用户定义类型，在这些类型中，你没有定义 hash 方法，回退 hash 方法取决于对象的内存地址（通过它的 objectid），因此可能会在运行时改变。如果你有这些键类型之一，或者如果你不确定，为了安全起见，你可以在你的 __init__ 函数中初始化这个字典。或者，你也可以使用 IdDict 字典类型，它由预编译特别处理，所以在编译时初始化是安全的。\n在使用预编译时，保持对编译阶段和执行阶段的清晰认识很重要。在这种模式下，往往会更清楚地认识到 Julia 是一个允许执行任意 Julia 代码的编译器，而不是一个同时生成编译代码的独立解释器。\n其他已知的潜在故障情况包括。\n 全局计数器（例如，用于试图唯一识别对象）。考虑以下代码片段。  mutable struct UniquedById myid::Int let counter = 0 UniquedById() = new(counter += 1) end end 虽然这段代码的目的是给每个实例一个唯一的 id，但计数器的值是在编译结束时记录的。这个增量编译模块的所有后续使用将从同一个计数器值开始。\n请注意，objectid（通过哈希内存指针工作）也有类似的问题（参见下面关于 Dict 用法的说明）。\n一种替代方法是使用宏来捕获 @MODULE，并将其与当前的计数器值一起单独存储，然而，重新设计代码使其不依赖于这个全局状态可能会更好。\n 关联集合(比如 Dict 和 Set)需要在 __init__ 中重新洗牌(将来可能会提供一个机制来注册一个初始化函数)。\n  根据编译时的副作用在加载时持续存在。例如：修改其他 Julia 模块中的数组或其他变量；维护打开的文件或设备的句柄；存储其他系统资源（包括内存）的指针。\n  通过直接引用而不是通过它的查找路径，从另一个模块创建意外的全局状态\u0026quot;副本\u0026rdquo;。例如，（在全局作用域内）。\n  #mystdout = Base.stdout #= will not work correctly, since this will copy Base.stdout into this module =# # instead use accessor functions: getstdout() = Base.stdout #= best option =# # or move the assignment into the runtime: __init__() = global mystdout = Base.stdout #= also works =# 对预编译代码时可以进行的操作进行了一些额外的限制，以帮助用户避免其他错误行为的情况。\n  调用 eval 引起另一个模块的副作用。当增量预编译标志被设置时，这也会导致发出警告。\n  在 __init__() 被启动后，从本地作用域调用 global const 语句(参见问题 #12010，计划为此增加一个错误)\n  在进行增量预编译时，替换一个模块是一个运行时错误。\n  还有几点需要注意。\n  在对源文件本身进行修改之后，不会进行代码重载/缓存无效化（包括通过 Pkg.update），而且在 Pkg.rm 之后也不会进行清理。\n  预编译不考虑重塑数组的内存共享行为 (每个视图都有自己的副本)\n  期待文件系统在编译时和运行时之间保持不变，例如 @FILE/source_path() 在运行时查找资源，或者 BinDeps 的 @checked_lib 宏。有时这是不可避免的。然而，在可能的情况下，在编译时将资源复制到模块中是一个很好的做法，这样它们就不需要在运行时被找到。\n  WeakRef 对象和 finalizers 目前还没有被序列化器正确处理（这将在即将发布的版本中得到修正）。\n  通常最好避免捕获对内部元数据对象实例的引用，如 Method、MethodInstance、MethodTable、TypeMapLevel、TypeMapEntry 以及这些对象的字段，因为这可能会混淆序列化器，可能不会导致你想要的结果。这样做不一定会出错，但你只需要做好准备，系统会尝试复制其中的一些对象，并为其他对象创建一个唯一的实例。\n  在模块开发过程中，有时关闭增量预编译是很有帮助的。命令行标志 --compiled-modules={yes|no} 可以让你开启或关闭模块预编译。当 Julia 以 --compiled-modules=no 启动时，当加载模块和模块依赖时，编译缓存中的序列化模块会被忽略。Base.compilecache 仍然可以被手动调用。这个命令行标志的状态被传递给 Pkg.build，以便在安装、更新和显式构建包时禁用自动预编译触发。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-05-modules-in-julia/","tags":["julia","module"],"title":"Julia 中的 模块"},{"categories":["programming"],"contents":"进入 Pkg 模式 Pkg 是 Julia 中包管理工具。Pkg 来自于 REPL, 在 Julia 的 REPL 中按下 ] 就进入 Pkg REPL 了。要回到 Julia REPL, 按退格键或 ^C。\n使用 Pkg add JSON # 添加一个 package add JSON StaticArrays # 添加多个 package rm JSON # 移除一个 package rm JSON StaticArrays # 移除多个 package add https://github.com/JuliaLang/Example.jl # 添加一个未注册的 package rm Example # 按名字移除 package update Example # 升级一个已安装的 package update # 升级所有已安装的 package environments 你可能已经注意到 Pkg REPL 提示符前面的 (@v1.5) 字符串了。这里的 (@v1.5) 就是激活环境(active environment)。激活环境是能被诸如 add、rm 和 update 等 Pkg 命令修改的环境。\n我们可以设置一个新的激活环境用于实验。要设置激活环境, 使用 activate:\n(@v1.5) pkg\u0026gt; activate tutorial Activating new environment at `~/tutorial/Project.toml` ~/tutorial/Project.toml 是激活环境的项目文件。项目文件是 Pkg 存储环境的元数据的地方。\n(tutorial) pkg\u0026gt; status Status `~/tutorial/Project.toml` (empty project) 现在这个新的环境是空的, 我们添加一个 package 观察下:\n(tutorial) pkg\u0026gt; add Example Updating registry at `~/.julia/registries/General` Updating git-repo `https://github.com/JuliaRegistries/General.git` Resolving package versions... Cloning [7876af07-990d-54b4-ab0e-23690620f79a] Example from https://github.com/JuliaLang/Example.jl.git Installed Example ─ v0.5.3 Updating `~/tutorial/Project.toml` [7876af07] + Example v0.5.3 Updating `~/tutorial/Manifest.toml` [7876af07] + Example v0.5.3 用 status 命令查看激活环境的信息:\n(tutorial) pkg\u0026gt; status Status `~/tutorial/Project.toml` [7876af07] Example v0.5.3 使用 develop 命令设置 Example package 的 一个 git clone, 以供我们修改这个本地仓库:\n(tutorial) pkg\u0026gt; develop --local Example Cloning git-repo `https://github.com/JuliaLang/Example.jl.git` Resolving package versions... Updating `~/tutorial/Project.toml` [7876af07] ~ Example v0.5.3 ⇒ v0.5.4 `dev/Example` Updating `~/tutorial/Manifest.toml` [7876af07] ~ Example v0.5.3 ⇒ v0.5.4 `dev/Example` 用 ; 切换到 shell 模式, 用 vi 修改 ~/tutorial/dev/Example/src/Example.jl 文件, 增加一个 plusone 函数, 保存。\n在 Julia 的 REPL 中, 导入修改后的 Example package:\njulia\u0026gt; import Example [ Info: Precompiling Example [7876af07-990d-54b4-ab0e-23690620f79a] julia\u0026gt; Example.plusone(1) 2 julia\u0026gt; Example.plusone(4) 5 可以看到我们添加的函数生效了, 这样就很方便我们添加测试新功能。如果我们已经不再需要本地的 Example 了, 需要使用 free 命令以停止使用本地克隆的 package, 转而使用已注册版本代替:\n(tutorial) pkg\u0026gt; free Example Resolving package versions... Updating `~/tutorial/Project.toml` [7876af07] ~ Example v0.5.4 `dev/Example` ⇒ v0.5.3 Updating `~/tutorial/Manifest.toml` [7876af07] ~ Example v0.5.4 `dev/Example` ⇒ v0.5.3 如果已经用 tutorial 做完实验了, 可以使用不带参数的 activate 回到默认环境:\n(tutorial) pkg\u0026gt; activate Activating environment at `~/.julia/environments/v1.5/Project.toml` (@v1.5) pkg\u0026gt; ","permalink":"https://ohmyweekly.github.io/notes/2020-08-04-pkg-in-julia/","tags":["julia","pkg"],"title":"Julia 中的 Pkg"},{"categories":["programming"],"contents":"Dates 模块的加载和使用 在 Julia 的 Pkg REPL 中, 输入 add Dates 添加 Dates 模块。回到 Julia 的 REPL 中, 输入:\njulia\u0026gt; using Dates julia\u0026gt; DateTime(2020) 2020-01-01T00:00:00 julia\u0026gt; typeof(DateTime(2020)) DateTime julia\u0026gt; DateTime(2020,8,1) 2020-08-01T00:00:00 julia\u0026gt; DateTime(2020,8,1,12) 2020-08-01T12:00:00 julia\u0026gt; DateTime(2020,8,1,12,30) 2020-08-01T12:30:00 julia\u0026gt; DateTime(2020,8,1,12,30,59) 2020-08-01T12:30:59 julia\u0026gt; DateTime(2020,8,1,12,30,59, 999) 2020-08-01T12:30:59.999 julia\u0026gt; Date(2020, 8) 2020-08-01 julia\u0026gt; Date(2020, 8, 1) 2020-08-01 julia\u0026gt; Date(Dates.Year(2020),Dates.Month(8),Dates.Day(1)) 2020-08-01 julia\u0026gt; Date(Dates.Month(8),Dates.Year(2020)) 2020-08-01 Date 和 DateTime 的算术操作 julia\u0026gt; dt = Date(2012,2,29) 2012-02-29 julia\u0026gt; dt2 = Date(2000,2,1) 2000-02-01 julia\u0026gt; dump(dt) Date instant: Dates.UTInstant{Day} periods: Day value: Int64 734562 julia\u0026gt; dump(dt2) Date instant: Dates.UTInstant{Day} periods: Day value: Int64 730151 julia\u0026gt; dt \u0026gt; dt2 true julia\u0026gt; dt != dt2 true julia\u0026gt; dt + dt2 ERROR: MethodError: no method matching +(::Date, ::Date) [...] julia\u0026gt; dt * dt2 ERROR: MethodError: no method matching *(::Date, ::Date) [...] julia\u0026gt; dt / dt2 ERROR: MethodError: no method matching /(::Date, ::Date) julia\u0026gt; dt - dt2 4411 days julia\u0026gt; typeof(dt - dt2) Day julia\u0026gt; dt2 - dt -4411 days julia\u0026gt; dt = DateTime(2012,2,29) 2012-02-29T00:00:00 julia\u0026gt; dt2 = DateTime(2000,2,1) 2000-02-01T00:00:00 julia\u0026gt; dt - dt2 381110400000 milliseconds julia\u0026gt; typeof(dt - dt2) Millisecond 访问器函数 因为 Date 和 DateTime 类型被存储为单个 Int64 值，所以日期部分或字段可以通过访问器函数进行检索。小写访问器函数以整数形式返回字段。\njulia\u0026gt; t = Date(2014, 1, 31) 2014-01-31 julia\u0026gt; Dates.year(t) 2014 julia\u0026gt; Dates.month(t) 1 julia\u0026gt; Dates.week(t) 5 julia\u0026gt; Dates.day(t) 31 而专有形式返回相应 Period 类型中的相同值。\njulia\u0026gt; Dates.Year(t) 2014 years julia\u0026gt; Dates.Day(t) 31 days Julia 还提供了复合方法，因为在同时需要多个字段的情况下，这些方法提供了一种效率衡量标准。\njulia\u0026gt; Dates.yearmonth(t) (2014, 1) julia\u0026gt; Dates.monthday(t) (1, 31) julia\u0026gt; Dates.yearmonthday(t) (2014, 1, 31) 也可以访问底层 UTInstant 或整数值。\njulia\u0026gt; dump(t) Date instant: Dates.UTInstant{Day} periods: Day value: Int64 735264 julia\u0026gt; t.instant Dates.UTInstant{Day}(Day(735264)) julia\u0026gt; Dates.value(t) 735264 查询函数 查询函数提供关于 TimeType 的历法信息。它们包括关于一周中的某一天的信息。\njulia\u0026gt; t = Date(2014, 1, 31) 2014-01-31 julia\u0026gt; Dates.dayofweek(t) 5 julia\u0026gt; Dates.dayname(t) \u0026#34;Friday\u0026#34; julia\u0026gt; Dates.dayofweekofmonth(t) # 5th Friday of January 5 julia\u0026gt; Dates.monthname(t) \u0026#34;January\u0026#34; julia\u0026gt; Dates.daysinmonth(t) 31 以及 TimeType 的年份和季度信息。\njulia\u0026gt; Dates.isleapyear(t) false julia\u0026gt; Dates.dayofyear(t) 31 julia\u0026gt; Dates.quarterofyear(t) 1 julia\u0026gt; Dates.dayofquarter(t) 31 dayname 和 monthname 方法也可以使用一个可选的 locale 关键字，它可以用来返回其他语言/地区的年份或月份的名称。这些函数也有返回缩写名称的版本，即 dayabbr 和 monthabbr。首先将映射加载到 LOCALES 变量中。\njulia\u0026gt; french_months = [\u0026#34;janvier\u0026#34;, \u0026#34;février\u0026#34;, \u0026#34;mars\u0026#34;, \u0026#34;avril\u0026#34;, \u0026#34;mai\u0026#34;, \u0026#34;juin\u0026#34;, \u0026#34;juillet\u0026#34;, \u0026#34;août\u0026#34;, \u0026#34;septembre\u0026#34;, \u0026#34;octobre\u0026#34;, \u0026#34;novembre\u0026#34;, \u0026#34;décembre\u0026#34;]; julia\u0026gt; french_monts_abbrev = [\u0026#34;janv\u0026#34;,\u0026#34;févr\u0026#34;,\u0026#34;mars\u0026#34;,\u0026#34;avril\u0026#34;,\u0026#34;mai\u0026#34;,\u0026#34;juin\u0026#34;, \u0026#34;juil\u0026#34;,\u0026#34;août\u0026#34;,\u0026#34;sept\u0026#34;,\u0026#34;oct\u0026#34;,\u0026#34;nov\u0026#34;,\u0026#34;déc\u0026#34;]; julia\u0026gt; french_days = [\u0026#34;lundi\u0026#34;,\u0026#34;mardi\u0026#34;,\u0026#34;mercredi\u0026#34;,\u0026#34;jeudi\u0026#34;,\u0026#34;vendredi\u0026#34;,\u0026#34;samedi\u0026#34;,\u0026#34;dimanche\u0026#34;]; julia\u0026gt; Dates.LOCALES[\u0026#34;french\u0026#34;] = Dates.DateLocale(french_months, french_monts_abbrev, french_days, [\u0026#34;\u0026#34;]); 然后可以利用上述函数进行查询。\njulia\u0026gt; Dates.dayname(t;locale=\u0026#34;french\u0026#34;) \u0026#34;vendredi\u0026#34; julia\u0026gt; Dates.monthname(t;locale=\u0026#34;french\u0026#34;) \u0026#34;janvier\u0026#34; julia\u0026gt; Dates.monthabbr(t;locale=\u0026#34;french\u0026#34;) \u0026#34;janv\u0026#34; 由于没有加载日期的缩写版本，试图使用函数 dayabbr 会出错。\njulia\u0026gt; Dates.dayabbr(t;locale=\u0026#34;french\u0026#34;) ERROR: BoundsError: attempt to access 1-element Array{String,1} at index [5] Stacktrace: [...] 时间类型-周期算术 在使用任何语言/日期框架时，熟悉如何处理日期-周期算术是一个很好的做法，因为有一些棘手的问题需要处理（尽管对于日-精度类型来说要少得多）。\nDates 模块的方法试图遵循简单的原则，即在做 Period 算术时尽量少改。这种方法也常被称为历法算术，或者说如果有人在对话中问你同样的计算方法，你可能会猜到。为什么要大惊小怪呢？我们举个经典的例子：把2014年1月31日加1个月。答案是什么？Javascript 会说3月3日（假设31天）。PHP 会说3月2日（假设30天）。事实上，没有正确的答案。在 Dates 模块中，它给出的结果是2月28日。它是如何计算出来的呢？我喜欢想到赌场里经典的 7-7-7 赌博游戏。\n现在只要想象一下，老虎机不是 7-7-7，而是年-月-日，或者在我们的例子中，2014-01-31。当你要求在这个日期的基础上增加1个月的时候，月份槽就会递增，所以现在我们有 2014-02-31。然后检查日号是否大于新月份的最后有效日，如果大于（如上例），则日号向下调整到最后有效日（28）。这种方法的后果是什么呢？继续在我们的日期上再加一个月，2014-02-28 + Month(1) == 2014-03-28。什么？你是在期待3月的最后一天吗？不对，对不起，记得 7-7-7 的档期。尽可能少的槽位要改变，所以我们先把月份槽位递增1，2014-03-28，轰，我们就完成了，因为这是一个有效的日期。另一方面，如果我们要在原来的日期 2014-01-31 的基础上增加2个月，那么我们最终的结果是 2014-03-31，正如预期的那样。这种方法的另一个后果是，当强行进行特定的排序时，关联性会有所损失（即以不同的顺序添加东西会导致不同的结果）。比如说：\njulia\u0026gt; (Date(2014,1,29)+Dates.Day(1)) + Dates.Month(1) 2014-02-28 julia\u0026gt; (Date(2014,1,29)+Dates.Month(1)) + Dates.Day(1) 2014-03-01 那是怎么回事呢？在第一行中，我们在1月29日的基础上加1天，结果是 2014-01-30；然后再加1个月，于是得到 2014-02-30，再往下调整为 2014-02-28。在第二个例子中，我们先加1个月，我们得到 2014-02-29，再往下调整为 2014-02-28，然后再加1天，结果是 2014-03-01。在这种情况下，有一个设计原则是有帮助的，那就是在存在多个 Periods 的情况下，操作将按照 Periods 的类型来排序，而不是按照它们的值或位置顺序来排序；这意味着总是先加 Year，然后加 Month，再加 Week 等。因此，以下确实会导致关联性并正好有用:\njulia\u0026gt; Date(2014,1,29) + Dates.Day(1) + Dates.Month(1) 2014-03-01 julia\u0026gt; Date(2014,1,29) + Dates.Month(1) + Dates.Day(1) 2014-03-01 棘手吗？也许吧。一个无辜的 Dates 用户该怎么做？最重要的是要注意，当处理月份时，明确地强制执行某种关联性，可能会导致一些意想不到的结果，但除此之外，一切都应该按照预期工作。值得庆幸的是，在 UT 中处理时间时，日期-周期算术中的奇特情况几乎就是这样了（避免了处理夏令时、闰秒等的 \u0026ldquo;乐趣\u0026rdquo;）。\n作为奖励，所有的周期算术对象都可以直接与范围一起工作。\njulia\u0026gt; dr = Date(2014,1,29):Day(1):Date(2014,2,3) Date(\u0026#34;2014-01-29\u0026#34;):Day(1):Date(\u0026#34;2014-02-03\u0026#34;) julia\u0026gt; collect(dr) 6-element Array{Date,1}: 2014-01-29 2014-01-30 2014-01-31 2014-02-01 2014-02-02 2014-02-03 julia\u0026gt; dr = Date(2014,1,29):Dates.Month(1):Date(2014,07,29) Date(\u0026#34;2014-01-29\u0026#34;):Month(1):Date(\u0026#34;2014-07-29\u0026#34;) julia\u0026gt; collect(dr) 7-element Array{Date,1}: 2014-01-29 2014-02-28 2014-03-29 2014-04-29 2014-05-29 2014-06-29 2014-07-29 for i in Date(\u0026quot;2020-08-01\u0026quot;):Day(1):Date(\u0026quot;2020-08-09\u0026quot;) println(i) end 2020-08-01 2020-08-02 2020-08-03 2020-08-04 2020-08-05 2020-08-06 2020-08-07 2020-08-08 2020-08-09 调整器函数 尽管日期-周期算术很方便，但经常需要在日期上进行的计算具有日历或时间的性质，而不是固定的周期数。节日就是一个很好的例子，大多数都遵循这样的规则：\u0026ldquo;纪念日 = 五月的最后一个星期一\u0026rdquo;，或者 \u0026ldquo;感恩节 = 十一月的第四个星期四\u0026rdquo;。这类时间表达式处理的是相对于日历的规则，比如本月的第一天或最后一天，下周二，或第一个和第三个星期三等。\nDates 模块通过几个方便的方法提供了调整器 API，这些方法有助于简单、简洁地表达时间规则。第一组调整器方法处理周、月、季度和年的首尾。它们每个方法都接收一个单一的 TimeType 作为输入，并返回或调整到相对于输入的所需时期的第一个或最后一个。\njulia\u0026gt; Dates.firstdayofweek(Date(2014,7,16)) # Adjusts the input to the Monday of the input\u0026#39;s week 2014-07-14 julia\u0026gt; Dates.lastdayofmonth(Date(2014,7,16)) # Adjusts to the last day of the input\u0026#39;s month 2014-07-31 julia\u0026gt; Dates.lastdayofquarter(Date(2014,7,16)) # Adjusts to the last day of the input\u0026#39;s quarter 2014-09-30 接下来的两个高阶方法 tonext 和 toprev，通过将一个 DateFunction 和一个起始 TimeType 作为第一个参数来概括处理时间表达式。DateFunction 只是一个函数，通常是匿名的，它接受一个单一的 TimeType 作为输入，并返回一个 Bool，true 表示满足调整标准。例如:\njulia\u0026gt; istuesday = x-\u0026gt;Dates.dayofweek(x) == Dates.Tuesday; # Returns true if the day of the week of x is Tuesday julia\u0026gt; Dates.tonext(istuesday, Date(2014,7,13)) # 2014-07-13 is a Sunday 2014-07-15 julia\u0026gt; Dates.tonext(Date(2014,7,13), Dates.Tuesday) # Convenience method provided for day of the week adjustments 2014-07-15 这对于更复杂的时间表达式的 do-block 语法是很有用的。\njulia\u0026gt; Dates.tonext(Date(2014,7,13)) do x # Return true on the 4th Thursday of November (Thanksgiving) Dates.dayofweek(x) == Dates.Thursday \u0026amp;\u0026amp; Dates.dayofweekofmonth(x) == 4 \u0026amp;\u0026amp; Dates.month(x) == Dates.November end 2014-11-27 Base.filter 方法可以用来获取指定范围内的所有有效日期/时刻。\n# 匹兹堡街道清洁; 从 4月到11月的每第二个周二 # 日期范围从 2014-01-01 到 2015-01-01 julia\u0026gt; dr = Dates.Date(2014):Day(1):Dates.Date(2015); julia\u0026gt; filter(dr) do x Dates.dayofweek(x) == Dates.Tue \u0026amp;\u0026amp; Dates.April \u0026lt;= Dates.month(x) \u0026lt;= Dates.Nov \u0026amp;\u0026amp; Dates.dayofweekofmonth(x) == 2 end 8-element Array{Date,1}: 2014-04-08 2014-05-13 2014-06-10 2014-07-08 2014-08-12 2014-09-09 2014-10-14 2014-11-11 在 Raku 中上面的代码可以写成:\nlazy my @dates = Date.new(\u0026#39;2014-01-01\u0026#39;) ... Date.new(\u0026#39;2015-01-01\u0026#39;); .say for @dates.grep: -\u0026gt; $d { $d.day-of-week == 2 \u0026amp;\u0026amp; 4 \u0026lt;= $d.month \u0026lt;= 11 \u0026amp;\u0026amp; $d.weekday-of-month == 2 } 其他的例子和测试可以在 stdlib/Dates/test/adjusters.jl 中找到。\n","permalink":"https://ohmyweekly.github.io/notes/2020-08-04-dates-in-julia/","tags":["julia","dates"],"title":"Julia 中的日期和时间"},{"categories":["programming"],"contents":"Apache Flink 提供的 Table API 是一个统一的、关系型的 API，用于批处理和流处理，即在无边界的、实时的流或有边界的、批处理的数据集上以相同的语义执行查询，并产生相同的结果。Flink 中的 Table API 通常用于简化数据分析、数据管道化和 ETL 应用的定义。\n你要构建什么? 在本教程中，你将学习如何构建一个实时的仪表盘，以按账户跟踪金融交易。该管道将从 Kafka 读取数据，并将结果写入 MySQL，通过 Grafana 可视化。\n先决条件 本演练假设你对 Java 或 Scala 有一定的熟悉，但即使你来自不同的编程语言，你也应该能够跟上。它还假设你熟悉基本的关系概念，如 SELECT 和 GROUP BY 子句。\n救命, 我被卡住了! 如果你遇到困难，请查看社区支持资源。特别是 Apache Flink 的用户邮件列表，它一直是 Apache 项目中最活跃的一个，也是快速获得帮助的好方法。\n如何跟进 如果你想跟着走，你需要一台电脑与:\n Java 8 或 11 Maven Docker  所需的配置文件可在 flink-playgrounds 资源库中获得。下载后，在 IDE 中打开项目 flink-playground/table-walkthrough，并导航到文件 SpendReport。\nEnvironmentSettings settings = EnvironmentSettings.newInstance().build(); TableEnvironment tEnv = TableEnvironment.create(settings); tEnv.executeSql(\u0026#34;CREATE TABLE transactions (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; amount BIGINT,\\n\u0026#34; + \u0026#34; transaction_time TIMESTAMP(3),\\n\u0026#34; + \u0026#34; WATERMARK FOR transaction_time AS transaction_time - INTERVAL \u0026#39;5\u0026#39; SECOND\\n\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;topic\u0026#39; = \u0026#39;transactions\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); tEnv.executeSql(\u0026#34;CREATE TABLE spend_report (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; log_ts TIMESTAMP(3),\\n\u0026#34; + \u0026#34; amount BIGINT\\n,\u0026#34; + \u0026#34; PRIMARY KEY (account_id, log_ts) NOT ENFORCED\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://mysql:3306/sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;table-name\u0026#39; = \u0026#39;spend_report\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;driver\u0026#39; = \u0026#39;com.mysql.jdbc.Driver\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;username\u0026#39; = \u0026#39;sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;password\u0026#39; = \u0026#39;demo-sql\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); Table transactions = tEnv.from(\u0026#34;transactions\u0026#34;); report(transactions).executeInsert(\u0026#34;spend_report\u0026#34;); 拆解代码 The Execution Environment 前两行设置了你的 TableEnvironment。表环境是你如何为你的 Job 设置属性，指定你是在写批处理还是流式应用，以及创建你的源。本演练创建了一个使用流式执行的标准表环境。\nEnvironmentSettings settings = EnvironmentSettings.newInstance().build(); TableEnvironment tEnv = TableEnvironment.create(settings); 注册表 接下来，在当前目录中注册了表，您可以使用这些表连接到外部系统，以便读写批处理和流数据。表源提供对存储在外部系统中的数据的访问，如数据库、键值存储、消息队列或文件系统。table sink 向外部存储系统发出一个表。根据源和 sink 的类型，它们支持不同的格式，如 CSV、JSON、Avro 或 Parquet。\ntEnv.executeSql(\u0026#34;CREATE TABLE transactions (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; amount BIGINT,\\n\u0026#34; + \u0026#34; transaction_time TIMESTAMP(3),\\n\u0026#34; + \u0026#34; WATERMARK FOR transaction_time AS transaction_time - INTERVAL \u0026#39;5\u0026#39; SECOND\\n\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;topic\u0026#39; = \u0026#39;transactions\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); 注册了两个表：一个是交易输入表，一个是消费报告输出表。交易(transaction)表让我们可以读取信用卡交易，其中包含账户ID(account_id)、时间戳(transaction_time)和美元金额(amount)。该表是在一个名为 transactions 的 Kafka 主题上的逻辑视图，包含 CSV 数据。\ntEnv.executeSql(\u0026#34;CREATE TABLE spend_report (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; log_ts TIMESTAMP(3),\\n\u0026#34; + \u0026#34; amount BIGINT\\n,\u0026#34; + \u0026#34; PRIMARY KEY (account_id, log_ts) NOT ENFORCED\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://mysql:3306/sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;table-name\u0026#39; = \u0026#39;spend_report\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;driver\u0026#39; = \u0026#39;com.mysql.jdbc.Driver\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;username\u0026#39; = \u0026#39;sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;password\u0026#39; = \u0026#39;demo-sql\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); 第二张表 spend_report，存储了最终的汇总结果。其底层存储是 MySql 数据库中的一张表。\n查询 配置好环境和注册好表之后，你就可以构建你的第一个应用程序了。从 TableEnvironment 中，你可以从一个输入表中读取它的行，然后使用 executeInsert 将这些结果写入到一个输出表中。report 函数是你实现业务逻辑的地方。它目前还没有被实现。\nTable transactions = tEnv.from(\u0026#34;transactions\u0026#34;); report(transactions).executeInsert(\u0026#34;spend_report\u0026#34;); 测试 该项目包含一个二次测试类 SpendReportTest，用于验证报表的逻辑。它以批处理模式创建了一个表环境。\nEnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build(); TableEnvironment tEnv = TableEnvironment.create(settings); Flink 的独特属性之一是它在批处理和流式处理之间提供一致的语义。这意味着你可以在静态数据集上以批处理模式开发和测试应用程序，并以流式应用程序的形式部署到生产中。\n尝试一下 现在有了 Job 设置的骨架，你就可以添加一些业务逻辑了。目标是建立一个报告，显示每个账户在一天中每个小时的总支出。这意味着时间戳列需要从毫秒到小时的颗粒度进行舍入。\nFlink 支持用纯 SQL 或使用 Table API 开发关系型应用。Table API 是一个受 SQL 启发的流畅 DSL，可以用 Python、Java 或 Scala 编写，并支持强大的 IDE 集成。就像 SQL 查询一样，Table 程序可以选择所需的字段，并通过你的键进行分组。这些功能，加上内置的函数，如 floor 和 sum，写这个报告问题不大。\npublic static Table report(Table transactions) { return transactions.select( $(\u0026#34;account_id\u0026#34;), $(\u0026#34;transaction_time\u0026#34;).floor(TimeIntervalUnit.HOUR).as(\u0026#34;log_ts\u0026#34;), $(\u0026#34;amount\u0026#34;)) .groupBy($(\u0026#34;account_id\u0026#34;), $(\u0026#34;log_ts\u0026#34;)) .select( $(\u0026#34;account_id\u0026#34;), $(\u0026#34;log_ts\u0026#34;), $(\u0026#34;amount\u0026#34;).sum().as(\u0026#34;amount\u0026#34;)); } 用户定义的函数 Flink 包含有限的内置函数，有时你需要用用户定义的函数来扩展它。如果 floor 不是预定义的，你可以自己实现它。\nimport java.time.LocalDateTime; import java.time.temporal.ChronoUnit; import org.apache.flink.table.annotation.DataTypeHint; import org.apache.flink.table.functions.ScalarFunction; public class MyFloor extends ScalarFunction { public @DataTypeHint(\u0026#34;TIMESTAMP(3)\u0026#34;) LocalDateTime eval( @DataTypeHint(\u0026#34;TIMESTAMP(3)\u0026#34;) LocalDateTime timestamp) { return timestamp.truncatedTo(ChronoUnit.HOURS); } } 然后迅速将其集成到你的应用程序中。\npublic static Table report(Table transactions) { return transactions.select( $(\u0026#34;account_id\u0026#34;), call(MyFloor.class, $(\u0026#34;transaction_time\u0026#34;)).as(\u0026#34;log_ts\u0026#34;), $(\u0026#34;amount\u0026#34;)) .groupBy($(\u0026#34;account_id\u0026#34;), $(\u0026#34;log_ts\u0026#34;)) .select( $(\u0026#34;account_id\u0026#34;), $(\u0026#34;log_ts\u0026#34;), $(\u0026#34;amount\u0026#34;).sum().as(\u0026#34;amount\u0026#34;)); } 这个查询会消耗 transactions 表的所有记录，计算报表，并以高效、可扩展的方式输出结果。使用该实现运行测试将通过。\n添加窗口 基于时间的数据分组是数据处理中的典型操作，尤其是在处理无限流时。基于时间的分组被称为窗口，Flink 提供了灵活的窗口语义。最基本的窗口类型称为 Tumble 窗口，它有一个固定的大小，其桶不重叠。\npublic static Table report(Table transactions) { return transactions .window(Tumble.over(lit(1).hour()).on($(\u0026#34;transaction_time\u0026#34;)).as(\u0026#34;log_ts\u0026#34;)) .groupBy($(\u0026#34;account_id\u0026#34;), $(\u0026#34;log_ts\u0026#34;)) .select( $(\u0026#34;account_id\u0026#34;), $(\u0026#34;log_ts\u0026#34;).start().as(\u0026#34;log_ts\u0026#34;), $(\u0026#34;amount\u0026#34;).sum().as(\u0026#34;amount\u0026#34;)); } 这就定义了你的应用程序使用基于时间戳列的一小时滚动窗口。因此，时间戳为 2019-06-01 01:23:47 的行被放在 2019-06-01 01:00:00 窗口中。\n基于时间的聚合是独一无二的，因为在连续流应用中，时间与其他属性不同，一般是向前移动的。与 floor 和你的 UDF 不同，窗口函数是内在的，它允许运行时应用额外的优化。在批处理上下文中，窗口提供了一个方便的 API，用于通过时间戳属性对记录进行分组。\n用这个实现运行测试也会通过。\n再来一次，用流! 就这样，一个功能齐全的、有状态的、分布式的流式应用! 查询不断地消耗 Kafka 的事务流，计算每小时的花费，并在结果准备好后立即发出。由于输入是有界的，所以查询一直在运行，直到手动停止。而且由于 Job 使用了基于时间窗口的聚合，所以当框架知道某个窗口不会再有记录到达时，Flink 可以进行特定的优化，比如状态清理。\n表游乐场是完全 docker 化的，可以作为流式应用在本地运行。该环境包含一个 Kafka 主题、一个连续数据生成器、MySql 和 Grafana。\n从 table-walkthrough 文件夹内启动 docker-compose 脚本。\n$ docker-compose build $ docker-compose up -d 你可以通过 Flink 控制台查看正在运行的作业信息。\n从 MySQL 里面探索结果。\n$ docker-compose exec mysql mysql -Dsql-demo -usql-demo -pdemo-sql mysql\u0026gt; use sql-demo; Database changed mysql\u0026gt; select count(*) from spend_report; +----------+ | count(*) | +----------+ | 110 | +----------+ 最后，去 Grafana 看看完全可视化的结果吧!\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/try-flink/table_api.html\n","permalink":"https://ohmyweekly.github.io/notes/2020-07-30-table-api-in-flink/","tags":["flink","table","api"],"title":"Flink 中的 Table API"},{"categories":["programming"],"contents":"语法 数值字面量系数 在标识符或圆括号前面直接放一个数字, 例如 2x 或 2(x+y), 会被认为是把标识符和它前面的数字相乘。这样写多项式就很方便了。\n向量化的点号运算符 [1,2,3] .+ 3 3-element Array{Int64,1}: 4 5 6 .+ 类似于 Raku 中的 »+» 超运算符:\n[1,2,3] »+» 3 [4 5 6] 但是 Julia 的 Vectorized \u0026quot;dot\u0026quot; 语法没有 Raku 的超运算符语法清晰。\n类似的例子还有:\nsin(0.5) # 0.479425538604203 A = [0.5, 1.0, 1.5] sin.(A) 3-element Array{Float64,1}: 0.479425538604203 0.8414709848078965 0.9974949866040544 f(x,y) = 3x + 4y; A = [1.0, 2.0, 3.0]; B = [4.0, 5.0, 6.0]; f.(pi, A) 3-element Array{Float64,1}: 13.42477796076938 17.42477796076938 21.42477796076938 f.(A, pi) 3-element Array{Float64,1}: 15.566370614359172 18.566370614359172 21.566370614359172 f.(A,B) 3-element Array{Float64,1}: 19.0 26.0 33.0 等价的 Raku 写法为:\nsub f(\\x, \\y) { 3*x + 4*y}; my \\A = [1.0, 2.0, 3.0]; my \\B = [4.0, 5.0, 6.0]; A».\u0026amp;f(pi) [15.566370614359172 18.566370614359172 21.566370614359172] 链式比较 1 \u0026lt; 2 \u0026lt;= 2 \u0026lt; 3 == 3 \u0026gt; 2 \u0026gt;= 1 == 1 \u0026lt; 3 != 5 true Raku 同样支持这种链式比较。\n虚数 real(1 + 2im) # 1 imag(1 + 2im) # 2 (1 + 2im) * (1 - 2im) # 5 + 0im (1 + 2i).re # 1 (1 + 2i).im # 2 (1 + 2i) * (1 - 2i) # 5+0i 命名参数 function plot(x, y; style=\u0026#34;solid\u0026#34;, width=1, color=\u0026#34;black\u0026#34;) ### end plot(x, y, width=2) plot(x, y, :width =\u0026gt; 2) 函数组合 (sqrt ∘ +)(3,6) # 3.0 map(first ∘ reverse ∘ uppercase, split(\u0026#34;you can compose functions like this\u0026#34;)) 6-element Array{Char,1}: \u0026#39;U\u0026#39; \u0026#39;N\u0026#39; \u0026#39;E\u0026#39; \u0026#39;S\u0026#39; \u0026#39;E\u0026#39; \u0026#39;S\u0026#39; Piping 1:10 |\u0026gt; sum |\u0026gt; sqrt # 7.416198487095663 # 等价于 (sqrt ∘ sum)(1:10) # 7.416198487095663 广播和管道一起使用 [\u0026#34;a\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;strings\u0026#34;] .|\u0026gt; [uppercase, reverse, titlecase, length] 4-element Array{Any,1}: \u0026#34;A\u0026#34; \u0026#34;tsil\u0026#34; \u0026#34;Of\u0026#34; 7 组合类型  不可变组合类型  struct Foo bar baz::Int qux::Float64 end foo = Foo(\u0026#34;rakulang\u0026#34;, 6, 1.5) typeof(foo) # Foo typeof(Foo) # DataType foo.bar # rakulang foo.qux # 1.5 foo.qux = 1 # ERROR: setfield! immutable struct of type Foo cannot be changed  可变组合类型  mutable struct Bar baz qux::Float64 end bar = Bar(\u0026#34;rakudo\u0026#34;, 6.0) bar.baz = 1//2 bar.qux = 2.0 联合类型 IntOrString = Union{Int,AbstractString} 1 :: IntOrString # 1 \u0026#34;rakulang\u0026#34; :: IntOrString # rakulang 参数化类型  参数化组合类型  struct Point{T} x::T y::T end point=Point{Float64}(1.0, 2.0) point.x # 1.0 point.y # 2.0 struct Circle{T,U} x::T y::U end c = Circle{Float64,AbstractString}(6.0, \u0026#34;rakulang\u0026#34;) c.x # 6.0 c.y # rakulang 多重分派 f(x::Float64, y::Float64) = 2x + y f(x::Number, y::Number) = 2x - y methods(f) # 2 methods for generic function \u0026#34;f\u0026#34;: [1] f(x::Float64, y::Float64) in Main at REPL[33]:1 [2] f(x::Number, y::Number) in Main at REPL[34]:1 f(2.0, 3.0) # 7 f(2, 3.0) # 1 ","permalink":"https://ohmyweekly.github.io/notes/2020-07-27-learning-julialang/","tags":["julialang","julia"],"title":"Julia 语言学习笔记"},{"categories":["programming"],"contents":"函数式编程是一种编程风格，现代语言或多或少都支持这种风格。在这篇文章中，我想解释一下函数式编程如何为你提供强大的抽象，使你的代码更加简洁。我将用 Raku 和 Python 中的例子来说明这一点，我们将看到这两种语言都是函数式编程的优秀语言。\nRaku: 简介 本文的代码示例是用 Python 和 Raku 编写的。我想大多数人都熟悉 Python，但 Raku 不太为人所知，所以我先解释一下基础知识。本文中的代码不是很习惯，所以如果你懂得其他编程语言，应该很容易理解。\nRaku 与 Perl 最为相似。两种语言在语法上都与 C/C++、Java 和 JavaScript 相似：基于块，语句用分号隔开，块用大括号分界，参数列表放在括号中，用逗号隔开。将 Perl 和 Raku 与其他语言区分开来的主要特征是使用魔符（\u0026ldquo;有趣的字符\u0026rdquo;）来识别变量的类型：$ 代表标量，@ 代表数组，% 代表哈希（映射），\u0026amp; 代表子程序。变量也有关键字来标识它们的作用域，我只用 my 来标识变量的词法作用域。子程序是用 sub 关键字来声明的，子程序可以是命名的，也可以是匿名的。\nsub square ($x) { $x*$x; } # anonymous subroutine  my $anon_square = sub ($x) { $x*$x; } 在 Python 中，这将是：\ndef square(x): return x*x # anonymous subroutine  anon_square = lambda x: x*x Raku 支持无符号变量，并使用 \\ 语法来声明它们。更多关于普通变量和无符号变量之间的区别，请参见 Raku 文档。例如(say 打印它的参数，后面加一个换行)。\nmy \\x = 42; # sigilless my $y = 43; say x + $y; 在本文的代码中，我将尽可能地使用无符号变量。\nRaku 有几种类型的序列数据结构。在下面的代码中，我将使用列表和数组以及范围。在 Raku 中，列表和数组的主要区别在于，列表是不可变的，这意味着一旦创建，就不能修改。所以它是一个只读的数据结构。要\u0026quot;更新\u0026quot;一个不可变的数据结构，你需要创建一个更新的副本。另一方面，数组是可变的，所以我们可以更新它们的元素，扩展它们，缩小它们等等。所有的更新都发生在原始数据的位置上。\nRaku 的数组类似于 Python 的 list，Raku 的 list 类似于 Python 的 tuple，也是不可变的。除了语法之外，Raku 中的范围与 Python 中的范围相似，都是不可变的。\nmy @array1 = 1,2,3; #=\u0026gt;an array because of the \u0026#39;@\u0026#39; sigil my \\array2 = [1,2,3]; #= an array, because of the \u0026#39;[...]\u0026#39; my \\range1 = 1 .. 10; #=\u0026gt;a range 1 .. 10 my @array3 = 1 .. 10; #= an array from a range, because of the \u0026#39;@\u0026#39; sigil my \\list1 = 1,2,3; #=\u0026gt;a list my $list2 = (1,2,3); #= also a list my \\list3 = |(1 .. 10); #=\u0026gt;an array from a range because of the \u0026#39;|\u0026#39; flattening operation 相应的 Python 代码为:\nlist1 = list((1,2,3)) #=\u0026gt; a list from a tuple list2 = [1,2,3]; #=\u0026gt; a list, because of the \u0026#39;[...]\u0026#39; range1 = range(1,11) #=\u0026gt; a range 1 .. 10 list3 = list(range(1,11)); #=\u0026gt; a list from a range tuple1 = 1,2,3; #=\u0026gt; a tuple tuple2 = tuple([1,2,3]) #=\u0026gt; a tuple from a list tuple3 = tuple(range(1,11)) #=\u0026gt; creates a tuple from a range 其他具体的语法或功能将针对具体的例子进行解释。\n其他任何名称的函数 - 作为值的函数 函数是函数式编程的精髓。正如我在\u0026ldquo;万物皆函数\u0026rdquo;一文中所解释的那样，在适当的函数式语言中，所有的结构都是由函数构建的。\n所有现代编程语言都有函数、程序、子程序或方法的概念。它们是代码重用的重要机制。通常，我们认为函数是对一些输入值进行操作以产生一个或多个输出值的东西。输入值可以是全局声明的，也可以是一个类的属性，或者作为参数传递给函数。同样，输出值可以直接返回，到全局变量，作为类的属性或通过修改输入值。\n要想从函数式编程中获益最多，最好是函数是纯粹的，这意味着对函数的调用总是对相同的输入产生相同的输出。在实践中，如果函数只接受输入作为参数，并直接返回输出，这一点比较容易实现，但这并不是必不可少的。\n函数式编程的关键特征是，函数的输入值和输出值本身可以是函数。所以函数必须是你语言中的值。有时这被称为 \u0026ldquo;函数必须是一等公民\u0026rdquo;，一个接收和/或返回函数的函数有时被称为\u0026quot;高阶函数\u0026quot;。\n如果函数是值，那么我们就可以将它们赋值给变量。特别是我们会将它们赋值给其他函数的参数。但我们也可以将它们赋值给普通的变量。\n让我们考虑以下函数，choose，它需要三个参数 t，f 和 c。\n# Raku sub choose (\\t, \\f, \\d) { if (d) {t} else {f} } # Python def choose (t, f, d): if d: return t else: return f 首先让我们用字符串作为前两个参数的值来调用 choose。\n# Raku my \\tstr = \u0026#34;True!\u0026#34;; my \\fstr = \u0026#34;False!\u0026#34;; my \\res_str = choose(tstr, fstr, True); say res_str; #=\u0026gt;says \u0026#34;True!\u0026#34; # Python tstr = \u0026#34;True!\u0026#34; fstr = \u0026#34;False!\u0026#34; res_str = choose(tstr,fstr,True) print(res_str) #= says \u0026#34;True!\u0026#34; 现在让我们尝试用函数作为参数:\n# Raku sub tt (\\s){ say \u0026#34;True {s}!\u0026#34;} sub ff (\\s) { say \u0026#34;False {s}!\u0026#34;} my \u0026amp;res_f = choose(\u0026amp;tt, \u0026amp;ff, False); say \u0026amp;res_f; #=\u0026gt;says \u0026amp;ff res_f(\u0026#34;rumour\u0026#34;); #= says \u0026#34;False rumour!\u0026#34; # Python def tt(s):print(\u0026#34;True \u0026#34;+s+\u0026#34;!\u0026#34;)def ff(s):print(\u0026#34;False\u0026#34;+s+\u0026#34;!\u0026#34;)res_f =choose(tt,ff,True)print(res_f)#=\u0026gt; says \u0026lt;function tt at 0x7f829c3aa310\u0026gt; res_f(\u0026#34;rumour\u0026#34;)#=\u0026gt; says \u0026#34;False rumour!\u0026#34; 因此，我们的函数 choose 接收两个函数作为它的前两个参数，并返回一个函数。在 Raku 中，我们需要在函数名上加上 \u0026amp; 符号，因为否则它们会被求值：像 tt 这样的裸函数名就等于调用没有参数的函数 tt()。通过将这个函数赋值给一个变量(res_f)，我们现在可以将 res_f 作为一个函数来调用，它最终会根据选择来调用 tt 或 ff。\n函数不需要名字 现在，如果我们可以将函数赋值给变量，它们本身其实并不需要一个名字。所以我们的函数可以是匿名的。大多数语言都支持匿名函数，在函数式语言中，它们通常被称为 \u0026ldquo;lambda 函数\u0026rdquo;。在 Raku 中，我们有两种方法来创建匿名函数。\n使用 sub (...) 语法:\nmy \\tt = sub (\\s){ say \u0026#34;True {s}!\u0026#34;}; 或者使用\u0026lsquo;尖号块\u0026rsquo;语法，这样更紧凑一些:\nmy \\ff = -\u0026gt; \\s {say \u0026#34;False {s}!\u0026#34;}; Python 使用 lambda 关键字:\ntt = lambda s : print( \u0026#34;True \u0026#34;+s+\u0026#34;!\u0026#34; ) ff = lambda s : print( \u0026#34;False \u0026#34;+s+\u0026#34;!\u0026#34; ) 所以现在我们可以说:\nmy \u0026amp;res_f = choose(tt, ff, True); say \u0026amp;res_f; #=\u0026gt;says sub { } res_f(\u0026#34;story\u0026#34;); #= says \u0026#34;True story!\u0026#34; 当我们打印出函数所绑定的变量时，Raku 返回 sub { } 来表示该变量包含一个函数。\n在 Python 中:\nres_f = choose(tt, ff, True); print( res_f) #=\u0026gt; says \u0026lt;function \u0026lt;lambda\u0026gt; at 0x7f829b298b80\u0026gt; res_f(\u0026#34;story\u0026#34;) #=\u0026gt; says \u0026#34;True story!\u0026#34; 例子: map、 grep 和 reduce 函数的功能有很多用途，我只想重点介绍三个在 Raku 中现成的例子：map、reduce 和 grep。Python 有 map 和 filter，并通过 functools 模块提供 reduce。这些函数的共同点是，它们提供了一种对列表进行 for 循环的替代方法。\nmap : 对列表中的所有元素进行函数应用 map 有两个参数：一个函数和一个列表。它将函数按顺序应用于列表中的所有值，并返回结果，例如将列表中的所有值平方。\nmy \\res = map -\u0026gt; \\x {x*x} , 1 .. 10; 在 Python 中，我们需要显式地创建元组，但除了语法上的差异，结构是完全一样的。\nres = tuple( map( lambda x : x*x , range(1,11))) 这是对传统 for 循环的功能替代。\n# Raku my \\res = []; for 1 .. 10 -\u0026gt; \\x { res.push(x*x); } # Python res = [] for x in range(1,11): res.append(x*x) 请注意，在 Raku 和 Python 中，我们需要为 for 循环版本使用一个可变的数据结构，而 map 版本则使用不可变的数据结构。\ngrep : 过滤列表 grep (在 Python 中称为 filter)也接受参数，一个函数和一个列表，但它只返回函数返回真的列表中的值。\n# Raku my \\res = grep -\u0026gt; \\x { x % 5 == 0 }, 1 .. 30; # Python res = tuple(filter( lambda x : x % 5 == 0 ,range(1,31))) 当然我们也可以用 for 循环和 if 语句来写，但这又需要一个可变的数据结构。\n# Raku my \\res = []; for 1 .. 30 -\u0026gt; \\x { if (x % 5 == 0) { res.push(x); } } # Python res = [] for x in range(1,31): if (x % 5 == 0): res.append(x) map 和 grep 的好处是，你可以很容易地把它们链在一起。\n# Raku grep -\u0026gt; \\x { x % 5 == 0 }, map -\u0026gt; \\x {x*x}, 1..30 # Python res = tuple(filter( lambda x : x % 5 == 0 ,map( lambda x : x*x ,range(1,31)))) 这是因为 map 和 grep 接受一个列表并返回一个列表，所以只要你需要对一个列表进行操作，就可以通过链式调用来实现。\nreduce : 化整为零 reduce 也接受一个函数和一个 list，但它使用函数将 list 的所有元素合并成一个结果。所以函数必须接受两个参数。第二个参数是从列表中取出的元素，第一个参数作为状态变量来组合所有元素。例如，计算一个数字列表的和:\n# Raku my \\sum = reduce sub (\\acc,\\elt) {acc+elt}, 1 .. 10; say sum; #=\u0026gt;says 55 # Python from functools import reduce sum = reduce(lambda acc,elt: acc+elt, range(1,11)) print( sum); #= says 55 这里发生的情况是，首先将 acc 设置为列表中的第一个元素(1)，然后加上第二个元素，所以 acc 变成 1+2=3；然后加上第三个元素(3)，以此类推。其效果是将列表中的所有数字连续相加。\n为了更清楚地说明这一点，我们来写一个我们自己的 reduce 版本。\n编写你自己的 在许多函数式语言中，从左到右（从最低索引开始）和从右到左（从最高索引开始）的还原是有区别的。这一点很重要，因为根据做还原的函数，如果从左边或右边消耗列表，结果可能会不同。例如，假设我们的化简函数是\n# Raku -\u0026gt; \\x,\\y {x+y} # Python lambda x,y: x+y 那么我们从哪个方向遍历列表并不重要。但考虑以下函数:\n# Raku -\u0026gt; \\x,\\y { x \u0026lt; y ?? x+y !! x } # Python lambda x,y: x+y if x\u0026lt;y else x ( ... ?? ... !! ... 是条件操作符的 Raku 句法，在大多数其他语言中是 ... ? ... : ... 在 Python 中是 ... if ... else ...)。\n在这种情况下，如果列表从左或从右还原，结果会有所不同。在 Raku 和 Python 中，reduce 是一种从左到右的还原。\n另外，reduce 函数可以不使用列表的第一个元素，而是取一个额外的参数，通常称为累加器。在函数式语言中，reduce 通常被称为 fold，所以我们可以有一个左折和一个右折。让我们来看看如何实现这些。\nLeft fold 实现左折的直接方法（所以和 reduce 一样）是在函数内部使用 for 循环。这意味着我们必须在循环的每次迭代上更新累加器的值。在 Raku 中，无符号变量是不可变的（我在这里简化了，完整的故事请看 Raku 文档），所以我们需要使用一个有符号的变量，$acc。\n# Raku sub foldll (\u0026amp;f, \\iacc, \\lst) { my $acc = iacc; for lst -\u0026gt; \\elt { $acc = f($acc,elt); } $acc; } # Python def foldll (f, iacc, lst): acc = iacc for elt in lst: acc = f(acc,elt) return acc 如果我们只想使用不可变的变量，我们可以使用递归。Raku 使这一点变得简单，因为它允许一个子程序有多个签名(multi sub)，并且它会调用与签名相匹配的变量。\n我们的 foldl 将消耗输入列表 lst，并使用 f 将其元素组合到累加器 acc 中，当列表被消耗后，计算结束，我们可以返回 acc 作为结果。所以我们的第一个变体说，如果输入列表是空的，我们应该返回 acc。 第二个变体从列表中取出一个元素 elt (关于 * 的细节请参见 Raku 文档)，并将其与 acc 结合到 f(acc,elt) 中。然后用这个新的累加器和 list 的剩余部分 rest 再次调用 foldl。\n# When the list is empty, return the accumulator multi sub foldl (\u0026amp;f, \\acc, ()) { acc } multi sub foldl (\u0026amp;f, \\acc, \\lst) { # Raku\u0026#39;s way of splitting a list in the first elt and the rest # The \u0026#39;*\u0026#39; is a shorthand for the end of the list my (\\elt,\\rest) = lst[0, 1 .. * ]; # The actual recursion foldl( \u0026amp;f, f(acc, elt), rest); } Python 不允许这种模式匹配，所以我们需要使用条件来编写递归。\ndef foldl (f, acc, lst): if lst == (): return acc else: # Python\u0026#39;s way of splitting a tuple in the first elt and the rest # rest will be a list, not a tuple, but we\u0026#39;ll let that pass (elt,*rest) = lst # The actual recursion return foldl( f, f(acc, elt), rest) 在这个实现中，所有的变量都不会被更新。所以所有的变量都可以是不可变的。\nRight fold 右折与左折颇为相似。对于基于循环的版本，我们所做的只是将列表反转(reverse)。\n# Raku sub foldrl (\u0026amp;f, \\acc, \\lst) { my $res = acc; for lst.reverse -\u0026gt; \\elt { $res = f($res,elt); } $res; } # Python def foldlr (f, iacc, lst): acc = iacc for elt in lst.reverse(): acc = f(acc,elt) return acc 在递归版本中，我们从列表中取最后一个元素而不是第一个元素。关于 ..^ * - 1 语法的细节，请参见 Raku 文档。\n# Raku multi sub foldr ( \u0026amp;f, \\acc, ()) { acc } multi sub foldr (\u0026amp;f, \\acc, \\lst) { my (\\rest,\\elt) = lst[0..^*-1, * ]; foldr( \u0026amp;f, f(acc, elt), rest); } # Python def foldr (f, acc, lst): if lst == (): return acc else: (*rest,elt) = lst return foldr( f, f(acc, elt), rest) map and grep are folds 现在，map 和 grep 呢？我们当然可以用 for 循环来实现，但我们也可以用我们的 foldl 来实现它们。\n# Raku sub map (\u0026amp;f,\\lst) { foldl( sub (\\acc,\\elt) { (|acc,f(elt)) }, (), lst); } # Python def map (f,lst): return foldl( lambda acc,elt:(*acc, f(elt)) ,() ,lst ) 因为函数 f 是可映射的，所以它只有一个参数。但是 foldl 需要一个有两个参数的函数，第一个参数为累加器。所以我们用两个参数的匿名函数调用 foldl。累积器本身是一个空列表。虽然我们前面说过，还原将原来列表的所有元素合并成一个返回值，当然这个返回值可以是任何数据类型，所以也是一个列表。所以我们对原始列表中的每一个元素都调用 f，并将其添加到累加器列表的末尾。(| 将列表扁平化，所以 (|acc,f(elt)) 是一个由 acc 的元素和 f(elt) 的结果建立的新列表。)\n类似地，我们也可以定义 grep:\n# Raku sub grep (\u0026amp;f,\\lst) { foldl( sub (\\acc,\\elt) { if (f(elt)) { (|acc,elt) } else { acc } }, (), lst); } # Python def filter (f,lst): return foldl( lambda acc,elt: (*acc,elt) if f(elt) else acc , (), lst) 就像在 map 实现中一样，我们用一个匿名函数调用 foldl。在这个函数中，我们测试 lst 中的每个 elt 是否为 f(elt) 为真。如果是真，我们就从 acc 和 elt 创建一个新的列表，否则我们就只返回 acc。 因为 map 和 grep 分别对列表中的每个元素进行操作，所以我们也可以使用右折来实现它们。\n通过这些例子，我希望无论是对函数工作的概念，还是对函数可能的实现方式，都变得更加清晰。递归实现的优点是它允许我们使用不可变的数据结构。\n为什么是不可变的数据结构？ 你可能会好奇为什么我关注这些不可变的数据结构。正如我们将看到的那样，函数式编程与不可改变的数据结构配合得非常好。而且它们有一个很大的优势：你永远不用担心是否不小心修改了你的数据，也不用担心是否应该做一个副本来确定。所以使用不可变数据结构可以使代码不易出错，更容易调试。它们还具有潜在的性能优势。而我们接下来会看到，在 Raku 中还有另一个优势。\n返回函数的函数 函数也可以返回函数。如果我们想拥有一个可参数化的函数，这一点尤其有用。举个简单的例子，假设我们想要一系列以固定值递增一个数字的函数：add1、add2 等。当然，我们可以分别写出每一个函数。\n# Raku sub add_1 (\\x) {x+1} sub add_2 (\\x) {x+2} sub add_3 (\\x) {x+3} sub add_4 (\\x) {x+4} sub add_5 (\\x) {x+5} say add_1(4); #=\u0026gt;says 5 # Python def add_1 (x) : return x+1 def add_2 (x) : return x+2 def add_3 (x) : return x+3 def add_4 (x) : return x+4 def add_5 (x) : return x+5 print( add_1(4)) #= says 5 或者我们可以使用一个充满匿名函数的列表。\n# Raku my \\add = sub (\\x) {x}, sub (\\x) {x+1}, sub (\\x) {x+2}, sub (\\x) {x+3}, sub (\\x) {x+4}, sub (\\x) {x+5}; say add[0].(4); #=\u0026gt;says 5 # Python add = ( lambda x : x+1, lambda x : x+2, lambda x : x+3, lambda x : x+4, lambda x : x+5 ) print( add[0](4)) #= says 5 我们可以做得更好，用一个循环来填充一个匿名函数的数组。\n# Raku my \\add = []; for 0 .. 5 -\u0026gt; \\n { add.push(sub (\\x) {x+n}); } say add[1].(4); #=\u0026gt;says 5 # Python add = [] for n in range(0,6): add.append(lambda x: x+n) 我们每次循环迭代都会创建一个新的匿名函数，并将其添加到数组中。但是，我们可以使用一个函数来创建这些匿名函数，然后我们可以使用 map 来代替循环，并使用一个不可改变的数据结构。\n# Raku sub gen_add(\\n) { sub (\\x) {x+n} } my \\add = map \u0026amp;gen_add, 0..5; say add[1].(4); #=\u0026gt;says 5 # Python def gen_add(n): return lambda x : x+n add = tuple(map( gen_add, range(0,6))) print( add[1](4)) #= says 5 Laziness 在 Raku 中，使用(不可改变的)范围有一个额外的好处：我们可以将范围的末端设置为无穷大，在 Raku 中可以写成 ∞(unicode 221E)、* 或 Inf。\n# Raku my \\add = map \u0026amp;gen_add, 0 .. ∞; say add[244].(7124); #=\u0026gt;says 7368 这是一个所谓的\u0026quot;懒惰求值\u0026quot;的例子，简称 laziness：Raku 不会尝试（和失败）处理这个无限的列表。相反，它将在我们实际使用该列表中的一个元素时进行处理。表达式的评估会延迟到需要结果的时候，所以当我们调用 add[244] 时，发生的情况是 gen_add(244) 被调用来生成该函数。请注意，这在 for 循环中是行不通的，因为要使用 for 循环，我们需要一个可变的数据结构，而惰性列表必须是不可变的。所以这是一个很好的例子，说明函数式编程风格如何让你从懒惰中获益。\n这也是为什么我们递归地实现了 foldl，然后用它来实现我们自己的 map 和 grep：基于递归的版本不需要更新任何变量，所以它们可以与不可变的惰性数据结构一起工作。\n函数组合 我们在上面看到，你可以把 map 和 grep 的调用链在一起。通常情况下，你只需要将 map 调用链在一起，例如\n# Raku map -\u0026gt; \\x { x + 5 }, map -\u0026gt; \\x {x*x}, 1..30; # Python map( lambda x : x + 5, map( lambda x : x*x, range(1,31))) 在这种情况下，我们可以做得更有效率一些：比起创建一个列表，然后在这个列表上调用 map，我们可以通过组合函数一次完成两个计算。Raku 为此提供了一个特殊的操作符。\nmap -\u0026gt; \\x { x + 5 } ∘ -\u0026gt; \\x { x * x }, 1..30; 操作符 ∘（\u0026ldquo;环形操作符\u0026rdquo;，unicode 2218，但你也可以用普通的 o）是函数组成操作符，它的发音是 \u0026ldquo;after\u0026rdquo;，所以 f ∘ g 是 \u0026ldquo;f after g\u0026rdquo;。它的作用是将两个现有的函数组合起来，创建一个新的函数。\nmy \u0026amp;h = \u0026amp;f ∘ \u0026amp;g; 是下面的代码是一样的:\nsub h (\\x) { f(g(x)) } 组成运算符的优点是，它可以适用于任何函数，包括匿名函数。但实际上，它只是另一个高阶函数。它只是下面函数的运算符形式。\n# Raku sub compose(\u0026amp;f,\u0026amp;g) { sub (\\x) { f(g(x)) } } Python 没有函数组成操作符，但你也可以很容易地在 Python 中拥有 compose。\n# Python def compose(f,g): return lambda x: f(g(x)) 结论 在这篇文章中，我用 Raku 和 Python 的例子介绍了三种关键的函数式编程技术：对函数进行操作的函数、返回函数的函数和函数组成。我已经展示了你如何使用函数 map、reduce(折叠)和 grep(过滤)来操作不可变的列表。我已经解释了哟(如何用递归和不递归实现这样的函数，以及递归实现的优势是什么。下面是《 Raku 与 Python》一文中的代码。\n当然，函数式编程的内容还有很多，我也写了几篇更高级的文章。本文介绍的概念应该为理解那些更高级的主题打下良好的基础。如果你想了解更多关于函数式编程的知识，你可以考虑我的免费在线课程。\n原文: https://wimvanderbauwhede.github.io/articles/decluttering-with-functional-programming/\n","permalink":"https://ohmyweekly.github.io/notes/2020-07-26-cleaner-code-with-functional-programming/","tags":["raku","functional programming"],"title":"通过函数式编程实现更简洁的代码"},{"categories":["programming"],"contents":"问题 在 6.d 版本中, 很多东西都发生了变化, 我们至少需要发布一个版本。这里有个列表。然而, 覆盖面是不完整的。弃用通知采取了不同的形式, 一些新的类型和方法在那里, 一些则没有\u0026hellip;\u0026hellip;\n用 #2632 引用这个问题, 并检查项目, 当你的工作, 无论是通过改变后的文档, 看到没有变化, 需要做它。在这种情况下, 请通过评论或如何解释为什么是这种情况。\n版本控制的变更  [6.d] \u0026amp;await 在等待的时候不再阻塞线程 [6.d] whenever 不在 react 抛出的词法作用域内 [6.d] 在 sub MAIN 里面的 $*ARGFILES 总是由 $*IN 馈入 [6.d] 结构(字面上的) $()、@() 和 %() 不复存在 [6.d] 带有 :D/:U 类型约束的变量默认为约束类型的类型对象(例如, 你可以在它们身上使用 .new) [6.d] start 块在 sink 上下文中附加异常处理程序 [6.d] 例程必须使用 return-rw 来返回一个 Proxy, 即使例程被标记为 is raw 或 is rw [6.d] 原生的 num 类型默认为 0e0 而不是 NaN [6.d] 在子程序名中, 保留了键名为 sym 的冒号对（如:sym\u0026lt;foo\u0026gt;）, 以备将来使用  废弃 这些方法在 6.d 语言中已被废弃, 并将在 6.e 中被删除。实现者可以选择发出弃用警告, 或者在 6.e 版本发布后更长的时间内提供这些方法。\n 使用 '-' (单连字符)作为 \u0026amp;open 的特殊路径, 表示特殊的句柄(使用 IO::Special 对象代替) IO::Handle.slurp-rest (使用 .slurp 代替) Any.flatmap (使用.flat 和 .map 方法的组合来代替) Cool.path (使用 .IO 代替) Pair.freeze (使用去容器化的参数的 Pair.new 来代替) Str.subst-mutate (使用带有 .= 方法调用赋值元运算符的 Str.subst 代替) Rational.norm (现在 Rational 类型必须在创建时标准化) IO::Path.child (使用 .add 代替) \u0026amp;undefine (直接分配 Empty/Nil 代替) :count \u0026amp;lines/Str.lines 例程上的参数(使用所返回的 Seq 上的 .elems 代替) \u0026amp;is_approx in Test.pm6 (使用与 \u0026amp;is-approx 非常相似的行为来代替)  新的行为  通过新的可定义的 \u0026amp;RUN-MAIN、\u0026amp;ARGS-TO-CAPTURE 和 \u0026amp;GENERATE-USAGE 子例程改善 sub MAIN 的自定义处理 % 变量中的 QuantHash/Map 和 @ 变量中的 List 可以用 is 特性来声明（例如，my %h is Set） 新的 \u0026lt;ww\u0026gt; regex 规则: 只在单词内匹配 循环可以从上一条语句的值中产生一个值的列表 循环中的 next/last 收集其最后的语句值, 对它们运行的迭代返回 Empty .perl 可以在消耗过的 Seq、多维数组、Date 和 CallFrame 上调用 .gist 可以在 Attribute 上调用 对自动生成的 USAGE 信息进行了大量改进 is hidden-from-USAGE 特性，从自动生成的 USAGE 消息中隐藏 sub MAIN 候选者 Parameter.perl 包括可内省的默认值 %*ENV 值是同素异形的 尝试使用变量 $;、$,、$.、$\\、$(、$)、$\u0026lt;、$\u0026gt;、$/、$\\、$[、$-、$+ 和 $@ 会抛出 X::Syntax::Perl5Var 默认的 Hash.keyof 返回一个 Str(Any) 强转类型对象 非 ASCII 数字可以在 :42foo 冒号对快捷方式中使用 StrDistance 字符串化为 .after 字符串 更明确的 Pod 表格格式 Enumeration.enums 返回一个 Map 各种整数类型的 .Range 返回它们支持的值的范围 min/max 例程也适用于 Hash Signature 字面值可以包含字符串/数字字面值以及调用者标记 List.invert 通过所需的 Pair 绑定映射, 导致潜在的类型检查失败 :exists 可以与多维关联下标一起使用 动态创建的列表可以用来定义一个枚举 在 .first 中, Junction 可以作为匹配器使用 原生属性可以作为参数中的绑定目标 Proc 可以与其他 Proc 中的 IO::Pipe 一起工作 类型数组可以用 my SomeType @array 和 my @array of SomeType 创建 当把 Mixy 强转为 Setty/Baggy  时, 负数权重的项将被删除 :nth 副词在 m// 上接受一个 Junction 作为参数 CX::Warn' 和 CX::Done可以在CONTROL` phaser 中捕获 next 可用于 whenever 中 require 符号不再过境性地暴露出来 通过 {...} 进行多维访问, 类似于 [...] 的工作方式 在 END 时间打开的任何手柄都会自动关闭 在缓存的 Seq 上, 当 \u0026amp;infix:\u0026lt;eqv\u0026gt;、.Slip、.join、.List、.list、.eager、.Array 和 .is-lazy 被调用时, 就会使用缓存列表 IO::Handle.encoding 以 Nil 表示切换到二进制模式 is default 特质与属性一起工作 在多重分派中, 带有 is rw 特性的参数被认为比没有特性的参数窄 Array、Blob 和 Map 的 .gist 被裁剪成100个元素 新的 for 语句修饰符 hyper for、race for 和 lazy for for 循环自动序列化 RaceSeq/HyperSeq；使用新的 for 语句修饰符hyper for/race for避免 \u0026amp;infix:\u0026lt;does\u0026gt; 可用于 RHS 上的非组合实例 数值比较器可以与 DateTime  对象一起使用 Pod 保留空白类型 定义了带 @、% 和 \u0026amp; 魔符常数的语义  Math  Rational 总是在创建时被化简, 并在其一生中保持不变 Inf、Inf 和 NaN 可以分别用 \u0026lt;-1/0\u0026gt;、\u0026lt;1/0\u0026gt; 和 \u0026lt;0/0\u0026gt; 表示, 通过Rational 类型进行舍去。零分母 Rational 被标准化为这三个值之一 在 ±Inf 和 NaN 上调用 .Int, 会抛出异常 改进了 Num 运算符和数学函数的 IEEE 754-2008 合规性 负零 Num(-0e0)被所有例程和语法结构正确处理 Num 类型的字符串化必须是可舍弃到原始 Num 的 定义了涉及零的 Complex  指数 .expmod 中的负数幂有效  Sets、Bags、Mixes(aka QuantHashes)和集合运算符  Set 运算符可以用在任何对象上, 在需要的时候会被强转  所以, 不需要也不希望有任何预先的强转 如果没有 QuantHash 就能实现所需的功能, 那么 Set 运算符可以自由地不创建任何 QuantHash   对不同类型的 QuantHashes 的 Set 操作将强转到最自由的形式（Set -\u0026gt; Bag -\u0026gt; Mix） 集合运算符的 set_precedes 家族( (\u0026lt;+)、≼、(\u0026gt;+)、≽) 已被移除  曾经是子集运算符的 Baggy 形式 QuantHash 升级为最自由的形式, 所以 (\u0026lt;=)、⊆、(\u0026gt;=)、⊇ 做正确的事情   .classify-list 方法可用于 Baggy 类型 .categorize-list 方法可用于 Baggy 类型 .invert 方法可用于核心 QuantHash 类型 .antipairs 方法可用于 QuantHash 类型 QuantHash 类型有 .new-from-pairs 和将一个 QuantHash 类型转换为另一个 QuantHash 类型的方法(例如 Set 类型的 .Bag 方法) QuantHash 类型上的 .hash 对键值进行了字符串化  新的形参和实参  Date.new 接受一个 :\u0026amp;formatter .first 可以接受 :kv unique 和 .repeated 可以接受 :\u0026amp;as 和 :\u0026amp;with Test.pm6 中的 \u0026amp;plan 可以接受 :skip-all \u0026amp;run/\u0026amp;shell 可以接受 :merge \u0026amp;note 可以在没有参数的情况下调用 open 接受 :$out-buffer IO::Path.resolve 可以接受 :completely IO::Path.parent 可以接受一个 Int 表示父级 Proc::Async.new 吞噬位置参数 Signature.ACCEPTS 接受非 Signature/Capture 参数 \u0026amp;EVAL 可以接受一个 Blob Promise.keep/.break 可以在没有参数的情况下调用 原生数组上的 .sum 可以接受 :wrap is required 现在可以接受一个表示理由的参数 IO::Socket::Async.listen 可以绑定到端口 0 以向操作系统申请免费端口 .encode 可以接受 :translate-nl  新的例程和运算符  新的 atomicint Unicode 运算符和 ASCII 等价物, 保证线程安全, 原子操作: \u0026amp;infix:\u0026lt;⚛=\u0026gt;/\u0026amp;atomic-assign、\u0026amp;prefix:\u0026lt;⚛\u0026gt;/\u0026amp;atomic-fetch、 \u0026amp;prefix:\u0026lt;++⚛\u0026gt;/\u0026amp;atomic-inc-fetch、\u0026amp;postfix:\u0026lt;⚛++\u0026gt;/\u0026amp;atomic-fetch-inc、 \u0026amp;prefix:\u0026lt;--⚛\u0026gt;/\u0026amp;atomic-dec-fetch、\u0026amp;postfix:\u0026lt;⚛--\u0026gt;/\u0026amp;atomic-fetch-dec、 \u0026amp;infix:\u0026lt;⚛-=\u0026gt;/\u0026amp;infix:\u0026lt;⚛−=\u0026gt;/\u0026amp;atomic-fetch-sub 和 \u0026amp;infix:\u0026lt;⚛+=\u0026gt;/\u0026amp;atomic-fetch-add \u0026amp;cas: 原子比较与交换 ≤、≥ 和 ≠ 运算符是 Unicode 运算符, 分别等价于 \u0026lt;=、\u0026gt;= 和 != \u0026amp;infix:\u0026lt;unicmp\u0026gt;/\u0026amp;infix:\u0026lt;coll\u0026gt;: \u0026amp;infix:\u0026lt;cmp\u0026gt; 的替代行为 TR///: tr/// 的非变异版本 submethod TWEAK: 与 BUILD 类似, 除了它与属性默认值兼容之外 \u0026amp;duckmap: 应用 \u0026amp;callable 到每个元素上 \u0026amp;deepmap: 应用 \u0026amp;callable 到每个元素上, 下降到 Iterable 中 \u0026amp;take-rw: 像 \u0026amp;take 一样, 但有一个可写的容器 \u0026amp;indir: 在给定的 $*CWD 中执行代码 \u0026amp;spurt: 参见 IO::Path.spurt \u0026amp;prompt: 提示用户输入 uniprops: uniprop 的多字符版本 symlink: 建立文件符号链接 link: 创建文件硬连接 .hyper/.race: 并行处理值的列表 Seq.from-loop: 从 Callable 生产一个 Seq Str.uniparse: 将一个或多个 Unicode 字符名解析为实际字符 Str.parse-base: Int.base 操作的反转 IO::Path 提供了 .ACCEPTS、.SPEC、.CWD、.Numeric、.add、.extension、.mode 和各种文件测试、.parts、.sibling 和 .spurt IO::Handle 提供了 .READ、.WRITE、.EOF、.DESTROY, .readchars、.flush、.lock、.unlock、.out-buffer、.tell, .say、.slurp、.seek、.printf、.print-nl 和 .watch IO::Pipe 提供了 .proc Iterator 提供了 .skip-one、.skip-at-least 和 .skip-at-least-pull-one Mu.emit: \u0026amp;emit 的方法形式 Test.pm6 模块中的 \u0026amp;fails-like: 允许测试失败 Test.pm6 模块中的 \u0026amp;bail-out: 退出失败的测试套件 Test.pm6 模块中的 \u0026amp;is-approx: 测试一个数字近似于另一个 Buf 拥有 .allocate、.reallocate、.append、.push、.pop、.splice、.subbuf-rw、.prepend 和 .unshift 方法 Range 支持了 .rand Backtrace 拥有方法 .map、.flat、.concise 和 .summary .classify-list 方法可用于 Hash 类型 .categorize-list 方法可用于 Hash 类型 Code.of: 返回返回类型约束 Code.line/.file: 返回定义的行/文件 Proc::Async 提供了 .Supply、.ready、.pid、.bind-stdin、.bind-stdout 和 .bind-stderr Proc.command/Proc::Async.command: 我们要执行的命令 Proc 提供了 .signal、.pid 和 .encoding Complex 提供了 .cis、.reals、.ceiling、.floor、.round、.truncate 和 .abs 方法, 并可以使用 \u0026lt;=\u0026gt; 进行比较(只要虚部可以忽略不计) DateTime 提供了 .offset-in-hours、.hh-mm-ss 和 .Date DateTime 可以使用 \u0026lt;=\u0026gt; 运算符和其它 DateTime 对象进行比较 Date 提供了 .DateTime 方法 \u0026amp;infix:\u0026lt;+\u0026gt;/\u0026amp;infix:\u0026lt;-\u0026gt; 可以被 Duration、DateTime 和 Real 类型调用 Enumeration 提供了 .Int、.pred、.succ、.kv 和 .pair .Date 可以在 Instant 上调用 Junction 能使用 Junction.new 调用来创建 List 类型拥有 .to 和 .from 方法 Map type 提供了 Int 方法, 返回 pair 的数量 Any.skip: 跳过列表中的值 Any.batch: .rotor 的更基本的表兄弟 Mu.iterator: 为一个列表中的值生成一个 Iterator IO::Spec::* 类型提供了 .tmpdir、.extension 和 .path Pair 提供了 .ACCEPTS、.Pair 和 .invert .Capture 方法对所有核心类型都有明确定义 定义了 .ACCEPTS 在同素异形体上的语义 Failure.self 使未处理的 Failure 爆发 Thread.is-initial-thread: 我们是在初始线程中运行吗 Match 提供了 .Int 和 .actions IO::Socket::Async 提供了 .socket-port 和 .peer-port Promise 提供了另一种构造函器 .kept 和 .broken WhateverCode 提供了 .assuming WhateverCode 和 Block 提供了 .cando .:\u0026lt;…\u0026gt; 语法用于调用前缀运算符作为后缀 $*KERNEL 提供了 .hostname Nil 拥有定义的 .FALLBACK 特殊方法来返回 Nil  新类型  atomicint: 原生的 int, 大小可用于新的原子运算符 Lock::Async: 互斥的非阻塞机制 Encoding::Registry: 管理可用的编码 Encoding::Encoder: 编码器, 用于特定的编码 Encoding::Decoder: 解码器, 用于特定的编码 IO::CatHandle: 将多个只读的 IO::Handle 视同一个 原生的 str 数组 Supplier::Preserving: 缓存的实时 Supply 工厂 Semaphore: 控制多线程对共享资源的访问 IO::Special: 特殊I/O设备的路径 (例如 STDOUT) Exceptions::JSON 自定义异常处理程序的实现(可与PERL6_EXCEPTIONS_HANDLER 环境变量一起使用) SeekType 枚举: IO::Handle.seek 中使用的值  新的变量   $*USAGE: 可在 MAIN 子例程中使用, 包含自动生成的 USAGE 信息\n  %*SUB-MAIN-OPTS: 设置 sub MAIN 的行为\n %*SUB-MAIN-OPTS\u0026lt;named-anywhere\u0026gt; 允许将命名参数放在命令行的任何位置    $*COLLATION: 配置四个 Unicode 校对级别\n  $*INIT-INSTANT: 代表程序启动时间的 Instant\n  $*HOME: 用户的主目录, 如果存在的话\n  \u0026amp;*chdir: Callable 包含 IO::Path.chdir 的变体, 也设置进程的当前目录\n  PERL6_TEST_DIE_ON_FAIL 环境变量: 在第一次失败时停止测试套件\n  PERL6_EXCEPTIONS_HANDLER 环境变量: 指定自定义异常处理类\n  对边缘情况/强转行为的澄清  UInt 与 Int 类型对象智能匹配为 True sink 语句前缀爆炸 Failure 定义了1项和0项列表以及负参数和非整数参数的 permutations/combinations 的行为 \u0026amp;val、Str.Numeric 和其他 Str 数字转换方法在试图转换 Unicode No 字符组或合成数字时会 fail :42foo 冒号对快捷方式中不能使用合成数字 现在、Enumeration 可以作为一个数组形状指定器使用 含有空格的 Str 的数值转换现在返回 0 带空的模式参数的 samark, 简单地返回调用者 .polymod 可用于 lazy 但有限的除数列表 定义了 .[*-0] 索引 .rotor 中大于子列表的负数空隙抛出异常 .rotor 的非 Int 参数被强转为 Int 参数 读取 /proc 文件时定义了 .lines 定义了字符串上后缀/前缀 ++/-- 中泰语数字的行为 sunk for 里面的 map 被视为 sunk Sunk for 循环将上一条语句的方法调用值下沉 Bool 对象上的 .Int 返回一个 Int 对象 splice 可用于扩展数组 classify 可以与 Junction 配合使用 .pairup on a type object returns an empty Seq .pairup 总是返回一个 Seq 拒绝接受 Date/DateTime 构造函数中的合成代码点 ⸨/⸩ 对儿现在可以作为引号结构中的匹配字符使用 Array 类型对象上的 .flat 简单地返回该类型对象 混合级 classify 在 Hash 上抛出异常 Junction 可以用于给 Hash 指定多个键 给 .classify-list 的 Callable 现在保证每项只执行一次 :delete 对 Hash 类型对象进行关联查找时返回 Nil Test.pm6 中的 \u0026amp;is-deeply 会自动 .cache 作为参数的 Seq, 并使用返回的 List 进行测试 Complex.new() 给出 \u0026lt;0+0i\u0026gt; Int.new 现在可以保证构建一个新的 Int (而不是, 比如说, 从常量缓存中重用一个) 定义了一个参数(1-arg)版本的 \u0026amp;infix:\u0026lt;=:=\u0026gt; 和 \u0026amp;infix:\u0026lt;eqv\u0026gt; 如果直接或间接地调用 .BIND-POS、.BIND-KEY、.ASSIGN-POS、.ASSIGN-KEY、.STORE、.push、.append、.unshift、.prepend、Nil 类型现在抛出异常 Nil.ord 返回一个空的 Seq Nil.chrs 返回一个 \u0026quot;\\0\u0026quot; Num.new 强转参数为 Num infix:\u0026lt;Z\u0026gt;() 返回一个空的 Seq .comb 总是返回一个 Seq 用 \u0026amp;infix:\u0026lt;+\u0026gt; 化简一个项, 简单地返回该项 ()[0] 返回 Nil 允许在(可能是无限的) Seq 上使用 Regex 智能匹配 定义了 Range 对象的智能匹配 Set 转换为Mix/Bag 不再有 Bool 权重 当一个或多个操作数为 0 时、gcd 是有定义的 defined 例程中的 Junction 自动线程化 sum 可以处理含有 Junction 的列表 Grammar.parse 让顶级 regex 回溯 U+2212 MINUS SIGN [Sm] (-) 现在得到更多结构的支持, 如 Str.Numeric 和 \u0026amp;val Arity-1 \u0026amp;infix:\u0026lt;~\u0026gt; 与 Blob 可以一起工作 在签名中, 所有的 Numeric 字面值都支持作为值字面值 正则表达式中的 \\b 和 \\B 抛出 X::Obsolete True 和 False 作为签名中的值字面量发出警告 .sort 和 IO::Spec::Unix.path 的返回值总是 Seq Range 对象上的 Out-of-range .AT-POS 返回 Nil 对于不存在的键、Pair.AT-KEY 返回 Nil 所有的 Cool 类型都提供了 .Rat/.FatRat 强转器 IO::Path 文件测试不缓存先前测试执行的结果 Seq eqv List 仅根据类型不匹配就定为 False 在 Hash 、Hash  和 QuantHash 上, 来自 .kv、.values 和 .pair 序列的值是可写的 参见 Raku/roast#614 和 #3519 \u0026amp;infix:\u0026lt;∘\u0026gt;/\u0026amp;infix:\u0026lt;o\u0026gt; 保留 LHF 的 .of 和 RHS 的 .arity 和 .count 完善了 regex 运算符副词中的可接受参数(例如::in(…)) 完善了 IO::Handle.open 中可接受的参数组合 IO::Path.Str 不包含 .CWD 属性的值 IO::Path 类型拒绝带有 nul 字节 (\u0026quot;\\0\u0026quot;) 的路径 IO::Pipe 的 .path/.IO 返回一个 IO::Path 类型对象 如果目的路径和源路径是一样的 IO::Path 的 .copy/.move 会 fail dir 创建的 IO::Path 绝对性由调用者控制 更多定义的边缘行为、Callable  处理、. defined 调用, 以及 \u0026amp;infix:\u0026lt;andthen\u0026gt;、\u0026amp;infix:\u0026lt;orelse\u0026gt; 和 \u0026amp;infix:\u0026lt;notandthen\u0026gt; 操作符的链接 Seq 的禅切不缓存它们 List.Capture 将任何包含的 Pair 对象的键字符串化 带处理的 Failure 参数的 \u0026amp;fail 把它标记为是未处理的 use lib 接受 IO::Path 对象 锚点 ^、^^、$ 和 $$ 在环视中有效 Grammar.made 支持类型对象 .isa 支持 subset  类型对象 :delete 可用于惰性数组 \u0026amp;infix:\u0026lt;eqv\u0026gt; 可以在某些情况下对惰性参数起作用 动态查询(::(...)) 是限制性的 regex 语法, 并且需要 use MONKEY-SEE-NO-EVAL 的许可 定义了带孔数组的 .Slip 和 .List Promise.in/.at 和 Supply.interval 可以用零值和负值工作 Supply.interval 最小值为 0.001；较低值被处理为 0.001, 并发出警告#, 参见 https://docs.perl6.org/type/Supply#method_interval PR [#2649] Supply 提供了 .Seq、.list 和 .zip 可以在构建方法中绑定到原生类型属性 WhateverCode 传播 use fatal say、note、put、print 和 printf 例程自动线程化 Junction IO::Handle.eof 值在 .seek 过终点后再返回时也会相应改变 定义了 .succ'、.pred和.Bool` 的同质异形体 在核心 Numeric 上定义了 .Bridge 在核心 Numeric 的类型对象上定义了 .Numeric/.Real 定义了关于零分母有理数的 Rational.Bool say/note 保证在 Str 的子类上调用 .gist 定义了 Junction.Str 返回 Junction 定义了 Junction.gist/.perl 返回一个 Str Map/Hash 的 .list/.cache 返回一个 List 定义了 .round 的返回类型 定义了 Enumeration:D 不 .ACCEPT 一个 Enumeration:U , 参见 rakudo/rakudo#2073  杂项  IO::ArgFiles 类型只是 IO::CatHandle 的一个空的子类 对常量的约束  约束是完全强制的 试图在常量上使用参数化类型约束(例如使用 my Foo constant @int) 会引发 X::ParametricConstant 异常   Pod =defn(定义列表)指令可用 Pod 提供了 :numbered 配置键 .^ver、.^auth 和 .^name 元方法在 module 中可用, 而在 package 中则没有, 这是设计上的原因 qww\u0026lt;…\u0026gt; 中支持花哨的引号(’…’、“…”、｢…｣ 和变体) \u0026amp;infix:\u0026lt; \u0026gt; 支持查找自动生成的 Callables (例如: \u0026amp;infix:\u0026lt;XX\u0026gt;) 使用命名的 anon 子例程不再产生重声明警告 ::?MODULE/$?MODULE 变量的扩展规范 sub MAIN 可以接受一个参数上的 Enumeration 类型约束和 where 子句 笑脸型约束可以用在子集上 start 块和 thunks 得到新的 $/ 和 $! 定义了与列表关联运算符一起使用的 R 元运算符 类型强转可以用在签名返回类型约束中 \u0026amp;infix:\u0026lt;x\u0026gt;/\u0026amp;infix:\u0026lt;x\u0026gt; 抛出了 -Inf/NaN 重复参数 字面结构 put 和 put for 抛出, 需要使用括号 扩大了 Unicode 例程和功能的规范覆盖面-将覆盖面升级到 Unicode 第11版 $. 方法调用语法能用在元方法中了  ","permalink":"https://ohmyweekly.github.io/notes/2020-07-21-checklist-for-6-dot-d/","tags":["raku","6.d"],"title":"Checklist for Raku 6.d"},{"categories":["programming"],"contents":"https://imagemagick.org/Usage/draw/\n在 IM 中绘图是在现有图像中添加新元素的方法。虽然在复合字体效果的示例页和图像注释中涵盖了很多文本绘制的内容，但本页涉及 \u0026ldquo;-draw\u0026rdquo; 操作符的其他更普遍的方面。\n绘制命令最初是作为一种创建简单图像的手段。但随着时间的推移，它已经扩展成为矢量图形到光栅图像转换的界面。\nImageMagick 绘制命令 计算机中的图像通常以两种不同的方式保存。第一种也是你在这些示例页面中看到的最常见的方式被称为光栅图形。在这种方式中，图像是以像素的矩形阵列来存储的。\n另一种方式不太常见，也不太容易修改，但从另一个意义上讲，它的通用性更强，即对象矢量图形。在这种形式下，图像是用线条、弧线、颜色填充，有时还有深度来描述的。这是非常有用的，因为你可以将这些图像放大到你想要的任何尺寸，而且它们仍然可以完美地显示。与光栅格式的图像相比，您还可以在很小的空间内描述非常大和复杂的图像。\n矢量图形图像的例子包括 postscript 和新的 SVG-可缩放矢量图形。\nTrue-Type 字体也是矢量图形的例子，因为它允许在任何比例下使用单个字符描述。\n\u0026ldquo;-draw\u0026rdquo; 图像操作符，是进入 ImageMagick 矢量绘图功能的一个窗口，并形成了一套与 IM 的普通命令行图像操作符相当独立的命令。\n 一般使用的矢量图形文件格式只有几种，因为每一种这样的格式通常与其他这样的格式有很大的不同。其结果是，很少有代码共享的可能。 基于这个原因，ImageMagick 更关注使用矢量图形来绘制 SVG 格式的图像。Postscript 和 true-type 字体图形被传递给其他外部的\u0026quot;代理\u0026ldquo;库和应用程序，它们更适合绘制这些类型的矢量图形格式。 这并不是说 SVG 没有代理库。一个例子是 RSVG 库或 GTK SVG 库，这些库在编译时是可用的。IM 会链接到这些库来转换 SVG，而不是自己尝试去做。\n 原始绘图命令 让我们从 MVG 命令的 \u0026ldquo;-draw\u0026rdquo; 图像操作符中最古老、最简单、最常见的绘图原语开始。\n请注意，所有的参数都被视为浮点数，不一定是整数，比如我在这些例子中通常使用的。\n# Single Pixel Draw (两种方式 -- 这些像素点都被放大了) # Point \u0026#39;paints\u0026#39; the color pixel convert -size 10x6 xc:skyblue -fill black \\  -draw \u0026#39;point 3,2\u0026#39; -scale 100x60 draw_point.gif # Color Point \u0026#39;replaces\u0026#39; the color pixel convert -size 10x6 xc:skyblue -fill black \\  -draw \u0026#39;color 6,3 point\u0026#39; -scale 100x60 draw_color_point.gif 根据给出的注释，当涉及半透明颜色时，这两种点方法会产生不同的结果。详情请参见下面的颜色填充原语。\n# 矩形 / 圆角矩形 / 矩形圆弧 convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;rectangle 20,10 80,50\u0026#34; draw_rect.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;roundrectangle 20,10 80,50 20,15\u0026#34; draw_rrect.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;arc 20,10 80,50 0,360\u0026#34; draw_arc.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;arc 20,10 80,50 45,270\u0026#34; draw_arc_partial.gif arc 绘制原语与矩形一起列出，因为它实际上只是一个\u0026quot;椭圆\u0026rdquo;，装在两个坐标定义的\u0026quot;矩形\u0026quot;(rectangle)内。部分弧线很少使用，因为很难确定端点，除非角度限制在九十度的倍数。\ncircle 和 ellipse 原语涉及\u0026quot;中心\u0026quot;坐标与\u0026quot;边缘\u0026quot;坐标，或\u0026quot;大小\u0026quot;和\u0026quot;角度\u0026quot;值。\n# 圆 / 椭圆 (以某一点为中心) convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;circle 50,30 40,10\u0026#34; draw_circle.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;ellipse 50,30 40,20 0,360\u0026#34; draw_ellipse.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;ellipse 50,30 40,20 45,270\u0026#34; draw_ellipse_partial.gif 你也可以看看 Push/Pop 上下文，了解如何创建一个旋转的椭圆的例子。\n# 直线 / 折线 / 多边形 / 贝塞尔曲线 convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;line 20,50 90,10\u0026#34; draw_line.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;polyline 40,10 20,50 90,10 70,40\u0026#34; draw_polyline.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;polygon 40,10 20,50 90,10 70,40\u0026#34; draw_polygon.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;bezier 40,10 20,50 90,10 70,40\u0026#34; draw_bezier.gif 比较好的画直线和曲线的方法是使用 SVG 路径画法，它的用途更广，甚至可以实现\u0026quot;比例画线\u0026quot;。\n# text drawing / image convert -size 100x60 xc:skyblue -fill white -stroke black \\  -font Candice -pointsize 40 -gravity center \\  -draw \u0026#34;text 0,0 \u0026#39;Hello\u0026#39;\u0026#34; draw_text.gif convert -size 100x60 xc:skyblue -gravity center \\  -draw \u0026#34;image over 0,0 0,0 \u0026#39;terminal.gif\u0026#39;\u0026#34; draw_image.gif 最后这两个填充类型的操作是目前唯一受 \u0026ldquo;-gravity\u0026rdquo; 影响的绘制操作。这些操作的其他修饰符，包括 \u0026ldquo;-fill\u0026quot;、\u0026quot;-tile\u0026quot;、\u0026quot;-origin\u0026quot;、\u0026quot;-stroke\u0026quot;、\u0026quot;-strokeidth\u0026quot;、\u0026quot;-font\u0026quot;、\u0026quot;-pointsize\u0026quot;、\u0026quot;-box\u0026quot;。他们还有其他的修饰符，但这些修饰符与更高级的 Magick Vector Graphics 语言有关。\n贝赛尔原语 \u0026ldquo;bezier\u0026rdquo; 原语用于绘制曲线。每条命令只画一条曲线段。通常会给出 4 个点（8 个数字）：一个起点\u0026rsquo;结'、两个控制点和一个终点\u0026rsquo;结'。两个控制点定义了曲线的方向以及曲线偏离附加的终点\u0026rsquo;结\u0026rsquo;点的速度。\n为了顺利地将两条曲线连接起来，应该将端点的控制点通过\u0026rsquo;结\u0026rsquo;镜像，形成下一条贝塞尔曲线的控制点。例如这里我画了两条平稳连接在一起的贝赛尔曲线。请注意控制线和点（也是画出来的）是如何通过连接坐标直线镜像的，无论是角度还是长度。这一点很重要，否则曲线将不平滑。\npoints=\u0026#34;10,10 30,90 25,10 50,50 50,50 75,90 70,10 90,40\u0026#34; clines=`echo \u0026#34;$points\u0026#34; | sed \u0026#39;s/ /\\n/g\u0026#39; |\\  while read line; do echo \u0026#34;line $line\u0026#34;; done` symbols=`echo path \u0026#34;\u0026#39;\u0026#34;; for point in $points; do echo \u0026#34;M $pointl -2,-2 +4,+4 -2,-2 l -2,+2 +4,-4 -2,+2\u0026#34; done; echo \u0026#34;\u0026#39;\u0026#34;` convert -size 100x100 xc:skyblue -fill none \\  -draw \u0026#34;stroke gray $clinesstroke blue $symbols\u0026#34; \\  -draw \u0026#34;stroke red bezier 10,10 30,90 25,10 50,50 \u0026#34; \\  -draw \u0026#34;stroke red bezier 50,50 75,90 70,10 90,40 \u0026#34; \\  draw_bezier_joined.gif 如果我移动其中一个控制点，使它不从同一\u0026quot;结\u0026quot;的另一个控制点通过附加的\u0026quot;结\u0026quot;进行\u0026quot;反射\u0026rdquo;，那么曲线将不连续。\npoints=\u0026#34;10,10 30,90 25,10 50,50 50,50 80,50 70,10 90,40\u0026#34; clines=`echo \u0026#34;$points\u0026#34; | sed \u0026#39;s/ /\\n/g\u0026#39; |\\  while read line; do echo \u0026#34;line $line\u0026#34;; done` symbols=`echo path \u0026#34;\u0026#39;\u0026#34;; for point in $points; do echo \u0026#34;M $pointl -2,-2 +4,+4 -2,-2 l -2,+2 +4,-4 -2,+2\u0026#34; done; echo \u0026#34;\u0026#39;\u0026#34;` convert -size 100x100 xc:skyblue -fill none \\  -draw \u0026#34;stroke gray $clinesstroke blue $symbols\u0026#34; \\  -draw \u0026#34;stroke red bezier 10,10 30,90 25,10 50,50 \u0026#34; \\  -draw \u0026#34;stroke red bezier 50,50 80,50 70,10 90,40 \u0026#34; \\  draw_bezier_disjoint.gif 如果再次移动控制点，使其与相关的\u0026quot;结\u0026quot;点相匹配，线条将直接从该点出发，完全没有任何\u0026quot;曲线\u0026quot;。\npoints=\u0026#34;10,10 30,90 25,10 50,50 50,50 50,50 70,10 90,40\u0026#34; clines=`echo \u0026#34;$points\u0026#34; | sed \u0026#39;s/ /\\n/g\u0026#39; |\\  while read line; do echo \u0026#34;line $line\u0026#34;; done` symbols=`echo path \u0026#34;\u0026#39;\u0026#34;; for point in $points; do echo \u0026#34;M $pointl -2,-2 +4,+4 -2,-2 l -2,+2 +4,-4 -2,+2\u0026#34; done; echo \u0026#34;\u0026#39;\u0026#34;` convert -size 100x100 xc:skyblue -fill none \\  -draw \u0026#34;stroke gray $clinesstroke blue $symbols\u0026#34; \\  -draw \u0026#34;stroke red bezier 10,10 30,90 25,10 50,50 \u0026#34; \\  -draw \u0026#34;stroke red bezier 50,50 50,50 70,10 90,40 \u0026#34; \\  draw_bezier_no_curve.gif 如果两个控制点都设置为各自的\u0026quot;结点\u0026quot;，那么就会生成一条直线。\npoints=\u0026#34;10,10 10,10 50,50 50,50 50,50 50,50 90,40 90,40\u0026#34; clines=`echo \u0026#34;$points\u0026#34; | sed \u0026#39;s/ /\\n/g\u0026#39; |\\  while read line; do echo \u0026#34;line $line\u0026#34;; done` symbols=`echo path \u0026#34;\u0026#39;\u0026#34;; for point in $points; do echo \u0026#34;M $pointl -2,-2 +4,+4 -2,-2 l -2,+2 +4,-4 -2,+2\u0026#34; done; echo \u0026#34;\u0026#39;\u0026#34;` convert -size 100x100 xc:skyblue -fill none \\  -draw \u0026#34;stroke gray $clinesstroke blue $symbols\u0026#34; \\  -draw \u0026#34;stroke red bezier 10,10 10,10 50,50 50,50 \u0026#34; \\  -draw \u0026#34;stroke red bezier 50,50 50,50 90,40 90,40 \u0026#34; \\  draw_bezier_lines.gif 如果不指定所有 4 个点，\u0026lsquo;bezier\u0026rsquo; 原语并不真正有用。只有第一个点和最后一个点被归类为\u0026rsquo;结'，曲线将通过（或结束）这两个点。所有其他的点纯粹被视为控制点，按照给定的顺序对曲线产生影响，控制点越远，对该段曲线的影响越大。\npoints=\u0026#34;10,10 30,90 25,10 75,90 70,10 90,40\u0026#34; symbols=`for point in $points; do echo \u0026#34;M $pointl -2,-2 +4,+4 -2,-2 l -2,+2 +4,-4 -2,+2\u0026#34; done` convert -size 100x100 xc:skyblue -fill none \\  -draw \u0026#34;stroke gray polyline $points\u0026#34; \\  -draw \u0026#34;stroke red bezier $points\u0026#34; \\  -draw \u0026#34;stroke blue path \u0026#39;$symbols\u0026#39; \u0026#34; \\  draw_bezier_multi.gif 为了保持简单，不建议你每条 \u0026lsquo;bezier\u0026rsquo; 曲线段使用超过或少于4个点。\n其实我建议你完全不要使用 \u0026lsquo;bezier\u0026rsquo; 原语，而是使用 SVG Path Cubic Bezier 来生成曲线。它有一个特殊的 \u0026ldquo;s\u0026rdquo; 曲线延续功能，可以自动做相应的控制点\u0026quot;反射\u0026quot;，生成平滑连接的曲线段，减少你需要使用的控制点数量。您还可以定义相对于路径中最后一个端点的点。\n颜色填充原语 除了上述 \u0026ldquo;简单\u0026quot;原语之外，\u0026quot;-draw\u0026ldquo;还提供了一组颜色填充或修改原语。这些原语根据所选方法，从指定的点开始修改图像中的颜色。\n这些填充方法实际上不是真正的 \u0026lsquo;draw\u0026rsquo; 命令，而是颜色替换函数。它们被添加到绘图中，因为在程序的早期版本中，将它们的操作插入到 ImageMagick 中是最简单的。\n就像上面一样，使用的颜色是用 \u0026ldquo;-fill\u0026ldquo;颜色设置的，但如果设置了，就会使用 \u0026ldquo;-tile\u0026rdquo; 图像来代替。\n上面的其他设置选项没有使用，对这些操作没有影响。\n两个额外的设置也将应用于这些原语，即 \u0026ldquo;-bordercolor\u0026rdquo; 和 \u0026ldquo;-fuzz\u0026ldquo;系数设置。但是，这些设置不能在 \u0026ldquo;MVG\u0026rdquo; 语言中定义，因此只能在使用 \u0026ldquo;-draw\u0026rdquo; 操作符之前进行设置。\n第一个 \u0026lsquo;color point\u0026rsquo; 你已经看到了，它是上述例子中 \u0026lsquo;point\u0026rsquo; 绘制原语的替代。如果你仔细观察，你会看到我们在测试图像中设置的单个白色像素。\nconvert color_test.png -fill white \\  -draw \u0026#39;color 30,20 point\u0026#39; color_point.png 然而在绘制透明色和半透明色时，这些功能是不一样的。\n这里我们有一个三个像素的红色图像（放大了），第二个或中间的像素我们用 point 函数在红色像素上画上半透明的蓝色，得到紫色的结果。然而如果使用 color point 函数（最后一个或右边的像素），红色完全被半透明的蓝色像素所取代。它不会被叠加。\nconvert -size 3x1 xc:red -matte -fill \u0026#39;#00F8\u0026#39; \\  -draw \u0026#39;point 1,0\u0026#39; \\  -draw \u0026#39;color 2,0 point\u0026#39; -scale 33x33 draw_points.png 所有的 color 函数都会进行全色替换，而其他所有的颜色原语都会在图像上面\u0026rsquo;画\u0026rsquo;出颜色。因此，你可以使用 color 来绘制透明色。\ncolor replace 绘制函数将在指定的位置替换所有精确给定颜色的实例。而且正如您所看到的，这些区域不一定要连接在一起。\nconvert color_test.png -fill white \\  -draw \u0026#39;color 30,20 replace\u0026#39; color_replace.png convert color_test.png -fill white -fuzz 13%\\  -draw \u0026#39;color 30,20 replace\u0026#39; color_replace_fuzz.png 然而，正如你在第一个结果中所看到的，一些沿边缘的像素没有被替换。这些像素与所选像素的颜色不完全相同，所以它们被忽略了。添加一个小的模糊因子也会包含与原色相似的颜色。如上面第二个例子所示。\n当然，fuzz factor 并不是一个很好的解决方案，因为它不会捕捉所有这样的边缘像素。这是所有这些 color fill 方法经常出现的问题，也是一个没有通用解决方案的问题。\n如果你想替换一个特定的已知颜色，而不是从图像本身选择一种颜色，那么可以使用 \u0026ldquo;-opaque\u0026rdquo; 图像操作符来代替。该函数还使用 \u0026ldquo;-fuzz\u0026rdquo; 因子设置来增加与给定颜色相匹配的颜色范围。\nfloodfill 的方法也很简单，因为它只会填充所选点周围的整个区域，而不会选择任何其他没有以某种方式连接的类似颜色的区域。\n你也可以通过使用 \u0026ldquo;-fuzz\u0026rdquo; 来扩大被填充的区域，以包括相似的颜色。在这种情况下，我们选择了一个足够高的值，也包括了交叉边界，允许洪水填充\u0026quot;泄漏\u0026quot;到图像的另一侧。\nconvert color_test.png -fill white \\  -draw \u0026#39;color 30,20 floodfill\u0026#39; color_floodfill.png convert color_test.png -fill white -fuzz 15% \\  -draw \u0026#39;color 30,20 floodfill\u0026#39; color_floodfill_fuzz.png 用颜色填充区域并非没有问题。颜色可能会越过薄薄的边界，渗入到不想要的区域，(请看背景图案上的 GIF，以证明这一点)。或者，它可能无法填满所选区域的边缘，（见反锯齿和泛滥填充问题）。但它确实有效。\nfilltoborder 就像 floodfill 一样，只是你指定了一个颜色，这个颜色的边界是要填充的区域，而不是填充过程中要替换的颜色。\n当然也建议在该边框颜色选择中加入 similar colors 的模糊因子，以进一步限制洪水填充。\nconvert color_test.png -fill white -bordercolor royalblue \\  -draw \u0026#39;color 30,20 filltoborder\u0026#39; color_filltoborder.png convert color_test.png -fill white -bordercolor blue \\  -draw \u0026#39;color 30,20 filltoborder\u0026#39; color_filltoborder2.png convert color_test.png -fill white -bordercolor blue -fuzz 30% \\  -draw \u0026#39;color 30,20 filltoborder\u0026#39; color_filltoborder_fuzz.png 最后的绘制颜色方法是 reset，它只是将整个图像替换或重置为填充颜色。在这种情况下，实际选择的像素对结果没有任何影响。\nconvert color_test.png -fill white \\  -draw \u0026#39;color 30,20 reset\u0026#39; color_reset.png 这实际上是非常有用的，因为它提供了一种简单的方法从现有图像生成纯色（或平铺图像）画布。(请参见 Canvases Sized to an Existing Image)以了解此方法和其他做同样事情的方法。\n未来：使用 \u0026ldquo;-tile\u0026rdquo; 图案来填充该区域。\nMatt 填充原语 matte 绘制原语的工作方式与上述 color 原语完全相同，只是它不会替换所选区域的颜色，只会替换所选区域的 matte 通道。（也就是只有 alpha 或 matte 通道被这些填充函数调整）。\n就像 color 填充函数一样，matte 值使用的是填充色（除非用 \u0026ldquo;-tile\u0026rdquo; 作为 alpha value 的来源）。\n这里我们使用上面同样的 color floodfill 例子，但这里只调整 matte 通道，使填充部分完全透明。也就是说，原来的颜色仍然存在，只是透明而已!\nconvert color_test.png -fill none \\  -draw \u0026#39;matte 30,20 floodfill\u0026#39; matte_floodfill.png convert color_test.png -fill none -fuzz 15% \\  -draw \u0026#39;matte 30,20 floodfill\u0026#39; matte_floodfill_fuzz.png 也可以使用 matte reset 函数使整个图像变成半透明的。当然在这种情况下，我们必须输出到 PNG，它可以接受半透明颜色的图像。\nconvert color_test.png -fill \u0026#39;#00000080\u0026#39; \\  -draw \u0026#39;matte 30,20 reset\u0026#39; matte_reset.png 注意，在操作中没有使用 black 颜色分量，只使用了颜色的 matte 分量。图像的原色保持原样。\n未来：使用 \u0026ldquo;-tile\u0026rdquo; 模式来制作有趣的哑光效果。\ncolor 和 matte 都是完全替换颜色的函数，它总是会产生一个布尔（all or nothing）类型的颜色替换。因此，这些区域的边缘总是会显示出 Aliasing 效果。\n正因为如此，除了设置 GIF 图像的透明区域（也是布尔型）外，一般来说，这些都不是一般图像显影的好图像运算符。不过也不是全无用处，从背景去除的例子中可以看出。\n关于绘图命令的具体内容 像素坐标 [-draw](https://imagemagick.org/Usage/option_link.cgi?draw) 命令（以及IM中的许多其他命令）使用的是所谓的\u0026quot;像素坐标\u0026rdquo;。也就是 \u0026ldquo;10,10\u0026rdquo; 的坐标是左上角往下10个像素的中心。\n在这个坐标系中，0,0 是左上角像素的中心，w-1,h-1 是右下角的中心。实际的边缘位于 -0.5,-0.5 和 w-0.5,h-0.5，中心像素（如果图像是奇数大小）位于 \u0026lsquo;(w-1)/2,(h-1)/2\u0026rsquo;。\n然而，当您对图像进行数学处理时（如使用扭曲时），实际的像素没有实际意义，因此它使用\u0026quot;图像坐标\u0026rdquo;。在这个系统中，图像的实际边缘在 \u0026lsquo;0,0\u0026rsquo; 和 \u0026lsquo;w,h\u0026rsquo; 处。而图像的中心（可能是，也可能不是像素的中心）在 \u0026lsquo;w/2,h/2\u0026rsquo;。\n要将 \u0026lsquo;像素坐标\u0026rsquo; 转换为图像坐标，请加上 ½ 如左上角像素的中心是 \u0026lsquo;0.5,0.5\u0026rsquo;，右下角像素的中心是 \u0026lsquo;w-0.5,h-0.5\u0026rsquo;。 例如：小图像中的圆心\n绘制伽马和色域校正 和几乎所有的 ImageMagick 操作一样， [-draw](https://imagemagick.org/Usage/option_link.cgi?draw) 是一个线性运算符，因此在线性 RGB 色彩空间中工作。这意味着，为了得到平滑的边缘，你可能需要对图像进行一些伽玛校正，然后再保存，这样就可以使用非线性（伽玛校正）的sRGB色彩空间来存储。\n例如，如果你画了一个大圆，然后保存它\u0026hellip;\nconvert -size 81x81 xc:black -fill white -draw \u0026#39;circle 40,40 40,3\u0026#39; \\  circle_raw.png 看看圆圈的边缘，其实看起来并不是真的很光滑。你可以看到明显的阶梯效果。\n那是因为你是在线性 RGB 色彩空间中画的圆。但是你却把图像保存成了真正的 sRGB 色域！这就导致了你的图像在保存的过程中出现了明显的阶梯效果。\n为了解决这个问题，我们需要在保存图像之前给图像添加一个伽玛校正。\nconvert -size 81x81 xc:black -fill white -draw \u0026#39;circle 40,40 40,3\u0026#39; \\  -gamma 2.2 circle_gamma.png 现在，圆圈边缘实际上看起来光滑圆润，就像它们应该的那样。\n如果你想正确地做这件事，我们真的应该使用色彩空间进行修正。然而，由于IM假设RGB是保存的默认色彩空间，你需要做一些棘手的处理来让它正确地做事情。\nconvert -size 81x81 xc:black -set colorspace RGB \\  -fill white -draw \u0026#39;circle 40,40 40,3\u0026#39; \\  -colorspace sRGB circle_sRGB.png  请注意，sRGB色彩空间（这是保存图像的正确方法）与简单地应用2.2伽玛校正并不完全相同。然而，两者之间的结果差异很小，只有在非常非常粗糙的图像中才能看到。 在IM v6.7.5-1之前，色彩空间名称 \u0026ldquo;sRGB\u0026rdquo; 和 \u0026ldquo;RGB\u0026rdquo;（线性-RGB）实际上是颠倒的。因此，在旧版本的IM中，上面的两个标签应该被调换。\n 要使用真实的图像（在IMv6中）正确地绘制（或进行任何\u0026quot;线性\u0026quot;图像处理），你需要首先删除任何现有的伽玛，处理图像，然后恢复该伽玛校正。更多细节请参见使用色域校正调整大小。\n下面是一个在真实图像上绘图的例子\u0026hellip;。首先是没有任何颜色校正（原始），然后是伽玛和色域校正。\nconvert rose: -fill none -stroke white -draw \u0026#39;line 5,40 65,5\u0026#39; rose_raw.png convert rose: -gamma .454545 \\  -fill none -stroke white -draw \u0026#39;line 5,40 65,5\u0026#39; \\  -gamma 2.2 rose_gamma.png convert rose: -colorspace RGB \\  -fill none -stroke white -draw \u0026#39;line 5,40 65,5\u0026#39; \\  -colorspace sRGB rose_sRGB.png 正如你所看到的，通过使用伽玛或色彩空间校正，线条变得非常平滑，没有锯齿状的\u0026quot;楼梯\u0026quot;别离效果，比直接绘制时可以看到。(你需要一个很好的显示器才能看到它)\n 上面的线条是用 [-stroke](https://imagemagick.org/Usage/option_link.cgi?stroke) 颜色绘制的。您可以使用 [-fill](https://imagemagick.org/Usage/option_link.cgi?fill) 来绘制线条，并得到同样的结果，但这样您就不能使用 [-strokewidth](https://imagemagick.org/Usage/option_link.cgi?strokewidth) 来控制线条粗细。更多信息请参见下面的\u0026rdquo;描边颜色设置\u0026quot;。 色域名称实际上是使用\u0026rsquo;sRGB\u0026rsquo;色彩空间的值来定义的，但通过绘制应用，就好像图像是在线性RGB色彩空间中一样。因此，对命名的颜色（不是\u0026quot;白\u0026quot;或\u0026quot;黑\u0026rdquo;）使用上述伽玛校正将导致这些颜色变得失真。在这种情况下，最好不要使用伽玛或色域校正，这样命名的颜色就会正确映射。 命名的\u0026rsquo;sRGB\u0026rsquo;颜色与图像的色彩空间的正确映射，将作为IMv7开发的一部分被修正。\n 描边、描边宽度和填充的相互作用 [-stroke](https://imagemagick.org/Usage/option_link.cgi?stroke) 和 [-strokewidth](https://imagemagick.org/Usage/option_link.cgi?strokewidth) 选项用于在字体边缘绘制轮廓。\n这些选项通常与 [-fill](https://imagemagick.org/Usage/option_link.cgi?fill) 一起使用，以使文字更有趣，而不费力。\nconvert -size 380x70 xc:lightblue -pointsize 50 -font Chisel \\  -fill green -stroke black -draw \u0026#39;text 10,55 \u0026#34;Black Border\u0026#34;\u0026#39; \\  stroke_font.jpg 默认设置是 -strokewidth 1 和 -stroke None。\n但这样做会使轮廓笔画不可见，只留下 [-fill](https://imagemagick.org/Usage/option_link.cgi?fill) 的颜色，你不会看到它。\n当 -strokewidth 为 \u0026ldquo;不可见 \u0026ldquo;时， -strokewidth 唯一的效果是对字体大小属性的影响，也就是说它仍然可以影响字体定位和 Label 和 Caption 图片生成的大小。否则，宽度在你使笔画可见之前是没有可见效果的。\n为了了解 [-strokewidth](https://imagemagick.org/Usage/option_link.cgi?strokewidth) 对字体外观的实际影响（当使其可见时），我在这里绘制了一些不同宽度的文字，从 \u0026ldquo;turned off\u0026rdquo; 到越来越大。\nconvert -size 320x420 xc:lightblue -pointsize 70 -font Vademecum \\  -fill red -stroke none -draw \u0026#39;text 30,80 \u0026#34;Stroke -\u0026#34;\u0026#39; \\  -fill red -stroke black -strokewidth 0 -draw \u0026#39;text 30,160 \u0026#34;Stroke 0\u0026#34;\u0026#39; \\  -fill red -stroke black -strokewidth 1 -draw \u0026#39;text 30,240 \u0026#34;Stroke 1\u0026#34;\u0026#39; \\  -fill red -stroke black -strokewidth 2 -draw \u0026#39;text 30,320 \u0026#34;Stroke 2\u0026#34;\u0026#39; \\  -fill red -stroke black -strokewidth 3 -draw \u0026#39;text 30,400 \u0026#34;Stroke 3\u0026#34;\u0026#39; \\  stroke_table.jpg 请注意，从上面的例子中，设置 -strokewidth 为 \u0026ldquo;0\u0026rdquo; 与设置 -stroke 颜色为 \u0026ldquo;none\u0026rdquo;（默认值）是不同的。前者会画出一个非常非常细的笔触轮廓，而后者则会有效地关闭它。在这两种情况下，笔触仍然会被绘制。\n然而你也应该注意到，即使 -strokewidth 为 \u0026ldquo;0\u0026rdquo;，图像的轮廓也会比普通的 filled 图像（使用 -stroke 颜色为 \u0026ldquo;none\u0026rdquo;）扩大非常小。\n最后，使用任何小于 \u0026ldquo;1.0\u0026rdquo; 的宽度都不能正常工作。在这种情况下，你应该谨慎行事。\n但是请记住， -strokewidth 也是一个浮点设置。也就是说，\u0026ldquo;0.5\u0026rdquo; 的笔画宽度也是有效的。然而，通常只有当您试图在关闭防锐化功能的情况下绘制薄的位图圆时，这才是重要的。\n下面是一个使用超大笔触宽度的例子。\nconvert -size 320x100 xc:lightblue -font Candice -pointsize 72 -fill white \\  -stroke black -strokewidth 15 -draw \u0026#34;text 25,65 \u0026#39;Anthony\u0026#39;\u0026#34; \\  stroke_thick.jpg 请注意， -strokewidth 可以向内和向外扩展线条。下面是同样的例子，但是重新绘制了字体，没有笔画轮廓，去掉了很粗的笔画的内侧部分。\nconvert -size 320x100 xc:lightblue -font Candice -pointsize 72 -fill white \\  -stroke black -strokewidth 15 -draw \u0026#34;text 25,65 \u0026#39;Anthony\u0026#39;\u0026#34; \\  -stroke none -draw \u0026#34;text 25,65 \u0026#39;Anthony\u0026#39;\u0026#34; \\  stroke_outline.jpg 更多使用描边的例子请看复合字体效果。请特别看一下\u0026rdquo;气球效果\u0026quot;。\n绘制（描边）线条 IM中的默认画线有一些奇怪的行为，值得了解。下面就为大家介绍一下默认画线\u0026hellip;\nconvert -size 100x40 xc:lightblue \\  -draw \u0026#34;line 5,35 95,5\u0026#34; \\  line_default.jpg 你可以用 -fill 选项设置线条的颜色。\nconvert -size 100x40 xc:lightblue \\  -fill white -draw \u0026#34;line 5,35 95,5\u0026#34; \\  line.jpg 此外，你还可以通过设置 [-stroke](https://imagemagick.org/Usage/option_link.cgi?stroke) 颜色，使线条稍微粗一些。\nconvert -size 100x40 xc:lightblue \\  -fill white -stroke black -draw \u0026#34;line 5,35 95,5\u0026#34; \\  line_stroke.jpg 但是我们用 -fill 选项指定的白色是怎么回事呢？\n这就是在 ImageMagick 中绘制线条的棘手之处。该程序所做的实际上是将线条视为一个约1像素宽的填充对象。这是自然的，因为通常情况下，多条线通常用于扫出一个要填充的区域。\n所以，就像我们在上一节中使用字体的描边一样，IM 用填充色绘制线条（或对象），然后用描边色在其周围绘制。结果就是，现在上面的描边色线条稍微粗了一点，填充色完全隐藏在下面。如果你把描边色做成半透明的，就可以让这个填充色再次显现出来。\n综上所述，线条会出现在 -fill 颜色下绘制，但一旦 -stroke 颜色被定义为默认的\u0026quot;无\u0026quot;或\u0026quot;透明\u0026quot;颜色以外的颜色，该选项就没有任何意义了。\n 选项 -linewidth 实际上只是 -strokewidth 的别名，不应该被使用。\n 例如，你可能会认为这个命令会产生很粗的线条。确实如此，但由于 -stroke 的颜色是不可见的，所以你看不到它。你只能看到线条的一个像素宽的区域的内部\u0026quot;填充\u0026rdquo;。\nconvert -size 100x40 xc:lightblue \\  -fill white -strokewidth 3 -draw \u0026#34;line 5,35 95,5\u0026#34; \\  line_fill_3.jpg  以上的结果其实我认为是一个 BUG，什么都没画，因为没有\u0026rsquo;区域\u0026rsquo;填充，也没有设置线的\u0026rsquo;描边颜色'。什么都没有画，因为没有\u0026quot;区域 \u0026ldquo;需要填充，也没有设置线的\u0026quot;笔触颜色\u0026rdquo;。IM 目前这样做的原因是为了避免新用户的混淆，但实际上这只会给高级用户带来问题。详见《绘制填充边界》。\n 但如果同时定义了描边颜色，就会得到要求的粗线\u0026hellip;\nconvert -size 100x40 xc:lightblue \\  -stroke black -strokewidth 3 -draw \u0026#34;line 5,35 95,5\u0026#34; \\  line_stroke_3.jpg 如果将 -strokewidth 设置为1，则上面的一行将被完全覆盖。\nconvert -size 100x40 xc:lightblue \\  -stroke black -strokewidth 1 -draw \u0026#34;line 5,35 95,5\u0026#34; \\  line_stroke_1.jpg 当然当你掌握了这些知识后，你就可以利用这些知识进行创作，就像画字体一样。\nconvert -size 100x40 xc:lightblue \\  -stroke black -strokewidth 5 -draw \u0026#34;line 5,35 95,5\u0026#34; \\  -stroke white -strokewidth 2 -draw \u0026#34;line 5,35 95,5\u0026#34; \\  line_multi.jpg 在这里，我使用了最薄的 -strokewidth 设置为 \u0026ldquo;0\u0026rdquo;，就像我对上述字体所做的一样。\nconvert -size 100x40 xc:lightblue \\  -fill white -stroke black -strokewidth 0 -draw \u0026#34;line 5,35 95,5\u0026#34; \\  line_stroke_0.jpg 这就产生了一个非常奇怪的结果，即由黑点和灰段组成的点线。这是笔触、填充和背景色之间奇怪的\u0026quot;色拍频率\u0026quot;的结果。\n下面是线条的放大图\u0026hellip;\nconvert -size 25x10 xc:lightblue \\  -fill white -stroke black -strokewidth 0 -draw \u0026#34;line 2,8 22,1\u0026#34; \\  -scale 400% line_stroke_0_white.jpg  颜色节拍频率 \u0026ldquo;的效果与\u0026quot;声音节拍\u0026quot;的效果并无二致，当你有两把非常轻微的不调的吉他时，你会得到这样的效果。在这种情况下，你会得到一个黑点，其中笔触颜色完全覆盖了基本的填充颜色，你会得到一个灰色的点，其中笔触颜色与填充和背景颜色混合。 颜色混合是反锯齿过程的自然结果，IM使用反锯齿过程来尝试改善线条和其他绘制对象的外观。更多信息请参见IM 中的抗锯齿讨论和示例页面。\n 需要注意的是，这种效果只出现在倾斜的线条上，而不是纯水平或垂直的线条，因为在这些线条上，别名没有影响，因此也就没有\u0026quot;色拍频率\u0026quot;的效果。\nconvert -size 100x40 xc:lightblue \\  -fill white -stroke black -strokewidth 0 -draw \u0026#34;line 5,20 95,20\u0026#34; \\  line_stroke_horz.jpg 在这里，我在放大的视图上使用了不同的底层填充颜色，所以你可以看到颜色如何改变结果的节拍。\nconvert -size 25x10 xc:lightblue \\  -fill none -stroke black -strokewidth 0 -draw \u0026#34;line 2,8 22,1\u0026#34; \\  -scale 400% line_stroke_0_none.jpg convert -size 25x10 xc:lightblue \\  -fill red -stroke black -strokewidth 0 -draw \u0026#34;line 2,8 22,1\u0026#34; \\  -scale 400% line_stroke_0_red.jpg convert -size 25x10 xc:lightblue \\  -fill black -stroke black -strokewidth 0 -draw \u0026#34;line 2,8 22,1\u0026#34; \\  -scale 400% line_stroke_0_black.jpg 让我们把它和无 stroke 相比\u0026hellip;\nconvert -size 25x10 xc:lightblue \\  -fill black -stroke none -draw \u0026#34;line 2,8 22,1\u0026#34; \\  -scale 400% line_stroke_-_black.jpg 正如你所看到的，当绘制非常细的线条时，你可以通过使用相同的填充和描边颜色，或者将其中一种颜色设置为无来减少\u0026quot;节拍\u0026rdquo;。虽然后者是最好的主意，但前者可能对你的特定编程需求更实用。\n注意，填充线的粗细是 \u0026ldquo;0\u0026rdquo;。但描边线可以有更大的厚度。它也是一个浮点值! 2.5像素宽的线条是完全有效的。\n 这些结果不仅是由于笔画宽度为0的bug，导致颜色跳动，而且在没有实际需要填充的区域时，\u0026ldquo;填充色\u0026quot;被画出了额外的1.0直径厚度。这我也认为是一个bug。请看绘制填充边界。\n 绘制填充边界 关于各种绘制原语，还有一些其他要点需要您注意。\n笔画宽度对于大于 1.0 的浮点值工作得很好，但对于小于 1.0 的值似乎会崩溃。这是由于使用的实现算法造成的，而不仅仅是因为它是错误的，因为它在较大厚度的线条上工作得很好。\n基本上，如果你使用的笔画宽度为零，你可以期待没有笔画颜色会被添加。相反，你会得到一种节拍模式，当线条穿过像素的实际\u0026quot;中心\u0026quot;时，笔触颜色就会达到全部强度。\n真正应该发生的是，添加到像素上的颜色数量应该反映出被绘制的线条的面积，而不是像素与该线条的距离。因此，零宽度的线条不应该给图像添加任何颜色，而厚度小于1.0的线条应该只添加较少的颜色。\n请看上面的例子 Draw Lines, with StrokeWidth and Stroke。\n另一个问题是，填充颜色没有应用到正在绘制的形状（多边形）的边缘，而是应用到更远的半像素处。这包括没有应用\u0026quot;描边\u0026quot;的情况，而边缘应该是精确的。它也包括画一条\u0026quot;线\u0026rdquo;，它的填充厚度实际上是\u0026quot;零\u0026rdquo;。\n基本上，如果你画了一条线，没有启用描边，从技术上讲，你应该看到，没有线，因为它没有\u0026quot;填充\u0026quot;厚度。相反，线条的绘制至少包括1个像素宽的\u0026quot;填充\u0026quot;颜色。这是出于历史原因，一般来说可以避免新用户对IM的混淆。不幸的是，这对高级用户来说是不正确的。\n这意味着，如果您只使用填充色绘制两个多边形，并共享一个边缘，该边缘将重叠1个像素，因为每个多边形的所有边缘都比它大半像素。换句话说，多边形和其他形状并不适合在一起，而是重叠的。\n例如，在这里我试着使用 draw 把一张图片分成两半（在白色上绘制黑色）。要做到这一点，我画了两个多边形，共享一个边缘，完全没有重叠。由此产生的\u0026quot;微小\u0026quot;图像，已被放大显示。\nconvert -size 10x10 xc: -draw \u0026#39;polygon 2,-1 7,10 10,10 10,-1\u0026#39; bound_left.gif convert -size 10x10 xc: -draw \u0026#39;polygon 2,-1 7,10 -1,10 -1,-1\u0026#39; bound_right.gif convert bound_left.gif bound_right.gif -compose Plus -composite bound_add.gif 两个黑色的部分(这是实际绘制的)实际上是相互重叠的! 换句话说，尽管我们试图使用绘制的多边形分别绘制这两个区域，但填充的区域比要求的略大。\n我还将两张图片加（加合成）在一起，这样你就可以实际看到绘制的黑色区域的重叠。如果两个多边形是完美契合的，那么 \u0026ldquo;添加 \u0026ldquo;的图画将是纯白色的。\n实际的重叠量相当于默认的 -strokewidth 1.0 设置。因此，通常情况下，这个额外的区域会被一个正常的笔画宽度所覆盖。然而它可能会造成一些实际问题。\n旁白: 对于一个完整的连接测试，你会在黑色背景上生成50%的灰色区域，然后把它们加在一起。这样你就可以看到这些区域是否不仅 \u0026ldquo;重叠\u0026rdquo;（如上图所示），而且还可以测试当你把这些区域加在一起时，它们是否 \u0026ldquo;重叠不足\u0026rdquo;（在填充的区域之间留下一个间隙）。所得到的图像应该是一个完美平滑的50%灰色，沿连接处没有颜色变化。透明度检查会涉及到，在一个完全透明的背景上应该使用50%透明，50%灰色的颜色。\n要查看一个完美的剪切和重新添加的例子，基于一个单一的蒙版图像，请参阅组成方法的例子，组成 DstOut。\n未来BUG修复：填充的区域应该是精确的，但为了在绘制形状时进行补偿，默认的 \u0026ldquo;描边颜色 \u0026ldquo;应该设置为填充颜色（除非它本身被特别设置）。\nMVG - 魔法矢量图形 上面显示的原语构成了所有 -draw 操作的基础。它们共同构成了 ImageMagick 中特殊内部语言的起点，称为 Magick Vector Graphics 语言。关于这种语言的更多细节，请参见IM网站上的 MVG 原语和语法摘要。\n这种 \u0026ldquo;MVG\u0026rdquo; 语言的设计目标是让 ImageMagick 处理更复杂的 SVG（可缩放矢量图形）语言。它通过尝试将给定 SVG 格式的图像转换为更简单的内部 MVG 格式来实现。更多细节请看下面的 SVG 处理。\n因此，你上面看到的只是 -draw 操作符的一小部分功能。如果你想绘制复杂的对象，我建议你使用SVG编辑器（如 \u0026ldquo;Sodipodi\u0026rdquo;）为对象创建一个单独的SVG格式图像。参见下面的非IM矢量图形程序）。\n与 SVG 不同，MVG 没有任何形式的\u0026quot;容器\u0026quot;或图像命令集。在转换过程中，这些命令都被删除，以产生一个简化的 MVG 绘图命令序列。相反，它使用图形上下文的概念来保存和恢复各种绘图设置，这就是我们现在要看的。\n命令行设置与 MVG 设置 首先，您通过命令行选项设置的几乎所有设置，绘制原语使用的设置在 MVG 绘制命令中都有直接对应的内容。\n通过命令行选项（如 -strokewidth）或使用 MVG 绘图字符串（如 strok-width）中的设置，两者之间的主要区别在于 MVG 设置只在 MVG 命令字符串的持续时间内有效。\n一般绘图设置的总结:\n __cmd_option__ __draw_MVG__ __Argument__ -fill fill color/tile for inside shapes -tile fill image tile, replaces fill color -stroke stroke line color/tile around the shapes -strokewidth stroke-width pixel width +antialias stroke-antialias 0/1 aliasing line edges -font font font_name / font_file -family font-family ? -weight ? ? -stretch ? ? -pointsize font-size height in points -kerning - extra inter-character spacing +antialias text-antialias 0/1 aliasing drawing text -box text-undercolor fill color for font bounding box - decorate (None, Underline, LineThrough or Overline) -gravity gravity (None, North, South-East,...) -fuzz - color delta / percentage -bordercolor - color Notes:\n - no such option ? unknown 这些设置通常很好理解，因为它们经常使用，上面也有演示。\n 字体、拉伸、样式和重量用于从 ImageMagick 字体列表中识别字体。然而，大多数人只是选择一个特定的字体和大小点来代替使用。因此，它们在IM中很少使用。 正如您所看到的，\u0026ldquo;color fill\u0026rdquo; 原语的特殊设置在 MVG 中并没有直接对应的设置。这就是 -bordercolor 和 -fuzz 因子设置。在使用 -draw 操作符之前，必须从命令行指定这些设置。\n 有些 MVG 设置作为全局命令行设置可能更有用，比如字体绘制的 decorate 设置。\n警告： [-gravity](https://imagemagick.org/Usage/option_link.cgi?gravity) 不是 SVG 规范的一部分。在 MVG 中，它只用于文本和图像的放置以及对齐。目前没有与默认的\u0026quot;引力\u0026quot;效果分开的调整设置。然而，由于调整是 SVG 文本处理的一部分，这可能会在未来的某个时候改变。\n现在，全局命令行设置（在 MVG 绘制字符串之外）用于初始化你所应用的每个 -draw 操作的设置，这就是为什么你可以设置一个 -fill 颜色，然后你可以用它来绘制该颜色的圆。\nconvert -size 100x60 xc:skyblue -fill red \\  -draw \u0026#34;circle 50,30 40,10\u0026#34; draw_circle_global.gif 你可以在 MVG 参数 -draw 中本地覆盖全局设置\u0026hellip;\nconvert -size 100x60 xc:skyblue -fill red \\  -draw \u0026#34;fill green circle 50,30 40,10\u0026#34; draw_circle_override.gif 然而，在单个 -draw MVG 参数中设置的设置只在 -draw 操作期间存在。也就是说， -draw 中的设置只限于该次绘制，而不会带入以后单独的 -draw 参数中。\nconvert -size 100x60 xc:skyblue -fill red -draw \u0026#39;fill green\u0026#39; \\  -draw \u0026#34;circle 50,30 40,10\u0026#34; draw_circle_local.gif 如果你打算进行大量的操作，那么在单个 MVG 字符串中进行这些操作可能会比多个 -draw 操作更好。\nconvert -size 100x60 xc:skyblue \\  -draw \u0026#34;fill green circle 41,39 44,57 fill blue circle 59,39 56,57 fill red circle 50,21 50,3 \u0026#34; draw_circle_multi.gif MVG 特定设置 其他控制线条和对象绘制方式的 MVG 设置，即使在使用原语操作时也需要了解。这些设置包括\u0026hellip;\n __draw_MVG__ __Description/Argument__ fill-opacity fill transparency, from 0.0 to 1.0 clip-rule fill style for crossed lines (evenodd, nonzero) stroke-opacity line transparency, number from 0.0 to 1.0 stroke-dasharray list of 'on' and 'off' lengths for lines stroke-dash stroke-linecap End of line look: butt round square stroke-linejoin Lines joins: butt miter round square stroke-miterlimit Angle when 'miter' joins become 'bevel' (or 'butt') 记住，所有 MVG 设置和绘图操作符的完整列表可以在 IM 网站的 MVG 原语和语法摘要中看到。\n让我们看看一些简单设置的效果\u0026hellip;\u0026hellip;\n# Stroke Opacity convert -size 100x60 xc:skyblue -fill none -stroke black \\  -draw \u0026#34; path \u0026#39;M 10,10 L 90,10\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-opacity 0.8 path \u0026#39;M 10,20 L 90,20\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-opacity 0.6 path \u0026#39;M 10,30 L 90,30\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-opacity 0.4 path \u0026#39;M 10,40 L 90,40\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-opacity 0.2 path \u0026#39;M 10,50 L 90,50\u0026#39;\u0026#34; \\  set_stroke_opacity.gif # Fill Opacity convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34; rectangle 5,10 15,50 \u0026#34; \\  -draw \u0026#34;fill-opacity 0.8 rectangle 20,10 30,50 \u0026#34; \\  -draw \u0026#34;fill-opacity 0.6 rectangle 35,10 45,50 \u0026#34; \\  -draw \u0026#34;fill-opacity 0.4 rectangle 50,10 60,50 \u0026#34; \\  -draw \u0026#34;fill-opacity 0.2 rectangle 65,10 75,50 \u0026#34; \\  -draw \u0026#34;fill-opacity 0 rectangle 80,10 90,50 \u0026#34; \\  set_fill_opacity.gif # Plain and Dashed Lines convert -size 100x60 xc:skyblue -fill none -stroke black \\  -draw \u0026#34; path \u0026#39;M 10,10 L 90,10\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-dasharray 5 3 path \u0026#39;M 10,20 L 90,20\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-dasharray 5 5 path \u0026#39;M 10,30 L 90,30\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-dasharray 10 3 3 3 path \u0026#39;M 10,40 L 90,40\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-dasharray 1 6 path \u0026#39;M 10,50 L 90,50\u0026#39;\u0026#34; \\  set_lines.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34; path \u0026#39;M 10,10 L 90,10\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-dasharray 5 3 path \u0026#39;M 10,20 L 90,20\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-dasharray 5 5 path \u0026#39;M 10,30 L 90,30\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-dasharray 10 3 3 3 path \u0026#39;M 10,40 L 90,40\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-dasharray 1 6 path \u0026#39;M 10,50 L 90,50\u0026#39;\u0026#34; \\  set_lines_fill.gif # Note: Technically the second image should be the same as the first # as the \u0026#39;filled\u0026#39; lines contain no area. This I regard as a BUG. # Stroke Ends and Joins convert -size 100x60 xc:skyblue -fill white -stroke black -strokewidth 8 \\  -draw \u0026#34; path \u0026#39;M 20,20 L 20,70\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-linecap butt path \u0026#39;M 40,20 L 40,70\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-linecap round path \u0026#39;M 60,20 L 60,70\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-linecap square path \u0026#39;M 80,20 L 80,70\u0026#39;\u0026#34; \\  set_endcaps.gif convert -size 100x60 xc:skyblue -fill white -stroke black -strokewidth 5 \\  -draw \u0026#34; path \u0026#39;M 5,70 L 20,20 35,70\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-linejoin miter path \u0026#39;M 35,70 L 50,20 65,70\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-linejoin bevel path \u0026#39;M 55,70 L 70,20 85,70\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-linejoin round path \u0026#39;M 75,70 L 90,20 105,70\u0026#39;\u0026#34; \\  set_linejoin.gif convert -size 100x60 xc:skyblue -fill white -stroke black -strokewidth 5 \\  -draw \u0026#34; path \u0026#39;M 5,70 L 20,20 35,70\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-miterlimit 7 path \u0026#39;M 35,70 L 50,20 65,70\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke-miterlimit 6 path \u0026#39;M 65,70 L 80,20 95,70\u0026#39;\u0026#34; \\  set_miterlimit.gif stroke-miterlimit 的设置是相当难以演示的。这个属性定义了将 miter 连接变为 bevel 连接的角度。基本上对于非常尖锐的角度，一个斜面可以从两条线的实际接合处延伸很长的距离。这就为这个锐角设置了一个最大限度，当它变得太长时，就会使角点变钝。但请注意，它代表的是某种角度的三角值，而不是长度或距离。该值必须大于1.0。\n上面显示了对于我所显示的连接角度，斜角会突然转换成一个介于6到7之间的斜角。\n例如，1.414 的 troke-miterlimit 会将小于90度的任何角度的 \u0026ldquo;miter\u0026rdquo; 转换为 bevel 。4.0的 值（默认值）将小于约29度的角度转换为连接。而10.0的值可以将小于约11.5度的角度转换为斜面。\nSVG s路径绘制 SVG 路径是 SVG 的基本绘图原语。它用于绘制线型、圆、曲线、弧线等。SVG 路径的完整规范可以在 SVG 路径规范文档中找到。\n然而这并不是一个容易阅读的文档，因为它确实是为程序员而不是用户准备的，所以我将简化和总结路径规范\u0026hellip;\n 字母是命令，而所有的数字（浮点）都是参数 逗号或空格可以作为参数分隔符，否则完全忽略 每个路径组件的最后两个参数 (x,y) 将成为该路径组件的终点(或\u0026quot;结\u0026rdquo;) 大写字母是指最终点的绝对坐标 小写字母是相对于前一个组件的终点而言的 例如 \u0026ldquo;M 1,2 L 3,4 L 2,4\u0026rdquo; 和 \u0026ldquo;M 1,2 L 4,6 L 6,2\u0026rdquo; 是一样的。 即在1,2上加了3,4，画线到4,6。 然后在1,2上加2,4，画出一条线到最后的坐标6,2。 每个元素的参数可以重复，不需要重新发布相同的路径字母，可以多加数字参数组。不过对于曲线，为了方便阅读，我建议你还是加上函数字母。 重复的参数 \u0026ldquo;M\u0026rdquo; 或 \u0026ldquo;m\u0026rdquo; 分别作为 \u0026ldquo;L\u0026rdquo; 或 \u0026ldquo;l\u0026rdquo; 处理。 例如：\u0026rdquo; M 1,2 3,4 5,6 \u0026quot; 和 \u0026quot; M 1,2 L 3,4 L 5,6 \u0026quot; 是一样的。 而：\u0026ldquo;m 1,2 3,4 2,4 \u0026quot; 与 \u0026quot; m 1,2 l 3,4 l 2,4 \u0026quot; 相同。 对于立方贝塞尔，所有的点（控制点和结点）都是相对于前一个路径组件的端点而言的。  请注意，您可以用绝对坐标或相对坐标来指定对象。因此，你可以用相对坐标来定义一个对象，只需提供一个初始的绝对\u0026quot;移动\u0026quot;坐标来定位整个路径。\n另一方面，你也可以使用其他的\u0026quot;图形内容\u0026quot;命令来移动整个图形在\u0026quot;视图框\u0026quot;或\u0026quot;转换\u0026quot;中的位置（见下文）。因此，在 SVG 路径中使用绝对或相对坐标并不重要。 移动、线条和路径闭合是学习 SVG 对象路径的最初起点。\n# Open, Completed and Closed Paths (same points) convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 40,10 L 20,50 90,10 70,40\u0026#39;\u0026#34; path_open.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 40,10 L 20,50 90,10 70,40 40,10\u0026#39;\u0026#34; path_complete.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 40,10 20,50 90,10 70,40 Z\u0026#39;\u0026#34; path_closed.gif 但是请注意，\u0026lsquo;z\u0026rsquo; 只是关闭循环。它并没有创建一个单独的对象。因此，两个\u0026quot;关闭\u0026quot;的路径仍然被归类为一个单一的绘制对象，无论它们是重叠的还是完全断开的。\n这里我们展示了两个闭合但重叠的循环，在同一方向上绘制。由于只使用了一条路径，所以对象是一个单一的对象，fill-rule 设置控制了重叠区域的填充方式。\n# Overlapping Paths and Fill Rule convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;fill-rule evenodd \\ path \u0026#39;M 40,10 20,20 70,50 Z M 20,40 70,40 90,10 Z\u0026#39; \u0026#34; path_evenodd.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;fill-rule nonzero \\ path \u0026#39;M 40,10 20,20 70,50 Z M 20,40 70,40 90,10 Z\u0026#39; \u0026#34; path_nonzero.gif 由于对象围绕中心以相同的角度方向绘制，两个闭合的循环将包含一个周期值为2的区域，因此 evenodd 规则使得该区域未被填充，而非零的 nonzero 规则则将其填充。但是请注意，所有的路径都是可见的，因为它们实际上是同一个对象。\n绘制路径的方向是非常重要的，一般情况下，所有的路径相对于对象的\u0026rsquo;内部\u0026rsquo;应该绘制在完全相同的方向上。\n例如这里我将第二个对象画成与第一个对象相反的方向。因此，当两个对象重叠时，该区域被圈出 \u0026lsquo;0\u0026rsquo; 次。也就是说，无论使用什么 fill-rule，它都将是未被填充的，形成一个\u0026rsquo;洞'。\n# Overlapping Closed Objects, Second object drawn in reverse convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;fill-rule evenodd \\ path \u0026#39;M 40,10 20,20 70,50 Z M 20,40 90,10 70,40 Z\u0026#39; \u0026#34; path_rvs_evenodd.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;fill-rule nonzero \\ path \u0026#39;M 40,10 20,20 70,50 Z M 20,40 90,10 70,40 Z\u0026#39; \u0026#34; path_rvs_nonzero.gif 这意味着你可以在物体上产生一个\u0026quot;洞\u0026rdquo;，通过反转方向，使物体的\u0026quot;内部\u0026quot;保持在行进方向的同一侧。\n# An object with a reversed drawn hole! convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,10 20,55 70,50 80,5 Z M 50,20 60,40 40,30 Z\u0026#39; \u0026#34; path_with_hole.gif 无论 fill-rule 的设置如何，结果都是一样的，因为这个洞是\u0026rsquo;偶数\u0026rsquo;和\u0026rsquo;零'，所以是未填充的。\n当然，如果您使用一个完全独立的 path 元素，您将生成一个完全独立的对象。在这种情况下，fill-rule 不适用，而只是按照给定的顺序，将对象画在彼此的上方。\n# Separate paths are separate objects convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 40,10 20,20 70,50 Z\u0026#39; path \u0026#39;M 20,40 70,40 90,10 Z\u0026#39; \u0026#34; path_separate.gif 未来：坐标对准路径 \u0026ldquo;H\u0026rdquo; 和 \u0026ldquo;V\u0026rdquo;。\n椭圆弧是 SVG 路径的圆圈绘制功能\u0026hellip;\nlarge 和 sweep 参数特别重要，因为它们用于决定从起点到终点的四种弧线中的哪一种。\nlarge 和 sweep 这两个标志定义了四条弧线中哪一条弧线将连接这两个点。\n# Elliptical Arcs : A radius_x,y angle large,sweep x,y convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 30,15 0 0,0 70,20\u0026#39;\u0026#34; path_arc.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 30,15 0 0,1 70,20\u0026#39;\u0026#34; path_arc2.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 30,15 0 1,0 70,20\u0026#39;\u0026#34; path_arc3.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 30,15 0 1,1 70,20\u0026#39;\u0026#34; path_arc4.gif 第二个标志 sweep 简单地决定了弧线路径的方向的哪一边应该被绘制。\nlarge 标志是用来选择较长的路径，绕着椭圆的中心走。这是设置的角度的弧线将大于180度。如果关闭，你会得到较小的\u0026rsquo;弧'，不包含椭圆的中心，并且弧线的角度小于180度。\n用 \u0026ldquo;Z\u0026rdquo; 来关闭弧线，只是画出最后的直线段。\n要创建一个完整的椭圆或圆，你至少需要两个\u0026rsquo;弧线\u0026rsquo;段，从第一点到第二点，然后回到第一点。两条弧线都应该有相同的 sweep 设置，所以弧线将在不同的边上，有不同的移动方向。其中一条弧线应该有 large 的设置。\n# Closed and angled elliptical arcs (defined by two edge points) convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 30,20 20 0,0 70,20 Z \u0026#39;\u0026#34; path_arc5.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 30,20 20 1,1 70,20 Z \u0026#39;\u0026#34; path_arc6.gif convert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 30,20 20 0,0 70,20 \\ A 30,20 20 1,0 30,40 Z \u0026#39;\u0026#34; path_arc7.gif 请注意，如果直线太长，在给定的角度下无法适应给定的椭圆大小，椭圆的大小将被放大，以适应以椭圆为中心的直线，这意味着通过使用小数字作为轴半径，你可以只指定轴长的比例，并保证直线路径通过椭圆的中心点。\n这意味着，通过使用小数字的轴半径，你可以只指定一个轴长的比例，并保证直线路径穿过椭圆的中心点。也就是说，路径从椭圆的一侧到另一侧形成一个椭圆直径。这并不是椭圆的主轴或次轴，只是一个椭圆的直径。\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 3,2 45 0,0 70,20\u0026#39;\u0026#34; path_arc_x.gif 当然，使用长度为 \u0026ldquo;1,1\u0026rdquo; 的结果是一个完美的半圆，从一个点，到下一个点。在这种情况下，椭圆角不会有任何区别。\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 1,1 0 0,0 70,20\u0026#39;\u0026#34; path_hcircle.gif 对于以两点为中心的全圆，用\u0026hellip;\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 1,1 0 0,0 70,20 A 1,1 0 1,0 30,40 Z\u0026#39;\u0026#34; path_circle.gif SVG 对 \u0026ldquo;弧线\u0026quot;的定义也声明，如果两个半径中的任何一个是0，那么就应该画一条直线。因此，任何半径为 \u0026ldquo;0,0\u0026rdquo; 的圆弧，都只是一条简单的直线圆弧\u0026hellip;\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 0,0 0 0,0 70,20\u0026#39;\u0026#34; path_arc_line.gif 如果你为弧线指定了一个非常大的半径，而没有为回程路径指定 large sweep，你可以在两点之间创建该半径的透镜形状。\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 30,40 A 50,50 0 0,0 70,20 A 50,50 0 0,0 30,40 Z\u0026#39;\u0026#34; path_lens.gif 这种类型的弧线是一个关键特征。它可以让你很容易地把原本是直线的东西变成一条小而明显的曲线。\n例如，与其说是一个简单的三角形，不如说是一个\u0026hellip;\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 20,55 L 25,10 L 70,5 L 20,55 Z\u0026#39; \u0026#34; triangle.gif 你可以用一个大半径的弧线代替每条线，让它们只是有轻微的曲线。\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 20,55 A 100,100 0 0,0 25,10 A 100,100 0 0,0 70,5 A 100,100 0 0,0 20,55 Z\u0026#39; \u0026#34; triangle_curved.gif 线条的端点没有变化，所发生的只是每个 \u0026ldquo;L\u0026rdquo; 被一个弧线段代替。然而弧线的大小应该与线的长度成正比。由于我没有这样做，较长的对角线比其他两条线有更深的曲线。\n请记住，当调整对象的大小或比例时，你也应该将半径的比例与线的长度相同，这样曲线的大小就会相应地调整，所以弧线的比例也会正确。\n请注意，sweep 标志可以控制曲线是向外凸起还是向内凸起，这取决于每个路径段的绘制方向(见上文)。\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 20,55 A 100,100 0 0,0 25,10 A 100,100 0 0,1 70,5 A 100,100 0 0,1 20,55 Z\u0026#39; \u0026#34; triangle_bulge.gif 看起来 \u0026ldquo;静态\u0026quot;的直边三角形，现在看起来有点像充满风的帆。\n如果你真的想让线条完美的直，而不把它们转换回真正的线段，你可以通过使用弧线半径为零来关闭曲线。\n因此，弧线不仅适合生成椭圆和圆，而且对于绘制直线和微曲线段也很有用。它是一种非常通用的通用点到点的绘制路径。\n使用椭圆弧来生成分离的曲线段的一个简单的替代方法是使用四元贝塞尔段来代替，主要的区别是使用一个单一的控制点，而不是一个圆形半径来定义弧线。这也允许您将弧线偏向线段的一端，但代价是难以生成对称的弧线。\n当然，您也可以通过使用这两种方法来进行 mix-n-match。\n饼图示例 为了完成对弧线的使用，让我们举个例子，使用为它们生成圆楔。当然，你可能需要使用一些外部的三角数学（你的高中数学有多好？）来确定所需的最终路径点。\nconvert -size 140x130 xc:white -stroke black \\  -fill red -draw \u0026#34;path \u0026#39;M 60,70 L 60,20 A 50,50 0 0,1 68.7,20.8 Z\u0026#39;\u0026#34; \\  -fill green -draw \u0026#34;path \u0026#39;M 60,70 L 68.7,20.8 A 50,50 0 0,1 77.1,23.0 Z\u0026#39;\u0026#34; \\  -fill blue -draw \u0026#34;path \u0026#39;M 68,65 L 85.1,18.0 A 50,50 0 0,1 118,65 Z\u0026#39;\u0026#34; \\  -fill gold -draw \u0026#34;path \u0026#39;M 60,70 L 110,70 A 50,50 0 1,1 60,20 Z\u0026#39;\u0026#34; \\  -fill black -stroke none -pointsize 10 \\  -draw \u0026#34;text 57,19 \u0026#39;10\u0026#39; text 70,20 \u0026#39;10\u0026#39; text 90,19 \u0026#39;70\u0026#39; text 113,78 \u0026#39;270\u0026#39;\u0026#34; \\  piechart.jpg 请注意，所有的弧线都是画在 line path 的左边，并有相应的标志（使用 sweep 标志）。但如果弧线覆盖的角度大于180度，则需要设置 large 标志。见上面例子中最后一个 gold 组件。\n还要注意的是，你应该完整地画出每个部分，尽管这意味着你可能要画两次边界线。如果你不这样做，你很可能要么不会完全用颜色填充该部分，要么填充颜色会覆盖之前绘制的部分轮廓。\n避免重复绘制多条线的唯一方法是绘制所有填充区域，然后重复这样绘制轮廓。就是说你需要把所有的东西都画两遍，确保东西正确匹配。因此，将轮廓加倍可能是最简单的解决方案。\n立方贝塞尔曲线可以使用 c 函数定义两个控制点，以及最终的终点。对于使用最后一个控制点的镜像的持续立方贝塞尔曲线（对于连续曲线），你可以使用 s 函数。\n下面是一个例子。由于这个函数的复杂性，我预先准备了一个画布，显示控制点的位置，以及最后一个控制点的 assumed mirror。\n# Cubic Bezier: C control_1_x,y control_2_x,y x,y # Smooth \u0026#34; : S control_2_x,y x,y convert path_cubic_canvas.gif -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 10,30 C 10,4 50,4 50,30 S 90,55 90,30\u0026#39; \u0026#34; \\  path_cubic.gif 连接控制点和该路径段路径上的最终点的线（控制线）基本上定义了通过路径上该点的曲线方向。长的控制线会在该点产生一条更平滑的曲线，而短的控制线则会在该点产生一条更清晰的曲线。如果控制点与曲线的点相吻合（控制线长度为零），则曲线在该点有一个尖锐的不连续性，就像只用直线段一样。\n作为一个更实际的例子，下面的代码是从 IM 实例 Logo 生成器脚本中提取出来的，该脚本创建了 IM 实例 Logo 的曲线溅射区域。\n这个例子的棘手之处在于，我将我使用的立方贝塞尔路径字符串，转换为另一个路径，显示用于生成贝塞尔曲线的控制线。这让我可以看到曲线的控制线角度和长度，使得调整结果变得更加容易。只需要调整一组点就可以同时显示曲线和控制线，将错误控制在最小范围内。\ncurve=\u0026#34;M 12,27 C 7,37 18,50 18,60 S 0,80 10,94 S 40,74 50,78 S 60,99 76,95 S 72,70 75,65 S 95,55 95,42 S 69,37 66,32 S 67,2 53,7 S 43,17 35,22 S 17,17 12,27 Z\u0026#34; c_ctrls=`echo $curve | \\  sed \u0026#39;1s/\\([0-9]\\) *\\([0-9]\\)/\\1 M \\2/; s/S/M/g; s/C/ /;\u0026#39; -` convert -size 100x100 xc:white \\  -draw \u0026#34;stroke None fill Green path \u0026#39;$curve\u0026#39;\u0026#34; \\  -draw \u0026#34;stroke Red fill None path \u0026#39;$c_ctrls\u0026#39;\u0026#34; \\  curvy_splash.gif 如果你仔细观察图像，你会发现曲线的起点和终点有两条方向相反的控制线。对于一个封闭的连续路径来说，开始和结束的控制线应该在相同的角度（只是在镜面方向），当然也应该是相同的长度。这一点很重要，因为很容易弄错。\n沿着曲线的所有其他点只有一个控制点/线，它与曲线的绘制方向相反。该线段越长，曲线在该控制点处的 sharp 越低，长度为零则产生一个 point。\ns 函数在内部根据前一段的数据生成下一段的镜像控制点/线，从而产生曲线的平滑延续。\n关于这个路径函数的更多例子，请看《SVG：立方贝塞尔曲线命令》。\n手动生成贝塞尔曲线是比较直接的，不需要任何花哨的 GUI 工具。\n 首先定义所有你想让曲线经过的坐标点，在列表的最后重复起始坐标。 现在将这个列表扩大，将所有的 x,y 坐标点加倍成对，并在每对坐标点前添加一个 s（Smooth Cubic）函数。每对中的第一个数字是控制点，连接到第二个数字代表曲线上的点。然而第一个点对却把这一点反过来了，第一个点是曲线的起点，第二个点代表第一个也是唯一一个反转的控制点。 将第一对坐标的函数字母由 \u0026lsquo;S\u0026rsquo; 改为 \u0026lsquo;M\u0026rsquo;，然后在这对坐标之间加一个 \u0026lsquo;C\u0026rsquo;。最后将第二对坐标的 \u0026lsquo;S\u0026rsquo; 去掉，完成初始的立方体（\u0026lsquo;C\u0026rsquo;）函数。 通过添加最后的 \u0026lsquo;Z\u0026rsquo; 来完成路径，关闭曲线。 请看上面的示例序列，它应该是怎样的。 此时您可以测试绘制您的路径。由于所有的控制线长度为零，所以路径将只由直线段组成。 现在您需要做的就是慢慢地、小心地调整控制线段的位置（每个 \u0026ldquo;S\u0026rdquo; 对的第一个坐标），以得到您想要的最终曲线。不要把控制线做得太长，或者方向不对，否则你会得到一条看起来非常滑稽的曲线。 为了帮助查看你的变化和发现错误，请使用上面的转换 \u0026ldquo;sed\u0026rdquo; 命令来绘制路径控制点和曲线控制点之间的控制线。但是请注意，零长度的控制线是不可见的，但是由于lin会产生一个尖锐的点，所以位置应该很明显。 最后，确保 \u0026ldquo;C\u0026rdquo; 之后的第一条控制点/线与终点控制点/线的位置完全相反。  交互式曲线的生成也可以通过使用一些矢量图形编辑器来实现。\n例如 Luis Guerra 报告说，\u0026ldquo;Inkscape\u0026rdquo; 生成的贝塞尔曲线可以使用 \u0026ldquo;Edit -\u0026gt; XML Editor\u0026rdquo; 功能，然后选择你想要控制点的路径或形状。\n 你知道有什么其他的方法可以用GUI工具来提取贝塞尔曲线（在曲线上每个点给出两个或一个控制点）。或者是一些其他的技术来生成这样的曲线？请发邮件给我! 我很乐意听到它。你将会像其他人一样，被记入该技术的名下。\n 二次方贝塞尔函数是立方贝塞尔函数的简化，当两个控制点合并成一个控制点时。同样，你可以用 \u0026lsquo;Q\u0026rsquo; 函数开始曲线，然后用 \u0026lsquo;T\u0026rsquo; 函数继续曲线，镜像最后一个控制点。\n# Quadratic Bezier: Q control_x,y x,y # Smooth \u0026#34; : T x,y convert path_quad_canvas.gif -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 10,30 Q 20,4 50,30 T 90,30\u0026#39; \u0026#34; \\  path_quad.gif 但我要提醒您，\u0026ldquo;T\u0026rdquo; 继续函数只适用于连接间距相等的点的路径。我不推荐使用它。\n二次方曲线的优点是可以替代椭圆弧，因为它使用的是实际的位置，而不是弧线的半径。它也可以使弧线偏向于一端而不是另一端，这在使用椭圆弧时并不实用。\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;path \u0026#39;M 20,55 Q 30,32 25,10 Q 50,1 70,5 Q 50,45 20,55 Z\u0026#39; \u0026#34; triangle_bulge_2.gif 在这种情况下，弧线不是那么均匀，你得到的东西就像一个倒立的鲨鱼鳍，而不是一个帆。\n请记住四边形弧线是抛物线，而椭圆弧线基本上是生成圆弧线段。这可能是决定你应该使用哪种类型的弧线段的关键。\n有关此路径功能的更多例子，请参见：SVG: Quadratic Bezier Curve Commands。\n绘图表面的变形 在这些能力之上，绘制对象的绘图表面可以以各种方式变形，让你做一些令人惊奇的事情。\n首先，你可以应用一些通用的绘图表面修改，比如\u0026hellip; \u0026ldquo;translate\u0026rdquo;、\u0026ldquo;rotate\u0026rdquo;、\u0026ldquo;scale\u0026rdquo;、\u0026ldquo;skewX\u0026rdquo;、\u0026ldquo;skewY\u0026rdquo; 和 \u0026ldquo;affine\u0026rdquo;。\n例如，给定一个线条的 path，我们可以 translate 绘图表面的原点或0,0点到另一个位置。\nconvert -size 100x60 xc:skyblue \\  -draw \u0026#34;translate 50,30 image over 3,3 0,0 \u0026#39;terminal.gif\u0026#39; fill white stroke black path \u0026#39;M 0,20 -45,20 20,-25 -25,-25\u0026#39; fill none stroke red path \u0026#39;M 0,10 0,-10 M 10,0 -10,0\u0026#39; \u0026#34; transform_translate.gif 请注意，\u0026ldquo;0,0\u0026rdquo; 或绘图区域的原点现在以图像为中心，尽管Y轴在图像的顶部仍然是负数，在底部仍然是正数。\nrotate 操作将旋转绘图表面，所以以后在该表面上绘制的任何东西都将被旋转绘制。当然，它会围绕转换后的原点进行旋转，所以最好同时使用这两个变换运算符。\nconvert -size 100x60 xc:skyblue \\  -draw \u0026#34;translate 50,30 rotate -30 image over 4,4 0,0 \u0026#39;terminal.gif\u0026#39; fill white stroke black path \u0026#39;M 0,20 -45,20 20,-25 -25,-25\u0026#39; fill none stroke red path \u0026#39;M 0,10 0,-10 M 10,0 -10,0\u0026#39; \u0026#34; transform_rotate.gif scale 将放大和缩小原点周围的绘图面。\nconvert -size 100x60 xc:skyblue \\  -draw \u0026#34;translate 50,30 scale 1.5,1.5 image over 4,4 0,0 \u0026#39;terminal.gif\u0026#39; fill white stroke black path \u0026#39;M 0,20 -45,20 20,-25 -25,-25\u0026#39; fill none stroke red path \u0026#39;M 0,10 0,-10 M 10,0 -10,0\u0026#39; \u0026#34; transform_scale.gif 一个常见的 scale 用法是将Y轴翻转，使Y的正值向上。当然原点也应该移到中心，或者左下角，以保持秩序。\nconvert -size 100x60 xc:skyblue \\  -draw \u0026#34;translate 50,30 scale 1,-1 image over 4,4 0,0 \u0026#39;terminal.gif\u0026#39; fill white stroke black path \u0026#39;M 0,20 -45,20 20,-25 -25,-25\u0026#39; fill none stroke red path \u0026#39;M 0,10 0,-10 M 10,0 -10,0\u0026#39; \u0026#34; transform_flip.gif 最后，\u0026ldquo;skewX\u0026rdquo; 和 \u0026ldquo;skewY\u0026rdquo; 在X和Y方向上对图像进行剪切。例如，这里我们使用 \u0026ldquo;skewX\u0026rdquo; 给图像的垂直Y轴一个倾斜。\nconvert -size 100x60 xc:skyblue \\  -draw \u0026#34;translate 50,30 skewX 20 image over 4,4 0,0 \u0026#39;terminal.gif\u0026#39; fill white stroke black path \u0026#39;M 0,20 -45,20 20,-25 -25,-25\u0026#39; fill none stroke red path \u0026#39;M 0,10 0,-10 M 10,0 -10,0\u0026#39; \u0026#34; transform_skewY.gif 这些操作符在 MVG -draw 字符串之外有对应的操作符，供一般使用。但是这些命令行版本是运算符，并且立即应用于内存中已经存在的图像，而不是应用于尚未绘制的矢量对象的绘制表面。更多细节请看扭曲图像。\n绘制表面的平移变形 上述所有五种画布变换都可以组合成一个通用的 Affine Matrix Operator，可以使用 MVG 原语 \u0026ldquo;affine\u0026rdquo;，也可以在调用 -draw 之前使用 -affine 设置 Affine 变换。\nAffine 变换使用一组 \u0026ldquo;Matrix Coefficients\u0026rdquo;，它定义了如何将你给出的坐标修改为实际的绘图坐标。\n关于这些 \u0026ldquo;coefficients\u0026rdquo; 如何工作的更多细节，请参阅 Affine Matrix Transforms。\n例如\u0026hellip; 要设置一个相对于对象绘制时的中心原点\u0026hellip;\nconvert -size 100x60 xc:skyblue \\  -draw \u0026#34;affine 1,0,0,1,50,30 image over 4,4 0,0 \u0026#39;terminal.gif\u0026#39; fill white stroke black path \u0026#39;M 0,20 -45,20 20,-25 -25,-25\u0026#39; fill none stroke red path \u0026#39;M 0,10 0,-10 M 10,0 -10,0\u0026#39; \u0026#34; affine_null.gif 翻转图像\u0026hellip;\nconvert -size 100x60 xc:skyblue \\  -draw \u0026#34;affine 1,0,0,-1,50,30 image over 4,4 0,0 \u0026#39;terminal.gif\u0026#39; fill white stroke black path \u0026#39;M 0,20 -45,20 20,-25 -25,-25\u0026#39; fill none stroke red path \u0026#39;M 0,10 0,-10 M 10,0 -10,0\u0026#39; \u0026#34; affine_flip.gif 绕原点旋转30度\u0026hellip;\nconvert -size 100x60 xc:skyblue \\  -draw \u0026#34;affine .866,-.5,.5,.866,50,30 image over 4,4 0,0 \u0026#39;terminal.gif\u0026#39; fill white stroke black path \u0026#39;M 0,20 -45,20 20,-25 -25,-25\u0026#39; fill none stroke red path \u0026#39;M 0,10 0,-10 M 10,0 -10,0\u0026#39; \u0026#34; affine_rot.gif 对于更复杂的仿射变换，您可以使用为此目的创建的仿射辅助脚本。这些脚本将诸如旋转角度和中心点之类的东西转换为 Affine 坐标，你可以直接在 -draw affine 或 -affine 设置中使用。\n推/弹上下文 一些 MVG 原语实际上依赖于这些变换的使用才能正确使用。例如，椭圆原语只能用正交对齐的轴直接指定。\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;ellipse 50,30 30,15 0,360\u0026#34; ellipse_orthogonal.gif 然而，通过使用绘图变换，我们可以很容易地给椭圆添加一个\u0026quot;旋转角\u0026rdquo;。\nconvert -size 100x60 xc:skyblue -fill white -stroke black \\  -draw \u0026#34;push graphic-context translate 50,30 rotate 30 fill white stroke black ellipse 0,0 30,15 0,360 pop graphic-context\u0026#34; ellipse_rotated.gif 请注意，椭圆的 center(旋转点)在应用旋转之前首先被转换。然后，ellipse 在 \u0026ldquo;0,0\u0026rdquo; 处的转换位置被绘制。\n上面还显示了两个新的 MVG 绘图原语。\u0026lsquo;push graphic-context\u0026rsquo; 和 \u0026lsquo;pop graphic-context\u0026rsquo;。在上面的例子中，并不是严格意义上的需要，但在进行主要的绘图转换时，建议使用这两个原语。\npush 和 pop 原语的作用是保存当前的绘图状态或 \u0026ldquo;graphic-context\u0026rdquo;，然后再次恢复。在这两个原语之间更改的任何绘图设置都会被遗忘。这包括曲面变形（如 \u0026ldquo;平移 \u0026ldquo;和 \u0026ldquo;旋转\u0026rdquo;）、颜色设置 fill 和 stroke 或任何其他修改了绘图 state 的设置。\n这些原语使您可以轻松地绘制具有许多变换的非常复杂的对象，然后将事物恢复到更 \u0026ldquo;正常\u0026quot;的状态，以便以后进行绘制操作。您可以在下面的绘制箭头中看到更实用的演示。\n推/弹特殊对象 建设中 更多专门用于 MVG 处理 SVG 格式的设置。\n font-family font-stretch font-style font-weight encoding 'UTF-8' push defs push gradient 'def_name' linear X1,Y1 X2,Y2 stop-color 'color' where stop-color 'color' where # where is a point between the two pixels given (0 = X1,Y1 1= X2,Y2) gradient-units 'objectBoundingBox|userSpaceOnUse' affine .... pop gradient push gradient 'def_name' radial CX,CY FX,FY R # Here CX,CY is the center of the radial gradient of radius R # the FX,FY is the focal, and is usually the same a CX,CY # unless you are trying to warp the gradient in a specific direction stop-color 'color' where ... pop gradient pop defs push graphic-context fill 'url(#def_name)' ... draw things here ... pop graphic-context 例子见 Florent Monnier 的开发网站\u0026hellip; http://www.linux-nantes.fr.eu.org/~fmonnier/OCaml/MVG/。\n阅读 MVG 文件 正如你在上面的例子中所看到的，MVG 的 -draw 参数可以变得很长。事实上，SVG 到 MVG 的转换可以产生一些非常长的 MVG 绘图参数（见下文）。\n然而，IM 的一般命令行界面允许您通过使用 \u0026quot;@filename\u0026quot; 参数从文件中读取任何字符串参数。这很方便，因为这意味着你可以从一个单独的文件中读取非常长和复杂的MVG绘图命令。\n例如，如果我将 MVG 操作放入一个名为 \u0026ldquo;draw_circles.mvg\u0026rdquo; 的文件中，那么我就可以像这样绘制\u0026hellip;\nconvert -size 100x60 xc:skyblue -draw @mvg_circles.mvg mvg_draw.gif 不仅如此，ImageMagick 还懂得直接读取 \u0026ldquo;MVG:\u0026rdquo; 图像文件格式，让你可以更直接地绘制此类命令。然而，除非 MVG文 件定义了一个画布，否则你可能需要指定初始画布（ -size 和\u0026rdquo;-background\u0026rdquo;）来绘制。\nconvert -size 100x60 -background limegreen mvg_circles.mvg mvg_file.gif 你可以通过在 MVG 文件中添加一个 \u0026ldquo;viewbox\u0026rdquo;，并添加适当的背景色填充绘制，将初始画布设置移动到 MVG 图像中。这样就完成了 MVG 图像文件作为一个完整的图像定义。\nconvert mvg_circles2.mvg mvg_image.gif  目前只有一种方法可以从 MVG 参数字符串中读取外部 MVG 文件，那就是使用 \u0026ldquo;图像 \u0026ldquo;绘图原语。遗憾的是，这将 MVG 的 include 转换为光栅图像，然后再将该图像覆盖到绘图表面。 换句话说，目前还没有MVG的 \u0026ldquo;include\u0026quot;功能。 :-(\n 建设中 您可以生成IM的低级绘制操作，使用 `+render 来记录它们。\n当你再给 -render 设置/操作时，IM将立即绘制这些保存的操作。\n奇怪的是，仅仅输出到 \u0026ldquo;MVG\u0026rdquo; 文件似乎也能做到这一点\u0026hellip;\u0026hellip;。\nconvert ... -draw \u0026#39;....\u0026#39; draw_commands.mvg 注意：如果你在输出 MVG 格式文件时画了一条曲线，文件中会列出以下内容。曲线是一系列的短线段，而不是原始曲线。\n当然您也可以完全使用更通用的 SVG 格式。参见下面的 \u0026ldquo;SVG 格式处理\u0026quot;。\nMVG 阿尔法构成 建设中 我没有看到任何使用 Alpha 构图的情况（除了 painters 的算法之外）。基本上是一种 \u0026ldquo;over\u0026rdquo; alpha 合成）来绘制对象。\n然而，这并不是说不能这样做。\n如果你喜欢将你的矩形、椭圆、圆形或其他物体用不同的阿尔法成分（如 \u0026ldquo;DstOver\u0026rdquo;，这是一个类似于 Under 的成分），然后在空白的透明画布上画出你的人物，同样的构图，并将其合成到您的图像上。\n然而，由于 SVG 允许您使用 alpha 合成来绘制文本和其他的图像，因此，您可以使用 SVG 来绘制图像项目到图像上，我想这将是一个未来的补充。\n敬请期待\n绘制符号 有时你在图像上有一组点，你想在那里绘制参考符号，如十字、圆圈等。遗憾的是，目前IM还没有可以轻松绘制这类符号的命令，但只要稍加努力，你就可以绘制这类符号。\n符号绘制技巧 在给定的位置列表中绘制多个符号的诀窍是使用 shell 脚本或任何你正在使用的 API 生成 MVG 绘图命令，以便将给定的点集转化为适当的绘图命令集。 例如，我在这里将一条线上的点转换为每一个点的\u0026quot;加号\u0026rdquo;\u0026hellip;\n# Define a string of X and Y coordinates # comma between values, space between coordinates. points=\u0026#34;6.6,7.7 25.0,75.0 42.2,85.4 75.8,94.7 51.5,39.3 92.5,66.6\u0026#34; # convert each point into a draw command for a cross (using \u0026#39;awk\u0026#39;) # the \u0026#39;tr\u0026#39; converts spaces into \u0026#39;newlines\u0026#39; (one point per line). crosses=`echo $points | tr -s \u0026#39; \u0026#39; \u0026#39;\\012\u0026#39; |\\  awk -F, \u0026#39;{ print \u0026#34;line \u0026#34; $1-3 \u0026#34;,\u0026#34; $2 \u0026#34; \u0026#34; $1+3 \u0026#34;,\u0026#34; $2 ; print \u0026#34;line \u0026#34; $1 \u0026#34;,\u0026#34; $2-3 \u0026#34; \u0026#34; $1 \u0026#34;,\u0026#34; $2+3 ; }\u0026#39; -` # draw a red line between the points, and blue crosses on the points. convert -size 100x100 xc:white \\  -draw \u0026#34;fill none stroke red polyline $points\u0026#34; \\  -draw \u0026#34;fill none stroke blue $crosses\u0026#34; \\  points_plus.gif 上面使用 \u0026ldquo;tr\u0026rdquo; 将每个点（两个数字）分成每条线上的一个点，然后使用 \u0026ldquo;awk\u0026rdquo; 进行所有数学计算，在给定的点上绘制\u0026quot;加号\u0026rdquo;。你可以使用任何你喜欢的东西，因为我只是在输入点列表上应用了一种文本宏扩展的形式。几乎所有的编程语言都可以做到这一点。对于上面的shell脚本案例，我只是发现 \u0026ldquo;awk\u0026rdquo; 是最简单、最快的手段。\n其实你甚至可以使用 Imagemagick 本身的 \u0026ldquo;convert\u0026rdquo; 格式选项来进行这种 \u0026ldquo;macro\u0026rdquo; 的扩展\u0026hellip;比如这里我用它来计算圆周上的一个点，对于这个\u0026quot;点符号\u0026rdquo;。\n# Define a string of X and Y coordinates # comma between values, space between coordinates. points=\u0026#34;6.6,7.7 25.0,75.0 42.2,85.4 75.8,94.7 51.5,39.3 92.5,66.6\u0026#34; # circle radius (or symbol size) to draw around each point. radius=3.5 # convert each point into a draw command for a cross # In this case, points are space separated by the shell circles=$(for point in $points; do x=$(echo \u0026#34;$point\u0026#34; | cut -d, -f1) y=$(echo \u0026#34;$point\u0026#34; | cut -d, -f2) # use IM to do some floating point math, EG: y2=$y+$radius y2=$(convert xc: -format \u0026#39;%[fx:\u0026#39;\u0026#34;$y\u0026#34;\u0026#39;+\u0026#39;\u0026#34;$radius\u0026#34;\u0026#39;]\u0026#39; info:) echo \u0026#34;circle $x,$y$x,$y2\u0026#34; done) # Draw a red line between the points, and blue circles on the points. convert -size 100x100 xc:white \\  -draw \u0026#34;fill none stroke red polyline $points\u0026#34; \\  -draw \u0026#34;fill none stroke blue $circles\u0026#34; \\  points_circle.gif 现在，您生成的绘制字符串可能会变得相当长，并且可能会开始导致您的最终命令的长度问题。所以，与其将点转换成长字符串，然后我们在命令行上传递给 IM，不如将绘制命令以文件的形式管道化给 IM。\n我这次也使用了 SVG 路径的绘制方法来代替 Draw Primitive 的绘制方法。另外我生成的符号是每个点周围的三角形。\n# Define a string of X and Y coordinates # comma between values, space between coordinates. points=\u0026#34;6.6,7.7 25.0,75.0 42.2,85.4 75.8,94.7 51.5,39.3 92.5,66.6\u0026#34; # convert each point into a draw commands to draw a triangle for point in $points; do echo \u0026#34;path \u0026#39;M $pointm 0,-5 -4,+8 +8,0 -4,-8\u0026#39;\u0026#34; done |\\  convert -size 100x100 xc:white \\  -fill none -stroke red -draw \u0026#34;path \u0026#39;M $points\u0026#39; \u0026#34; \\  -fill none -stroke blue -draw \u0026#39;@-\u0026#39; \\  points_tri.gif SVG 路径实际上使这一点变得更容易，通过允许相对像素移动，允许你设计符号，所以它只需要一个单一的初始绝对移动\u0026rsquo;M'，然后再给出\u0026rsquo;移动\u0026rsquo;和\u0026rsquo;线\u0026rsquo;的序列来绘制符号。正因为如此，你实际上根本不需要任何浮点计算，因为IM draw会完成所需的定位数学。\n 相对移动SVG路径项\u0026rsquo;m\u0026rsquo;在IM v6.4.3-5之前被破坏了。如果你的IM比这更老，上面（和下一个）的例子可能什么也画不出来。您可以通过将上面的相对移动\u0026rsquo;m\u0026rsquo;替换为适当的相对线序列\u0026rsquo;l\u0026rsquo;来解决旧版本的问题。\n 现在你可以更进一步，将一个完整的MVG文件，包括画布规格，直接作为一个绘画命令的流水线输入到IM中。这次让我们做一个\u0026rsquo;十字'，这和上面第一个\u0026rsquo;加\u0026rsquo;的例子类似，需要大量的计算。\n# Define a string of X and Y coordinates # comma between values, space between coordinates. points=\u0026#34;6.6,7.7 25.0,75.0 42.2,85.4 75.8,94.7 51.5,39.3 92.5,66.6\u0026#34; # Generate a MVG file for IM to draw all components ( echo \u0026#34;viewbox 0 0 100 100 fill white rectangle 0,0 100 100\u0026#34; echo \u0026#34;fill none stroke red path \u0026#39;M $points\u0026#39;\u0026#34; echo \u0026#34;fill none stroke blue path \u0026#39;\u0026#34; for point in $points; do echo \u0026#34; M $pointm -2,-2 +4,+4 m -4,0 +4,-4\u0026#34; done echo \u0026#34;\u0026#39;\u0026#34; ) | convert mvg:- points_cross.gif 这使用了特殊的shell编程技术，在shell括号中 \u0026ldquo;echo\u0026rdquo; 的任何东西都将作为MVG文件被输入到最终的\u0026quot;转换\u0026quot;命令中。第一个 \u0026ldquo;echo\u0026rdquo; 定义并填充图像的绘图画布，而 \u0026ldquo;while\u0026rdquo; 循环则将每个 \u0026ldquo;点\u0026rdquo; 转换为一个给定半径的圆。\n这种方法的优点是，你不会受到任何字符串的限制，而使用其他两种方法可能会受到限制。\n其他你可以生成的符号包括方框、钻石、错误条等。\n也请参见下面的\u0026quot;绘制圆圈\u0026rdquo;，了解其他圆圈方法，包括不计算相对 \u0026ldquo;路径\u0026quot;的圆圈绘制。\n绘制符号的替代方法 除了直接绘制符号外，还有其他的方法可以将符号添加到图像中。\n符号字体 您可以从 Symbol Font 中提取符号，并将其保存为一个小位图。你也可以使用小的预定义但色彩丰富的图像来做这件事。\n然而这样做可能会出现问题，无法准确定位字体相对于特定像素的位置。也就是说这不是一个非常精确的技术。但是你可以在任何像素位置组成任何图像。例如这些符号是从一些字体中提取出来的，用于这些示例页面的具体使用。\n\u0026lt;= =\u0026gt; x + + + o o o o 在分层图像一节中给出了将图像合成到大背景上的例子。然而，循环的方法可能更有用，例如在分层图像的程序化定位中给出了。\n未来：使用坐标对图像进行分层的例子\n形态学 另一种选择是使用形态学，使用特殊的\u0026quot;形状\u0026quot;内核，如 \u0026ldquo;磁盘\u0026rdquo;、\u0026ldquo;环形 \u0026ldquo;和 \u0026ldquo;加\u0026rdquo;，甚至是你自己的用户定义内核，来 \u0026ldquo;稀释\u0026quot;单个像素。\n例如\u0026hellip;\nconvert -size 80x80 xc:black -fill white \\  -draw \u0026#39;point 20,15 point 55,30 point 40,60\u0026#39; points_pixels.gif convert points_pixels.gif -morphology Dilate Ring points_rings.gif convert points_pixels.gif -morphology Dilate Plus:4 points_pluses.gif convert points_pixels.gif -morphology Dilate Cross:3 points_crosses.gif 然后可以通过使用 Alpha Shape Operator 将结果直接转换为彩色叠加。\n这样做的最大好处是，你其实不需要知道每个符号的单独位置。或者有多少个符号。但这也可能是一个缺点。一个主要的缺点是，位置只在整数位置。你不能使用浮点 \u0026ldquo;子像素\u0026quot;定位来\u0026quot;绘制\u0026rdquo;。\n卷积 一个几乎相同的技术是使用 Convolve，使用专门设计的内核，它允许你设置不同的灰度，而不仅仅是一个简单的开/关结果，如上所述。\n通过使用不同的用户定义内核，为图像的每一个通道（红、绿、蓝和阿尔法），甚至可以从每个像素坐标创建多色符号。\n为此，我使用了一个我写的特殊脚本 image2kernel 来将彩色图像（见右图）转换为每个通道的独立浮点卷积核。\nimage2kernel -q marker.png marker.dat 这将生成四个文件，比如 marker_R.dat，每个通道都有一个非常小的输入图像，这是用户定义的图像表示（原点在图像中心）。\n现在使用这些内核数据文件，我们可以将这些单点在透明的背景上 Convolve 成我们彩色的标记图像。\nconvert points_pixels.gif -alpha off \\  \\( -clone 0 -morphology Convolve @marker_R.dat \\) \\  \\( -clone 0 -morphology Convolve @marker_G.dat \\) \\  \\( -clone 0 -morphology Convolve @marker_B.dat \\) \\  \\( -clone 0 -morphology Convolve @marker_A.dat \\) \\  -delete 0 -channel RGBA -combine point_markers.png  在IM v6.7.6-9之前，Combine Operator 要求图像的透明度通道以 \u0026ldquo;哑光\u0026quot;值而不是alpha值的形式给出，因此，由此产生的alpha通道需要被否定。EG:\n ... \u0026#34;`cat marker_A.dat`\u0026#34; -negate \\) \\ 只能使用小图像，像素点要足够分散，符号不要重叠。这是因为 Convolve 会将重叠的区域加在一起，使其比预期的更亮。\n以上内容已被转换成UNIX shell脚本 convolve_image，以方便使用。\nconvolve_image points_pixels.gif marker.png point_markers.png 这个技术源于IM论坛上的一个讨论-IM 的有趣体验。用户希望在足球场的背景图上放置小人，让他们的位置在图片中拼出一个人的名字。\n分层 一种不同的技术，如图像层，使用你从源图像中提取的像素列表进行定位，可能是更好的方法。你可以先叠加更远的符号图像，然后再叠加前景图像，你可以通过编程选择或随机选择什么符号替换什么点。\n关于这个例子，请看地图中的图钉。\n绘制圆圈 绘制选项为您提供了许多方法来完成一些非常基本的工作。绘制圆。\n例如，您可以在圆周上的任何一点上画一个圆，因此您需要计算一个中心点和一个半径为25像素的第二点。因此，您需要计算一个中心点和第二个点，这个点的半径（例如25像素）离第一个点的距离。\nconvert -size 100x60 xc: -stroke Firebrick -fill tomato -strokewidth 2 \\  -draw \u0026#39;circle 50,30 50,55\u0026#39; circle_circle.gif Fred Weinhaus 指出，通过使用平移，你可以消除计算圆边坐标的需要，而直接给出半径即可。\nconvert -size 100x60 xc: -stroke SeaGreen -fill PaleGreen -strokewidth 2 \\  -draw \u0026#39;translate 50,30 circle 0,0 25,0\u0026#39; circle_circle_trans.gif 但是当绘制多个圆时，上述操作需要对每个圆进行单独的 -draw 操作，或者使用 Context Pushing。\n使用椭圆可以直接指定半径为轴长\nconvert -size 100x60 xc: -stroke Sienna -fill Wheat -strokewidth 2 \\  -draw \u0026#39;ellipse 50,30 25,25 0,360\u0026#39; circle_ellipse.gif 你也可以用 stroke-lineecap round 画一条非常非常短的线来生成一个圆。描边的宽度可以设置圆的直径。注意线条必须有一定的长度（无论多小），否则画不出任何东西。\nconvert -size 100x60 xc: -stroke Blue -strokewidth 50 \\  -draw \u0026#39;stroke-linecap round line 50,30 50,30.0001\u0026#39; \\  circle_line.gif 这种技术，不幸的是不能勾勒出生成的圆，但对于覆盖大面积的区域，大笔触宽度是有用的。请看下面一些简单的例子。\n这种方法利用了 SVG 路径的绘制方法，所以可以在不需要计算任何额外坐标的情况下绘制圆形。\nconvert -size 100x60 xc: -stroke Blue -fill DodgerBlue -strokewidth 2 \\  -draw \u0026#34;path \u0026#39;M 50,30 m 0,25 a 1,1 0 0,0 0,-50 a 1,1 0 1,0 0,50\u0026#39;\u0026#34; \\  circle_path.gif 只有最初的绝对移动 \u0026lsquo;M\u0026rsquo; 是用来定义中心的，接下来的路径组件中的 \u0026lsquo;25\u0026rsquo; 和 \u0026lsquo;50\u0026rsquo; 是定义相对于这个中心的圆的半径和直径。\n 在IM v6.4.3-5之前，相对移动SVG路径项 \u0026rsquo;m' 是被破坏的。 如果您的IM比这更早，圆可能只显示为一个像素。您可以通过将上面的 \u0026rsquo;m' 替换为 \u0026lsquo;l\u0026rsquo; 来修复旧版本的问题。\n Fred Weinhaus 添加了以下贝塞尔圆的方法。它非常接近真实的圆（虽然不精确），并且需要进行浮点计算。\nr=25; cx=50; cy=30; x1=25; x2=75; # = cx ± radius y1=-3.25; y2=63.25; # = cy ± radius*1.275 convert -size 100x60 xc: -stroke Purple -fill Violet -strokewidth 2 \\  -draw \u0026#34;bezier $x1,$cy$x1,$y1$x2,$y1$x2,$cy\u0026#34; \\  -draw \u0026#34;bezier $x1,$cy$x1,$y2$x2,$y2$x2,$cy\u0026#34; \\  circle_bezier.gif 如果画一个精确的圆并不重要，你可以使用这个4 Bezier段 SVG 路径，它只使用圆的X和Y边界进行计算。\nr=25; cx=50; cy=30; x1=25; x2=75; # X bounds = cx ± radius y1=5; y2=55; # Y bounds = cy ± radius convert -size 100x60 xc: -stroke Tomato -fill Gold -strokewidth 2 \\  -draw \u0026#34;path \u0026#39;M $cx,$y1Q $x1,$y1$x1,$cyT $cx,$y2$x2,$cy$cx,$y1z\u0026#39;\u0026#34; \\  circle_bezier_path.gif 如果你喜欢一个完全相对于中心起点绘制的，你可以使用这种技术。只使用半径值，使其生成简单，只使用 API 中的字符串函数。\nconvert -size 100x60 xc: -stroke Orange -fill LemonChiffon -strokewidth 2 \\  -draw \u0026#34;path \u0026#39;M 50,30 m 0,25 q 25,0 25,-25 t -25,-25 -25,25 25,25 z\u0026#39;\u0026#34;\\  circle_bezier_path_rel.gif 你能想到其他画圆的方法吗？\n绘制箭头-定位、旋转和缩放符号 使用上述技术，你可以创建一个特殊的符号，如箭头，你可以定位，使其点在线的最末端，并画在它上面。如果你画的箭头后的线（典型的情况），那么箭头将被绘制在该行的顶部。\n然而，他们有三种类型的箭头，可以定义，每个类型的定义在不同的方式取决于它的使用。\n 测量时，你只是想用箭头头标记线的两端，以指示一些工程图中的测量极限。非常简单。 矢量，显示一些数值的方向和强度。例如在天气风图中。需要一个尾巴，0,0点就是尾巴的末端。通常情况下，这样的向量会形成一个大网格。 指标，指出一些细节。对于这个0,0点可能应该是箭尖，或者是箭头本身前面的一些距离。  测量箭头 简单地在一条线的末端添加一个箭头是比较容易做到的。基本上，您可以创建一个 \u0026lsquo;arrow head\u0026rsquo; 符号，并将其画在正确的位置。\n例如\u0026hellip;\narrow_head=\u0026#34;l -15,-5 +5,+5 -5,+5 +15,-5 z\u0026#34; convert -size 100x60 xc: -draw \u0026#39;line 10,30 80,30\u0026#39; \\  -draw \u0026#34;stroke blue fill skyblue path \u0026#39;M 80,30 $arrow_head\u0026#39; \u0026#34; \\  arrow_horizontal.gif 请注意，我画的符号，使其起点是线的最末端。这样一来，它就可以在之前画好的线上面向后画，形成一个非常漂亮的整齐的符号。\n箭头却有一个相关的方向。你可以在许多不同的角度创建大量的箭头定义，许多程序都这样做。但是既然箭头是一个矢量，那么为什么不把箭头作为一个矢量进行旋转呢。IM绘图命令内置了绘图旋转（Canvas Warping），所以让我们使用它们。\n这样做还有一个好处，就是把位置从箭头头的 path 定义中移出来，让你把整个路径指定为 constant\u0026hellip;\narrow_head=\u0026#34;path \u0026#39;M 0,0 l -15,-5 +5,+5 -5,+5 +15,-5 z\u0026#39;\u0026#34; convert -size 100x60 xc: -draw \u0026#39;line 25,55 70,10\u0026#39; \\  -draw \u0026#34;stroke blue fill skyblue translate 70,10 rotate -45 $arrow_head\u0026#34; \\  arrow_rotate.gif 如果你喜欢改变箭头的大小，可以在旋转后增加一个 scale 绘制选项。\narrow_head=\u0026#34;path \u0026#39;M 0,0 l -15,-5 +5,+5 -5,+5 +15,-5 z\u0026#39;\u0026#34; convert -size 100x60 xc: -draw \u0026#39;line 25,55 70,10\u0026#39; \\  -draw \u0026#34;stroke blue fill skyblue translate 70,10 rotate -45 scale 2,2 $arrow_head\u0026#34; \\  arrow_scale.gif 请注意，它是如何放大离开箭头的\u0026quot;尖端\u0026rdquo;，你指定的地方。这是处理箭头的一个非常重要的方面，因为它是唯一的终点，和角度的线，你正在添加箭头的事项。\ntransforms 的顺序是很重要的，而且实际上与它们实际被处理的顺序是相反的。也就是先将比例尺应用于座标，然后是旋转，再是平移。如果座标变换不是按照这个顺序进行的，我们最终也会对箭头的最终位置进行缩放，而不是我们期望的位置。\n另外由于比例尺有两个数字，而原来的箭头头符号是水平设计的（角度为零），所以可以分别将箭头的宽度与高度进行比例。同时注意笔画宽度也随着箭头的大小而缩放，保持一致。\narrow_head=\u0026#34;path \u0026#39;M 0,0 l -15,-5 +5,+5 -5,+5 +15,-5 z\u0026#39;\u0026#34; convert -size 100x60 xc: -draw \u0026#39;line 25,55 70,10\u0026#39; \\  -draw \u0026#34;stroke blue fill skyblue translate 70,10 rotate -45 scale 2,1 $arrow_head\u0026#34; \\  arrow_scale_aspect.gif 现在，当你在画布上翘起一个个箭头时，也许还有许多其他的绘画操作，你可能喜欢在一次 [\u0026quot;-draw\u0026quot;](https://imagemagick.org/Usage/option_link.cgi?draw) 操作中把它们全部画完。说要画线，然后在两端添加箭头，需要不同的颜色、位置、旋转，甚至可能是不同的比例。这意味着我们需要将画布翘曲的范围限制在每个单独箭头头的绘制上。如果你不限制范围，你可能会在以后开始影响到其他后面的绘制操作，永远不能很确定你生成的是什么。\n为了限制翘曲的范围（以及所有其他的绘图属性），您将涉及到的部分包裹在一个 graphic-context\u0026hellip;\narrow_head=\u0026#34;path \u0026#39;M 0,0 l -15,-5 +5,+5 -5,+5 +15,-5 z\u0026#39;\u0026#34; convert -size 100x60 xc: \\  -draw \u0026#34;stroke black fill none path \u0026#39;M 10,40 A 50,50 0 0,1 90,40\u0026#39; push graphic-context stroke blue fill skyblue translate 10,40 rotate 135 $arrow_headpop graphic-context push graphic-context stroke firebrick fill tomato translate 90,40 rotate 45 $arrow_headpop graphic-context \u0026#34; \\  arrow_context.gif push 主要是将当前所有的绘图属性保存起来，以备将来使用，而 pop 则恢复这些属性，用之前保存的设置替换任何设置（颜色、扭曲、位置等）。这意味着在 \u0026lsquo;popping\u0026rsquo; 之后，\u0026lsquo;canvas warp\u0026rsquo; 会被取消，画图会回到修改之前的状态。\n上述技术只是生成箭头的一种方法，在绘制箭头作为测量距离的一部分时，如在技术图纸中，是一种很好的方法。\n矢量箭头 如前所述，矢量既显示方向，又显示某个数值的强度。这意味着箭头的长度是可变的，箭头头可以在任何位置远离向量的起点。\n现在，你可以做一些沉重的数学计算的位置，箭头头应该是地方给定的向量的长度和角度，但他们是一个更好的方法，这让 ImageMagick 为你做这些计算。\n解决的办法是在 Warped Canvas Space 中画一条长度合适的水平线作为向量长度。当这条线画好后，只需将绘图空间再次转换到线的末端，而画布仍然是\u0026quot;扭曲的\u0026rdquo;(warped)。现在你的位置已经正确了，只要正确的旋转就可以像正常的那样画出矢量的 'arrow head'。\n例如，这里我以-35度角生成一个70像素长的矢量。\nvector_head=\u0026#34;path \u0026#39;M 0,0 l -15,-5 +5,+5 -5,+5 +15,-5 z\u0026#39;\u0026#34; indicator=\u0026#34;path \u0026#39;M 10,0 l +15,+5 -5,-5 +5,-5 -15,+5 m +10,0 +20,0 \u0026#39;\u0026#34; convert -size 100x100 xc: \\  -draw \u0026#34;stroke black fill none circle 20,50 23,50 push graphic-context stroke blue fill skyblue translate 20,50 rotate -35 line 0,0 70,0 translate 70,0 $vector_headpop graphic-context push graphic-context stroke firebrick fill tomato translate 20,50 rotate 40 $indicatortranslate 40,0 rotate -40 stroke none fill firebrick text 3,6 \u0026#39;Center\u0026#39; pop graphic-context \u0026#34; \\  arrow_with_tails.gif 指示箭头 在上面我还演示了一个指示箭头，指向之前矢量箭头的起点。\n然而我没有像之前那样画箭头，而是将它创建为一个反向的箭头符号，它的起始点距离原点（或起始点）10像素。这是一个符号位于我想要指示的位置，所以我实际上并不希望箭头直接在那个位置上面，而是离它稍微远一点。\n现在，虽然指示符比向量处理起来更简单，通常不需要可变长度，但你通常要在指示符的远端添加文字来指定指示的内容。和之前一样，计算这个位置可能很困难，何必呢。\n文字定位的解决方法也和向量一样。保留原来用于绘制指示箭头的翘曲空间，并将原点转换到该箭头的尾端（在翘曲空间中水平40像素）。现在我们已经重新定位了，我们可以围绕这个新的位置解除扭曲，这样你就可以像正常的那样绘制文本（有轻微的偏移）。\n不幸的是，虽然默认的文字对齐方式是\u0026quot;左\u0026rdquo;，但目前你不能在 MVG 中指定文字对齐方式，作为重力的单独设置。如果这是一个问题，请在 IM bugs 论坛上提出请求，希望文本对齐（作为独立于重力定位）能够成为现实，尤其是它实际上是 SVG 规范的一部分。\n绘制对象 宽广的色彩笔触 你不需要用路径或轮廓完全封闭填充区域，就能创造出各种形状。使用非常大和宽的笔触，你可以在画布上生成大面积的颜色和色块。\n例如，一个宽阔的笔触椭圆弧形可以生成一个漂亮的颜色区域，我实际上已经看到用于创建海报。\nconvert -size 100x100 xc: -fill none -stroke powderblue \\  -draw \u0026#39;stroke-width 70 ellipse -30,0 90,90 10,50\u0026#39; \\  -rotate 180 arc_background.gif 或者你可以生成一个小丑相当复杂的笑容。\nconvert -size 100x100 xc: \\  -draw \u0026#39;fill none stroke-linecap round stroke-width 40 stroke tomato ellipse 50,0 70,70 65,115 stroke-width 2 stroke black ellipse 50,0 70,70 60,120 stroke-width 40 stroke palegreen line 50,40 50,40.01\u0026#39; clown.gif 你能想出什么办法？请告诉我们。\n圆柱体 在 IM 论坛的讨论中，有一个关于使用 ImageMagick 绘制命令绘制圆柱体（特别是阴影圆柱体）的重要讨论。\n绘制圆柱体的诀窍是绘制 roundrectangle 原语，使其末端形成椭圆形。也就是说，如果圆柱体的宽度为 50 像素，则将矩形的角分别舍去 25 和 12 像素。这就是矩形宽度的一半，然后再减半。\n这样，一个圆柱体就变成了两个相互叠加的圆角矩形。第二个颜色填充的 'end oval' 的大小正好是两个角的两倍。例如\u0026hellip;\nconvert -size 60x100 xc:white -stroke snow4 \\  -fill chartreuse3 -draw \u0026#39;roundrectangle 5,5 55,95 25,12\u0026#39; \\  -fill chartreuse2 -draw \u0026#39;roundrectangle 5,5 55,29 25,12\u0026#39; \\  cylinder.gif 通过将第一种填充色替换为渐变色（使用记忆中的平铺技术），你可以让圆柱体看起来更像3D\u0026hellip;\u0026hellip;。\nconvert -size 60x100 xc:white -stroke snow4 \\  \\( -size 1x60 gradient:chartreuse1-chartreuse4 -rotate -90 \\  -write mpr:shading +delete \\) \\  -tile mpr:shading -draw \u0026#39;roundrectangle 5,5 55,95 25,12\u0026#39; +tile \\  -fill chartreuse2 -draw \u0026#39;roundrectangle 5,5 55,29 25,12\u0026#39; \\  cylinder_shade.gif 通过慢慢完善圆柱体的绘制（如IM论坛中讨论的那样），你可以走很长的路来生成非常复杂和具有视觉吸引力的圆柱体。这包括增加封闭半透明玻璃圆柱体、阴影效果和标签。\n该讨论的最终结果是一个脚本 \u0026ldquo;cylinder_bar\u0026quot;，生成一个圆柱体百分比条\u0026hellip;\u0026hellip;\ncylinder_bar 95 cylinder_95.png 该脚本可以生成任何尺寸的图像，根据该尺寸和脚本顶部定义的其他设置适当调整所有参数。还包括 glass thickness 的概念，在一个封闭的半透明玻璃圆柱体和内部的彩色圆柱体之间创造一个间隙。\n请注意圆柱体非常微妙的阴影，特别是当绿色圆柱体的末端与玻璃圆柱体的末端重叠时。只要稍加预想，就能做出惊人的效果。\n在文字串中绘制特殊字符 引号还是反斜杠？ 人们在使用 -draw 时遇到的最大的问题之一是绘制字符，这些字符对 UNIX shell 和 DOS 命令行甚至其他语言如 C、Perl、PHP、R 或 Visual Basic 都有特殊意义。\n在这方面最大的罪魁祸首是两种类型的引号字符，以及变量替换字符，如美元 '$' 和 shell 和 ImageMagick 的转义字符，反斜杠 '\\'。\n基本上作为 -draw 的MVG参数需要加引号，而里面的 'text' 字符串参数也可能需要一些额外的引号。\n为了解决这个问题，用户通常会使用两个不同的引号字符，一个用于 shell，另一个用于 MVG 文本字符串。\n-draw \u0026#39;... text 0,0 \u0026#34;string\u0026#34; ...\u0026#39; 需要注意的是，对于 windows 用户来说，这是唯一真正的选择，它有自己的引号问题和方法。或者他们会交换引号，用\u0026hellip;\n-draw \u0026#34;... text 0,0 \u0026#39;string\u0026#39; ...\u0026#34; 它允许你包含 shell 变量替换(使用 '$' 而不进行转义。)\n选择正确的形式可以解决大多数问题，但有些字符仍然存在困难，每个解决方案都取决于你到底使用哪一组引号，因为它们也定义了特殊字符应该如何转义。\n以下是四种情况下的引号，以及特殊字符的处理\u0026hellip;\n 对 shell 参数使用单引号，对 MVG 文本字符串周围使用双引号。  处理绘制文本字符串的最简单的技术是为包装 shell 参数使用单引号。然而，这意味着要在绘制的字符串中包含一个撇号，你需要离开 shell 的\u0026quot;单引号模式\u0026rdquo;，并在 shell 的单引号之外提供撇号。\n例如，这里是如何处理我提到的四个特殊字符。\nconvert -size 250x50 xc:none -box white -pointsize 20 -gravity center \\  -draw \u0026#39;text 0,0 \u0026#34; \u0026#39;\\\u0026#39;\u0026#39; \\\u0026#34; $ \\\\ \u0026#34; \u0026#39; \\  -trim +repage text_special_sd.gif 请注意，由于美元符号不需要转义，你也不能用它来替代 shell 变量的内容。\n重要的是要记住，反斜杠是IM绘图字符串处理的唯一特殊字符。同时，它存在的原因也纯粹是为了让你可以转义任何\u0026quot;IM 绘制字符串引号\u0026rdquo;，比如我们在上面使用的双引号。除此以外，其他所有的怪异都是由 UNIX 命令行 shell 引起的，而不是 IM。\nPC-DOS 有它自己的怪异之处，我希望在使用环境中的IM时，能对特殊字符进行转义。\n 在 shell 参数中使用双引号。 在 MVG 文本字符串周围使用单引号。  如果你确实想在绘制的字符串中插入一个 \u0026lsquo;shell variable\u0026rsquo;，那么你将不得不在 shell 参数的外面使用双引号。这使得整个事情变得更加复杂，因为你失去了 shell 的保护，你现在不仅要转义美元 '$' 符号，而且还要转义反斜杠 '\\'。\n另一方面，shell 将不需要使用单引号字符作为它的参数结束限制字符，所以这方面被简化了。让我们总结一下我们的特殊字符短名单的结果。\nconvert -size 250x50 xc:none -box white -pointsize 20 -gravity center \\  -draw \u0026#34;text 0,0 \u0026#39; \\\\\u0026#39; \\\u0026#34; \\$ \\\\\\\\ \u0026#39; \u0026#34; \\  -trim +repage text_special_ds.gif 请注意，如果你想画一个反斜线本身，MVG 文本字符串需要将反斜线加倍（如前面的例子），但是 shell 本身也需要将每个反斜线加倍，总共产生四个反斜线才能产生一个这样的字符。\n这种翻倍很快就会让人不知所措，需要大量的反斜杠才能达到你想要的效果。只要采取的是慢慢的、简单的方法，你就会针对自己的情况想出办法。\n 使用单引号进行 shell 论证。 与 MVG 文本字符串周围的单引号。  最后，让我们总结一下最后两种引号组合。我将让你去弄清楚它们是如何被 shell 和 MVG 解码的。\nconvert -size 250x50 xc:none -box white -pointsize 20 -gravity center \\  -draw \u0026#39;text 0,0 \u0026#39;\\\u0026#39;\u0026#39; \\\u0026#39;\\\u0026#39;\u0026#39; \u0026#34; $ \\\\ \u0026#39;\\\u0026#39;\u0026#39; \u0026#39; \\  -trim +repage text_special_ss.gif  在shell参数中使用双引号。  在 MVG 文本字符串周围加上双引号。\nconvert -size 250x50 xc:none -box white -pointsize 20 -gravity center \\  -draw \u0026#34;text 0,0 \\\u0026#34; \u0026#39; \\\\\\\u0026#34; \\$ \\\\\\\\ \\\u0026#34;\u0026#34; \\  -trim +repage text_special_dd.gif 正如你所看到的，来自命令行的 -draw 参数既要处理命令行 shell，也要处理 MVG 文本字符串中的反斜杠和引号转义。其结果可能会让人感到困惑和棘手。只要记住，shell 对这两种引号的处理方式不同，而 MVG 文本字符串则不同。\n当然，在复杂的脚本中，更好的方法可能是完全避免 shell 和任何脚本问题。你可以通过读取 MVG 绘图文件中的 -draw 参数来实现。\n-draw @drawfile.mvg 当然，你仍然需要对你使用的任何引号字符进行反斜杠处理，以及对文本中的任何反斜杠进行处理。然而，这比起同时处理 shell 自己的引号和转义系统要简单得多。\nconvert -size 500x50 xc:lightblue -font Candice -pointsize 36 \\  -gravity center -draw @text_quotes.mvg text_quotes.gif 第一张图片来自我使用的一个 \u0026ldquo;MVG\u0026rdquo; 文本文件。它不包含转义符或引号。因此，只有 MVG 引号和转义符是存在的。\n请注意，在上面的例子中，如果我对 MVG 文本字符串使用了单引号，唯一的变化是我需要对字符串中的单引号字符进行反斜杠处理，而不是双引号字符。\n关于百分比字符 最后一点是关于 \u0026quot;-draw text\u0026quot; 操作符中的特殊 escape 字符。百分号字符 \u0026quot;%\u0026quot; 应该\u0026quot;按原样\u0026quot;绘制。你不需要做任何特殊的操作来绘制它们。如果它们不能\u0026quot;按原样\u0026quot;绘制，那么你的IM版本较旧，应该尽快升级。\n 直到IM 6.2.4版本，\u0026quot;%\u0026quot; 字符被用作转义字符，在绘制的文本字符串中包含额外的图像信息。现在不再是这样了，因为当SVG图像也试图绘制百分数字符时，这种转义符是混乱和不正确的。 百分号 \u0026ldquo;转义符 \u0026ldquo;的使用（以及\u0026rdquo;/n \u0026ldquo;换行符）被认为与 -draw 操作符和MVG格式处理SVG图像格式的预期用途不兼容。因此，从 IM 6.2.4 版本开始，%转义就不适用了，反斜杠只能转义自己和周围的引号。\n convert -size 250x50 xc:none -box white -pointsize 20 -gravity center \\  -draw \u0026#39;text 0,0 \u0026#34;%w\\n%h\u0026#34;\u0026#39; -trim +repage text_escapes.gif 关于\u0026quot;百分比错误\u0026quot;的更多细节，以及在旧版 ImageMagick 中使用 -draw 时避免该错误的方法，请参见 Drawing a Percent Bug 页面。\n用注释代替绘制 避免这类问题的较好方法是使用 [-annotate](https://imagemagick.org/Usage/option_link.cgi?annotate) 而不是 draw 来绘制文本。这个操作符是 draw 操作符的一个封装器，可以使用 draw 的所有功能，但形式更简单。\n基本上这个操作符只需要一组引号（对于 shell）。这使得处理特殊字符变得更加简单。\n不幸的是，虽然你不再需要为 IM 转义引号，但你现在有百分比转义，如 '@' 文件读取，'\\n' 换行，以及其他百分号转义扩展。\n例如，使用单引号\u0026hellip;\nconvert -size 200x50 xc:none -box white -pointsize 20 -gravity center \\  -annotate 0 \u0026#39;\\@ \u0026#39;\\\u0026#39;\u0026#39; \u0026#34; $ \\\\ %% \u0026#39; \\  -trim +repage annotate_s.gif 而对于双引号\u0026hellip;\nconvert -size 200x50 xc:none -box white -pointsize 20 -gravity center \\  -annotate 0 \u0026#34;\\@ \u0026#39; \\\u0026#34; \\$ \\\\\\\\ %% \u0026#34; \\  -trim +repage annotate_d.gif 然而，如果你使用 '@' 转义符从文件中读取字符串，所有的注释引号和转义符将被完全忽略。\n例如，我们在这里包含了一个图像的宽度和高度的信息!\nconvert -size 200x50 xc:none -box white -pointsize 20 -gravity center \\  -annotate 0 \u0026#39;%w\\n%h\u0026#39; -trim +repage annotate_percents.gif 然而，当从文件中读取注释字符串时，所有的转义都会被完全忽略。\necho -n \u0026#39;@ %w\\n%h\u0026#39; |\\  convert -size 200x50 xc:none -box white -pointsize 20 -gravity center \\  -annotate 0 \u0026#39;@-\u0026#39; -trim +repage annotate_file.gif 更多信息请参见 Annotate Text Drawing Operator，特别是 Annotate Escape Characters。\nIM 和 SVG 处理 SVG 输入驱动。RSVG 与 MSVG 处理实际的 SVG 图像格式是一项非常复杂的工作。引擎需要处理 SVG\u0026ndash;可缩放矢量图形文档所定义的所有方面。这需要大量的编程工作和时间。\n因此，ImageMagick 在 SVG 格式图像的处理上提供了两种方法。第一种是使用一个开源的 RSV G库，将 SVG 格式转换成 IM 没有问题的光栅图像。这个引擎几乎在 SVG 处理的所有方面都是完整的。\n第二种方法是IM尝试将 SVG 转换为 MVG，使用一个名为 MSVG 的内置IM方法。MSVG 试图将SVG图像转换成IM的 -draw 运算符 \u0026ldquo;MVG\u0026quot;绘图语言。绘制 MVG 的很多功能都是专门为此而创建的。不幸的是，虽然基本的线条绘制和着色功能是存在的，但它远不是一个完整的 SVG 转换器。\n你可以通过使用特殊的输入格式 \u0026ldquo;MSVG:\u0026quot;（IM v6.3.4新增）读取 SVG 图像来强制使用内部的 MSVG 转换器。但如果 RSVG 库存在，大多数 ImageMagick 将使用它来渲染 SVG 图像。\n要找出你的 IM 会做什么，请用\u0026hellip;\nconvert -list format | grep SVG 从括号中的 \u0026ldquo;RSVG\u0026rdquo; 可以看出，我自己的 IM 将使用我电脑上的 RSVG 库，并给出了版本。\n在这里，我\u0026quot;绘制\u0026quot;了一个小的、手工制作的 SVG 图像 \u0026ldquo;diagonal.svg\u0026quot;（由论坛用户 penciledin 贡献），它在白色背景上创建了一个简单的对角线渐变的矩形。\nconvert diagonal.svg diagonal_rsvg.gif 完美的。一个正确的对角线梯度被生成。\n然而，如果你使用内部的 MSVG（如果没有 RSVG 库，则为默认值）来渲染\u0026hellip;\nconvert msvg:diagonal.svg diagonal_msvg.gif 正如你所看到的，内部 MSVG 转换失败，返回的是垂直梯度而不是对角线。\nconvert msvg:diagonal.svg diagonal.mvg 你大概可以看到MSVG转换器是如何尝试将 SVG 转换为 MVG 绘图命令的。\n当前内部 MSVG 已知失败的地方包括\u0026hellip;\n 非垂直梯度(没有转换到新的MVG梯度处理) 沿着弯曲路径的文字 文本对齐（与重力分开）。  然而大多数基本的绘图动作都被处理了。\n还请记住，MVG 语言实际上可以处理 SVG 不能处理的事情，包括使用重力来定位图像和文本。重力不是 SVG 规范的一部分，尽管它是 IM 文本和字体处理的一个组成部分。\n另外请记住，MVG 没有 SVG 所拥有的容器机制。内部的 MSVG 转换器用图形上下文的推送和弹出代替了 XML 容器（见上面的 MVG 输出），效果是一样的。 SVG 的设置\nSVG 图像格式是一种矢量格式（请参见关于矢量图像格式的一句话），因此图像通常没有一个默认的\u0026quot;大小\u0026rdquo;，而是以特定的  \u0026quot;-density\u0026quot; 来 \u0026ldquo;绘制\u0026quot;或\u0026quot;渲染\u0026rdquo;，就像 postscript 一样（默认密度是72dpi）。\n另外，如果 SVG 没有 \u0026ldquo;绘制\u0026quot;背景，您可以通过使用 -background 设置指定要使用的背景颜色。\n例如这里是另一个小的 SVG 图像 \u0026ldquo;home.svg\u0026rdquo;，它已经使用3种不同的密度，3种不同的背景进行\u0026quot;渲染\u0026rdquo;，包括一个透明的背景。\nconvert -density 36 home.svg home_1.gif convert -background skyblue home.svg home_2.gif convert -density 144 -background none home.svg home_3.png 请注意，我使用了 PNG 格式的图片来制作上面例子中较大的透明背景版本。由于半透明的边缘像素，这产生的图像比 GIF 图像格式产生的图像更干净。当最终图像中涉及到透明度时，总是建议使用 PNG。\n 我发现有些 SVG 图像不能缩放。也就是说，它们被定义为\u0026quot;像素\u0026rdquo;，而不是现实世界中的长度，如\u0026quot;点\u0026rdquo;、\u0026ldquo;英寸\u0026quot;或\u0026quot;毫米\u0026rdquo;。因此，虽然 -density 设置可能会改变图像的整体大小（以现实世界为单位），但 \u0026ldquo;像素 \u0026ldquo;的大小不会改变，因此图像本身的大小也不会改变。然而这样的SVG图像是相当罕见的。\n 更糟糕的是，一些 SVG 图像使用了\u0026quot;像素\u0026quot;和 \u0026ldquo;点\u0026quot;的混合测量，除非作者故意这样做，否则你可能会得到一个真正的混乱，你可以尝试使用不同的密度，而不是作者想要的。幸运的是，这种情况更加罕见。\n一个简单的解决方法就是将 SVG 中所有的 \u0026ldquo;像素\u0026quot;单位改成\u0026quot;点\u0026rdquo;，但也不能盲目的使用，以防故意使用\u0026quot;像素\u0026rdquo;。\nSVG 输出处理 从 IM v6.4.2 开始，IM 可以将任何位图图像转换为 SVG 矢量图! 转换并不总是成功的，但较大和/或较简单的图像（如位图蒙版）会转换得很好。\n例如，我在这里将一个可怕的位图形状转换为SVG图像，然后再将其转换回来，以便将位图平滑为一个适当的反锯齿形状。\nconvert -pointsize 72 -font Candice label:A -threshold 50% \\  -trim +repage -bordercolor white -border 5x5 A.gif convert A.gif A.svg convert A.svg A.png -\u0026gt; 然而，要使之工作，必须安装\u0026quot;开发\u0026quot;的 AutoTrace 库，并在IM中配置 -with-autotrace 开关。\n如果没有安装 AutoTrace 库并将其编译到 IM 中，那么生成的 SVG 输出将是大量的单像素圆圈，生成一个二进制结果，而不是一个平滑的 SVG 轮廓图像。这样的图像比较巨大，通过 SVG 渲染往往需要很长的时间来渲染。\n其实需要一种更好的默认栅格到矢量的技术，可能会使用 Morphology skeletion 和 MAT 技术。\n有一个 autotrace: 输入代理，来\u0026quot;平滑输入位图图像\u0026rdquo;，直接使用 \u0026ldquo;autotrace\u0026rdquo; 命令一次性完成上述所有步骤。然而我最后一次看到这个代理已经消失了。\n你应该这样使用它\u0026hellip;\nconvert autotrace:A.gif A_traced.png 当然这并不能让你从 \u0026ldquo;autotrace\u0026rdquo; 命令中得到 SVG 输出，只是通过 SVG 过滤输入的图像来平滑它。\n作为一种替代方法，你可以直接使用 \u0026ldquo;autotrace\u0026rdquo; 命令，如 Raster to Vector Edging 和 Skeleton using Autotrace 示例所示。\n你也可以看看 cancerberosgx 的结果，在生成 SVG 图像，他研究了转换照片的解决方案。\n非IM矢量图形编辑器 ImageMagick 是一个像素数组处理器，它一般不会保存矢量图像（\u0026lsquo;MVG\u0026rsquo; 是唯一的例外），只会读取图像并将其转换为像素数组。\n其他像素图像编辑器也是如此，如 Gimp、Photoshop 等。\n对于编辑和处理基于矢量的图像，可以使用以下程序，如\n Sodipodi 基于SVG的矢量图形编辑器。 Xfig 简单但非常好的矢量对象编辑器。(适用于标志、地图和在页面上排列照片) Dia AutoTrace 将位图数组中的形状转换为矢量轮廓。 Sketch 基于 Python 的曲线文字矢量编辑器。  当然，这不是一个完整的列表。即使是许多文字处理程序，如 OpenOffice、Word 和 TeX，一般都有各种简单的，虽然往往难以使用的对象编辑器。\n然而对于一般将矢量图形格式转换为不同的矢量格式，不要使用 ImageMagick。ImageMagick 本质上是一个光栅图像或位图图形转换器和操作器，而且永远都是这样。更多信息请参见 A word about Vector Image formats。\n 创建于: 24 March 2004 更新于: 14 March 2011 作者: Anthony Thyssen, Anthony.Thyssen@gmail.com Examples Generated with: [version image] URL: http://www.imagemagick.org/Usage/draw/  ","permalink":"https://ohmyweekly.github.io/notes/imagemagick-drawing/","tags":["imagemagick","image"],"title":"ImageMagick - Drawing"},{"categories":["programming"],"contents":"https://imagemagick.org/Usage/transform/index.html\n","permalink":"https://ohmyweekly.github.io/notes/imagemagick-transform/","tags":["imagemagick","transform"],"title":"ImageMagick - Transform"},{"categories":["programming"],"contents":"几个有意思的 ImageMagick 脚本 http://www.fmwconcepts.com/imagemagick/randomclipart/index.php http://www.fmwconcepts.com/imagemagick/sketching/index.php http://www.fmwconcepts.com/imagemagick/sphericalpano2cube/index.php http://www.fmwconcepts.com/imagemagick/surroundblur/index.php http://www.fmwconcepts.com/imagemagick/transfercolor/index.php http://www.fmwconcepts.com/imagemagick/colorcells/index.php\n给图片添加网格线 ./glasseffects -e none -k simple -o \u0026#39;#FFDAB9\u0026#39; -t single -m overlay -c 200 -w 1 -s 20 -r 10 in.jpeg out.jpg ./grid -o 0.3 -s 200 -c white in.jpeg out.jpeg 折叠图片 ./picturefold -o 80 -h 50 in.jpeg out.jpeg 给图片添加心形图片 ./randomclipart -d \u0026#34;64,16\u0026#34; -a \u0026#34;45,-45\u0026#34; -p 50 -c random in.jpeg heart.png out.jpeg 给图片分成带颜色的方块 ./colorcells -n 8,8 -d 100,100 in.jpeg out.jpeg ","permalink":"https://ohmyweekly.github.io/notes/imagemagick-notes/","tags":["imagemagick","image"],"title":"ImageMagick 笔记"},{"categories":["ffmpeg"],"contents":"概要 ffmpeg [global_options] {[input_file_options] -i input_url} ... {[output_file_options] output_url} ... 描述 ffmpeg 是一款非常快速的视频和音频转换器，它还可以从实时音频/视频源中抓取。它还可以在任意采样率之间进行转换，并通过高质量的多相滤波器在飞行中调整视频大小。\nffmpeg 从任意数量的输入\u0026quot;文件\u0026quot;（可以是常规文件、管道、网络流、抓取设备等）中读取，由 -i 选项指定，并写入任意数量的输出\u0026quot;文件\u0026quot;，由一个普通的输出 url 指定。在命令行中找到的任何不能被解释为选项的东西都被认为是一个输出 url。\n原则上，每个输入或输出 url 可以包含任意数量的不同类型的流（视频/音频/字幕/附件/数据）。允许的流的数量和/或类型可能受到容器格式的限制。选择哪些输入的流将进入哪些输出，可以自动完成，也可以使用 -map 选项完成（请参见流选择章节）。\n要在选项中引用输入文件，您必须使用它们的索引（基于 0）。例如，第一个输入文件是 0，第二个是 1，等等。同样，一个文件中的流也用它们的索引来表示。例如，2:3 指的是第三个输入文件中的第四个流。也请参见流指定符一章。\n一般来说，选项会应用到下一个指定的文件。因此，顺序是很重要的，您可以在命令行中多次出现同一个选项。每次出现都会被应用到下一个输入或输出文件。这条规则的例外是全局选项（例如 verbosity level），应该先指定。\n不要混合输入和输出文件-首先指定所有输入文件，然后再指定所有输出文件。也不要混合属于不同文件的选项。所有选项只适用于下一个输入或输出文件，并在文件之间被重置。\n 要将输出文件的视频比特率设置为 64 kbit/s:  ffmpeg -i input.avi -b:v 64k -bufsize 64k output.avi  要强制输出文件的帧率为24帧/秒:  ffmpeg -i input.avi -r 24 output.avi  强制输入文件的帧率（仅对原始格式有效）为1帧/秒，输出文件的帧率为24帧/秒:  ffmpeg -r 1 -i input.m2v -r 24 output.avi 原始输入文件可能需要格式选项。\n详情描述 ffmpeg 中每个输出的转码过程可以用下面的图来描述:\n _______ ______________ | | | | | input | demuxer | encoded data | decoder | file | ---------\u0026gt; | packets | -----+ |_______| |______________| | v _________ | | | decoded | | frames | |_________| ________ ______________ | | | | | | | output | \u0026lt;-------- | encoded data | \u0026lt;----+ | file | muxer | packets | encoder |________| |______________| ffmpeg 调用 libavformat 库（包含 demuxers）来读取输入文件，并从其中获取包含编码数据的数据包。当有多个输入文件时，ffmpeg 试图通过跟踪任何活动输入流上的最低时间戳来保持它们的同步。\n编码后的数据包会被传递给解码器（除非为流选择了 streamcopy，详见下文）。解码器产生未压缩的帧（原始视频/PCM音频/\u0026hellip;），这些帧可以通过过滤进一步处理（见下一节）。过滤后，这些帧被传给编码器，编码器对它们进行编码并输出编码数据包。最后，这些帧被传给 muxer，muxer 将编码后的数据包写入输出文件。\n滤波 在编码之前，ffmpeg 可以使用 libavfilter 库中的过滤器处理原始音频和视频帧。ffmpeg 区分了两种类型的滤波图：简单和复杂。\n简单的滤波图 简单的滤波图是指那些只有一个输入和输出的滤波图，两者类型相同。在上图中，它们可以通过简单地在解码和编码之间插入一个额外的步骤来表示:\n _________ ______________ | | | | | decoded | | encoded data | | frames |\\ _ | packets | |_________| \\ /||______________| \\ __________ / simple _\\|| | / encoder filtergraph | filtered |/ | frames | |__________| 简单的滤波图是用 per-stream -filter 选项配置的（视频和音频分别用 -vf 和 -af 别名）。例如，一个简单的视频滤波图可以是这样的:\n _______ _____________ _______ ________ | | | | | | | | | input | ---\u0026gt; | deinterlace | ---\u0026gt; | scale | ---\u0026gt; | output | |_______| |_____________| |_______| |________| 请注意，有些滤镜会改变帧的属性，但不会改变帧的内容。例如，上面例子中的 fps 过滤器改变了帧数，但没有触及帧内容。另一个例子是 setpts 过滤器，它只设置了时间戳，而在其他方面没有改变帧的内容。\n复杂的滤波图 复杂的滤波图是那些不能简单地描述为应用于一个流的线性处理链的图。例如，当图形有一个以上的输入和/或输出时，或者当输出流类型与输入不同时，就会出现这种情况。它们可以用下图来表示:\n _________ | | | input 0 |\\ __________ |_________| \\ | | \\ _________ /| output 0 | \\ | | / |__________| _________ \\| complex | / | | | |/ | input 1 |----\u0026gt;| filter |\\ |_________| | | \\ __________ /| graph | \\ | | / | | \\| output 1 | _________ / |_________| |__________| | | / | input 2 |/ |_________| 复杂的滤波图是用 -filter_complex 选项配置的。注意这个选项是全局性的，因为复杂的滤波图，就其本质而言，不能明确地与一个单一的流或文件相关联。\n-lavfi 选项相当于 -filter_complex。\n一个简单的例子是 overlay 滤波器，它有两个视频输入和一个视频输出，其中一个视频叠加在另一个视频上。它的音频对应的是 amix 滤波器。\n流复制 流复制是通过向 -codec 选项提供 copy 参数来选择的模式，它使 ffmpeg 省略了对指定流的解码和编码步骤，因此它只做解复用(demuxing)和混叠(muxing)。它对于改变容器格式或修改容器级元数据非常有用。上面的图，在这种情况下，会简化成这样:\n _______ ______________ ________ | | | | | | | input | demuxer | encoded data | muxer | output | | file | ---------\u0026gt; | packets | -------\u0026gt; | file | |_______| |______________| |________| 由于不需要解码或编码，所以速度非常快，而且没有质量损失。但是，由于很多因素的影响，在某些情况下可能无法工作。应用过滤器显然也是不可能的，因为过滤器是在未压缩的数据上工作的。\n流选择 ffmpeg 提供了 -map 选项来手动控制每个输出文件的流选择。用户可以跳过 -map 选项，让 ffmpeg 执行自动流选择，如下所述。-vn / -an / -sn / -dn 选项可以分别用来跳过视频、音频、字幕和数据流，无论是手动映射还是自动选择，但那些复杂的滤波图输出的流除外。\n描述： 下面的小节描述了涉及到流选择的各种规则。接下来的例子将展示这些规则是如何在实践中应用的。\n虽然我们尽力准确地反映了程序的行为，但 FFmpeg 仍在不断地开发中，代码可能会在写这篇文章的时候有所改变。\n自动选择流 在没有任何特定输出文件的映射选项的情况下，ffmpeg 会检查输出格式，以检查哪些类型的流可以被包含在其中，即视频、音频和/或字幕。对于每一种可接受的流类型，ffmpeg 将从所有输入中选择一个可用的流。\n它将根据以下标准选择该流:\n 对于视频，它是最高分辨率的流, 对于音频来说，它是拥有最多通道的流, 对于字幕，它是第一个找到的字幕流，但有一个注意事项。输出格式的默认字幕编码器可以是基于文本的，也可以是基于图像的，而且只会选择相同类型的字幕流。  在几个相同类型的流速率相同的情况下，会选择指数最低的流。\n数据流或附件流不会被自动选择，只能使用 -map 来包含。\n手动选择流 当使用 -map 时，只有用户映射的流才会被包含在该输出文件中，下面描述的滤波图输出可能是一个例外。\n复杂的滤波图 如果有任何复杂的滤波图输出流带有未标记的填补(pad)，它们将被添加到第一个输出文件中。如果流类型不被输出格式支持，这将导致一个致命的错误。在没有 map 选项的情况下，包含这些流会导致自动选择流的类型被跳过。如果存在 map 选项，这些滤波图流会被包含在映射流之外。\n带有标签填补的复杂滤波图输出流必须被映射一次，而且是精确地映射一次。\n流处理 流处理是独立于流选择的，下面描述的字幕除外。流处理是通过针对特定输出文件中的流的 -codec 选项来设置的。特别是，编解码器选项是在流选择过程之后由 ffmpeg 应用的，因此不会影响后者。如果没有为某个流类型指定 -codec 选项，ffmpeg 将选择输出文件 muxer 注册的默认编码器。\n字幕存在一个例外。ffmpeg 不会验证指定的编码器是否可以转换所选的流，也不会验证转换后的流是否可以在输出格式中接受。这通常也适用于：当用户手动设置编码器时，流选择过程不能检查编码后的流是否能被混入输出文件中。如果不能，ffmpeg 将中止，所有的输出文件将无法被处理。\n示例 下面的例子说明了 ffmpeg 流选择方法的行为、怪癖和限制。\n它们假设以下三个输入文件:\ninput file 'A.avi' stream 0: video 640x360 stream 1: audio 2 channels input file 'B.mp4' stream 0: video 1920x1080 stream 1: audio 2 channels stream 2: subtitles (text) stream 3: audio 5.1 channels stream 4: subtitles (text) input file 'C.mkv' stream 0: video 1280x720 stream 1: audio 2 channels stream 2: subtitles (image) ","permalink":"https://ohmyweekly.github.io/notes/ffmpeg/","tags":["ffmpeg","tiktok"],"title":"FFmpeg"},{"categories":["ffmpeg"],"contents":"抖音短视频有很多好听的歌, 网易云音乐、QQ音乐和虾米音乐上都没有, 但是可以把视频转成 mp3 格式。 使用 FFmpeg 来搞定。点击抖音上的转发按钮, 如果「保存本地」的按钮不是灰色的, 表示可以下载到手机上。下载完后, 再转发到电脑上, 下载好 ffmpeg:\nbrew install ffmpeg 从视频中采集音频 ffmpeg -i input.mp4 -f mp3 -vn output.mp3 或\nffmpeg -i input.mp4 -q:a 0 -map a output.mp3 或者只截取全部音频中的一小段:\nffmpeg -ss 00:00:03 -t 0:0:14 -i alcastar.mp4 -f mp3 -vn alcastar.mp3 其中 -ss 00:00:03 用于指定要截取的音频的起始时间, 即从第三秒开始截取; -t 0:0:14 用于指定要截取的音频的持续时长, 即截取 14 秒的音频。 -i alcastar.mp4 用于指定输入文件, 即下载好的视频文件; -f mp3 用于指定输出格式为 mp3; -vn 即 no vedio, 即不保留视频; 最后的 alcastar.mp3 是输出文件名。\n去除音频中的静音 ffmpeg -i input.mp3 -af silenceremove=1:0:-50dB output.mp3 给视频添加字幕 ffmpeg -i input.mp4 -i SRT文件 -c copy -c:s mov_text -metadata:s:s:0 language=\u0026lt;language code\u0026gt; output.mp4 移除超过1秒的静止画面 ffmpeg -i in.mp4 -vf \u0026#34;select=\u0026#39;if(gt(scene,0.01),st(1,t),lte(t-ld(1),1))\u0026#39;,setpts=N/FRAME_RATE/TB\u0026#34; trimmed.mp4 将图片和音频合成视频 ffmpeg -loop 1 -i $image -i $audio_file -q:v 1 -c:a copy -shortest $video_file 将多个视频合并成一个 ffmpeg -safe 0 -f concat -i $list_file -c:v libx264 $final list file 的格式是：\nfile './data_1.ts' file './data_2.ts' file './data_3.ts' ","permalink":"https://ohmyweekly.github.io/notes/ffmpeg-notes/","tags":["ffmpeg","tiktok"],"title":"使用 FFmpeg 提取抖音短视频中的音乐"},{"categories":["programming"],"contents":"Dart 生态系统使用包来共享软件，如库和工具。本页告诉你如何创建一个包，重点是最常见的一种包，库包。\n是什么造就了一个库包 下图是最简单的库包的布局:\n一个库的最低要求是\npubspec 文件\n库的 pubspec.yaml 文件和应用程序包的文件是一样的-没有特别的名称来表示这个包是一个库。\nlib 目录\n正如你所期望的那样，库代码存在于 lib 目录下，对其他包是公开的。你可以根据需要在 lib 下创建任何层次结构。按照惯例，实现代码被放在 lib/src 下。lib/src 下的代码被认为是私有的；其他包不应该需要导入 src/...。要使 lib/src 下的 API 公开，您可以从直接位于 lib 下的文件导出 lib/src 文件。\n注意：当没有指定 library 指令时，会根据每个库的路径和文件名为其生成一个唯一的标签。因此，我们建议您从代码中省略 library 指令，除非您计划生成库级文档。\n组织一个库包 当你创建小的、单独的库（称为迷你库）时，库包的维护、扩展和测试是最容易的。在大多数情况下，每个类都应该在自己的迷你库中，除非你有两个类是紧密耦合的情况。\n注意：你可能听说过 part 指令，它允许你将一个库分割成多个 Dart 文件。我们建议你避免使用 part 指令，而是创建迷你库。\n直接在 lib 下创建一个\u0026quot;主\u0026quot;库文件，lib/\u0026lt;package-name\u0026gt;.dart，导出所有的公共 API。这样用户就可以通过导入一个文件来获得一个库的所有功能。\nlib 目录也可能包含其他可导入的、非src的库。例如，也许你的主库可以跨平台使用，但是你创建了单独的库，这些库依赖于 dart:io 或者 dart:html。有些包有单独的库，这些库是要用前缀导入的，而主库不是。\n让我们来看看一个现实世界中的库包的组织： shelf。shelf 包提供了一种使用 Dart 创建 web 服务器的简单方法，它的布局结构是 Dart 库包常用的:\n直接在 lib 下，主库文件 shelf.dart 从 lib/src 导出几个文件:\nexport \u0026#39;src/cascade.dart\u0026#39;; export \u0026#39;src/handler.dart\u0026#39;; export \u0026#39;src/handlers/logger.dart\u0026#39;; export \u0026#39;src/hijack_exception.dart\u0026#39;; export \u0026#39;src/middleware.dart\u0026#39;; export \u0026#39;src/pipeline.dart\u0026#39;; export \u0026#39;src/request.dart\u0026#39;; export \u0026#39;src/response.dart\u0026#39;; export \u0026#39;src/server.dart\u0026#39;; export \u0026#39;src/server_handler.dart\u0026#39;; shelf 包还包含一个迷你库： shelf_io。这个适配器处理来自 dart:io 的 HttpRequest 对象。\n对网络应用的提示: 为了在使用 dartdevc 开发时获得最佳性能，请将实现文件放在 /lib/src 下，而不是放在 /lib 下的其他地方。同时，避免导入 package:package_name/src/... 的文件。\n导入库文件 当从其他包中导入一个库文件时，使用 package: 指令来指定该文件的 URI。\nimport \u0026#39;package:utilities/utilities.dart\u0026#39;; 当从自己的包中导入一个库文件时，当两个文件都在 lib 内，或者两个文件都在 lib 外时，使用相对路径。使用 :package 当导入的文件在 lib 内，而导入者在 lib 外时。\n下图显示了如何从 lib 和 web 中导入 lib/foo/a.dart。\n有条件地导入和导出库文件 如果你的库支持多个平台，那么你可能需要有条件地导入或导出库文件。一个常见的用例是一个同时支持 web 和原生平台的库。\n要有条件的导入或导出，你需要检查 dart:* 库的存在。下面是一个有条件导出代码的例子，它检查 dart:io 和 dart:html 的存在:\nexport \u0026#39;src/hw_none.dart\u0026#39; // Stub implementation if (dart.library.io) \u0026#39;src/hw_io.dart\u0026#39; // dart:io implementation if (dart.library.html) \u0026#39;src/hw_html.dart\u0026#39;; // dart:html implementation 下面是这段代码的作用。\n 在一个可以使用 dart:io 的应用程序中(例如，一个命令行应用程序)，导出 src/hw_io.dart 在一个可以使用 dart:html 的应用程序中(一个 web 应用程序)，导出 src/hw_html.dart 否则，导出 src/hw_none.dart  要有条件地导入一个文件，使用与上面相同的代码，但将 exporrt 改为 import。\n注意：有条件的导入或导出只检查库在当前平台上是否可用，而不是检查是否实际导入或使用。\n所有有条件导出的库都必须实现相同的 API。例如，这里是 dart:io 的实现:\nimport \u0026#39;dart:io\u0026#39;; void alarm([String text]) { stderr.writeln(text ?? message); } String get message =\u0026gt; \u0026#39;Hello World from the VM!\u0026#39;; 这里是默认的实现，它是一个抛出 UnsupportedErrors 的 stub。\nvoid alarm([String text]) =\u0026gt; throw UnsupportedError(\u0026#39;hw_none alarm\u0026#39;); String get message =\u0026gt; throw UnsupportedError(\u0026#39;hw_none message\u0026#39;); 在任何平台上，你都可以导入有条件导出代码的库。\nimport \u0026#39;package:hw_mp/hw_mp.dart\u0026#39;; void main() { print(message); } 提供补充文件 一个设计良好的库包是很容易测试的。我们建议你使用 test 包来编写测试，将测试代码放在测试包顶部的 test 目录中。\n如果你创建了任何旨在供公众使用的命令行工具，请将这些工具放在 bin 目录下，这是公共的。启用从命令行运行工具，使用 pub global activate。将工具列在 pubspec 的可执行文件部分，允许用户直接运行它，而无需调用 pub global run。\n如果你包含了一个如何使用你的库的例子，这将会很有帮助。这将被放入软件包顶部的 example 目录中。\n你在开发过程中创建的任何工具或可执行文件，如果不是公开使用的，都会进入 tool 目录。\n如果你把你的库发布到 pub.dev 站点，其他需要的文件，如 README.md 和 CHANGELOG.md，将在发布软件包中描述。有关如何组织包目录的更多信息，请参见 pub 包布局惯例。\n编写库文档 你可以使用 dartdoc 工具为你的库生成 API 文档。Dartdoc 解析源码寻找文档注释，其中使用了 /// 语法:\n/// The event handler responsible for updating the badge in the UI. void updateBadge() { ... } 关于生成文档的例子，请看 shelf 文档。\n注意：要在生成的文档中包含任何库级文档，你必须指定 library 指令。请参阅 问题 1082。\n分发一个开源库 如果你的库是开源的，我们建议在 pub.dev 站点上分享它。要发布或更新库，请使用 pub publish，它可以上传您的包并创建或更新其页面。例如，请看 shelf 包的页面。有关如何准备发布软件包的详细信息，请参见发布包。\npub.dev 站点不仅托管您的软件包，而且还生成和托管您软件包的 API 参考文档。最新生成的文档的链接在软件包的 About 框中；例如，请看 shelf 包的 API 文档。到以前版本的文档的链接在软件包页面的版本选项卡中。\n要确保你的软件包的 API 文档在 pub.dev 网站上看起来不错，请按照以下步骤进行。\n 在发布你的软件包之前，运行 dartdoc 工具，以确保你的 docs 成功生成，并且看起来符合预期。 发布软件包后，检查 Versions 选项卡以确保文档成功生成。 如果文档根本没有生成，点击 Verrsions 选项卡中的 failed，查看 dartdoc 的输出。  资源 使用以下资源了解更多关于库包的信息。\n 语言之旅中的库和可见性包括使用库文件。 包文档很有用，特别是包的布局约定。 不应提交的内容涵盖了哪些不应该被检查到源代码库中。 dart-lang 组织下的较新的库包倾向于展示最佳实践。可以考虑研究这些例子：dart_style、path、shelf、source_gen 和 test。  ","permalink":"https://ohmyweekly.github.io/notes/creating-packages/","tags":["flutter","packages"],"title":"创建包"},{"categories":["programming"],"contents":"pub 软件包管理器不仅仅是用来使用别人的软件包。它还允许你与世界分享你的软件包。如果您有一个有用的项目，并且您希望其他人能够使用它，请使用 pub publish 命令。\n注意: 如果要发布到 pub.dev 以外的其他位置，或者要防止在任何地方发布，请使用 pubspec 中定义的 publish_to 字段。\n发布是永远的 请记住，发布是永远的。只要你发布你的包，用户就可以依赖它。一旦他们开始这样做，删除包就会破坏他们的包。为了避免这种情况，pub.dev 政策不允许取消发布软件包，除非是极少数情况。\n你可以随时上传你的包的新版本，但旧的包将继续为那些还没有准备好升级的用户提供服务。\n对于已经发布的包，如果不再相关或正在维护，你可以将其标记为停止发布。\n准备发布 当发布一个软件包时，遵循 pubspec 格式和包布局惯例是很重要的。其中有些是必须的，以便其他人能够使用你的软件包。另一些则是为了帮助用户更容易理解和使用您的软件包而提出的建议。在这两种情况下，pub 都会尝试帮助你，指出哪些改变会帮助你的软件包在 Dart 生态系统中发挥得更好。上传包有一些额外的要求:\n  你必须包含一个包含开源许可证的 LICENSE 文件。我们推荐 BSD 许可证，这是 Dart 自己使用的。你也必须有合法的权利来重新发布你上传的任何东西作为你的包的一部分。\n  你的软件包在经过 gzip 压缩后必须小于 10 MB。如果太大，可以考虑将其分割成多个包，或者减少包含的资源或例子的数量。\n  你的包应该只依赖托管的依赖项(来自默认的 pub 包服务器)和 SDK 依赖项(sdk: flutter)。这些限制确保了你的包的依赖性不会在未来变得不可用。\n  您必须有一个 Google 帐户，pub 用来管理包的上传权限。您的 Google 账户可以与 Gmail 地址或任何其他电子邮件地址关联。\n  注意：除非您使用已验证的发布者发布，否则 pub.dev 会显示与您的 Google 帐户关联的电子邮件地址。\n重要文件 Pub 使用一些文件的内容为你的包创建一个页面，地址是 pub.dev/packages/\u0026lt;your_package\u0026gt;。以下是影响你的包的页面外观的文件。\n README.md: README.md 文件是你的包页面中的主要内容。该文件的内容以 Markdown 的形式呈现。 CHANGELOG.md：CHANGELOG.md 文件是你的包页面中的主要内容。你的包的 CHANGELOG.md 文件，如果找到的话，也会在你的包页面的一个标签中显示，这样开发者就可以直接从 pub.dev 中读取它。该文件的内容会以 Markdown 的形式呈现。 pubspec: 你的包的 pubspec.yaml 文件用来在你的包的页面右侧填写关于你的包的详细信息，比如它的描述、主页等。  使用经过验证的发布者的优势 您可以使用已验证的发布者（推荐）或独立的谷歌账户发布软件包。使用经过验证的发布者有以下优势。\n 您的包的消费者知道发布者的域名已经被验证。 您可以避免让 pub.dev 显示您的个人电子邮件地址。取而代之的是，pub.dev会显示发布者的域名和联系地址。 经验证的发布者徽章 pub.dev 经验证的发布者标识会在搜索页面和单个软件包页面上显示在您的软件包名称旁边。  创建一个验证过的发布者 要创建一个已验证的发布者，请按照以下步骤进行。\n 进入 pub.dev。 使用 Google 账户登录 pub.dev。 在右上角的用户菜单中，选择创建发布者。 输入您要与您的发布者相关联的域名(例如，dart.dev)，然后单击\u0026quot;创建发布者\u0026quot;。 在确认对话框中，选择\u0026quot;确定\u0026quot;。 如果提示，完成验证流程，这将打开 Google 搜索控制台。   在添加 DNS 记录时，可能需要几个小时后，搜索控制台才会反映出变化。 验证流程完成后，返回步骤4。  发布你的包 使用 pub publish 命令来首次发布您的软件包，或将其更新到新版本。\n执行 dry run 为了测试 pub publish 的工作情况，你可以进行一次 dry run:\n$ pub publish --dry-run Pub 会确保你的软件包遵循 pubspec 格式和包布局约定，然后将你的软件包上传到 pub.dev。Pub 还会向你展示它打算发布的所有文件。下面是一个发布名为 transmogrify 的软件包的例子:\nPublishing transmogrify 1.0.0.gitignoreCHANGELOG.mdREADME.mdlibtransmogrify.dartsrctransmogrifier.darttransmogrification.dartpubspec.yamltesttransmogrify_test.dartPackage has 0 warnings.发布 当你准备好发布你的包时，请删除 --dry-run 参数:\n$ pub publish 注意: pub 命令目前不支持直接将新软件包发布到已验证的发布者。作为一个临时的变通方法，可以将新的软件包发布到Google账户，然后将包转移到发布者。\n一旦软件包被转移到发布者，你就可以使用 pub publish 更新软件包。\n当你的包成功上传到 pub.dev 后，任何 pub 用户都可以下载它或在他们的项目中依赖它。例如，如果你刚刚发布了 1.0.0 版本的 transmogrify 包，那么另一个 Dart 开发者可以在他们的 pubspec.yaml 中添加它作为依赖:\ndependencies:transmogrify:^1.0.0将软件包传输给已验证的发布者 要将一个软件包转移到已验证的发布者，您必须是该软件包的上传者和已验证发布者的管理员。\n注意：这个过程是不可逆的。一旦你将一个软件包转移到一个发布者，你不能将它转移回个人账户。\n以下是如何将软件包转移到已验证的发布者:\n 用一个被列为软件包上传者的 Google 账户登录到 pub.dev。 进入软件包的详细信息页面(例如，https://pub.dev/packages/http)。 选择\u0026quot;管理\u0026quot;选项卡。 输入发布者的名称，然后单击\u0026quot;传输到发布者\u0026quot;。  哪些文件会被发布？ 您的软件包中的所有文件都包含在已发布的软件包中，但有以下例外:\n 任何包的目录。 您的软件包的 lockfile 文件。 如果你没有使用 Git，所有隐藏的文件（也就是名字以 . 开头的文件）。 如果使用 Git，则是所有被 .gitignore 文件忽略的文件。  请确保删除任何你不想包含的文件(或将它们添加到 .gitignore 中)。 pub publish 在上传你的包之前列出了它要发布的所有文件，所以在完成上传之前要仔细检查列表。\n上传者 谁发布了软件包的第一个版本，谁就会自动成为第一个也是唯一一个被授权上传该软件包其他版本的人。要允许或不允许其他人上传版本，请使用 pub uploader 命令或将软件包转移到已验证的发布者那里。\n如果一个软件包有一个经过验证的发布者，那么该软件包的 pub.dev 页面会显示发布者的域名。否则，该页面将显示该软件包的授权上传者的电子邮件地址。\n发布预发包 当你在做一个包的时候，考虑把它作为一个预发布。当以下任何一种情况发生时，预发布都是有用的。\n 你正在积极开发软件包的下一个主要版本。 你想为软件包的下一个发行候选版本招募测试者。 该包依赖于 Dart 或 Flutter SDK 的不稳定版本。  正如在语义版本化中所描述的那样，为了使一个版本的预发布，你要给版本附加一个后缀。例如，要对 2.0.0 版本进行预发布，你可以使用 2.0.0-dev.1 版本。以后，当你发布 2.0.0 版本时，它将优先于所有 2.0.0-XXX 预发布版本。\n因为 pub 更倾向于在可用的时候发布稳定版，所以一个预发布包的用户可能需要改变他们的依赖约束。例如，如果用户想要测试 2.1 版本的预发布包，那么他们可以指定 ^2.1.0-dev.1，而不是 ^2.0.0 或 ^2.1.0。\n注意: 如果依赖关系图中的稳定包依赖于一个 prerelease，那么 pub 会选择那个 prerelease 而不是稳定版本。\n当一个 prerelease 被发布到 pub.dev 时，软件包页面会同时显示到 prerelease 和稳定版的链接。prerelease 不会影响分析得分，不会出现在搜索结果中，也不会替换包的 README.md 和文档。\n将软件包标记为已停产的软件包 尽管软件包总是保持发布，但向开发者发出信号，表明一个软件包不再被积极维护，是很有用的。为此，您可以将一个软件包标记为 discontinued。一个已停用的软件包仍然可以在 pub.dev 上发布和查看，但它有一个清晰的 DISCONTINUED 徽章，并且不会出现在 pub.dev 的搜索结果中。\n要将软件包标记为已停用，请使用该软件包的上传者或已验证的发布者管理员的 Google 帐户登录 pub.dev。然后使用单个软件包的管理选项卡将该软件包标记为已停用。\n资源 有关更多信息，请参见以下 pub 命令的参考页面。\n pub publish pub uploader  ","permalink":"https://ohmyweekly.github.io/notes/publishing-packages/","tags":["flutter","packages"],"title":"发布包"},{"categories":["programming"],"contents":"有什么意义呢？\n HTTP 协议允许客户端和服务器进行通信。 dart:io 包有编写 HTTP 程序的类。 服务器监听主机和端口上的请求。 客户端使用 HTTP 方法请求发送请求。 http_server 包提供了更高级别的构件。  前提条件: HTTP 服务器和客户端严重依赖 future 和流，本教程中没有解释这些内容。你可以从异步编程 codelab和流教程中了解它们。\nHTTP（超文本传输协议）是一种通信协议，用于通过互联网将数据从一个程序发送到另一个程序。数据传输的一端是服务器，另一端是客户端。客户端通常是基于浏览器的（用户在浏览器中输入或在浏览器中运行的脚本），但也可能是一个独立的程序。\n服务器与主机和端口绑定（它与一个IP地址和一个端口号建立专属连接）。然后服务器监听请求。由于 Dart 的异步性，服务器可以同时处理很多请求，具体如下。\n 服务器监听 客户端连接 服务器接受并接收请求(并继续监听) 服务器可以继续接受其他请求 服务器写入请求的响应或几个请求，可能是交错的请求 服务器最终结束(关闭)响应  在 Dart 中，dart:io 库包含了编写 HTTP 客户端和服务器所需的类和函数。此外，http_server 包包含了一些更高层次的类，使其更容易编写客户端和服务器。\n重要：基于浏览器的程序不能使用 dart:io 库。\ndart:io 库中的 API 只适用于独立的命令行程序。它们不能在浏览器中工作。要从基于浏览器的客户端发出 HTTP 请求，请参考 dart:html HttpRequest 类。\n本教程提供了几个例子，说明编写 Dart HTTP 服务器和客户端是多么容易。从服务器的 hello world 开始，你将学习如何编写服务器的代码，从绑定和监听到响应请求。你还可以学习到客户端：提出不同类型的请求(GET 和 POST)，编写基于浏览器和命令行的客户端。\n获取源码  获取 Dart 教程的示例代码。 查看 httpserver 目录，其中包含本教程所需的源码。  运行 hello world 服务器 本节的示例文件：hello_world_server.dart。\n让我们从一个小型的服务器开始，用字符串 Hello, world 来响应所有的请求。\n在命令行中，运行 hello_world_server.dart 脚本:\n$ cd httpserver $ dart bin/hello_world_server.dart listening on localhost, port 4040 在任何浏览器中，访问 localhost:4040。浏览器会显示 Hello, world!。\n在这种情况下，服务器是一个 Dart 程序，客户端是你使用的浏览器。然而，你可以用 Dart 编写客户端程序-无论是基于浏览器的客户端脚本，还是独立的程序。\n快速浏览一下代码 在 hello world 服务器的代码中，一个 HTTP 服务器与主机和端口绑定，监听 HTTP 请求，并写入响应。需要注意的是，该程序导入了 dart:io 库，其中包含了服务器端程序和客户端程序的 HTTP 相关类(但不包含 Web 应用)。\nimport \u0026#39;dart:io\u0026#39;; Future main() async { var server = await HttpServer.bind( InternetAddress.loopbackIPv4, 4040, ); print(\u0026#39;Listening on localhost:${server.port}\u0026#39;); await for (HttpRequest request in server) { request.response.write(\u0026#39;Hello, world!\u0026#39;); await request.response.close(); } } 接下来的几节内容包括服务器端绑定、发出客户端 GET 请求、监听和响应。\n将服务器绑定到主机和端口 本节示例：hello_world_server.dart。\nmain() 中的第一条语句使用 HttpServer.bind() 创建一个 HttpServer 对象，并将其绑定到主机和端口。\nvar server = await HttpServer.bind( InternetAddress.loopbackIPv4, 4040, ); 该代码使用 await 异步调用 bind 方法。\n主机名 bind() 的第一个参数是指定主机名。你可以用一个字符串来指定一个特定的主机名或IP地址，也可以用 InternetAddress 类提供的这些预定义的值来指定主机。\n   值 用例     回环 IPv4 或 loopbackIPv6 服务器在 loopback 地址上监听客户端活动，该地址实际上是 localhost。使用IP协议的4或6版本。这些主要用于测试。我们建议您使用这些值而不是 localhost 或 127.0.0.1。   任何 IPv4 或 anyIPv6 服务器监听任何 IP 地址上指定端口上的客户端活动。使用IP协议的4或6版本。    默认情况下，当使用V6互联网地址时，也会使用V4监听器。\n端口 bind() 的第二个参数是指定端口的整数。端口唯一地标识主机上的服务。1024 以下的端口号为标准服务保留(0除外)。例如，FTP 数据传输通常在端口20上运行，每日报价在端口17上运行，HTTP 在端口80上运行。你的程序应该使用1024以上的端口号。如果端口已经在使用中，你的服务器的连接将被拒绝。\n侦听请求 服务器使用 await for 开始监听 HTTP 请求。每收到一个请求，代码就会发送一个 \u0026ldquo;Hello, world!\u0026rdquo; 的响应。\nawait for (HttpRequest request in server) { request.response.write(\u0026#39;Hello, world!\u0026#39;); await request.response.close(); } 你将在监听和处理请求一节中了解更多关于 HttpRequest 对象包含的内容以及如何编写响应。但首先，让我们看看客户端产生请求的一种方式。\n使用 HTML 表单发出 GET 请求 本节的示例文件：number_thinker.dart 和 make_a_guess.html。\n本节介绍了一个命令行服务器，它可以随机选择一个0到9之间的数字。客户端是一个基本的 HTML 网页，make_a_guess.html，你可以用它来猜数字。\n试试吧\n 运行数字思考者服务器  在命令行，运行 number_thinker.dart server。你应该看到类似下面的东西:\n$ cd httpserver $ dart bin/number_thinker.dart I\u0026#39;m thinking of a number: 6 启动网络服务器  从应用程序的顶部目录运行 webdev serve。\n更多信息：webdev 文档\n打开 HTML 页面  在浏览器中，进入 localhost:8080/make_a_guess.html。\n做一个猜测  选择一个数字，然后按猜测按钮。\n在客户端中没有涉及到 Dart 代码。客户端请求是通过浏览器向 Dart 服务器发出的，在 make_a_guess.html 中的 HTML 表单，它提供了一个自动制定和发送客户端 HTTP 请求的方法。该表单包含下拉列表和按钮。该表单还指定了 URL，其中包括端口号，以及请求的种类（请求方法）。它还可能包含建立查询字符串的元素。\n下面是 make_a_guess.html 中的表单 HTML。\n\u0026lt;form action=\u0026#34;http://localhost:4041\u0026#34; method=\u0026#34;GET\u0026#34;\u0026gt; \u0026lt;select name=\u0026#34;q\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;0\u0026#34;\u0026gt;0\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;1\u0026#34;\u0026gt;1\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;2\u0026#34;\u0026gt;2\u0026lt;/option\u0026gt; \u0026lt;!-- ··· --\u0026gt; \u0026lt;option value=\u0026#34;9\u0026#34;\u0026gt;9\u0026lt;/option\u0026gt; \u0026lt;/select\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Guess\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; 下面是表单的工作原理:\n 表单的 action 属性被分配给发送请求的 URL 表单的 method 属性定义了请求的类型，这里是 GET。其他常见的请求类型包括 POST、PUT 和 DELETE。 表单中任何有名称(name)的元素，比如 \u0026lt;select\u0026gt; 元素，都会成为查询字符串中的一个参数。 当按下提交按钮(\u0026lt;input type=\u0026quot;submit\u0026quot;...\u0026gt;)时，提交按钮会根据表单的内容制定请求并发送。  一个 RESTful GET 请求 REST(REpresentational State Transfer)是一套设计 Web 服务的原则。乖巧的 HTTP 客户端和服务器遵守为 GET 请求定义的 REST 原则。\n一个 GET 请求:\n 只检索数据 不会改变服务器的状态 有长度限制 可以在请求的 URL 中发送查询字符串  在这个例子中，客户端发出了一个符合 REST 的 GET 请求。\n监听和处理请求 本节的示例文件: number_thinker.dart 和 make_a_guess.html。\n现在你已经看到这个基于浏览器的客户端的例子，让我们看看数字思维服务器的 Dart 代码，从 main() 开始。\n再一次，服务器绑定了一个主机和端口。在这里，每收到一个请求都会调用顶层的 handleRequest() 方法。因为 HttpServer 实现了 Stream，所以可以使用 await for 来处理请求。\nimport \u0026#39;dart:io\u0026#39;; import \u0026#39;dart:math\u0026#39; show Random; Random intGenerator = Random(); int myNumber = intGenerator.nextInt(10); Future main() async { print(\u0026#34;I\u0026#39;m thinking of a number: $myNumber\u0026#34;); HttpServer server = await HttpServer.bind( InternetAddress.loopbackIPv4, 4041, ); await for (var request in server) { handleRequest(request); } } 当一个 GET 请求到达时，handleRequest() 方法会调用 handleGet() 来处理该请求。\nvoid handleRequest(HttpRequest request) { try { if (request.method == \u0026#39;GET\u0026#39;) { handleGet(request); } else { // ···  } } catch (e) { print(\u0026#39;Exception in handleRequest: $e\u0026#39;); } print(\u0026#39;Request handled.\u0026#39;); } 一个 HttpRequest 对象有很多属性，提供了关于请求的信息。下表列出了一些有用的属性。\n   属性 信息     method \u0026lsquo;GET\u0026rsquo;, \u0026lsquo;POST\u0026rsquo;, \u0026lsquo;PUT\u0026rsquo; 等方法中的一个。   uri 一个 Uri 对象：scheme、host、port、query string 和其他关于请求资源的信息。   response 一个 HttpResponse 对象：服务器将其响应写入其中。   headers 一个 HttpHeaders 对象：请求的头信息，包括 ContentType、内容长度、日期等。    使用方法属性 下面的数想器例子中的代码使用 HttpRequest 的 method 属性来确定收到了什么样的请求。这个服务器只处理 GET 请求。\nif (request.method == \u0026#39;GET\u0026#39;) { handleGet(request); } else { request.response ..statusCode = HttpStatus.methodNotAllowed ..write(\u0026#39;Unsupported request: ${request.method}.\u0026#39;) ..close(); } 使用 uri 属性 在浏览器中输入一个 URL 会产生一个 GET 请求，它只是简单地从指定的资源中请求数据。它可以通过附加在 URI 上的查询字符串随请求发送少量数据。\nvoid handleGet(HttpRequest request) { final guess = request.uri.queryParameters[\u0026#39;q\u0026#39;]; // ··· } 使用 HttpRequest 对象的 uri 属性来获取一个 Uri 对象，这个 Uri 对象包含了用户输入的 URL 的信息。Uri 对象的 queryParameters 属性是一个 Map，包含查询字符串的组件。通过名称来引用所需的参数。本例使用 q 来标识猜测的数字。\n设置响应的状态码 服务器应该设置状态码来表示请求的成功或失败。前面看到数想家将状态码设置为 methodNotAllowed 来拒绝非 GET 请求。在后面的代码中，为了表示请求成功，响应完成，数想家服务器将 HttpResponse 状态码设置为 HttpStatus.ok。\nvoid handleGet(HttpRequest request) { final guess = request.uri.queryParameters[\u0026#39;q\u0026#39;]; final response = request.response; response.statusCode = HttpStatus.ok; // ··· } HttpStatus.ok 和 HttpStatus.methodNotAllowed 是 HttpStatus 类中许多预定义状态码中的两个。另一个有用的预定义状态码是 HttpStatus.notFound(经典的 404）。\n除了状态码(statusCode)，HttpResponse 对象还有其他有用的属性:\n   属性 信息     contentLength 响应的长度，-1 表示事先不知道长度。   cookies 要在客户端设置的 Cookies 列表。   encoding 编写字符串时使用的编码，如 JSON 和 UTF-8。   headers 响应头，是一个 HttpHeaders 对象。    将响应写到 HttpResponse 对象 每个 HttpRequest 对象都有一个对应的 HttpResponse 对象。服务器通过响应对象将数据发回给客户端。\n使用 HttpResponse 写方法之一(write()、writeln()、writeAll() 或 writeCharCodes())将响应数据写入 HttpResponse 对象。或者通过 addStream 将 HttpResponse 对象连接到一个流，并写入流。响应完成后关闭对象。关闭 HttpResponse 对象会将数据发回给客户端。\nvoid handleGet(HttpRequest request) { // ···  if (guess == myNumber.toString()) { response ..writeln(\u0026#39;true\u0026#39;) ..writeln(\u0026#34;I\u0026#39;m thinking of another number.\u0026#34;) ..close(); // ···  } } 从独立的客户端进行 POST 请求 本节的示例文件：basic_writer_server.dart 和 basic_writer_client.dart。\n在 hello world 和 number thinker 的例子中，浏览器生成了简单的 GET 请求，对于更复杂的 GET 请求和其他类型的请求，如 POST、PUT 或 DELETE，你需要写一个客户端程序，其中有两种。\n 一个独立的客户端程序，它使用 dart:io 的 HttpClient 类。 基于浏览器的客户端，使用 dart:html 中的 API。本教程不涉及基于浏览器的客户端。要查看基于浏览器的客户端和相关服务器的代码，请参见 note_client.dart、note_server.dart 和 note_taker.html。  让我们看看一个独立的客户端，basic_writer_client.dart 和它的服务器 basic_writer_server.dart。客户端发出一个 POST 请求，将 JSON 数据保存到服务器端的文件中。服务器接受请求并保存文件。\n试试吧 在命令行上运行服务器和客户端。\n 首先，运行服务器:  cd httpserver $ dart bin/basic_writer_server.dart 在一个新的终端中，运行客户端:  $ cd httpserver $ dart bin/basic_writer_client.dart Wrote data for Han Solo. 看看服务器写入 file.txt 的 JSON 数据:\n{\u0026#34;name\u0026#34;:\u0026#34;Han Solo\u0026#34;,\u0026#34;job\u0026#34;:\u0026#34;reluctant hero\u0026#34;,\u0026#34;BFF\u0026#34;:\u0026#34;Chewbacca\u0026#34;,\u0026#34;ship\u0026#34;:\u0026#34;Millennium Falcon\u0026#34;,\u0026#34;weakness\u0026#34;:\u0026#34;smuggling debts\u0026#34;} 客户端创建一个 HttpClient 对象，并使用 post() 方法进行请求。发起一个请求涉及两个 Future。\n post() 方法建立与服务器的网络连接并完成第一个 Future，返回一个 HttpClientRequest 对象。 客户端组成请求对象并关闭它。close() 方法将请求发送到服务器并返回第二个 Future，它以一个 HttpClientResponse 对象完成。  import \u0026#39;dart:io\u0026#39;; import \u0026#39;dart:convert\u0026#39;; String _host = InternetAddress.loopbackIPv4.host; String path = \u0026#39;file.txt\u0026#39;; Map jsonData = { \u0026#39;name\u0026#39;: \u0026#39;Han Solo\u0026#39;, \u0026#39;job\u0026#39;: \u0026#39;reluctant hero\u0026#39;, \u0026#39;BFF\u0026#39;: \u0026#39;Chewbacca\u0026#39;, \u0026#39;ship\u0026#39;: \u0026#39;Millennium Falcon\u0026#39;, \u0026#39;weakness\u0026#39;: \u0026#39;smuggling debts\u0026#39; }; Future main() async { HttpClientRequest request = await HttpClient().post(_host, 4049, path) /*1*/ ..headers.contentType = ContentType.json /*2*/ ..write(jsonEncode(jsonData)); /*3*/ HttpClientResponse response = await request.close(); /*4*/ await utf8.decoder.bind(response /*5*/).forEach(print); } /1/ post() 方法需要主机、端口和请求资源的路径。除了 post() 之外，HttpClient 类还提供了其他类型的请求函数，包括 postUrl()、get() 和 open()。\n/2/ 一个 HttpClientRequest 对象有一个 HttpHeaders 对象，它包含了请求头的信息。对于一些请求头，比如 contentType，HttpHeaders 有一个针对该请求头的属性。对于其他的请求头，使用 set() 方法将该请求头放入 HttpHeaders 对象中。\n/3/ 客户端使用 write() 向请求对象写入数据。编码，在这个例子中是 JSON，与 ContentType 头中指定的类型相匹配。\n/4/ close() 方法将请求发送到服务器，完成后返回一个 HttpClientResponse 对象。\n/5/ 来自服务器的 UTF-8 响应将被解码。使用在 dart:convert 库中定义的转换器将数据转换为常规的 Dart 字符串格式。\n一个 RESTful POST 请求 与 GET 请求类似，REST 为 POST 请求提供了指导方针。\n一个 POST 请求:\n 创建一个资源(在这个例子中，一个文件) 使用一个 URI，其结构与文件和目录路径名相似；例如，URI 没有查询字符串。 以 JSON 或 XML 格式传输数据 没有状态，也不会改变服务器的状态。 无长度限制  这个例子中的客户端发出 REST 兼容的 POST 请求。\n要想看到使 REST 兼容的 GET 请求的客户端代码，请看 number_guesser.dart。它是一个独立的客户端，用于数字思考者服务器，定期进行猜测，直到猜对为止。\n在服务器中处理一个 POST 请求 本节的示例文件：basic_writer_server.dart 和 basic_writer_client.dart。\n一个 HttpRequest 对象是一个字节列表流(Stream\u0026lt;List\u0026lt;int\u0026gt;)。要获得客户端发送的数据，就要监听 HttpRequest 对象上的数据。\n如果来自客户端的请求包含了大量的数据，数据可能会以多个分块的形式到达。你可以使用 Stream 中的 join() 方法来连接这些分块的字符串值。\nbasic_writer_server.dart 文件实现了一个遵循这种模式的服务器。\nimport \u0026#39;dart:io\u0026#39;; import \u0026#39;dart:convert\u0026#39;; String _host = InternetAddress.loopbackIPv4.host; Future main() async { var server = await HttpServer.bind(_host, 4049); await for (var req in server) { ContentType contentType = req.headers.contentType; HttpResponse response = req.response; if (req.method == \u0026#39;POST\u0026#39; \u0026amp;\u0026amp; contentType?.mimeType == \u0026#39;application/json\u0026#39; /*1*/) { try { String content = await utf8.decoder.bind(req).join(); /*2*/ var data = jsonDecode(content) as Map; /*3*/ var fileName = req.uri.pathSegments.last; /*4*/ await File(fileName) .writeAsString(content, mode: FileMode.write); req.response ..statusCode = HttpStatus.ok ..write(\u0026#39;Wrote data for ${data[\u0026#39;name\u0026#39;]}.\u0026#39;); } catch (e) { response ..statusCode = HttpStatus.internalServerError ..write(\u0026#39;Exception during file I/O: $e.\u0026#39;); } } else { response ..statusCode = HttpStatus.methodNotAllowed ..write(\u0026#39;Unsupported request: ${req.method}.\u0026#39;); } await response.close(); } } /1/ 该请求有一个 HttpHeaders 对象。记得客户端将 contentType 头设置为 JSON(application/json)。该服务器拒绝不是 JSON 编码的请求。\n/2/ 一个 POST 请求对它可以发送的数据量没有限制，数据可能会以多块形式发送。此外，JSON 是 UTF-8，而 UTF-8 字符可以在多个字节上进行编码。join() 方法将这些分块放在一起。\n/3/ 客户端发送的数据是 JSON 格式的。服务器使用 dart:convert 库中的 JSON 编解码器对其进行解码。\n/4/ 请求的 URL 是 localhost:4049/file.txt。代码 req.uri.pathSegments.last 从 URI 中提取文件名: file.txt。\n关于 CORS 头的说明 如果你想为运行在不同源头（不同主机或端口）的客户端提供服务，你需要添加 CORS 头。下面的代码，取自 note_server.dart，允许从任何来源的 POST 和 OPTIONS 请求。谨慎使用 CORS 头文件，因为它们会给你的网络带来安全风险。\nvoid addCorsHeaders(HttpResponse response) { response.headers.add(\u0026#39;Access-Control-Allow-Origin\u0026#39;, \u0026#39;*\u0026#39;); response.headers .add(\u0026#39;Access-Control-Allow-Methods\u0026#39;, \u0026#39;POST, OPTIONS\u0026#39;); response.headers.add(\u0026#39;Access-Control-Allow-Headers\u0026#39;, \u0026#39;Origin, X-Requested-With, Content-Type, Accept\u0026#39;); } 更多信息，请参考维基百科的跨源资源共享一文。\n使用 http_server 包 本节的示例文件：mini_file_server.dart 和 static_file_server.dart。\n对于一些更高层次的构件，我们推荐你尝试 http_server pub 包，它包含了一组类，与 dart:io 库中的 HttpServer 类一起，使得实现 HTTP 务器更加容易。\n在本节中，我们比较了一个只使用 dart:io 的 API 编写的服务器和一个使用 dart:io 和 http_server 一起编写的具有相同功能的服务器。\n你可以在 mini_file_server.dart 中找到第一个服务器。它通过从 web 目录返回 index.html 文件的内容来响应所有请求。\n试试吧 在命令行中运行服务器:\n$ cd httpserver $ dart bin/mini_file_server.dart 在浏览器中输入 localhost:4044。服务器会显示一个 HTML 文件。\n这是迷你文件服务器的代码:\nimport \u0026#39;dart:io\u0026#39;; File targetFile = File(\u0026#39;web/index.html\u0026#39;); Future main() async { Stream\u0026lt;HttpRequest\u0026gt; server; try { server = await HttpServer.bind(InternetAddress.loopbackIPv4, 4044); } catch (e) { print(\u0026#34;Couldn\u0026#39;t bind to port 4044: $e\u0026#34;); exit(-1); } await for (HttpRequest req in server) { if (await targetFile.exists()) { print(\u0026#34;Serving ${targetFile.path}.\u0026#34;); req.response.headers.contentType = ContentType.html; try { await req.response.addStream(targetFile.openRead()); } catch (e) { print(\u0026#34;Couldn\u0026#39;t read file: $e\u0026#34;); exit(-1); } } else { print(\u0026#34;Can\u0026#39;t open ${targetFile.path}.\u0026#34;); req.response.statusCode = HttpStatus.notFound; } await req.response.close(); } } 这段代码确定文件是否存在，如果存在，则打开文件，并将文件内容管道化到HttpResponse对象。\n第二个服务器，你可以在 basic_file_server.dart 中找到它的代码，使用 http_server 包。\n试试吧 在命令行中运行服务器:\n$ cd httpserver $ dart bin/basic_file_server.dart 在浏览器中输入 localhost:4046。服务器显示与之前相同的 index.html 文件。\n在这个服务器中，处理请求的代码要短得多，因为 VirtualDirectory 类处理服务文件的细节。\nimport \u0026#39;dart:io\u0026#39;; import \u0026#39;package:http_server/http_server.dart\u0026#39;; File targetFile = File(\u0026#39;web/index.html\u0026#39;); Future main() async { VirtualDirectory staticFiles = VirtualDirectory(\u0026#39;.\u0026#39;); var serverRequests = await HttpServer.bind(InternetAddress.loopbackIPv4, 4046); await for (var request in serverRequests) { staticFiles.serveFile(targetFile, request); } } 这里，请求的资源 index.html 是由 VirtualDirectory 类中的 serviceFile() 方法提供的。你不需要写代码来打开一个文件并将其内容用管道传送到请求中。\n另一个文件服务器 static_file_server.dart 也使用 http_server 包。这个服务器可以服务于服务器目录或子目录中的任何文件。\n运行 static_file_server.dart，用 localhost:4048 这个 URL 进行测试。\n下面是 static_file_server.dart 的代码:\nimport \u0026#39;dart:io\u0026#39;; import \u0026#39;package:http_server/http_server.dart\u0026#39;; Future main() async { var staticFiles = VirtualDirectory(\u0026#39;web\u0026#39;); staticFiles.allowDirectoryListing = true; /*1*/ staticFiles.directoryHandler = (dir, request) /*2*/ { var indexUri = Uri.file(dir.path).resolve(\u0026#39;index.html\u0026#39;); staticFiles.serveFile(File(indexUri.toFilePath()), request); /*3*/ }; var server = await HttpServer.bind(InternetAddress.loopbackIPv4, 4048); print(\u0026#39;Listening on port 4048\u0026#39;); await server.forEach(staticFiles.serveRequest); /*4*/ } /1/ 允许客户端请求服务器目录内的文件。\n/2/ 一个匿名函数，处理对目录本身的请求，即 URL 不包含文件名。该函数将这些请求重定向到 index.html。\n/3/ serveFile 方法为一个文件提供服务，在这个例子中，它为目录请求服务index.html。\n/4/ VirtualDirectory 类提供的 serviceRequest 方法处理指定文件的请求。\n使用 bindSecure() 的 https 方法 本节的示例：hello_world_server_secure.dart。\n你可能已经注意到，HttpServer 类定义了一个叫做 bindSecure() 的方法，它使用 HTTPS(Hyper Text Transfer Protocol with Secure Sockets Layer)提供安全连接。要使用 bindSecure() 方法，你需要一个证书，这个证书由证书颁发机构(CA)提供。有关证书的更多信息，请参考什么是 SSL 和什么是证书？\n为了说明问题，下面的服务器 hello_world_server_secure.dart 使用 Dart 团队创建的证书调用 bindSecure() 进行测试。你必须为你的服务器提供自己的证书。\nimport \u0026#39;dart:io\u0026#39;; String certificateChain = \u0026#39;server_chain.pem\u0026#39;; String serverKey = \u0026#39;server_key.pem\u0026#39;; Future main() async { var serverContext = SecurityContext(); /*1*/ serverContext.useCertificateChain(certificateChain); /*2*/ serverContext.usePrivateKey(serverKey, password: \u0026#39;dartdart\u0026#39;); /*3*/ var server = await HttpServer.bindSecure( \u0026#39;localhost\u0026#39;, 4047, serverContext, /*4*/ ); print(\u0026#39;Listening on localhost:${server.port}\u0026#39;); await for (HttpRequest request in server) { request.response.write(\u0026#39;Hello, world!\u0026#39;); await request.response.close(); } } /1/ 安全网络连接的可选设置在 SecurityContext 对象中指定，有一个默认的对象 SecurityContext.defaultContext，包括知名证书机构的可信根证书。\n/2/ 一个包含从服务器证书到签名机关根证书链的文件，格式为 PEM。\n/3/ 一个包含（加密的）服务器证书私钥的文件，PEM 格式。\n/4/ 在服务器上，上下文参数是必需的，对客户端来说是可选的。如果省略它，则使用默认的内置可信根的上下文。\n其他资源 请访问这些 API 文档，了解本教程中讨论的类和库的更多细节。\n   Dart 类 目的     HttpServer 一个 HTTP 服务器   HttpClient 一个 HTTP 客户端   HttpRequest 一个服务器端请求对象   HttpResponse 一个服务器端响应对象   HttpClientRequest 一个客户端请求对象   HttpClientResponse 一个客户端响应对象   HttpHeaders 请求头   HttpStatus 响应的状态   InternetAddress 一个互联网地址   SecurityContext 包含安全连接的证书、密钥和信任信息。   http_server 包 一个具有较高级别的 HTTP 类的包    下一步该怎么做？  如果你还没有尝试过服务器端的 codelab，可以尝试编写一个服务器应用程序。 Servers with Dart 链接到编写独立 Dart 应用程序的资源，包括服务器。  ","permalink":"https://ohmyweekly.github.io/notes/write-http-clients/","tags":["flutter","client"],"title":"编写HTTP客户端和服务器"},{"categories":["programming"],"contents":"Dart 生态系统使用包来管理共享软件，如库和工具。要获得 Dart 包，你可以使用 pub 包管理器。你可以在 pub.dev 网站上找到公开的包，也可以从本地文件系统或其他地方加载包，比如 Git 仓库。无论你的包来自哪里，pub 都会管理版本依赖关系，帮助你获得相互之间以及与 SDK 版本兼容的包版本。\n大多数精通 Dart 的 IDE 都提供了对 pub 的支持，包括创建、下载、更新和发布包。或者你可以在命令行中使用 pub。\n至少，一个 Dart 包是一个包含 pubspec 文件的目录。pubspec 包含一些关于包的元数据。此外，一个包可以包含依赖关系(在 pubspec 中列出)，Dart 库，应用程序，资源，测试，图像和例子。\n要使用一个包，请执行以下操作:\n 创建一个 pubspec(一个名为 pubspec.yaml 的文件，它列出了软件包的依赖关系，并包含其他元数据，如版本号)。 使用 pub 来获取你的包的依赖关系。 如果你的 Dart 代码依赖于软件包中的一个库，则导入该库。  创建一个 pubspec pubspec 是一个名为 pubspec.yaml 的文件，它位于你的应用程序的顶级目录中。最简单的 pubspec 只列出了包名:\nname: my_app 下面是一个 pubspec 的例子，它声明了两个包(js 和 intl)的依赖关系，这两个包都托管在 pub.dev 站点上:\nname: my_app dependencies: js: ^0.6.0 intl: ^0.15.8 关于创建 pubspec 的详细信息，请参见 pubspec 文档和你要使用的包的文档。\n获取软件包 一旦你有了 pubspec，你就可以从你的应用程序的顶级目录中运行 pub get:\n$ cd \u0026lt;path-to-my_app\u0026gt; $ pub get 这个过程被称为获取依赖关系。\npub get 命令可以确定您的应用程序依赖于哪些软件包，并将它们放在中央系统缓存中。如果您的应用程序依赖于已发布的包，pub 会从 pub.dev 站点下载该包。对于 Git 依赖，pub 会克隆 Git 仓库。还包括了过渡性依赖。例如，如果 js 包依赖于 test 包，pub 会同时抓取 js 包和 test 包。\nPub 会创建一个 .packages 文件(在你的应用程序的顶层目录下)，将你的应用程序所依赖的每个包名映射到系统缓存中的对应包。\n从包中导入库 要导入在包中找到的库，使用 package: 前缀:\nimport \u0026#39;package:js/js.dart\u0026#39; as js; import \u0026#39;package:intl/intl.dart\u0026#39;; Dart 运行时在 package: 之后的所有内容都会在应用程序的 .package 文件中进行查找。\n你也可以使用这种风格从你自己的包中导入库。比方说，transmogrify 包的布局如下:\ntransmogrify/ lib/ transmogrify.dart parser.dart test/ parser/ parser_test.dart parser_test.dart 文件可以这样导入 parser.dart:\nimport \u0026#39;package:transmogrify/parser.dart\u0026#39;; 升级依赖关系 当你第一次为你的软件包获取一个新的依赖关系时，pub 会下载与你的其他依赖关系兼容的最新版本。然后，它通过创建一个 lockfile 锁文件来锁定您的软件包，使其始终使用该版本。这是一个名为 pubspec.lock 的文件，由 pub 创建并存储在 pubspec 的旁边。它列出了您的软件包所使用的每个依赖关系的特定版本 (即时的和过渡的)。\n如果你的包是一个应用程序包，你应该把这个文件检查到源代码控制中。这样，在你的应用程序上工作的每个人都会使用所有包的相同版本。在 lockfile 文件中检查也可以确保你部署的应用使用相同版本的代码。\n当你准备好将你的依赖项升级到最新版本时，使用 pub upgrade 命令:\n$ pub upgrade pub upgrade 命令告诉 pub 使用你的包的依赖关系的最新版本来重新生成 lockfile 文件。如果你只想升级一个依赖关系，你可以指定要升级的软件包:\n$ pub upgrade transmogrify 该命令将 transmogrify 升级到最新版本，但其他一切都保持不变。\n由于 pubspec 中有冲突的版本限制，pub upgrade 命令并不能总是将每个软件包升级到最新版本。要识别需要编辑 pubspec 的过期软件包，请使用 pub outdated。\n更多信息 下面的页面有更多关于软件包和 pub 包管理器的信息。\n如何使用\n 创建包 发布包  参考\n Pub 依赖 Pub 环境变量 Pub 词汇表 Pub 包布局约定 Pub 版本哲学 Pubspec 格式化  Pub 命令\npub 工具提供了以下命令:\n pub cache pub deps pub downgrade pub get pub global pub outdated pub publish pub run pub upgrade pub uploader  有关所有 pub 命令的概述，请参阅 pub 工具文档。\n疑难解答\n故障排除 pub 提供了使用 pub 时可能遇到的问题的解决方案。\n","permalink":"https://ohmyweekly.github.io/notes/how-to-use-packages/","tags":["flutter","packages"],"title":"如何使用包"},{"categories":["programming"],"contents":"本页列出了一些 Dart 开发者发布的最流行和最有用的包。要找到更多的软件包\u0026ndash;也可以搜索核心库\u0026ndash;请使用 pub.dev 网站。\n常用的软件包可分为三类:\n 通用包 扩展 Dart 核心库的包 特定的包  通用包 以下包对各种项目都很有用:\n   包 描述 常用的 API     archive 对各种档案和压缩格式进行编码和解码。 Archive, ArchiveFile, TarEncoder, TarDecoder, ZipEncoder, ZipDecoder   characters 对用户感知的字符进行字符串操作(Unicode 字符簇) String.characters, Characters, CharacterRange   http 一组高级的函数和类，使其易于消费 HTTP 资源 delete(), get(), post(), read()   intl 国际化和本地化设施，支持复数和性别、日期和数字格式化和解析以及双向文本 Bidi, DateFormat, MicroMoney, TextDirection   json_serializable 一个易于使用的代码生成包。更多信息，请参阅 JSON 支持 @JsonSerializable   logging 一个可配置的机制，为你的应用程序添加消息记录 LoggerHandler, Level, LogRecord   mockito 一个在测试中模拟对象的流行框架。如果你正在编写依赖注入的测试，特别有用。与 test 包一起使用 Answering, Expectation, Verification   path 操作不同类型路径的常用操作。更多信息，请参见拆包: path absolute(), basename(), extension(), join(), normalize(), relative(), split()   quiver 实用工具，使 Dart 核心库的使用更加方便。Quiver 提供额外支持的一些库包括 async、cache、collection、core、iterables、pattern 和 测试 CountdownTimer (quiver.async); MapCache (quiver.cache); MultiMap, TreeSet (quiver.collection); EnumerateIterable (quiver.iterables); center(), compareIgnoreCase(), isWhiteSpace() (quiver.strings)   shelf Dart 的 Web 服务器中间件。Shelf 使它能轻松地创建和组成 Web 服务器，以及 Web 服务器的一部分 Cascade, Pipeline, Request, Response, Server   stack_trace 用于解析、检查和处理由底层 Dart 实现产生的堆栈痕迹的方法。还提供了以比原生 StackTrace 实现更可读的格式生成堆栈跟踪的字符串表示的函数, 更多信息，请参见拆包: stack_trace Trace.current(), Trace.format(), Trace.from()   stagehand 一个 Dart 项目生成器。当你创建一个新的应用程序时，WebStorm 和 IntelliJ 使用 Stagehand 模板，但你也可以从命令行使用模板 一般通过 IDE 或 stagehand 命令来使用   test 在 Dart 中编写和运行测试的标准方法 expect(), group(), test()   yaml YAML 解析器 loadYaml(), loadYamlStream()    扩展 Dart 核心库的包 以下每个包都建立在一个核心库的基础上，增加了功能并填补了缺失的功能:\n   包 描述 常用的 API     async 在 dart:async 的基础上进行了扩展，增加了实用类来处理异步计算。更多信息，请参见拆包: async 第1部分、第2部分和第3部分 AsyncMemoizer, CancelableOperation, FutureGroup, LazyStream, Result, StreamCompleter, StreamGroup, StreamSplitter   collection 在 dart:collection 的基础上进行了扩展，增加了实用函数和类，使处理集合的工作变得更加容易。更多信息，请看拆包：collection Equality, CanonicalizedMap, MapKeySet, MapValueSet, PriorityQueue, QueueList   convert 在 dart:convert 的基础上，增加了编码器和解码器，用于在不同的数据表现形式之间进行转换。其中一种数据表示方式是百分比编码，也被称为 URL 编码 HexDecoder, PercentDecoder   io 包含两个库，ansi和io，以简化对文件、标准流和进程的处理。使用 ansi 库可以自定义终端输出。io 库有处理进程、stdin 和文件复制的 API copyPath(), isExecutable(), ExitCode, ProcessManager, sharedStdIn    专用包 下面是一些寻找比较专业的包的技巧，比如手机的包(Flutter)和网页开发的包。\nFlutter 包 请看 Flutter 网站上的使用包。或者使用 pub.dev 网站搜索 Flutter 包。\n网络包 参见网络库和包。或者使用 pub.dev 站点搜索 web 包。\n命令行和服务器软件包 参见命令行和服务器库和包。或者使用 pub.dev 站点搜索其他包。\n","permalink":"https://ohmyweekly.github.io/notes/commonly-used-packages/","tags":["flutter","packages"],"title":"常用的包"},{"categories":["programming"],"contents":"async-await\n这个 codelab 教你如何使用 futures、async 和 await 关键字编写异步代码。使用内嵌的 DartPad 编辑器，你可以通过运行示例代码和完成练习来测试你的知识。\n要想从这个 codelab 中获得最大的收获，你应该具备以下条件。\n 掌握基本的 Dart 语法 有用其他语言编写异步代码的经验。  这个 codelab 包括以下材料。\n 如何以及何时使用 async 和 await 关键字。 使用 async 和 await 如何影响执行顺序。 如何在 async 函数中使用 try-catch 表达式处理异步调用中的错误。  估计完成这个代码实验的时间。40-60分钟\n注意：本页面使用嵌入式 DartPads 来显示示例和练习。如果你看到的是空框而不是 DartPads，请转到 DartPad 故障排除页面。\n为什么异步代码很重要 异步操作让你的程序在等待另一个操作完成时完成工作。下面是一些常见的异步操作。\n 通过网络获取数据。 写入数据库。 从文件中读取数据。  要在 Dart 中执行异步操作，你可以使用 Future 类以及 async 和 await 关键字。\n例子: 错误地使用异步函数 下面的例子显示了使用异步函数(fetchUserOrder())的错误方法。稍后你将使用 async 和 await 来修复这个例子。在运行这个例子之前，试着发现这个问题-你认为输出会是什么？\n// This example shows how *not* to write asynchronous Dart code.  String createOrderMessage() { var order = fetchUserOrder(); return \u0026#39;Your order is: $order\u0026#39;; } Future\u0026lt;String\u0026gt; fetchUserOrder() =\u0026gt; // Imagine that this function is more complex and slow.  Future.delayed( Duration(seconds: 2), () =\u0026gt; \u0026#39;Large Latte\u0026#39;, ); void main() { print(createOrderMessage()); } 下面是这个例子为什么不能打印 fetchUserOrder() 最终产生的值。\n fetchUserOrder() 是一个异步函数，在延迟之后，提供一个描述用户订单的字符串：\u0026ldquo;Large Latte\u0026rdquo;。 为了得到用户的订单，createOrderMessage() 应该调用 fetchUserOrder()，并等待其完成。由于 createOrderMessage() 没有等待 fetchUserOrder() 完成，createOrderMessage() 无法获得 fetchUserOrder() 最终提供的字符串值。 取而代之的是，createOrderMessage() 得到的是待完成工作的表示：一个未完成的未来。您将在下一节了解更多关于未来的信息。 因为 createOrderMessage() 没有得到描述用户订单的值，所以这个例子没有打印 \u0026ldquo;Large Latte\u0026rdquo; 到控制台，而是打印 \u0026ldquo;Your order is: Instance of \u0026lsquo;_Future\u0026rsquo;\u0026quot;。  在接下来的章节中，你将学习关于 futures 和关于使用 futures 的工作（使用 async 和 await），这样你就能编写必要的代码，使 fetchUserOrder() 向控制台打印所需的值(\u0026ldquo;Large Latte\u0026rdquo;)。\n关键术语:\n 同步操作: 同步操作会阻止其他操作的执行，直到它完成。 同步函数：同步函数只执行同步操作。 异步操作：异步操作一旦启动，就允许其他操作在它完成之前执行。 异步函数：异步函数至少执行一个异步操作，也可以执行同步操作。  什么是未来？ future(小写 \u0026ldquo;f\u0026rdquo;)是 Future（大写 \u0026ldquo;F\u0026rdquo;）类的一个实例。一个 future 代表异步操作的结果，可以有两种状态：未完成或完成。\n注意：未完成是一个 Dart 术语，指的是一个未来的状态，在它产生一个值之前。\n未完成的 当你调用一个异步函数时，它会返回一个未完成的未来。这个未来正在等待函数的异步操作完成或抛出一个错误。\n已完成的 如果异步操作成功，未来就以一个值完成。否则它将以一个错误完成。\n用一个值来完成 类型为 Future\u0026lt;T\u0026gt; 的 future 用一个类型为 T 的值来完成。例如，一个类型为 Future\u0026lt;String\u0026gt; 的 future 会产生一个字符串值。如果一个 future 没有产生一个可用的值，那么 future 的类型是 Future\u0026lt;void\u0026gt;。\n用一个错误来完成 如果函数执行的异步操作因为任何原因而失败，future 就会以错误的方式完成。\n例子: 介绍 future 在下面的例子中，fetchUserOrder() 返回一个在打印到控制台后完成的 future。因为它没有返回一个可用的值，fetchUserOrder() 的类型是 Future\u0026lt;void\u0026gt;。在运行这个例子之前，试着预测一下哪个会先打印：\u0026ldquo;Large Latte\u0026rdquo; 或 \u0026ldquo;Fetching user order\u0026hellip;\u0026quot;。\nFuture\u0026lt;void\u0026gt; fetchUserOrder() { // Imagine that this function is fetching user info from another service or database.  return Future.delayed(Duration(seconds: 2), () =\u0026gt; print(\u0026#39;Large Latte\u0026#39;)); } void main() { fetchUserOrder(); print(\u0026#39;Fetching user order...\u0026#39;); } 在前面的例子中，尽管 fetchUserOrder() 在第8行的 print() 调用之前执行，控制台还是在 fetchUserOrder() 的输出 (\u0026ldquo;Large Latte\u0026rdquo;) 之前显示了第8行的输出 (\u0026ldquo;Fetching user order\u0026hellip;\u0026quot;)。这是因为 fetchUserOrder() 在打印 \u0026ldquo;Large Latte\u0026rdquo; 之前会有延迟。\n例子: 完成时出现错误 运行下面的例子，看看未来如何完成一个错误。稍后你将学习如何处理错误。\nFuture\u0026lt;void\u0026gt; fetchUserOrder() { // Imagine that this function is fetching user info but encounters a bug  return Future.delayed(Duration(seconds: 2), () =\u0026gt; throw Exception(\u0026#39;Logout failed: user ID is invalid\u0026#39;)); } void main() { fetchUserOrder(); print(\u0026#39;Fetching user order...\u0026#39;); } 在这个例子中，fetchUserOrder() 完成时出现错误，表明用户ID无效。\n你已经学习了 future 和它们如何完成，但你如何使用异步函数的结果呢？在下一节中，你将学习如何使用 async 和 await 关键字来获取结果。\n快速回顾:\n 一个 Future\u0026lt;T\u0026gt; 实例会产生一个 T 类型的值。 如果一个 future 没有产生一个可用的值，那么 future 的类型是 Future\u0026lt;void\u0026gt;。 一个 future 可以处于两种状态之一：未完成或完成。 当你调用一个返回 future 的函数时，函数会把要做的工作排队，并返回一个未完成的 future。 当一个 future 的操作完成时，future 以一个值或以一个错误完成。  关键术语:\n Future: Dart Future 类。 future：Dart Future 类的一个实例。  使用 future：async 和 await async 和 await 关键字提供了一种声明式的方式来定义异步函数并使用它们的结果。在使用 async 和 await 时，请记住以下两个基本准则。\n 要定义一个异步函数，请在函数主体前添加 async。 await 关键字只能在 async 函数中使用。  下面是一个将 main() 从同步函数转换为异步函数的例子。\n首先，在函数体前添加 async 关键字:\nvoid main() async { ··· } 如果函数有声明的返回类型，那么更新类型为 Future\u0026lt;T\u0026gt;，其中 T 是函数返回的值的类型。如果函数没有明确返回值，那么返回类型为 Future\u0026lt;void\u0026gt;。\nFuture\u0026lt;void\u0026gt; main() async { ··· } 现在你已经有了一个 async 函数，你可以使用 await 关键字来等待一个 future 的完成:\nprint(await createOrderMessage()); 正如下面两个例子所显示的，async 和a wait 关键字导致异步代码看起来很像同步代码。唯一的区别在异步示例中突出显示，如果你的窗口足够宽，它就在同步示例的右边。\n示例：同步函数\nString createOrderMessage() { var order = fetchUserOrder(); return \u0026#39;Your order is: $order\u0026#39;; } Future\u0026lt;String\u0026gt; fetchUserOrder() =\u0026gt; // Imagine that this function is  // more complex and slow.  Future.delayed( Duration(seconds: 2), () =\u0026gt; \u0026#39;Large Latte\u0026#39;, ); void main() { print(\u0026#39;Fetching user order...\u0026#39;); print(createOrderMessage()); } Fetching user order... Your order is: Instance of _Future\u0026lt;String\u0026gt; 例子：异步函数\nFuture\u0026lt;String\u0026gt; createOrderMessage() async { var order = await fetchUserOrder(); return \u0026#39;Your order is: $order\u0026#39;; } Future\u0026lt;String\u0026gt; fetchUserOrder() =\u0026gt; // Imagine that this function is  // more complex and slow.  Future.delayed( Duration(seconds: 2), () =\u0026gt; \u0026#39;Large Latte\u0026#39;, ); Future\u0026lt;void\u0026gt; main() async { print(\u0026#39;Fetching user order...\u0026#39;); print(await createOrderMessage()); } Fetching user order... Your order is: Large Latte 异步示例在三个方面有所不同。\n createOrderMessage() 的返回类型从 String 变为 Future\u0026lt;String\u0026gt;。 async 关键字出现在 createOrderMessage() 和 main() 的函数体之前。 await 关键字出现在调用异步函数 fetchUserOrder() 和 createOrderMessage() 之前。  关键术语:\n async: 你可以在一个函数的主体前使用 async 关键字来标记它为异步函数。 async 函数: async 函数是一个标有 async 关键字的函数。 await：可以使用 await 关键字来获取异步表达式的完成结果。await 关键字只在 async 函数中起作用。  使用 async 和 await 的执行流程 一个异步函数在第一个 await 关键字之前是同步运行的。这意味着在一个 async 函数体中，第一个 await 关键字之前的所有同步代码都会立即执行。\n版本说明：在 Dart 2.0 之前，一个异步函数立即返回，而不会在异步函数体中执行任何代码。\n例子：在异步函数内执行。在异步函数中执行 运行下面的例子，看看如何在异步函数体中执行。你认为输出会是什么？\nFuture\u0026lt;void\u0026gt; printOrderMessage() async { print(\u0026#39;Awaiting user order...\u0026#39;); var order = await fetchUserOrder(); print(\u0026#39;Your order is: $order\u0026#39;); } Future\u0026lt;String\u0026gt; fetchUserOrder() { // Imagine that this function is more complex and slow.  return Future.delayed(Duration(seconds: 4), () =\u0026gt; \u0026#39;Large Latte\u0026#39;); } Future\u0026lt;void\u0026gt; main() async { countSeconds(4); await printOrderMessage(); } // You can ignore this function - it\u0026#39;s here to visualize delay time in this example. void countSeconds(int s) { for (var i = 1; i \u0026lt;= s; i++) { Future.delayed(Duration(seconds: i), () =\u0026gt; print(i)); } } 运行上例中的代码后，尝试将第2行和第3行反过来。\nvar order = await fetchUserOrder(); print(\u0026#39;Awaiting user order...\u0026#39;); 注意到输出的时间发生了变化，现在 print('Awaiting user order') 出现在 printOrderMessage() 中第一个 await 关键字之后。\n练习: 练习使用 async 和 await 下面的练习是一个失败的单元测试，其中包含部分完成的代码片段。你的任务是通过编写代码使测试通过来完成练习。你不需要实现 main()。\n为了模拟异步操作，调用以下函数，这些函数是为你提供的。\n   函数 类型签名 描述     fetchRole() FuturefetchRole() 获取用户角色的简短描述。   fetchLoginAmount() FuturefetchLoginAmount() 获取用户的登录次数。    第1部分：reportUserRole()\n为 reportUserRole() 函数添加代码，使其执行以下操作。\n 返回一个以下列字符串完成的 future： \u0026quot;User role: \u0026lt;user role\u0026gt;\u0026quot;。  注意：你必须使用 fetchRole() 返回的实际值；复制和粘贴示例返回值不会使测试通过。 示例返回值: \u0026ldquo;User role: tester\u0026rdquo;   通过调用提供的函数 fetchRole() 获取用户角色。  第二部分：reportLogins()\n实现一个异步函数 reportLogins()，使其执行以下操作。\n 返回字符串 \u0026ldquo;Total number of logins: \u0026lt;# of logins\u0026gt;\u0026quot;。  注意：你必须使用 fetchLoginAmount() 返回的实际值；复制和粘贴示例返回值不会使测试通过。 reportLogins() 的返回值示例: \u0026quot;Total number of logins: 57\u0026quot;。   通过调用提供的函数 fetchLoginAmount() 来获取登录次数。  Future\u0026lt;String\u0026gt; reportUserRole() async { var username = await fetchRole(); return \u0026#39;User role: $username\u0026#39;; } Future\u0026lt;String\u0026gt; reportLogins() async { var logins = await fetchLoginAmount(); return \u0026#39;Total number of logins: $logins\u0026#39;; } 注意：如果你的代码通过了测试，你可以忽略信息级的消息。\n处理错误 要处理 async 函数中的错误，使用 try-catch:\ntry { var order = await fetchUserOrder(); print(\u0026#39;Awaiting user order...\u0026#39;); } catch (err) { print(\u0026#39;Caught error: $err\u0026#39;); } 在一个 async 函数中，你可以像在同步代码中一样编写 try-catch 子句。\n例子：async 和 await 的 try-catch 子句 运行下面的例子，看看如何处理一个异步函数的错误。你认为输出会是什么？\nFuture\u0026lt;void\u0026gt; printOrderMessage() async { try { var order = await fetchUserOrder(); print(\u0026#39;Awaiting user order...\u0026#39;); print(order); } catch (err) { print(\u0026#39;Caught error: $err\u0026#39;); } } Future\u0026lt;String\u0026gt; fetchUserOrder() { // Imagine that this function is more complex.  var str = Future.delayed( Duration(seconds: 4), () =\u0026gt; throw \u0026#39;Cannot locate user order\u0026#39;); return str; } Future\u0026lt;void\u0026gt; main() async { await printOrderMessage(); } 练习: 练习处理错误 下面的练习提供了使用异步代码处理错误的练习，使用上一节中描述的方法。为了模拟异步操作，你的代码将调用以下函数，该函数为你提供。\n| 函数 | 类型签名 | 描述 | | fetchNewUsername() | FuturefetchNewUsername() |\t返回你可以用来替换旧用户名的新用户名。|\n使用 async 和 await 来实现一个异步的 changeUsername() 函数，该函数执行以下操作。\n 调用提供的异步函数 fetchNewUsername() 并返回其结果。  changeUsername() 的返回值示例: \u0026ldquo;jane_smith_92\u0026rdquo;   捕获任何发生的错误并返回错误的字符串值。  你可以使用 toString() 方法对 Exceptions 和Errors 进行字符串化。    Future\u0026lt;String\u0026gt; changeUsername () async { try { return await fetchNewUsername(); } catch (err) { return err.toString(); } } 练习: 把所有的东西放在一起 现在是时候在最后一个练习中练习所学的知识了。为了模拟异步操作，本练习提供了异步函数 fetchUsername() 和 logoutUser():\n| 函数 | 类型签名 | 描述 | | fetchUsername() |\tFuturefetchUsername() | 返回与当前用户相关联的名称。 | | logoutUser()\t| FuturelogoutUser()\t| 执行当前用户的注销，并返回被注销的用户名。 |\n编写以下内容。\n第一部分：addHello()\n 编写一个函数 addHello()，它接受一个单一的 String 参数。 addHello() 返回它的 String 参数，前面加 \u0026lsquo;Hello\u0026rsquo;。 例如：addHello('Jon') 返回 \u0026lsquo;Hello Jon\u0026rsquo;。  第二部分：greetUser()\n 编写一个不接受参数的函数 greetUser()。 为了得到用户名，greetUser() 调用提供的异步函数 fetchUsername()。 greetUser() 通过调用 addHello() 为用户创建一个问候语，传递用户名，并返回结果。 例子: 如果 fetchUsername() 返回 \u0026lsquo;Jenny\u0026rsquo;, 那么 greetUser() 返回 \u0026lsquo;Hello Jenny\u0026rsquo;.  第三部分：sayGoodbye()\n  编写一个函数 sayGoodbye()，它的功能如下。\n 不接受任何参数 捕获任何错误。 调用所提供的异步函数 logoutUser().    如果 logoutUser() 失败，sayGoodbye() 返回任何你喜欢的字符串。\n  如果 logoutUser() 成功，sayGoodbye() 返回字符串 '\u0026lt;result\u0026gt; Thanks, see you next time'，其中 \u0026lt;result\u0026gt; 是调用 logoutUser() 返回的字符串值。\n  String addHello(user) =\u0026gt; \u0026#39;Hello $user\u0026#39;; Future\u0026lt;String\u0026gt; greetUser() async { var username = await fetchUsername(); return addHello(username); } Future\u0026lt;String\u0026gt; sayGoodbye() async { try { var result = await logoutUser(); return \u0026#39;$resultThanks, see you next time\u0026#39;; } catch (e) { return \u0026#39;Failed to logout user: $e\u0026#39;; } } 下一步是什么？ 恭喜你，你已经完成了 codelab 的学习！如果你还想了解更多，这里有一些下一步的建议。\n 玩玩 DartPad。 尝试另一个 codelab。 学习更多关于 futures 和异步的知识。  Streams tutorial: 学习如何使用异步事件的序列。 来自 Google 的 Dart视频: 观看一个或多个关于异步编码的视频。或者，如果你喜欢，阅读基于这些视频的文章。(从隔离和事件循环开始。)   获取 Dart SDK。  如果你对使用嵌入式 DartPads 感兴趣，就像这个 codelab 一样，请看教程中使用 DartPad 的最佳实践。\n","permalink":"https://ohmyweekly.github.io/notes/futures-async-await/","tags":["async","futures","await","dart"],"title":"异步编程：futures、async、await。"},{"categories":["programming"],"contents":"介绍 Flutter 是 Google 的 UI 工具包，用于从单一代码库中为手机、网页和桌面构建漂亮的、原生编译的应用程序。Flutter 可以与现有的代码一起工作，被世界各地的开发者和组织使用，并且是免费和开源的。\n在这个代码实验室中，你将创建一个简单的手机 Flutter 应用。如果你熟悉面向对象的代码和基本的编程概念-如变量、循环和条件, 那么你就可以完成这个 codelab。你不需要以前有 Dart、手机或 Web 编程的经验。\n你将在第1部分学到什么  如何编写一款在 iOS、Android 和 Web 上看起来很自然的 Flutter 应用？ Flutter 应用程序的基本结构。 寻找和使用包来扩展功能。 使用热重装来加快开发周期。 如何实现一个有状态的小组件。 如何创建一个无限的、懒加载的列表。  在这个 codelab 的第2部分中，你将添加交互性，修改应用程序的主题，并添加导航到新页面的能力(在 Flutter 中称为路由)。\n你将在第1部分中构建什么 你将实现一个移动应用，为一家创业公司生成建议的名字。用户可以选择和取消选择名字，保存最好的名字。代码一次懒惰地生成10个名字。随着用户的滚动，会生成更多的名字。用户可以滚动的范围没有限制。\n下面的 GIF 动画显示了应用程序在完成部分时的工作情况。\n设置你的 Flutter 环境 你需要两个软件来完成这个实验室-Flutter SDK和一个编辑器。(codelab 假设你使用 Android Studio，但你可以使用你的首选编辑器。)\n你可以通过使用以下任何设备来运行 codelab。\n 一个物理的 Android 或 iOS 设备连接到你的计算机并设置为开发者模式。 iOS 模拟器(需要安装 Xcode 工具) 安卓模拟器(需要在 Android Studio 中进行设置) 浏览器(调试时需要使用 Chrome 浏览器)  如果你想编译你的应用程序以在 web 上运行，你必须启用此功能（目前处于测试阶段）。要启用 web 支持，请使用以下说明。\nflutter channel beta flutter upgrade flutter config --enable-web 你只需要运行一次 config 命令。启用 Web 支持后，你创建的每个 Flutter 应用也会为 Web 编译。在你的 IDE 的设备下拉菜单下面，或者在命令行使用 flutter devices，你现在应该看到 Chrome 和 Web 服务器被列出。Chrome 设备会自动启动 Chrome。Web 服务器会启动一个托管应用程序的服务器，这样你就可以从任何浏览器加载它。在开发过程中使用 Chrome 设备，以便你可以使用 DevTools，而当你要在其他浏览器上进行测试时使用 Web 服务器。有关更多信息，请参阅使用 Flutter 构建 Web 应用程序和在 Web 上编写你的第一个 Flutter 应用程序。\n创建 Flutter 应用程序的启动器 通过使用创建应用程序中的说明来创建一个简单的、模板化的 Flutter 应用程序。输入 startup_namer(而不是 flutter_app)作为项目名称。您将修改启动器应用程序来创建完成的应用程序。\n提示：如果你在 IDE 中没有看到能够启动一个新的 Flutter 项目作为一个选项，那么请确保你已经安装了 Flutter 和 Dart 的插件。\n你将主要编辑 lib/main.dart，Dart 的代码就在这里。\n替换 lib/main.dart 的内容。 删除 lib/main.dart 中的所有代码，并用下面的代码替换，在屏幕中央显示 \u0026ldquo;Hello World\u0026rdquo;。\nimport \u0026#39;package:flutter/material.dart\u0026#39;; void main() =\u0026gt; runApp(MyApp()); class MyApp extends StatelessWidget { @override Widget build(BuildContext context) { return MaterialApp( title: \u0026#39;Welcome to Flutter\u0026#39;, home: Scaffold( appBar: AppBar( title: const Text(\u0026#39;Welcome to Flutter\u0026#39;), ), body: const Center( child: const Text(\u0026#39;Hello World\u0026#39;), ), ), ); } } 提示：当把代码粘贴到你的应用程序中时，缩进会变得歪斜。你可以用以下 Flutter 工具来解决。\n Android Studio/IntelliJ IDEA: 右键点击 Dart 代码，选择用 dartfmt 重格式代码。 VS code: 右键点击并选择格式化文档。 终端: 运行 flutter format \u0026lt;文件名\u0026gt;。  运行应用程序。您应该看到 Android，iOS 或 Web 输出，取决于您的设备。\n安卓系统:\niOS:\n小贴士：第一次在物理设备上运行时，可能需要一段时间来加载。之后，你可以使用热重载来快速更新。在支持的 IDE 中，如果应用正在运行，Save 也会执行热重载。当使用 flutter run 直接从控制台运行应用程序时，输入 r 来执行热重载。\n观察:\n 这个例子创建了一个 Material 应用。Material 是一种视觉设计语言，是移动和 Web 的标准。Flutter 提供了一套丰富的 Material 部件。 main 方法使用箭头(=\u0026gt;)符号。对单行函数或方法使用箭头符号。 应用程序扩展了 StatelessWidget，这使得应用程序本身成为一个组件。在 Flutter 中，几乎所有的东西都是组件，包括对齐、填充和布局。 Scaffold 组件来自 Material 库，它提供了一个默认的应用栏、一个标题和一个 body 属性，其中存放着主屏幕的组件树。组件子树可以相当复杂。 组件的主要工作是提供一个 build 方法，描述如何用其他低级组件来显示该组件。 本例的主体由包含 Text 子部件的 Center 部件组成。Center 组件将其组件子树对齐到屏幕的中心。  使用外部软件包 在这一步中，您将开始使用一个名为 english_words 的开源包，它包含了几千个最常用的英语单词，还有一些实用函数。\n你可以在 pub.dev 找到 english_words 包，以及许多其他开源包。\npubspec 文件管理着 Flutter 应用的资产。在 pubspec.yaml 中，附加 english_words: ^3.1.5(english_words 3.1.5 或更高)到依赖列表中。\ndependencies:flutter:sdk:fluttercupertino_icons:^0.1.2english_words:^3.1.5 # add this line在 Android Studio 的编辑器视图中查看 pubspec 时，点击 Packages get。这将把包拉到你的项目中。你应该在控制台中看到以下内容。\nflutter packages get Running \u0026#34;flutter packages get\u0026#34; in startup_namer... Process finished with exit code 0 执行 Pub get 也会自动生成 \u0026ldquo;pubspec.lock\u0026rdquo; 文件，其中包含所有拉入项目的包的列表和它们的版本号。\n在 lib/main.dart 中，导入新包:\nimport \u0026#39;package:flutter/material.dart\u0026#39;; import \u0026#39;package:english_words/english_words.dart\u0026#39;; // Add this line. 当你输入时，Android Studio 会给你建议导入的库。然后，它将导入的字符串渲染成灰色，让你知道导入的库是未使用的（到目前为止）。\n接下来，你将使用 english_words 包来生成文本，而不是使用 \u0026ldquo;Hello World\u0026rdquo;。\n做以下修改。\nimport \u0026#39;package:flutter/material.dart\u0026#39;; import \u0026#39;package:english_words/english_words.dart\u0026#39;; void main() =\u0026gt; runApp(MyApp()); class MyApp extends StatelessWidget { @override Widget build(BuildContext context) { final wordPair = WordPair.random(); // Add this line.  return MaterialApp( title: \u0026#39;Welcome to Flutter\u0026#39;, home: Scaffold( appBar: AppBar( title: Text(\u0026#39;Welcome to Flutter\u0026#39;), ), body: Center( //child: Text(\u0026#39;Hello World\u0026#39;), // Replace this text...  child: Text(wordPair.asPascalCase), // With this text.  ), ), ); } } 提示: Pascal 大小写(也称为上驼形大小写)意味着字符串中的每个单词，包括第一个单词，都以大写字母开头。所以，uppercamelcase 就变成了 UpperCamelCase。\n如果应用程序正在运行，热重载来更新正在运行的应用程序。(在命令行中，你可以输入 r 来热重载。)每次点击热重载或保存项目时，你应该会在运行中的应用程序中看到一个不同的单词对，随机选择。这是因为单词对是在 build 方法里面生成的，每次 MaterialApp 需要渲染时，或者在 Flutter Inspector 中切换 Platform 时，都会运行该方法。\nAndroid:\niOS:\n有问题？ 如果您的应用程序没有正确运行，请查找错别字。如果需要，请使用以下链接中的代码来恢复正常。\n pubspec.yaml lib/main.dart  添加一个有状态的组件 无状态组件是不可改变的，这意味着它们的属性不能改变-所有值都是最终值。\n有状态组件维护的状态可能在组件的生命周期内发生变化。实现一个有状态的组件至少需要两个类。1) 一个 StatefulWidget，它可以创建一个 State 类的实例。StatefulWidget 对象本身是不可变的，可以被丢弃和再生，但 State 对象会在 widget 的生命周期内持久存在。\n在这一步骤中，您将添加一个有状态的组件 RandomWords，并创建其 State 类 _RandomWordsState。然后，您将在现有的 MyApp 无状态组件中使用 RandomWords 作为子类。\n为有状态组件创建模板代码。\n它可以放在 MyApp 以外的文件中的任何位置，但解决方案将其放在文件的底部。在 lib/main.dart 中，将光标定位在所有代码之后，输入回车键几次，重新开始一行。在你的 IDE 中，开始输入 stful。编辑器会询问你是否要创建一个 Stateful 的组件。按回车键接受。两个类的模板代码出现了，光标定位让你输入无状态组件的名称。\n输入 RandomWords 作为您的小组件的名称。\n正如您在下面的代码中所看到的，RandomWords 组件除了创建它的 State 类之外，几乎没有其他的功能。\n一旦您输入 RandomWords 作为有状态组件的名称，IDE 会自动更新相应的 State 类，将其命名为 _RandomWordState。默认情况下，State 类的名称是以下划线为前缀的。在标识符前加上下划线可以加强 Dart 语言的隐私性，也是 State 对象的最佳实践。\nIDE 也会自动更新 State 类以扩展 State\u0026lt;RandomWords\u0026gt;，表明你正在使用一个专门用于 RandomWords 的通用 State 类。应用程序的大部分逻辑都在这里-它为 RandomWords 组件维护状态。这个类保存了生成的词对列表，随着用户的滚动而无限增长，在本实验室的第二部分中，当用户通过切换心形图标从列表中添加或删除这些词对时，该类会对其进行收藏。\n现在两个类的外观如下:\nclass RandomWords extends StatefulWidget { @override _RandomWordsState createState() =\u0026gt; _RandomWordsState(); } class _RandomWordsState extends State\u0026lt;RandomWords\u0026gt; { @override Widget build(BuildContext context) { return Container(); } } 更新 _RandomWordsState 中的 build() 方法。\n用以下两行替换 return Container();:\nclass _RandomWordsState extends State\u0026lt;RandomWords\u0026gt; { @override Widget build(BuildContext context) { final wordPair = WordPair.random(); // NEW  return Text(wordPair.asPascalCase); // NEW  } } 通过以下修改，删除 MyApp 中的文字生成代码:\nclass MyApp extends StatelessWidget { @override Widget build(BuildContext context) { final wordPair = WordPair.random(); // DELETE  return MaterialApp( title: \u0026#39;Welcome to Flutter\u0026#39;, home: Scaffold( appBar: AppBar( title: Text(\u0026#39;Welcome to Flutter\u0026#39;), ), body: Center( //child: Text(wordPair.asPascalCase), // REPLACE with...  child: RandomWords(), // ...this line  ), ), ); } } 热重载应用程序。应用程序应该像以前一样，每次热重载或保存应用程序时都会显示一个单词配对。\n提示：如果您在热重载时看到警告，表明您可能需要重新启动应用程序，您应该考虑重新启动应用程序。这可能是一个假阳性，但重启可以确保您的更改反映在应用程序的 UI 中。\n遇到问题了？ 如果您的应用程序没有正确运行，您可以使用以下链接中的代码来恢复正常。\n lib/main.dart  创建一个无限滚动的 ListView 在这一步中，您将展开 _RandomWordsState 来生成并显示单词配对列表。随着用户的滚动，列表（显示在 ListView 小组件中）会无限增长。ListView 中的构建器工厂构造函数允许你按需懒惰地构建一个列表视图。\n在 _RandomWordState 类中添加一些状态变量。\n增加一个 _suggestions 列表，用于保存建议的单词配对。另外，添加一个 _biggerFont 变量，用于使字体大小变大。\nclass _RandomWordsState extends State\u0026lt;RandomWords\u0026gt; { final List\u0026lt;WordPair\u0026gt; _suggestions = \u0026lt;WordPair\u0026gt;[]; // NEW  final TextStyle _biggerFont = const TextStyle(fontSize: 18); // NEW  ... } 接下来，你将在 _RandomWordsState 类中添加一个 _buildSuggestions() 函数。这个方法可以构建显示建议词对的 ListView。\nListView 类提供了一个构建器属性 itemBuilder，它是一个工厂构建器和回调函数，指定为一个匿名函数。两个参数被传递给函数\u0026ndash;BuildContext 和行迭代器 i。迭代器从0开始，每次调用函数时递增，每一个建议的单词配对都会递增一次。这个模型允许建议列表在用户滚动时继续增长。\n添加整个 _buildSuggestions 函数。\n在 _RandomWordsState 类中，添加以下函数，如果你喜欢，请删除注释:\nWidget _buildSuggestions() { return ListView.builder( padding: const EdgeInsets.all(16), // The itemBuilder callback is called once per suggested  // word pairing, and places each suggestion into a ListTile  // row. For even rows, the function adds a ListTile row for  // the word pairing. For odd rows, the function adds a  // Divider widget to visually separate the entries. Note that  // the divider may be difficult to see on smaller devices.  itemBuilder: (BuildContext _context, int i) { // Add a one-pixel-high divider widget before each row  // in the ListView.  if (i.isOdd) { return Divider(); } // The syntax \u0026#34;i ~/ 2\u0026#34; divides i by 2 and returns an  // integer result.  // For example: 1, 2, 3, 4, 5 becomes 0, 1, 1, 2, 2.  // This calculates the actual number of word pairings  // in the ListView,minus the divider widgets.  final int index = i ~/ 2; // If you\u0026#39;ve reached the end of the available word  // pairings...  if (index \u0026gt;= _suggestions.length) { // ...then generate 10 more and add them to the  // suggestions list.  _suggestions.addAll(generateWordPairs().take(10)); } return _buildRow(_suggestions[index]); } ); } _buildSuggestions 函数对每个词对调用一次 _buildRow。该函数在 ListTile 中显示每一个新的词对，这使得你可以在第2部分中使行更有吸引力。\n在 _RandomWordsState 中添加一个 _buildRow 函数。\nWidget _buildRow(WordPair pair) { return ListTile( title: Text( pair.asPascalCase, style: _biggerFont, ), ); } 更新 _RandomWordsState 的构建方法。\n将其改为使用 _buildSuggestions()，而不是直接调用单词生成库。(Scaffold 实现了基本的 Material Design 视觉布局。)\n@override Widget build(BuildContext context) { //final wordPair = WordPair.random(); // Delete these...  //return Text(wordPair.asPascalCase); // ... two lines.  return Scaffold ( // Add from here...  appBar: AppBar( title: Text(\u0026#39;Startup Name Generator\u0026#39;), ), body: _buildSuggestions(), ); // ... to here.  } 更新 MyApp 的构建方法，更改标题，删除 AppBar，并将 home 属性改为 RandomWords 部件。\n@override Widget build(BuildContext context) { return MaterialApp( title: \u0026#39;Startup Name Generator\u0026#39;, home: RandomWords(), ); } 重新启动应用程序。无论你滚动多远，你都应该看到一个单词配对的列表。\nAndroid:\niOS:\n遇到问题了？ 如果你的应用程序不能正常运行，你可以使用下面链接中的代码来回到正轨。\n lib/main.dart  今后的步骤 恭喜你！\n你已经完成了这个代码实验室的第一部分! 如果你想扩展这款应用，请进入第二部分，你将对应用进行如下修改。\n 增加互动性 增加导航到新路由的功能。 修改主题颜色。  当第2部分完成后，应用程序将是这样的：\n其他后续步骤 通过以下资源了解更多关于 Flutter SDK 的信息。\n Flutter 中的布局 增加互动性教程 组件介绍 为 Android 开发者提供的 Flutter 针对 React Native 开发者的 Flutter Web 开发人员的 Flutter Flutter YouTube 频道  其他资源包括以下几点:\n 用 Flutter 构建本地移动应用 从 Java 到 Dart codelab Flutter cookbook 融入 Dart 的 Bootstrap：了解更多关于这门语言的信息  同时，与 Flutter 社区联系起来!\n","permalink":"https://ohmyweekly.github.io/notes/write-your-first-flutter-app/","tags":["flutter","app"],"title":"编写你的第一个 Flutter 应用，第一部分"},{"categories":["programming"],"contents":"介绍 Flutter 是 Google 的 UI 工具包，用于从单一代码库中为移动、Web 和桌面构建漂亮的、原生编译的应用程序。Flutter 可以与现有的代码一起工作，被世界各地的开发者和组织使用，并且是免费和开源的。\n在这个代码实验室中，您将扩展一个基本的、移动的 Flutter 应用程序，以包含交互性。您还将创建一个用户可以导航到的第二个页面（称为路由）。最后，您将修改应用程序的主题（颜色）。这个代码实验室扩展了第1部分，在这部分中，你将创建一个无限的懒惰加载的列表，但如果你想从第2部分开始，我们将提供起始代码。\n你将在第二部分学到什么  如何编写一款在 iOS、Android 和 Web 上看起来很自然的 Flutter 应用？ 如何使用热重装，加快开发周期？ 如何为有状态的 widget 添加交互性？ 如何创建并导航到第二个屏幕？ 如何使用主题来改变应用程序的外观？  你将在第二部分建立什么 您将从一个简单的移动应用程序开始，为创业公司生成一个无尽的建议名称列表。在代码实验室结束时，您的最终用户可以选择和取消选择名称，保存最好的名称。点击应用栏右上角的列表图标可以导航到一个新的页面（称为路由），该页面只列出了最喜欢的名字。\n下面的 GIF 动画显示了完成的应用程序将如何工作。\n设置您的 Flutter 环境 如果你还没有完成第1部分，请看设置你的 Flutter 环境，在编写你的第一个Flutter应用，第1部分，设置你的 Flutter 开发环境。\n获取启动应用程序 如果你已经完成了这个 codelab 的第一部分，你已经有了启动应用程序，startup_namer。你可以进行下一步。\n如果你没有 startup_namer，不要害怕，你可以使用下面的说明得到它。\n使用创建应用程序中的说明创建一个简单的模板化 Flutter 应用程序。将项目命名为 startup_namer（而不是 flutter_app）。\n删除 lib/main.dart 中的所有代码。用这个文件中的代码替换，它显示了一个无限的，懒惰加载的建议启动名称列表。\n更新 pubspec.yaml，加入英文单词包。\ndependencies:flutter:sdk:fluttercupertino_icons:^0.1.2english_words:^3.1.5 // NEW英文单词包会生成一对随机的单词，作为潜在的启动名称。\n在 Android Studio 的编辑器视图中查看 pubspec 时，点击右上角的 Pub get，这将包拉到你的项目中。你应该在控制台中看到以下内容:\nflutter pub get Running \u0026#34;flutter pub get\u0026#34; in startup_namer... Process finished with exit code 0 运行该应用。\n随意滚动，查看持续供应的拟创业公司名称。\n将图标添加到列表中 在这一步中，你将为每一行添加心形图标。在下一步中，您将使它们可点击并保存收藏夹。\n在 _RandomWordsState 中添加一个 _saved Set。这个 Set 存储了用户收藏的单词配对。Set 比 List 更受欢迎，因为一个正确实现的 Set 不允许重复的条目。\nclass _RandomWordsState extends State\u0026lt;RandomWords\u0026gt; { final _suggestions = \u0026lt;WordPair\u0026gt;[]; final _saved = Set\u0026lt;WordPair\u0026gt;(); // NEW  final _biggerFont = TextStyle(fontSize: 18.0); ... } 在 _buildRow 函数中，添加一个 alreadySaved 检查，以确保一个单词配对还没有被添加到收藏夹中。\nWidget _buildRow(WordPair pair) { final alreadySaved = _saved.contains(pair); // NEW  ... } 在 _buildRow() 中，你还将为 ListTile 对象添加心形图标以实现收藏夹。在下一步中，你将添加与心形图标交互的功能。\n在文本之后添加图标，如下图所示。\nWidget _buildRow(WordPair pair) { final alreadySaved = _saved.contains(pair); return ListTile( title: Text( pair.asPascalCase, style: _biggerFont, ), trailing: Icon( // NEW from here...  alreadySaved ? Icons.favorite : Icons.favorite_border, color: alreadySaved ? Colors.red : null, ), // ... to here.  ); } 热重新加载应用程序。\n你现在应该看到每一行都有空心，但它们还没有互动。\nAndroid\niOS\n遇到问题了？ 如果你的应用程序不能正常运行，你可以使用下面链接中的代码来回到正轨。\n lib/main.dart  增加互动性 在这一步中，你将使心形图标可以点击。当用户点击列表中的一个条目，切换其收藏状态时，该词对就会从一组保存的收藏夹中添加或删除。\n要做到这一点，你将修改 _buildRow 函数。如果一个词条已经被添加到收藏夹中，再次点击它就会将其从收藏夹中删除。当一个磁贴被点击后，函数会调用 setState() 来通知框架状态已经改变。\n在 _buildRow 方法中加入 onTap，如下图所示:\nWidget _buildRow(WordPair pair) { final alreadySaved = _saved.contains(pair); return ListTile( title: Text( pair.asPascalCase, style: _biggerFont, ), trailing: Icon( alreadySaved ? Icons.favorite : Icons.favorite_border, color: alreadySaved ? Colors.red : null, ), onTap: () { // NEW lines from here...  setState(() { if (alreadySaved) { _saved.remove(pair); } else { _saved.add(pair); } }); }, // ... to here.  ); } 提示：在 Flutter 的反应式框架中，调用 setState() 会触发对 State 对象的 build() 方法的调用，导致 UI 的更新。\n热重载应用。\n你应该能够点击任何磁贴来收藏或不收藏该条目。点击瓷砖会产生一个隐含的从点击点发出的泼墨动画。\nAndroid\niOS\n遇到问题了？ 如果你的应用程序不能正常运行，你可以使用下面链接中的代码来回到正轨。\n lib/main.dart  导航到一个新的屏幕 在这一步中，您将添加一个新的页面（在 Flutter 中称为路由），显示收藏夹。您将学习如何在主页路线和新路由之间进行导航。\n在 Flutter 中，Navigator 管理着一个包含应用程序路由的堆栈。将一个路由推到 Navigator 的堆栈上，会将显示更新到该路由。从 Navigator 的堆栈中弹出一条路由，会将显示返回到之前的路由。\n接下来，您将在 _RandomWordsState 的 build 方法中为 AppBar 添加一个列表图标。当用户点击列表图标时，一个包含保存的收藏夹的新路由会被推送到 Navigator，显示图标。\n在 build 方法中添加图标及其对应的操作:\nclass _RandomWordsState extends State\u0026lt;RandomWords\u0026gt; { ... @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: Text(\u0026#39;Startup Name Generator\u0026#39;), actions: [ // NEW lines from here...  IconButton(icon: Icon(Icons.list), onPressed: _pushSaved), ], // ... to here.  ), home: RandomWords(), ); } ... } 提示：一些小组件属性会取一个小组件(child)，而其他属性，如 action，会取一组小组件(children), 如方括号([])所示。\n在 _RandomWordsState 类中添加一个 _pushSaved() 函数。\nvoid _pushSaved() { } 热重新加载应用程序。列表图标出现在应用栏中。点击它还没有任何作用，因为 _pushSaved 函数是空的。\n接下来，你将建立一条路由，并将其推送到 Navigator 的栈中。这个操作会改变屏幕以显示新的路由。新页面的内容是在 MaterialPageRoute 的构建器属性中以匿名函数的方式构建的。\n调用 Navigator.push，如下图所示，它将路由推送到 Navigator 的堆栈中。IDE 会抱怨无效代码，但你会在下一节中解决这个问题。\nvoid _pushSaved() { Navigator.of(context).push( ); } 接下来，你将添加 MaterialPageRoute 和它的构建器。现在，添加生成 ListTile 行的代码。ListTile 的 divideTiles() 方法在每个 ListTile 之间增加了水平间距。被划分的变量持有通过方便函数 toList() 转换为列表的最终行。\n添加代码，如下面的代码片段所示:\nvoid _pushSaved() { Navigator.of(context).push( MaterialPageRoute\u0026lt;void\u0026gt;( // NEW lines from here...  builder: (BuildContext context) { final tiles = _saved.map( (WordPair pair) { return ListTile( title: Text( pair.asPascalCase, style: _biggerFont, ), ); }, ); final divided = ListTile.divideTiles( context: context, tiles: tiles, ).toList(); return Scaffold( appBar: AppBar( title: Text(\u0026#39;Saved Suggestions\u0026#39;), ), body: ListView(children: divided), ); }, // ...to here.  ), ); } } builder 属性返回一个 Scaffold，包含名为 SavedSuggestions 的新路由的应用栏。新路由的主体由一个包含 ListTiles 行的 ListView 组成。每一行都由一个分隔符隔开。\n热重载应用。将一些选择收藏起来，然后点击应用栏中的列表图标。新的路由出现，包含收藏夹。请注意，Navigator 在应用栏中增加了一个\u0026quot;返回\u0026quot;按钮。你不必明确地实现 Navigator.pop。点击\u0026quot;返回\u0026quot;按钮就可以返回到主路由。\niOS - Main route\niOS - Saved suggestions route\n遇到问题了？ 如果你的应用程序没有正确运行，那么你可以使用下面链接中的代码来回到正轨。\n lib/main.dart  使用主题改变用户界面 在这一步中，您将修改应用程序的主题。主题控制你的应用程序的外观和感觉。您可以使用默认主题，这取决于物理设备或模拟器，或者自定义主题以反映您的品牌。\n您可以通过配置 ThemeData 类轻松更改应用程序的主题。应用程序使用默认主题，但你会将应用程序的主色调改为白色。\n在 MyApp 类中更改颜色:\nclass MyApp extends StatelessWidget { @override Widget build(BuildContext context) { return MaterialApp( title: \u0026#39;Startup Name Generator\u0026#39;, theme: ThemeData( // Add the 3 lines from here...  primaryColor: Colors.white, ), // ... to here.  home: RandomWords(), ); } } 热重载应用。现在整个背景都是白色的，甚至应用栏也是白色的。\n作为一个练习，使用 ThemeData 来改变 UI 的其他方面。Material 库中的 Colors 类提供了许多你可以玩的颜色常量。热重载使得对 UI 的实验变得快速而简单。\nAndroid\niOS\n遇到问题了？ 如果你已经偏离了轨道，那么使用下面链接中的代码来查看最终应用的代码。\n= lib/main.dart\n做得很好！ 你写了一个交互式的 Flutter 应用，可以在 iOS 和 Android 上运行，具体做法如下\n 编写 Dart 代码。 使用热重载来加快开发周期。 实现一个有状态的 widget，为你的应用添加交互性。 创建途径并添加在原途径和新途径之间移动的逻辑。 学习如何使用主题改变你的应用程序的 UI 外观。  今后的步骤 从以下资源中了解更多关于 Flutter SDK 的信息。\n Flutter 中的布局 增加互动性教程 组件介绍 为 Android 开发者提供的 Flutter 针对 React Native 开发者的 Flutter Web 开发人员的 Flutter Flutter YouTube 频道  其他资源包括以下几点:\n 用 Flutter 构建本地移动应用 从 Java 到 Dart codelab Flutter cookbook 融入 Dart 的 Bootstrap：了解更多关于这门语言的信息  同时，与 Flutter 社区联系起来!\n","permalink":"https://ohmyweekly.github.io/notes/write-your-first-flutter-app-part-two/","tags":["flutter","app"],"title":"编写你的第一个 Flutter 应用，第二部分"},{"categories":null,"contents":"毕竟谁人终得鹿, 不如终日梦为鱼。\n友情链接  ohmycloud ✔ ohmycloudy ✔ ohmysummer ✔ ohmyshunny ✔ ohmysunny ✔ ohmyraku ✔ ohmypanda ✔ ohmyweekly ✔ rakulang ✔ ","permalink":"https://ohmyweekly.github.io/posts/about/","tags":null,"title":"第一篇日志"},{"categories":["programming"],"contents":"重要的概念 与 Java 不同，Dart 没有关键字 \u0026ldquo;public\u0026rdquo;, \u0026ldquo;protected\u0026rdquo; 和 \u0026ldquo;private\u0026rdquo;。 如果标识符以下划线（_）开头，则它相对于库是私有的。 有关更多信息，参考库和可见性。\n变量 var name = \u0026#39;Bob\u0026#39;; // 类型推断  dynamic name = \u0026#39;Bob\u0026#39;; // 动态类型 String name = \u0026#39;Bob\u0026#39;; // 显式声明 默认值 未初始化的变量默认值是 null。即使变量是数字, 类型默认值也是 null, 因为在 Dart 中一切都是对象，数字类型也不例外。\nint lineCount; assert(lineCount == null); final 和 const 使用 final 关键字声明的变量, 其值只能被设置一次, 使用 const 关键字声明的变量, 其值在编译时就已固定:\nfinal name = \u0026#39;Bob\u0026#39;; // 不使用类型注解 final String nickname = \u0026#39;Bobby\u0026#39;; name = \u0026#39;Alice\u0026#39;; // Error: final 变量只能被设置一次 const 声明一个在编译时就固定不变的值, 例如数字字面量、字符串字面量:\nconst bar = 1000000; // 压力单位(dynes/cm2) const double atm = 1.01325 * bar; // 一个标准大气压强 内置类型 Dart 语言支持以下内置类型:\n   类型 字面量 对象     Number 2⁶³ -1    String \u0026lsquo;Hello\u0026rsquo;    Boolean true,false    List [1,2,3]    Set {\u0026lsquo;raku\u0026rsquo;,\u0026lsquo;perl\u0026rsquo;}    Map {\u0026lsquo;lan\u0026rsquo;: \u0026lsquo;raku\u0026rsquo;}    Rune \\u2665, \\u{1f600}    Symbol #dadix, #bar     ","permalink":"https://ohmyweekly.github.io/notes/dart/","tags":["dart","examples"],"title":"Dart 入门"},{"categories":["programming"],"contents":"这个代码实验室教你如何使用实现 Iterable类的集合-例如 List和 Set。迭代类是各种 Dart 应用程序的基本构建模块，你可能已经在使用它们，甚至没有注意到。这个代码实验室将帮助你充分利用它们。\n使用嵌入式 DartPad 编辑器，你可以通过运行示例代码和完成练习来测试你的知识。\n要想从这个 codelab 中获得最大的收获，你应该具备基本的 Dart 语法知识。\n本课程包括以下内容。\n 如何读取一个 Iterable 的元素。 如何检查一个 Iterable 的元素是否满足一个条件。 如何过滤一个 Iterable 的内容。 如何将一个 Iterable 的内容映射到不同的值。  估计完成这个代码实验所需的时间: 60分钟。\n什么是集合? 集合是代表一组对象的对象，这些对象称为元素。迭代元素是集合的一种。\n集合可以是空的，也可以包含许多元素。根据不同的目的，集合可以有不同的结构和实现。这些是一些最常见的集合类型:\n List: 用来通过索引读取元素。 Set: 用于包含只能出现一次的元素。 Map：用于通过键来读取元素。  什么是Iterable? Iterable 是一个元素的集合，它可以被依次访问。\n在 Dart 中，Iterable 是一个抽象类，这意味着你不能直接实例化它。然而，你可以通过创建一个新的 List 或 Set 来创建一个新的 Iterable。\nList 和 Set 都是 Iterable，所以它们和 Iterable 类有相同的方法和属性。\nMap 在内部使用不同的数据结构，这取决于它的实现。例如，HashMap 使用了一个哈希表，其中的元素(也称为值)是通过一个键获得的。通过使用 Map 的 entries 或 values 属性，Map 的元素也可以作为 Iterable 对象读取。\n这个例子显示了一个 int 的 List，它也是一个 int 的 Iterable:\nIterable\u0026lt;int\u0026gt; iterable = [1, 2, 3]; 与 List 的区别在于，使用 Iterable，你无法保证按索引读取元素的效率。Iterable 与 List 相比，没有 [] 操作符。\n例如，考虑以下代码，这是无效的:\nIterable\u0026lt;int\u0026gt; iterable = [1, 2, 3]; int value = iterable[1]; 如果你用 [] 读取元素，编译器会告诉你 '[]' 这个运算符没有为 Iterable 类定义，这意味着在这种情况下你不能使用 [index]。\n你可以用 elementAt() 来读取元素，它可以遍历迭代的元素，直到它到达那个位置。\nIterable\u0026lt;int\u0026gt; iterable = [1, 2, 3]; int value = iterable.elementAt(1); 继续下一节，了解更多关于如何访问 Iterable 的元素。\n读取元素 你可以使用 for-in 循环，依次读取一个迭代元素。\n例子: 使用 for-in 循环 下面的例子展示了如何使用 for-in 循环读取元素。\nvoid main() { var iterable = [\u0026#39;Salad\u0026#39;, \u0026#39;Popcorn\u0026#39;, \u0026#39;Toast\u0026#39;]; for (var element in iterable) { print(element); } } 在幕后，for-in 循环使用了一个迭代器。然而，你很少看到直接使用迭代器 API，因为 for-in 更容易阅读和理解，而且不容易出错。\n关键术语:\n Iterable: Dart Iterable 类。 Iterator: for-in 用来从一个 Iterable 对象中读取元素的对象。 for-in 循环: 从一个 Iterable 对象中依次读取元素的简单方法。  例子：使用第一个和最后一个元素 在某些情况下，你只想访问一个 Iterable 的第一个或最后一个元素。\n在 Iterable 类中，你不能直接访问元素，所以你不能调用 iterable[0] 来访问第一个元素。相反，你可以使用 first，它可以获取第一个元素。\n另外，使用 Iterable 类，你不能使用操作符 [] 来访问最后一个元素，但是你可以使用 last 属性。\n因为访问一个 Iterable 的最后一个元素需要踏过所有其他元素，所以 last 可能会很慢。在一个空的 Iterable 上使用 first 或 last 会导致一个 StateError。\nvoid main() { Iterable iterable = [\u0026#39;Salad\u0026#39;, \u0026#39;Popcorn\u0026#39;, \u0026#39;Toast\u0026#39;]; print(\u0026#39;The first element is ${iterable.first}\u0026#39;); print(\u0026#39;The last element is ${iterable.last}\u0026#39;); } 在这个例子中，你看到了如何使用 first 和 last 来获得一个 Iterable 的第一个和最后一个元素。也可以找到满足条件的第一个元素。下一节将展示如何使用名为 firstWhere() 的方法来实现这一目标。\n例子: 使用 firstWhere() 你已经看到，你可以依次访问一个 Iterable 的元素，你可以很容易地得到第一个或最后一个元素。\n现在，你要学习如何使用 firstWhere() 来寻找满足某些条件的第一个元素。这个方法需要你传递一个谓词，它是一个函数，如果输入满足一定的条件就返回 true。\nString element = iterable.firstWhere((element) =\u0026gt; element.length \u0026gt; 5); 例如，如果你想找到第一个超过 5 个字符的 String，你必须传递一个当元素大小大于 5 时返回 true 的谓词。\n运行下面的例子，看看 firstWhere() 是如何工作的。你认为所有的函数都会给出相同的结果吗？\nbool predicate(String element) { return element.length \u0026gt; 5; } main() { var items = [\u0026#39;Salad\u0026#39;, \u0026#39;Popcorn\u0026#39;, \u0026#39;Toast\u0026#39;, \u0026#39;Lasagne\u0026#39;]; // You can find with a simple expression:  var element1 = items.firstWhere((element) =\u0026gt; element.length \u0026gt; 5); print(element1); // Or try using a function block:  var element2 = items.firstWhere((element) { return element.length \u0026gt; 5; }); print(element2); // Or even pass in a function reference:  var element3 = items.firstWhere(predicate); print(element3); // You can also use an `orElse` function in case no value is found!  var element4 = items.firstWhere( (element) =\u0026gt; element.length \u0026gt; 10, orElse: () =\u0026gt; \u0026#39;None!\u0026#39;, ); print(element4); } 在这个例子中，你可以看到三种不同的方式来写一个谓词。\n 作为一个表达式: 测试代码中有一行使用了箭头语法(=\u0026gt;)。 作为一个块: 测试代码在括号和返回语句之间有多行。 作为一个函数: 测试代码在一个外部函数中，作为参数传递给 firstWhere() 方法。  没有正确或错误的方式。使用最适合你的方式，并且让你的代码更容易阅读和理解。\n在这个例子中，firstWhereWithOrElse() 调用 firstWhere() 时，使用了可选的命名参数 orElse，它在没有找到元素时提供了一个替代方案。在这种情况下，返回文本 \u0026ldquo;None!\u0026quot;，因为没有元素满足提供的条件。\n注意：如果没有元素满足测试谓词，并且没有提供 orElse 参数，那么 firstWhere() 会抛出一个 StateError。\n快速回顾。\n Iterable 的元素必须按顺序访问。 迭代所有元素的最简单方法是使用 for-in 循环。 你可以使用 first 和 last getters 来获取第一个和最后一个元素。 你也可以用 firstWhere() 找到满足条件的第一个元素。 你可以把测试谓词写成表达式、块或函数。  关键术语。\n谓词: 当某个条件被满足时，返回 true 的函数。\n练习: 练习写一个测试谓词 下面的练习是一个失败的单元测试，其中包含一个部分完整的代码片段。你的任务是通过编写代码使测试通过来完成练习。你不需要实现 main()。\n这个练习介绍了 singleWhere() 这个方法的工作原理类似于 firstWhere()，但在这种情况下，它只期望 Iterable 中的一个元素满足谓词。如果 Iterable 中超过一个或没有元素满足谓词条件，那么该方法会抛出一个 StateError 异常。\nsingleWhere() 对整个 Iterable 进行步进，直到最后一个元素，如果 Iterable 是无限的或包含一个大的元素集合，这可能会引起问题。\n你的目标是实现满足以下条件的 singleWhere() 谓词。\n 元素包含字符 \u0026lsquo;a\u0026rsquo;。 该元素以字符 \u0026lsquo;M\u0026rsquo; 开头。  测试数据中的所有元素都是字符串，你可以查看类文档以获得帮助。\nString singleWhere(Iterable\u0026lt;String\u0026gt; items) { return items.singleWhere((element) =\u0026gt; element.startsWith(\u0026#39;M\u0026#39;) \u0026amp;\u0026amp; element.contains(\u0026#39;a\u0026#39;)); } 检查条件 在使用 Iterable 时，有时你需要验证一个集合的所有元素是否满足某些条件。\n你可能会想用 for-in 循环来写一个解决方案，比如这个:\nfor (var item in items) { if (item.length \u0026lt; 5) { return false; } } return true; 然而，你可以使用 every() 方法实现同样的目的:\nreturn items.every((element) =\u0026gt; element.length \u0026gt;= 5); 使用 every() 方法可以使代码更易读、更紧凑、更不容易出错。\n例子: 使用 any() 和 every() Iterable 类提供了两个可以用来验证条件的方法。\n any(): 如果至少有一个元素满足条件，则返回 true。 every(): 如果所有元素都满足条件，则返回 true。  运行这个练习来看看它们的作用。\nvoid main() { var items = [\u0026#39;Salad\u0026#39;, \u0026#39;Popcorn\u0026#39;, \u0026#39;Toast\u0026#39;]; if (items.any((element) =\u0026gt; element.contains(\u0026#39;a\u0026#39;))) { print(\u0026#39;At least one element contains \u0026#34;a\u0026#34;\u0026#39;); } if (items.every((element) =\u0026gt; element.length \u0026gt;= 5)) { print(\u0026#39;All elements have length \u0026gt;= 5\u0026#39;); } } 在这个例子中，any() 验证了至少一个元素包含字符 a，every() 验证了所有元素的长度等于或大于 5。\n运行代码后，尝试更改 any() 的谓词，使其返回 false:\nif (items.any((element) =\u0026gt; element.contains(\u0026#39;Z\u0026#39;))) { print(\u0026#39;At least one element contains \u0026#34;Z\u0026#34;\u0026#39;); } else { print(\u0026#39;No element contains \u0026#34;Z\u0026#34;\u0026#39;); } 你也可以使用 any() 来验证一个 Iterable 中没有元素满足某个条件。\n练习： 验证一个 Iterable 是否满足一个条件 下面的练习提供了使用前面例子中描述的 any() 和 every() 方法的练习。在本例中，你的工作对象是一组用户，由具有成员字段 age 的 User 对象表示。\n使用 any() 和 every() 实现两个函数。\n 第1部分：实现 anyUserUnder18()。  如果至少有一个用户是17岁或更小，则返回 true。   第2部分：实现 everyUserOver13()。  如果所有用户都是14岁或以上，则返回 true。    bool anyUserUnder18(Iterable\u0026lt;User\u0026gt; users) { return users.any((user) =\u0026gt; user.age \u0026lt; 18); } bool everyUserOver13(Iterable\u0026lt;User\u0026gt; users) { return users.every((user) =\u0026gt; user.age \u0026gt; 13); } class User { String name; int age; User( this.name, this.age, ); } 快速回顾:\n 虽然你可以使用 for-in 循环来检查条件，但还有更好的方法。 方法 any() 可以让你检查任何元素是否满足条件。 方法 every() 可以让你验证所有元素是否满足条件。  过滤 前面的章节介绍了 firstWhere() 或 singleWhere() 等方法，这些方法可以帮助你找到满足某个谓词的元素。\n但是如果你想找到满足某个条件的所有元素呢？你可以使用 where() 方法来实现。\nvar evenNumbers = numbers.where((number) =\u0026gt; number.isEven); 在这个例子中，numbers 包含一个有多个 int 值的 Iterable，where() 可以找到所有偶数的数字。\nwhere() 的输出是另一个 Iterable，你可以用它来迭代它或应用其他 Iterable 方法。在下一个例子中，where() 的输出直接在 for-in 循环中使用。\nvar evenNumbers = numbers.where((number) =\u0026gt; number.isEven); for (var number in evenNumbers) { print(\u0026#39;$numberis even\u0026#39;); } 例子: 使用 where() 运行这个例子，看看如何将 where() 与其他方法如 any() 一起使用。\nmain() { var evenNumbers = [1, -2, 3, 42].where((number) =\u0026gt; number.isEven); for (var number in evenNumbers) { print(\u0026#39;$numberis even.\u0026#39;); } if (evenNumbers.any((number) =\u0026gt; number.isNegative)) { print(\u0026#39;evenNumbers contains negative numbers.\u0026#39;); } // If no element satisfies the predicate, the output is empty.  var largeNumbers = evenNumbers.where((number) =\u0026gt; number \u0026gt; 1000); if (largeNumbers.isEmpty) { print(\u0026#39;largeNumbers is empty!\u0026#39;); } } 在这个例子中，where() 用于查找所有偶数，然后用 any() 检查结果是否包含负数。\n在本例的后面，再次使用 where() 来查找所有大于1000的数字，由于没有，结果是一个空的 Iterable。\n注意：如果没有元素满足 where() 中的谓词，那么该方法返回一个空的 Iterable。与 singleWhere() 或 firstWhere() 不同，where() 不会抛出 StateError 异常。\n例子: 使用 takeWhile 方法 takeWhile() 和 skipWhile() 也可以帮助你从一个 Iterable 中过滤元素。\n运行这个例子，看看 takeWhile() 和 skipWhile() 如何分割一个包含数字的 Iterable。\nmain() { var numbers = [1, 3, -2, 0, 4, 5]; var numbersUntilZero = numbers.takeWhile((number) =\u0026gt; number != 0); print(\u0026#39;Numbers until 0: $numbersUntilZero\u0026#39;); var numbersAfterZero = numbers.skipWhile((number) =\u0026gt; number != 0); print(\u0026#39;Numbers after 0: $numbersAfterZero\u0026#39;); } 输出如下:\nNumbers until 0: (1, 3, -2) Numbers after 0: (0, 4, 5) 在这个例子中，takeWhile() 返回一个 Iterable，它包含了通往满足谓词的元素的所有元素。另一方面， skipWhile() 返回一个 Iterable，同时跳过满足谓词的元素之前的所有元素。请注意，满足谓词的元素也会被包含在内。\n运行该示例后，将 takeWhile() 改为取元素，直到到达第一个负数。\nvar numbersUntilNegative = numbers.takeWhile((number) =\u0026gt; !number.isNegative); 注意，条件 number.isNegative 是用 ! 否定的。\n练习: 从列表中过滤元素 下面的练习提供了使用上一练习中的 User 类的 where() 方法的练习。\n使用 where() 实现两个函数。\n 第1部分：实现 filterUnder21()。  返回一个包含所有21岁以上用户的 Iterable。   第2部分：实现 findShortNamed()。  返回一个包含所有名字长度为 3 或更少的用户的 Iterable。    Iterable\u0026lt;User\u0026gt; filterUnder21(Iterable\u0026lt;User\u0026gt; users) { return users.where((user) =\u0026gt; user.age \u0026gt;= 21); } Iterable\u0026lt;User\u0026gt; findShortNamed(Iterable\u0026lt;User\u0026gt; users) { return users.where((user) =\u0026gt; user.name.length \u0026lt;= 3); } class User { String name; int age; User( this.name, this.age, ); } 快速回顾:\n 用 where() 过滤一个 Iterable 的元素。 where() 的输出是另一个 Iterable。 使用 takeWhile() 和 skipWhile() 来获取元素，直到满足一个条件或之后。 这些方法的输出可以是一个空的 Iterable。  Map 通过 map() 方法映射 Iterables，你可以在每个元素上应用一个函数，用一个新的元素替换每个元素。\nIterable\u0026lt;int\u0026gt; output = numbers.map((number) =\u0026gt; number * 10); 在这个例子中，Iterable 数字的每个元素都被乘以 10。\n你也可以使用 map() 将一个元素转换为不同的对象-例如，将所有 int 转换为 String，在下面的例子中可以看到。\nIterable\u0026lt;String\u0026gt; output = numbers.map((number) =\u0026gt; number.toString()); 注意：map() 返回一个懒惰的 Iterable，这意味着只有在元素被迭代时才会调用所提供的函数。\n例子: 使用 map 改变元素 运行这个例子，看看如何使用 map() 将一个 Iterable 中的所有元素乘以2，你认为输出会是什么？\nmain() { var numbersByTwo = [1, -2, 3, 42].map((number) =\u0026gt; number * 2); print(\u0026#39;Numbers: $numbersByTwo.\u0026#39;); } 练习: 映射到不同类型 在前面的例子中，你把一个 Iterable 的元素乘以2，输入和输出都是 int 的 Iterable。\n在这个练习中，你的代码接收一个 User的 Iterable，你需要返回一个包含用户名和年龄的字符串的 Iterable。\nIterable 中的每个字符串必须遵循这样的格式。'{name} is {age}'-例如 'Alice is 21'。\nIterable\u0026lt;String\u0026gt; getNameAndAges(Iterable\u0026lt;User\u0026gt; users) { return users.map((user) =\u0026gt; \u0026#39;${user.name}is ${user.age}\u0026#39;); } class User { String name; int age; User( this.name, this.age, ); } 快速回顾:\n map() 将一个函数应用于一个 Iterable 的所有元素。 map() 的输出是另一个 Iterable。 在 Iterable 被迭代之前，函数不会被计算。  练习: 把所有的东西放在一起 现在是练习所学知识的时候了，在最后一个练习中。\n这个练习提供了类 EmailAddress，它有一个构造函数，接收一个字符串。另一个提供的函数是 isValidEmailAddress()，它测试一个电子邮件地址是否有效。\n   构造函数/函数 类型签名 描述     EmailAddress() EmailAddress(String address) 为指定的地址创建一个 EmailAddress。   isValidEmailAddress() bool isValidEmailAddress(EmailAddress) 如果提供的 EmailAddress 有效，返回 true。    编写以下代码。\n第1部分：实现 parseEmailAddresses()。\n 编写函数 parseEmailAddresses()，它接收一个包含电子邮件地址的 Iterable\u0026lt;String\u0026gt;，并返回一个 Iterable\u0026lt;EmailAddress\u0026gt;。 使用方法 map() 从 String 映射到 EmailAddress。 使用构造函数 EmailAddress(String) 创建 EmailAddress 对象。  第二部分：实现 anyInvalidEmailAddress()。\n 编写函数 anyInvalidEmailAddress()，它接收一个 Iterable\u0026lt;EmailAddress\u0026gt;，并在 Iterable 中的任何 EmailAddress 无效时返回 true。 使用方法 any() 和提供的函 isValidEmailAddress()。  第3部分：实现 validEmailAddresses()。\n 编写函数 validEmailAddresses()，它接收一个 Iterable\u0026lt;EmailAddress\u0026gt; 并返回另一个只包含有效地址的 Iterable\u0026lt;EmailAddress\u0026gt;。 使用方法 where() 来过滤 Iterable\u0026lt;EmailAddress\u0026gt;。 使用提供的函数 isValidEmailAddress() 来评估一个 EmailAddress 是否有效。  Iterable\u0026lt;EmailAddress\u0026gt; parseEmailAddresses(Iterable\u0026lt;String\u0026gt; strings) { return strings.map((s) =\u0026gt; EmailAddress(s)); } bool anyInvalidEmailAddress(Iterable\u0026lt;EmailAddress\u0026gt; emails) { return emails.any((email) =\u0026gt; !isValidEmailAddress(email)); } Iterable\u0026lt;EmailAddress\u0026gt; validEmailAddresses(Iterable\u0026lt;EmailAddress\u0026gt; emails) { return emails.where((email) =\u0026gt; isValidEmailAddress(email)); } class EmailAddress { String address; EmailAddress(this.address); @override bool operator ==(Object other) =\u0026gt; identical(this, other) || other is EmailAddress \u0026amp;\u0026amp; runtimeType == other.runtimeType \u0026amp;\u0026amp; address == other.address; @override int get hashCode =\u0026gt; address.hashCode; @override String toString() { return \u0026#39;EmailAddress{address: $address}\u0026#39;; } } 下一步是什么? 恭喜你，你完成了 codelab 的学习! 如果你想了解更多，这里有一些下一步的建议。\n 玩玩 DartPad。 试试另一个代码实验。 阅读 Iterable API 参考资料，了解本 codelab 未涉及的方法。  ","permalink":"https://ohmyweekly.github.io/notes/dart-iterable-collections/","tags":["iterable","collection","dart"],"title":"Dart 可迭代集合"},{"categories":["programming"],"contents":"字符串插值 使用 ${expression} 将表达式的值放到字符串里面。如果表达式是一个标识符, 就可以省略 {}。\n下面是字符串插值的例子:\n   字符串 结果     \u0026lsquo;${3 + 2}\u0026rsquo; \u0026lsquo;5\u0026rsquo;   \u0026lsquo;${\u0026ldquo;word\u0026rdquo;.toUpperCase()}\u0026rsquo; \u0026lsquo;WORD\u0026rsquo;   \u0026lsquo;$myObject\u0026rsquo; The value of myObject.toString()    代码示例 下面的函数接收两个整数作为参数。使其返回一个包含两个整数的字符串，并以空格分隔。例如 stringify(2, 3) 应该返回 \u0026lsquo;2 3\u0026rsquo;。\nString stringify(int x, int y) { return \u0026#39;$x$y\u0026#39;; } Null 无感知操作符 Dart 提供了一些方便的操作符来处理可能为空的值。其中一个是 ??= 赋值运算符，只有当一个变量当前为空时，它才会给这个变量赋值:\nint a; // a 的初始值为 null a ??= 3; print(a); // 打印 3  a ??= 5; print(a); // 仍然打印 3 另一个 null-aware 操作符是 ??，它返回其左边的表达式，除非该表达式的值为 null，在这种情况下，它计算并返回其右边的表达式:\nprint(1 ?? 3); // 打印 1 print(null ?? 12); // 打印 12 代码示例 String foo = \u0026#39;a string\u0026#39;; String bar; // Unassigned objects are null by default.  // makes \u0026#39;a string\u0026#39; be assigned to baz. String baz = foo ?? bar; void updateSomeVars() { // makes \u0026#39;a string\u0026#39; be assigned to bar.  bar ??= \u0026#39;a string\u0026#39;; } 有条件的属性访问 要保护对对象的一个可能为空的属性或方法的访问，请在点(.)前加上一个问号(?):\nmyObject?.someProperty 上述代码等同于以下代码:\n(myObject != null) ? myObject.someProperty : null 你可以在一个表达式中把 ?. 的多个使用链接在一起:\nmyObject?.someProperty?.someMethod() 如果 myObject 或 myObject.someProperty 为 null，前面的代码将返回 null(并且从不调用 someMethod())。\n代码示例 尝试使用条件属性访问来完成下面的代码片段。\n// This method should return the uppercase version of `str` // or null if `str` is null. String upperCaseIt(String str) { // Try conditionally accessing the `toUpperCase` method here.  return str?.toUpperCase(); } 集合字面量 Dart 内置了对列表、映射和集合的支持。你可以使用字面量创建它们:\nfinal aListOfStrings = [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]; final aSetOfStrings = {\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;}; final aMapOfStringsToInts = { \u0026#39;one\u0026#39;: 1, \u0026#39;two\u0026#39;: 2, \u0026#39;three\u0026#39;: 3, } Dart 的类型推理可以为你分配类型给这些变量。在本例中，推断的类型是 List\u0026lt;String\u0026gt;、Set\u0026lt;String\u0026gt; 和 Map\u0026lt;String, int\u0026gt;。\n或者你可以自己指定类型:\nfinal aListOfInts = \u0026lt;int\u0026gt;[]; final aSetOfInts = \u0026lt;int\u0026gt;{}; final aMapOfIntToDouble = \u0026lt;int, double\u0026gt;{}; 当你用子类型的内容初始化一个列表，但仍然希望列表是 List\u0026lt;BaseType\u0026gt; 时，指定类型是很方便的:\nfinal aListOfBaseType = \u0026lt;BaseType\u0026gt;[SubType(), SubType()]; 代码示例 尝试将以下变量设置为指定的值。\n// Assign this a list containing \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, and \u0026#39;c\u0026#39; in that order: final aListOfStrings = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]; // Assign this a set containing 3, 4, and 5: final aSetOfInts = {3, 4, 5}; // Assign this a map of String to int so that aMapOfStringsToInts[\u0026#39;myKey\u0026#39;] returns 12: final aMapOfStringsToInts = {\u0026#39;myKey\u0026#39;: 12}; // Assign this an empty List\u0026lt;double\u0026gt;: final anEmptyListOfDouble = \u0026lt;double\u0026gt;[]; // Assign this an empty Set\u0026lt;String\u0026gt;: final anEmptySetOfString = \u0026lt;String\u0026gt;{}; // Assign this an empty Map of double to int: final anEmptyMapOfDoublesToInts = \u0026lt;double, int\u0026gt;{}; 箭头语法 你可能在 Dart 代码中看到过 =\u0026gt; 符号。这种箭头语法是一种定义函数的方式，该函数执行其右边的表达式并返回其值。\n例如，考虑这个对 List 类的 any() 方法的调用:\nbool hasEmpty = aListOfStrings.any((s) { return s.isEmpty; }); 这里有一个更简单的方法来写这个代码:\nbool hasEmpty = aListOfStrings.any((s) =\u0026gt; s.isEmpty); 代码示例 试着完成以下使用箭头语法的语句:\nclass MyClass { int _value1 = 2; int _value2 = 3; int _value3 = 5; // Returns the product of the above values:  int get product =\u0026gt; _value1 * _value2 * _value3; // Adds one to _value1:  void incrementValue1() =\u0026gt; _value1++; // Returns a string containing each item in the  // list, separated by commas (e.g. \u0026#39;a,b,c\u0026#39;):  String joinWithCommas(List\u0026lt;String\u0026gt; strings) =\u0026gt; strings.join(\u0026#39;,\u0026#39;); } 级联 要对同一对象进行一系列操作，可以使用级联(...)。我们都见过这样的表达式:\nmyObject.someMethod() 它在 myObject 上调用 someMethod()，表达式的结果是 someMethod() 的返回值。\n下面是同样的表达式，有一个级联:\nmyObject..someMethod() 虽然它仍然在 myObject 上调用 someMethod()，但表达式的结果并不是返回值-它是对 myObject 的引用! 使用级联，你可以将原本需要单独语句的操作串联起来。例如，请看以下代码:\nvar button = querySelector(\u0026#39;#confirm\u0026#39;); button.text = \u0026#39;Confirm\u0026#39;; button.classes.add(\u0026#39;important\u0026#39;); button.onClick.listen((e) =\u0026gt; window.alert(\u0026#39;Confirmed!\u0026#39;)); 有了级联，代码就会变得短得多，而且你也不需要 button 变量:\nquerySelector(\u0026#39;#confirm\u0026#39;) ..text = \u0026#39;Confirm\u0026#39; ..class.add(\u0026#39;important\u0026#39;) ..onClick.listen((e) =\u0026gt; window.alert(\u0026#39;Confirmed!\u0026#39;)); 代码示例 使用级联来创建一个单一的语句，将一个 BigObject 的 anInt、aString 和 aList 属性设置为 1、\u0026lsquo;String!\u0026rsquo; 和 [3.0](分别地)，然后调用 allDone()。\nclass BigObject{ int anInt = 0; String aString = \u0026#39;\u0026#39;; List\u0026lt;double\u0026gt; aList = []; bool _done = false; void allDone() { _done = true; } } BigObject fillBigObject(BigObject obj) { return obj ..anInt = 1 ..aString = \u0026#39;String!\u0026#39; ..aList.add(3) ..allDone(); } getters 和 setters 当你需要对一个属性进行更多的控制时，你可以定义 getter 和 setter，而不是简单的字段。\n例如，你可以确保一个属性的值是有效的:\nclass MyClass { int _aProperty = 0; int get aProperty =\u0026gt; _aProperty; set aProperty(int value) { if (value \u0026gt;= 0) { _aProperty = value; } } } 你也可以使用 getter 来定义计算属性:\nclass MyClass { List\u0026lt;int\u0026gt; _values = []; void addValue(int value) { _values.add(value); } // 一个计算属性  int get count { return _values.length; } } 代码示例 想象一下，你有一个购物车类，它保存了一个私有的 List\u0026lt;double\u0026gt; 的价格。添加以下内容:\n 一个叫做 total 的 getter，返回价格的总和。 用一个新的列表替换列表的 setter，只要新的列表不包含任何负价格(在这种情况下，setter 应该抛出一个 InvalidPriceException)。  class InvalidPriceException {} class ShoppingCart { List\u0026lt;double\u0026gt; _prices = []; double get total =\u0026gt; _prices.fold(0, (e, t) =\u0026gt; e + t); set prices(List\u0026lt;double\u0026gt; value) { if (value.any((p) =\u0026gt; p \u0026lt; 0)) { throw InvalidPriceException(); } _prices = value; } } 可选位置参数 Dart 有两种函数参数：位置参数和命名参数。位置参数是你可能熟悉的那种:\nint sumUp(int a, int b, int c) { return a + b + c; } // ... int total = sumUp(1, 2, 3); 在 Dart 中，你可以将这些位置参数用括号包裹起来，使其成为可选的参数:\nint sumUpToFive(int a, [int b, int c, int d, int e]) { int sum = a; if (b != null) sum += b; if (c != null) sum += c; if (d != null) sum += d; if (e != null) sum += e; return sum; } // ... int total = sumUpToFive(1,2); int otherTotal = sumUpToFive(1, 2, 3, 4, 5); 可选的位置参数在函数的参数列表中总是最后一个。它们的默认值是空的，除非你提供了另一个默认值:\nint sumUpToFive(int a, [int b = 2, int c = 3, int d = 4, int e = 5]) { // ··· } // ··· int newTotal = sumUpToFive(1); print(newTotal); // \u0026lt;-- prints 15 代码示例 实现一个名为 joinWithCommas() 的函数，接受 1 到 5 个整数，然后返回一个用逗号分隔的数字字符串。下面是一些函数调用和返回值的例子:\n   函数调用 返回值     joinWithCommas(1) \u0026lsquo;1\u0026rsquo;   joinWithCommas(1, 2, 3) \u0026lsquo;1,2,3\u0026rsquo;   joinWithCommas(1, 1, 1, 1, 1) \u0026lsquo;1,1,1,1,1\u0026rsquo;    main() { var res = joinWithCommas(1,2,3,4); print(res); } String joinWithCommas(int a, [int b, int c, int d, int e]) { List\u0026lt;int\u0026gt; sum = []; sum.add(a); if (b != null) sum.add(b); if (c != null) sum.add(c); if (d != null) sum.add(d); if (e != null) sum.add(e); return sum.join(\u0026#39;,\u0026#39;); } 可选命名参数 使用大括号语法，你可以定义有名称的可选参数。\nvoid printName(String firstName, String lastName, {String suffix}) { print(\u0026#39;$firstName$lastName${suffix ?? \u0026#39;\u0026#39;}\u0026#39;); } // ··· printName(\u0026#39;Avinash\u0026#39;, \u0026#39;Gupta\u0026#39;); printName(\u0026#39;Poshmeister\u0026#39;, \u0026#39;Moneybuckets\u0026#39;, suffix: \u0026#39;IV\u0026#39;); 正如你所期望的，这些参数的值默认为空，但你可以提供默认值。\nvoid printName(String firstName, String lastName, {String suffix = \u0026#39;\u0026#39;}) { print(\u0026#39;$firstName$lastName$suffix\u0026#39;); } 一个函数不能同时拥有可选的位置参数和可选的命名参数。\n代码示例 为 MyDataObject 类添加一个 copyWith() 实例方法。它应该接受三个命名参数:\n int newInt String newString double newDouble  当调用时，copyWith() 应该基于当前实例返回一个新的 MyDataObject，并将前面参数（如果有的话）的数据复制到对象的属性中。例如，如果 newInt 是非空的，那么将其值复制到 anInt 中。\nclass MyDataObject { final int anInt; final String aString; final double aDouble; MyDataObject({ this.anInt = 1, this.aString = \u0026#39;Old!\u0026#39;, this.aDouble = 2.0, }); MyDataObject copyWith({int newInt, String newString, double newDouble}) { return MyDataObject( anInt: newInt ?? this.anInt, aString: newString ?? this.aString, aDouble: newDouble ?? this.aDouble, ); } } 异常 Dart 代码可以抛出和捕获异常。与 Java 相比，Dart 的所有异常都是未检查的异常。方法不声明它们可能会抛出哪些异常，你也不需要捕捉任何异常。\nDart 提供了 Exception 和 Error 类型，但你可以抛出任何非空对象:\nthrow Exception(\u0026#39;Something bad happened.\u0026#39;); throw \u0026#39;Waaaaaaah!\u0026#39;; 在处理异常时使用 try、on 和 catch 关键字:\ntry { breedMoreLlamas(); } on OutOfLlamasException { // A specific exception  buyMoreLlamas(); } on Exception catch (e) { // Anything else that is an exception  print(\u0026#39;Unknown exception: $e\u0026#39;); } catch (e) { // No specified type, handles all  print(\u0026#39;Something really unknown: $e\u0026#39;); } try 关键字的工作原理和其他大多数语言一样。使用 on 关键字按类型过滤特定的异常，使用 catch 关键字获取异常对象的引用。\n如果不能完全处理异常，可以使用 rethrow 关键字来传播异常:\ntry { breedMoreLlamas(); } catch (e) { print(\u0026#39;I was just trying to breed llamas!.\u0026#39;); rethrow; } 无论是否抛出异常，都要执行代码，使用 final:\ntry { breedMoreLlamas(); } catch (e) { // ... handle exception ... } finally { // Always clean up, even if an exception is thrown.  cleanLlamaStalls(); } 代码示例 实现下面的 tryFunction()。它应该执行一个不可信的方法，然后做如下操作:\n 如果 untrustworthy() 抛出一个 ExceptionWithMessage，调用 logger.logException，并提供异常类型和消息(尝试使用 on 和 catch)。 如果 untrustworthy() 抛出一个 Exception，调用 logger.logException，并注明异常类型(尝试使用 on)。 如果 untrustworthy() 抛出任何其他对象，不要捕获异常。 当所有的东西都被捕获和处理后，调用 logger.doneLogging(尝试使用 finally)。  typedef VoidFunction = void Function(); class ExceptionWithMessage { final String message; const ExceptionWithMessage(this.message); } abstract class Logger { void logException(Type t, [String msg]); void doneLogging(); } void tryFunction(VoidFunction untrustworthy, Logger logger) { try { untrustworthy(); } on ExceptionWithMessage catch (e) { logger.logException(e.runtimeType, e.message); } on Exception { logger.logException(Exception); } finally { logger.doneLogging(); } } 在构造函数中使用 this Dart 提供了一个方便的快捷方式来为构造函数中的属性赋值：在声明构造函数时使用 this.propertyName:\nclass MyColor { int red; int green; int blue; MyColor(this.red, this.green, this.blue) } final color = MyColor(80, 80, 128); 这种技术也适用于命名参数。属性名成为参数的名称:\nclass MyColor { ... MyColor({this.red, this.green, this.blue}); } final color = MyColor(red: 80, green: 80, blue: 80); 对于可选参数，默认值按预期工作:\nMyColor([this.red = 0, this.green = 0, this.blue = 0]); // or MyColor({this.red = 0, this.green = 0, this.blue = 0}); 代码示例 为 MyClass 添加一个单行构造函数，使用 this. 语法来接收和分配类的三个属性的值:\nclass MyClass { final int anInt; final String aString; final double aDouble; MyClass(this.anInt, this.aString, this.aDouble); } 初始化器列表 有时候，当你实现一个构造函数时，你需要在构造函数体执行之前做一些设置。例如，在构造函数体执行之前，final 字段必须有值。在一个初始化器列表中做这些工作，它位于构造函数的签名和它的主体之间。\nPoint.fromJson(Map\u0026lt;String, num\u0026gt; json) : x = json[\u0026#39;x\u0026#39;], y = json[\u0026#39;y\u0026#39;] { print(\u0026#39;In Point.fromJson(): ($x, $y)\u0026#39;); } 初始化器列表也是一个方便放置断言的地方，它只在开发过程中运行:\nNonNegativePoint(this.x, this.y) : assert(x \u0026gt;= 0), assert(y \u0026gt;= 0) { print(\u0026#39;I just made a NonNegativePoint: ($x, $y)\u0026#39;); } 代码示例 完成下面的 FirstTwoLetters 构造函数。使用初始化器列表将 word 中的前两个字符分配给 letterOne 和 LetterTwo 属性。为了获得额外的积分，可以添加一个断言来捕获少于两个字符的单词。\nclass FirstTwoLetters { final String letterOne; final String letterTwo; // Create a constructor with an initializer list here:  FirstTwoLetters(String word) : assert(word.length \u0026gt;=2), letterOne = word[0], letterTwo = word[1]; } 命名构造器 为了允许类有多个构造函数，Dart 支持命名构造函数:\nclass Point { double x, y; Point(this.x, this.y); Point.origin() { x = 0; y = 0; } } 要使用命名构造函数，请使用它的全名来调用它:\nfinal myPoint = Point.origin(); 代码示例 给 Color 类一个名为 Color.black 的构造函数，将三个属性都设置为 0。\nclass Color { int red; int green; int blue; Color(this.red, this.green, this.blue); Color.black() { red = 0; green = 0; blue = 0; } } 工厂构造函数 Dart 支持工厂构造函数，它可以返回子类型甚至 null。要创建一个工厂构造函数，请使用 factory 关键字:\nclass Square extends Shape {} class Circle extends Shape {} class Shape { Shape(); factory Shape.fromTypeName(String typeName) { if (typeName == \u0026#39;square\u0026#39;) return Square(); if (typeName == \u0026#39;circle\u0026#39;) return Circle(); print(\u0026#39;I don\\\u0026#39;t recognize $typeName\u0026#39;); return null } } 代码示例 填入名为 IntegerHolder.fromList 的工厂构造函数，使其做以下工作:\n 如果列表有一个值，就用这个值创建一个 IntegerSingle。 如果列表有两个值，则用该值依次创建一个 IntegerDouble。 如果列表有三个值，则按顺序创建一个 IntegerTriple。 否则，返回 null。  class IntegerHolder { IntegerHolder(); factory IntegerHolder.fromList(List\u0026lt;int\u0026gt; list) { if (list?.length == 1) { return IntegerSingle(list[0]); } else if (list?.length == 2) { return IntegerDouble(list[0], list[1]); } else if (list?.length == 3) { return IntegerTriple(list[0], list[1], list[2]); } else { return null; } } } class IntegerSingle extends IntegerHolder { final int a; IntegerSingle(this.a); } class IntegerDouble extends IntegerHolder { final int a; final int b; IntegerDouble(this.a, this.b); } class IntegerTriple extends IntegerHolder { final int a; final int b; final int c; IntegerTriple(this.a, this.b, this.c); } 重定向构造函数 有时，一个构造函数的唯一目的是重定向到同一类中的另一个构造函数。重定向构造函数的主体是空的，构造函数调用出现在冒号(:)之后。\nclass Automobile { String make; String model; int mpg; // 这个类的主构造函数  Automobile(this.make, this.model, this.mpg); // 代理到主构造函数  Automobile.hybrid(String make, String model) : this(make, model, 60); // 代理到命名构造函数  Automobile.fancyHybrid() : this.hybrid(\u0026#39;Futurecar\u0026#39;, \u0026#39;Mark 2\u0026#39;); } 代码示例 还记得上面的 Color 类吗？创建一个名为 black 的命名构造函数，但不是手动分配属性，而是将其重定向到默认构造函数，参数为 0。\nclass Color { int red; int green; int blue; Color(this.red, this.green, this.blue); Color.black() : this(0, 0, 0); } 常量构造函数 如果你的类产生的对象永远不会改变，你可以让这些对象成为编译时常量。要做到这一点，请定义一个 const 构造函数，并确保所有的实例变量都是最终变量。\nclass ImmutablePoint { const ImmutablePoint(this.x, this.y); final int x; final int y; static const ImmutablePoint origin = ImmutablePoint(0, 0); } 代码示例 修改 Recipe 类，使它的实例可以是常量，并创建一个常量构造函数，执行以下操作。\n 有三个参数： ingredients, calories 和 milligramsOfSodium(按顺序)。 使用 this. 语法，自动将参数值分配给同名的对象属性。 是常量，在构造函数声明中，const 关键字就在 Recipe 前面。  class Recipe { final List\u0026lt;String\u0026gt; ingredients; final int calories; final double milligramsOfSodium; const Recipe(this.ingredients, this.calories, this.milligramsOfSodium); } 下一步是什么？ 我们希望你喜欢使用这个 codelab 来学习或测试你对 Dart 语言一些最有趣的功能的知识。这里有一些关于现在要做什么的建议。\n 试试其他的 Dart 代码实验室. 阅读 Dart 语言之旅。 玩 DartPad。 获取 Dart SDK。  ","permalink":"https://ohmyweekly.github.io/notes/dart-cheatsheet-codelab/","tags":["string","cheatsheet","dart"],"title":"Dart 语言速查表"},{"categories":["programming"],"contents":"端午节快乐, 一起爬山吗？\n","permalink":"https://ohmyweekly.github.io/notes/go-hiking/","tags":["hiking","holiday"],"title":"一起爬山吗?"},{"categories":null,"contents":"Results from static site search implemented using Fusejs, jquery and mark.js. \u0026ndash; Source\n","permalink":"https://ohmyweekly.github.io/search/","tags":null,"title":"Search"}]