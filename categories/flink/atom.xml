<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-us">
    <generator uri="https://gohugo.io/" version="0.85.0">Hugo</generator><title type="html"><![CDATA[Flink on 焉知非鱼]]></title>
    
        <subtitle type="html"><![CDATA[rakulang, dartlang, nimlang, golang, rustlang, lang lang no see]]></subtitle>
    
    
    
            <link href="https://ohmyweekly.github.io/categories/flink/" rel="alternate" type="text/html" title="HTML" />
            <link href="https://ohmyweekly.github.io/categories/flink/index.xml" rel="alternate" type="application/rss+xml" title="RSS" />
            <link href="https://ohmyweekly.github.io/categories/flink/atom.xml" rel="self" type="application/atom+xml" title="Atom" />
            <link href="https://ohmyweekly.github.io/categories/flink/jf2feed.json" rel="alternate" type="application/jf2feed+json" title="jf2feed" />
    <updated>2021-07-12T22:13:54+08:00</updated>
    
    
    
    
        <id>https://ohmyweekly.github.io/categories/flink/</id>
    
        
        <entry>
            <title type="html"><![CDATA[Raku is a match for *]]></title>
            <link href="https://ohmyweekly.github.io/notes/2021-03-12-raku-is-a-match-for-star/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-12-20-flink-cep-complex-event-processing-for-flink/?utm_source=atom_feed" rel="related" type="text/html" title="FlinkCEP - Flink 的复杂事件处理" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-10-execution-mode/?utm_source=atom_feed" rel="related" type="text/html" title="执行模式(批/流)" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-application-building-block/?utm_source=atom_feed" rel="related" type="text/html" title="Application Building Blocks" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-logical-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Logical Functions" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-python-walkthrough/?utm_source=atom_feed" rel="related" type="text/html" title="Python 演练" />
            
                <id>https://ohmyweekly.github.io/notes/2021-03-12-raku-is-a-match-for-star/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2021-03-12T00:00:00+08:00</published>
            <updated>2021-03-12T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Raku Is a Match for *</blockquote><p>PimDaniel <a href="https://gist.github.com/gfldex/ffa8d0e60cbd3845b4d91bd2029fcd5b">提出</a>了一个有趣的问题。</p>
<p>我如何在匹配时测试 match is True : 这不起作用。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">if</span> <span class="k">my</span> <span class="p">(</span><span class="nv">$type</span><span class="o">,</span><span class="nv">$a</span><span class="o">,</span><span class="nv">$b</span><span class="o">,</span><span class="nv">$c</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="nv">$v</span> <span class="o">~~</span> <span class="p">/</span><span class="ni">^</span><span class="sr"> </span><span class="p">(&#39;</span><span class="s1">horiz</span><span class="p">&#39;</span><span class="o">|</span><span class="p">&#39;</span><span class="s1">vertic</span><span class="p">&#39;)</span><span class="sr"> </span><span class="p">&#39;</span><span class="s1">_</span><span class="p">&#39;</span><span class="sr"> </span><span class="p">(</span><span class="se">\d</span><span class="o">+</span><span class="p">)</span><span class="sr"> </span><span class="p">&#39;</span><span class="s1">_</span><span class="p">&#39;</span><span class="sr"> </span><span class="p">(</span><span class="se">\d</span><span class="o">+</span><span class="p">)</span><span class="sr"> </span><span class="p">&#39;</span><span class="s1">_</span><span class="p">&#39;</span><span class="sr"> </span><span class="p">(</span><span class="se">\d</span><span class="o">+</span><span class="p">)</span><span class="sr"> </span><span class="ni">$</span><span class="p">/)</span><span class="o">&gt;&gt;.</span><span class="nb">Str</span> <span class="p">{</span> <span class="o">...</span> <span class="p">}</span>
</code></pre></div><p>好吧，我分2次做了1/捕捉并测试匹配，2/将匹配转换为Str。</p>
<p>没有得到及时的回答，完全没有改进。我也找不到一个好的方法来快速完成这个任务。事实上我花了一个小时才破解这个螺母。这里的主要问题是，一个失败的匹配会产生 <code>.Str</code> 会抱怨的 Nil。所以让我们把 if 的布尔检查和转换为 Str 的过程分开。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">my</span> <span class="nv">$a</span> <span class="o">=</span> <span class="p">&#39;</span><span class="s1">1 B</span><span class="p">&#39;;</span>

<span class="k">if</span> <span class="nv">$a</span> <span class="o">~~</span> <span class="p">/(&lt;</span><span class="nb">digit</span><span class="p">&gt;)</span><span class="sr"> </span><span class="se">\s</span><span class="sr"> </span><span class="p">(&lt;</span><span class="nb">alpha</span><span class="p">&gt;)/</span> <span class="k">-&gt;</span> <span class="nv">$_</span> <span class="p">{</span>
    <span class="k">my</span> <span class="p">(</span><span class="nv">$one</span><span class="o">,</span> <span class="nv">$B</span><span class="p">)</span> <span class="o">=</span> <span class="o">.</span><span class="nb">deepmap</span><span class="o">:</span> <span class="o">*.</span><span class="nb">Str</span><span class="p">;</span>
    <span class="nb">say</span> <span class="p">&#34;</span><span class="nv">$one</span><span class="s2"> </span><span class="nv">$B</span><span class="p">&#34;;</span>
<span class="p">}</span>
<span class="c1"># OUTPUT: 1 B</span>
</code></pre></div><p>通过将条件表达式的结果强行放入主题中，我们可以在匹配的结果上运行任何方法，但前提是 <code>Match.bool</code> 返回 true。我没有 <code>CS*</code> 学位，但如果 Raku-signatures 不会变成 turing complete，我会非常惊讶。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">if</span> <span class="nv">$a</span> <span class="o">~~</span> <span class="p">/(&lt;</span><span class="nb">digit</span><span class="p">&gt;)</span><span class="sr"> </span><span class="se">\s</span><span class="sr"> </span><span class="p">(&lt;</span><span class="nb">alpha</span><span class="p">&gt;)/</span> <span class="k">-&gt;</span> <span class="nb">Match</span> <span class="p">(</span><span class="nb">Str</span><span class="p">()</span> <span class="nv">$one</span><span class="o">,</span> <span class="nb">Str</span><span class="p">()</span> <span class="nv">$B</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">dd</span> <span class="nv">$one</span><span class="p">;</span>
    <span class="n">dd</span> <span class="nv">$B</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1"># OUTPUT: &#34;1&#34;</span>
          <span class="p">&#34;</span><span class="s2">B</span><span class="p">&#34;</span>
</code></pre></div><p><code>if</code> 块的签名将 <code>Match</code> 胁迫为一个列表。我们选择其中的两个元素，并将这些元素胁迫为 <code>Str</code>。当然，我们可以根据捕获的位置来强制到任何我们喜欢的东西。</p>
<p>Raku 中的 Regexes 被编译成相同的字节码，然后程序的其余部分。事实上，语法只是一个具有有趣语法的类。这就是为什么我们可以轻松地在 regex  里面运行Raku 代码。这意味着我们可以把整个程序从内部翻出来。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">my</span> <span class="nv">@a</span> <span class="o">=</span> <span class="p">&lt;</span><span class="s">1 B 3 D 4</span><span class="p">&gt;;</span>
<span class="k">my</span> <span class="nv">@b</span><span class="p">;</span>

<span class="k">my</span> <span class="nv">$hit</span><span class="p">;</span>

<span class="k">for</span> <span class="nv">@a</span> <span class="k">-&gt;</span> <span class="nv">$e</span> <span class="p">{</span>
    <span class="nv">@b</span><span class="o">.</span><span class="nb">push</span><span class="o">:</span> <span class="p">(</span><span class="nv">$e</span> <span class="o">~~</span> <span class="p">/(&lt;</span><span class="nb">alpha</span><span class="p">&gt;)</span><span class="sr"> </span><span class="o">||</span><span class="sr"> </span><span class="p">{</span> <span class="nb">next</span> <span class="p">}</span><span class="sr"> </span><span class="p">/)</span><span class="o">.</span><span class="nb">Str</span><span class="p">;</span>
<span class="p">}</span>

<span class="nb">say</span> <span class="nv">@b</span><span class="p">;</span>
<span class="c1"># OUTPUT: [B D]</span>
</code></pre></div><p>在这里，如果匹配不成功，我们就跳过 <code>.push</code>，用 <code>next</code> 跳过循环体的其余部分。我们可以在 <code>regex</code> 内部发出任何控制异常。这意味着我们可以将整个过程粘在一个 <code>sub</code> 中，然后从 <code>regex</code> 中返回我们正在寻找的值。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">sub</span> <span class="nf">cherry-pick-numeric</span><span class="p">(</span><span class="nb">Str</span> <span class="nv">$matchee</span><span class="p">)</span> <span class="p">{</span>
    <span class="nv">$matchee</span> <span class="o">~~</span> <span class="k">m</span><span class="p">/(&lt;</span><span class="nb">digit</span><span class="p">&gt;)</span><span class="sr"> </span><span class="o">&amp;&amp;</span><span class="sr"> </span><span class="p">{</span> <span class="k">return</span> <span class="o">.</span><span class="nb">Numeric</span> <span class="p">}/;</span>
    <span class="nb">Empty</span>
<span class="p">}</span>

<span class="nv">@b</span> <span class="o">=</span> <span class="nb">do</span> <span class="o">.&amp;</span><span class="nf">cherry-pick-numeric</span> <span class="k">for</span> <span class="nv">@a</span><span class="p">;</span>

<span class="n">dd</span> <span class="nv">@b</span><span class="p">;</span>
<span class="c1"># OUTPUT: Array @b = [1, 3, 4]</span>
</code></pre></div><p>Raku 已经酝酿了10年。这是一个巨大的任务。现在，困难的部分来了。我们必须从那庞大的语言中找到所有好的惯用法。好东西会降临到那些等待的人身上（IRC上）。</p>
<p>*) 阅读。不要相信我写的任何东西。你已经被警告了。</p>
<p>更新一下。</p>
<p>我以真正的懒惰方式，想出了一个办法，在本该完成的工作之后，把匹配变成一个惰性列表。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="nv">$a</span> <span class="o">=</span> <span class="p">&#39;</span><span class="s1">1B3D4</span><span class="p">&#39;;</span>

<span class="k">my</span> <span class="nv">\ll</span> <span class="o">:=</span> <span class="k">gather</span> <span class="nv">$a</span> <span class="o">~~</span> <span class="k">m</span><span class="p">:</span><span class="na">g</span><span class="p">/</span><span class="sr">
</span><span class="sr">      </span><span class="p">[</span><span class="sr"> </span><span class="p">&lt;</span><span class="nb">alpha</span><span class="p">&gt;</span><span class="sr"> </span><span class="o">&amp;&amp;</span><span class="sr"> </span><span class="p">{</span> <span class="nb">take</span> <span class="nv">$/</span><span class="p">&lt;</span><span class="s">alpha</span><span class="p">&gt;</span><span class="o">.</span><span class="nb">Str</span> <span class="p">}</span><span class="sr"> </span><span class="p">]</span><span class="sr">
</span><span class="sr">    </span><span class="o">|</span><span class="sr"> </span><span class="p">[</span><span class="sr"> </span><span class="p">&lt;</span><span class="nb">digit</span><span class="p">&gt;</span><span class="sr"> </span><span class="o">&amp;&amp;</span><span class="sr"> </span><span class="p">{</span> <span class="nb">take</span> <span class="nv">$/</span><span class="o">.&lt;</span><span class="n">digit</span><span class="o">&gt;.</span><span class="nb">Numeric</span> <span class="p">}</span><span class="sr"> </span><span class="p">]</span><span class="sr">
</span><span class="sr">    </span><span class="o">|</span><span class="sr"> </span><span class="p">[</span><span class="sr"> </span><span class="p">{</span> <span class="nb">say</span> <span class="p">&#39;</span><span class="s1">step</span><span class="p">&#39;</span> <span class="p">}</span><span class="sr"> </span><span class="p">]</span><span class="sr">
</span><span class="sr"></span><span class="p">/;</span>
<span class="nb">say</span> <span class="n">ll</span><span class="o">[</span><span class="mi">0</span><span class="o">]</span><span class="p">;</span>
<span class="nb">say</span> <span class="n">ll</span><span class="o">[</span><span class="mi">3</span><span class="o">]</span><span class="p">;</span>
<span class="c1"># OUTPUT: 1</span>
          <span class="n">step</span>
          <span class="n">step</span>
          <span class="n">step</span>
          <span class="n">D</span>
</code></pre></div><p>技巧是用 <code>:g</code> 副词强制匹配一直运行到字符串的末尾。这个运行将被 <code>take</code> 打断（通过抛出 <code>CX::Take</code>），当从 <code>gather</code> 返回的 <code>Seq</code> 中询问下一个值时再继续。我不知道这是否是有效的内存思想。可能会有一个 <code>Match</code> 实例为每个 <code>take</code> 保留在身边。</p>
<p>原文链接: <a href="https://gfldex.wordpress.com/2021/03/11/raku-is-a-match-for/">https://gfldex.wordpress.com/2021/03/11/raku-is-a-match-for/</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[如果集合如我所想]]></title>
            <link href="https://ohmyweekly.github.io/notes/2021-02-07-if-sets-would-dwim/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2021-01-30-caesarean-substrings-with-raku-and-perl/?utm_source=atom_feed" rel="related" type="text/html" title="Caesarean Substrings With Raku and Perl" />
                <link href="https://ohmyweekly.github.io/notes/2020-10-04-the-strange-case-of-the-greedy-junction/?utm_source=atom_feed" rel="related" type="text/html" title="贪婪 Junction 的奇闻异事" />
                <link href="https://ohmyweekly.github.io/notes/2020-10-04-reconstructing-raku-junctions/?utm_source=atom_feed" rel="related" type="text/html" title="重构 Raku 的 Junction" />
            
                <id>https://ohmyweekly.github.io/notes/2021-02-07-if-sets-would-dwim/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2021-02-07T00:00:00+08:00</published>
            <updated>2021-02-07T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>If Sets Would DWIM</blockquote><h1 id="if-sets-would-dwimhttpsdonaldhwtf202101if-sets-would-dwim"><a href="https://donaldh.wtf/2021/01/if-sets-would-dwim/">If Sets Would DWIM</a></h1>
<p>每当我在 Raku 中使用集合的时候，它们经常无法 <a href="https://docs.raku.org/language/glossary#DWIM">DWIM</a>。这是一个简短的探索，看看是否可以改进 DWIMminess。</p>
<p>我最近重新审视了我前段时间写的一个利用 <code>(-)</code> 集差运算符的脚本。这段代码有一个 bug 潜伏在那里，显而易见，因为下面的代码并没有按照我的直觉去做。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku">  <span class="k">my</span> <span class="nv">@allowed</span> <span class="o">=</span> <span class="p">&lt;</span><span class="s">m c i p l o t</span><span class="p">&gt;;</span>
  <span class="k">my</span> <span class="nv">@chars</span> <span class="o">=</span> <span class="p">&#39;</span><span class="s1">impolitic</span><span class="p">&#39;</span><span class="o">.</span><span class="nb">comb</span><span class="p">;</span>

  <span class="k">my</span> <span class="nv">@remainder</span> <span class="o">=</span> <span class="nv">@allowed</span> <span class="ow">(-)</span> <span class="nv">@chars</span><span class="p">;</span>

  <span class="k">if</span> <span class="o">+</span><span class="nv">@remainder</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">{</span>
     <span class="nb">say</span> <span class="p">&#39;</span><span class="s1">pangram</span><span class="p">&#39;;</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
     <span class="nb">say</span> <span class="p">&#34;</span><span class="s2">unused: [</span><span class="p">{</span><span class="nv">@remainder</span><span class="o">.</span><span class="nb">join</span><span class="p">(&#39;</span><span class="s1"> </span><span class="p">&#39;)}</span><span class="s2">]</span><span class="p">&#34;;</span>
  <span class="p">}</span>
<span class="n">unused:</span> <span class="o">[]</span>
</code></pre></div><p>错误的原因是 <code>(-)</code> 产生了一个 Set，而赋值给 <code>@remainder</code> 会产生1项的 Array。总是这样。但不方便的是，当它是一个空集合时，它就会字符串化为一个空字符串，这只是帮助掩盖了这个潜伏的错误。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">my</span> <span class="nv">@items</span> <span class="o">=</span> <span class="p">&lt;</span><span class="s">a b c d e</span><span class="p">&gt;</span> <span class="ow">(-)</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">b</span> <span class="n">c</span> <span class="nb">d</span> <span class="nb">e</span><span class="o">&gt;</span><span class="p">;</span>
<span class="nb">say</span> <span class="nv">@items</span><span class="o">.</span><span class="nb">raku</span><span class="p">;</span>
<span class="nb">say</span> <span class="o">+</span><span class="nv">@items</span><span class="p">;</span>
</code></pre></div><pre><code>[Set.new()]
1
</code></pre><p>解决方法比较简单。只要不赋值给数组就可以了。使用一个标量容器来代替。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">my</span> <span class="nv">$items</span> <span class="o">=</span> <span class="p">&lt;</span><span class="s">a b c d e</span><span class="p">&gt;</span> <span class="ow">(-)</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">b</span> <span class="nb">d</span><span class="o">&gt;</span><span class="p">;</span>
<span class="nb">say</span> <span class="nv">$items</span><span class="o">.</span><span class="nb">raku</span><span class="p">;</span>
<span class="nb">say</span> <span class="o">+</span><span class="nv">$items</span><span class="p">;</span>
</code></pre></div><pre><code>Set.new(&quot;e&quot;,&quot;c&quot;)
2
</code></pre><p>甚至是关联容器也可以。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">my</span> <span class="nv">%items</span> <span class="o">=</span> <span class="p">&lt;</span><span class="s">a b c d e</span><span class="p">&gt;</span> <span class="ow">(-)</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">b</span> <span class="nb">d</span><span class="o">&gt;</span><span class="p">;</span>
<span class="nb">say</span> <span class="nv">%items</span><span class="o">.</span><span class="nb">raku</span><span class="p">;</span>
<span class="nb">say</span> <span class="o">+</span><span class="nv">%items</span><span class="p">;</span>
</code></pre></div><pre><code>{:c(Bool::True), :e(Bool::True)}
2
</code></pre><p>或在赋值前明确地取出键的列表。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">my</span> <span class="nv">@items</span> <span class="o">=</span> <span class="p">(&lt;</span><span class="s">a b c d e</span><span class="p">&gt;</span> <span class="ow">(-)</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">b</span> <span class="nb">d</span><span class="o">&gt;</span><span class="p">)</span><span class="o">.</span><span class="nb">keys</span><span class="p">;</span>
<span class="nb">say</span> <span class="nv">@items</span><span class="o">.</span><span class="nb">raku</span><span class="p">;</span>
<span class="nb">say</span> <span class="o">+</span><span class="nv">@items</span><span class="p">;</span>
</code></pre></div><pre><code>[&quot;e&quot;, &quot;c&quot;]
2
</code></pre><p>很好，起作用了。只是不要用数组容器来处理 <code>Setty</code> 这样的东西。只是这并不能阻止我的直觉时不时地碰上这个错误。同一类的 bug 在我的代码中出现过好几次，因为它实在是太容易犯错了。Raku 不会告诉我，我做错了什么，因为也许是故意的。但重要的是， Raku 没有设法 DWIM。</p>
<p>我可以采取的另一个方法是养成添加类型信息的习惯。这样确实可以让 Raku 在我掉进这个陷阱的时候告诉我。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">my</span> <span class="nb">Str</span> <span class="nv">@a</span> <span class="o">=</span> <span class="p">&lt;</span><span class="s">a b c d e</span><span class="p">&gt;</span> <span class="ow">(-)</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">b</span> <span class="nb">d</span><span class="o">&gt;</span><span class="p">;</span>
</code></pre></div><pre><code>Type check failed in assignment to @a; expected Str but got Set (Set.new(&quot;e&quot;,&quot;c&quot;))
  in sub  at EVAL_0 line 3
  in block &lt;unit&gt; at EVAL_0 line 5
  in block &lt;unit&gt; at -e line 1
</code></pre><p>这是一个明显的例子，添加类型信息有助于 Raku 编译器帮助我避免引入这种 bug。</p>
<h2 id="实验---为-set-自定义数组存储">实验 - 为 Set 自定义数组存储</h2>
<p>我开始研究核心设置(core setting)，看看可以做什么。我惊喜地发现，我可以在 <code>Array.STORE</code> 的多重分派中添加我正在寻找的语义。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">use</span> <span class="nn">MONKEY</span><span class="p">;</span>

<span class="k">augment</span> <span class="k">class</span> <span class="nb">Array</span> <span class="p">{</span>
   <span class="k">multi</span> <span class="k">method</span> <span class="nb">STORE</span><span class="p">(</span><span class="nb">Array</span><span class="p">:</span><span class="s">D</span><span class="o">:</span> <span class="nb">Set</span> \<span class="nb">item</span> <span class="k">--&gt;</span> <span class="nb">Array:D</span><span class="p">)</span> <span class="p">{</span>
       <span class="nb">self</span><span class="o">.</span><span class="nb">STORE</span><span class="p">(</span><span class="nb">item</span><span class="o">.</span><span class="nb">keys</span><span class="p">)</span>
   <span class="p">}</span>
<span class="p">}</span>

<span class="k">my</span> <span class="nv">@a</span> <span class="o">=</span> <span class="p">&lt;</span><span class="s">a b c d e</span><span class="p">&gt;</span> <span class="ow">(-)</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">b</span> <span class="nb">d</span><span class="o">&gt;</span><span class="p">;</span>
<span class="nb">say</span> <span class="nv">@a</span><span class="o">.</span><span class="nb">raku</span><span class="p">;</span>
<span class="nb">say</span> <span class="o">+</span><span class="nv">@a</span><span class="p">;</span>
</code></pre></div><pre><code>[&quot;c&quot;, &quot;e&quot;]
2
</code></pre><p>分享这个似乎是谨慎的，看看我的小 DWIM 是否有任何我没有考虑到的问题或缺点。一个可能的缺点是，如果你需要这样做的话，你需要使用 <code>,</code> 来强制将一个集合变成一个数组。</p>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="k">my</span> <span class="nv">@a</span> <span class="o">=</span> <span class="p">&lt;</span><span class="s">a b c d e</span><span class="p">&gt;</span> <span class="ow">(-)</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">b</span> <span class="nb">d</span><span class="o">&gt;</span> <span class="o">,</span> <span class="p">;</span>
<span class="nb">say</span> <span class="nv">@a</span><span class="o">.</span><span class="nb">raku</span><span class="p">;</span>
</code></pre></div><pre><code>[Set.new(&quot;e&quot;,&quot;c&quot;)]
</code></pre><h2 id="下一步是什么">下一步是什么</h2>
<p>我希望这能引发关于这个问题以及其他我们的直觉和 Raku 的行为不太一致的情况的讨论。也许还有其他相关的语言边缘可以被磨平，以消除这种危害。</p>
<h2 id="后续">后续</h2>
<p>在 <a href="https://t.co/0QSoMxrSXf?amp=1">Reddit</a> 上有一些非常有启发性的讨论，涵盖了语言语义和各种替代方法。公平地说，我建议的方法引入了更多的不一致性，而不是价值，但讨论可能会导致一个语言一致的解决方案。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/raku" term="raku" label="Raku" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/rakulang" term="rakulang" label="Rakulang" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[元编程]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-12-29-metaprogramming/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-repl-in-julia/?utm_source=atom_feed" rel="related" type="text/html" title="Julia 中的 REPL" />
            
                <id>https://ohmyweekly.github.io/notes/2020-12-29-metaprogramming/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-12-29T00:00:00+08:00</published>
            <updated>2020-12-29T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Metaprogramming</blockquote><h1 id="元编程">元编程</h1>
<p>在Julia语言中，Lisp最强的遗产是它对元编程的支持。和Lisp一样，Julia也将自己的代码表示为语言本身的数据结构。由于代码是由可以在语言内部创建和操作的对象来表示的，所以程序可以转换和生成自己的代码。这使得复杂的代码生成不需要额外的构建步骤，也允许真正的Lisp式的宏在<a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">抽象语法树</a>的层次上操作。相比之下，预处理器的 &ldquo;宏 &ldquo;系统，就像C和C++一样，在任何实际的解析或解释发生之前，都会进行文本操作和替换。由于Julia中所有的数据类型和代码都是由Julia数据结构来表示的，因此，强大的<a href="https://en.wikipedia.org/wiki/Reflection_%28computer_programming%29">反射</a>功能可以像其他数据一样探索程序及其类型的内部。</p>
<h2 id="程序表示">程序表示</h2>
<p>每个Julia程序都是以字符串的形式开始的。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">prog</span> <span class="o">=</span> <span class="s">&#34;1 + 1&#34;</span>
<span class="s">&#34;1 + 1&#34;</span>
</code></pre></div><p><strong>接下来会发生什么？</strong></p>
<p>下一步是将每个字符串<a href="https://en.wikipedia.org/wiki/Parsing#Computer_languages">解析</a>成一个对象，称为表达式，用Julia类型 <a href="https://docs.julialang.org/en/v1/base/base/#Core.Expr">Expr</a> 表示。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex1</span> <span class="o">=</span> <span class="n">Meta</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">prog</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">typeof</span><span class="p">(</span><span class="n">ex1</span><span class="p">)</span>
<span class="kt">Expr</span>
</code></pre></div><p>Expr对象包含两部分。</p>
<p>a 符号标识表达式的种类。符号是一个内嵌的字符串标识符（下文将详细讨论）。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex1</span><span class="o">.</span><span class="n">head</span>
<span class="o">:</span><span class="n">call</span>
</code></pre></div><p>表达式参数，可以是符号、其他表达式或文字值。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex1</span><span class="o">.</span><span class="n">args</span>
<span class="mi">3</span><span class="o">-</span><span class="n">element</span> <span class="kt">Array</span><span class="p">{</span><span class="kt">Any</span><span class="p">,</span><span class="mi">1</span><span class="p">}</span><span class="o">:</span>
  <span class="o">:+</span>
 <span class="mi">1</span>
 <span class="mi">1</span>
</code></pre></div><p>也可以直接用前缀符号构造表达式。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex2</span> <span class="o">=</span> <span class="kt">Expr</span><span class="p">(</span><span class="ss">:call</span><span class="p">,</span> <span class="o">:+</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>上面构造的两个表达式&ndash;通过解析和直接构造&ndash;是等价的。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex1</span> <span class="o">==</span> <span class="n">ex2</span>
<span class="nb">true</span>
</code></pre></div><p>这里的关键点是，Julia代码在内部被表示为一个数据结构，可以从语言本身访问。</p>
<p>dump函数提供了Expr对象的缩进和注释显示。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">dump</span><span class="p">(</span><span class="n">ex2</span><span class="p">)</span>
<span class="kt">Expr</span>
  <span class="n">head</span><span class="o">:</span> <span class="kt">Symbol</span> <span class="n">call</span>
  <span class="n">args</span><span class="o">:</span> <span class="kt">Array</span><span class="p">{</span><span class="kt">Any</span><span class="p">}((</span><span class="mi">3</span><span class="p">,))</span>
    <span class="mi">1</span><span class="o">:</span> <span class="kt">Symbol</span> <span class="o">+</span>
    <span class="mi">2</span><span class="o">:</span> <span class="kt">Int64</span> <span class="mi">1</span>
    <span class="mi">3</span><span class="o">:</span> <span class="kt">Int64</span> <span class="mi">1</span>
</code></pre></div><p>Expr对象也可以被嵌套。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex3</span> <span class="o">=</span> <span class="n">Meta</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s">&#34;(4 + 4) / 2&#34;</span><span class="p">)</span>
<span class="o">:</span><span class="p">((</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div><p>另一种查看表达式的方法是使用Meta.show_sexpr，它可以显示给定Expr的S表达式形式，这对于Lisp的用户来说可能看起来非常熟悉。下面是一个例子，说明如何在嵌套的Expr上显示。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">Meta</span><span class="o">.</span><span class="n">show_sexpr</span><span class="p">(</span><span class="n">ex3</span><span class="p">)</span>
<span class="p">(</span><span class="ss">:call</span><span class="p">,</span> <span class="o">:/</span><span class="p">,</span> <span class="p">(</span><span class="ss">:call</span><span class="p">,</span> <span class="o">:+</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div><p>符号</p>
<p>在Julia中，:字符有两种语法用途。第一种形式是创建一个Symbol，一个内部字符串，作为表达式的一个构件。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="ss">:foo</span>
<span class="ss">:foo</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">typeof</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span>
<span class="kt">Symbol</span>
</code></pre></div><p>符号构造函数接受任何数量的参数，并通过将它们的字符串表示连接在一起来创建一个新的符号。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="ss">:foo</span> <span class="o">==</span> <span class="kt">Symbol</span><span class="p">(</span><span class="s">&#34;foo&#34;</span><span class="p">)</span>
<span class="nb">true</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="kt">Symbol</span><span class="p">(</span><span class="s">&#34;func&#34;</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="ss">:func10</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="kt">Symbol</span><span class="p">(</span><span class="ss">:var</span><span class="p">,</span><span class="sc">&#39;_&#39;</span><span class="p">,</span><span class="s">&#34;sym&#34;</span><span class="p">)</span>
<span class="ss">:var_sym</span>
</code></pre></div><p>请注意，要使用 : 语法，符号的名称必须是一个有效的标识符。否则必须使用Symbol(str)构造函数。</p>
<p>在表达式的上下文中，符号用于指示对变量的访问；当表达式被评估时，符号会被替换为在适当的作用域中与该符号绑定的值。</p>
<p>有时，为了避免在解析时产生歧义，需要在 : 的参数周围加括号。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="o">:</span><span class="p">(</span><span class="o">:</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="o">:</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="o">:</span><span class="p">(</span><span class="o">::</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="o">::</span><span class="p">)</span>
</code></pre></div><p>表达式和评价</p>
<dl>
<dt>引用</dt>
<dd>
<p>字符的第二个语法目的是在不使用显式Expr构造函数的情况下创建表达式对象。这就是所谓的引用。在Julia代码的单条语句周围，用成对的括号跟上 : 字符，就可以根据所附的代码生成一个Expr对象。下面是用于引用一个算术表达式的简短形式的例子。</p>
</dd>
</dl>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">typeof</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="kt">Expr</span>
</code></pre></div><p>(要查看这个表达式的结构，可以试试ex.head和ex.args，或者使用上面的dump或Meta.@dump)</p>
<p>注意，可以使用Meta.parse或直接使用Expr形式构造等价的表达式。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span>      <span class="o">:</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">*</span><span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>       <span class="o">==</span>
       <span class="n">Meta</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s">&#34;a + b*c + 1&#34;</span><span class="p">)</span> <span class="o">==</span>
       <span class="kt">Expr</span><span class="p">(</span><span class="ss">:call</span><span class="p">,</span> <span class="o">:+</span><span class="p">,</span> <span class="ss">:a</span><span class="p">,</span> <span class="kt">Expr</span><span class="p">(</span><span class="ss">:call</span><span class="p">,</span> <span class="o">:*</span><span class="p">,</span> <span class="ss">:b</span><span class="p">,</span> <span class="ss">:c</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">true</span>
</code></pre></div><p>解析器提供的表达式一般只有符号、其他表达式和字面值作为其args，而Julia代码构建的表达式可以有任意的运行时值，没有字面形式作为args。在这个具体的例子中，+和a是符号，*(b,c)是一个子表达式，1是一个64位有符号整数的文字形式。</p>
<p>对于多个表达式，还有第二种引用的语法形式：用引号&hellip;&hellip;结尾括起来的代码块。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span> <span class="o">=</span> <span class="k">quote</span>
           <span class="n">x</span> <span class="o">=</span> <span class="mi">1</span>
           <span class="n">y</span> <span class="o">=</span> <span class="mi">2</span>
           <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
       <span class="k">end</span>
<span class="k">quote</span>
    <span class="cm">#= none:2 =#</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="cm">#= none:3 =#</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="cm">#= none:4 =#</span>
    <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="k">end</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">typeof</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="kt">Expr</span>
</code></pre></div><p>插值</p>
<p>用值参数直接构造Expr对象是很强大的，但与 &ldquo;正常的 &ldquo;Julia语法相比，Expr构造函数可能很乏味。作为一种替代方法，Julia允许将字元或表达式插值到引用的表达式中。插值由前缀$表示。</p>
<p>在这个例子中，变量a的值被内插了。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="o">$</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div><p>不支持向未引用的表达式插值，并会导致编译时错误。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="o">$</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">ERROR</span><span class="o">:</span> <span class="n">syntax</span><span class="o">:</span> <span class="s">&#34;$&#34;</span> <span class="n">expression</span> <span class="n">outside</span> <span class="k">quote</span>
</code></pre></div><p>在这个例子中，元组(1,2,3)作为表达式被内插到一个条件测试中。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">a</span> <span class="k">in</span> <span class="o">$:</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> <span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="n">a</span> <span class="k">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div><p>在表达式插值中使用$是有意让人联想到字符串插值和命令插值。表达式插值可以方便的、可读的程序化构造复杂的Julia表达式。</p>
<p>劈叉插值</p>
<p>请注意，$插值语法只允许在一个包围表达式中插入一个表达式。偶尔，你有一个表达式数组，需要它们全部成为包围表达式的参数。这可以用语法$(xs&hellip;)来完成。例如，下面的代码生成了一个函数调用，其中的参数数是通过编程确定的。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="ss">:x</span><span class="p">,</span> <span class="ss">:y</span><span class="p">,</span> <span class="ss">:z</span><span class="p">];</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="o">:</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">$</span><span class="p">(</span><span class="n">args</span><span class="o">...</span><span class="p">)))</span>
<span class="o">:</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">))</span>
</code></pre></div><p>嵌套引用</p>
<p>自然，引号表达式有可能包含其他引号表达式。在这些情况下，理解内插是如何工作的可能有点棘手。考虑一下这个例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="p">);</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">e</span> <span class="o">=</span> <span class="k">quote</span> <span class="k">quote</span> <span class="o">$</span><span class="n">x</span> <span class="k">end</span> <span class="k">end</span>
<span class="k">quote</span>
    <span class="cm">#= none:1 =#</span>
    <span class="o">$</span><span class="p">(</span><span class="kt">Expr</span><span class="p">(</span><span class="ss">:quote</span><span class="p">,</span> <span class="k">quote</span>
    <span class="cm">#= none:1 =#</span>
    <span class="o">$</span><span class="p">(</span><span class="kt">Expr</span><span class="p">(</span><span class="o">:$</span><span class="p">,</span> <span class="ss">:x</span><span class="p">))</span>
<span class="k">end</span><span class="p">))</span>
<span class="k">end</span>
</code></pre></div><p>注意，结果中包含$x，这意味着x还没有被评估。换句话说，$表达式 &ldquo;属于 &ldquo;内部引号表达式，因此它的参数只有在内部引号表达式时才会被评估。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="k">quote</span>
    <span class="cm">#= none:1 =#</span>
    <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span>
<span class="k">end</span>
</code></pre></div><p>但是，外引号表达式能够对内引号中的$内的值进行插值。这是用多个$来完成的。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">e</span> <span class="o">=</span> <span class="k">quote</span> <span class="k">quote</span> <span class="o">$$</span><span class="n">x</span> <span class="k">end</span> <span class="k">end</span>
<span class="k">quote</span>
    <span class="cm">#= none:1 =#</span>
    <span class="o">$</span><span class="p">(</span><span class="kt">Expr</span><span class="p">(</span><span class="ss">:quote</span><span class="p">,</span> <span class="k">quote</span>
    <span class="cm">#= none:1 =#</span>
    <span class="o">$</span><span class="p">(</span><span class="kt">Expr</span><span class="p">(</span><span class="o">:$</span><span class="p">,</span> <span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)))</span>
<span class="k">end</span><span class="p">))</span>
<span class="k">end</span>
</code></pre></div><p>注意到(1+2)现在出现在结果中，而不是符号x。对这个表达式进行评估，得到一个内插的3。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="k">quote</span>
    <span class="cm">#= none:1 =#</span>
    <span class="mi">3</span>
<span class="k">end</span>
</code></pre></div><p>这种行为背后的直觉是，x对每个$都会被评估一次：一个$的工作原理类似于eval(:x)，给出x的值，而两个$的工作原理相当于eval(eval(:x))。</p>
<p>QuoteNode</p>
<p>引号形式在AST中的通常表示是一个带头:quote的Expr。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">dump</span><span class="p">(</span><span class="n">Meta</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s">&#34;:(1+2)&#34;</span><span class="p">))</span>
<span class="kt">Expr</span>
  <span class="n">head</span><span class="o">:</span> <span class="kt">Symbol</span> <span class="k">quote</span>
  <span class="n">args</span><span class="o">:</span> <span class="kt">Array</span><span class="p">{</span><span class="kt">Any</span><span class="p">}((</span><span class="mi">1</span><span class="p">,))</span>
    <span class="mi">1</span><span class="o">:</span> <span class="kt">Expr</span>
      <span class="n">head</span><span class="o">:</span> <span class="kt">Symbol</span> <span class="n">call</span>
      <span class="n">args</span><span class="o">:</span> <span class="kt">Array</span><span class="p">{</span><span class="kt">Any</span><span class="p">}((</span><span class="mi">3</span><span class="p">,))</span>
        <span class="mi">1</span><span class="o">:</span> <span class="kt">Symbol</span> <span class="o">+</span>
        <span class="mi">2</span><span class="o">:</span> <span class="kt">Int64</span> <span class="mi">1</span>
        <span class="mi">3</span><span class="o">:</span> <span class="kt">Int64</span> <span class="mi">2</span>
</code></pre></div><p>正如我们所看到的，这类表达式支持用$进行插值，但是在某些情况下，有必要在不进行插值的情况下引用代码。这种引用还没有语法，但在内部表示为一个类型为QuoteNode的对象。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">Meta</span><span class="o">.</span><span class="n">quot</span><span class="p">(</span><span class="kt">Expr</span><span class="p">(</span><span class="o">:$</span><span class="p">,</span> <span class="o">:</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">2</span><span class="p">))))</span>
<span class="mi">3</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="kt">QuoteNode</span><span class="p">(</span><span class="kt">Expr</span><span class="p">(</span><span class="o">:$</span><span class="p">,</span> <span class="o">:</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">2</span><span class="p">))))</span>
<span class="o">:</span><span class="p">(</span><span class="o">$</span><span class="p">(</span><span class="kt">Expr</span><span class="p">(</span><span class="o">:$</span><span class="p">,</span> <span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="p">))))</span>
</code></pre></div><p>该解析器产生的QuoteNodes用于简单的引用项目，如符号。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">dump</span><span class="p">(</span><span class="n">Meta</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s">&#34;:x&#34;</span><span class="p">))</span>
<span class="kt">QuoteNode</span>
  <span class="n">value</span><span class="o">:</span> <span class="kt">Symbol</span> <span class="n">x</span>
</code></pre></div><p>QuoteNode还可以用于某些高级元编程任务。</p>
<p>评估表达式</p>
<p>给定一个表达式对象，可以使用eval使Julia在全局范围内对其进行评估（执行）。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span>
<span class="mi">3</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="n">ERROR</span><span class="o">:</span> <span class="kt">UndefVarError</span><span class="o">:</span> <span class="n">b</span> <span class="n">not</span> <span class="n">defined</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="mi">3</span>
</code></pre></div><p>每个模块都有自己的 eval 函数，它可以在全局范围内评估表达式。传递给eval的表达式并不局限于返回值&ndash;它们也可以产生副作用，改变模块环境的状态。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">x</span>
<span class="n">ERROR</span><span class="o">:</span> <span class="kt">UndefVarError</span><span class="o">:</span> <span class="n">x</span> <span class="n">not</span> <span class="n">defined</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="mi">1</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">x</span>
<span class="mi">1</span>
</code></pre></div><p>在这里，对表达式对象的评价会导致一个值被分配给全局变量x。</p>
<p>由于表达式只是Expr对象，可以通过编程构造，然后进行评估，因此可以动态生成任意代码，然后使用eval进行运行。下面是一个简单的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span> <span class="o">=</span> <span class="kt">Expr</span><span class="p">(</span><span class="ss">:call</span><span class="p">,</span> <span class="o">:+</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="ss">:b</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="mi">3</span>
</code></pre></div><p>a的值用于构造表达式ex，该表达式将+函数应用于值1和变量b，注意a和b的使用方式的重要区别。</p>
<p>变量a的值在表达式构造时被用作表达式中的即时值。因此，当表达式被评估时，a的值不再重要：表达式中的值已经是1，无论a的值是多少，都是独立的。
另一方面，在表达式的构造中使用了符号:b，所以变量b的值在那个时候是无关紧要的&ndash;:b只是一个符号，变量b甚至不需要被定义。但在表达式评估时，符号:b的值是通过查找变量b的值来解决的。
表达式上的函数</p>
<p>如上所述，Julia的一个极其有用的特性是在Julia本身内部生成和操作Julia代码的能力。我们已经看到了一个函数返回Expr对象的例子：parse函数，它接收一串Julia代码并返回相应的Expr。一个函数也可以接受一个或多个Expr对象作为参数，并返回另一个Expr。下面是一个简单的、激励性的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">function</span> <span class="n">math_expr</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">op1</span><span class="p">,</span> <span class="n">op2</span><span class="p">)</span>
           <span class="n">expr</span> <span class="o">=</span> <span class="kt">Expr</span><span class="p">(</span><span class="ss">:call</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">op1</span><span class="p">,</span> <span class="n">op2</span><span class="p">)</span>
           <span class="k">return</span> <span class="n">expr</span>
       <span class="k">end</span>
<span class="n">math_expr</span> <span class="p">(</span><span class="n">generic</span> <span class="k">function</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span>  <span class="n">ex</span> <span class="o">=</span> <span class="n">math_expr</span><span class="p">(</span><span class="o">:+</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kt">Expr</span><span class="p">(</span><span class="ss">:call</span><span class="p">,</span> <span class="o">:*</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="mi">21</span>
</code></pre></div><p>作为另一个例子，这里有一个函数，它可以将任何数字参数翻倍，但不考虑表达式。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">function</span> <span class="n">make_expr2</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">opr1</span><span class="p">,</span> <span class="n">opr2</span><span class="p">)</span>
           <span class="n">opr1f</span><span class="p">,</span> <span class="n">opr2f</span> <span class="o">=</span> <span class="n">map</span><span class="p">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="k">isa</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="kt">Number</span><span class="p">)</span> <span class="o">?</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">:</span> <span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">opr1</span><span class="p">,</span> <span class="n">opr2</span><span class="p">))</span>
           <span class="n">retexpr</span> <span class="o">=</span> <span class="kt">Expr</span><span class="p">(</span><span class="ss">:call</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">opr1f</span><span class="p">,</span> <span class="n">opr2f</span><span class="p">)</span>
           <span class="k">return</span> <span class="n">retexpr</span>
       <span class="k">end</span>
<span class="n">make_expr2</span> <span class="p">(</span><span class="n">generic</span> <span class="k">function</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">make_expr2</span><span class="p">(</span><span class="o">:+</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span> <span class="o">=</span> <span class="n">make_expr2</span><span class="p">(</span><span class="o">:+</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kt">Expr</span><span class="p">(</span><span class="ss">:call</span><span class="p">,</span> <span class="o">:*</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="o">:</span><span class="p">(</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">8</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="mi">42</span>
</code></pre></div><h2 id="宏">宏</h2>
<p>宏提供了一种将生成的代码包含在程序的最后主体中的方法。宏将一个参数元组映射到一个返回的表达式，生成的表达式直接被编译，而不需要运行时 <a href="https://docs.julialang.org/en/v1/base/base/#Base.MainInclude.eval">eval</a> 调用。宏参数可以包括表达式、字面值和符号。</p>
<h3 id="基础知识">基础知识</h3>
<p>这里有一个特别简单的宏。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">sayhello</span><span class="p">()</span>
           <span class="k">return</span> <span class="o">:</span><span class="p">(</span> <span class="n">println</span><span class="p">(</span><span class="s">&#34;Hello, world!&#34;</span><span class="p">)</span> <span class="p">)</span>
       <span class="k">end</span>
<span class="nd">@sayhello</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>
</code></pre></div><p>在 Julia 的语法中，宏有一个专门的字符：<code>@</code>(at 符号)，后面是 <code>macro NAME ... end</code> 块中声明的唯一名称&hellip;.。在这个例子中，编译器将用 <code>@sayhello</code> 替换所有的实例。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="o">:</span><span class="p">(</span> <span class="n">println</span><span class="p">(</span><span class="s">&#34;Hello, world!&#34;</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div><p>当在 REPL 中输入 <code>@sayhello</code> 时，表达式会立即执行，因此我们只看到求值结果。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@sayhello</span><span class="p">()</span>
<span class="n">Hello</span><span class="p">,</span> <span class="n">world!</span>
</code></pre></div><p>现在，考虑一个稍微复杂的宏。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">sayhello</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
           <span class="k">return</span> <span class="o">:</span><span class="p">(</span> <span class="n">println</span><span class="p">(</span><span class="s">&#34;Hello, &#34;</span><span class="p">,</span> <span class="o">$</span><span class="n">name</span><span class="p">)</span> <span class="p">)</span>
       <span class="k">end</span>
<span class="nd">@sayhello</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>
</code></pre></div><p>这个宏只取一个参数：<code>name</code>。当遇到 <code>@sayhello</code> 时，引用的表达式会被展开，将参数的值内插到最终的表达式中。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@sayhello</span><span class="p">(</span><span class="s">&#34;human&#34;</span><span class="p">)</span>
<span class="n">Hello</span><span class="p">,</span> <span class="n">human</span>
</code></pre></div><p>我们可以使用函数 <a href="https://docs.julialang.org/en/v1/base/base/#Base.macroexpand">macroexpand</a> 查看引用的返回表达式（重要提示：这是调试宏的一个极其有用的工具）。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span> <span class="o">=</span> <span class="n">macroexpand</span><span class="p">(</span><span class="n">Main</span><span class="p">,</span> <span class="o">:</span><span class="p">(</span><span class="nd">@sayhello</span><span class="p">(</span><span class="s">&#34;human&#34;</span><span class="p">))</span> <span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="n">Main</span><span class="o">.</span><span class="n">println</span><span class="p">(</span><span class="s">&#34;Hello, &#34;</span><span class="p">,</span> <span class="s">&#34;human&#34;</span><span class="p">))</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">typeof</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="kt">Expr</span>
</code></pre></div><p>我们可以看到，&ldquo;human&rdquo; 的字面值已经被插进了表达式中。</p>
<p>此外，还存在一个宏 <a href="https://docs.julialang.org/en/v1/base/base/#Base.@macroexpand">@macroexpand</a>，也许比 <code>macroexpand</code> 函数更方便一些。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@macroexpand</span> <span class="nd">@sayhello</span> <span class="s">&#34;human&#34;</span>
<span class="o">:</span><span class="p">(</span><span class="n">println</span><span class="p">(</span><span class="s">&#34;Hello, &#34;</span><span class="p">,</span> <span class="s">&#34;human&#34;</span><span class="p">))</span>
</code></pre></div><h3 id="等等为什么是宏">等等：为什么是宏？</h3>
<p>我们在前一节已经看到了一个函数 <code>f(::Expr...) -&gt; Expr</code>。其实，<a href="https://docs.julialang.org/en/v1/base/base/#Base.macroexpand">macroexpand</a> 也是这样一个函数。那么，为什么要有宏的存在呢？</p>
<p>宏是必要的，因为它们在代码解析时执行，因此，宏允许程序员在完整程序运行之前生成并包含自定义代码的片段。为了说明两者的区别，请考虑下面的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">twostep</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
           <span class="n">println</span><span class="p">(</span><span class="s">&#34;I execute at parse time. The argument is: &#34;</span><span class="p">,</span> <span class="n">arg</span><span class="p">)</span>
           <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="n">println</span><span class="p">(</span><span class="s">&#34;I execute at runtime. The argument is: &#34;</span><span class="p">,</span> <span class="o">$</span><span class="n">arg</span><span class="p">))</span>
       <span class="k">end</span>
<span class="nd">@twostep</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span> <span class="o">=</span> <span class="n">macroexpand</span><span class="p">(</span><span class="n">Main</span><span class="p">,</span> <span class="o">:</span><span class="p">(</span><span class="nd">@twostep</span> <span class="o">:</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="p">);</span>
<span class="n">I</span> <span class="n">execute</span> <span class="n">at</span> <span class="n">parse</span> <span class="n">time</span><span class="o">.</span> <span class="n">The</span> <span class="n">argument</span> <span class="n">is</span><span class="o">:</span> <span class="o">:</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div><p>当调用 <a href="https://docs.julialang.org/en/v1/base/base/#Base.macroexpand">macroexpand</a> 时，会执行对 <a href="https://docs.julialang.org/en/v1/base/io-network/#Base.println">println</a> 的第一次调用。结果的表达式只包含第二个 <code>println</code>。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">typeof</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="kt">Expr</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">ex</span>
<span class="o">:</span><span class="p">(</span><span class="n">println</span><span class="p">(</span><span class="s">&#34;I execute at runtime. The argument is: &#34;</span><span class="p">,</span> <span class="o">$</span><span class="p">(</span><span class="kt">Expr</span><span class="p">(</span><span class="ss">:copyast</span><span class="p">,</span> <span class="o">:</span><span class="p">(</span><span class="o">$</span><span class="p">(</span><span class="kt">QuoteNode</span><span class="p">(</span><span class="o">:</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))))))))</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">eval</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
<span class="n">I</span> <span class="n">execute</span> <span class="n">at</span> <span class="n">runtime</span><span class="o">.</span> <span class="n">The</span> <span class="n">argument</span> <span class="n">is</span><span class="o">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div><h3 id="宏调用">宏调用</h3>
<p>宏的调用有以下一般语法。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="nd">@name</span> <span class="n">expr1</span> <span class="n">expr2</span> <span class="o">...</span>
<span class="nd">@name</span><span class="p">(</span><span class="n">expr1</span><span class="p">,</span> <span class="n">expr2</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</code></pre></div><p>注意在宏名前有区别的 <code>@</code>，第一种形式的参数表达式之间没有逗号，第二种形式的 <code>@</code> 名后没有空格。两种样式不能混用。例如，下面的语法与上面的例子不同，它将元组 (expr1, expr2, &hellip;) 作为一个参数传递给宏。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="nd">@name</span> <span class="p">(</span><span class="n">expr1</span><span class="p">,</span> <span class="n">expr2</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</code></pre></div><p>在数组字面量（或解析）上调用宏的另一种方法是将两者并列，而不使用括号。在这种情况下，数组将是唯一输入宏的表达式。下面的语法是等价的（与 <code>@name [a b] * v</code> 不同）。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="nd">@name</span><span class="p">[</span><span class="n">a</span> <span class="n">b</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span>
<span class="nd">@name</span><span class="p">([</span><span class="n">a</span> <span class="n">b</span><span class="p">])</span> <span class="o">*</span> <span class="n">v</span>
</code></pre></div><p>需要强调的是，宏以表达式、字面值或符号的形式接收其参数。探索宏参数的一种方法是在宏体中调用 <a href="https://docs.julialang.org/en/v1/base/io-network/#Base.show-Tuple%7BIO,Any%7D">show</a> 函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">showarg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
           <span class="n">show</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
           <span class="c"># ... remainder of macro, returning an expression</span>
       <span class="k">end</span>
<span class="nd">@showarg</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@showarg</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="ss">:a</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@showarg</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="o">:</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@showarg</span><span class="p">(</span><span class="n">println</span><span class="p">(</span><span class="s">&#34;Yo!&#34;</span><span class="p">))</span>
<span class="o">:</span><span class="p">(</span><span class="n">println</span><span class="p">(</span><span class="s">&#34;Yo!&#34;</span><span class="p">))</span>
</code></pre></div><p>除了给定的参数列表之外，每个宏都会被传递额外的参数 <code>__source__</code> 和 <code>__module__</code>。</p>
<p>参数 <code>__source__</code> 提供了关于来自宏调用的 <code>@</code> 符号的解析器位置的信息（以 <code>LineNumberNode</code> 对象的形式）。这使得宏能够包含更好的错误诊断信息，并且通常被日志、字符串解析器宏和文档等使用，例如，也被用来实现 <a href="https://docs.julialang.org/en/v1/base/file/#Base.@__LINE__">@<strong>LINE</strong></a>、<a href="https://docs.julialang.org/en/v1/base/file/#Base.@__FILE__">@<strong>FILE</strong></a> 和 <a href="https://docs.julialang.org/en/v1/base/file/#Base.@__DIR__">@<strong>DIR</strong></a> 宏。</p>
<p>位置信息可以通过引用 <code>__source__.line</code> 和 <code>__source__.file</code> 来访问。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">__LOCATION__</span><span class="p">();</span> <span class="k">return</span> <span class="kt">QuoteNode</span><span class="p">(</span><span class="n">__source__</span><span class="p">);</span> <span class="k">end</span>
<span class="nd">@__LOCATION__</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">dump</span><span class="p">(</span>
            <span class="nd">@__LOCATION__</span><span class="p">(</span>
       <span class="p">))</span>
<span class="kt">LineNumberNode</span>
  <span class="n">line</span><span class="o">:</span> <span class="kt">Int64</span> <span class="mi">2</span>
  <span class="n">file</span><span class="o">:</span> <span class="kt">Symbol</span> <span class="n">none</span>
</code></pre></div><p>参数 <code>__module__</code> 提供了关于宏调用的扩展上下文的信息（以 <code>Module</code> 对象的形式）。这允许宏查找上下文信息，如现有的绑定，或者将该值作为额外的参数插入到在当前模块中做自省的运行时函数调用中。</p>
<h3 id="构建一个高级宏">构建一个高级宏</h3>
<p>这里是 Julia 的 <a href="https://docs.julialang.org/en/v1/base/base/#Base.@assert">@assert</a> 宏的简化定义。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">assert</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
           <span class="k">return</span> <span class="o">:</span><span class="p">(</span> <span class="o">$</span><span class="n">ex</span> <span class="o">?</span> <span class="nb">nothing</span> <span class="o">:</span> <span class="n">throw</span><span class="p">(</span><span class="kt">AssertionError</span><span class="p">(</span><span class="o">$</span><span class="p">(</span><span class="n">string</span><span class="p">(</span><span class="n">ex</span><span class="p">))))</span> <span class="p">)</span>
       <span class="k">end</span>
<span class="nd">@assert</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>
</code></pre></div><p>这个宏可以这样使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@assert</span> <span class="mi">1</span> <span class="o">==</span> <span class="mf">1.0</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@assert</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">0</span>
<span class="n">ERROR</span><span class="o">:</span> <span class="kt">AssertionError</span><span class="o">:</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div><p>宏调用在解析时扩展到它的返回结果。这就相当于写。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="mi">1</span> <span class="o">==</span> <span class="mf">1.0</span> <span class="o">?</span> <span class="nb">nothing</span> <span class="o">:</span> <span class="n">throw</span><span class="p">(</span><span class="kt">AssertionError</span><span class="p">(</span><span class="s">&#34;1 == 1.0&#34;</span><span class="p">))</span>
<span class="mi">1</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">?</span> <span class="nb">nothing</span> <span class="o">:</span> <span class="n">throw</span><span class="p">(</span><span class="kt">AssertionError</span><span class="p">(</span><span class="s">&#34;1 == 0&#34;</span><span class="p">))</span>
</code></pre></div><p>也就是说，在第一次调用中，表达式 <code>:(1 == 1.0)</code> 被拼接到测试条件槽中，而 <code>string(:(1 == 1.0))</code> 的值被拼接到断言消息槽中。这样构造出来的整个表达式，就被放到了发生 <code>@assert</code> 宏调用的语法树中。然后在执行时，如果测试表达式评估为真，那么返回 <a href="https://docs.julialang.org/en/v1/base/constants/#Core.nothing">nothing</a>，而如果测试为假，则会引发一个错误，表明断言表达式是假的。注意，如果把这个写成函数就不行了，因为只有条件的值，不可能在错误信息中显示计算条件的表达式。</p>
<p>Julia Base 中 <code>@assert</code> 的实际定义比较复杂。它允许用户有选择地指定自己的错误信息，而不是只打印失败的表达式。就像在参数数可变的函数（<a href="https://docs.julialang.org/en/v1/manual/functions/#Varargs-Functions">Varargs Functions</a>）中一样，在最后一个参数后面用省略号来指定。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">assert</span><span class="p">(</span><span class="n">ex</span><span class="p">,</span> <span class="n">msgs</span><span class="o">...</span><span class="p">)</span>
           <span class="n">msg_body</span> <span class="o">=</span> <span class="n">isempty</span><span class="p">(</span><span class="n">msgs</span><span class="p">)</span> <span class="o">?</span> <span class="n">ex</span> <span class="o">:</span> <span class="n">msgs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
           <span class="n">msg</span> <span class="o">=</span> <span class="n">string</span><span class="p">(</span><span class="n">msg_body</span><span class="p">)</span>
           <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="o">$</span><span class="n">ex</span> <span class="o">?</span> <span class="nb">nothing</span> <span class="o">:</span> <span class="n">throw</span><span class="p">(</span><span class="kt">AssertionError</span><span class="p">(</span><span class="o">$</span><span class="n">msg</span><span class="p">)))</span>
       <span class="k">end</span>
<span class="nd">@assert</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>
</code></pre></div><p>现在 <code>@assert</code> 有两种操作模式，取决于它接收到的参数数量！如果只有一个参数，那么 <code>msgs</code> 捕获的表达式元组将是空的，它的行为和上面的简单定义一样。如果只有一个参数，msgs 捕获的表达式元组将是空的，它的行为与上面的简单定义相同。但现在如果用户指定了第二个参数，它将被打印在消息正文中，而不是失败的表达式。你可以用 <a href="https://docs.julialang.org/en/v1/base/base/#Base.@macroexpand">@macroexpand</a> 宏来检查宏扩展的结果。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@macroexpand</span> <span class="nd">@assert</span> <span class="n">a</span> <span class="o">==</span> <span class="n">b</span>
<span class="o">:</span><span class="p">(</span><span class="k">if</span> <span class="n">Main</span><span class="o">.</span><span class="n">a</span> <span class="o">==</span> <span class="n">Main</span><span class="o">.</span><span class="n">b</span>
        <span class="n">Main</span><span class="o">.</span><span class="nb">nothing</span>
    <span class="k">else</span>
        <span class="n">Main</span><span class="o">.</span><span class="n">throw</span><span class="p">(</span><span class="n">Main</span><span class="o">.</span><span class="kt">AssertionError</span><span class="p">(</span><span class="s">&#34;a == b&#34;</span><span class="p">))</span>
    <span class="k">end</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@macroexpand</span> <span class="nd">@assert</span> <span class="n">a</span><span class="o">==</span><span class="n">b</span> <span class="s">&#34;a should equal b!&#34;</span>
<span class="o">:</span><span class="p">(</span><span class="k">if</span> <span class="n">Main</span><span class="o">.</span><span class="n">a</span> <span class="o">==</span> <span class="n">Main</span><span class="o">.</span><span class="n">b</span>
        <span class="n">Main</span><span class="o">.</span><span class="nb">nothing</span>
    <span class="k">else</span>
        <span class="n">Main</span><span class="o">.</span><span class="n">throw</span><span class="p">(</span><span class="n">Main</span><span class="o">.</span><span class="kt">AssertionError</span><span class="p">(</span><span class="s">&#34;a should equal b!&#34;</span><span class="p">))</span>
    <span class="k">end</span><span class="p">)</span>
</code></pre></div><p>实际的 <code>@assert</code> 宏还可以处理另一种情况：如果除了打印 &ldquo;a should equal b&rdquo; 之外，我们还想打印它们的值呢？人们可能会天真地尝试在自定义消息中使用字符串插值，例如，<code>@assert a==b &quot;a ($a) should equal b ($b)!&quot;</code>，但这在上面的宏中不会像预期的那样工作。你能明白为什么吗？从<a href="https://docs.julialang.org/en/v1/manual/strings/#string-interpolation">字符串插值</a>回想一下，插值后的字符串会被改写成对<a href="https://docs.julialang.org/en/v1/base/strings/#Base.string">字符串</a>的调用。比较一下。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">typeof</span><span class="p">(</span><span class="o">:</span><span class="p">(</span><span class="s">&#34;a should equal b&#34;</span><span class="p">))</span>
<span class="kt">String</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">typeof</span><span class="p">(</span><span class="o">:</span><span class="p">(</span><span class="s">&#34;a (</span><span class="si">$a</span><span class="s">) should equal b (</span><span class="si">$b</span><span class="s">)!&#34;</span><span class="p">))</span>
<span class="kt">Expr</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">dump</span><span class="p">(</span><span class="o">:</span><span class="p">(</span><span class="s">&#34;a (</span><span class="si">$a</span><span class="s">) should equal b (</span><span class="si">$b</span><span class="s">)!&#34;</span><span class="p">))</span>
<span class="kt">Expr</span>
  <span class="n">head</span><span class="o">:</span> <span class="kt">Symbol</span> <span class="n">string</span>
  <span class="n">args</span><span class="o">:</span> <span class="kt">Array</span><span class="p">{</span><span class="kt">Any</span><span class="p">}((</span><span class="mi">5</span><span class="p">,))</span>
    <span class="mi">1</span><span class="o">:</span> <span class="kt">String</span> <span class="s">&#34;a (&#34;</span>
    <span class="mi">2</span><span class="o">:</span> <span class="kt">Symbol</span> <span class="n">a</span>
    <span class="mi">3</span><span class="o">:</span> <span class="kt">String</span> <span class="s">&#34;) should equal b (&#34;</span>
    <span class="mi">4</span><span class="o">:</span> <span class="kt">Symbol</span> <span class="n">b</span>
    <span class="mi">5</span><span class="o">:</span> <span class="kt">String</span> <span class="s">&#34;)!&#34;</span>
</code></pre></div><p>因此，现在宏不是在 <code>msg_body</code> 中得到一个普通的字符串，而是接收一个完整的表达式，这个表达式需要被评估，以便按照预期的方式显示。这可以直接拼接到返回的表达式中，作为<a href="https://docs.julialang.org/en/v1/base/strings/#Base.string">字符串</a>调用的一个参数；完整的实现请参见 <a href="https://github.com/JuliaLang/julia/blob/master/base/error.jl">error.jl</a>。</p>
<p><code>@assert</code> 宏很好地利用了拼接成引号的表达式，简化了宏体内部对表达式的操作。</p>
<h3 id="卫生宏">卫生宏</h3>
<p>在比较复杂的宏中会出现一个问题，那就是<a href="https://en.wikipedia.org/wiki/Hygienic_macro">卫生</a>问题。简而言之，宏必须确保它们在返回的表达式中引入的变量不会意外地与它们扩展到的周围代码中的现有变量发生冲突。相反，作为参数传入宏中的表达式往往要在周围代码的上下文中进行评估，与现有变量进行交互和修改。另一个关注点来自于一个事实，即一个宏可能会在与它被定义的模块不同的地方被调用。在这种情况下，我们需要确保所有的全局变量被解析到正确的模块中。与具有文本宏扩展的语言（如C语言）相比，Julia 已经有了很大的优势，因为它只需要考虑返回的表达式。所有其他的变量（比如上面 <code>@assert</code> 中的 <code>msg</code>）都遵循<a href="https://docs.julialang.org/en/v1/manual/variables-and-scoping/#scope-of-variables">正常的作用域块行为</a>。</p>
<p>为了证明这些问题，让我们考虑写一个 <code>@time</code> 宏，它接受一个表达式作为参数，记录时间，评估表达式，再次记录时间，打印前后时间的差值，然后以表达式的值作为其最终值。这个宏可能是这样的。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">macro</span> <span class="n">time</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
    <span class="k">return</span> <span class="k">quote</span>
        <span class="k">local</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">time_ns</span><span class="p">()</span>
        <span class="k">local</span> <span class="n">val</span> <span class="o">=</span> <span class="o">$</span><span class="n">ex</span>
        <span class="k">local</span> <span class="n">t1</span> <span class="o">=</span> <span class="n">time_ns</span><span class="p">()</span>
        <span class="n">println</span><span class="p">(</span><span class="s">&#34;elapsed time: &#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">t1</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span><span class="o">/</span><span class="mf">1e9</span><span class="p">,</span> <span class="s">&#34; seconds&#34;</span><span class="p">)</span>
        <span class="n">val</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div><p>在这里，我们希望 <code>t0</code>、<code>t1</code> 和 <code>val</code> 是私有的临时变量，我们希望 <code>time</code> 引用 Julia Base 中的 <a href="https://docs.julialang.org/en/v1/base/base/#Base.Libc.time-Tuple%7B%7D">time</a> 函数，而不是用户可能拥有的任何 <code>time</code> 变量（同样适用于 <code>println</code>）。想象一下，如果用户表达式 <code>ex</code> 也包含了对一个叫 <code>t0</code> 的变量的赋值，或者定义了自己的 <code>time</code> 变量，可能会出现的问题。我们可能会得到错误，或者神秘的不正确行为。</p>
<p>Julia 的宏扩展器用以下方式解决了这些问题。首先，宏结果中的变量被分为局部或全局。如果一个变量被分配给（而不是声明为全局）、声明为局部，或者被用作函数参数名，那么它被认为是局部的。否则，它被认为是全局变量。局部变量就会被重命名为唯一的（使用 <a href="https://docs.julialang.org/en/v1/base/base/#Base.gensym">gensym</a> 函数，生成新的符号），全局变量则在宏定义环境中解决。因此，上述两个问题都得到了处理；宏的局部变量不会与任何用户变量冲突，<code>time</code> 和 <code>println</code> 将引用 Julia Base 定义。</p>
<p>然而，仍然存在一个问题。考虑下面这个宏的使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">module</span> <span class="n">MyModule</span>
<span class="k">import</span> <span class="n">Base</span><span class="o">.</span><span class="nd">@time</span>

<span class="n">time</span><span class="p">()</span> <span class="o">=</span> <span class="o">...</span> <span class="c"># compute something</span>

<span class="nd">@time</span> <span class="n">time</span><span class="p">()</span>
<span class="k">end</span>
</code></pre></div><p>这里的用户表达式 <code>ex</code> 是对 <code>time</code> 的调用，但不是宏使用的那个 <code>time</code> 函数。它显然是指 <code>MyModule.time</code>。因此我们必须安排 <code>ex</code> 中的代码在宏调用环境中进行解析。这可以通过用 <a href="https://docs.julialang.org/en/v1/base/base/#Base.esc">esc</a> 对表达式进行&quot;转义&quot;来实现。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">macro</span> <span class="n">time</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="k">local</span> <span class="n">val</span> <span class="o">=</span> <span class="o">$</span><span class="p">(</span><span class="n">esc</span><span class="p">(</span><span class="n">ex</span><span class="p">))</span>
    <span class="o">...</span>
<span class="k">end</span>
</code></pre></div><p>以这种方式包装的表达式，宏扩展器不会管它，只需逐字粘贴到输出中即可。因此它将在宏调用环境中被解析。</p>
<p>在必要的时候，可以利用这种转义机制来&quot;违反&quot;卫生，以便引入或操作用户变量。例如，下面的宏在调用环境中将 <code>x</code> 设为零。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">zerox</span><span class="p">()</span>
           <span class="k">return</span> <span class="n">esc</span><span class="p">(</span><span class="o">:</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>
       <span class="k">end</span>
<span class="nd">@zerox</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="k">function</span> <span class="n">foo</span><span class="p">()</span>
           <span class="n">x</span> <span class="o">=</span> <span class="mi">1</span>
           <span class="nd">@zerox</span>
           <span class="k">return</span> <span class="n">x</span> <span class="c"># is zero</span>
       <span class="k">end</span>
<span class="n">foo</span> <span class="p">(</span><span class="n">generic</span> <span class="k">function</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">foo</span><span class="p">()</span>
<span class="mi">0</span>
</code></pre></div><p>这种对变量的操作应谨慎使用，但偶尔也很方便。</p>
<p>掌握正确的卫生规则可能是一个艰巨的挑战。在使用宏之前，你可能需要考虑一个函数闭包是否足够。另一个有用的策略是将尽可能多的工作推迟到运行时。例如，许多宏简单地将其参数包裹在 QuoteNode 或其他类似的 <a href="https://docs.julialang.org/en/v1/base/base/#Core.Expr">Expr</a> 中。一些例子包括 <code>@task body</code>，它简单地返回 <code>schedule(Task(()-&gt; $body))</code>，以及 <code>@eval expr</code>，它简单地返回 <code>eval(QuoteNode(expr))</code>。</p>
<p>为了演示，我们可以将上面的 <code>@time</code> 例子重写为。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">macro</span> <span class="n">time</span><span class="p">(</span><span class="n">expr</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="n">timeit</span><span class="p">(()</span> <span class="o">-&gt;</span> <span class="o">$</span><span class="p">(</span><span class="n">esc</span><span class="p">(</span><span class="n">expr</span><span class="p">))))</span>
<span class="k">end</span>
<span class="k">function</span> <span class="n">timeit</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time_ns</span><span class="p">()</span>
    <span class="n">val</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">time_ns</span><span class="p">()</span>
    <span class="n">println</span><span class="p">(</span><span class="s">&#34;elapsed time: &#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">t1</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span><span class="o">/</span><span class="mf">1e9</span><span class="p">,</span> <span class="s">&#34; seconds&#34;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">val</span>
<span class="k">end</span>
</code></pre></div><p>然而，我们不这样做是有充分的理由的：将 <code>expr</code> 包装在一个新的作用域块中（匿名函数）也会稍微改变表达式的含义（其中任何变量的作用域），同时我们希望 <code>@time</code> 可以使用，而对被包装的代码影响最小。</p>
<h3 id="宏和调度">宏和调度</h3>
<p>宏，就像 Julia 函数一样，是通用的。这意味着它们也可以有多个方法定义，这要归功于多重分派。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">m</span> <span class="k">end</span>
<span class="nd">@m</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">0</span> <span class="n">methods</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">m</span><span class="p">(</span><span class="n">args</span><span class="o">...</span><span class="p">)</span>
           <span class="n">println</span><span class="p">(</span><span class="s">&#34;</span><span class="si">$</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="n">args</span><span class="p">))</span><span class="s"> arguments&#34;</span><span class="p">)</span>
       <span class="k">end</span>
<span class="nd">@m</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
           <span class="n">println</span><span class="p">(</span><span class="s">&#34;Two arguments&#34;</span><span class="p">)</span>
       <span class="k">end</span>
<span class="nd">@m</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">2</span> <span class="n">methods</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@m</span> <span class="s">&#34;asd&#34;</span>
<span class="mi">1</span> <span class="n">arguments</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@m</span> <span class="mi">1</span> <span class="mi">2</span>
<span class="n">Two</span> <span class="n">arguments</span>
</code></pre></div><p>然而我们应该记住，宏调度是基于交给宏的 AST 类型，而不是 AST 在运行时评估的类型。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">macro</span> <span class="n">m</span><span class="p">(</span><span class="o">::</span><span class="kt">Int</span><span class="p">)</span>
           <span class="n">println</span><span class="p">(</span><span class="s">&#34;An Integer&#34;</span><span class="p">)</span>
       <span class="k">end</span>
<span class="nd">@m</span> <span class="p">(</span><span class="k">macro</span> <span class="n">with</span> <span class="mi">3</span> <span class="n">methods</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@m</span> <span class="mi">2</span>
<span class="n">An</span> <span class="kt">Integer</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">2</span>
<span class="mi">2</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@m</span> <span class="n">x</span>
<span class="mi">1</span> <span class="n">arguments</span>
</code></pre></div><h2 id="代码生成">代码生成</h2>
<p>当需要大量重复的模板代码时，通常会以编程方式生成，以避免冗余。在大多数语言中，这需要一个额外的构建步骤，以及一个单独的程序来生成重复的代码。在 Julia 中，表达式插值和 <a href="https://docs.julialang.org/en/v1/base/base/#Base.MainInclude.eval">eval</a> 允许这样的代码生成在程序执行的正常过程中进行。例如，考虑以下自定义类型</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">struct</span> <span class="kt">MyNumber</span>
    <span class="n">x</span><span class="o">::</span><span class="kt">Float64</span>
<span class="k">end</span>
<span class="c"># output</span>
</code></pre></div><p>我们想为其添加一些方法。我们可以在下面的循环中以编程的方式进行。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">for</span> <span class="n">op</span> <span class="o">=</span> <span class="p">(</span><span class="ss">:sin</span><span class="p">,</span> <span class="ss">:cos</span><span class="p">,</span> <span class="ss">:tan</span><span class="p">,</span> <span class="ss">:log</span><span class="p">,</span> <span class="ss">:exp</span><span class="p">)</span>
    <span class="n">eval</span><span class="p">(</span><span class="k">quote</span>
        <span class="n">Base</span><span class="o">.$</span><span class="n">op</span><span class="p">(</span><span class="n">a</span><span class="o">::</span><span class="kt">MyNumber</span><span class="p">)</span> <span class="o">=</span> <span class="n">MyNumber</span><span class="p">(</span><span class="o">$</span><span class="n">op</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">x</span><span class="p">))</span>
    <span class="k">end</span><span class="p">)</span>
<span class="k">end</span>
<span class="c"># output</span>
</code></pre></div><p>现在我们可以用我们的自定义类型来使用这些函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">MyNumber</span><span class="p">(</span><span class="nb">π</span><span class="p">)</span>
<span class="n">MyNumber</span><span class="p">(</span><span class="mf">3.141592653589793</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">MyNumber</span><span class="p">(</span><span class="mf">1.2246467991473532e-16</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">MyNumber</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div><p>这样一来，Julia 就像自己的<a href="https://en.wikipedia.org/wiki/Preprocessor">预处理器</a>一样，可以从语言内部生成代码。上面的代码可以使用 <code>:</code> 前缀引号的形式，写得稍显生硬。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">for</span> <span class="n">op</span> <span class="o">=</span> <span class="p">(</span><span class="ss">:sin</span><span class="p">,</span> <span class="ss">:cos</span><span class="p">,</span> <span class="ss">:tan</span><span class="p">,</span> <span class="ss">:log</span><span class="p">,</span> <span class="ss">:exp</span><span class="p">)</span>
    <span class="n">eval</span><span class="p">(</span><span class="o">:</span><span class="p">(</span><span class="n">Base</span><span class="o">.$</span><span class="n">op</span><span class="p">(</span><span class="n">a</span><span class="o">::</span><span class="kt">MyNumber</span><span class="p">)</span> <span class="o">=</span> <span class="n">MyNumber</span><span class="p">(</span><span class="o">$</span><span class="n">op</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">x</span><span class="p">))))</span>
<span class="k">end</span>
</code></pre></div><p>不过，这种使用 <code>eval(quote(...))</code> 模式生成的语言内代码很常见，所以 Julia 自带了一个宏来缩写这种模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">for</span> <span class="n">op</span> <span class="o">=</span> <span class="p">(</span><span class="ss">:sin</span><span class="p">,</span> <span class="ss">:cos</span><span class="p">,</span> <span class="ss">:tan</span><span class="p">,</span> <span class="ss">:log</span><span class="p">,</span> <span class="ss">:exp</span><span class="p">)</span>
    <span class="nd">@eval</span> <span class="n">Base</span><span class="o">.$</span><span class="n">op</span><span class="p">(</span><span class="n">a</span><span class="o">::</span><span class="kt">MyNumber</span><span class="p">)</span> <span class="o">=</span> <span class="n">MyNumber</span><span class="p">(</span><span class="o">$</span><span class="n">op</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">x</span><span class="p">))</span>
<span class="k">end</span>
</code></pre></div><p><a href="https://docs.julialang.org/en/v1/base/base/#Base.@eval">@eval</a> 宏重写了这个调用，使之与上述较长版本的调用完全等同。对于较长的生成代码块，给 <a href="https://docs.julialang.org/en/v1/base/base/#Base.@eval">@eval</a> 的表达式参数可以是一个块。</p>
<pre><code class="language-juliaa" data-lang="juliaa">@eval begin
    # multiple lines
end
</code></pre><h2 id="非标准字符串字面值">非标准字符串字面值</h2>
<p>从 <a href="https://docs.julialang.org/en/v1/manual/strings/#non-standard-string-literals">Strings</a> 中回想一下，以标识符为前缀的字符串字元称为非标准字符串字元，其语义可能与未加前缀的字符串字面值不同。例如</p>
<p>r&rdquo;^\s*(?:#|$) &ldquo;产生一个正则表达式对象，而不是一个字符串。
b &ldquo;DATA\xff\u2200 &ldquo;是一个[68,65,84,65,255,226,136,128]的字节数组文字。
也许令人惊讶的是，这些行为并没有被硬编码到Julia解析器或编译器中。相反，它们是由一个通用机制提供的自定义行为，任何人都可以使用：前缀的字符串字元被解析为对特别命名的宏的调用。例如，正则表达式宏就如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">macro</span> <span class="n">r_str</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="kt">Regex</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div><p>就是这样。这个宏表示应该将字符串字词r&rdquo;^\s*(?:#|$) &ldquo;的字面内容传递给@r_str宏，并将扩展的结果放入字符串字词出现的语法树中。换句话说，表达式r&rdquo;^\s*(?:#|$) &ldquo;相当于将下面的对象直接放入语法树中。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="kt">Regex</span><span class="p">(</span><span class="s">&#34;^</span><span class="se">\\</span><span class="s">s*(?:#|</span><span class="se">\$</span><span class="s">)&#34;</span><span class="p">)</span>
</code></pre></div><p>字符串形式不仅更短、更方便，而且效率更高：由于正则表达式是编译的，而Regex对象实际上是在代码编译时创建的，所以编译只发生一次，而不是每次执行代码时。考虑一下如果正则表达式发生在循环中。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">for</span> <span class="n">line</span> <span class="o">=</span> <span class="n">lines</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">match</span><span class="p">(</span><span class="sa">r</span><span class="sr">&#34;^</span><span class="err">\</span><span class="sr">s*(?:#|$)&#34;</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span> <span class="o">===</span> <span class="nb">nothing</span>
        <span class="c"># non-comment</span>
    <span class="k">else</span>
        <span class="c"># comment</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div><p>由于正则表达式r&rdquo;^/ds*(?:#|$) &ldquo;在解析这段代码时被编译并插入语法树中，所以该表达式只被编译一次，而不是每次循环执行时都被编译。为了在不使用宏的情况下实现这个目标，必须这样写这个循环。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">re</span> <span class="o">=</span> <span class="kt">Regex</span><span class="p">(</span><span class="s">&#34;^</span><span class="se">\\</span><span class="s">s*(?:#|</span><span class="se">\$</span><span class="s">)&#34;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">line</span> <span class="o">=</span> <span class="n">lines</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">match</span><span class="p">(</span><span class="n">re</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span> <span class="o">===</span> <span class="nb">nothing</span>
        <span class="c"># non-comment</span>
    <span class="k">else</span>
        <span class="c"># comment</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div><p>此外，如果编译器不能确定regex对象在所有循环中都是恒定的，某些优化可能就无法实现，这就使得这个版本的效率仍然不如上面更方便的文字形式。当然，在有些情况下，非字面形式还是比较方便的：如果需要在正则表达式中插入一个变量，就必须采取这种比较啰嗦的方式；在正则表达式模式本身是动态的，有可能在每次循环迭代时发生变化的情况下，必须在每次迭代时构造一个新的正则表达式对象。然而，在绝大多数用例中，正则表达式并不是基于运行时数据来构造的。在这大多数情况下，将正则表达式写成编译时值的能力是非常宝贵的。</p>
<p>与非标准字符串字元一样，非标准命令字元也是使用命令字元语法的前缀变体存在的。命令字元custom<code>literal</code>被解析为@custom_cmd &ldquo;literal&rdquo;。Julia本身不包含任何非标准的命令字元，但包可以利用这种语法。除了语法不同和用_cmd后缀代替_str后缀外，非标准命令字元的行为与非标准字符串字元完全相同。</p>
<p>如果两个模块提供了名称相同的非标准字符串或命令字元，可以用模块名称来限定字符串或命令字元。例如，如果Foo和Bar都提供了非标准的字符串字元@x_str，那么可以写成Foo.x &ldquo;字元 &ldquo;或Bar.x &ldquo;字元 &ldquo;来区分两者。</p>
<p>用户定义字符串字元的机制是深刻的、强大的。不仅Julia的非标准字元用它来实现，而且命令字元语法（<code>echo &quot;Hello, $person&quot;</code>）也用下面这个看似无害的宏来实现。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">macro</span> <span class="n">cmd</span><span class="p">(</span><span class="n">str</span><span class="p">)</span>
    <span class="o">:</span><span class="p">(</span><span class="n">cmd_gen</span><span class="p">(</span><span class="o">$</span><span class="p">(</span><span class="n">shell_parse</span><span class="p">(</span><span class="n">str</span><span class="p">)[</span><span class="mi">1</span><span class="p">])))</span>
<span class="k">end</span>
</code></pre></div><p>当然，这个宏定义中使用的函数中隐藏着大量的复杂性，但它们只是函数，完全是用Julia编写的。你可以阅读它们的源码，并精确地看到它们的作用&ndash;它们所做的只是构造表达式对象，以便插入到你的程序的语法树中。</p>
<p>生成的函数</p>
<p>一个非常特殊的宏是@generated，它允许你定义所谓的生成函数。这些函数能够根据其参数的类型生成专门的代码，其灵活性和/或代码量比使用多重调度时更少。宏在解析时与表达式一起工作，不能访问其输入的类型，而生成函数在参数类型已知时得到扩展，但函数尚未编译。</p>
<p>生成函数声明不是执行一些计算或动作，而是返回一个引号的表达式，然后形成与参数类型相对应的方法的主体。当一个生成函数被调用时，它返回的表达式会被编译，然后运行。为了提高效率，通常会对结果进行缓存。而为了使之可推断，只有有限的语言子集可以使用。因此，生成函数提供了一种灵活的方式，将工作从运行时转移到编译时，但代价是对允许的构造有更大的限制。</p>
<p>在定义生成函数时，与普通函数有五个主要区别。</p>
<p>你用 @generated 宏来注释函数声明。这在AST中添加了一些信息，让编译器知道这是一个生成函数。
在生成函数的主体中，你只能访问参数的类型，而不能访问它们的值。
你不是计算一些东西或执行一些操作，而是返回一个引号的表达式，当它被评估时，就会执行你想要的东西。
生成函数只允许调用在生成函数定义之前定义的函数。(如果不遵守这一点，可能会得到引用未来世界时代函数的MethodErrors。)
生成的函数不得突变或观察任何非常态的全局状态（包括，例如，IO、锁、非本地字典或使用hasmethod）。这意味着它们只能读取全局常量，不能有任何副作用。换句话说，它们必须是完全纯粹的。由于实现上的限制，这也意味着它们目前不能定义闭包或生成器。
用一个例子来说明这一点是最简单的。我们可以将一个生成函数foo声明为</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@generated</span> <span class="k">function</span> <span class="n">foo</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
           <span class="n">Core</span><span class="o">.</span><span class="n">println</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
           <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
       <span class="k">end</span>
<span class="n">foo</span> <span class="p">(</span><span class="n">generic</span> <span class="k">function</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>
</code></pre></div><p>请注意，函数体返回的是一个引号表达式，即:(x * x)，而不仅仅是x * x的值。</p>
<p>从调用者的角度来看，这和正则函数是一样的；事实上，你不必知道你调用的是正则函数还是生成函数。让我们来看看foo是如何表现的。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">foo</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span> <span class="c"># note: output is from println() statement in the body</span>
<span class="kt">Int64</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">x</span>           <span class="c"># now we print x</span>
<span class="mi">4</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">foo</span><span class="p">(</span><span class="s">&#34;bar&#34;</span><span class="p">);</span>
<span class="kt">String</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">y</span>
<span class="s">&#34;barbar&#34;</span>
</code></pre></div><p>所以，我们看到，在生成函数的主体中，x是传递的参数的类型，而生成函数返回的值，是我们从定义中返回的引用表达式的评估结果，现在的值是x。</p>
<p>如果我们用已经使用过的类型再次评估foo，会发生什么？</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">foo</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="mi">16</span>
</code></pre></div><p>注意，没有打印出Int64的结果。我们可以看到，这里只针对特定的参数类型集执行了一次生成函数的主体，结果被缓存。之后，对于本例来说，第一次调用时生成函数返回的表达式被重新用作方法体。但是，实际的缓存行为是一种实现定义的性能优化，所以过于依赖这种行为是无效的。</p>
<p>生成函数的次数可能只有一次，但也可能更频繁，或者看起来根本没有发生。因此，你永远不应该写一个有副作用的生成函数&ndash;副作用何时发生，多久发生一次，都是没有定义的。(这对宏来说也是如此&ndash;就像对宏一样，在生成函数中使用eval是一个标志，表明你做了一些错误的事情。) 然而，与宏不同的是，运行时系统无法正确处理对eval的调用，所以不允许使用它。</p>
<p>同样重要的是看@生成函数如何与方法重新定义交互。按照正确的@生成函数不能观察任何可突变的状态或引起任何全局状态的突变的原则，我们看到以下行为。观察生成函数不能调用任何在生成函数本身定义之前没有定义的方法。</p>
<p>最初f(x)有一个定义</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="s">&#34;original definition&#34;</span><span class="p">;</span>
</code></pre></div><p>定义其他使用f(x)的操作。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@generated</span> <span class="n">gen1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@generated</span> <span class="n">gen2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">));</span>
</code></pre></div><p>现在我们为f(x)添加一些新的定义。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Int</span><span class="p">)</span> <span class="o">=</span> <span class="s">&#34;definition for Int&#34;</span><span class="p">;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Type</span><span class="p">{</span><span class="kt">Int</span><span class="p">})</span> <span class="o">=</span> <span class="s">&#34;definition for Type{Int}&#34;</span><span class="p">;</span>
</code></pre></div><p>并比较这些结果有何不同。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">f</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="s">&#34;definition for Int&#34;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">g</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="s">&#34;definition for Int&#34;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">gen1</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="s">&#34;original definition&#34;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">gen2</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="s">&#34;definition for Int&#34;</span>
</code></pre></div><p>生成函数的每个方法都有自己的定义函数视图。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@generated</span> <span class="n">gen1</span><span class="p">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Real</span><span class="p">)</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">gen1</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="s">&#34;definition for Type{Int}&#34;</span>
</code></pre></div><p>上面的生成函数foo例子并没有做任何普通函数foo(x) = x * x不能做的事情（除了在第一次调用时打印类型，并产生较高的开销）。然而，生成函数的强大之处在于它能够根据传递给它的类型计算不同的引号表达式。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@generated</span> <span class="k">function</span> <span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
           <span class="k">if</span> <span class="kt">x</span> <span class="o">&lt;:</span> <span class="kt">Integer</span>
               <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="n">x</span> <span class="o">^</span> <span class="mi">2</span><span class="p">)</span>
           <span class="k">else</span>
               <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
           <span class="k">end</span>
       <span class="k">end</span>
<span class="n">bar</span> <span class="p">(</span><span class="n">generic</span> <span class="k">function</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">bar</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="mi">16</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">bar</span><span class="p">(</span><span class="s">&#34;baz&#34;</span><span class="p">)</span>
<span class="s">&#34;baz&#34;</span>
</code></pre></div><p>(当然，这个人为的例子可以更容易地使用多个调度来实现&hellip;)</p>
<p>滥用这一点将破坏运行时系统并导致未定义的行为。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@generated</span> <span class="k">function</span> <span class="n">baz</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
           <span class="k">if</span> <span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">.9</span>
               <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span>
           <span class="k">else</span>
               <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="s">&#34;boo!&#34;</span><span class="p">)</span>
           <span class="k">end</span>
       <span class="k">end</span>
<span class="n">baz</span> <span class="p">(</span><span class="n">generic</span> <span class="k">function</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>
</code></pre></div><p>由于生成的函数主体是非确定的，所以它的行为以及所有后续代码的行为都是未定义的。</p>
<p>不要抄袭这些例子!</p>
<p>这些例子希望对说明生成函数的工作方式有所帮助，包括在定义端和调用端；但是，不要复制它们，原因如下。</p>
<p>foo函数有副作用 (对Core.println的调用), 而这些副作用究竟何时发生, 多久发生一次, 或发生多少次, 都是没有定义的
bar函数解决了一个问题，而这个问题最好用多重调度来解决&ndash;定义bar(x) = x和bar(x::Integer) = x ^ 2会做同样的事情，但它既简单又快速。
baz函数是病态的
请注意，在生成的函数中不应该尝试的操作集是没有限制的，运行时系统目前只能检测到无效操作的一个子集。还有许多其他的操作会在不通知的情况下简单地破坏运行时系统，通常以微妙的方式与坏定义没有明显的联系。因为函数生成器是在推理过程中运行的，它必须尊重该代码的所有限制。</p>
<p>一些不应该尝试的操作包括。</p>
<p>缓存本地指针
以任何方式与Core.Compiler的内容或方法进行交互。
观察任何可变状态。</p>
<p>对生成的函数的推理可以在任何时候运行，包括在您的代码试图观察或突变此状态时。
占用任何锁。你调用的C代码可以在内部使用锁，（例如，调用malloc是没有问题的，即使大多数实现在内部需要锁），但不要试图在执行Julia代码时持有或获取任何锁。
调用在生成函数主体之后定义的任何函数。对于增量加载的预编译模块，这个条件是放宽的，允许调用模块中的任何函数。
好了，现在我们对生成函数的工作原理有了更好的理解，让我们用它们来构建一些更高级的（有效的）功能&hellip;</p>
<p>一个高级的例子</p>
<p>茱莉亚的基础库有一个内部的sub2ind函数，用来计算一个线性索引到n维数组中，基于n个多线性索引的集合，换句话说，就是计算可以用A[i]来索引到数组A中的索引i，而不是A[x,y,z,&hellip;]。一种可能的实现方式如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">function</span> <span class="n">sub2ind_loop</span><span class="p">(</span><span class="n">dims</span><span class="o">::</span><span class="kt">NTuple</span><span class="p">{</span><span class="kt">N</span><span class="p">},</span> <span class="n">I</span><span class="o">::</span><span class="kt">Integer</span><span class="o">...</span><span class="p">)</span> <span class="k">where</span> <span class="kt">N</span>
           <span class="n">ind</span> <span class="o">=</span> <span class="n">I</span><span class="p">[</span><span class="n">N</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
           <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="n">👎1</span>
               <span class="n">ind</span> <span class="o">=</span> <span class="n">I</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span> <span class="o">+</span> <span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">ind</span>
           <span class="k">end</span>
           <span class="k">return</span> <span class="n">ind</span> <span class="o">+</span> <span class="mi">1</span>
       <span class="k">end</span>
<span class="n">sub2ind_loop</span> <span class="p">(</span><span class="n">generic</span> <span class="k">function</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">sub2ind_loop</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="mi">4</span>
</code></pre></div><p>同样的事情也可以用递归来完成。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">sub2ind_rec</span><span class="p">(</span><span class="n">dims</span><span class="o">::</span><span class="kt">Tuple</span><span class="p">{})</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">sub2ind_rec</span><span class="p">(</span><span class="n">dims</span><span class="o">::</span><span class="kt">Tuple</span><span class="p">{},</span> <span class="n">i1</span><span class="o">::</span><span class="kt">Integer</span><span class="p">,</span> <span class="n">I</span><span class="o">::</span><span class="kt">Integer</span><span class="o">...</span><span class="p">)</span> <span class="o">=</span>
           <span class="n">i1</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">?</span> <span class="n">sub2ind_rec</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">I</span><span class="o">...</span><span class="p">)</span> <span class="o">:</span> <span class="n">throw</span><span class="p">(</span><span class="kt">BoundsError</span><span class="p">());</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">sub2ind_rec</span><span class="p">(</span><span class="n">dims</span><span class="o">::</span><span class="kt">Tuple</span><span class="p">{</span><span class="kt">Integer</span><span class="p">,</span> <span class="kt">Vararg</span><span class="p">{</span><span class="kt">Integer</span><span class="p">}},</span> <span class="n">i1</span><span class="o">::</span><span class="kt">Integer</span><span class="p">)</span> <span class="o">=</span> <span class="n">i1</span><span class="p">;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">sub2ind_rec</span><span class="p">(</span><span class="n">dims</span><span class="o">::</span><span class="kt">Tuple</span><span class="p">{</span><span class="kt">Integer</span><span class="p">,</span> <span class="kt">Vararg</span><span class="p">{</span><span class="kt">Integer</span><span class="p">}},</span> <span class="n">i1</span><span class="o">::</span><span class="kt">Integer</span><span class="p">,</span> <span class="n">I</span><span class="o">::</span><span class="kt">Integer</span><span class="o">...</span><span class="p">)</span> <span class="o">=</span>
           <span class="n">i1</span> <span class="o">+</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">sub2ind_rec</span><span class="p">(</span><span class="n">Base</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="n">dims</span><span class="p">),</span> <span class="n">I</span><span class="o">...</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">sub2ind_rec</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="mi">4</span>
</code></pre></div><p>这两种实现虽然不同，但本质上都是一样的：在数组的维度上进行运行时循环，将每个维度的偏移量收集到最后的索引中。</p>
<p>然而，我们在循环中所需要的所有信息都嵌入在参数的类型信息中。因此，我们可以利用生成函数将迭代移动到编译时；用编译器的说法，我们使用生成函数手动展开循环。身体变得几乎相同，但我们不是计算线性指数，而是建立一个计算指数的表达式。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@generated</span> <span class="k">function</span> <span class="n">sub2ind_gen</span><span class="p">(</span><span class="n">dims</span><span class="o">::</span><span class="kt">NTuple</span><span class="p">{</span><span class="kt">N</span><span class="p">},</span> <span class="n">I</span><span class="o">::</span><span class="kt">Integer</span><span class="o">...</span><span class="p">)</span> <span class="k">where</span> <span class="kt">N</span>
           <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="o">$</span><span class="n">N</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
           <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="n">👎1</span>
               <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="o">$</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">dims</span><span class="p">[</span><span class="o">$</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="o">$</span><span class="n">ex</span><span class="p">)</span>
           <span class="k">end</span>
           <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="o">$</span><span class="n">ex</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
       <span class="k">end</span>
<span class="n">sub2ind_gen</span> <span class="p">(</span><span class="n">generic</span> <span class="k">function</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">sub2ind_gen</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="mi">4</span>
</code></pre></div><p>这将产生什么代码呢？</p>
<p>一个简单的方法是将主体提取到另一个（常规）函数中。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@generated</span> <span class="k">function</span> <span class="n">sub2ind_gen</span><span class="p">(</span><span class="n">dims</span><span class="o">::</span><span class="kt">NTuple</span><span class="p">{</span><span class="kt">N</span><span class="p">},</span> <span class="n">I</span><span class="o">::</span><span class="kt">Integer</span><span class="o">...</span><span class="p">)</span> <span class="k">where</span> <span class="kt">N</span>
           <span class="k">return</span> <span class="n">sub2ind_gen_impl</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">I</span><span class="o">...</span><span class="p">)</span>
       <span class="k">end</span>
<span class="n">sub2ind_gen</span> <span class="p">(</span><span class="n">generic</span> <span class="k">function</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="k">function</span> <span class="n">sub2ind_gen_impl</span><span class="p">(</span><span class="n">dims</span><span class="o">::</span><span class="kt">Type</span><span class="p">{</span><span class="kt">T</span><span class="p">},</span> <span class="n">I</span><span class="o">...</span><span class="p">)</span> <span class="k">where</span> <span class="kt">T</span> <span class="o">&lt;:</span> <span class="kt">NTuple</span><span class="p">{</span><span class="kt">N</span><span class="p">,</span><span class="kt">Any</span><span class="p">}</span> <span class="k">where</span> <span class="kt">N</span>
           <span class="n">length</span><span class="p">(</span><span class="n">I</span><span class="p">)</span> <span class="o">==</span> <span class="n">N</span> <span class="o">||</span> <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="n">error</span><span class="p">(</span><span class="s">&#34;partial indexing is unsupported&#34;</span><span class="p">))</span>
           <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="o">$</span><span class="n">N</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
           <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="n">👎1</span>
               <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="o">$</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">dims</span><span class="p">[</span><span class="o">$</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="o">$</span><span class="n">ex</span><span class="p">)</span>
           <span class="k">end</span>
           <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="o">$</span><span class="n">ex</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
       <span class="k">end</span>
<span class="n">sub2ind_gen_impl</span> <span class="p">(</span><span class="n">generic</span> <span class="k">function</span> <span class="n">with</span> <span class="mi">1</span> <span class="n">method</span><span class="p">)</span>
</code></pre></div><p>现在我们可以执行sub2ind_gen_impl并检查它返回的表达式。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">sub2ind_gen_impl</span><span class="p">(</span><span class="kt">Tuple</span><span class="p">{</span><span class="kt">Int</span><span class="p">,</span><span class="kt">Int</span><span class="p">},</span> <span class="kt">Int</span><span class="p">,</span> <span class="kt">Int</span><span class="p">)</span>
<span class="o">:</span><span class="p">(((</span><span class="n">I</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>所以，这里要用到的方法体根本不包含循环&ndash;只是索引到两个元组，乘法和加/减法。所有的循环都是在编译时进行的，我们完全避免了执行过程中的循环。因此，我们对每个类型只进行一次循环，在本例中，每N个类型只循环一次（除非在函数生成一次以上的边缘情况下&ndash;见上面的免责声明）。</p>
<p>可选生成的函数</p>
<p>生成函数可以在运行时实现高效率，但也有编译时间成本：每一个具体参数类型的组合都必须生成一个新的函数体。通常情况下，Julia能够编译 &ldquo;通用 &ldquo;版本的函数，这些函数将适用于任何参数，但对于生成函数，这是不可能的。这意味着大量使用生成函数的程序可能无法静态编译。</p>
<p>为了解决这个问题，语言提供了编写正常的、非生成函数的替代实现的语法。应用到上面的sub2ind例子中，会是这样的。</p>
<div class="highlight"><pre class="chroma"><code class="language-julia" data-lang="julia"><span class="k">function</span> <span class="n">sub2ind_gen</span><span class="p">(</span><span class="n">dims</span><span class="o">::</span><span class="kt">NTuple</span><span class="p">{</span><span class="kt">N</span><span class="p">},</span> <span class="n">I</span><span class="o">::</span><span class="kt">Integer</span><span class="o">...</span><span class="p">)</span> <span class="k">where</span> <span class="kt">N</span>
    <span class="k">if</span> <span class="n">N</span> <span class="o">!=</span> <span class="n">length</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
        <span class="n">throw</span><span class="p">(</span><span class="kt">ArgumentError</span><span class="p">(</span><span class="s">&#34;Number of dimensions must match number of indices.&#34;</span><span class="p">))</span>
    <span class="k">end</span>
    <span class="k">if</span> <span class="nd">@generated</span>
        <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="o">$</span><span class="n">N</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="n">👎1</span>
            <span class="n">ex</span> <span class="o">=</span> <span class="o">:</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="o">$</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">dims</span><span class="p">[</span><span class="o">$</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="o">$</span><span class="n">ex</span><span class="p">)</span>
        <span class="k">end</span>
        <span class="k">return</span> <span class="o">:</span><span class="p">(</span><span class="o">$</span><span class="n">ex</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">I</span><span class="p">[</span><span class="n">N</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="n">👎1</span>
            <span class="n">ind</span> <span class="o">=</span> <span class="n">I</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">ind</span>
        <span class="k">end</span>
        <span class="k">return</span> <span class="n">ind</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div><p>在内部，这段代码创建了函数的两个实现：一个是生成的，其中使用了if @generated中的第一个块，另一个是正常的，其中使用了 else块。在if @generated块的then部分内部，代码的语义与其他生成函数相同：参数名指的是类型，代码应该返回一个表达式。可能会出现多个if @生成块，在这种情况下，生成的实现使用所有的then块，而备用的实现使用所有的else块。</p>
<p>注意，我们在函数的顶部添加了一个错误检查。这段代码在两个版本中都是通用的，并且在两个版本中都是运行时代码（它将被引用并作为生成版本的表达式返回）。这意味着局部变量的值和类型在代码生成时是不可用的&ndash;代码生成代码只能看到参数的类型。</p>
<p>在这种风格的定义中，代码生成功能本质上是一种可选的优化。编译器在方便的情况下会使用它，但其他情况下可能会选择使用正常的实现来代替。这种风格是首选，因为它允许编译器做出更多的决定，并以更多的方式编译程序，而且正常代码比代码生成代码更易读。但是，使用哪种实现取决于编译器的实现细节，所以两种实现的行为必须完全相同。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/julia" term="julia" label="Julia" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/julia-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="julia-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Julia 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[FlinkCEP - Flink 的复杂事件处理]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-12-20-flink-cep-complex-event-processing-for-flink/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-12-10-execution-mode/?utm_source=atom_feed" rel="related" type="text/html" title="执行模式(批/流)" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-application-building-block/?utm_source=atom_feed" rel="related" type="text/html" title="Application Building Blocks" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-logical-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Logical Functions" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-python-walkthrough/?utm_source=atom_feed" rel="related" type="text/html" title="Python 演练" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-sdk/?utm_source=atom_feed" rel="related" type="text/html" title="Sdk" />
            
                <id>https://ohmyweekly.github.io/notes/2020-12-20-flink-cep-complex-event-processing-for-flink/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-12-20T00:00:00+08:00</published>
            <updated>2020-12-20T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Flink Cep Complex Event Processing for Flink</blockquote><p>FlinkCEP 是在 Flink 之上实现的复杂事件处理（CEP）库。它允许你在无尽的事件流中检测事件模式, 让你有机会掌握数据中的重要内容。</p>
<p>本页介绍了 Flink CEP 中可用的 API 调用。我们首先介绍 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#the-pattern-api">Pattern API</a>, 它允许你指定你想在你的流中检测的模式, 然后介绍你如何<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#detecting-patterns">检测并对匹配的事件序列采取行动</a>。然后, 我们将介绍 CEP 库在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#handling-lateness-in-event-time">处理事件时间的延迟</a>时做出的假设, 以及如何将你的工作从旧版 Flink <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#migrating-from-an-older-flink-versionpre-13">迁移</a>到 Flink-1.3。</p>
<h1 id="入门">入门</h1>
<p>如果你想直接进入, <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html">设置一个 Flink 程序</a>, 并将 FlinkCEP 依赖关系添加到项目的 pom.xml 中。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="o">&lt;</span><span class="n">dependency</span><span class="o">&gt;</span>
  <span class="o">&lt;</span><span class="n">groupId</span><span class="o">&gt;</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">flink</span><span class="o">&lt;/</span><span class="n">groupId</span><span class="o">&gt;</span>
  <span class="o">&lt;</span><span class="n">artifactId</span><span class="o">&gt;</span><span class="n">flink</span><span class="o">-</span><span class="n">cep</span><span class="o">-</span><span class="n">scala_2</span><span class="o">.</span><span class="mi">11</span><span class="o">&lt;/</span><span class="n">artifactId</span><span class="o">&gt;</span>
  <span class="o">&lt;</span><span class="n">version</span><span class="o">&gt;</span><span class="mf">1.12</span><span class="o">.</span><span class="mi">0</span><span class="o">&lt;/</span><span class="n">version</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">dependency</span><span class="o">&gt;</span>
</code></pre></div><p>信息：FlinkCEP 不是二进制发行版的一部分。请在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html">这里</a>查看如何与它链接进行集群执行。</p>
<p>现在你可以开始使用模式 API 编写你的第一个 CEP 程序了。</p>
<p>注意: 你想应用模式匹配的 DataStream 中的事件必须实现适当的 <code>equals()</code> 和 <code>hashCode()</code> 方法, 因为 FlinkCEP 使用它们来比较和匹配事件。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Event</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">pattern</span> <span class="k">=</span> <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getId</span> <span class="o">==</span> <span class="mi">42</span><span class="o">)</span>
  <span class="o">.</span><span class="n">next</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">subtype</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">SubEvent</span><span class="o">]).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getVolume</span> <span class="o">&gt;=</span> <span class="mf">10.0</span><span class="o">)</span>
  <span class="o">.</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;end&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getName</span> <span class="o">==</span> <span class="s">&#34;end&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">patternStream</span> <span class="k">=</span> <span class="nc">CEP</span><span class="o">.</span><span class="n">pattern</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">pattern</span><span class="o">)</span>

<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Alert</span><span class="o">]</span> <span class="k">=</span> <span class="n">patternStream</span><span class="o">.</span><span class="n">process</span><span class="o">(</span>
    <span class="k">new</span> <span class="nc">PatternProcessFunction</span><span class="o">[</span><span class="kt">Event</span>, <span class="kt">Alert</span><span class="o">]()</span> <span class="o">{</span>
        <span class="k">override</span> <span class="k">def</span> <span class="n">processMatch</span><span class="o">(</span>
              <span class="n">`match`</span><span class="k">:</span> <span class="kt">util.Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">util.List</span><span class="o">[</span><span class="kt">Event</span><span class="o">]],</span>
              <span class="n">ctx</span><span class="k">:</span> <span class="kt">PatternProcessFunction.Context</span><span class="o">,</span>
              <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Alert</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
            <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">createAlertFrom</span><span class="o">(</span><span class="n">pattern</span><span class="o">))</span>
        <span class="o">}</span>
    <span class="o">})</span>
</code></pre></div><h1 id="pattern-api">Pattern API</h1>
<p>模式 API 允许你定义你想从输入流中提取的复杂模式序列。</p>
<p>每个复杂模式序列由多个简单模式组成, 即寻找具有相同属性的单个事件的模式。从现在开始, 我们将把这些简单模式称为模式, 而最终我们要在流中寻找的复杂模式序列, 就是模式序列。你可以把模式序列看成是这样的模式图, 根据用户指定的条件, 从一个模式过渡到下一个模式, 例如 event.getName().equals(&ldquo;end&rdquo;)。一个匹配是一个输入事件的序列, 它通过有效的模式转换序列, 访问复杂模式图的所有模式。</p>
<p>注意: 每个模式必须有一个唯一的名称, 你以后用它来识别匹配事件。</p>
<p>注意: 模式名称不能包含字符 &ldquo;:&quot;。</p>
<p>在本节的其余部分, 我们将首先介绍如何定义 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#individual-patterns">单个模式</a>, 然后介绍如何将单个模式组合成 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#combining-patterns">复杂模式</a>。</p>
<h2 id="单个模式">单个模式</h2>
<p>模式可以是单个模式, 也可以是循环模式。单元模式只接受一个事件, 而循环模式可以接受多个事件。在模式匹配符号中, 模式 &ldquo;a b+ c?d&rdquo;(或 &ldquo;a&rdquo;, 后面跟着一个或多个 &ldquo;b&rdquo;, 可选地跟着一个 &ldquo;c&rdquo;, 后面跟着一个 &ldquo;d&rdquo;), a、c? 和 d 是单个模式, 而 b+ 是循环模式。默认情况下, 模式是一个单个模式, 你可以通过使用量词将其转换为一个循环模式。每个模式可以有一个或多个条件, 基于这些条件, 它可以接受事件。</p>
<h3 id="量词">量词</h3>
<p>在 FlinkCEP 中, 你可以使用这些方法来指定循环模式：pattern.oneOrMore(), 用于期望给定事件出现一次或多次的模式(例如前面提到的 b+)；以及 pattern.times(#ofTimes), 用于期望给定事件出现的特定次数的模式, 例如 4 个 a；以及 pattern.times(#fromTimes, #toTimes), 用于期望给定事件的特定最小出现次数和最大出现次数的模式, 例如 2-4 个 a。</p>
<p>你可以使用 pattern.greedy() 方法使循环模式变得贪婪, 但你还不能使分组模式变得贪婪。你可以使用 pattern.option() 方法使所有模式, 不管是否循环, 都是可选的。</p>
<p>对于名为 start 的模式, 以下是有效的量词。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// expecting 4 occurrences
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">times</span><span class="o">(</span><span class="mi">4</span><span class="o">)</span>

<span class="c1">// expecting 0 or 4 occurrences
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">times</span><span class="o">(</span><span class="mi">4</span><span class="o">).</span><span class="n">optional</span><span class="o">()</span>

<span class="c1">// expecting 2, 3 or 4 occurrences
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">times</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">4</span><span class="o">)</span>

<span class="c1">// expecting 2, 3 or 4 occurrences and repeating as many as possible
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">times</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">4</span><span class="o">).</span><span class="n">greedy</span><span class="o">()</span>

<span class="c1">// expecting 0, 2, 3 or 4 occurrences
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">times</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">4</span><span class="o">).</span><span class="n">optional</span><span class="o">()</span>

<span class="c1">// expecting 0, 2, 3 or 4 occurrences and repeating as many as possible
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">times</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">4</span><span class="o">).</span><span class="n">optional</span><span class="o">().</span><span class="n">greedy</span><span class="o">()</span>

<span class="c1">// expecting 1 or more occurrences
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">oneOrMore</span><span class="o">()</span>

<span class="c1">// expecting 1 or more occurrences and repeating as many as possible
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">oneOrMore</span><span class="o">().</span><span class="n">greedy</span><span class="o">()</span>

<span class="c1">// expecting 0 or more occurrences
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">oneOrMore</span><span class="o">().</span><span class="n">optional</span><span class="o">()</span>

<span class="c1">// expecting 0 or more occurrences and repeating as many as possible
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">oneOrMore</span><span class="o">().</span><span class="n">optional</span><span class="o">().</span><span class="n">greedy</span><span class="o">()</span>

<span class="c1">// expecting 2 or more occurrences
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">timesOrMore</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>

<span class="c1">// expecting 2 or more occurrences and repeating as many as possible
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">timesOrMore</span><span class="o">(</span><span class="mi">2</span><span class="o">).</span><span class="n">greedy</span><span class="o">()</span>

<span class="c1">// expecting 0, 2 or more occurrences
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">timesOrMore</span><span class="o">(</span><span class="mi">2</span><span class="o">).</span><span class="n">optional</span><span class="o">()</span>

<span class="c1">// expecting 0, 2 or more occurrences and repeating as many as possible
</span><span class="c1"></span><span class="n">start</span><span class="o">.</span><span class="n">timesOrMore</span><span class="o">(</span><span class="mi">2</span><span class="o">).</span><span class="n">optional</span><span class="o">().</span><span class="n">greedy</span><span class="o">()</span>
</code></pre></div><p>条件</p>
<p>对于每个模式, 你可以指定一个条件, 传入的事件必须满足这个条件才能被&quot;接受&quot;到模式中, 例如, 它的值应该大于 5, 或者大于之前接受的事件的平均值。你可以通过 pattern.where()、pattern.or() 或 pattern.until() 方法来指定事件属性的条件。这些条件可以是 IterativeConditions 或 SimpleConditions。</p>
<p>迭代条件。这是最通用的条件类型。你可以通过这种方式指定一个条件, 该条件基于之前接受的事件的属性或其中一个子集的统计量来接受后续事件。</p>
<p>下面是一个迭代条件的代码, 如果一个名为 &ldquo;middle&rdquo; 的模式的名称以 &ldquo;foo&rdquo; 开头, 并且如果该模式之前接受的事件的价格加上当前事件的价格之和不超过 5.0 的值, 则接受该模式的下一个事件。迭代条件可以发挥强大的作用, 尤其是与循环模式相结合, 例如 oneOrMore()。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">middle</span><span class="o">.</span><span class="n">oneOrMore</span><span class="o">()</span>
    <span class="o">.</span><span class="n">subtype</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">SubEvent</span><span class="o">])</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(</span>
        <span class="o">(</span><span class="n">value</span><span class="o">,</span> <span class="n">ctx</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span>
            <span class="k">lazy</span> <span class="k">val</span> <span class="n">sum</span> <span class="k">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">getEventsForPattern</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getPrice</span><span class="o">).</span><span class="n">sum</span>
            <span class="n">value</span><span class="o">.</span><span class="n">getName</span><span class="o">.</span><span class="n">startsWith</span><span class="o">(</span><span class="s">&#34;foo&#34;</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">sum</span> <span class="o">+</span> <span class="n">value</span><span class="o">.</span><span class="n">getPrice</span> <span class="o">&lt;</span> <span class="mf">5.0</span>
        <span class="o">}</span>
    <span class="o">)</span>
</code></pre></div><p>注意：调用 ctx.getEventsForPattern(&hellip;) 可以为给定的潜在匹配找到所有之前接受的事件。这个操作的成本可能会有所不同, 所以在实现你的条件时, 尽量减少它的使用。</p>
<p>描述的上下文使人们也可以访问事件的时间特征。更多信息请看时间上下文。</p>
<p>简单条件。这种类型的条件扩展了前面提到的 IterativeCondition 类, 仅根据事件本身的属性来决定是否接受一个事件。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">start</span><span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">event</span> <span class="k">=&gt;</span> <span class="n">event</span><span class="o">.</span><span class="n">getName</span><span class="o">.</span><span class="n">startsWith</span><span class="o">(</span><span class="s">&#34;foo&#34;</span><span class="o">))</span>
</code></pre></div><p>最后, 你还可以通过 pattern.subtype(subClass) 方法将接受的事件类型限制为初始事件类型的一个子类型（这里是 Event）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">start</span><span class="o">.</span><span class="n">subtype</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">SubEvent</span><span class="o">]).</span><span class="n">where</span><span class="o">(</span><span class="n">subEvent</span> <span class="k">=&gt;</span> <span class="o">...</span> <span class="cm">/* some condition */</span><span class="o">)</span>
</code></pre></div><p>组合条件。如上所示, 你可以将子类型条件与其他条件结合起来。这对每个条件都适用。你可以通过依次调用 where() 来任意组合条件。最后的结果将是各个条件的结果的逻辑 AND。要使用 OR 组合条件, 可以使用 or() 方法, 如下所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">event</span> <span class="k">=&gt;</span> <span class="o">...</span> <span class="cm">/* some condition */</span><span class="o">).</span><span class="n">or</span><span class="o">(</span><span class="n">event</span> <span class="k">=&gt;</span> <span class="o">...</span> <span class="cm">/* or condition */</span><span class="o">)</span>
</code></pre></div><p><strong>停止条件</strong>：如果是循环模式(oneOrMore() 和 oneOrMore().option()), 你也可以指定一个停止条件, 例如, 接受值大于 5 的事件, 直到值的总和小于 50。</p>
<p>为了更好地理解它, 请看下面的例子。给定：</p>
<p>像 &ldquo;(a+ until b)&rdquo; (一个或多个 &ldquo;a&rdquo; 直到 &ldquo;b&rdquo;) 这样的模式</p>
<p>输入事件的序列 &ldquo;a1&rdquo; &ldquo;c&rdquo; &ldquo;a2&rdquo; &ldquo;b&rdquo; &ldquo;a3&rdquo;</p>
<p>该库将输出结果: {a1 a2} {a1} {a2} {a3}.</p>
<p>正如你所看到的 {a1 a2 a3} 或 {a2 a3} 由于停止条件没有返回。</p>
<ul>
<li>where(条件) - 定义当前模式的条件。要匹配模式, 一个事件必须满足条件。多个连续的 where() 子句会导致其条件被 AND 化。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">event</span> <span class="k">=&gt;</span> <span class="o">...</span> <span class="cm">/* some condition */</span><span class="o">)</span>
</code></pre></div><ul>
<li>or(条件) - 添加一个新的条件, 该条件与现有的条件相匹配。一个事件只有在通过至少一个条件的情况下才能与模式匹配。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">event</span> <span class="k">=&gt;</span> <span class="o">...</span> <span class="cm">/* some condition */</span><span class="o">)</span>
       <span class="o">.</span><span class="n">or</span><span class="o">(</span><span class="n">event</span> <span class="k">=&gt;</span> <span class="o">...</span> <span class="cm">/* alternative condition */</span><span class="o">)</span>
</code></pre></div><ul>
<li>until(条件) - 指定循环模式的停止条件。意思是如果发生了与给定条件相匹配的事件, 则不会再接受更多的事件进入模式。</li>
</ul>
<p>仅与 oneOrMore() 结合使用。</p>
<p>注意：它允许在事件条件下对相应的模式进行清洗状态。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">oneOrMore</span><span class="o">().</span><span class="n">until</span><span class="o">(</span><span class="n">event</span> <span class="k">=&gt;</span> <span class="o">...</span> <span class="cm">/* some condition */</span><span class="o">)</span>
</code></pre></div><ul>
<li>subtype(subClass)	- 为当前模式定义一个子类型条件。只有当一个事件属于这个子类型时, 它才能与模式相匹配。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">subtype</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">SubEvent</span><span class="o">])</span>
</code></pre></div><ul>
<li>oneOrMore() - 指定该模式期望匹配事件至少出现一次。</li>
</ul>
<p>默认情况下, 使用的是放宽的内部连续（在后续事件之间）。关于内部连续性的更多信息, 请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#consecutive_scala">consecutive</a>。</p>
<p>注意：建议使用 until() 或 within() 来启用状态清除。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">oneOrMore</span><span class="o">()</span>
</code></pre></div><ul>
<li>timesOrMore(#times) - 指定该模式期望一个匹配事件至少出现 #times 次。</li>
</ul>
<p>默认情况下, 使用的是放宽的内部连续（在后续事件之间）。关于内部相邻性的更多信息, 请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#consecutive_scala">consecutive</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">timesOrMore</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
</code></pre></div><ul>
<li>times(#ofTimes) - 指定该模式期望匹配事件的准确出现次数。</li>
</ul>
<p>默认情况下, 使用的是放宽的内部连续性（在后续事件之间）。关于内部相邻性的更多信息, 请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#consecutive_scala">consecutive</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">times</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
</code></pre></div><ul>
<li>times(#fromTimes, #toTimes) - 指定该模式期望匹配事件的 #fromTimes 和 #toTimes 之间出现。</li>
</ul>
<p>默认情况下, 使用的是放宽的内部连续性（在后续事件之间）。关于内部相邻性的更多信息, 请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#consecutive_scala">consecutive</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">times</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">4</span><span class="o">)</span>
</code></pre></div><ul>
<li>optional() - 指定该模式是可选的, 即它可能根本不会出现。这适用于上述所有量词。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">oneOrMore</span><span class="o">().</span><span class="n">optional</span><span class="o">()</span>
</code></pre></div><ul>
<li>greedy() - 指定该模式是贪婪的, 即会尽可能多的重复。这只适用于量词, 目前不支持组模式。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">oneOrMore</span><span class="o">().</span><span class="n">greedy</span><span class="o">()</span>
</code></pre></div><h3 id="组合模式">组合模式</h3>
<p>现在你已经看到了单个模式的样子, 现在是时候看看如何将它们组合成一个完整的模式序列了。</p>
<p>一个模式序列必须从一个初始模式开始, 如下所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">start</span> <span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">(</span><span class="s">&#34;start&#34;</span><span class="o">)</span>
</code></pre></div><p>下一步, 你可以通过指定它们之间所需的毗连条件, 将更多的模式附加到你的模式序列中。FlinkCEP 支持以下形式的事件之间的相邻性。</p>
<ul>
<li><strong>严格相邻</strong>: 希望所有匹配的事件严格地一个接一个出现, 中间没有任何非匹配的事件。</li>
<li><strong>Relaxed Contiguity</strong>: 忽略匹配事件之间出现的非匹配事件。</li>
<li>非决定性的松弛相邻性（Non-Deterministic Relaxed Contiguity）。进一步放宽相邻性, 允许忽略一些匹配事件的额外匹配。</li>
</ul>
<p>要在连续模式之间应用它们, 你可以使用:</p>
<ul>
<li>next(), 用于严格相邻,</li>
<li>followedBy(), 用于松散相邻, 和</li>
<li>followedByAny(), 用于非确定性的松散相邻。</li>
</ul>
<p>或</p>
<ul>
<li>notNext(), 如果你不希望一个事件类型直接跟随另一个事件类型</li>
<li>notFollowedBy(), 如果你不想让一个事件类型位于两个其他事件类型之间的任何地方。</li>
</ul>
<p>注意：模式序列不能以 notFollowedBy() 结束。</p>
<p>注意： NOT 模式不能在前面加上一个可选模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// strict contiguity
</span><span class="c1"></span><span class="k">val</span> <span class="n">strict</span><span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">next</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>

<span class="c1">// relaxed contiguity
</span><span class="c1"></span><span class="k">val</span> <span class="n">relaxed</span><span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>

<span class="c1">// non-deterministic relaxed contiguity
</span><span class="c1"></span><span class="k">val</span> <span class="n">nonDetermin</span><span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">followedByAny</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>

<span class="c1">// NOT pattern with strict contiguity
</span><span class="c1"></span><span class="k">val</span> <span class="n">strictNot</span><span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">notNext</span><span class="o">(</span><span class="s">&#34;not&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>

<span class="c1">// NOT pattern with relaxed contiguity
</span><span class="c1"></span><span class="k">val</span> <span class="n">relaxedNot</span><span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">notFollowedBy</span><span class="o">(</span><span class="s">&#34;not&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>
</code></pre></div><p>松散毗连意味着只有第一个后续的匹配事件才会被匹配, 而对于非确定性的松散毗连, 同一开头会发出多个匹配。举个例子, 一个模式 &ldquo;a b&rdquo;, 给定事件序列 &ldquo;a&rdquo;, &ldquo;c&rdquo;, &ldquo;b1&rdquo;, &ldquo;b2&rdquo;, 将得到以下结果。</p>
<p>&ldquo;a&rdquo; 和 &ldquo;b&rdquo; 之间有严格的毗连性。{} (不匹配), &ldquo;a&rdquo; 后面的 &ldquo;c&rdquo; 会导致 &ldquo;a&rdquo; 被丢弃。</p>
<p>&ldquo;a&rdquo; 和 &ldquo;b&rdquo; 之间的松散相邻性。{a b1}, 因为松散连续性被看作是 &ldquo;跳过非匹配事件, 直到下一个匹配事件&rdquo;。</p>
<p>&ldquo;a&rdquo; 和 &ldquo;b&rdquo; 之间的非确定性松散相邻性。{a b1}, {a b2}, 因为这是最一般的形式。</p>
<p>也可以定义一个时间约束, 让模式有效。例如, 你可以通过 pattern.within() 方法定义一个模式应该在 10 秒内发生。处理时间和事件时间都支持时间模式。</p>
<p>注意: 模式序列只能有一个时间约束。如果在不同的单个模式上定义了多个这样的约束, 那么就采用最小的约束。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">next</span><span class="o">.</span><span class="n">within</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">))</span>
</code></pre></div><p>循环模式中的相邻性</p>
<p>你可以在循环模式中应用与上一节讨论的相同的相邻性条件。相邻性将被应用在这样一个模式中的元素之间。为了举例说明, 模式序列 &ldquo;a b+ c&rdquo;（&ldquo;a&rdquo; 后面跟着一个或多个 &ldquo;b&rdquo; 的任意（非确定的松散的）序列, 后面跟着一个 &ldquo;c&rdquo;）, 输入 &ldquo;a&rdquo;、&ldquo;b1&rdquo;、&ldquo;d1&rdquo;、&ldquo;b2&rdquo;、&ldquo;d2&rdquo;、&ldquo;b3&rdquo;、&ldquo;c&rdquo;, 会有以下结果。</p>
<p>严格相邻性：{a b3 c} - &ldquo;b1&rdquo; 后面的 &ldquo;d1&rdquo; 会导致 &ldquo;b1&rdquo; 被丢弃, &ldquo;b2&rdquo; 也会因为 &ldquo;d2&rdquo; 而被丢弃。</p>
<p>放宽相邻性：{a b1 c}, {a b1 b2 c}, {a b1 b2 b3 c}, {a b2 c}, {a b2 b3 c}, {a b3 c} - &ldquo;d&rdquo; 被忽略。</p>
<p>非确定性的松弛相邻性：{a b1 c}, {a b1 b2 c}, {a b1 b3 c}, {a b1 b2 b3 c}, {a b2 c}, {a b2 b3 c}, {a b3 c} - 注意{a b1 b3 c}, 这是 &ldquo;b&rdquo; 之间松弛相邻性的结果。</p>
<p>对于循环模式(例如 oneOrMore() 和 times()), 默认是放宽毗连性。如果你想要严格的相邻性, 你必须通过使用 continuous() 调用来明确指定, 如果你想要非确定性的松弛相邻性, 你可以使用 allowCombinations() 调用。</p>
<ul>
<li>consecutive()</li>
</ul>
<p>与 oneOrMore() 和 times() 一起使用, 并在匹配的事件之间施加严格的毗连性, 即任何不匹配的元素都会中断匹配（如 next()）。</p>
<p>如果不应用, 则使用宽松的连续性（如 followedBy()）。</p>
<p>例如, 像这样的模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">(</span><span class="s">&#34;start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getName</span><span class="o">().</span><span class="n">equals</span><span class="o">(</span><span class="s">&#34;c&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getName</span><span class="o">().</span><span class="n">equals</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">))</span>
                       <span class="o">.</span><span class="n">oneOrMore</span><span class="o">().</span><span class="n">consecutive</span><span class="o">()</span>
  <span class="o">.</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;end1&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getName</span><span class="o">().</span><span class="n">equals</span><span class="o">(</span><span class="s">&#34;b&#34;</span><span class="o">))</span>
</code></pre></div><p>将为一个输入序列生成以下匹配。C D A1 A2 A3 D A4 B</p>
<p>与连续应用。{C A1 B}, {C A1 A2 B}, {C A1 A2 A3 B}。</p>
<p>不连续应用。{C A1 B}, {C A1 A2 B}, {C A1 A2 A3 B}, {C A1 A2 A3 A4 B}。</p>
<ul>
<li>allowCombinations()</li>
</ul>
<p>与 oneOrMore()和 times()一起使用, 并在匹配的事件之间施加非确定性的松散相邻性（如 followedByAny()）。</p>
<p>如果不应用, 则使用松散的相邻性（如 followedBy()）。</p>
<p>例如, 像这样的模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">(</span><span class="s">&#34;start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getName</span><span class="o">().</span><span class="n">equals</span><span class="o">(</span><span class="s">&#34;c&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getName</span><span class="o">().</span><span class="n">equals</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">))</span>
                       <span class="o">.</span><span class="n">oneOrMore</span><span class="o">().</span><span class="n">allowCombinations</span><span class="o">()</span>
  <span class="o">.</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;end1&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getName</span><span class="o">().</span><span class="n">equals</span><span class="o">(</span><span class="s">&#34;b&#34;</span><span class="o">))</span>
</code></pre></div><p>将为一个输入序列生成以下匹配。C D A1 A2 A3 D A4 B</p>
<p>启用组合。{C A1 B}、{C A1 A2 B}、{C A1 A3 B}、{C A1 A4 B}、{C A1 A2 A3 B}、{C A1 A2 A4 B}、{C A1 A3 A4 B}、{C A1 A2 A3 A4 B}、{C A1 A2 A3 A4 B}。</p>
<p>不启用组合。{C A1 B}, {C A1 A2 B}, {C A1 A2 A3 B}, {C A1 A2 A3 A4 B}。</p>
<h3 id="模式组">模式组</h3>
<p>也可以定义一个模式序列作为 begin、followBy、followByAny 和 next 的条件。该模式序列将被视为逻辑上的匹配条件, 并将返回一个 GroupPattern, 并且可以对 GroupPattern 应用 oneOrMore()、times(#ofTimes)、times(#fromTimes、#toTimes)、optional()、continuous()、allowCombinations()。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">start</span><span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">(</span>
    <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...).</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;start_middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>
<span class="o">)</span>

<span class="c1">// strict contiguity
</span><span class="c1"></span><span class="k">val</span> <span class="n">strict</span><span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">next</span><span class="o">(</span>
    <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;next_start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...).</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;next_middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>
<span class="o">).</span><span class="n">times</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>

<span class="c1">// relaxed contiguity
</span><span class="c1"></span><span class="k">val</span> <span class="n">relaxed</span><span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">followedBy</span><span class="o">(</span>
    <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;followedby_start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...).</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;followedby_middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>
<span class="o">).</span><span class="n">oneOrMore</span><span class="o">()</span>

<span class="c1">// non-deterministic relaxed contiguity
</span><span class="c1"></span><span class="k">val</span> <span class="n">nonDetermin</span><span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">followedByAny</span><span class="o">(</span>
    <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;followedbyany_start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...).</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;followedbyany_middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>
<span class="o">).</span><span class="n">optional</span><span class="o">()</span>
</code></pre></div><ul>
<li>begin(#name) - 定义一个起始模式。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">start</span> <span class="k">=</span> <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;start&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>begin(#pattern_sequence) - 定义一个起始模式。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">start</span> <span class="k">=</span> <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">(</span>
    <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...).</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>next(#name) - 添加一个新的模式。一个匹配事件必须直接接替前一个匹配事件（严格相邻）。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">next</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">next</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>next(#pattern_sequence) - 添加一个新的模式。一个匹配事件的序列必须直接接替前一个匹配事件（严格相邻）。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">next</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">next</span><span class="o">(</span>
    <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...).</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>followedBy(#name) - 添加一个新的模式。其他事件可以发生在一个匹配事件和上一个匹配事件之间（松散的相邻性）。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">followedBy</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>followedBy(#pattern_sequence)	- 添加一个新的模式。其他事件可以发生在一系列匹配事件和前一个匹配事件之间（放松的相邻性）。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">followedBy</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">followedBy</span><span class="o">(</span>
    <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...).</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>followedByAny(#name) - 添加一个新的模式。在一个匹配事件和上一个匹配事件之间可以发生其他事件, 并且对每一个备选匹配事件都会呈现备选匹配（非确定性的松散毗连性）。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">followedByAny</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">followedByAny</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>followedByAny(#pattern_sequence) - 添加一个新的模式。在一个匹配事件序列和前一个匹配事件之间可以发生其他事件, 并且将为每一个可供选择的匹配事件序列呈现备选匹配（非确定性的松散毗连性）。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">followedByAny</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">followedByAny</span><span class="o">(</span>
    <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;start&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...).</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(...)</span>
<span class="o">)</span>
</code></pre></div><p>notNext() - 添加一个新的否定模式。一个匹配（负值）事件必须直接接替前一个匹配事件（严格的相邻性）, 以使部分匹配被丢弃。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">notNext</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">notNext</span><span class="o">(</span><span class="s">&#34;not&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>notFollowedBy() - 添加一个新的负模式。即使在匹配（负值）事件和前一个匹配事件之间发生了其他事件, 部分匹配事件序列也会被丢弃（松散的相邻性）。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">notFollowedBy</span> <span class="k">=</span> <span class="n">start</span><span class="o">.</span><span class="n">notFollowedBy</span><span class="o">(</span><span class="s">&#34;not&#34;</span><span class="o">)</span>
</code></pre></div><p>within(time) - 定义事件序列匹配模式的最大时间间隔。如果一个未完成的事件序列超过了这个时间, 它将被丢弃。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">pattern</span><span class="o">.</span><span class="n">within</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">))</span>
</code></pre></div><h3 id="after-match-skip-strategy">After Match Skip Strategy</h3>
<p>对于一个给定的模式, 同一个事件可能会被分配给多个成功的匹配。要控制一个事件将被分配到多少个匹配中, 你需要指定名为 AfterMatchSkipStrategy 的跳过策略。有五种类型的跳过策略, 如下所示。</p>
<ul>
<li>NO_SKIP: 每一个可能的匹配都会被发出。</li>
<li>SKIP_TO_NEXT：丢弃每一个局部的匹配, 从相同的事件开始, 发射匹配开始。</li>
<li>SKIP_PAST_LAST_EVENT: 丢弃每一个在匹配开始后但结束前开始的部分匹配。</li>
<li>SKIP_TO_FIRST: 丢弃每个在匹配开始后但在 PatternName 的第一个事件发生之前开始的部分匹配。</li>
<li>SKIP_TO_LAST: 丢弃在匹配开始后但在 PatternName 的最后一个事件发生之前开始的每一个部分匹配。</li>
</ul>
<p>注意, 当使用 SKIP_TO_FIRST 和 SKIP_TO_LAST 跳过策略时, 还应该指定一个有效的 PatternName。</p>
<p>例如, 对于给定的模式 b+ c 和数据流 b1 b2 b3 c, 这四种跳过策略的区别如下。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Skip Strategy</th>
<th style="text-align:left">结果</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">NO_SKIP</td>
<td style="text-align:left">b1 b2 b3 c <!-- raw HTML omitted --> b2 b3 c <!-- raw HTML omitted --> b3 c</td>
<td style="text-align:left">找到匹配的 b1 b2 b3 c 后, 匹配过程不会丢弃任何结果。</td>
</tr>
<tr>
<td style="text-align:left">SKIP_TO_NEXT</td>
<td style="text-align:left">b1 b2 b3 c <!-- raw HTML omitted --> b2 b3 c <!-- raw HTML omitted --> b3 c</td>
<td style="text-align:left">找到匹配的 b1 b2 b3 c 后, 匹配过程不会丢弃任何结果, 因为没有其他匹配可以从 b1 开始。</td>
</tr>
<tr>
<td style="text-align:left">SKIP_PAST_LAST_EVENT</td>
<td style="text-align:left">b1 b2 b3 c</td>
<td style="text-align:left">在找到匹配的 b1 b2 b3 c 后, 匹配过程将放弃所有开始的部分匹配。</td>
</tr>
<tr>
<td style="text-align:left">SKIP_TO_FIRST[b]</td>
<td style="text-align:left">b1 b2 b3 c <!-- raw HTML omitted --> b2 b3 c <!-- raw HTML omitted --> b3 c</td>
<td style="text-align:left">找到匹配的 b1 b2 b3 c 后, 匹配过程会尝试丢弃所有在 b1 之前开始的部分匹配, 但没有这样的匹配。因此, 没有任何匹配结果会被丢弃。</td>
</tr>
<tr>
<td style="text-align:left">SKIP_TO_LAST[b]</td>
<td style="text-align:left">b1 b2 b3 c <!-- raw HTML omitted --> b3 c</td>
<td style="text-align:left">找到匹配的 b1 b2 b3 c 后, 匹配过程会尝试丢弃所有在 b3 之前开始的部分匹配。有一个这样的匹配 b2 b3 c。</td>
</tr>
</tbody>
</table>
<p>还可以看看另一个例子, 以更好地了解 NO_SKIP 和 SKIP_TO_FIRST 的区别：模式: (a | b | c) (b | c) c+.greedy d 和序列: a b c1 c2 c3 d 那么结果将是:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Skip Strategy</th>
<th style="text-align:left">结果</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">NO_SKIP</td>
<td style="text-align:left">a b c1 c2 c3 d <!-- raw HTML omitted --> b c1 c2 c3 d <!-- raw HTML omitted --> c1 c2 c3 d</td>
<td style="text-align:left">找到匹配的 a b c1 c2 c3 d 后, 匹配过程不会丢弃任何结果。</td>
</tr>
<tr>
<td style="text-align:left">SKIP_TO_FIRST[c*]</td>
<td style="text-align:left">a b c1 c2 c3 d <!-- raw HTML omitted --> c1 c2 c3 d</td>
<td style="text-align:left">在找到匹配的 a b c1 c2 c3 d 后, 匹配过程将丢弃所有在 c1 之前开始的部分匹配, 有一个这样的匹配 b c1 c2 c3 d。有一个这样的匹配 b c1 c2 c3 d。</td>
</tr>
</tbody>
</table>
<p>为了更好地理解 NO_SKIP 和 SKIP_TO_NEXT 的区别, 请看下面的例子: Pattern: a b+ 和序列: a b1 b2 b3 那么结果将是:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Skip Strategy</th>
<th style="text-align:left">结果</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">NO_SKIP</td>
<td style="text-align:left">a b1 <!-- raw HTML omitted --> a b1 b2 <!-- raw HTML omitted --> a b1 b2 b3</td>
<td style="text-align:left">找到匹配的 b1 后, 匹配过程不会丢弃任何结果。</td>
</tr>
<tr>
<td style="text-align:left">SKIP_TO_NEXT</td>
<td style="text-align:left">a b1</td>
<td style="text-align:left">在找到匹配的 b1 后, 匹配过程将丢弃从 a 开始的所有部分匹配, 这意味着既不能生成 b1 b2, 也不能生成 b1 b2 b3。</td>
</tr>
</tbody>
</table>
<p>要指定使用哪种跳过策略, 只需调用 AfterMatchSkipStrategy 来创建一个 AfterMatchSkipStrategy。</p>
<table>
<thead>
<tr>
<th style="text-align:left">功能</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">AfterMatchSkipStrategy.noSkip()</td>
<td style="text-align:left">创建一个 NO_SKIP 跳过策略</td>
</tr>
<tr>
<td style="text-align:left">AfterMatchSkipStrategy.skipToNext()</td>
<td style="text-align:left">创建一个 SKIP_TO_NEXT 跳过策略。</td>
</tr>
<tr>
<td style="text-align:left">AfterMatchSkipStrategy.skipPastLastEvent()</td>
<td style="text-align:left">创建一个 SKIP_PAST_LAST_EVENT 跳过策略。</td>
</tr>
<tr>
<td style="text-align:left">AfterMatchSkipStrategy.skipToFirst(patternName)</td>
<td style="text-align:left">用引用的模式名 patternName 创建一个 SKIP_TO_FIRST 跳过策略。</td>
</tr>
<tr>
<td style="text-align:left">AfterMatchSkipStrategy.skipToLast(patternName)</td>
<td style="text-align:left">用引用的模式名 patternName 创建一个 SKIP_TO_LAST 跳过策略。</td>
</tr>
</tbody>
</table>
<p>然后通过调用跳过策略来应用于模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">skipStrategy</span> <span class="k">=</span> <span class="o">...</span>
<span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">(</span><span class="s">&#34;patternName&#34;</span><span class="o">,</span> <span class="n">skipStrategy</span><span class="o">)</span>
</code></pre></div><p>注意 对于 SKIP_TO_FIRST/LAST 有两个选项来处理没有元素映射到指定变量的情况。默认情况下, 将使用 NO_SKIP 策略。另一个选项是在这种情况下抛出异常。我们可以通过以下方式启用这个选项</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">AfterMatchSkipStrategy</span><span class="o">.</span><span class="n">skipToFirst</span><span class="o">(</span><span class="n">patternName</span><span class="o">).</span><span class="n">throwExceptionOnMiss</span><span class="o">()</span>
</code></pre></div><h2 id="检测模式">检测模式</h2>
<p>在指定了你要寻找的模式序列后, 现在是时候将其应用到你的输入流中以检测潜在的匹配。要针对你的模式序列运行事件流, 你必须创建一个 PatternStream。给定一个输入流输入、一个模式模式和一个可选的比较器比较器, 用于在 EventTime 的情况下对具有相同时间戳的事件或在同一时刻到达的事件进行排序, 你可以通过调用创建 PatternStream。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span> <span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Event</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">pattern</span> <span class="k">:</span> <span class="kt">Pattern</span><span class="o">[</span><span class="kt">Event</span>, <span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">var</span> <span class="n">comparator</span> <span class="k">:</span> <span class="kt">EventComparator</span><span class="o">[</span><span class="kt">Event</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// optional
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">patternStream</span><span class="k">:</span> <span class="kt">PatternStream</span><span class="o">[</span><span class="kt">Event</span><span class="o">]</span> <span class="k">=</span> <span class="nc">CEP</span><span class="o">.</span><span class="n">pattern</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">pattern</span><span class="o">,</span> <span class="n">comparator</span><span class="o">)</span>
</code></pre></div><p>输入流可以是 keyed 的, 也可以是 non-keyed 的, 这取决于你的使用情况。</p>
<p>注意: 在 non-keyed 流上应用模式将导致作业的并行度等于 1。</p>
<h3 id="从模式中选择">从模式中选择</h3>
<p>一旦你获得了一个 PatternStream, 你就可以对检测到的事件序列进行转换。建议的方法是通过 PatternProcessFunction 来实现。</p>
<p>PatternProcessFunction 有一个 processMatch 方法, 它对每个匹配的事件序列都会被调用。它以 <code>Map&lt;String, List&lt;IN&gt;&gt;</code> 的形式接收匹配, 其中键是你的模式序列中每个模式的名称, 值是该模式的所有接受事件的列表（IN 是你的输入元素的类型）。给定模式的事件是按时间戳排序的。返回每个模式所接受的事件列表的原因是, 当使用循环模式(例如 oneToMany() 和 times())时, 一个给定模式可能会接受多个事件。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">MyPatternProcessFunction</span><span class="o">&lt;</span><span class="nc">IN</span><span class="o">,</span> <span class="nc">OUT</span><span class="o">&gt;</span> <span class="k">extends</span> <span class="nc">PatternProcessFunction</span><span class="o">&lt;</span><span class="nc">IN</span><span class="o">,</span> <span class="nc">OUT</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="nd">@Override</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">processMatch</span><span class="o">(</span><span class="nc">Map</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">List</span><span class="o">&lt;</span><span class="nc">IN</span><span class="o">&gt;&gt;</span> <span class="k">match</span><span class="o">,</span> <span class="nc">Context</span> <span class="n">ctx</span><span class="o">,</span> <span class="nc">Collector</span><span class="o">&lt;</span><span class="nc">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="n">throws</span> <span class="nc">Exception</span><span class="o">;</span>
        <span class="nc">IN</span> <span class="n">startEvent</span> <span class="k">=</span> <span class="k">match</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="s">&#34;start&#34;</span><span class="o">).</span><span class="n">get</span><span class="o">(</span><span class="mi">0</span><span class="o">);</span>
        <span class="nc">IN</span> <span class="n">endEvent</span> <span class="k">=</span> <span class="k">match</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="s">&#34;end&#34;</span><span class="o">).</span><span class="n">get</span><span class="o">(</span><span class="mi">0</span><span class="o">);</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">OUT</span><span class="o">(</span><span class="n">startEvent</span><span class="o">,</span> <span class="n">endEvent</span><span class="o">));</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>PatternProcessFunction 提供了对 Context 对象的访问。通过它, 我们可以访问与时间相关的特性, 如当前处理时间或当前匹配的时间戳（这是分配给匹配的最后一个元素的时间戳）。更多信息请看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#time-context">时间上下文</a>。通过这个上下文, 我们还可以将结果发送到一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/side_output.html">侧输出</a>。</p>
<h3 id="处理超时的部分模式">处理超时的部分模式</h3>
<p>当一个模式通过 within 关键字附加了一个窗口长度时, 部分事件序列有可能因为超过窗口长度而被丢弃。要对一个超时的部分匹配采取行动, 可以使用 TimedOutPartialMatchHandler 接口。该接口应该以混搭的方式使用。这意味着你可以在你的 PatternProcessFunction 中额外实现这个接口。TimedOutPartialMatchHandler 提供了额外的 processTimedOutMatch 方法, 该方法将为每个超时部分匹配调用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">MyPatternProcessFunction</span><span class="o">&lt;</span><span class="nc">IN</span><span class="o">,</span> <span class="nc">OUT</span><span class="o">&gt;</span> <span class="k">extends</span> <span class="nc">PatternProcessFunction</span><span class="o">&lt;</span><span class="nc">IN</span><span class="o">,</span> <span class="nc">OUT</span><span class="o">&gt;</span> <span class="n">implements</span> <span class="nc">TimedOutPartialMatchHandler</span><span class="o">&lt;</span><span class="nc">IN</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="nd">@Override</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">processMatch</span><span class="o">(</span><span class="nc">Map</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">List</span><span class="o">&lt;</span><span class="nc">IN</span><span class="o">&gt;&gt;</span> <span class="k">match</span><span class="o">,</span> <span class="nc">Context</span> <span class="n">ctx</span><span class="o">,</span> <span class="nc">Collector</span><span class="o">&lt;</span><span class="nc">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="n">throws</span> <span class="nc">Exception</span><span class="o">;</span>
        <span class="o">...</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">processTimedOutMatch</span><span class="o">(</span><span class="nc">Map</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">List</span><span class="o">&lt;</span><span class="nc">IN</span><span class="o">&gt;&gt;</span> <span class="k">match</span><span class="o">,</span> <span class="nc">Context</span> <span class="n">ctx</span><span class="o">)</span> <span class="n">throws</span> <span class="nc">Exception</span><span class="o">;</span>
        <span class="nc">IN</span> <span class="n">startEvent</span> <span class="k">=</span> <span class="k">match</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="s">&#34;start&#34;</span><span class="o">).</span><span class="n">get</span><span class="o">(</span><span class="mi">0</span><span class="o">);</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">output</span><span class="o">(</span><span class="n">outputTag</span><span class="o">,</span> <span class="n">T</span><span class="o">(</span><span class="n">startEvent</span><span class="o">));</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>注意: processTimedOutMatch 不给人访问主输出的机会。但你仍然可以通过 Context 对象, 通过<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/side_output.html">侧输出</a>来发出结果。</p>
<h4 id="方便的-api">方便的 API</h4>
<p>前面提到的 PatternProcessFunction 是在 Flink 1.8 中引入的, 从那时起, 它就是推荐的与匹配交互的方式。人们仍然可以使用老式的 API, 比如 select/flatSelect, 内部会被翻译成 PatternProcessFunction。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">patternStream</span><span class="k">:</span> <span class="kt">PatternStream</span><span class="o">[</span><span class="kt">Event</span><span class="o">]</span> <span class="k">=</span> <span class="nc">CEP</span><span class="o">.</span><span class="n">pattern</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">pattern</span><span class="o">)</span>

<span class="k">val</span> <span class="n">outputTag</span> <span class="k">=</span> <span class="nc">OutputTag</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;side-output&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">SingleOutputStreamOperator</span><span class="o">[</span><span class="kt">ComplexEvent</span><span class="o">]</span> <span class="k">=</span> <span class="n">patternStream</span><span class="o">.</span><span class="n">flatSelect</span><span class="o">(</span><span class="n">outputTag</span><span class="o">){</span>
    <span class="o">(</span><span class="n">pattern</span><span class="k">:</span> <span class="kt">Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Iterable</span><span class="o">[</span><span class="kt">Event</span><span class="o">]],</span> <span class="n">timestamp</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">TimeoutEvent</span><span class="o">])</span> <span class="k">=&gt;</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">TimeoutEvent</span><span class="o">())</span>
<span class="o">}</span> <span class="o">{</span>
    <span class="o">(</span><span class="n">pattern</span><span class="k">:</span> <span class="kt">mutable.Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Iterable</span><span class="o">[</span><span class="kt">Event</span><span class="o">]],</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">ComplexEvent</span><span class="o">])</span> <span class="k">=&gt;</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">ComplexEvent</span><span class="o">())</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">timeoutResult</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">TimeoutEvent</span><span class="o">]</span> <span class="k">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getSideOutput</span><span class="o">(</span><span class="n">outputTag</span><span class="o">)</span>
</code></pre></div><h2 id="在-cep-库中的时间">在 CEP 库中的时间</h2>
<h3 id="处理事件时间的延迟">处理事件时间的延迟</h3>
<p>在 CEP 中, 处理元素的顺序很重要。为了保证元素在事件时间工作时以正确的顺序进行处理, 一个传入的元素最初会被放在一个缓冲区中, 在这个缓冲区中, 元素根据其时间戳按升序排序, 当一个水印到达时, 这个缓冲区中所有时间戳小于水印的元素都会被处理。这意味着水印之间的元素是按事件时间顺序处理的。</p>
<p>注意: 当在事件时间内工作时, 该库假定水印的正确性。</p>
<p>为了保证水印之间的元素按事件时间顺序处理, Flink 的 CEP 库假设水印的正确性, 并将时间戳小于最后看到的水印的元素视为迟到元素。迟到的元素不会被进一步处理。另外, 你可以指定一个 sideOutput 标签来收集最后一次看到的水印之后的迟到元素, 你可以这样使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">patternStream</span><span class="k">:</span> <span class="kt">PatternStream</span><span class="o">[</span><span class="kt">Event</span><span class="o">]</span> <span class="k">=</span> <span class="nc">CEP</span><span class="o">.</span><span class="n">pattern</span><span class="o">(</span><span class="n">input</span><span class="o">,</span> <span class="n">pattern</span><span class="o">)</span>

<span class="k">val</span> <span class="n">lateDataOutputTag</span> <span class="k">=</span> <span class="nc">OutputTag</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;late-data&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">SingleOutputStreamOperator</span><span class="o">[</span><span class="kt">ComplexEvent</span><span class="o">]</span> <span class="k">=</span> <span class="n">patternStream</span>
      <span class="o">.</span><span class="n">sideOutputLateData</span><span class="o">(</span><span class="n">lateDataOutputTag</span><span class="o">)</span>
      <span class="o">.</span><span class="n">select</span><span class="o">{</span>
          <span class="n">pattern</span><span class="k">:</span> <span class="kt">Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Iterable</span><span class="o">[</span><span class="kt">ComplexEvent</span><span class="o">]]</span> <span class="k">=&gt;</span> <span class="nc">ComplexEvent</span><span class="o">()</span>
      <span class="o">}</span>

<span class="k">val</span> <span class="n">lateData</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getSideOutput</span><span class="o">(</span><span class="n">lateDataOutputTag</span><span class="o">)</span>
</code></pre></div><h3 id="时间上下文">时间上下文</h3>
<p>在 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#selecting-from-patterns">PatternProcessFunction</a> 以及 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#conditions">IterativeCondition</a> 中, 用户可以访问一个实现 TimeContext 的上下文, 如下所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="cm">/**
</span><span class="cm"> * Enables access to time related characteristics such as current processing time or timestamp of
</span><span class="cm"> * currently processed element. Used in {@link PatternProcessFunction} and
</span><span class="cm"> * {@link org.apache.flink.cep.pattern.conditions.IterativeCondition}
</span><span class="cm"> */</span>
<span class="nd">@PublicEvolving</span>
<span class="kd">public</span> <span class="kd">interface</span> <span class="nc">TimeContext</span> <span class="o">{</span>

	<span class="cm">/**
</span><span class="cm">	 * Timestamp of the element currently being processed.
</span><span class="cm">	 *
</span><span class="cm">	 * &lt;p&gt;In case of {@link org.apache.flink.streaming.api.TimeCharacteristic#ProcessingTime} this
</span><span class="cm">	 * will be set to the time when event entered the cep operator.
</span><span class="cm">	 */</span>
	<span class="kt">long</span> <span class="nf">timestamp</span><span class="o">();</span>

	<span class="cm">/** Returns the current processing time. */</span>
	<span class="kt">long</span> <span class="nf">currentProcessingTime</span><span class="o">();</span>
<span class="o">}</span>
</code></pre></div><p>这个上下文让用户可以访问处理事件的时间特征（在 IterativeCondition 的情况下是传入记录, 在 PatternProcessFunction 的情况下是匹配）。调用 TimeContext#currentProcessingTime 总是给你当前处理时间的值, 这个调用应该比调用 System.currentTimeMillis()更可取。</p>
<p>在 TimeContext#timestamp() 的情况下, 返回的值等于 EventTime 中分配的时间戳。在 ProcessingTime 中, 这将等于所述事件进入 cep 运算符的时间点(或者在 PatternProcessFunction 的情况下生成匹配时)。这意味着该值将在对该方法的多次调用中保持一致。</p>
<h2 id="例子">例子</h2>
<p>下面的例子是在事件的键控数据流上检测模式 start, middle(name = &ldquo;error&rdquo;) -&gt; end(name = &ldquo;critical&rdquo;)。这些事件通过其 id 进行 keyed, 一个有效的模式必须在 10 秒内出现。整个处理过程是以事件时间来完成的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">:</span> <span class="kt">StreamExecutionEnvironment</span> <span class="o">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">input</span> <span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Event</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">partitionedInput</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="n">event</span> <span class="k">=&gt;</span> <span class="n">event</span><span class="o">.</span><span class="n">getId</span><span class="o">)</span>

<span class="k">val</span> <span class="n">pattern</span> <span class="k">=</span> <span class="nc">Pattern</span><span class="o">.</span><span class="n">begin</span><span class="o">[</span><span class="kt">Event</span><span class="o">](</span><span class="s">&#34;start&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">next</span><span class="o">(</span><span class="s">&#34;middle&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getName</span> <span class="o">==</span> <span class="s">&#34;error&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">followedBy</span><span class="o">(</span><span class="s">&#34;end&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getName</span> <span class="o">==</span> <span class="s">&#34;critical&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">within</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">))</span>

<span class="k">val</span> <span class="n">patternStream</span> <span class="k">=</span> <span class="nc">CEP</span><span class="o">.</span><span class="n">pattern</span><span class="o">(</span><span class="n">partitionedInput</span><span class="o">,</span> <span class="n">pattern</span><span class="o">)</span>

<span class="k">val</span> <span class="n">alerts</span> <span class="k">=</span> <span class="n">patternStream</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">createAlert</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>
</code></pre></div><h2 id="从旧版本13-前迁移到-14-以上版本">从旧版本(1.3 前)迁移到 1.4 以上版本</h2>
<h3 id="迁移到-14-版本">迁移到 1.4+ 版本</h3>
<p>在 Flink-1.4 中, CEP 库与 &lt;= Flink 1.2 的向后兼容性被取消。不幸的是, 无法恢复曾经在 1.2.x 下运行的 CEP 作业。</p>
<h3 id="迁移到-13x">迁移到 1.3.x</h3>
<p>Flink-1.3 中的 CEP 库有很多新的特性, 这导致了 API 的一些变化。在这里, 我们描述了为了能够在 Flink-1.3 中运行, 你需要对你的旧 CEP 作业进行的修改。在做了这些改变并重新编译你的作业后, 你将能够从旧版作业的保存点恢复执行, 也就是说, 不需要重新处理你过去的数据。</p>
<p>所需的更改是:</p>
<ul>
<li>
<p>改变你的条件（在 where(&hellip;) 子句中的条件）来扩展 SimpleCondition 类, 而不是实现 FilterFunction 接口。</p>
</li>
<li>
<p>改变你的函数作为参数提供给 select(&hellip;) 和 flatSelect(&hellip;) 方法, 以期望与每个模式相关联的事件列表(Java 中为 List, Scala 中为 Iterable)。这是因为增加了循环模式后, 多个输入事件可以匹配一个（循环）模式。</p>
</li>
<li>
<p>Flink 1.1 和 1.2 中的 followBy() 暗示了非确定性的松散毗连性（见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html#conditions-on-contiguity">这里</a>）。在 Flink 1.3 中, 这一点发生了变化, followBy() 意味着松散毗连, 而 followByAny() 应该在需要非确定性松散毗连的情况下使用。</p>
</li>
</ul>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[执行模式(批/流)]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-12-10-execution-mode/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-application-building-block/?utm_source=atom_feed" rel="related" type="text/html" title="Application Building Blocks" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-logical-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Logical Functions" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-python-walkthrough/?utm_source=atom_feed" rel="related" type="text/html" title="Python 演练" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-sdk/?utm_source=atom_feed" rel="related" type="text/html" title="Sdk" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-distributed-architecture/?utm_source=atom_feed" rel="related" type="text/html" title="分布式架构" />
            
                <id>https://ohmyweekly.github.io/notes/2020-12-10-execution-mode/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-12-10T00:00:00+08:00</published>
            <updated>2020-12-10T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Execution Mode(Batch/Streaming)</blockquote><h1 id="执行模式流批">执行模式(流/批)</h1>
<p>DataStream API 支持不同的运行时执行模式，你可以根据用例的要求和作业的特点从中选择。</p>
<p>DataStream API 有一种&quot;经典&quot;的执行行为，我们称之为 <strong>STREAMING</strong> 执行模式。这应该用于需要连续增量处理并预计无限期保持在线的无边界作业。</p>
<p>此外，还有一种批式执行模式，我们称之为 <strong>BATCH</strong> 执行模式。这种执行作业的方式更容易让人联想到批处理框架，如 MapReduce。这应该用于有边界的作业，对于这些作业，你有一个已知的固定输入，并且不会连续运行。</p>
<p>Apache Flink 对流和批处理的统一方法意味着，无论配置何种执行模式，在有界输入上执行的 DataStream 应用都会产生相同的最终结果。重要的是要注意这里的 final 是什么意思：在 streaming 模式下执行的作业可能会产生增量更新（想想数据库中的 upserts），而 batch 作业在最后只会产生一个最终结果。如果解释正确的话，最终的结果是一样的，但是到达那里的方式可能是不同的。</p>
<p>通过启用 <strong>BATCH</strong> 执行，我们允许 Flink 应用额外的优化，而这些优化只有在我们知道我们的输入是有边界的情况下才能进行。例如，可以使用不同的 join/aggregation 策略，此外还可以使用不同的 shuffle 实现，允许更高效的任务调度和故障恢复行为。下面我们将介绍一些执行行为的细节。</p>
<h2 id="什么时候可以应该使用-batch-执行模式">什么时候可以/应该使用 BATCH 执行模式？</h2>
<p><strong>BATCH</strong> 执行模式只能用于有边界的 Job/Link 程序。边界性是数据源的一个属性，它告诉我们在执行之前，来自该数据源的所有输入是否都是已知的，或者是否会有新的数据出现，可能是无限的。而一个作业，如果它的所有源都是有界的，则是有界的，否则就是无界的。</p>
<p>另一方面，<strong>STREAMING</strong> 执行模式既可以用于有界作业，也可以用于无界作业。</p>
<p>作为经验法则，当你的程序是有界的时候，你应该使用 <strong>BATCH</strong> 执行模式，因为这样会更有效率。当你的程序是无边界的时候，你必须使用 <strong>STREAMING</strong> 执行模式，因为只有这种模式足够通用，能够处理连续的数据流。</p>
<p>一个明显的例外情况是，当你想使用一个有界作业来引导一些作业状态，然后你想在一个无界作业中使用。例如，通过使用 <strong>STREAMING</strong> 模式运行一个有界作业，取一个保存点，然后在一个无界作业上恢复该保存点。这是一个非常特殊的用例，当我们允许将保存点作为 <strong>BATCH</strong> 执行作业的额外输出时，这个用例可能很快就会过时。</p>
<p>另一个可能使用 <strong>STREAMING</strong> 模式运行有边界作业的情况是为最终将在无边界源中运行的代码编写测试时。对于测试来说，在这些情况下使用有界源可能更自然。</p>
<h2 id="配置-batch-执行模式">配置 BATCH 执行模式</h2>
<p>执行模式可以通过 <code>execute.runtim-mode</code> 设置来配置。有三种可能的值:</p>
<ul>
<li><strong>STREAMING</strong>: 经典的 DataStream 执行模式(默认)</li>
<li><strong>BATCH</strong>: 在 DataStream API 上进行批量式执行</li>
<li><strong>AUTOMATIC</strong>：让系统根据源的边界性来决定</li>
</ul>
<p>这可以通过 <code>bin/flink run ...</code> 的命令行参数进行配置，或者在创建/配置 StreamExecutionEnvironment 时进行编程。</p>
<p>下面是如何通过命令行配置执行模式:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ bin/flink run -Dexecution.runtime-mode<span class="o">=</span>BATCH examples/streaming/WordCount.jar
</code></pre></div><p>这个例子展示了如何在代码中配置执行模式:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>
<span class="n">env</span><span class="o">.</span><span class="na">setRuntimeMode</span><span class="o">(</span><span class="n">RuntimeExecutionMode</span><span class="o">.</span><span class="na">BATCH</span><span class="o">);</span>
</code></pre></div><blockquote>
<p>注意：我们建议用户不要在程序中设置运行模式，而是在提交应用程序时使用命令行进行设置。保持应用程序代码的免配置可以让程序更加灵活，因为同一个应用程序可以在任何执行模式下执行。</p>
</blockquote>
<h2 id="执行行为">执行行为</h2>
<p>本节概述了 BATCH 执行模式的执行行为，并与 STREAMING 执行模式进行了对比。详细内容请参考介绍该功能的 <a href="https://cwiki.apache.org/confluence/x/4i94CQ">FLIP-134</a> 和 <a href="https://cwiki.apache.org/confluence/x/kDh4CQ">FLIP-140</a>。</p>
<h3 id="任务调度和网络洗牌shuffle">任务调度和网络洗牌(Shuffle)</h3>
<p>Flink 作业(job)由不同的操作(operation)组成，这些操作在数据流图中连接在一起。系统决定如何在不同的进程/机器（TaskManager）上安排这些操作的执行，以及如何在它们之间洗牌（发送）数据。</p>
<p>多个操作/运算符可以使用一种称为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/#task-chaining-and-resource-groups">链式</a>的功能链在一起。Flink 认为作为调度单位的一组一个或多个（链式）运算符(operators )被称为任务(<em>task</em>)。通常，子任务(subtask)一词用来指在多个 TaskManager 上并行运行的单个任务实例，但我们在这里只使用任务(task)一词。</p>
<p>任务调度和网络洗牌对于 <strong>BATCH</strong> 和 <strong>STREAMING</strong> 执行模式的工作方式不同。主要是由于我们知道我们的输入数据在 <strong>BATCH</strong> 执行模式下是有边界的，这使得 Flink 可以使用更高效的数据结构和算法。</p>
<p>我们将用这个例子来解释任务调度和网络传输的差异。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

<span class="n">DataStreamSource</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">source</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(...);</span>

<span class="n">source</span><span class="o">.</span><span class="na">name</span><span class="o">(</span><span class="s">&#34;source&#34;</span><span class="o">)</span>
	<span class="o">.</span><span class="na">map</span><span class="o">(...).</span><span class="na">name</span><span class="o">(</span><span class="s">&#34;map1&#34;</span><span class="o">)</span>
	<span class="o">.</span><span class="na">map</span><span class="o">(...).</span><span class="na">name</span><span class="o">(</span><span class="s">&#34;map2&#34;</span><span class="o">)</span>
	<span class="o">.</span><span class="na">rebalance</span><span class="o">()</span>
	<span class="o">.</span><span class="na">map</span><span class="o">(...).</span><span class="na">name</span><span class="o">(</span><span class="s">&#34;map3&#34;</span><span class="o">)</span>
	<span class="o">.</span><span class="na">map</span><span class="o">(...).</span><span class="na">name</span><span class="o">(</span><span class="s">&#34;map4&#34;</span><span class="o">)</span>
	<span class="o">.</span><span class="na">keyBy</span><span class="o">((</span><span class="n">value</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">value</span><span class="o">)</span>
	<span class="o">.</span><span class="na">map</span><span class="o">(...).</span><span class="na">name</span><span class="o">(</span><span class="s">&#34;map5&#34;</span><span class="o">)</span>
	<span class="o">.</span><span class="na">map</span><span class="o">(...).</span><span class="na">name</span><span class="o">(</span><span class="s">&#34;map6&#34;</span><span class="o">)</span>
	<span class="o">.</span><span class="na">sinkTo</span><span class="o">(...).</span><span class="na">name</span><span class="o">(</span><span class="s">&#34;sink&#34;</span><span class="o">);</span>
</code></pre></div><p>暗示操作之间1对1连接模式的操作，如 <code>map()</code>、<code>flatMap()</code> 或 <code>filter()</code>，可以直接将数据转发到下一个操作，这使得这些操作可以链在一起。这意味着 Flink 通常不会在它们之间插入网络洗牌。</p>
<p>而 <code>keyBy()</code> 或 <code>rebalance()</code> 等操作则需要在不同的任务并行实例之间进行数据洗牌。这就会引起网络洗牌。</p>
<p>对于上面的例子，Flink 会把操作分组为这样的任务。</p>
<ul>
<li>Task1: source, map1 和 map2</li>
<li>Task2: map3, map4</li>
<li>Task3: map5, map6 和 sink</li>
</ul>
<p>而我们在任务1和2，以及任务2和3之间进行网络洗牌。这是该作业的可视化表示。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/datastream-example-job-graph.svg" alt="img"></p>
<h3 id="streaming-执行模式">STREAMING 执行模式</h3>
<p>在 <strong>STREAMING</strong> 执行模式下，所有任务需要一直在线/运行。这使得 Flink 可以通过整个管道立即处理新的记录，而我们需要的是连续和低延迟的流处理。这也意味着分配给一个任务的 TaskManagers 需要有足够的资源来同时运行所有的任务。</p>
<p>网络洗牌是流水线式的，这意味着记录会被立即发送到下游任务，并在网络层上进行一些缓冲。同样，这也是需要的，因为当处理连续的数据流时，在任务（或任务的管道）之间没有自然的数据点（时间点）可以物化。这与 <strong>BATCH</strong> 执行模式形成了鲜明的对比，在 <strong>BATCH</strong> 执行模式下，中间的结果可以被具体化，如下所述。</p>
<h3 id="batch-执行模式">BATCH 执行模式</h3>
<p>在 <strong>BATCH</strong> 执行模式下，一个作业的任务可以被分离成可以一个接一个执行的阶段。我们之所以能做到这一点，是因为输入是有边界的，因此 Flink 可以在进入下一个阶段之前完全处理管道的一个阶段。在上面的例子中，工作会有三个阶段，对应着被洗牌障碍分开的三个任务。</p>
<p>分阶段处理并不是像上面针对 STREAMING 模式所解释的那样，立即向下游任务发送记录，而是需要 Flink 将任务的中间结果物化到一些非永续存储中，让下游任务在上游任务已经下线后再读取。这将增加处理的延迟，但也会带来其他有趣的特性。首先，这允许 Flink 在故障发生时回溯到最新的可用结果，而不是重新启动整个任务。另一个副作用是，BATCH 作业可以在更少的资源上执行（就 TaskManagers 的可用槽而言），因为系统可以一个接一个地顺序执行任务。</p>
<p>TaskManagers 将至少在下游任务没有消耗它们的情况下保留中间结果。(从技术上讲，它们将被保留到消耗的流水线区域产生它们的输出为止)。在这之后，只要空间允许，它们就会被保留，以便在失败的情况下，可以回溯到前面提到的结果。</p>
<h3 id="状态后端状态">状态后端/状态</h3>
<p>在 STREAMING 模式下，Flink 使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state_backends.html">StateBackend</a> 来控制状态的存储方式和检查点的工作方式。</p>
<p>在 BATCH 模式下，配置的状态后端被忽略。取而代之的是，keyed 操作的输入按键分组（使用排序），然后我们依次处理一个键的所有记录。这样就可以同时只保留一个键的状态。当转到下一个键时，一个给定键的状态将被丢弃。</p>
<p>关于这方面的背景信息，请参见 <a href="https://cwiki.apache.org/confluence/x/kDh4CQ">FLIP-140</a>。</p>
<h3 id="事件时间水印">事件时间/水印</h3>
<p>在支持<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_time.html">事件时间</a>方面，Flink 的流运行时建立在一个悲观的假设上，即事件可能会出现顺序外，即一个时间戳t的事件可能会在一个时间戳t+1的事件之后出现。正因为如此，系统永远无法确定在给定的时间戳T下，未来不会再有时间戳 <code>t&lt;T</code> 的元素出现。为了摊平这种失序性对最终结果的影响，同时使系统实用，在 STREAMING 模式下，Flink 使用了一种名为 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/timely-stream-processing.html#event-time-and-watermarks">Watermarks</a> 的启发式方法。一个带有时间戳T的水印标志着没有时间戳 <code>t&lt;T</code> 的元素会跟随。</p>
<p>在 BATCH 模式下，输入的数据集是事先已知的，不需要这样的启发式，因为至少可以按照时间戳对元素进行排序，从而按照时间顺序进行处理。对于熟悉流的读者来说，在 BATCH 中，我们可以假设&quot;完美的水印&quot;。</p>
<p>鉴于上述情况，在 BATCH 模式下，我们只需要在输入的末尾有一个与每个键相关的 MAX_WATERMARK，如果输入流没有键，则在输入的末尾有一个。基于这个方案，所有注册的定时器都会在时间结束时触发，用户定义的 WatermarkAssigners 或 WatermarkStrategies 会被忽略。</p>
<h3 id="处理时间">处理时间</h3>
<p>处理时间是指在处理记录的具体实例上，处理记录的机器上的挂钟时间。根据这个定义，我们看到，基于处理时间的计算结果是不可重复的。这是因为同一条记录被处理两次，会有两个不同的时间戳。</p>
<p>尽管如此，在 STREAMING 模式下使用处理时间还是很有用的。原因与流媒体管道经常实时摄取其无限制的输入有关，所以事件时间和处理时间之间存在相关性。此外，由于上述原因，在 STREAMING 模式下，事件时间的1h往往可以几乎是1h的处理时间，也就是挂钟时间。所以使用处理时间可以用于早期（不完全）发射，给出预期结果的提示。</p>
<p>在批处理世界中，这种相关性并不存在，因为在批处理世界中，输入的数据集是静态的，是预先知道的。鉴于此，在 BATCH 模式中，我们允许用户请求当前的处理时间，并注册处理时间计时器，但与事件时间的情况一样，所有的计时器都要在输入结束时发射。</p>
<p>在概念上，我们可以想象，在作业执行过程中，处理时间不会提前，当整个输入处理完毕后，我们会快进到时间结束。</p>
<h3 id="故障恢复">故障恢复</h3>
<p>在 STREAMING 执行模式下，Flink 使用检查点进行故障恢复。请看一下<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/checkpointing.html">检查点文档</a>，了解关于这个和如何配置它的实践文档。关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/fault_tolerance.html">通过状态快照进行容错</a>，也有一个比较入门的章节，从更高的层面解释了这些概念。</p>
<p>Checkpointing 用于故障恢复的特点之一是，Flink 在发生故障时，会从检查点重新启动所有正在运行的任务。这可能比我们在 BATCH 模式下所要做的事情更昂贵（如下文所解释），这也是如果你的任务允许的话应该使用 BATCH 执行模式的原因之一。</p>
<p>在 BATCH 执行模式下，Flink 会尝试并回溯到之前的处理阶段，对于这些阶段，仍然有中间结果。潜在地，只有失败的任务（或它们在图中的前辈）才需要重新启动，与从检查点重新启动所有任务相比，可以提高作业的处理效率和整体处理时间。</p>
<h3 id="重要的考虑因素">重要的考虑因素</h3>
<p>与经典的 STREAMING 执行模式相比，在 BATCH 模式下，有些东西可能无法按照预期工作。一些功能的工作方式会略有不同，而其他功能则不支持。</p>
<p>BATCH 模式下的行为变化。</p>
<ul>
<li>&ldquo;滚动&quot;操作，如 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/#reduce"><code>reduce()</code></a> 或 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/#aggregations"><code>sum()</code></a>，会对 STREAMING 模式下每一条新记录发出增量更新。在 BATCH 模式下，这些操作不是&quot;滚动&rdquo;。它们只发出最终结果。</li>
</ul>
<p>BATCH 模式下不支持的:</p>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/stateful-stream-processing.html#stateful-stream-processing">Checkpointing</a> 和任何依赖于 checkpointing 的操作都不工作。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/broadcast_state.html">广播状态</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/#iterate">迭代</a></li>
</ul>
<p>自定义操作符应谨慎执行，否则可能会有不恰当的行为。更多细节请参见下面的补充说明。</p>
<h3 id="检查点">检查点</h3>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/datastream_execution_mode.html#failure-recovery">如上</a>所述，批处理程序的故障恢复不使用检查点。</p>
<p>重要的是要记住，因为没有检查点，某些功能如 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/api/common/state/CheckpointListener.html">CheckpointListener</a>，以及因此，Kafka 的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#kafka-producers-and-fault-tolerance">EXACTLY_ONCE</a> 模式或 StreamingFileSink 的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/streamfile_sink.html#rolling-policy">OnCheckpointRollingPolicy</a> 将无法工作。如果你需要一个在 BATCH 模式下工作的事务型接收器，请确保它使用 <a href="https://cwiki.apache.org/confluence/x/KEJ4CQ">FLIP-143</a> 中提出的统一接收器 API。</p>
<p>你仍然可以使用所有的状态<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state.html#working-with-state">原语</a>，只是用于故障恢复的机制会有所不同。</p>
<h3 id="广播状态">广播状态</h3>
<p>引入这个特性是为了让用户实现这样的用例：一个&quot;控制&quot;流需要被广播到所有下游任务，而广播的元素，例如规则，需要应用到另一个流的所有输入元素。</p>
<p>在这种模式下，Flink 不提供关于读取输入的顺序的保证。像上面这样的用例在流媒体世界中是有意义的，因为在这个世界中，作业预计会运行很长时间，而输入数据是事先不知道的。在这些设置中，需求可能会随着时间的推移而改变，这取决于输入的数据。</p>
<p>但在批处理世界中，我们认为这种用例没有太大意义，因为输入（包括元素和控制流）是静态的，而且是预先知道的。</p>
<p>我们计划在未来为BATCH处理支持这种模式的变化，即完全先处理广播端。</p>
<h3 id="编写自定义操作符">编写自定义操作符</h3>
<blockquote>
<p>注意：自定义操作符是 Apache Flink 的一种高级使用模式。对于大多数的使用情况，可以考虑使用(keyed-)过程函数来代替。</p>
</blockquote>
<p>在编写自定义操作符时，记住 BATCH 执行模式的假设是很重要的。否则，一个在 STREAMING 模式下运行良好的操作符可能会在 BATCH 模式下产生错误的结果。操作符永远不会被限定在一个特定的键上，这意味着他们看到了 Flink 试图利用的 BATCH 处理的一些属性。</p>
<p>首先你不应该在一个操作符内缓存最后看到的水印。在 BATCH 模式下，我们会逐个键处理记录。因此，水印会在每个键之间从 MAX_VALUE 切换到 MIN_VALUE。你不应该认为水印在一个操作符中总是上升的。出于同样的原因，定时器将首先按键的顺序发射，然后按每个键内的时间戳顺序发射。此外，不支持手动更改键的操作。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Application Building Blocks]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-12-02-application-building-block/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-logical-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Logical Functions" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-python-walkthrough/?utm_source=atom_feed" rel="related" type="text/html" title="Python 演练" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-sdk/?utm_source=atom_feed" rel="related" type="text/html" title="Sdk" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-distributed-architecture/?utm_source=atom_feed" rel="related" type="text/html" title="分布式架构" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-custom-serializer/?utm_source=atom_feed" rel="related" type="text/html" title="Custom Serializer" />
            
                <id>https://ohmyweekly.github.io/notes/2020-12-02-application-building-block/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-12-02T00:00:00+08:00</published>
            <updated>2020-12-02T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Application Building Block</blockquote><h2 id="应用构件">应用构件</h2>
<p>Stateful Functions 为构建事件驱动应用程序提供了一个框架。在这里，我们将解释 Stateful Function 架构的重要方面。</p>
<h2 id="事件输入">事件输入</h2>
<p>有状态函数应用正好坐在事件驱动的领域，所以自然要从把事件摄入系统开始。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/statefun-app-ingress.svg" alt="img"></p>
<p>在有状态函数中，将记录摄入系统的组件称为事件入口。这可以是任何东西，从 Kafka 主题，到 messsage 队列，再到 http 请求 - 任何能够将数据引入系统并触发初始函数开始计算的东西。</p>
<h2 id="有状态函数">有状态函数</h2>
<p>该图的核心是命名的有状态函数。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/statefun-app-functions.svg" alt="img"></p>
<p>把这些函数看作是你的服务的构件。它们可以任意地相互发送消息，这也是这个框架摆脱传统的流处理观点的一种方式。这些函数可以以任意的、可能是循环的、甚至是往返的方式相互通信，而不是建立一个静态的数据流 DAG。</p>
<p>如果你熟悉 actor 编程，这在组件之间动态消息的能力上确实有某些相似之处。然而，也有一些显著的区别。</p>
<h2 id="持续状态">持续状态</h2>
<p>首先是所有函数都有本地嵌入的状态，即所谓的持久化状态。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/statefun-app-state.svg" alt="img"></p>
<p>Apache Flink 的核心优势之一就是它能够提供容错的本地状态。当在一个函数内部，当它在执行一些计算时，你总是在本地变量中处理本地状态。</p>
<h2 id="容错">容错</h2>
<p>对于状态和消息传递，Stateful Functions 能够提供用户从现代数据处理框架中期望的精确的一次保证。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/statefun-app-fault-tolerance.svg" alt="img"></p>
<p>在失败的情况下，整个世界的状态（包括持久化的状态和消息）都会被回滚，以模拟完全无故障的执行。</p>
<p>这些保证是在不需要数据库的情况下提供的，相反，Stateful Function 利用了 Apache Flink 的成熟快照机制。</p>
<h2 id="事件出口">事件出口</h2>
<p>最后，应用程序可以通过事件出口向外部系统输出数据。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/statefun-app-egress.svg" alt="img"></p>
<p>当然，函数执行任意计算，可以随心所欲，这包括进行 RPC 调用和连接到其他系统。通过使用事件出口，应用程序可以利用建立在 Apache Flink 连接器生态系统之上的预建集成。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Logical Functions]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-12-02-logical-functions/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-application-building-block/?utm_source=atom_feed" rel="related" type="text/html" title="Application Building Blocks" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-python-walkthrough/?utm_source=atom_feed" rel="related" type="text/html" title="Python 演练" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-sdk/?utm_source=atom_feed" rel="related" type="text/html" title="Sdk" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-distributed-architecture/?utm_source=atom_feed" rel="related" type="text/html" title="分布式架构" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-custom-serializer/?utm_source=atom_feed" rel="related" type="text/html" title="Custom Serializer" />
            
                <id>https://ohmyweekly.github.io/notes/2020-12-02-logical-functions/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-12-02T00:00:00+08:00</published>
            <updated>2020-12-02T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Logical Functions</blockquote><h2 id="逻辑函数">逻辑函数</h2>
<p>有状态函数是以逻辑方式分配的，这意味着系统可以用有限的资源支持无限制的实例数量。逻辑实例在不被主动调用时不使用CPU、内存或线程，所以理论上没有可以创建的实例数量上限。我们鼓励用户根据对其应用最合理的情况，尽可能地对其应用进行细化建模，而不是围绕资源限制来设计应用。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/address.svg" alt="img"></p>
<h2 id="函数地址">函数地址</h2>
<p>在本地环境中，对象的地址和对它的引用是一样的。但是在有状态函数应用程序中，函数实例是虚拟的，它们的运行位置不会暴露给用户。相反，地址是用来引用系统中特定的有状态函数的。</p>
<p>一个地址由两个组件组成，一个是 FunctionType，一个是 ID。函数类型类似于面向对象语言中的类，它声明了地址所引用的函数类型。ID 是一个主键，它将函数调用的范围限定在函数类型的一个特定实例上。</p>
<p>当一个函数被调用时，所有的操作&ndash;包括对持久化状态的读和写 - 都会被限定在当前地址上。</p>
<p>例如，想象有一个有状态函数应用程序来跟踪仓库的库存。一个可能的实现可以包括一个库存函数，它可以跟踪一个特定物品的库存单位数量；这将是函数类型。然后，仓库管理的每个 SKU 都会有一个该类型的逻辑实例。如果是服装，可能会有一个衬衫的实例和另一个裤子的实例；&ldquo;衬衫&quot;和 &ldquo;裤子&quot;将是两个 ID。每个实例都可以独立地进行交互和消息传递。应用程序可以根据库存中物品的类型自由创建实例。</p>
<h2 id="函数生命周期">函数生命周期</h2>
<p>逻辑函数既不被创建也不被销毁，而是在应用程序的整个生命周期中始终存在。当应用程序启动时，框架的每个并行工作者将为每个函数类型创建一个物理对象。这个对象将用于执行该类型的所有逻辑实例，该类型的逻辑实例由该特定的 worker 运行。第一次向一个地址发送消息时，它将像该实例一直存在一样，其持久化状态为空。</p>
<p>清除一个类型的所有持久化状态与销毁它是一样的。如果一个实例没有状态，也没有主动运行，那么它就不占用 CPU，不占用线程，也不占用内存。</p>
<p>一个实例在其一个或多个持久化值中存储了数据，它只占用存储该数据所需的资源。状态存储由 Apache Flink 运行时管理，并存储在配置的状态后端。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Python 演练]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-12-02-python-walkthrough/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-application-building-block/?utm_source=atom_feed" rel="related" type="text/html" title="Application Building Blocks" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-logical-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Logical Functions" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-sdk/?utm_source=atom_feed" rel="related" type="text/html" title="Sdk" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-distributed-architecture/?utm_source=atom_feed" rel="related" type="text/html" title="分布式架构" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-custom-serializer/?utm_source=atom_feed" rel="related" type="text/html" title="Custom Serializer" />
            
                <id>https://ohmyweekly.github.io/notes/2020-12-02-python-walkthrough/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-12-02T00:00:00+08:00</published>
            <updated>2020-12-02T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Python Walkthrough</blockquote><h2 id="python-演练">Python 演练</h2>
<p>Stateful Functions 为构建健壮的、有状态的事件驱动的应用程序提供了一个平台。它提供了对状态和时间的精细控制，这使得高级系统的实现成为可能。在本步骤指南中，您将学习如何使用 Stateful Functions API 构建有状态的应用程序。</p>
<h2 id="你要构建什么">你要构建什么？</h2>
<p>就像软件中所有伟大的介绍一样，这个演练将从开头开始：打招呼。该应用程序将运行一个简单的函数，该函数将接受一个请求并以问候语进行响应。它不会试图涵盖所有复杂的应用程序开发，而是专注于构建一个有状态的函数 - 这是你实现业务逻辑的地方。</p>
<h2 id="先决条件">先决条件</h2>
<p>这个演练假设您对 Python 有一定的了解，但即使您来自不同的编程语言，您也应该能够跟上。</p>
<h2 id="帮助我卡住了">帮助，我卡住了</h2>
<p>如果你被卡住了，请查看<a href="https://flink.apache.org/gettinghelp.html">社区支持资源</a>。特别是 Apache Flink 的<a href="https://flink.apache.org/community.html#mailing-lists">用户邮件列表</a>，一直被认为是 Apache 项目中最活跃的一个，也是快速获得帮助的好方法。</p>
<h2 id="如何跟进">如何跟进</h2>
<p>如果你想跟上，你需要一台装有 <a href="https://www.python.org/">Python 3</a> 以及 <a href="https://www.docker.com/">Docker</a> 的电脑。</p>
<blockquote>
<p>注意：为了简洁起见，本演练中的每个代码块可能不包含完整的周边类。完整的代码可以在<a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/getting-started/python_walkthrough.html#full-application">本页底部</a>找到。</p>
</blockquote>
<p>你可以通过点击<a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/downloads/walkthrough.zip">这里</a>下载一个包含骨架项目的 zip 文件。</p>
<p>解压包后，你会发现一些文件。这些文件包括 dockerfiles 和数据生成器，用于在本地自包含环境中运行此演练。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ tree statefun-walkthrough
statefun-walkthrough
├── Dockerfile
├── docker-compose.yml
├── generator
│   ├── Dockerfile
│   ├── event-generator.py
│   └── messages_pb2.py
├── greeter
│   ├── Dockerfile
│   ├── greeter.py
│   ├── messages.proto
│   ├── messages_pb2.py
│   └── requirements.txt
└── module.yaml
</code></pre></div><h2 id="从事件开始">从事件开始</h2>
<p>Stateful Functions 是一个事件驱动的系统，所以开发从定义我们的事件开始。问候者应用程序将使用<a href="https://developers.google.com/protocol-buffers">协议缓冲区</a>定义其事件。当一个特定用户的问候请求被摄入时，它将被路由到相应的函数。响应将返回一个适当的问候。第三种类型，SeenCount，是一个实用类，后期将用于帮助管理用户到目前为止被看到的次数。</p>
<pre><code>syntax = &quot;proto3&quot;;

package example;

// External request sent by a user who wants to be greeted
message GreetRequest {
    // The name of the user to greet
    string name = 1;
}
// A customized response sent to the user
message GreetResponse {
    // The name of the user being greeted
    string name = 1;
    // The users customized greeting
    string greeting = 2;
}
// An internal message used to store state
message SeenCount {
    // The number of times a users has been seen so far
    int64 seen = 1;
}
</code></pre><h2 id="我们的第一个函数">我们的第一个函数</h2>
<p>在底层，消息是使用<a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/sdk/python.html">有状态的函数</a>来处理的，也就是任何绑定到 <code>StatefulFunction</code> 运行时的两个参数函数。函数用 <code>@function.bind</code> 装饰器绑定到运行时。当绑定一个函数时，它会被注解为一个函数类型。这是在向这个函数发送消息时用来引用它的名称。</p>
<p>当你打开文件 <code>greeter/greeter.py</code> 时，你应该看到以下代码。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">statefun</span> <span class="kn">import</span> <span class="n">StatefulFunctions</span>

<span class="n">functions</span> <span class="o">=</span> <span class="n">StatefulFunctions</span><span class="p">()</span>

<span class="nd">@functions</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="s2">&#34;example/greeter&#34;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">greet</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">greet_request</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div><p>一个有状态函数需要两个参数，即上下文和消息。<a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/sdk/python.html#context-reference">上下文</a>提供了对有状态函数运行时功能的访问，如状态管理和消息传递。您将在本演练中探索其中的一些功能。</p>
<p>另一个参数是传递给这个函数的输入消息。默认情况下，消息是以 protobuf <a href="https://developers.google.com/protocol-buffers/docs/reference/python-generated#wkt">Any</a> 的形式传递的。如果一个函数只接受一个已知的类型，你可以使用 <a href="https://www.python.org/">Python 3</a> 类型语法覆盖消息类型。这样您就不需要对消息进行拆包或检查类型。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">messages_pb2</span> <span class="kn">import</span> <span class="n">GreetRequest</span>
<span class="kn">from</span> <span class="nn">statefun</span> <span class="kn">import</span> <span class="n">StatefulFunctions</span>

<span class="n">functions</span> <span class="o">=</span> <span class="n">StatefulFunctions</span><span class="p">()</span>

<span class="nd">@functions</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="s2">&#34;example/greeter&#34;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">greet</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">greet_request</span><span class="p">:</span> <span class="n">GreetRequest</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div><h2 id="发送回复">发送回复</h2>
<p>有状态函数接受消息，也可以将消息发送出去。消息可以被发送到其他函数，以及外部系统（或<a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/io-module/index.html#egress">出口</a>）。</p>
<p>一个流行的外部系统是 <a href="http://kafka.apache.org/">Apache Kafka</a>。第一步，让我们更新 <code>greeter/greeter.py</code> 中的函数，通过向 Kafka 主题发送问候语来响应每个输入。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">messages_pb2</span> <span class="kn">import</span> <span class="n">GreetRequest</span><span class="p">,</span> <span class="n">GreetResponse</span>
<span class="kn">from</span> <span class="nn">statefun</span> <span class="kn">import</span> <span class="n">StatefulFunctions</span>

<span class="n">functions</span> <span class="o">=</span> <span class="n">StatefulFunctions</span><span class="p">()</span>

<span class="nd">@functions</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="s2">&#34;example/greeter&#34;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">greet</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">greet_request</span><span class="p">:</span> <span class="n">GreetRequest</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">GreetResponse</span><span class="p">()</span>
    <span class="n">response</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">greet_request</span><span class="o">.</span><span class="n">name</span>
    <span class="n">response</span><span class="o">.</span><span class="n">greeting</span> <span class="o">=</span> <span class="s2">&#34;Hello </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">greet_request</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    
    <span class="n">egress_message</span> <span class="o">=</span> <span class="n">kafka_egress_record</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="s2">&#34;greetings&#34;</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">greet_request</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
    <span class="n">context</span><span class="o">.</span><span class="n">pack_and_send_egress</span><span class="p">(</span><span class="s2">&#34;example/greets&#34;</span><span class="p">,</span> <span class="n">egress_message</span><span class="p">)</span>
</code></pre></div><p>对于每条消息，都会构造一个响应，并发送到一个名为 <code>greetings</code> 的 Kafka 主题，该主题按名称分区。<code>egress_message</code> 被发送到一个名为 <code>example/greets</code> 的出口。这个标识符指向一个特定的 Kafka 集群，并在下面的部署中进行配置。</p>
<h2 id="一个有状态的-hello">一个有状态的 Hello</h2>
<p>这是一个很好的开端，但并没有展现出有状态函数的真正威力 - 与状态一起工作。假设你想根据每个用户发送请求的次数，为他们生成个性化的响应。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">compute_greeting</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">seen</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    Compute a personalized greeting, based on the number of times this @name had been seen before.
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">templates</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="s2">&#34;Welcome </span><span class="si">%s</span><span class="s2">&#34;</span><span class="p">,</span> <span class="s2">&#34;Nice to see you again </span><span class="si">%s</span><span class="s2">&#34;</span><span class="p">,</span> <span class="s2">&#34;Third time is a charm </span><span class="si">%s</span><span class="s2">&#34;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">seen</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">templates</span><span class="p">):</span>
        <span class="n">greeting</span> <span class="o">=</span> <span class="n">templates</span><span class="p">[</span><span class="n">seen</span><span class="p">]</span> <span class="o">%</span> <span class="n">name</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">greeting</span> <span class="o">=</span> <span class="s2">&#34;Nice to see you at the </span><span class="si">%d</span><span class="s2">-nth time </span><span class="si">%s</span><span class="s2">!&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">seen</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">GreetResponse</span><span class="p">()</span>
    <span class="n">response</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">response</span><span class="o">.</span><span class="n">greeting</span> <span class="o">=</span> <span class="n">greeting</span>

    <span class="k">return</span> <span class="n">response</span>
</code></pre></div><p>为了&quot;记住&quot;多条问候信息，你需要将一个持久化的值域（ <code>seen_count</code> ）关联到 <code>Greet</code> 函数。对于每个用户，函数现在可以跟踪他们被看到的次数。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="nd">@functions</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="s2">&#34;example/greeter&#34;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">greet</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">greet_request</span><span class="p">:</span> <span class="n">GreetRequest</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="s1">&#39;seen_count&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="n">SeenCount</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">state</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">SeenCount</span><span class="p">()</span>
        <span class="n">state</span><span class="o">.</span><span class="n">seen</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">state</span><span class="o">.</span><span class="n">seen</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">context</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="s1">&#39;seen_count&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">compute_greeting</span><span class="p">(</span><span class="n">greet_request</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">seen</span><span class="p">)</span>

    <span class="n">egress_message</span> <span class="o">=</span> <span class="n">kafka_egress_record</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="s2">&#34;greetings&#34;</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">greet_request</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
    <span class="n">context</span><span class="o">.</span><span class="n">pack_and_send_egress</span><span class="p">(</span><span class="s2">&#34;example/greets&#34;</span><span class="p">,</span> <span class="n">egress_message</span><span class="p">)</span>
</code></pre></div><p>状态 <code>seen_count</code> 始终是当前名称的范围，因此它可以独立地跟踪每个用户。</p>
<h2 id="连接在一起">连接在一起</h2>
<p>有状态的 Function 应用程序使用 http 与 Apache Flink 运行时进行通信。Python SDK 提供了一个 RequestReplyHandler，它可以基于 RESTful HTTP POSTS 自动分配函数调用。RequestReplyHandler 可以使用任何 HTTP 框架暴露。</p>
<p>一个流行的 Python web 框架是 <a href="https://palletsprojects.com/p/flask/">Flask</a>。它可以用来快速、轻松地将应用程序暴露给 Apache Flink 运行时。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">statefun</span> <span class="kn">import</span> <span class="n">StatefulFunctions</span>
<span class="kn">from</span> <span class="nn">statefun</span> <span class="kn">import</span> <span class="n">RequestReplyHandler</span>

<span class="n">functions</span> <span class="o">=</span> <span class="n">StatefulFunctions</span><span class="p">()</span>

<span class="nd">@functions</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="s2">&#34;example/greeter&#34;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">greeter</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="n">GreetRequest</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="n">handler</span> <span class="o">=</span> <span class="n">RequestReplyHandler</span><span class="p">(</span><span class="n">functions</span><span class="p">)</span>

<span class="c1"># Serve the endpoint</span>

<span class="kn">from</span> <span class="nn">flask</span> <span class="kn">import</span> <span class="n">request</span>
<span class="kn">from</span> <span class="nn">flask</span> <span class="kn">import</span> <span class="n">make_response</span>
<span class="kn">from</span> <span class="nn">flask</span> <span class="kn">import</span> <span class="n">Flask</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">Flask</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="nd">@app</span><span class="o">.</span><span class="n">route</span><span class="p">(</span><span class="s1">&#39;/statefun&#39;</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;POST&#39;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">handle</span><span class="p">():</span>
    <span class="n">response_data</span> <span class="o">=</span> <span class="n">handler</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">make_response</span><span class="p">(</span><span class="n">response_data</span><span class="p">)</span>
    <span class="n">response</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s1">&#39;Content-Type&#39;</span><span class="p">,</span> <span class="s1">&#39;application/octet-stream&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
    <span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</code></pre></div><h2 id="配置运行时">配置运行时</h2>
<p>有状态函数运行时通过向 Flask 服务器进行 http 调用来向 <code>greeter</code> 函数发出请求。要做到这一点，它需要知道它可以使用什么端点来到达服务器。这也是配置我们连接到输入和输出 Kafka 主题的好时机。配置在一个名为 module.yaml 的文件中。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;1.0&#34;</span><span class="w">
</span><span class="w"></span><span class="nt">module</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">meta</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">remote</span><span class="w">
</span><span class="w">  </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">functions</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">function</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">meta</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">            </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">example/greeter</span><span class="w">
</span><span class="w">          </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l">http://python-worker:8000/statefun</span><span class="w">
</span><span class="w">            </span><span class="nt">states</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="l">seen_count</span><span class="w">
</span><span class="w">            </span><span class="nt">maxNumBatchRequests</span><span class="p">:</span><span class="w"> </span><span class="m">500</span><span class="w">
</span><span class="w">            </span><span class="nt">timeout</span><span class="p">:</span><span class="w"> </span><span class="l">2min</span><span class="w">
</span><span class="w">    </span><span class="nt">ingresses</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">ingress</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">meta</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">statefun.kafka.io/routable-protobuf-ingress</span><span class="w">
</span><span class="w">            </span><span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="l">example/names</span><span class="w">
</span><span class="w">          </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">address</span><span class="p">:</span><span class="w"> </span><span class="l">kafka-broker:9092</span><span class="w">
</span><span class="w">            </span><span class="nt">consumerGroupId</span><span class="p">:</span><span class="w"> </span><span class="l">my-group-id</span><span class="w">
</span><span class="w">            </span><span class="nt">topics</span><span class="p">:</span><span class="w">
</span><span class="w">              </span>- <span class="nt">topic</span><span class="p">:</span><span class="w"> </span><span class="l">names</span><span class="w">
</span><span class="w">                </span><span class="nt">typeUrl</span><span class="p">:</span><span class="w"> </span><span class="l">com.googleapis/example.GreetRequest</span><span class="w">
</span><span class="w">                </span><span class="nt">targets</span><span class="p">:</span><span class="w">
</span><span class="w">                  </span>- <span class="l">example/greeter</span><span class="w">
</span><span class="w">    </span><span class="nt">egresses</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">egress</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">meta</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">statefun.kafka.io/generic-egress</span><span class="w">
</span><span class="w">            </span><span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="l">example/greets</span><span class="w">
</span><span class="w">          </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">address</span><span class="p">:</span><span class="w"> </span><span class="l">kafka-broker:9092</span><span class="w">
</span><span class="w">            </span><span class="nt">deliverySemantic</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">exactly-once</span><span class="w">
</span><span class="w">              </span><span class="nt">transactionTimeoutMillis</span><span class="p">:</span><span class="w"> </span><span class="m">100000</span><span class="w">
</span></code></pre></div><p>这个配置做了一些有趣的事情。</p>
<p>首先是声明我们的函数 <code>example/greeter</code>。它包括它可以到达的端点以及函数可以访问的状态。</p>
<p><code>ingress</code> 是将 <code>GreetRequest</code> 消息路由到函数的输入 Kafka 主题。除了 broker 地址和消费者组等基本属性，它还包含一个目标列表。这些是每个消息将被发送到的函数。</p>
<p>出口是输出的 Kafka 集群。它包含 broker 特定的配置，但允许每个消息路由到任何主题。</p>
<h2 id="部署">部署</h2>
<p>现在已经构建了 <code>greeter</code> 应用程序，是时候部署了。部署 Stateful Function 应用程序最简单的方法是使用社区提供的基础映像并加载你的模块。基础镜像提供了 Stateful Function 运行时，它将使用提供的 module.yaml 来为这个特定的工作进行配置。这可以在根目录下的 Docker 文件中找到。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">FROM flink-statefun:2.2.0

RUN mkdir -p /opt/statefun/modules/greeter
ADD module.yaml /opt/statefun/modules/greeter
</code></pre></div><p>现在您可以使用提供的 Docker 设置在本地运行此应用程序。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ docker-compose up -d
</code></pre></div><p>那么，要想在行动中看到例子，就看看话题问候出来的内容。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">docker-compose logs -f event-generator 
</code></pre></div><h2 id="想更进一步">想更进一步？</h2>
<p>这个 Greeter 永远不会忘记一个用户。试着修改这个函数，使它能够为任何没有与系统交互的用户花超过60秒的时间重置 <code>seen_count</code>。</p>
<p>查看 <a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/sdk/python.html">Python SDK</a> 页面以获得更多关于如何实现这一功能的信息。</p>
<h2 id="完整应用">完整应用</h2>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">messages_pb2</span> <span class="kn">import</span> <span class="n">SeenCount</span><span class="p">,</span> <span class="n">GreetRequest</span><span class="p">,</span> <span class="n">GreetResponse</span>

<span class="kn">from</span> <span class="nn">statefun</span> <span class="kn">import</span> <span class="n">StatefulFunctions</span>
<span class="kn">from</span> <span class="nn">statefun</span> <span class="kn">import</span> <span class="n">RequestReplyHandler</span>
<span class="kn">from</span> <span class="nn">statefun</span> <span class="kn">import</span> <span class="n">kafka_egress_record</span>

<span class="n">functions</span> <span class="o">=</span> <span class="n">StatefulFunctions</span><span class="p">()</span>

<span class="nd">@functions</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="s2">&#34;example/greeter&#34;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">greet</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">greet_request</span><span class="p">:</span> <span class="n">GreetRequest</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="s1">&#39;seen_count&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="n">SeenCount</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">state</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">SeenCount</span><span class="p">()</span>
        <span class="n">state</span><span class="o">.</span><span class="n">seen</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">state</span><span class="o">.</span><span class="n">seen</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">context</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="s1">&#39;seen_count&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">compute_greeting</span><span class="p">(</span><span class="n">greet_request</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">seen</span><span class="p">)</span>

    <span class="n">egress_message</span> <span class="o">=</span> <span class="n">kafka_egress_record</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="s2">&#34;greetings&#34;</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">greet_request</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
    <span class="n">context</span><span class="o">.</span><span class="n">pack_and_send_egress</span><span class="p">(</span><span class="s2">&#34;example/greets&#34;</span><span class="p">,</span> <span class="n">egress_message</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">compute_greeting</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">seen</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    Compute a personalized greeting, based on the number of times this @name had been seen before.
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">templates</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="s2">&#34;Welcome </span><span class="si">%s</span><span class="s2">&#34;</span><span class="p">,</span> <span class="s2">&#34;Nice to see you again </span><span class="si">%s</span><span class="s2">&#34;</span><span class="p">,</span> <span class="s2">&#34;Third time is a charm </span><span class="si">%s</span><span class="s2">&#34;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">seen</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">templates</span><span class="p">):</span>
        <span class="n">greeting</span> <span class="o">=</span> <span class="n">templates</span><span class="p">[</span><span class="n">seen</span><span class="p">]</span> <span class="o">%</span> <span class="n">name</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">greeting</span> <span class="o">=</span> <span class="s2">&#34;Nice to see you at the </span><span class="si">%d</span><span class="s2">-nth time </span><span class="si">%s</span><span class="s2">!&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">seen</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">GreetResponse</span><span class="p">()</span>
    <span class="n">response</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">response</span><span class="o">.</span><span class="n">greeting</span> <span class="o">=</span> <span class="n">greeting</span>

    <span class="k">return</span> <span class="n">response</span>


<span class="n">handler</span> <span class="o">=</span> <span class="n">RequestReplyHandler</span><span class="p">(</span><span class="n">functions</span><span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># Serve the endpoint</span>
<span class="c1">#</span>

<span class="kn">from</span> <span class="nn">flask</span> <span class="kn">import</span> <span class="n">request</span>
<span class="kn">from</span> <span class="nn">flask</span> <span class="kn">import</span> <span class="n">make_response</span>
<span class="kn">from</span> <span class="nn">flask</span> <span class="kn">import</span> <span class="n">Flask</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">Flask</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="nd">@app</span><span class="o">.</span><span class="n">route</span><span class="p">(</span><span class="s1">&#39;/statefun&#39;</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;POST&#39;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">handle</span><span class="p">():</span>
    <span class="n">response_data</span> <span class="o">=</span> <span class="n">handler</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">make_response</span><span class="p">(</span><span class="n">response_data</span><span class="p">)</span>
    <span class="n">response</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s1">&#39;Content-Type&#39;</span><span class="p">,</span> <span class="s1">&#39;application/octet-stream&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
    <span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</code></pre></div>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Sdk]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-12-02-sdk/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-application-building-block/?utm_source=atom_feed" rel="related" type="text/html" title="Application Building Blocks" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-logical-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Logical Functions" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-python-walkthrough/?utm_source=atom_feed" rel="related" type="text/html" title="Python 演练" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-distributed-architecture/?utm_source=atom_feed" rel="related" type="text/html" title="分布式架构" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-custom-serializer/?utm_source=atom_feed" rel="related" type="text/html" title="Custom Serializer" />
            
                <id>https://ohmyweekly.github.io/notes/2020-12-02-sdk/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-12-02T00:00:00+08:00</published>
            <updated>2020-12-02T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Sdk</blockquote><h2 id="sdk">SDK</h2>
<p>有状态函数应用程序由一个或多个模块组成。一个模块是一个由运行时加载的函数捆绑，并提供给消息。来自所有加载模块的函数都是多路复用的，并且可以自由地相互发送消息。</p>
<p>有状态函数支持两种类型的模块。远程模块和嵌入式模块。</p>
<h2 id="远程模块">远程模块</h2>
<p>远程模块作为 Apache Flink® 运行时的外部进程运行；在同一容器中，作为 sidecar，使用无服务器平台或其他外部位置。这种模块类型可以支持任何数量的语言 SDK。远程模块通过 YAML 配置文件在系统中注册。</p>
<h2 id="技术指标">技术指标</h2>
<p>一个远程模块配置由一个元部分和一个规范部分组成。<code>meta</code> 包含了模块的辅助信息；而 <code>spec</code> 则描述了模块中包含的功能并定义了它们的持久值。</p>
<h2 id="定义函数">定义函数</h2>
<p>module.spec.functions 声明了一个由远程模块实现的函数对象列表。一个函数通过一些属性来描述。</p>
<ul>
<li>function.meta.kind
<ul>
<li>用于与远程功能通信的协议。</li>
<li>所支持的值 - http</li>
</ul>
</li>
<li>function.meta.type
<ul>
<li>函数类型那个, 被定义为 <code>&lt;namespace&gt;/&lt;name&gt;</code>。</li>
</ul>
</li>
<li>function.spec.endpoint
<ul>
<li>函数可到达的端点。</li>
<li>所支持的 schemes: http, https.</li>
<li>使用 http+unix 或 https+unix 方案支持通过 UNIX 域套接字进行传输。</li>
<li>当使用 UNIX 域套接字时，端点格式是: <code>http+unix://&lt;socket-file-path&gt;/&lt;serve-url-path&gt;</code>。例如, <code>http+unix:///uds.sock/path/of/url</code>。</li>
</ul>
</li>
<li>function.spec.states
<ul>
<li>在远程函数中声明的持久化值的列表</li>
<li>每个条目由 <code>name</code> 属性和可选的 <code>expireAfter</code> 属性组成。</li>
<li>expireAfter 的默认值为 0，表示状态过期被禁用。</li>
</ul>
</li>
<li>function.spec.maxNumBatchRequests
<ul>
<li>在调用系统背压之前，一个函数可以处理的特定地址的最大记录数。</li>
<li>默认值：1000</li>
</ul>
</li>
<li>function.spec.timeout
<ul>
<li>运行时在失败前等待远程函数返回的最长时间。这涵盖了整个调用过程，包括连接到函数端点、编写请求、函数处理和读取响应。</li>
<li>默认值：1分钟</li>
</ul>
</li>
<li>function.spec.connectTimeout
<ul>
<li>运行时等待连接到远程函数端点的最长时间。</li>
<li>默认值：10秒。</li>
</ul>
</li>
<li>function.spec.readTimeout
<ul>
<li>运行时等待单个读IO操作的最大时间，如读取调用响应。</li>
<li>默认值：10秒。</li>
</ul>
</li>
<li>function.spec.writeTimeout
<ul>
<li>运行时等待单个写IO操作的最大时间，比如写调用请求。</li>
<li>默认值：10秒。</li>
</ul>
</li>
</ul>
<h2 id="完整示例">完整示例</h2>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;2.0&#34;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">module</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">meta</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">remote</span><span class="w">
</span><span class="w">  </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">functions</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">function</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">meta</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">          </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">example/greeter</span><span class="w">
</span><span class="w">        </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">endpoint</span><span class="p">:</span><span class="w"> </span><span class="l">http://&lt;host-name&gt;/statefun</span><span class="w">
</span><span class="w">          </span><span class="nt">states</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">seen_count</span><span class="w">
</span><span class="w">              </span><span class="nt">expireAfter</span><span class="p">:</span><span class="w"> </span><span class="l">5min</span><span class="w">
</span><span class="w">          </span><span class="nt">maxNumBatchRequests</span><span class="p">:</span><span class="w"> </span><span class="m">500</span><span class="w">
</span><span class="w">          </span><span class="nt">timeout</span><span class="p">:</span><span class="w"> </span><span class="l">2min</span><span class="w">
</span></code></pre></div><h2 id="嵌入式模块">嵌入式模块</h2>
<p>嵌入式模块与 Apache Flink® 运行时共存，并嵌入其中。</p>
<p>这种模块类型只支持基于 JVM 的语言，并通过实现 StatefulFunctionModule 接口来定义。嵌入模块提供了一个单一的配置方法，有状态的函数根据其<a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/concepts/logical.html#function-address">函数类型</a>与系统绑定。运行时配置可以通过 globalConfiguration 来实现，它是应用程序 flink-conf.yaml 中前缀 statefun.module.global-config 下的所有配置以及以 <code>--key value</code> 形式传递的任何命令行参数的联合。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">package</span> <span class="nn">org.apache.flink.statefun.docs</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">java.util.Map</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.statefun.sdk.spi.StatefulFunctionModule</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">BasicFunctionModule</span> <span class="kd">implements</span> <span class="n">StatefulFunctionModule</span> <span class="o">{</span>

	<span class="kd">public</span> <span class="kt">void</span> <span class="nf">configure</span><span class="o">(</span><span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">globalConfiguration</span><span class="o">,</span> <span class="n">Binder</span> <span class="n">binder</span><span class="o">)</span> <span class="o">{</span>

		<span class="c1">// Declare the user function and bind it to its type
</span><span class="c1"></span>		<span class="n">binder</span><span class="o">.</span><span class="na">bindFunctionProvider</span><span class="o">(</span><span class="n">FnWithDependency</span><span class="o">.</span><span class="na">TYPE</span><span class="o">,</span> <span class="k">new</span> <span class="n">CustomProvider</span><span class="o">());</span>

		<span class="c1">// Stateful functions that do not require any configuration
</span><span class="c1"></span>		<span class="c1">// can declare their provider using java 8 lambda syntax
</span><span class="c1"></span>		<span class="n">binder</span><span class="o">.</span><span class="na">bindFunctionProvider</span><span class="o">(</span><span class="n">Identifiers</span><span class="o">.</span><span class="na">HELLO_TYPE</span><span class="o">,</span> <span class="n">unused</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">FnHelloWorld</span><span class="o">());</span>
	<span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>嵌入式模块利用 <a href="https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html">Java 的服务提供者接口</a>（SPI）进行发现。这意味着每个 JAR 都应该在 <code>META_INF/services</code> 资源目录下包含一个文件 <code>org.apache.flink.statefun.sdk.spi.StatefulFunctionModule</code>，该文件列出了它提供的所有可用模块。</p>
<pre><code>org.apache.flink.statefun.docs.BasicFunctionModule
</code></pre>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[分布式架构]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-12-02-distributed-architecture/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-application-building-block/?utm_source=atom_feed" rel="related" type="text/html" title="Application Building Blocks" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-logical-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Logical Functions" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-python-walkthrough/?utm_source=atom_feed" rel="related" type="text/html" title="Python 演练" />
                <link href="https://ohmyweekly.github.io/notes/2020-12-02-sdk/?utm_source=atom_feed" rel="related" type="text/html" title="Sdk" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-custom-serializer/?utm_source=atom_feed" rel="related" type="text/html" title="Custom Serializer" />
            
                <id>https://ohmyweekly.github.io/notes/2020-12-02-distributed-architecture/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-12-02T00:00:00+08:00</published>
            <updated>2020-12-02T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Distributed Architecture</blockquote><h2 id="分布式架构">分布式架构</h2>
<p>一个有状态的 Functions 部署是由几个组件交互在一起组成的。在这里，我们将描述这些组件及其相互之间的关系和 Apache Flink 运行时。</p>
<h2 id="高层视图">高层视图</h2>
<p>一个 Stateful Functions 部署由一组 Apache Flink Stateful Functions 进程和可选的执行远程函数的各种部署组成。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/arch_overview.svg" alt="img"></p>
<p>Flink Worker 进程（TaskManagers）从入口系统（Kafka、Kinesis 等）接收事件并将其路由到目标函数。它们调用函数并将产生的消息路由到下一个各自的目标函数。指定用于出口的消息被写入出口系统（同样，Kafka、Kinesis&hellip;）。</p>
<h2 id="组成部分">组成部分</h2>
<p>繁重的工作由 Apache Flink 进程完成，它管理状态，处理消息传递，并调用有状态的函数。Flink 集群通常由一个主进程和多个工作者（TaskManagers）组成。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/arch_components.svg" alt="img"></p>
<p>除了 Apache Flink 进程，完整的部署还需要 <a href="https://zookeeper.apache.org/">ZooKeeper</a>（用于主站<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/jobmanager_high_availability.html">故障转移</a>）和批量存储（S3、HDFS、NAS、GCS、Azure Blob Store等）来存储 Flink 的<a href="https://ci.apache.org/projects/flink/flink-docs-master/concepts/stateful-stream-processing.html#checkpointing">检查点</a>。而部署时不需要数据库，Flink 进程也不需要持久化卷。</p>
<h2 id="逻辑同位物理分离">逻辑同位，物理分离</h2>
<p>许多流处理器的一个核心原则是，应用逻辑和应用状态必须是共位的。这种方法是它们开箱即用的一致性的基础。Stateful Functions 采用了一种独特的方法，在逻辑上将状态和计算共置，但允许在物理上将它们分开。</p>
<ul>
<li>
<p>逻辑上的共置。消息传递、状态访问/更新和函数调用被紧密地管理在一起，与 Flink 的 DataStream API 的方式相同。状态按键分片，消息按键路由到状态。每个 key 一次有一个写入器，也是对函数调用进行调度。</p>
</li>
<li>
<p>物理分离。函数可以远程执行，消息和状态访问作为调用请求的一部分。这样，函数就可以像无状态进程一样独立管理。</p>
</li>
</ul>
<h2 id="函数的部署风格">函数的部署风格</h2>
<p>有状态的函数本身可以以不同的方式部署，这些方式可以相互交换某些特性：一方面是松散的耦合和独立的扩展，另一方面是性能开销。每个函数模块可以是不同的种类，所以有些函数可以远程运行，而有些函数可以嵌入式运行。</p>
<h3 id="远程函数">远程函数</h3>
<p>远程功能采用上述物理分离的原则，同时保持逻辑上的同位。状态/消息层（即 Flink 进程）和功能层是独立部署、管理和扩展的。</p>
<p>功能调用通过 HTTP/gRPC 协议发生，并通过服务将调用请求路由到任何可用的端点，例如 Kubernetes（负载平衡）服务、Lambda 的 AWS 请求网关等。因为调用是自足的（包含消息、状态、访问计时器等），所以目标函数可以像任何无状态的应用程序一样对待。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/arch_funs_remote.svg" alt="img"></p>
<p>详情请参考 <a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/sdk/python.html">Python SDK</a> 和<a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/sdk/index.html#remote-module">远程模块</a>的文档。</p>
<h3 id="共置函数">共置函数</h3>
<p>部署函数的另一种方式是与 Flink JVM 进程共处。在这样的设置中，每个 Flink TaskManager 将与坐在&quot;旁边&quot;的一个 Function 进程对话。一种常见的方式是使用 Kubernetes 这样的系统，部署由 Flink 容器和 Function 侧车容器组成的 pod；两者通过 pod-local 网络进行通信。</p>
<p>这种模式支持不同的语言，同时避免了要通过 Service/LoadBalancer 来路由调用，但它不能独立扩展状态和计算部分。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/arch_funs_colocated.svg" alt="img"></p>
<p>这种部署方式类似于 Flink 的 Table API 和 API Beam 的可移植层部署和执行非 JVM 函数的方式。</p>
<h3 id="嵌入式函数">嵌入式函数</h3>
<p>嵌入式函数类似于 Stateful Functions 1.0 的执行模式，也类似于 Flink 的 Java/Scala 流处理 API。函数在 JVM 中运行，直接调用消息和状态访问。这是最有性能的方式，不过代价是只支持 JVM 语言。函数的更新意味着更新 Flink 集群。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.2/fig/concepts/arch_funs_embedded.svg" alt="img"></p>
<p>按照数据库的类比，嵌入式 Functions 有点像存储程序，但方式更有原则。这里的函数是实现标准接口的普通 Java/Scala/Kotlin 函数，可以在任何 IDE 中开发/测试。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Custom Serializer]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-custom-serializer/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Read and Write" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Streaming" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 方言" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 集成 - 概览" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-custom-serializer/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Custom Serializer</blockquote><h1 id="为你的-flink-程序注册一个自定义的序列器">为你的 Flink 程序注册一个自定义的序列器</h1>
<p>如果你在 Flink 程序中使用的自定义类型不能被 Flink 类型序列化器序列化，Flink 就会回到使用通用的 Kryo 序列化器。你可以用 Kryo 注册你自己的序列化器或像 Google Protobuf 或 Apache Thrift 这样的序列化系统。要做到这一点，只需在 Flink 程序的 ExecutionConfig 中注册类型类和序列化器。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

<span class="c1">// register the class of the serializer as serializer for a type
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">getConfig</span><span class="o">().</span><span class="na">registerTypeWithKryoSerializer</span><span class="o">(</span><span class="n">MyCustomType</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">MyCustomSerializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

<span class="c1">// register an instance as serializer for a type
</span><span class="c1"></span><span class="n">MySerializer</span> <span class="n">mySerializer</span> <span class="o">=</span> <span class="k">new</span> <span class="n">MySerializer</span><span class="o">();</span>
<span class="n">env</span><span class="o">.</span><span class="na">getConfig</span><span class="o">().</span><span class="na">registerTypeWithKryoSerializer</span><span class="o">(</span><span class="n">MyCustomType</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">mySerializer</span><span class="o">);</span>
</code></pre></div><p>请注意，你的自定义序列化器必须扩展 Kryo 的序列化器类。在 Google Protobuf 或 Apache Thrift 的情况下，这已经为你完成了。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

<span class="c1">// register the Google Protobuf serializer with Kryo
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">getConfig</span><span class="o">().</span><span class="na">registerTypeWithKryoSerializer</span><span class="o">(</span><span class="n">MyCustomType</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">ProtobufSerializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

<span class="c1">// register the serializer included with Apache Thrift as the standard serializer
</span><span class="c1">// TBaseSerializer states it should be initialized as a default Kryo serializer
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">getConfig</span><span class="o">().</span><span class="na">addDefaultKryoSerializer</span><span class="o">(</span><span class="n">MyCustomType</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">TBaseSerializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</code></pre></div><p>为了使上面的例子有效，你需要在 Maven 项目文件(pom.xml)中加入必要的依赖关系。在依赖关系部分，为 Apache Thrift 添加以下内容。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
	<span class="nt">&lt;groupId&gt;</span>com.twitter<span class="nt">&lt;/groupId&gt;</span>
	<span class="nt">&lt;artifactId&gt;</span>chill-thrift<span class="nt">&lt;/artifactId&gt;</span>
	<span class="nt">&lt;version&gt;</span>0.7.6<span class="nt">&lt;/version&gt;</span>
	<span class="c">&lt;!-- exclusions for dependency conversion --&gt;</span>
	<span class="nt">&lt;exclusions&gt;</span>
		<span class="nt">&lt;exclusion&gt;</span>
			<span class="nt">&lt;groupId&gt;</span>com.esotericsoftware.kryo<span class="nt">&lt;/groupId&gt;</span>
			<span class="nt">&lt;artifactId&gt;</span>kryo<span class="nt">&lt;/artifactId&gt;</span>
		<span class="nt">&lt;/exclusion&gt;</span>
	<span class="nt">&lt;/exclusions&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="c">&lt;!-- libthrift is required by chill-thrift --&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
	<span class="nt">&lt;groupId&gt;</span>org.apache.thrift<span class="nt">&lt;/groupId&gt;</span>
	<span class="nt">&lt;artifactId&gt;</span>libthrift<span class="nt">&lt;/artifactId&gt;</span>
	<span class="nt">&lt;version&gt;</span>0.11.0<span class="nt">&lt;/version&gt;</span>
	<span class="nt">&lt;exclusions&gt;</span>
		<span class="nt">&lt;exclusion&gt;</span>
			<span class="nt">&lt;groupId&gt;</span>javax.servlet<span class="nt">&lt;/groupId&gt;</span>
			<span class="nt">&lt;artifactId&gt;</span>servlet-api<span class="nt">&lt;/artifactId&gt;</span>
		<span class="nt">&lt;/exclusion&gt;</span>
		<span class="nt">&lt;exclusion&gt;</span>
			<span class="nt">&lt;groupId&gt;</span>org.apache.httpcomponents<span class="nt">&lt;/groupId&gt;</span>
			<span class="nt">&lt;artifactId&gt;</span>httpclient<span class="nt">&lt;/artifactId&gt;</span>
		<span class="nt">&lt;/exclusion&gt;</span>
	<span class="nt">&lt;/exclusions&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>对于 Google Protobuf，你需要以下 Maven 依赖。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
	<span class="nt">&lt;groupId&gt;</span>com.twitter<span class="nt">&lt;/groupId&gt;</span>
	<span class="nt">&lt;artifactId&gt;</span>chill-protobuf<span class="nt">&lt;/artifactId&gt;</span>
	<span class="nt">&lt;version&gt;</span>0.7.6<span class="nt">&lt;/version&gt;</span>
	<span class="c">&lt;!-- exclusions for dependency conversion --&gt;</span>
	<span class="nt">&lt;exclusions&gt;</span>
		<span class="nt">&lt;exclusion&gt;</span>
			<span class="nt">&lt;groupId&gt;</span>com.esotericsoftware.kryo<span class="nt">&lt;/groupId&gt;</span>
			<span class="nt">&lt;artifactId&gt;</span>kryo<span class="nt">&lt;/artifactId&gt;</span>
		<span class="nt">&lt;/exclusion&gt;</span>
	<span class="nt">&lt;/exclusions&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="c">&lt;!-- We need protobuf for chill-protobuf --&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
	<span class="nt">&lt;groupId&gt;</span>com.google.protobuf<span class="nt">&lt;/groupId&gt;</span>
	<span class="nt">&lt;artifactId&gt;</span>protobuf-java<span class="nt">&lt;/artifactId&gt;</span>
	<span class="nt">&lt;version&gt;</span>3.7.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>请根据需要调整两个库的版本。</p>
<h2 id="使用-kryo-的-javaserializer-的问题">使用 Kryo 的 JavaSerializer 的问题。</h2>
<p>如果你为你的自定义类型注册了 Kryo 的 JavaSerializer，你可能会遇到 ClassNotFoundExceptions，即使你的自定义类型类包含在提交的用户代码 jar 中。这是由于 Kryo 的 JavaSerializer 的一个已知问题，它可能会错误地使用错误的 classloader。</p>
<p>在这种情况下，你应该使用 org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer 来代替解决这个问题。这是 Flink 中重新实现的 JavaSerializer，它可以确保使用用户代码类加载器。</p>
<p>更多细节请参考 <a href="https://issues.apache.org/jira/browse/FLINK-6025">FLINK-6025</a>。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/custom_serializers.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/custom_serializers.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Hive Read and Write]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Streaming" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 方言" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 集成 - 概览" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hivecatalog/?utm_source=atom_feed" rel="related" type="text/html" title="HiveCatalog" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Hive Read and Write</blockquote><h1 id="hive-读写">Hive 读写</h1>
<p>使用 HiveCatalog 和 Flink 与 Hive 的连接器，Flink 可以从 Hive 数据中读取和写入数据，作为 Hive 批处理引擎的替代。请务必按照说明在你的应用中加入正确的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#depedencies">依赖关系</a>。同时请注意，Hive 连接器只适用于 blink planner。</p>
<h2 id="从-hive-读取数据">从 Hive 读取数据</h2>
<p>假设 Hive 在其默认的数据库中包含一个名为 people 的单表，该表包含多条记录。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">hive&gt; show databases<span class="p">;</span>
OK
default
Time taken: 0.841 seconds, Fetched: <span class="m">1</span> row<span class="o">(</span>s<span class="o">)</span>

hive&gt; show tables<span class="p">;</span>
OK
Time taken: 0.087 seconds

hive&gt; CREATE TABLE mytable<span class="o">(</span>name string, value double<span class="o">)</span><span class="p">;</span>
OK
Time taken: 0.127 seconds

hive&gt; SELECT * FROM mytable<span class="p">;</span>
OK
Tom   4.72
John  8.0
Tom   24.2
Bob   3.14
Bob   4.72
Tom   34.9
Mary  4.79
Tiff  2.72
Bill  4.33
Mary  77.7
Time taken: 0.097 seconds, Fetched: <span class="m">10</span> row<span class="o">(</span>s<span class="o">)</span>
</code></pre></div><p>数据准备好后，你可以<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#connecting-to-hive">连接到现有的 Hive 安装</a>并开始查询。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; show catalogs<span class="p">;</span>
myhive
default_catalog

<span class="c1"># ------ Set the current catalog to be &#39;myhive&#39; catalog if you haven&#39;t set it in the yaml file ------</span>

Flink SQL&gt; use catalog myhive<span class="p">;</span>

<span class="c1"># ------ See all registered database in catalog &#39;mytable&#39; ------</span>

Flink SQL&gt; show databases<span class="p">;</span>
default

<span class="c1"># ------ See the previously registered table &#39;mytable&#39; ------</span>

Flink SQL&gt; show tables<span class="p">;</span>
mytable

<span class="c1"># ------ The table schema that Flink sees is the same that we created in Hive, two columns - name as string and value as double ------ </span>
Flink SQL&gt; describe mytable<span class="p">;</span>
root
 <span class="p">|</span>-- name: name
 <span class="p">|</span>-- type: STRING
 <span class="p">|</span>-- name: value
 <span class="p">|</span>-- type: DOUBLE

<span class="c1"># ------ Select from hive table or hive view ------ </span>
Flink SQL&gt; SELECT * FROM mytable<span class="p">;</span>

   name      value
__________ __________

    Tom      4.72
    John     8.0
    Tom      24.2
    Bob      3.14
    Bob      4.72
    Tom      34.9
    Mary     4.79
    Tiff     2.72
    Bill     4.33
    Mary     77.7
</code></pre></div><h3 id="查询-hive-视图">查询 Hive 视图</h3>
<p>如果你需要查询 Hive 视图，请注意。</p>
<p>在查询该目录中的视图之前，必须先使用 Hive 目录作为当前目录。可以通过 Table API 中的 <code>tableEnv.useCatalog(...)</code> 或者 SQL Client 中的 USE CATALOG &hellip;来实现。
Hive 和 Flink SQL 有不同的语法，例如，不同的保留关键字和字元。请确保视图的查询与 Flink 语法兼容。</p>
<h2 id="写入-hive">写入 Hive</h2>
<p>同样，也可以使用 INSERT 子句将数据写入 hive 中。</p>
<p>考虑有一个名为 &ldquo;mytable &ldquo;的示例表，表中有两列：name 和 age，类型为 string 和 int。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># ------ INSERT INTO will append to the table or partition, keeping the existing data intact ------ </span>
Flink SQL&gt; INSERT INTO mytable SELECT <span class="s1">&#39;Tom&#39;</span>, 25<span class="p">;</span>

<span class="c1"># ------ INSERT OVERWRITE will overwrite any existing data in the table or partition ------ </span>
Flink SQL&gt; INSERT OVERWRITE mytable SELECT <span class="s1">&#39;Tom&#39;</span>, 25<span class="p">;</span>
</code></pre></div><p>我们也支持分区表，考虑有一个名为 myparttable 的分区表，有四列：name、age、my_type 和 my_date，在 type 中&hellip;<code>my_type</code> 和 <code>my_date</code> 是分区键。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># ------ Insert with static partition ------ </span>
Flink SQL&gt; INSERT OVERWRITE myparttable PARTITION <span class="o">(</span><span class="nv">my_type</span><span class="o">=</span><span class="s1">&#39;type_1&#39;</span>, <span class="nv">my_date</span><span class="o">=</span><span class="s1">&#39;2019-08-08&#39;</span><span class="o">)</span> SELECT <span class="s1">&#39;Tom&#39;</span>, 25<span class="p">;</span>

<span class="c1"># ------ Insert with dynamic partition ------ </span>
Flink SQL&gt; INSERT OVERWRITE myparttable SELECT <span class="s1">&#39;Tom&#39;</span>, 25, <span class="s1">&#39;type_1&#39;</span>, <span class="s1">&#39;2019-08-08&#39;</span><span class="p">;</span>

<span class="c1"># ------ Insert with static(my_type) and dynamic(my_date) partition ------ </span>
Flink SQL&gt; INSERT OVERWRITE myparttable PARTITION <span class="o">(</span><span class="nv">my_type</span><span class="o">=</span><span class="s1">&#39;type_1&#39;</span><span class="o">)</span> SELECT <span class="s1">&#39;Tom&#39;</span>, 25, <span class="s1">&#39;2019-08-08&#39;</span><span class="p">;</span>
</code></pre></div><h2 id="格式">格式</h2>
<p>我们测试了以下表格存储格式：文本、csv、SequenceFile、ORC 和 Parquet。</p>
<h2 id="优化">优化</h2>
<h3 id="分区修剪">分区修剪</h3>
<p>Flink 使用分区修剪作为一种性能优化，以限制 Flink 在查询 Hive 表时读取的文件和分区的数量。当你的数据被分区后，当查询符合某些过滤条件时，Flink 只会读取 Hive 表中的分区子集。</p>
<h3 id="投影下推">投影下推</h3>
<p>Flink 利用投影下推，通过从表扫描中省略不必要的字段，最大限度地减少 Flink 和 Hive 表之间的数据传输。</p>
<p>当一个表包含许多列时，它尤其有利。</p>
<h3 id="限制下推">限制下推</h3>
<p>对于带有 LIMIT 子句的查询，Flink 会尽可能地限制输出记录的数量，以减少跨网络传输的数据量。</p>
<h3 id="读取时的向量优化">读取时的向量优化</h3>
<p>当满足以下条件时，会自动使用优化功能。</p>
<ul>
<li>格式： ORC 或 Parquet。</li>
<li>没有复杂数据类型的列，如 hive 类型: List, Map, Struct, Union。</li>
</ul>
<p>这个功能默认是开启的。如果出现问题，可以使用这个配置选项来关闭 Vectorized Optimization。</p>
<pre><code>table.exec.hive.fallback-mapred-reader=true
</code></pre><h3 id="source-并行性推断">Source 并行性推断</h3>
<p>默认情况下，Flink 根据分割次数来推断 hive 源的并行度，分割次数是根据文件的数量和文件中的块数来推断的。</p>
<p>Flink 允许你灵活配置并行度推断的策略。你可以在 TableConfig 中配置以下参数（注意，这些参数会影响作业的所有源）。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Key</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">table.exec.hive.infer-source-parallelism</td>
<td style="text-align:left">true</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">如果为真，则根据分割数来推断源的并行度，如果为假，则根据配置来设置源的并行度。如果为 false，则通过配置来设置源的并行度。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.hive.infer-source-parallelism.max</td>
<td style="text-align:left">1000</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">设置源运算符的最大推断并行度。</td>
</tr>
</tbody>
</table>
<h2 id="路线图">路线图</h2>
<p>我们正在规划并积极开发支持功能，如:</p>
<ul>
<li>ACID 表</li>
<li>分桶表</li>
<li>更多格式</li>
</ul>
<p>更多功能需求请联系社区 <a href="https://flink.apache.org/community.html#mailing-lists">https://flink.apache.org/community.html#mailing-lists</a></p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_read_write.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_read_write.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/hive" term="hive" label="Hive" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Hive Streaming]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Read and Write" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 方言" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 集成 - 概览" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hivecatalog/?utm_source=atom_feed" rel="related" type="text/html" title="HiveCatalog" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Hive Streaming</blockquote><h1 id="hive-流">Hive 流</h1>
<p>一个典型的 hive 作业是周期性地安排执行的，所以会有较大的延迟。</p>
<p>Flink 支持以流式的形式写入、读取和加入 hive 表。</p>
<p>流式数据有三种类型。</p>
<ul>
<li>将流式数据写入 Hive 表。</li>
<li>以流的形式增量读取 Hive 表。</li>
<li>流式表使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table">Temporal 表</a>连接 Hive 表。</li>
</ul>
<h2 id="流式写入">流式写入</h2>
<p>Hive 表支持流式写入，基于 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#streaming-sink">Filesystem Streaming Sink</a>。</p>
<p>Hive Streaming Sink 重用 Filesystem Streaming Sink，将 Hadoop OutputFormat/RecordWriter 整合到流式写入。Hadoop RecordWriters 是 Bulk-encoded Formats，Bulk Formats 在每个检查点上滚动文件。</p>
<p>默认情况下，现在只有重命名提交者，这意味着 S3 文件系统不能支持精确的 once，如果你想在 S3 文件系统中使用 Hive 流媒体汇，你可以在 TableConfig 中把以下参数配置为 false，以使用 Flink 原生写入器（只对 parquet 和 orc 有效）（注意这些参数会影响所有作业的汇）。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Key</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">table.exec.hive.fallback-mapred-writer</td>
<td style="text-align:left">true</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">如果是假的，用 flink native writer 写 parquet 和 orc 文件；如果是真的，用 hadoop mapred record writer 写 parquet 和 orc 文件。</td>
</tr>
</tbody>
</table>
<p>下面展示了如何使用流接收器写一个流式查询，将数据从 Kafka 写到一个有 partition-commit 的 Hive 表中，并运行一个批处理查询将这些数据读回。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SET</span><span class="w"> </span><span class="k">table</span><span class="p">.</span><span class="k">sql</span><span class="o">-</span><span class="n">dialect</span><span class="o">=</span><span class="n">hive</span><span class="p">;</span><span class="w">
</span><span class="w"></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">hive_table</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="n">user_id</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="n">order_amount</span><span class="w"> </span><span class="n">DOUBLE</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w"> </span><span class="n">PARTITIONED</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="p">(</span><span class="n">dt</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w"> </span><span class="n">hr</span><span class="w"> </span><span class="n">STRING</span><span class="p">)</span><span class="w"> </span><span class="n">STORED</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">parquet</span><span class="w"> </span><span class="n">TBLPROPERTIES</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="s1">&#39;partition.time-extractor.timestamp-pattern&#39;</span><span class="o">=</span><span class="s1">&#39;$dt $hr:00:00&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="s1">&#39;sink.partition-commit.trigger&#39;</span><span class="o">=</span><span class="s1">&#39;partition-time&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="s1">&#39;sink.partition-commit.delay&#39;</span><span class="o">=</span><span class="s1">&#39;1 h&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="s1">&#39;sink.partition-commit.policy.kind&#39;</span><span class="o">=</span><span class="s1">&#39;metastore,success-file&#39;</span><span class="w">
</span><span class="w"></span><span class="p">);</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">SET</span><span class="w"> </span><span class="k">table</span><span class="p">.</span><span class="k">sql</span><span class="o">-</span><span class="n">dialect</span><span class="o">=</span><span class="k">default</span><span class="p">;</span><span class="w">
</span><span class="w"></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">kafka_table</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="n">user_id</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="n">order_amount</span><span class="w"> </span><span class="n">DOUBLE</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="n">log_ts</span><span class="w"> </span><span class="k">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span><span class="w">
</span><span class="w">  </span><span class="n">WATERMARK</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">log_ts</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">log_ts</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;5&#39;</span><span class="w"> </span><span class="k">SECOND</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(...);</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c1">-- streaming sql, insert into hive table
</span><span class="c1"></span><span class="k">INSERT</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">hive_table</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="n">user_id</span><span class="p">,</span><span class="w"> </span><span class="n">order_amount</span><span class="p">,</span><span class="w"> </span><span class="n">DATE_FORMAT</span><span class="p">(</span><span class="n">log_ts</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;yyyy-MM-dd&#39;</span><span class="p">),</span><span class="w"> </span><span class="n">DATE_FORMAT</span><span class="p">(</span><span class="n">log_ts</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;HH&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">kafka_table</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c1">-- batch sql, select with partition pruning
</span><span class="c1"></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">hive_table</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">dt</span><span class="o">=</span><span class="s1">&#39;2020-05-20&#39;</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">hr</span><span class="o">=</span><span class="s1">&#39;12&#39;</span><span class="p">;</span><span class="w">
</span></code></pre></div><h2 id="流式读取">流式读取</h2>
<p>为了提高 hive 读取的实时性，Flink 支持实时 Hive 表流读取。</p>
<ul>
<li>分区表，监控分区的生成，并逐步读取新分区。</li>
<li>非分区表，监控文件夹中新文件的生成，并增量读取新文件。</li>
</ul>
<p>甚至可以采用 10 分钟级别的分区策略，利用 Flink 的 Hive 流式读取和 Hive 流式写入，大大提高 Hive 数据仓库的实时性能，达到准实时分钟级别。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Key</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">streaming-source.enable</td>
<td style="text-align:left">false</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">是否启用流媒体源。注意：请确保每个分区/文件都是以原子方式写入，否则读者可能会得到不完整的数据。请确保每个分区/文件都应该以原子方式写入，否则读者可能会得到不完整的数据。</td>
</tr>
<tr>
<td style="text-align:left">streaming-source.monitor-interval</td>
<td style="text-align:left">1 m</td>
<td style="text-align:left">Duration</td>
<td style="text-align:left">连续监控分区/文件的时间间隔。</td>
</tr>
<tr>
<td style="text-align:left">streaming-source.consume-order</td>
<td style="text-align:left">create-time</td>
<td style="text-align:left">String</td>
<td style="text-align:left">流源的消耗顺序，支持 create-time 和 partition-time。create-time 比较的是分区/文件的创建时间，这不是 Hive metaStore 中的分区创建时间，而是文件系统中的文件夹/文件修改时间；partition-time 比较的是分区名称所代表的时间，如果分区文件夹以某种方式得到更新，比如在文件夹中添加新文件，就会影响数据的消耗方式。对于非分区表，这个值应该一直是 &ldquo;创建时间&rdquo;。</td>
</tr>
<tr>
<td style="text-align:left">streaming-source.consume-start-offset</td>
<td style="text-align:left">1970-00-00</td>
<td style="text-align:left">String</td>
<td style="text-align:left">流式消费的起始偏移量。如何解析和比较偏移量取决于你的顺序。对于创建时间和分区时间，应该是一个时间戳字符串（yyyy-[m]m-[d]d [hh:mm:ss]）。对于分区时间，将使用分区时间提取器从分区中提取时间。</td>
</tr>
</tbody>
</table>
<p>注意:</p>
<ul>
<li>监控策略是现在扫描位置路径中的所有目录/文件。如果分区太多，会出现性能问题。</li>
<li>非分区的流式读取需要将每个文件原子地放入目标目录中。</li>
<li>分区的流式读取要求在 hive metastore 的视图中原子地添加每个分区。这意味着新添加到现有分区的数据不会被消耗掉。</li>
<li>流读取不支持 Flink DDL 中的水印语法。所以它不能用于窗口操作符。</li>
</ul>
<p>下面展示了如何增量读取 Hive 表。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">hive_table</span><span class="w"> </span><span class="cm">/*+ OPTIONS(&#39;streaming-source.enable&#39;=&#39;true&#39;, &#39;streaming-source.consume-start-offset&#39;=&#39;2020-05-20&#39;) */</span><span class="p">;</span><span class="w">
</span></code></pre></div><h2 id="hive-表作为临时表">Hive 表作为临时表</h2>
<p>您可以使用 Hive 表作为时态表，并将流式数据加入其中。请按照<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table">示例</a>来了解如何连接一个时态表。</p>
<p>在执行 join 时，Hive 表将被缓存在 TM 内存中，并在 Hive 表中查找来自流的每一条记录，以决定是否找到匹配。你不需要任何额外的设置就可以使用 Hive 表作为时态表。但可以选择用以下属性配置 Hive 表缓存的 TTL。缓存过期后，将再次扫描 Hive 表以加载最新的数据。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Key</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">lookup.join.cache.ttl</td>
<td style="text-align:left">60 min</td>
<td style="text-align:left">Duration</td>
<td style="text-align:left">在查找连接中构建表的缓存 TTL（例如 10 分钟）。默认情况下，TTL 为 60 分钟。</td>
</tr>
</tbody>
</table>
<p>注意:</p>
<ol>
<li>每个加入子任务都需要保留自己的 Hive 表的缓存。请确保 Hive 表可以放入 TM 任务槽的内存中。</li>
<li>你应该为 lookup.join.cache.ttl 设置一个相对较大的值。如果你的 Hive 表需要太频繁的更新和重载，你可能会有性能问题。</li>
<li>目前，每当缓存需要刷新时，我们只是简单地加载整个 Hive 表。没有办法区分新数据和旧数据。</li>
</ol>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/hive" term="hive" label="Hive" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Hive 函数]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Read and Write" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Streaming" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 方言" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 集成 - 概览" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hivecatalog/?utm_source=atom_feed" rel="related" type="text/html" title="HiveCatalog" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Hive Functions</blockquote><h1 id="hive-函数">Hive 函数</h1>
<h2 id="通过-hivemodule-使用-hive-内置功能">通过 HiveModule 使用 Hive 内置功能</h2>
<p>HiveModule 将 Hive 内置函数作为 Flink 系统（内置）函数提供给 Flink SQL 和 Table API 用户。</p>
<p>具体信息请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/modules.html#hivemodule">HiveModule</a>。</p>
<ul>
<li>Scala</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">name</span>            <span class="k">=</span> <span class="s">&#34;myhive&#34;</span>
<span class="k">val</span> <span class="n">version</span>         <span class="k">=</span> <span class="s">&#34;2.3.4&#34;</span>

<span class="n">tableEnv</span><span class="o">.</span><span class="n">loadModue</span><span class="o">(</span><span class="n">name</span><span class="o">,</span> <span class="k">new</span> <span class="nc">HiveModule</span><span class="o">(</span><span class="n">version</span><span class="o">));</span>
</code></pre></div><ul>
<li>YAML</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">modules</span><span class="p">:</span><span class="w">
</span><span class="w">   </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">core</span><span class="w">
</span><span class="w">     </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">core</span><span class="w">
</span><span class="w">   </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myhive</span><span class="w">
</span><span class="w">     </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">hive</span><span class="w">
</span></code></pre></div><ul>
<li>注意，旧版本中的一些 Hive 内置功能存在<a href="https://issues.apache.org/jira/browse/HIVE-16183">线程安全问题</a>。我们建议用户给自己的 Hive 打上补丁来修复它们。</li>
</ul>
<h2 id="hive-用户定义的函数">Hive 用户定义的函数</h2>
<p>用户可以在 Flink 中使用他们现有的 Hive 用户定义函数。</p>
<p>支持的 UDF 类型包括:</p>
<ul>
<li>UDF</li>
<li>GenericUDF</li>
<li>GenericUDTF</li>
<li>UDAF</li>
<li>GenericUDAFResolver2</li>
</ul>
<p>在查询规划和执行时，Hive 的 UDF 和 GenericUDF 会自动翻译成 Flink 的 ScalarFunction，Hive 的 GenericUDTF 会自动翻译成 Flink 的 TableFunction，Hive 的 UDAF 和 GenericUDAFResolver2 会翻译成 Flink 的 AggregateFunction。</p>
<p>要使用 Hive 的用户定义函数，用户必须做到:</p>
<ul>
<li>设置一个由 Hive Metastore 支持的 HiveCatalog 作为会话的当前目录，其中包含该函数。</li>
<li>在 Flink 的 classpath 中加入一个包含该函数的 jar。</li>
<li>使用 Blink 计划器。</li>
</ul>
<h2 id="使用-hive-用户定义函数">使用 Hive 用户定义函数</h2>
<p>假设我们在 Hive Metastore 中注册了以下 Hive 函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="cm">/**
</span><span class="cm"> * Test simple udf. Registered under name &#39;myudf&#39;
</span><span class="cm"> */</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">TestHiveSimpleUDF</span> <span class="kd">extends</span> <span class="n">UDF</span> <span class="o">{</span>

	<span class="kd">public</span> <span class="n">IntWritable</span> <span class="nf">evaluate</span><span class="o">(</span><span class="n">IntWritable</span> <span class="n">i</span><span class="o">)</span> <span class="o">{</span>
		<span class="k">return</span> <span class="k">new</span> <span class="n">IntWritable</span><span class="o">(</span><span class="n">i</span><span class="o">.</span><span class="na">get</span><span class="o">());</span>
	<span class="o">}</span>

	<span class="kd">public</span> <span class="n">Text</span> <span class="nf">evaluate</span><span class="o">(</span><span class="n">Text</span> <span class="n">text</span><span class="o">)</span> <span class="o">{</span>
		<span class="k">return</span> <span class="k">new</span> <span class="n">Text</span><span class="o">(</span><span class="n">text</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
	<span class="o">}</span>
<span class="o">}</span>

<span class="cm">/**
</span><span class="cm"> * Test generic udf. Registered under name &#39;mygenericudf&#39;
</span><span class="cm"> */</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">TestHiveGenericUDF</span> <span class="kd">extends</span> <span class="n">GenericUDF</span> <span class="o">{</span>

	<span class="nd">@Override</span>
	<span class="kd">public</span> <span class="n">ObjectInspector</span> <span class="nf">initialize</span><span class="o">(</span><span class="n">ObjectInspector</span><span class="o">[]</span> <span class="n">arguments</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">UDFArgumentException</span> <span class="o">{</span>
		<span class="n">checkArgument</span><span class="o">(</span><span class="n">arguments</span><span class="o">.</span><span class="na">length</span> <span class="o">==</span> <span class="n">2</span><span class="o">);</span>

		<span class="n">checkArgument</span><span class="o">(</span><span class="n">arguments</span><span class="o">[</span><span class="n">1</span><span class="o">]</span> <span class="k">instanceof</span> <span class="n">ConstantObjectInspector</span><span class="o">);</span>
		<span class="n">Object</span> <span class="n">constant</span> <span class="o">=</span> <span class="o">((</span><span class="n">ConstantObjectInspector</span><span class="o">)</span> <span class="n">arguments</span><span class="o">[</span><span class="n">1</span><span class="o">]).</span><span class="na">getWritableConstantValue</span><span class="o">();</span>
		<span class="n">checkArgument</span><span class="o">(</span><span class="n">constant</span> <span class="k">instanceof</span> <span class="n">IntWritable</span><span class="o">);</span>
		<span class="n">checkArgument</span><span class="o">(((</span><span class="n">IntWritable</span><span class="o">)</span> <span class="n">constant</span><span class="o">).</span><span class="na">get</span><span class="o">()</span> <span class="o">==</span> <span class="n">1</span><span class="o">);</span>

		<span class="k">if</span> <span class="o">(</span><span class="n">arguments</span><span class="o">[</span><span class="n">0</span><span class="o">]</span> <span class="k">instanceof</span> <span class="n">IntObjectInspector</span> <span class="o">||</span>
				<span class="n">arguments</span><span class="o">[</span><span class="n">0</span><span class="o">]</span> <span class="k">instanceof</span> <span class="n">StringObjectInspector</span><span class="o">)</span> <span class="o">{</span>
			<span class="k">return</span> <span class="n">arguments</span><span class="o">[</span><span class="n">0</span><span class="o">];</span>
		<span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
			<span class="k">throw</span> <span class="k">new</span> <span class="n">RuntimeException</span><span class="o">(</span><span class="s">&#34;Not support argument: &#34;</span> <span class="o">+</span> <span class="n">arguments</span><span class="o">[</span><span class="n">0</span><span class="o">]);</span>
		<span class="o">}</span>
	<span class="o">}</span>

	<span class="nd">@Override</span>
	<span class="kd">public</span> <span class="n">Object</span> <span class="nf">evaluate</span><span class="o">(</span><span class="n">DeferredObject</span><span class="o">[]</span> <span class="n">arguments</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">HiveException</span> <span class="o">{</span>
		<span class="k">return</span> <span class="n">arguments</span><span class="o">[</span><span class="n">0</span><span class="o">].</span><span class="na">get</span><span class="o">();</span>
	<span class="o">}</span>

	<span class="nd">@Override</span>
	<span class="kd">public</span> <span class="n">String</span> <span class="nf">getDisplayString</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">children</span><span class="o">)</span> <span class="o">{</span>
		<span class="k">return</span> <span class="s">&#34;TestHiveGenericUDF&#34;</span><span class="o">;</span>
	<span class="o">}</span>
<span class="o">}</span>

<span class="cm">/**
</span><span class="cm"> * Test split udtf. Registered under name &#39;mygenericudtf&#39;
</span><span class="cm"> */</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">TestHiveUDTF</span> <span class="kd">extends</span> <span class="n">GenericUDTF</span> <span class="o">{</span>

	<span class="nd">@Override</span>
	<span class="kd">public</span> <span class="n">StructObjectInspector</span> <span class="nf">initialize</span><span class="o">(</span><span class="n">ObjectInspector</span><span class="o">[]</span> <span class="n">argOIs</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">UDFArgumentException</span> <span class="o">{</span>
		<span class="n">checkArgument</span><span class="o">(</span><span class="n">argOIs</span><span class="o">.</span><span class="na">length</span> <span class="o">==</span> <span class="n">2</span><span class="o">);</span>

		<span class="c1">// TEST for constant arguments
</span><span class="c1"></span>		<span class="n">checkArgument</span><span class="o">(</span><span class="n">argOIs</span><span class="o">[</span><span class="n">1</span><span class="o">]</span> <span class="k">instanceof</span> <span class="n">ConstantObjectInspector</span><span class="o">);</span>
		<span class="n">Object</span> <span class="n">constant</span> <span class="o">=</span> <span class="o">((</span><span class="n">ConstantObjectInspector</span><span class="o">)</span> <span class="n">argOIs</span><span class="o">[</span><span class="n">1</span><span class="o">]).</span><span class="na">getWritableConstantValue</span><span class="o">();</span>
		<span class="n">checkArgument</span><span class="o">(</span><span class="n">constant</span> <span class="k">instanceof</span> <span class="n">IntWritable</span><span class="o">);</span>
		<span class="n">checkArgument</span><span class="o">(((</span><span class="n">IntWritable</span><span class="o">)</span> <span class="n">constant</span><span class="o">).</span><span class="na">get</span><span class="o">()</span> <span class="o">==</span> <span class="n">1</span><span class="o">);</span>

		<span class="k">return</span> <span class="n">ObjectInspectorFactory</span><span class="o">.</span><span class="na">getStandardStructObjectInspector</span><span class="o">(</span>
			<span class="n">Collections</span><span class="o">.</span><span class="na">singletonList</span><span class="o">(</span><span class="s">&#34;col1&#34;</span><span class="o">),</span>
			<span class="n">Collections</span><span class="o">.</span><span class="na">singletonList</span><span class="o">(</span><span class="n">PrimitiveObjectInspectorFactory</span><span class="o">.</span><span class="na">javaStringObjectInspector</span><span class="o">));</span>
	<span class="o">}</span>

	<span class="nd">@Override</span>
	<span class="kd">public</span> <span class="kt">void</span> <span class="nf">process</span><span class="o">(</span><span class="n">Object</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">HiveException</span> <span class="o">{</span>
		<span class="n">String</span> <span class="n">str</span> <span class="o">=</span> <span class="o">(</span><span class="n">String</span><span class="o">)</span> <span class="n">args</span><span class="o">[</span><span class="n">0</span><span class="o">];</span>
		<span class="k">for</span> <span class="o">(</span><span class="n">String</span> <span class="n">s</span> <span class="o">:</span> <span class="n">str</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&#34;,&#34;</span><span class="o">))</span> <span class="o">{</span>
			<span class="n">forward</span><span class="o">(</span><span class="n">s</span><span class="o">);</span>
			<span class="n">forward</span><span class="o">(</span><span class="n">s</span><span class="o">);</span>
		<span class="o">}</span>
	<span class="o">}</span>

	<span class="nd">@Override</span>
	<span class="kd">public</span> <span class="kt">void</span> <span class="nf">close</span><span class="o">()</span> <span class="o">{</span>
	<span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>从 Hive CLI 中，我们可以看到他们已经注册了。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">hive&gt; show functions<span class="p">;</span>
OK
......
mygenericudf
myudf
myudtf
</code></pre></div><p>然后，用户可以在 SQL 中使用它们作为。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; <span class="k">select</span> mygenericudf<span class="o">(</span>myudf<span class="o">(</span>name<span class="o">)</span>, 1<span class="o">)</span> as a, mygenericudf<span class="o">(</span>myudf<span class="o">(</span>age<span class="o">)</span>, 1<span class="o">)</span> as b, s from mysourcetable, lateral table<span class="o">(</span>myudtf<span class="o">(</span>name, 1<span class="o">))</span> as T<span class="o">(</span>s<span class="o">)</span><span class="p">;</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_functions.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_functions.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/hive" term="hive" label="Hive" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Hive 方言]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Read and Write" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Streaming" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 集成 - 概览" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hivecatalog/?utm_source=atom_feed" rel="related" type="text/html" title="HiveCatalog" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Hive Dialect</blockquote><h1 id="hive-方言">Hive 方言</h1>
<p>从 1.11.0 开始，当使用 Hive 方言时，Flink 允许用户用 Hive 语法编写 SQL 语句。通过提供与 Hive 语法的兼容性，我们旨在提高与 Hive 的互操作性，减少用户为了执行不同的语句而需要在 Flink 和 Hive 之间切换的情况。</p>
<h2 id="使用-hive-方言">使用 Hive 方言</h2>
<p>Flink 目前支持两种 SQL 方言：默认和 Hive。在使用 Hive 语法编写之前，需要先切换到 Hive 方言。下面介绍如何通过 SQL Client 和 Table API 来设置方言。同时注意，你可以为你执行的每一条语句动态切换方言。不需要重新启动会话来使用不同的方言。</p>
<h3 id="sql-客户端">SQL 客户端</h3>
<p>SQL 方言可以通过 table.sql-dialect 属性来指定，因此你可以在你的 SQL 客户端的 yaml 文件的配置部分设置初始方言。因此，你可以在 SQL 客户端的 yaml 文件的配置部分设置要使用的初始方言。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">execution</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">planner</span><span class="p">:</span><span class="w"> </span><span class="l">blink</span><span class="w">
</span><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">batch</span><span class="w">
</span><span class="w">  </span><span class="nt">result-mode</span><span class="p">:</span><span class="w"> </span><span class="l">table</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">configuration</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">table.sql-dialect</span><span class="p">:</span><span class="w"> </span><span class="l">hive</span><span class="w">
</span></code></pre></div><p>你也可以在 SQL 客户端启动后设置方言。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; <span class="nb">set</span> table.sql-dialect<span class="o">=</span>hive<span class="p">;</span> -- to use hive dialect
<span class="o">[</span>INFO<span class="o">]</span> Session property has been set.

Flink SQL&gt; <span class="nb">set</span> table.sql-dialect<span class="o">=</span>default<span class="p">;</span> -- to use default dialect
<span class="o">[</span>INFO<span class="o">]</span> Session property has been set.
</code></pre></div><h3 id="table-api">Table API</h3>
<p>You can set dialect for your TableEnvironment with Table API.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pyflink.table</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">settings</span> <span class="o">=</span> <span class="n">EnvironmentSettings</span><span class="o">.</span><span class="n">new_instance</span><span class="p">()</span><span class="o">.</span><span class="n">in_batch_mode</span><span class="p">()</span><span class="o">.</span><span class="n">use_blink_planner</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="n">t_env</span> <span class="o">=</span> <span class="n">BatchTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">environment_settings</span><span class="o">=</span><span class="n">settings</span><span class="p">)</span>

<span class="c1"># to use hive dialect</span>
<span class="n">t_env</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span><span class="o">.</span><span class="n">set_sql_dialect</span><span class="p">(</span><span class="n">SqlDialect</span><span class="o">.</span><span class="n">HIVE</span><span class="p">)</span>
<span class="c1"># to use default dialect</span>
<span class="n">t_env</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span><span class="o">.</span><span class="n">set_sql_dialect</span><span class="p">(</span><span class="n">SqlDialect</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">)</span>
</code></pre></div><h2 id="ddl">DDL</h2>
<p>本节列出了 Hive 方言支持的 DDL。在这里我们将主要关注语法。关于每个 DDL 语句的语义，你可以参考 <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">Hive 文档</a>。</p>
<h3 id="database">DATABASE</h3>
<ul>
<li>Show</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SHOW</span><span class="w"> </span><span class="n">DATABASES</span><span class="p">;</span><span class="w">
</span></code></pre></div><ul>
<li>Create</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="p">(</span><span class="k">DATABASE</span><span class="o">|</span><span class="k">SCHEMA</span><span class="p">)</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="n">database_name</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">database_comment</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">LOCATION</span><span class="w"> </span><span class="n">fs_path</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">WITH</span><span class="w"> </span><span class="n">DBPROPERTIES</span><span class="w"> </span><span class="p">(</span><span class="n">property_name</span><span class="o">=</span><span class="n">property_value</span><span class="p">,</span><span class="w"> </span><span class="p">...)];</span><span class="w">
</span></code></pre></div><ul>
<li>Alter</li>
</ul>
<p>更新属性</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="p">(</span><span class="k">DATABASE</span><span class="o">|</span><span class="k">SCHEMA</span><span class="p">)</span><span class="w"> </span><span class="n">database_name</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="n">DBPROPERTIES</span><span class="w"> </span><span class="p">(</span><span class="n">property_name</span><span class="o">=</span><span class="n">property_value</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span><span class="w">
</span></code></pre></div><p>更新所有者</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="p">(</span><span class="k">DATABASE</span><span class="o">|</span><span class="k">SCHEMA</span><span class="p">)</span><span class="w"> </span><span class="n">database_name</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="k">OWNER</span><span class="w"> </span><span class="p">[</span><span class="k">USER</span><span class="o">|</span><span class="k">ROLE</span><span class="p">]</span><span class="w"> </span><span class="n">user_or_role</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>更新位置</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="p">(</span><span class="k">DATABASE</span><span class="o">|</span><span class="k">SCHEMA</span><span class="p">)</span><span class="w"> </span><span class="n">database_name</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="k">LOCATION</span><span class="w"> </span><span class="n">fs_path</span><span class="p">;</span><span class="w">
</span></code></pre></div><ul>
<li>Drop</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">DROP</span><span class="w"> </span><span class="p">(</span><span class="k">DATABASE</span><span class="o">|</span><span class="k">SCHEMA</span><span class="p">)</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="n">database_name</span><span class="w"> </span><span class="p">[</span><span class="k">RESTRICT</span><span class="o">|</span><span class="k">CASCADE</span><span class="p">];</span><span class="w">
</span></code></pre></div><ul>
<li>Use</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="n">USE</span><span class="w"> </span><span class="n">database_name</span><span class="p">;</span><span class="w">
</span></code></pre></div><h3 id="table">TABLE</h3>
<ul>
<li>Show</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SHOW</span><span class="w"> </span><span class="n">TABLES</span><span class="p">;</span><span class="w">
</span></code></pre></div><ul>
<li>Create</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="p">[</span><span class="k">EXTERNAL</span><span class="p">]</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="k">table_name</span><span class="w">
</span><span class="w">  </span><span class="p">[(</span><span class="n">col_name</span><span class="w"> </span><span class="n">data_type</span><span class="w"> </span><span class="p">[</span><span class="n">column_constraint</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">col_comment</span><span class="p">],</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="p">[</span><span class="n">table_constraint</span><span class="p">])]</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">table_comment</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="n">PARTITIONED</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="p">(</span><span class="n">col_name</span><span class="w"> </span><span class="n">data_type</span><span class="w"> </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">col_comment</span><span class="p">],</span><span class="w"> </span><span class="p">...)]</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="w">
</span><span class="w">    </span><span class="p">[</span><span class="k">ROW</span><span class="w"> </span><span class="n">FORMAT</span><span class="w"> </span><span class="n">row_format</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="p">[</span><span class="n">STORED</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">file_format</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">LOCATION</span><span class="w"> </span><span class="n">fs_path</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="n">TBLPROPERTIES</span><span class="w"> </span><span class="p">(</span><span class="n">property_name</span><span class="o">=</span><span class="n">property_value</span><span class="p">,</span><span class="w"> </span><span class="p">...)]</span><span class="w">
</span><span class="w">  
</span><span class="w"></span><span class="n">row_format</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="n">DELIMITED</span><span class="w"> </span><span class="p">[</span><span class="n">FIELDS</span><span class="w"> </span><span class="n">TERMINATED</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="nb">char</span><span class="w"> </span><span class="p">[</span><span class="n">ESCAPED</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="nb">char</span><span class="p">]]</span><span class="w"> </span><span class="p">[</span><span class="n">COLLECTION</span><span class="w"> </span><span class="n">ITEMS</span><span class="w"> </span><span class="n">TERMINATED</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="nb">char</span><span class="p">]</span><span class="w">
</span><span class="w">      </span><span class="p">[</span><span class="k">MAP</span><span class="w"> </span><span class="n">KEYS</span><span class="w"> </span><span class="n">TERMINATED</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="nb">char</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="n">LINES</span><span class="w"> </span><span class="n">TERMINATED</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="nb">char</span><span class="p">]</span><span class="w">
</span><span class="w">      </span><span class="p">[</span><span class="k">NULL</span><span class="w"> </span><span class="k">DEFINED</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="nb">char</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="o">|</span><span class="w"> </span><span class="n">SERDE</span><span class="w"> </span><span class="n">serde_name</span><span class="w"> </span><span class="p">[</span><span class="k">WITH</span><span class="w"> </span><span class="n">SERDEPROPERTIES</span><span class="w"> </span><span class="p">(</span><span class="n">property_name</span><span class="o">=</span><span class="n">property_value</span><span class="p">,</span><span class="w"> </span><span class="p">...)]</span><span class="w">
</span><span class="w">  
</span><span class="w"></span><span class="n">file_format</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="n">SEQUENCEFILE</span><span class="w">
</span><span class="w">  </span><span class="o">|</span><span class="w"> </span><span class="n">TEXTFILE</span><span class="w">
</span><span class="w">  </span><span class="o">|</span><span class="w"> </span><span class="n">RCFILE</span><span class="w">
</span><span class="w">  </span><span class="o">|</span><span class="w"> </span><span class="n">ORC</span><span class="w">
</span><span class="w">  </span><span class="o">|</span><span class="w"> </span><span class="n">PARQUET</span><span class="w">
</span><span class="w">  </span><span class="o">|</span><span class="w"> </span><span class="n">AVRO</span><span class="w">
</span><span class="w">  </span><span class="o">|</span><span class="w"> </span><span class="n">INPUTFORMAT</span><span class="w"> </span><span class="n">input_format_classname</span><span class="w"> </span><span class="n">OUTPUTFORMAT</span><span class="w"> </span><span class="n">output_format_classname</span><span class="w">
</span><span class="w">  
</span><span class="w"></span><span class="n">column_constraint</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">NULL</span><span class="w"> </span><span class="p">[[</span><span class="n">ENABLE</span><span class="o">|</span><span class="n">DISABLE</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="n">VALIDATE</span><span class="o">|</span><span class="n">NOVALIDATE</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="n">RELY</span><span class="o">|</span><span class="n">NORELY</span><span class="p">]]</span><span class="w">
</span><span class="w">  
</span><span class="w"></span><span class="n">table_constraint</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="k">CONSTRAINT</span><span class="w"> </span><span class="k">constraint_name</span><span class="p">]</span><span class="w"> </span><span class="k">PRIMARY</span><span class="w"> </span><span class="k">KEY</span><span class="w"> </span><span class="p">(</span><span class="n">col_name</span><span class="p">,</span><span class="w"> </span><span class="p">...)</span><span class="w"> </span><span class="p">[[</span><span class="n">ENABLE</span><span class="o">|</span><span class="n">DISABLE</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="n">VALIDATE</span><span class="o">|</span><span class="n">NOVALIDATE</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="n">RELY</span><span class="o">|</span><span class="n">NORELY</span><span class="p">]]</span><span class="w">
</span></code></pre></div><ul>
<li>Alter</li>
</ul>
<p>重命名</p>
<pre><code class="language-ssql" data-lang="ssql">ALTER TABLE table_name RENAME TO new_table_name;
</code></pre><p>更新属性</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">table_name</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="n">TBLPROPERTIES</span><span class="w"> </span><span class="p">(</span><span class="n">property_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">property_value</span><span class="p">,</span><span class="w"> </span><span class="n">property_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">property_value</span><span class="p">,</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="p">);</span><span class="w">
</span></code></pre></div><p>更新位置</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">table_name</span><span class="w"> </span><span class="p">[</span><span class="n">PARTITION</span><span class="w"> </span><span class="n">partition_spec</span><span class="p">]</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="k">LOCATION</span><span class="w"> </span><span class="n">fs_path</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>partition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。</p>
<p>更新文件格式</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">table_name</span><span class="w"> </span><span class="p">[</span><span class="n">PARTITION</span><span class="w"> </span><span class="n">partition_spec</span><span class="p">]</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="n">FILEFORMAT</span><span class="w"> </span><span class="n">file_format</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>partition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。</p>
<p>更新 SerDe 属性</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">table_name</span><span class="w"> </span><span class="p">[</span><span class="n">PARTITION</span><span class="w"> </span><span class="n">partition_spec</span><span class="p">]</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="n">SERDE</span><span class="w"> </span><span class="n">serde_class_name</span><span class="w"> </span><span class="p">[</span><span class="k">WITH</span><span class="w"> </span><span class="n">SERDEPROPERTIES</span><span class="w"> </span><span class="n">serde_properties</span><span class="p">];</span><span class="w">
</span><span class="w"> 
</span><span class="w"></span><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">table_name</span><span class="w"> </span><span class="p">[</span><span class="n">PARTITION</span><span class="w"> </span><span class="n">partition_spec</span><span class="p">]</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="n">SERDEPROPERTIES</span><span class="w"> </span><span class="n">serde_properties</span><span class="p">;</span><span class="w">
</span><span class="w"> 
</span><span class="w"></span><span class="n">serde_properties</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="n">property_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">property_value</span><span class="p">,</span><span class="w"> </span><span class="n">property_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">property_value</span><span class="p">,</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="p">)</span><span class="w">
</span></code></pre></div><p>partition_spec 如果存在，需要是一个完整的规格，即有所有分区列的值。而当它存在时，操作将被应用到相应的分区而不是表。</p>
<p>添加分区</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">table_name</span><span class="w"> </span><span class="k">ADD</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="p">(</span><span class="n">PARTITION</span><span class="w"> </span><span class="n">partition_spec</span><span class="w"> </span><span class="p">[</span><span class="k">LOCATION</span><span class="w"> </span><span class="n">fs_path</span><span class="p">])</span><span class="o">+</span><span class="p">;</span><span class="w">
</span></code></pre></div><ul>
<li>Drop Partitions</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">table_name</span><span class="w"> </span><span class="k">DROP</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="n">PARTITION</span><span class="w"> </span><span class="n">partition_spec</span><span class="p">[,</span><span class="w"> </span><span class="n">PARTITION</span><span class="w"> </span><span class="n">partition_spec</span><span class="p">,</span><span class="w"> </span><span class="p">...];</span><span class="w">
</span></code></pre></div><ul>
<li>新增/替换 列</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">table_name</span><span class="w">
</span><span class="w">  </span><span class="k">ADD</span><span class="o">|</span><span class="k">REPLACE</span><span class="w"> </span><span class="n">COLUMNS</span><span class="w"> </span><span class="p">(</span><span class="n">col_name</span><span class="w"> </span><span class="n">data_type</span><span class="w"> </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">col_comment</span><span class="p">],</span><span class="w"> </span><span class="p">...)</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">CASCADE</span><span class="o">|</span><span class="k">RESTRICT</span><span class="p">]</span><span class="w">
</span></code></pre></div><ul>
<li>Change Column</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">table_name</span><span class="w"> </span><span class="n">CHANGE</span><span class="w"> </span><span class="p">[</span><span class="k">COLUMN</span><span class="p">]</span><span class="w"> </span><span class="n">col_old_name</span><span class="w"> </span><span class="n">col_new_name</span><span class="w"> </span><span class="n">column_type</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">col_comment</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">FIRST</span><span class="o">|</span><span class="k">AFTER</span><span class="w"> </span><span class="k">column_name</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">CASCADE</span><span class="o">|</span><span class="k">RESTRICT</span><span class="p">];</span><span class="w">
</span></code></pre></div><ul>
<li>Drop</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">DROP</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="k">table_name</span><span class="p">;</span><span class="w">
</span></code></pre></div><h2 id="view">VIEW</h2>
<ul>
<li>Create</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="n">view_name</span><span class="w"> </span><span class="p">[(</span><span class="k">column_name</span><span class="p">,</span><span class="w"> </span><span class="p">...)</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">view_comment</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="n">TBLPROPERTIES</span><span class="w"> </span><span class="p">(</span><span class="n">property_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">property_value</span><span class="p">,</span><span class="w"> </span><span class="p">...)]</span><span class="w">
</span><span class="w">  </span><span class="k">AS</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="p">...;</span><span class="w">
</span></code></pre></div><ul>
<li>Alter</li>
</ul>
<p>注意：改变视图只在表 API 中工作，但不支持通过 SQL 客户端。</p>
<p>重命名</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="n">view_name</span><span class="w"> </span><span class="k">RENAME</span><span class="w"> </span><span class="k">TO</span><span class="w"> </span><span class="n">new_view_name</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>更新属性</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="n">view_name</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="n">TBLPROPERTIES</span><span class="w"> </span><span class="p">(</span><span class="n">property_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">property_value</span><span class="p">,</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="p">);</span><span class="w">
</span></code></pre></div><ul>
<li>Update As Select</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="n">view_name</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">select_statement</span><span class="p">;</span><span class="w">
</span></code></pre></div><ul>
<li>Drop</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">DROP</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="n">view_name</span><span class="p">;</span><span class="w">
</span></code></pre></div><h2 id="function">FUNCTION</h2>
<ul>
<li>Show</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SHOW</span><span class="w"> </span><span class="n">FUNCTIONS</span><span class="p">;</span><span class="w">
</span></code></pre></div><ul>
<li>Create</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">FUNCTION</span><span class="w"> </span><span class="n">function_name</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">class_name</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>Drop</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">DROP</span><span class="w"> </span><span class="k">FUNCTION</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="n">function_name</span><span class="p">;</span><span class="w">
</span></code></pre></div><h2 id="dml">DML</h2>
<h3 id="nsert">NSERT</h3>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">INSERT</span><span class="w"> </span><span class="p">(</span><span class="k">INTO</span><span class="o">|</span><span class="n">OVERWRITE</span><span class="p">)</span><span class="w"> </span><span class="p">[</span><span class="k">TABLE</span><span class="p">]</span><span class="w"> </span><span class="k">table_name</span><span class="w"> </span><span class="p">[</span><span class="n">PARTITION</span><span class="w"> </span><span class="n">partition_spec</span><span class="p">]</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="p">...;</span><span class="w">
</span></code></pre></div><p>partition_spec，如果存在的话，可以是完整规格或部分规格。如果 partition_spec 是部分规格，动态分区列名可以省略。</p>
<h2 id="dql">DQL</h2>
<p>目前，Hive 方言支持的 DQL 语法与 Flink SQL 相同。详情请参考 Flink SQL 查询。而且建议切换到默认方言来执行 DQL。</p>
<h2 id="注意事项">注意事项</h2>
<p>以下是使用 Hive 方言的一些注意事项。</p>
<ul>
<li>Hive 方言只能用于操作 Hive 表，而不是通用表。而且 Hive 方言应该和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_catalog.html">HiveCatalog</a> 一起使用。</li>
<li>虽然所有的 Hive 版本都支持相同的语法，但是否有特定的功能还是取决于你使用的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#supported-hive-versions">Hive 版本</a>。例如，更新数据库位置只在 Hive-2.4.0 或更高版本中支持。</li>
<li>Hive 和 Calcite 有不同的保留关键字集。例如，在 Calcite 中默认是保留关键字，而在 Hive 中是非保留关键字。即使是 Hive 方言，你也必须用反引号（`）来引用这些关键字，才能将它们作为标识符使用。</li>
<li>由于扩展查询不兼容，在 Flink 中创建的视图不能在 Hive 中查询。</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_dialect.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_dialect.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/hive" term="hive" label="Hive" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Hive 集成 - 概览]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Read and Write" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Streaming" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 方言" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hivecatalog/?utm_source=atom_feed" rel="related" type="text/html" title="HiveCatalog" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Overview</blockquote><h1 id="hive-集成">Hive 集成</h1>
<p>Apache Hive 已经确立了自己作为数据仓库生态系统的焦点。它不仅是大数据分析和 ETL 的 SQL 引擎，也是一个数据管理平台，在这里，数据被发现、定义和发展。</p>
<p>Flink 与 Hive 提供了两方面的整合。</p>
<p>第一是利用 Hive 的 Metastore 作为一个持久性目录，与 Flink 的 HiveCatalog 进行跨会话存储 Flink 特定的元数据。例如，用户可以通过使用 HiveCatalog 将 Kafka 或 ElasticSearch 表存储在 Hive Metastore 中，并在以后的 SQL 查询中重复使用。</p>
<p>二是提供 Flink 作为读写 Hive 表的替代引擎。</p>
<p>HiveCatalog 的设计是 &ldquo;开箱即用&rdquo;，与现有的 Hive 安装兼容。您不需要修改现有的 Hive Metastore，也不需要改变数据位置或表的分区。</p>
<h2 id="支持的-hive-版本">支持的 Hive 版本</h2>
<p>Flink 支持以下 Hive 版本。</p>
<ul>
<li>1.0
<ul>
<li>1.0.0</li>
<li>1.0.1</li>
</ul>
</li>
<li>1.1
<ul>
<li>1.1.0</li>
<li>1.1.1</li>
</ul>
</li>
<li>1.2
<ul>
<li>1.2.0</li>
<li>1.2.1</li>
<li>1.2.2</li>
</ul>
</li>
<li>2.0
<ul>
<li>2.0.0</li>
<li>2.0.1</li>
</ul>
</li>
<li>2.1
<ul>
<li>2.1.0</li>
<li>2.1.1</li>
</ul>
</li>
<li>2.2
<ul>
<li>2.2.0</li>
</ul>
</li>
<li>2.3
<ul>
<li>2.3.0</li>
<li>2.3.1</li>
<li>2.3.2</li>
<li>2.3.3</li>
<li>2.3.4</li>
<li>2.3.5</li>
<li>2.3.6</li>
</ul>
</li>
<li>3.1
<ul>
<li>3.1.0</li>
<li>3.1.1</li>
<li>3.1.2</li>
</ul>
</li>
</ul>
<p>请注意 Hive 本身在不同的版本有不同的功能，这些问题不是 Flink 造成的。</p>
<ul>
<li>1.2.0 及以后版本支持 Hive 内置函数。</li>
<li>3.1.0 及以后版本支持列约束，即 PRIMARY KEY 和 NOT NULL。</li>
<li>在 1.2.0 及以后的版本中，支持修改表的统计数据。</li>
<li>在 1.2.0 及以后的版本中支持 DATE 列统计。</li>
<li>在 2.0.x 中不支持写入 ORC 表。</li>
</ul>
<h3 id="依赖性">依赖性</h3>
<p>为了与 Hive 集成，你需要在 Flink 发行版的 <code>/lib/</code> 目录下添加一些额外的依赖关系，以使集成工作在 Table API 程序或 SQL 客户端中。另外，你也可以将这些依赖项放在一个专门的文件夹中，并分别用 <code>-C</code> 或 <code>-l</code> 选项将它们添加到 <code>classpath</code> 中，用于 Table API 程序或 SQL Client。</p>
<p>Apache Hive 是建立在 Hadoop 上的，所以首先需要 Hadoop 依赖，请参考提供 Hadoop 类。</p>
<p>有两种方法可以添加 Hive 依赖。首先是使用 Flink 的捆绑式 Hive jars。你可以根据你使用的 metastore 的版本来选择捆绑的 Hive jar。第二种是分别添加每个所需的 jar。如果你使用的 Hive 版本没有在这里列出，第二种方式就会很有用。</p>
<h4 id="使用捆绑的-hive-jar">使用捆绑的 Hive jar</h4>
<p>下表列出了所有可用的捆绑的 hive jar，你可以选择一个到 Flink 发行版的 <code>/lib/</code> 目录下。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Metastore version</th>
<th style="text-align:left">Maven dependency</th>
<th style="text-align:left">SQL Client JAR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1.0.0 - 1.2.2</td>
<td style="text-align:left">flink-sql-connector-hive-1.2.2</td>
<td style="text-align:left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-1.2.2_2.11/1.11.0/flink-sql-connector-hive-1.2.2_2.11-1.11.0.jar">Download</a></td>
</tr>
<tr>
<td style="text-align:left">2.0.0 - 2.2.0</td>
<td style="text-align:left">flink-sql-connector-hive-2.2.0</td>
<td style="text-align:left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-2.2.0_2.11/1.11.0/flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar">Download</a></td>
</tr>
<tr>
<td style="text-align:left">2.3.0 - 2.3.6</td>
<td style="text-align:left">flink-sql-connector-hive-2.3.6</td>
<td style="text-align:left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-2.3.6_2.11/1.11.0/flink-sql-connector-hive-2.3.6_2.11-1.11.0.jar">Download</a></td>
</tr>
<tr>
<td style="text-align:left">3.0.0 - 3.1.2</td>
<td style="text-align:left">flink-sql-connector-hive-3.1.2</td>
<td style="text-align:left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-3.1.2_2.11/1.11.0/flink-sql-connector-hive-3.1.2_2.11-1.11.0.jar">Download</a></td>
</tr>
</tbody>
</table>
<h4 id="用户定义的依赖性">用户定义的依赖性</h4>
<p>请在下面找到不同 Hive 主要版本所需的依赖关系。</p>
<ul>
<li>Hive 2.3.4</li>
</ul>
<pre><code>/flink-1.11.0
   /lib

       // Flink's Hive connector.Contains flink-hadoop-compatibility and flink-orc jars
       flink-connector-hive_2.11-1.11.0.jar

       // Hive dependencies
       hive-exec-2.3.4.jar
</code></pre><ul>
<li>Hive 1.0.0</li>
</ul>
<pre><code>/flink-1.11.0
   /lib

       // Flink's Hive connector
       flink-connector-hive_2.11-1.11.0.jar

       // Hive dependencies
       hive-metastore-1.0.0.jar
       hive-exec-1.0.0.jar
       libfb303-0.9.0.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately
       
       // Orc dependencies -- required by the ORC vectorized optimizations
       orc-core-1.4.3-nohive.jar
       aircompressor-0.8.jar // transitive dependency of orc-core
</code></pre><ul>
<li>Hive 1.1.0</li>
</ul>
<pre><code>/flink-1.11.0
   /lib

       // Flink's Hive connector
       flink-connector-hive_2.11-1.11.0.jar

       // Hive dependencies
       hive-metastore-1.1.0.jar
       hive-exec-1.1.0.jar
       libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately

       // Orc dependencies -- required by the ORC vectorized optimizations
       orc-core-1.4.3-nohive.jar
       aircompressor-0.8.jar // transitive dependency of orc-core
</code></pre><ul>
<li>Hive 1.2.1</li>
</ul>
<pre><code>/flink-1.11.0
   /lib

       // Flink's Hive connector
       flink-connector-hive_2.11-1.11.0.jar

       // Hive dependencies
       hive-metastore-1.2.1.jar
       hive-exec-1.2.1.jar
       libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately

       // Orc dependencies -- required by the ORC vectorized optimizations
       orc-core-1.4.3-nohive.jar
       aircompressor-0.8.jar // transitive dependency of orc-core
</code></pre><ul>
<li>Hive 2.0.0</li>
</ul>
<pre><code>/flink-1.11.0
   /lib

       // Flink's Hive connector
       flink-connector-hive_2.11-1.11.0.jar

       // Hive dependencies
       hive-exec-2.0.0.jar
</code></pre><ul>
<li>Hive 2.1.0</li>
</ul>
<pre><code>/flink-1.11.0
   /lib

       // Flink's Hive connector
       flink-connector-hive_2.11-1.11.0.jar

       // Hive dependencies
       hive-exec-2.1.0.jar
</code></pre><ul>
<li>Hive 2.2.0</li>
</ul>
<pre><code>/flink-1.11.0
   /lib

       // Flink's Hive connector
       flink-connector-hive_2.11-1.11.0.jar

       // Hive dependencies
       hive-exec-2.2.0.jar

       // Orc dependencies -- required by the ORC vectorized optimizations
       orc-core-1.4.3.jar
       aircompressor-0.8.jar // transitive dependency of orc-core
</code></pre><ul>
<li>Hive 3.1.0</li>
</ul>
<pre><code>/flink-1.11.0
   /lib

       // Flink's Hive connector
       flink-connector-hive_2.11-1.11.0.jar

       // Hive dependencies
       hive-exec-3.1.0.jar
       libfb303-0.9.3.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately
</code></pre><h3 id="program-maven">Program maven</h3>
<p>如果你正在构建你自己的程序，你需要在你的 mvn 文件中加入以下依赖关系。建议不要在生成的 jar 文件中包含这些依赖关系。你应该在运行时添加上面所说的依赖关系。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="c">&lt;!-- Flink Dependency --&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-connector-hive_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>

<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-table-api-java-bridge_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>

<span class="c">&lt;!-- Hive Dependency --&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.hive<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>hive-exec<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>${hive.version}<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><h2 id="连接到-hive">连接到 Hive</h2>
<p>通过表环境或 YAML 配置，使用目录接口和 HiveCatalog 连接到现有的 Hive 安装。</p>
<p>如果 <code>hive-conf/hive-site.xml</code> 文件存储在远程存储系统中，用户应先将 hive 配置文件下载到本地环境中。</p>
<p>请注意，虽然 HiveCatalog 不需要特定的规划师，但读/写 Hive 表只适用于 blink 规划师。因此强烈建议您在连接 Hive 仓库时使用 blink planner。</p>
<p>HiveCatalog 能够自动检测使用中的 Hive 版本。建议不要指定 Hive 版本，除非自动检测失败。</p>
<p>以 Hive 2.3.4 版本为例。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">settings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">().</span><span class="n">inBatchMode</span><span class="o">().</span><span class="n">build</span><span class="o">()</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">settings</span><span class="o">)</span>

<span class="k">val</span> <span class="n">name</span>            <span class="k">=</span> <span class="s">&#34;myhive&#34;</span>
<span class="k">val</span> <span class="n">defaultDatabase</span> <span class="k">=</span> <span class="s">&#34;mydatabase&#34;</span>
<span class="k">val</span> <span class="n">hiveConfDir</span>     <span class="k">=</span> <span class="s">&#34;/opt/hive-conf&#34;</span> <span class="c1">// a local path
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">hive</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HiveCatalog</span><span class="o">(</span><span class="n">name</span><span class="o">,</span> <span class="n">defaultDatabase</span><span class="o">,</span> <span class="n">hiveConfDir</span><span class="o">)</span>
<span class="n">tableEnv</span><span class="o">.</span><span class="n">registerCatalog</span><span class="o">(</span><span class="s">&#34;myhive&#34;</span><span class="o">,</span> <span class="n">hive</span><span class="o">)</span>

<span class="c1">// set the HiveCatalog as the current catalog of the session
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">useCatalog</span><span class="o">(</span><span class="s">&#34;myhive&#34;</span><span class="o">)</span>
</code></pre></div><h2 id="ddl">DDL</h2>
<p>建议使用 <a href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect">Hive 方言</a>在 Flink 中执行 DDL 来创建 Hive 表、视图、分区、函数。</p>
<h2 id="dml">DML</h2>
<p>Flink 支持 DML 写入 Hive 表。请参考<a href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write">读写 Hive 表</a>的细节。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/hive" term="hive" label="Hive" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[HiveCatalog]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-hivecatalog/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Read and Write" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Streaming" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 方言" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 集成 - 概览" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-hivecatalog/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>HiveCatalog</blockquote><h1 id="hivecatalog">HiveCatalog</h1>
<p>Hive Metastore 经过多年的发展，已经成为 Hadoop 生态系统中事实上的元数据中心。很多公司在生产中都有一个 Hive Metastore 服务实例来管理他们所有的元数据，无论是 Hive 元数据还是非 Hive 元数据，都是真理的来源。</p>
<p>对于同时部署了 Hive 和 Flink 的用户，HiveCatalog 可以让他们使用 Hive Metastore 来管理 Flink 的元数据。</p>
<p>对于只有 Flink 部署的用户来说，HiveCatalog 是 Flink 开箱即用的唯一持久化目录。如果没有持久化目录，用户使用 <a href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements">Flink SQL CREATE DDL</a> 必须在每个会话中反复创建元对象，比如 Kafka 表，这就浪费了很多时间。HiveCatalog 填补了这一空白，使用户只需创建一次表和其他元对象，以后就可以跨会话方便地引用和管理它们。</p>
<h2 id="设置-hivecatalog">设置 HiveCatalog</h2>
<h3 id="依赖性">依赖性</h3>
<p>在 Flink 中设置 HiveCatalog 需要与 Flink-Hive 集成相同的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#dependencies">依赖关系</a>。</p>
<h3 id="配置">配置</h3>
<p>在 Flink 中设置 HiveCatalog 需要与 Flink-Hive 集成相同的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#connecting-to-hive">配置</a>。</p>
<h2 id="如何使用-hivecatalog">如何使用 HiveCatalog</h2>
<p>一旦配置得当，HiveCatalog 应该可以直接使用。用户可以用 DDL 创建 Flink 元对象，并在创建后立即看到它们。</p>
<p>HiveCatalog 可以用来处理两种表。Hive 兼容表和通用表。Hive-compatible 表是指那些以 Hive 兼容的方式存储的表，在存储层的元数据和数据方面都是如此。因此，通过 Flink 创建的 Hive 兼容表可以从 Hive 端进行查询。</p>
<p>而通用表则是针对 Flink 的。当使用 HiveCatalog 创建通用表时，我们只是使用 HMS 来持久化元数据。虽然这些表对 Hive 是可见的，但 Hive 不太可能理解这些元数据。因此在 Hive 中使用这样的表会导致未定义的行为。</p>
<p>Flink 使用属性 &ldquo;is_generic&rdquo; 来判断一个表是与 Hive 兼容还是通用。当用 HiveCatalog 创建一个表时，它默认被认为是通用的。如果你想创建一个 Hive 兼容的表，请确保在你的表属性中把 is_generic 设置为 false。</p>
<p>如上所述，通用表不应该从 Hive 使用。在 Hive CLI 中，你可以调用 describe FORMATTED 对一个表进行检查，通过检查 is_generic 属性来决定它是否是通用的。通用表会有 is_generic=true。</p>
<h3 id="例子">例子</h3>
<p>我们在这里将通过一个简单的例子进行讲解。</p>
<p>第 1 步：设置 Hive Metastore。</p>
<p>有一个 Hive Metastore 在运行。</p>
<p>在这里，我们在本地路径 <code>/opt/hive-conf/hive-site.xml</code> 中设置一个本地 Hive Metastore 和我们的 hive-site.xml 文件。我们有如下的一些配置。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;configuration&gt;</span>
   <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionURL<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true<span class="nt">&lt;/value&gt;</span>
      <span class="nt">&lt;description&gt;</span>metadata is stored in a MySQL server<span class="nt">&lt;/description&gt;</span>
   <span class="nt">&lt;/property&gt;</span>

   <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionDriverName<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>com.mysql.jdbc.Driver<span class="nt">&lt;/value&gt;</span>
      <span class="nt">&lt;description&gt;</span>MySQL JDBC driver class<span class="nt">&lt;/description&gt;</span>
   <span class="nt">&lt;/property&gt;</span>

   <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionUserName<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>...<span class="nt">&lt;/value&gt;</span>
      <span class="nt">&lt;description&gt;</span>user name for connecting to mysql server<span class="nt">&lt;/description&gt;</span>
   <span class="nt">&lt;/property&gt;</span>

   <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionPassword<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>...<span class="nt">&lt;/value&gt;</span>
      <span class="nt">&lt;description&gt;</span>password for connecting to mysql server<span class="nt">&lt;/description&gt;</span>
   <span class="nt">&lt;/property&gt;</span>

   <span class="nt">&lt;property&gt;</span>
       <span class="nt">&lt;name&gt;</span>hive.metastore.uris<span class="nt">&lt;/name&gt;</span>
       <span class="nt">&lt;value&gt;</span>thrift://localhost:9083<span class="nt">&lt;/value&gt;</span>
       <span class="nt">&lt;description&gt;</span>IP address (or fully-qualified domain name) and port of the metastore host<span class="nt">&lt;/description&gt;</span>
   <span class="nt">&lt;/property&gt;</span>

   <span class="nt">&lt;property&gt;</span>
       <span class="nt">&lt;name&gt;</span>hive.metastore.schema.verification<span class="nt">&lt;/name&gt;</span>
       <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>

<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div><p>用 Hive Cli 测试连接到 HMS。运行一些命令，我们可以看到我们有一个名为 default 的数据库，里面没有表。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">hive&gt; show databases<span class="p">;</span>
OK
default
Time taken: 0.032 seconds, Fetched: <span class="m">1</span> row<span class="o">(</span>s<span class="o">)</span>

hive&gt; show tables<span class="p">;</span>
OK
Time taken: 0.028 seconds, Fetched: <span class="m">0</span> row<span class="o">(</span>s<span class="o">)</span>
</code></pre></div><p>步骤 2：配置 Flink 集群和 SQL CLI</p>
<p>将所有 Hive 的依赖关系添加到 Flink 发行版的 <code>/lib</code> 目录下，并修改 SQL CLI 的 yaml 配置文件 sql-cli-defaults.yaml 如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">execution</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">planner</span><span class="p">:</span><span class="w"> </span><span class="l">blink</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">streaming</span><span class="w">
</span><span class="w">    </span><span class="l">...</span><span class="w">
</span><span class="w">    </span><span class="nt">current-catalog</span><span class="p">:</span><span class="w"> </span><span class="l">myhive </span><span class="w"> </span><span class="c"># set the HiveCatalog as the current catalog of the session</span><span class="w">
</span><span class="w">    </span><span class="nt">current-database</span><span class="p">:</span><span class="w"> </span><span class="l">mydatabase</span><span class="w">
</span><span class="w">    
</span><span class="w"></span><span class="nt">catalogs</span><span class="p">:</span><span class="w">
</span><span class="w">   </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myhive</span><span class="w">
</span><span class="w">     </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">hive</span><span class="w">
</span><span class="w">     </span><span class="nt">hive-conf-dir</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/hive-conf </span><span class="w"> </span><span class="c"># contains hive-site.xml</span><span class="w">
</span></code></pre></div><p>第三步：建立 Kafka 集群</p>
<p>Bootstrap 一个本地的 Kafka 2.3.0 集群，主题命名为 &ldquo;test&rdquo;，并以 name 和 age 的元组形式向主题产生一些简单的数据。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">localhost$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic <span class="nb">test</span>
&gt;tom,15
&gt;john,21
</code></pre></div><p>这些消息可以通过启动 Kafka 控制台消费者看到。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">localhost$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <span class="nb">test</span> --from-beginning

tom,15
john,21
</code></pre></div><p>第四步：启动 SQL Client，用 Flink SQL DDL 创建一个 Kafka 表。</p>
<p>启动 Flink SQL Client，通过 DDL 创建一个简单的 Kafka 2.3.0 表，并验证其模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="n">Flink</span><span class="w"> </span><span class="k">SQL</span><span class="o">&gt;</span><span class="w"> </span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">mykafka</span><span class="w"> </span><span class="p">(</span><span class="n">name</span><span class="w"> </span><span class="n">String</span><span class="p">,</span><span class="w"> </span><span class="n">age</span><span class="w"> </span><span class="nb">Int</span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">   </span><span class="s1">&#39;connector.type&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;kafka&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">   </span><span class="s1">&#39;connector.version&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;universal&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">   </span><span class="s1">&#39;connector.topic&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;test&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">   </span><span class="s1">&#39;connector.properties.bootstrap.servers&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;localhost:9092&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">   </span><span class="s1">&#39;format.type&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;csv&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">   </span><span class="s1">&#39;update-mode&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;append&#39;</span><span class="w">
</span><span class="w"></span><span class="p">);</span><span class="w">
</span><span class="w"></span><span class="p">[</span><span class="n">INFO</span><span class="p">]</span><span class="w"> </span><span class="k">Table</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">created</span><span class="p">.</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">Flink</span><span class="w"> </span><span class="k">SQL</span><span class="o">&gt;</span><span class="w"> </span><span class="k">DESCRIBE</span><span class="w"> </span><span class="n">mykafka</span><span class="p">;</span><span class="w">
</span><span class="w"></span><span class="n">root</span><span class="w">
</span><span class="w"> </span><span class="o">|</span><span class="c1">-- name: STRING
</span><span class="c1"></span><span class="w"> </span><span class="o">|</span><span class="c1">-- age: INT
</span></code></pre></div><p>验证该表也是通过 Hive Cli 对 Hive 可见的，注意该表有属性 is_generic=true。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">hive&gt; show tables<span class="p">;</span>
OK
mykafka
Time taken: 0.038 seconds, Fetched: <span class="m">1</span> row<span class="o">(</span>s<span class="o">)</span>

hive&gt; describe formatted mykafka<span class="p">;</span>
OK
<span class="c1"># col_name            	data_type           	comment</span>


<span class="c1"># Detailed Table Information</span>
Database:           	default
Owner:              	null
CreateTime:         	......
LastAccessTime:     	UNKNOWN
Retention:          	<span class="m">0</span>
Location:           	......
Table Type:         	MANAGED_TABLE
Table Parameters:
	flink.connector.properties.bootstrap.servers	localhost:9092
	flink.connector.topic	<span class="nb">test</span>
	flink.connector.type	kafka
	flink.connector.version	universal
	flink.format.type   	csv
	flink.generic.table.schema.0.data-type	VARCHAR<span class="o">(</span>2147483647<span class="o">)</span>
	flink.generic.table.schema.0.name	name
	flink.generic.table.schema.1.data-type	INT
	flink.generic.table.schema.1.name	age
	flink.update-mode   	append
	is_generic          	<span class="nb">true</span>
	transient_lastDdlTime	......

<span class="c1"># Storage Information</span>
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat
OutputFormat:       	org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
Compressed:         	No
Num Buckets:        	-1
Bucket Columns:     	<span class="o">[]</span>
Sort Columns:       	<span class="o">[]</span>
Storage Desc Params:
	serialization.format	<span class="m">1</span>
Time taken: 0.158 seconds, Fetched: <span class="m">36</span> row<span class="o">(</span>s<span class="o">)</span>
</code></pre></div><p>第五步：运行 Flink SQL 查询 Kakfa 表。</p>
<p>在 Flink 集群中，无论是单机还是 yarn-session，从 Flink SQL Client 中运行一个简单的选择查询。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; <span class="k">select</span> * from mykafka<span class="p">;</span>
</code></pre></div><p>在 Kafka 主题中多产生一些信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">localhost$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <span class="nb">test</span> --from-beginning

tom,15
john,21
kitty,30
amy,24
kaiky,18
</code></pre></div><p>你现在应该可以在 SQL Client 中看到 Flink 产生的结果，如。</p>
<pre><code>             SQL Query Result (Table)
 Refresh: 1 s    Page: Last of 1     

        name                       age
         tom                        15
        john                        21
       kitty                        30
         amy                        24
       kaiky                        18
</code></pre><h2 id="支持的类型">支持的类型</h2>
<p>HiveCatalog 支持通用表的所有 Flink 类型。</p>
<p>对于 Hive 兼容的表，HiveCatalog 需要将 Flink 数据类型映射到相应的 Hive 类型，如下表所述。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Flink Data Type</th>
<th style="text-align:left">Hive Data Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">CHAR(p)</td>
<td style="text-align:left">CHAR(p)</td>
</tr>
<tr>
<td style="text-align:left">VARCHAR(p)</td>
<td style="text-align:left">VARCHAR(p)</td>
</tr>
<tr>
<td style="text-align:left">STRING</td>
<td style="text-align:left">STRING</td>
</tr>
<tr>
<td style="text-align:left">BOOLEAN</td>
<td style="text-align:left">BOOLEAN</td>
</tr>
<tr>
<td style="text-align:left">TINYINT</td>
<td style="text-align:left">TINYINT</td>
</tr>
<tr>
<td style="text-align:left">SMALLINT</td>
<td style="text-align:left">SMALLINT</td>
</tr>
<tr>
<td style="text-align:left">INT</td>
<td style="text-align:left">INT</td>
</tr>
<tr>
<td style="text-align:left">BIGINT</td>
<td style="text-align:left">LONG</td>
</tr>
<tr>
<td style="text-align:left">FLOAT</td>
<td style="text-align:left">FLOAT</td>
</tr>
<tr>
<td style="text-align:left">DOUBLE</td>
<td style="text-align:left">DOUBLE</td>
</tr>
<tr>
<td style="text-align:left">DECIMAL(p, s)</td>
<td style="text-align:left">DECIMAL(p, s)</td>
</tr>
<tr>
<td style="text-align:left">DATE</td>
<td style="text-align:left">DATE</td>
</tr>
<tr>
<td style="text-align:left">TIMESTAMP(9)</td>
<td style="text-align:left">TIMESTAMP</td>
</tr>
<tr>
<td style="text-align:left">BYTES</td>
<td style="text-align:left">BINARY</td>
</tr>
<tr>
<td style="text-align:left">ARRAY<!-- raw HTML omitted --></td>
<td style="text-align:left">LIST<!-- raw HTML omitted --></td>
</tr>
<tr>
<td style="text-align:left">MAP&lt;K, V&gt;</td>
<td style="text-align:left">MAP&lt;K, V&gt;</td>
</tr>
<tr>
<td style="text-align:left">ROW</td>
<td style="text-align:left">STRUCT</td>
</tr>
</tbody>
</table>
<p>关于类型映射需要注意的地方。</p>
<ul>
<li>Hive 的 CHAR(p) 最大长度为 255。</li>
<li>Hive 的 VARCHAR(p) 最大长度为 65535。</li>
<li>Hive 的 MAP 只支持基元键类型，而 Flink 的 MAP 可以是任何数据类型。</li>
<li>不支持 Hive 的 UNION 类型。</li>
<li>Hive 的 TIMESTAMP 的精度总是 9，不支持其他精度。而 Hive 的 UDF 则可以处理精度&lt;=9 的 TIMESTAMP 值。</li>
<li>Hive 不支持 Flink 的 TIMESTAMP_WITH_TIME_ZONE、TIMESTAMP_WITH_LOCAL_TIME_ZONE 和 MULTISET。</li>
<li>Flink 的 INTERVAL 类型还不能映射到 Hive 的 INTERVAL 类型。</li>
</ul>
<h2 id="scala-shell">Scala Shell</h2>
<p>注意：由于目前 Scala Shell 并不支持 blink planner，所以不建议在 Scala Shell 中使用 Hive 连接器。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_catalog.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_catalog.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Streaming Aggregation]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-streaming-aggregation/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Read and Write" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Streaming" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 方言" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 集成 - 概览" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-streaming-aggregation/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Streaming Aggregation</blockquote><h1 id="流式聚合">流式聚合</h1>
<p>SQL 是数据分析中使用最广泛的语言。Flink 的 Table API 和 SQL 可以让用户用更少的时间和精力定义高效的流分析应用。此外，Flink Table API 和 SQL 还进行了有效的优化，它集成了大量的查询优化和调整后的运算符实现。但并不是所有的优化都是默认启用的，所以对于一些工作负载，可以通过开启一些选项来提高性能。</p>
<p>在本页面中，我们将介绍一些有用的优化选项和流式聚合的内部结构，在某些情况下会带来很大的改善。</p>
<p>注意: 目前，本页面中提到的优化选项仅在 Blink 计划器中支持。</p>
<p>注意: 目前，流式聚合的优化只支持<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#aggregations">无边界聚合</a>。未来将支持<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#group-windows">窗口聚合</a>的优化。</p>
<p>默认情况下，无界聚合运算符对输入记录进行逐一处理，即：（1）从状态中读取累加器，（2）累加/缩减记录到累加器，（3）将累加器写回状态，（4）下一条记录将从（1）开始重新做处理。这种处理模式可能会增加 StateBackend 的开销（尤其是对于 RocksDB StateBackend）。此外，在生产中很常见的数据偏斜也会使问题更加严重，容易出现作业背压的情况。</p>
<h2 id="迷你批处理minibatch聚合">迷你批处理(MiniBatch)聚合</h2>
<p>迷你批处理(mini-batch)聚合的核心思想是将一捆输入缓存在聚合运算器内部的缓冲区中。当触发处理该捆输入时，每个键只需要一个操作来访问状态。这样可以大大降低状态开销，获得更好的吞吐量。但是，这可能会增加一些延迟，因为它缓冲了一些记录，而不是在瞬间处理它们。这就是吞吐量和延迟之间的权衡。</p>
<p>下图解释了迷你批处理聚合如何减少状态操作。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/table-streaming/minibatch_agg.png" alt="img"></p>
<p>MiniBatch 优化默认为禁用。为了启用此优化，您应该设置选项 table.exec.mini-batch.enabled、table.exec.mini-batch.allow-latency 和 table.exec.mini-batch.size。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/config.html#execution-options">配置</a>页面了解更多详情。</p>
<p>下面的示例展示了如何启用这些选项。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// instantiate table environment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tEnv</span><span class="k">:</span> <span class="kt">TableEnvironment</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1">// access flink configuration
</span><span class="c1"></span><span class="k">val</span> <span class="n">configuration</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">getConfig</span><span class="o">().</span><span class="n">getConfiguration</span><span class="o">()</span>
<span class="c1">// set low-level key-value options
</span><span class="c1"></span><span class="n">configuration</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.exec.mini-batch.enabled&#34;</span><span class="o">,</span> <span class="s">&#34;true&#34;</span><span class="o">)</span> <span class="c1">// enable mini-batch optimization
</span><span class="c1"></span><span class="n">configuration</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.exec.mini-batch.allow-latency&#34;</span><span class="o">,</span> <span class="s">&#34;5 s&#34;</span><span class="o">)</span> <span class="c1">// use 5 seconds to buffer input records
</span><span class="c1"></span><span class="n">configuration</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.exec.mini-batch.size&#34;</span><span class="o">,</span> <span class="s">&#34;5000&#34;</span><span class="o">)</span> <span class="c1">// the maximum number of records can be buffered by each aggregate operator task
</span></code></pre></div><h2 id="local-global-聚合">Local-Global 聚合</h2>
<p>Local-Global 是为了解决数据偏斜问题而提出的，将一个分组聚合分为两个阶段，即先在上游做局部聚合，然后在下游做全局聚合，这类似于 MapReduce 中的 Combine+Reduce 模式。例如，考虑以下 SQL。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">color</span><span class="p">,</span><span class="w"> </span><span class="k">sum</span><span class="p">(</span><span class="n">id</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">T</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">color</span><span class="w">
</span></code></pre></div><p>有可能数据流中的记录是倾斜的，因此一些聚合运算符实例要处理的记录比其他实例多得多，从而导致热点。本地聚合可以帮助将一定数量的具有相同键的输入累积到一个累积器中。全局聚合将只接收减少的累加器，而不是大量的原始输入。这可以显著降低网络洗牌和状态访问的成本。本地聚合每次积累的输入数量是基于小批量间隔的。这意味着本地-全局聚合取决于迷你批量优化的启用。</p>
<p>下图显示了本地-全局聚合如何提高性能。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/table-streaming/local_agg.png" alt="img"></p>
<p>下面的例子展示了如何启用本地-全局聚合。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// instantiate table environment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tEnv</span><span class="k">:</span> <span class="kt">TableEnvironment</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1">// access flink configuration
</span><span class="c1"></span><span class="k">val</span> <span class="n">configuration</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">getConfig</span><span class="o">().</span><span class="n">getConfiguration</span><span class="o">()</span>
<span class="c1">// set low-level key-value options
</span><span class="c1"></span><span class="n">configuration</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.exec.mini-batch.enabled&#34;</span><span class="o">,</span> <span class="s">&#34;true&#34;</span><span class="o">)</span> <span class="c1">// local-global aggregation depends on mini-batch is enabled
</span><span class="c1"></span><span class="n">configuration</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.exec.mini-batch.allow-latency&#34;</span><span class="o">,</span> <span class="s">&#34;5 s&#34;</span><span class="o">)</span>
<span class="n">configuration</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.exec.mini-batch.size&#34;</span><span class="o">,</span> <span class="s">&#34;5000&#34;</span><span class="o">)</span>
<span class="n">configuration</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.optimizer.agg-phase-strategy&#34;</span><span class="o">,</span> <span class="s">&#34;TWO_PHASE&#34;</span><span class="o">)</span> <span class="c1">// enable two-phase, i.e. local-global aggregation
</span></code></pre></div><h2 id="split-distinct-聚合">Split Distinct 聚合</h2>
<p>Local-Global 优化对于一般的聚合，如 SUM、COUNT、MAX、MIN、AVG，可以有效地消除数据偏斜。但在处理不同的聚合时，其性能并不理想。</p>
<p>例如，如果我们想分析今天有多少独特的用户登录。我们可以有如下查询:</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">day</span><span class="p">,</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="n">user_id</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">T</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">day</span><span class="w">
</span></code></pre></div><p>如果 distinct key（即 user_id）的值很稀疏，<code>count distinct</code> 不好减少记录。即使启用了局部-全局优化，也没有什么帮助。因为累加器仍然包含了几乎所有的原始记录，而全局聚合将是瓶颈（大部分重度累加器是由一个任务处理的，即在同一天）。</p>
<p>这个优化的思路是将不同的聚合（如 COUNT(DISTINCT col)）分成两个层次。第一层聚合由组键和一个额外的桶键进行洗牌。桶键使用 HASH_CODE(distinct_key) % BUCKET_NUM 计算。BUCKET_NUM 默认为 1024，可以通过 <code>table.optimizer.distinct-agg.split.bucket-num</code> 选项进行配置。第二次聚合是按原组键进行洗牌，用 SUM 聚合不同桶的 COUNT DISTINCT 值。因为相同的 distinct key 只会在同一个 bucket 中计算，所以转换是等价的。桶键起到了额外的组键的作用，分担组键中热点的负担。桶键使得工作具有可扩展性，可以解决 distinct aggregations 中的数据偏斜/热点问题。</p>
<p>拆分不同的聚合后，上面的查询会自动改写成下面的查询:</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">day</span><span class="p">,</span><span class="w"> </span><span class="k">SUM</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="k">SELECT</span><span class="w"> </span><span class="k">day</span><span class="p">,</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="n">user_id</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">cnt</span><span class="w">
</span><span class="w">    </span><span class="k">FROM</span><span class="w"> </span><span class="n">T</span><span class="w">
</span><span class="w">    </span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">day</span><span class="p">,</span><span class="w"> </span><span class="k">MOD</span><span class="p">(</span><span class="n">HASH_CODE</span><span class="p">(</span><span class="n">user_id</span><span class="p">),</span><span class="w"> </span><span class="mi">1024</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">day</span><span class="w">
</span></code></pre></div><p>下图显示了拆分不同的聚合如何提高性能（假设颜色代表天数，字母代表 user_id）。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/table-streaming/distinct_split.png" alt="img"></p>
<p>注：以上是最简单的例子，可以从这个优化中受益。除此之外，Flink 还支持拆分更复杂的聚合查询，例如，多个不同键的不同聚合（如 COUNT(DISTINCT a)，SUM(DISTINCT b)），以及与其他非不同聚合（如 SUM, MAX, MIN, COUNT）一起工作。</p>
<p>注意，目前，分割优化不支持包含用户定义的 AggregateFunction 的聚合。</p>
<p>下面的例子展示了如何启用拆分不同的聚合优化。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// instantiate table environment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tEnv</span><span class="k">:</span> <span class="kt">TableEnvironment</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">tEnv</span><span class="o">.</span><span class="n">getConfig</span>         <span class="c1">// access high-level configuration
</span><span class="c1"></span>  <span class="o">.</span><span class="n">getConfiguration</span>    <span class="c1">// set low-level key-value options
</span><span class="c1"></span>  <span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.optimizer.distinct-agg.split.enabled&#34;</span><span class="o">,</span> <span class="s">&#34;true&#34;</span><span class="o">)</span>  <span class="c1">// enable distinct agg split
</span></code></pre></div><h2 id="在-distinct-聚合上使用-filter-修饰符">在 Distinct 聚合上使用 FILTER 修饰符</h2>
<p>在某些情况下，用户可能需要从不同的维度来计算 UV(唯一访客)的数量，例如：Android 的 UV、iPhone 的 UV、Web 的 UV 以及总的 UV。很多用户会选择 CASE WHEN 来支持，比如。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w">
</span><span class="w"> </span><span class="k">day</span><span class="p">,</span><span class="w">
</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="n">user_id</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">total_uv</span><span class="p">,</span><span class="w">
</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="k">CASE</span><span class="w"> </span><span class="k">WHEN</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="k">IN</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;android&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;iphone&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">THEN</span><span class="w"> </span><span class="n">user_id</span><span class="w"> </span><span class="k">ELSE</span><span class="w"> </span><span class="k">NULL</span><span class="w"> </span><span class="k">END</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">app_uv</span><span class="p">,</span><span class="w">
</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="k">CASE</span><span class="w"> </span><span class="k">WHEN</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="k">IN</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;wap&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;other&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">THEN</span><span class="w"> </span><span class="n">user_id</span><span class="w"> </span><span class="k">ELSE</span><span class="w"> </span><span class="k">NULL</span><span class="w"> </span><span class="k">END</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">web_uv</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">T</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">day</span><span class="w">
</span></code></pre></div><p>但是，在这种情况下，建议使用 FILTER 语法，而不是 CASE WHEN。因为 FILTER 更符合 SQL 标准，会得到更多的性能提升。FILTER 是用于聚合函数上的修饰符，用于限制聚合中使用的值。用 FILTER 修饰符替换上面的例子，如下所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w">
</span><span class="w"> </span><span class="k">day</span><span class="p">,</span><span class="w">
</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="n">user_id</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">total_uv</span><span class="p">,</span><span class="w">
</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="n">user_id</span><span class="p">)</span><span class="w"> </span><span class="n">FILTER</span><span class="w"> </span><span class="p">(</span><span class="k">WHERE</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="k">IN</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;android&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;iphone&#39;</span><span class="p">))</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">app_uv</span><span class="p">,</span><span class="w">
</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="n">user_id</span><span class="p">)</span><span class="w"> </span><span class="n">FILTER</span><span class="w"> </span><span class="p">(</span><span class="k">WHERE</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="k">IN</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;wap&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;other&#39;</span><span class="p">))</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">web_uv</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">T</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">day</span><span class="w">
</span></code></pre></div><p>Flink SQL 优化器可以识别同一个独立键上的不同过滤参数。例如，在上面的例子中，三个 COUNT DISTINCT 都在 user_id 列上。那么 Flink 就可以只使用一个共享状态实例而不是三个状态实例来减少状态访问和状态大小。在一些工作负载中，这可以得到显著的性能提升。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tuning/streaming_aggregation_optimization.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tuning/streaming_aggregation_optimization.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[User Defined Sources and Sinks]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-user-defined-sources-and-sinks/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Read and Write" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Streaming" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 方言" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 集成 - 概览" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-user-defined-sources-and-sinks/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>User Defined Sources and Sinks</blockquote><h1 id="用户自定义源和接收器">用户自定义源和接收器</h1>
<p>动态表是 Flink 的表与 SQL API 的核心概念，用于统一处理有界和无界数据。</p>
<p>因为动态表只是一个逻辑概念，Flink 并不拥有数据本身。相反，动态表的内容存储在外部系统（如数据库、键值存储、消息队列）或文件中。</p>
<p>动态源和动态汇可以用来从外部系统读取和写入数据。在文档中，源和汇通常被总结为连接器一词。</p>
<p>Flink 为 Kafka、Hive 和不同的文件系统提供了预定义的连接器。有关内置表源和汇的更多信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/">连接器部分</a>。</p>
<p>本页主要介绍如何开发一个自定义的、用户定义的连接器。</p>
<p>注意: Flink 1.11 中引入了新的表源和表汇接口，作为 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-95%3A+New+TableSource+and+TableSink+interfaces">FLIP-95</a> 的一部分。同时工厂接口也被重新设计。FLIP-95 还没有完全实现。许多能力接口还不支持(例如用于过滤器或分区推倒)。如果有必要，还请看一下<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/legacySourceSinks.html">旧 table source 和接收器的页面</a>。为了向后兼容，这些接口仍然被支持。</p>
<h2 id="概述">概述</h2>
<p>在许多情况下，实现者不需要从头开始创建一个新的连接器，而是希望稍微修改现有的连接器或挂入现有的堆栈。在其他情况下，实现者希望创建专门的连接器。</p>
<p>本节将为这两种用例提供帮助。它解释了表连接器的一般架构，从 API 中的纯声明到将在集群上执行的运行时代码。</p>
<p>填充的箭头显示了在翻译过程中，对象如何从一个阶段转换到下一个阶段的其他对象。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/table_connectors.svg" alt="img"></p>
<h3 id="元数据">元数据</h3>
<p>表 API 和 SQL 都是声明式 API。这包括表的声明。因此，执行 CREATE TABLE 语句的结果是更新目标目录中的元数据。</p>
<p>对于大多数目录实现来说，外部系统中的物理数据不会因为这样的操作而被修改。特定于连接器的依赖关系还不必存在于 classpath 中。在 WITH 子句中声明的选项既不进行验证，也不进行其他解释。</p>
<p>动态表（通过 DDL 创建或由目录提供）的元数据被表示为 CatalogTable 的实例。表名将在必要时在内部被解析为 CatalogTable。</p>
<h3 id="计划planning">计划(Planning)</h3>
<p>当涉及到表程序的规划和优化时，需要将 CatalogTable 解析为 DynamicTableSource（用于在 SELECT 查询中读取）和 DynamicTableSink（用于在 INSERT INTO 语句中写入）。</p>
<p>DynamicTableSourceFactory 和 DynamicTableSinkFactory 提供了连接器特有的逻辑，用于将 CatalogTable 的元数据翻译成 DynamicTableSource 和 DynamicTableSink 的实例。在大多数情况下，工厂的目的是验证选项（如示例中的 &lsquo;port&rsquo; = &lsquo;5022&rsquo;），配置编码/解码格式（如果需要），并创建表连接器的参数化实例。</p>
<p>默认情况下，DynamicTableSourceFactory 和 DynamicTableSinkFactory 的实例是通过 Java 的<a href="https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html">服务提供商接口</a>（SPI）发现的。连接器选项（如本例中的&rsquo;连接器'=&lsquo;自定义&rsquo;）必须对应一个有效的工厂标识符。</p>
<p>虽然在类的命名中可能并不明显，但 DynamicTableSource 和 DynamicTableSink 也可以被看作是有状态的工厂，最终产生具体的运行时实现来读取/写入实际数据。</p>
<p>规划者使用源和汇实例来执行特定连接器的双向通信，直到找到一个最佳的逻辑计划。根据可选声明的能力接口（如 Supp SupportsProjectionPushDown 或 Supp SupportsOverwrite），规划者可能会对一个实例进行更改，从而对生成的运行时实现进行突变。</p>
<h3 id="运行时">运行时</h3>
<p>逻辑规划完成后，规划师将从表连接器中获取运行时实现。运行时逻辑在 Flink 的核心连接器接口中实现，如 InputFormat 或 SourceFunction。</p>
<p>这些接口被另一层抽象归为 ScanRuntimeProvider、LookupRuntimeProvider 和 SinkRuntimeProvider 的子类。</p>
<p>例如，OutputFormatProvider(提供 <code>org.apache.flink.api.common.io.OutputFormat</code>) 和 SinkFunctionProvider(提供 <code>org.apache.flink.streaming.api.function.sink.SinkFunction</code>) 都是规划者可以处理的 SinkRuntimeProvider 的具体实例。</p>
<h2 id="扩展点">扩展点</h2>
<p>本节解释了用于扩展 Flink 的表连接器的可用接口。</p>
<h3 id="动态表因素">动态表因素</h3>
<p>动态表工厂用于根据目录和会话信息为外部存储系统配置动态表连接器。</p>
<p><code>org.apache.flink.table.fants.DynamicTableSourceFactory</code> 可以实现来构造一个 DynamicTableSource。</p>
<p><code>org.apache.flink.table.fants.DynamicTableSinkFactory</code> 可以被实现来构造一个 DynamicTableSink。</p>
<p>默认情况下，使用连接器选项的值作为工厂标识符和 Java 的服务提供者接口来发现工厂。</p>
<p>在 JAR 文件中，可以在服务文件中添加对新实现的引用。</p>
<p>META-INF/services/org.apache.flink.table.factory.Factory。</p>
<p>框架将检查单个匹配的工厂，该工厂由工厂标识符和请求的基类（如 DynamicTableSourceFactory）唯一识别。</p>
<p>如果有必要，工厂发现过程可以由目录实现绕过。为此，目录需要在 <code>org.apache.flink.table.catalog.Catalog#getFactory</code> 中返回一个实现请求的基类的实例。</p>
<h3 id="动态-table-source">动态 Table Source</h3>
<p>根据定义，动态表可以随时间变化。</p>
<p>当读取一个动态表时，其内容可以被认为是。</p>
<ul>
<li>一个变化日志（有限的或无限的），所有的变化都会被持续消耗，直到变化日志耗尽。这由 ScanTableSource 接口来表示。</li>
<li>一个持续变化的或非常大的外部表，其内容通常不会被完全读取，而是在必要时查询单个值。这由 LookupTableSource 接口来表示。</li>
</ul>
<p>一个类可以同时实现这两个接口。规划师根据指定的查询来决定它们的用途。</p>
<h4 id="扫描-table-source">扫描 Table Source</h4>
<p>ScanTableSource 在运行时扫描来自外部存储系统的所有行，扫描的行不一定只包含插入，也可以包含更新和删除。</p>
<p>扫描的行不一定只包含插入，也可以包含更新和删除。因此，该表源可用于读取（有限或无限）的变更日志。返回的变更日志模式表示计划员在运行时可以预期的变更集。</p>
<p>对于常规的批处理方案，源可以发出只插入行的有界流。</p>
<p>对于常规的流式方案，源可以发出只插入行的无界流。</p>
<p>对于变化数据捕获（CDC）场景，源可以发出有界或无界的流，包含插入、更新和删除行。</p>
<p>Table Source 可以实现更多的能力接口，如 Supp SupportsProjectionPushDown，可能在规划期间突变一个实例。所有的能力都列在 <code>org.apache.flink.table.connector.source.abilities</code> 包和 <code>org.apache.flink.table.connector.source.ScanTableSource</code> 的文档中。</p>
<p>ScanTableSource 的运行时实现必须产生内部数据结构。因此，记录必须以 org.apache.flink.table.data.RowData 的形式发出。框架提供了运行时转换器，这样一个源仍然可以在普通的数据结构上工作，并在最后进行转换。</p>
<h5 id="查询-table-source">查询 Table Source</h5>
<p>LookupTableSource 在运行时通过一个或多个键来查找外部存储系统的行。</p>
<p>与 ScanTableSource 相比，LookupTableSource 不需要读取整个表，可以在必要的时候从外部表（可能是不断变化的）中懒惰地获取单个值。</p>
<p>与 ScanTableSource 相比，LookupTableSource 目前只支持发出只插入的变化。</p>
<p>不支持更多的能力。更多信息请参见 <code>org.apache.flink.table.connector.source.LookupTableSource</code> 的文档。</p>
<p>LookupTableSource 的运行时实现是一个 TableFunction 或 AsyncTableFunction。该函数将在运行时调用给定的查找键的值。</p>
<h3 id="动态-table-sink">动态 Table Sink</h3>
<p>根据定义，动态表可以随时间变化。</p>
<p>在编写动态表时，可以始终将内容视为一个 changelog（有限或无限），对于这个 changelog，所有的变化都会被连续写出来，直到 changelog 用完为止。返回的 changelog 模式表明了 sink 在运行时接受的变化集。</p>
<p>对于常规的批处理方案，sink 可以只接受只插入的行，并写出有界流。</p>
<p>对于常规的流式方案，sink 可以只接受只插入的行，并且可以写出无约束的流。</p>
<p>对于变化数据捕获（CDC）场景，table sink 可以写出有界流或无界流，有插入、更新和删除行。</p>
<p>Table sink 可以实现更多的能力接口，如 SupportsOverwrite，可能在规划期间突变一个实例。所有的能力都列在 <code>org.apache.flink.table.connector.sink.abilities</code> 包和 org.apache.flink.table.connector.sink.DynamicTableSink 的文档中。</p>
<p>DynamicTableSink 的运行时实现必须消耗内部数据结构。因此，记录必须被接受为 org.apache.flink.table.data.RowData。该框架提供了运行时转换器，这样一个 sink 仍然可以在普通的数据结构上工作，并在开始时执行转换。</p>
<h3 id="编码解码格式">编码/解码格式</h3>
<p>一些表连接器接受不同的格式，对键和/或值进行编码和解码。</p>
<p>格式的工作模式类似于 DynamicTableSourceFactory-&gt;DynamicTableSource-&gt;ScanRuntimeProvider，工厂负责翻译选项，源头负责创建运行时逻辑。</p>
<p>因为格式可能位于不同的模块中，所以使用 Java 的服务提供者接口发现它们，类似于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sourceSinks.html#dynamic-table-factories">表工厂</a>。为了发现格式工厂，动态表工厂会搜索与工厂标识符和连接器特定基类相对应的工厂。</p>
<p>例如，Kafka 表源需要一个 DeserializationSchema 作为解码格式的运行时接口。因此，Kafka 表源工厂使用 value.format 选项的值来发现一个 DeserializationFormatFactory。</p>
<p>目前支持以下格式工厂。</p>
<ul>
<li>org.apache.flink.table.factories.DeserializationFormatFactory</li>
<li>org.apache.flink.table.factories.SerializationFormatFactory</li>
</ul>
<p>格式工厂将选项翻译成 EncodingFormat 或 DecodingFormat。这些接口是另一种工厂，为给定的数据类型产生专门的格式运行时逻辑。</p>
<p>例如，对于 Kafka table source 工厂，DeserializationFormatFactory 将返回一个 <code>EncodingFormat&lt;DeserializationSchema&gt;</code>，它可以传递到 Kafka 表源中。</p>
<h2 id="全栈示例">全栈示例</h2>
<p>本节简要介绍了如何实现一个扫描表源，其解码格式支持 changelog 语义。这个例子说明了所有提到的组件如何一起发挥作用。它可以作为一个参考实现。</p>
<p>特别是，它展示了如何</p>
<ul>
<li>创建解析和验证选项的工厂。</li>
<li>实现表连接器。</li>
<li>实现和发现自定义格式。</li>
<li>并使用提供的实用程序，如数据结构转换器和 FactoryUtil。</li>
</ul>
<p>Table Source 使用一个简单的单线程 SourceFunction 来打开一个监听传入字节的套接字。原始字节由一个可插拔的格式解码成行。该格式期望以 changelog 标志作为第一列。</p>
<p>我们将使用上面提到的大部分接口来实现下面的 DDL。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">UserScores</span><span class="w"> </span><span class="p">(</span><span class="n">name</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w"> </span><span class="n">score</span><span class="w"> </span><span class="nb">INT</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="s1">&#39;connector&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;socket&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="s1">&#39;hostname&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="s1">&#39;port&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;9999&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="s1">&#39;byte-delimiter&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;10&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="s1">&#39;format&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;changelog-csv&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="s1">&#39;changelog-csv.column-delimiter&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;|&#39;</span><span class="w">
</span><span class="w"></span><span class="p">);</span><span class="w">
</span></code></pre></div><p>由于该格式支持 changelog 语义，我们能够在运行时摄取更新，并创建一个能够持续评估变化数据的更新视图。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="k">SUM</span><span class="p">(</span><span class="n">score</span><span class="p">)</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">UserScores</span><span class="w"> </span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">name</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>使用以下命令在终端中摄取数据。</p>
<pre><code>&gt; nc -lk 9999
INSERT|Alice|12
INSERT|Bob|5
DELETE|Alice|12
INSERT|Alice|18
</code></pre><h3 id="工厂">工厂</h3>
<p>本节说明了如何将来自目录的元数据翻译成具体的连接器实例。</p>
<p>这两个工厂都被添加到 META-INF/services 目录中。</p>
<p><strong>SocketDynamicTableFactory</strong></p>
<p>SocketDynamicTableFactory 将目录表翻译成表源。由于表源需要解码格式，为了方便，我们使用提供的 FactoryUtil 发现格式。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.serialization.DeserializationSchema</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.configuration.ConfigOption</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.configuration.ConfigOptions</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.configuration.ReadableConfig</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.format.DecodingFormat</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.source.DynamicTableSource</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.data.RowData</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.factories.DeserializationFormatFactory</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.factories.DynamicTableSourceFactory</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.factories.FactoryUtil</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.types.DataType</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">SocketDynamicTableFactory</span> <span class="kd">implements</span> <span class="n">DynamicTableSourceFactory</span> <span class="o">{</span>

  <span class="c1">// define all options statically
</span><span class="c1"></span>  <span class="kd">public</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">ConfigOption</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">HOSTNAME</span> <span class="o">=</span> <span class="n">ConfigOptions</span><span class="o">.</span><span class="na">key</span><span class="o">(</span><span class="s">&#34;hostname&#34;</span><span class="o">)</span>
    <span class="o">.</span><span class="na">stringType</span><span class="o">()</span>
    <span class="o">.</span><span class="na">noDefaultValue</span><span class="o">();</span>

  <span class="kd">public</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">ConfigOption</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">PORT</span> <span class="o">=</span> <span class="n">ConfigOptions</span><span class="o">.</span><span class="na">key</span><span class="o">(</span><span class="s">&#34;port&#34;</span><span class="o">)</span>
    <span class="o">.</span><span class="na">intType</span><span class="o">()</span>
    <span class="o">.</span><span class="na">noDefaultValue</span><span class="o">();</span>

  <span class="kd">public</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">ConfigOption</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">BYTE_DELIMITER</span> <span class="o">=</span> <span class="n">ConfigOptions</span><span class="o">.</span><span class="na">key</span><span class="o">(</span><span class="s">&#34;byte-delimiter&#34;</span><span class="o">)</span>
    <span class="o">.</span><span class="na">intType</span><span class="o">()</span>
    <span class="o">.</span><span class="na">defaultValue</span><span class="o">(</span><span class="n">10</span><span class="o">);</span> <span class="c1">// corresponds to &#39;\n&#39;
</span><span class="c1"></span>
  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">String</span> <span class="nf">factoryIdentifier</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="s">&#34;socket&#34;</span><span class="o">;</span> <span class="c1">// used for matching to `connector = &#39;...&#39;`
</span><span class="c1"></span>  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">Set</span><span class="o">&lt;</span><span class="n">ConfigOption</span><span class="o">&lt;?&gt;&gt;</span> <span class="n">requiredOptions</span><span class="o">()</span> <span class="o">{</span>
    <span class="kd">final</span> <span class="n">Set</span><span class="o">&lt;</span><span class="n">ConfigOption</span><span class="o">&lt;?&gt;&gt;</span> <span class="n">options</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashSet</span><span class="o">&lt;&gt;();</span>
    <span class="n">options</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">HOSTNAME</span><span class="o">);</span>
    <span class="n">options</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">PORT</span><span class="o">);</span>
    <span class="n">options</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">FactoryUtil</span><span class="o">.</span><span class="na">FORMAT</span><span class="o">);</span> <span class="c1">// use pre-defined option for format
</span><span class="c1"></span>    <span class="k">return</span> <span class="n">options</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">Set</span><span class="o">&lt;</span><span class="n">ConfigOption</span><span class="o">&lt;?&gt;&gt;</span> <span class="n">optionalOptions</span><span class="o">()</span> <span class="o">{</span>
    <span class="kd">final</span> <span class="n">Set</span><span class="o">&lt;</span><span class="n">ConfigOption</span><span class="o">&lt;?&gt;&gt;</span> <span class="n">options</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashSet</span><span class="o">&lt;&gt;();</span>
    <span class="n">options</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">BYTE_DELIMITER</span><span class="o">);</span>
    <span class="k">return</span> <span class="n">options</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">DynamicTableSource</span> <span class="nf">createDynamicTableSource</span><span class="o">(</span><span class="n">Context</span> <span class="n">context</span><span class="o">)</span> <span class="o">{</span>
    <span class="c1">// either implement your custom validation logic here ...
</span><span class="c1"></span>    <span class="c1">// or use the provided helper utility
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">FactoryUtil</span><span class="o">.</span><span class="na">TableFactoryHelper</span> <span class="n">helper</span> <span class="o">=</span> <span class="n">FactoryUtil</span><span class="o">.</span><span class="na">createTableFactoryHelper</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">context</span><span class="o">);</span>

    <span class="c1">// discover a suitable decoding format
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">DecodingFormat</span><span class="o">&lt;</span><span class="n">DeserializationSchema</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;&gt;</span> <span class="n">decodingFormat</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="na">discoverDecodingFormat</span><span class="o">(</span>
      <span class="n">DeserializationFormatFactory</span><span class="o">.</span><span class="na">class</span><span class="o">,</span>
      <span class="n">FactoryUtil</span><span class="o">.</span><span class="na">FORMAT</span><span class="o">);</span>

    <span class="c1">// validate all options
</span><span class="c1"></span>    <span class="n">helper</span><span class="o">.</span><span class="na">validate</span><span class="o">();</span>

    <span class="c1">// get the validated options
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">ReadableConfig</span> <span class="n">options</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="na">getOptions</span><span class="o">();</span>
    <span class="kd">final</span> <span class="n">String</span> <span class="n">hostname</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">HOSTNAME</span><span class="o">);</span>
    <span class="kd">final</span> <span class="kt">int</span> <span class="n">port</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">PORT</span><span class="o">);</span>
    <span class="kd">final</span> <span class="kt">byte</span> <span class="n">byteDelimiter</span> <span class="o">=</span> <span class="o">(</span><span class="kt">byte</span><span class="o">)</span> <span class="o">(</span><span class="kt">int</span><span class="o">)</span> <span class="n">options</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">BYTE_DELIMITER</span><span class="o">);</span>

    <span class="c1">// derive the produced data type (excluding computed columns) from the catalog table
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">DataType</span> <span class="n">producedDataType</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getCatalogTable</span><span class="o">().</span><span class="na">getSchema</span><span class="o">().</span><span class="na">toPhysicalRowDataType</span><span class="o">();</span>

    <span class="c1">// create and return dynamic table source
</span><span class="c1"></span>    <span class="k">return</span> <span class="k">new</span> <span class="n">SocketDynamicTableSource</span><span class="o">(</span><span class="n">hostname</span><span class="o">,</span> <span class="n">port</span><span class="o">,</span> <span class="n">byteDelimiter</span><span class="o">,</span> <span class="n">decodingFormat</span><span class="o">,</span> <span class="n">producedDataType</span><span class="o">);</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>ChangelogCsvFormatFactory</strong></p>
<p>ChangelogCsvFormatFactory 将特定格式的选项翻译成一种格式。SocketDynamicTableFactory 中的 FactoryUtil 负责相应地调整选项键，并处理像 changelog-csv.column-delimiter 那样的前缀。</p>
<p>因为这个工厂实现了 DeserializationFormatFactory，所以它也可以用于其他支持反序列化格式的连接器，比如 Kafka 连接器。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.serialization.DeserializationSchema</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.configuration.ConfigOption</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.configuration.ConfigOptions</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.configuration.ReadableConfig</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.format.DecodingFormat</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.data.RowData</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.factories.FactoryUtil</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.factories.DeserializationFormatFactory</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.factories.DynamicTableFactory</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">ChangelogCsvFormatFactory</span> <span class="kd">implements</span> <span class="n">DeserializationFormatFactory</span> <span class="o">{</span>

  <span class="c1">// define all options statically
</span><span class="c1"></span>  <span class="kd">public</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">ConfigOption</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">COLUMN_DELIMITER</span> <span class="o">=</span> <span class="n">ConfigOptions</span><span class="o">.</span><span class="na">key</span><span class="o">(</span><span class="s">&#34;column-delimiter&#34;</span><span class="o">)</span>
    <span class="o">.</span><span class="na">stringType</span><span class="o">()</span>
    <span class="o">.</span><span class="na">defaultValue</span><span class="o">(</span><span class="s">&#34;|&#34;</span><span class="o">);</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">String</span> <span class="nf">factoryIdentifier</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="s">&#34;changelog-csv&#34;</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">Set</span><span class="o">&lt;</span><span class="n">ConfigOption</span><span class="o">&lt;?&gt;&gt;</span> <span class="n">requiredOptions</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">Collections</span><span class="o">.</span><span class="na">emptySet</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">Set</span><span class="o">&lt;</span><span class="n">ConfigOption</span><span class="o">&lt;?&gt;&gt;</span> <span class="n">optionalOptions</span><span class="o">()</span> <span class="o">{</span>
    <span class="kd">final</span> <span class="n">Set</span><span class="o">&lt;</span><span class="n">ConfigOption</span><span class="o">&lt;?&gt;&gt;</span> <span class="n">options</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashSet</span><span class="o">&lt;&gt;();</span>
    <span class="n">options</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">COLUMN_DELIMITER</span><span class="o">);</span>
    <span class="k">return</span> <span class="n">options</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">DecodingFormat</span><span class="o">&lt;</span><span class="n">DeserializationSchema</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;&gt;</span> <span class="nf">createDecodingFormat</span><span class="o">(</span>
      <span class="n">DynamicTableFactory</span><span class="o">.</span><span class="na">Context</span> <span class="n">context</span><span class="o">,</span>
      <span class="n">ReadableConfig</span> <span class="n">formatOptions</span><span class="o">)</span> <span class="o">{</span>
    <span class="c1">// either implement your custom validation logic here ...
</span><span class="c1"></span>    <span class="c1">// or use the provided helper method
</span><span class="c1"></span>    <span class="n">FactoryUtil</span><span class="o">.</span><span class="na">validateFactoryOptions</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">formatOptions</span><span class="o">);</span>

    <span class="c1">// get the validated options
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">String</span> <span class="n">columnDelimiter</span> <span class="o">=</span> <span class="n">formatOptions</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">COLUMN_DELIMITER</span><span class="o">);</span>

    <span class="c1">// create and return the format
</span><span class="c1"></span>    <span class="k">return</span> <span class="k">new</span> <span class="n">ChangelogCsvFormat</span><span class="o">(</span><span class="n">columnDelimiter</span><span class="o">);</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="table-source-和解码格式">Table Source 和解码格式</h3>
<p>本节说明了如何从规划层的实例转化为运到集群的运行时实例。</p>
<p><strong>SocketDynamicTableSource</strong></p>
<p>在规划过程中会用到 SocketDynamicTableSource。在我们的例子中，我们没有实现任何可用的能力接口。因此，主要的逻辑可以在 <code>getScanRuntimeProvider(...)</code> 中找到，我们在其中实例化了所需的 SourceFunction 和其运行时的 DeserializationSchema。这两个实例都被参数化为返回内部数据结构（即 RowData）。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.serialization.DeserializationSchema</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.streaming.api.functions.source.SourceFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.ChangelogMode</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.format.DecodingFormat</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.source.DynamicTableSource</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.source.ScanTableSource</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.source.SourceFunctionProvider</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.data.RowData</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.types.DataType</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">SocketDynamicTableSource</span> <span class="kd">implements</span> <span class="n">ScanTableSource</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="kd">final</span> <span class="n">String</span> <span class="n">hostname</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">port</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kd">final</span> <span class="kt">byte</span> <span class="n">byteDelimiter</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kd">final</span> <span class="n">DecodingFormat</span><span class="o">&lt;</span><span class="n">DeserializationSchema</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;&gt;</span> <span class="n">decodingFormat</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kd">final</span> <span class="n">DataType</span> <span class="n">producedDataType</span><span class="o">;</span>

  <span class="kd">public</span> <span class="nf">SocketDynamicTableSource</span><span class="o">(</span>
      <span class="n">String</span> <span class="n">hostname</span><span class="o">,</span>
      <span class="kt">int</span> <span class="n">port</span><span class="o">,</span>
      <span class="kt">byte</span> <span class="n">byteDelimiter</span><span class="o">,</span>
      <span class="n">DecodingFormat</span><span class="o">&lt;</span><span class="n">DeserializationSchema</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;&gt;</span> <span class="n">decodingFormat</span><span class="o">,</span>
      <span class="n">DataType</span> <span class="n">producedDataType</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">hostname</span> <span class="o">=</span> <span class="n">hostname</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">port</span> <span class="o">=</span> <span class="n">port</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">byteDelimiter</span> <span class="o">=</span> <span class="n">byteDelimiter</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">decodingFormat</span> <span class="o">=</span> <span class="n">decodingFormat</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">producedDataType</span> <span class="o">=</span> <span class="n">producedDataType</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">ChangelogMode</span> <span class="nf">getChangelogMode</span><span class="o">()</span> <span class="o">{</span>
    <span class="c1">// in our example the format decides about the changelog mode
</span><span class="c1"></span>    <span class="c1">// but it could also be the source itself
</span><span class="c1"></span>    <span class="k">return</span> <span class="n">decodingFormat</span><span class="o">.</span><span class="na">getChangelogMode</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">ScanRuntimeProvider</span> <span class="nf">getScanRuntimeProvider</span><span class="o">(</span><span class="n">ScanContext</span> <span class="n">runtimeProviderContext</span><span class="o">)</span> <span class="o">{</span>

    <span class="c1">// create runtime classes that are shipped to the cluster
</span><span class="c1"></span>
    <span class="kd">final</span> <span class="n">DeserializationSchema</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="n">deserializer</span> <span class="o">=</span> <span class="n">decodingFormat</span><span class="o">.</span><span class="na">createRuntimeDecoder</span><span class="o">(</span>
      <span class="n">runtimeProviderContext</span><span class="o">,</span>
      <span class="n">producedDataType</span><span class="o">);</span>

    <span class="kd">final</span> <span class="n">SourceFunction</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="n">sourceFunction</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SocketSourceFunction</span><span class="o">(</span>
      <span class="n">hostname</span><span class="o">,</span>
      <span class="n">port</span><span class="o">,</span>
      <span class="n">byteDelimiter</span><span class="o">,</span>
      <span class="n">deserializer</span><span class="o">);</span>

    <span class="k">return</span> <span class="n">SourceFunctionProvider</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">sourceFunction</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">DynamicTableSource</span> <span class="nf">copy</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="k">new</span> <span class="n">SocketDynamicTableSource</span><span class="o">(</span><span class="n">hostname</span><span class="o">,</span> <span class="n">port</span><span class="o">,</span> <span class="n">byteDelimiter</span><span class="o">,</span> <span class="n">decodingFormat</span><span class="o">,</span> <span class="n">producedDataType</span><span class="o">);</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">String</span> <span class="nf">asSummaryString</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="s">&#34;Socket Table Source&#34;</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>ChangelogCsvFormat</strong></p>
<p>ChangelogCsvFormat 是一种解码格式，在运行时使用 DeserializationSchema。它支持发出 INSERT 和 DELETE 更改。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.serialization.DeserializationSchema</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.api.common.typeinfo.TypeInformation</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.ChangelogMode</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.format.DecodingFormat</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.source.DynamicTableSource</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.source.DynamicTableSource.DataStructureConverter</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.data.RowData</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.types.DataType</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.types.logical.LogicalType</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.types.RowKind</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">ChangelogCsvFormat</span> <span class="kd">implements</span> <span class="n">DecodingFormat</span><span class="o">&lt;</span><span class="n">DeserializationSchema</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;&gt;</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="kd">final</span> <span class="n">String</span> <span class="n">columnDelimiter</span><span class="o">;</span>

  <span class="kd">public</span> <span class="nf">ChangelogCsvFormat</span><span class="o">(</span><span class="n">String</span> <span class="n">columnDelimiter</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">columnDelimiter</span> <span class="o">=</span> <span class="n">columnDelimiter</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="nd">@SuppressWarnings</span><span class="o">(</span><span class="s">&#34;unchecked&#34;</span><span class="o">)</span>
  <span class="kd">public</span> <span class="n">DeserializationSchema</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="nf">createRuntimeDecoder</span><span class="o">(</span>
      <span class="n">DynamicTableSource</span><span class="o">.</span><span class="na">Context</span> <span class="n">context</span><span class="o">,</span>
      <span class="n">DataType</span> <span class="n">producedDataType</span><span class="o">)</span> <span class="o">{</span>
    <span class="c1">// create type information for the DeserializationSchema
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">TypeInformation</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="n">producedTypeInfo</span> <span class="o">=</span> <span class="o">(</span><span class="n">TypeInformation</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;)</span> <span class="n">context</span><span class="o">.</span><span class="na">createTypeInformation</span><span class="o">(</span>
      <span class="n">producedDataType</span><span class="o">);</span>

    <span class="c1">// most of the code in DeserializationSchema will not work on internal data structures
</span><span class="c1"></span>    <span class="c1">// create a converter for conversion at the end
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">DataStructureConverter</span> <span class="n">converter</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">createDataStructureConverter</span><span class="o">(</span><span class="n">producedDataType</span><span class="o">);</span>

    <span class="c1">// use logical types during runtime for parsing
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">LogicalType</span><span class="o">&gt;</span> <span class="n">parsingTypes</span> <span class="o">=</span> <span class="n">producedDataType</span><span class="o">.</span><span class="na">getLogicalType</span><span class="o">().</span><span class="na">getChildren</span><span class="o">();</span>

    <span class="c1">// create runtime class
</span><span class="c1"></span>    <span class="k">return</span> <span class="k">new</span> <span class="n">ChangelogCsvDeserializer</span><span class="o">(</span><span class="n">parsingTypes</span><span class="o">,</span> <span class="n">converter</span><span class="o">,</span> <span class="n">producedTypeInfo</span><span class="o">,</span> <span class="n">columnDelimiter</span><span class="o">);</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">ChangelogMode</span> <span class="nf">getChangelogMode</span><span class="o">()</span> <span class="o">{</span>
    <span class="c1">// define that this format can produce INSERT and DELETE rows
</span><span class="c1"></span>    <span class="k">return</span> <span class="n">ChangelogMode</span><span class="o">.</span><span class="na">newBuilder</span><span class="o">()</span>
      <span class="o">.</span><span class="na">addContainedKind</span><span class="o">(</span><span class="n">RowKind</span><span class="o">.</span><span class="na">INSERT</span><span class="o">)</span>
      <span class="o">.</span><span class="na">addContainedKind</span><span class="o">(</span><span class="n">RowKind</span><span class="o">.</span><span class="na">DELETE</span><span class="o">)</span>
      <span class="o">.</span><span class="na">build</span><span class="o">();</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="运行时-1">运行时</h3>
<p>为了完整起见，本节说明了 SourceFunction 和 DeserializationSchema 的运行时逻辑。</p>
<p><strong>ChangelogCsvDeserializer</strong></p>
<p>ChangelogCsvDeserializer 包含了一个简单的解析逻辑，用于将字节转换为带有行种类的整数行和字符串。最后的转换步骤将这些转换为内部数据结构。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.serialization.DeserializationSchema</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.api.common.typeinfo.TypeInformation</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.RuntimeConverter.Context</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.connector.source.DynamicTableSource.DataStructureConverter</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.data.RowData</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.types.logical.LogicalType</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.types.logical.LogicalTypeRoot</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.types.Row</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.types.RowKind</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">ChangelogCsvDeserializer</span> <span class="kd">implements</span> <span class="n">DeserializationSchema</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="kd">final</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">LogicalType</span><span class="o">&gt;</span> <span class="n">parsingTypes</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kd">final</span> <span class="n">DataStructureConverter</span> <span class="n">converter</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kd">final</span> <span class="n">TypeInformation</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="n">producedTypeInfo</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kd">final</span> <span class="n">String</span> <span class="n">columnDelimiter</span><span class="o">;</span>

  <span class="kd">public</span> <span class="nf">ChangelogCsvDeserializer</span><span class="o">(</span>
      <span class="n">List</span><span class="o">&lt;</span><span class="n">LogicalType</span><span class="o">&gt;</span> <span class="n">parsingTypes</span><span class="o">,</span>
      <span class="n">DataStructureConverter</span> <span class="n">converter</span><span class="o">,</span>
      <span class="n">TypeInformation</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="n">producedTypeInfo</span><span class="o">,</span>
      <span class="n">String</span> <span class="n">columnDelimiter</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">parsingTypes</span> <span class="o">=</span> <span class="n">parsingTypes</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">converter</span> <span class="o">=</span> <span class="n">converter</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">producedTypeInfo</span> <span class="o">=</span> <span class="n">producedTypeInfo</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">columnDelimiter</span> <span class="o">=</span> <span class="n">columnDelimiter</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">TypeInformation</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="nf">getProducedType</span><span class="o">()</span> <span class="o">{</span>
    <span class="c1">// return the type information required by Flink&#39;s core interfaces
</span><span class="c1"></span>    <span class="k">return</span> <span class="n">producedTypeInfo</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">open</span><span class="o">(</span><span class="n">InitializationContext</span> <span class="n">context</span><span class="o">)</span> <span class="o">{</span>
    <span class="c1">// converters must be open
</span><span class="c1"></span>    <span class="n">converter</span><span class="o">.</span><span class="na">open</span><span class="o">(</span><span class="n">Context</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">ChangelogCsvDeserializer</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getClassLoader</span><span class="o">()));</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">RowData</span> <span class="nf">deserialize</span><span class="o">(</span><span class="kt">byte</span><span class="o">[]</span> <span class="n">message</span><span class="o">)</span> <span class="o">{</span>
    <span class="c1">// parse the columns including a changelog flag
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">String</span><span class="o">[]</span> <span class="n">columns</span> <span class="o">=</span> <span class="k">new</span> <span class="n">String</span><span class="o">(</span><span class="n">message</span><span class="o">).</span><span class="na">split</span><span class="o">(</span><span class="n">Pattern</span><span class="o">.</span><span class="na">quote</span><span class="o">(</span><span class="n">columnDelimiter</span><span class="o">));</span>
    <span class="kd">final</span> <span class="n">RowKind</span> <span class="n">kind</span> <span class="o">=</span> <span class="n">RowKind</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">columns</span><span class="o">[</span><span class="n">0</span><span class="o">]);</span>
    <span class="kd">final</span> <span class="n">Row</span> <span class="n">row</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Row</span><span class="o">(</span><span class="n">kind</span><span class="o">,</span> <span class="n">parsingTypes</span><span class="o">.</span><span class="na">size</span><span class="o">());</span>
    <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">parsingTypes</span><span class="o">.</span><span class="na">size</span><span class="o">();</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
      <span class="n">row</span><span class="o">.</span><span class="na">setField</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">parse</span><span class="o">(</span><span class="n">parsingTypes</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">i</span><span class="o">).</span><span class="na">getTypeRoot</span><span class="o">(),</span> <span class="n">columns</span><span class="o">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">1</span><span class="o">]));</span>
    <span class="o">}</span>
    <span class="c1">// convert to internal data structure
</span><span class="c1"></span>    <span class="k">return</span> <span class="o">(</span><span class="n">RowData</span><span class="o">)</span> <span class="n">converter</span><span class="o">.</span><span class="na">toInternal</span><span class="o">(</span><span class="n">row</span><span class="o">);</span>
  <span class="o">}</span>

  <span class="kd">private</span> <span class="kd">static</span> <span class="n">Object</span> <span class="nf">parse</span><span class="o">(</span><span class="n">LogicalTypeRoot</span> <span class="n">root</span><span class="o">,</span> <span class="n">String</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">switch</span> <span class="o">(</span><span class="n">root</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">case</span> <span class="n">INTEGER</span><span class="o">:</span>
        <span class="k">return</span> <span class="n">Integer</span><span class="o">.</span><span class="na">parseInt</span><span class="o">(</span><span class="n">value</span><span class="o">);</span>
      <span class="k">case</span> <span class="n">VARCHAR</span><span class="o">:</span>
        <span class="k">return</span> <span class="n">value</span><span class="o">;</span>
      <span class="k">default</span><span class="o">:</span>
        <span class="k">throw</span> <span class="k">new</span> <span class="n">IllegalArgumentException</span><span class="o">();</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">isEndOfStream</span><span class="o">(</span><span class="n">RowData</span> <span class="n">nextElement</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="kc">false</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>SocketSourceFunction</strong></p>
<p>SocketSourceFunction 打开一个套接字并消耗字节。它通过给定的字节定界符（默认为 <code>\n</code>）分割记录，并将解码委托给一个可插拔的 DeserializationSchema。源函数只能以 1 的并行度工作。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.serialization.DeserializationSchema</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.api.common.typeinfo.TypeInformation</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.api.java.typeutils.ResultTypeQueryable</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.configuration.Configuration</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.streaming.api.functions.source.RichSourceFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.data.RowData</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">SocketSourceFunction</span> <span class="kd">extends</span> <span class="n">RichSourceFunction</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="kd">implements</span> <span class="n">ResultTypeQueryable</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="kd">final</span> <span class="n">String</span> <span class="n">hostname</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">port</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kd">final</span> <span class="kt">byte</span> <span class="n">byteDelimiter</span><span class="o">;</span>
  <span class="kd">private</span> <span class="kd">final</span> <span class="n">DeserializationSchema</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="n">deserializer</span><span class="o">;</span>

  <span class="kd">private</span> <span class="kd">volatile</span> <span class="kt">boolean</span> <span class="n">isRunning</span> <span class="o">=</span> <span class="kc">true</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">Socket</span> <span class="n">currentSocket</span><span class="o">;</span>

  <span class="kd">public</span> <span class="nf">SocketSourceFunction</span><span class="o">(</span><span class="n">String</span> <span class="n">hostname</span><span class="o">,</span> <span class="kt">int</span> <span class="n">port</span><span class="o">,</span> <span class="kt">byte</span> <span class="n">byteDelimiter</span><span class="o">,</span> <span class="n">DeserializationSchema</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="n">deserializer</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">hostname</span> <span class="o">=</span> <span class="n">hostname</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">port</span> <span class="o">=</span> <span class="n">port</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">byteDelimiter</span> <span class="o">=</span> <span class="n">byteDelimiter</span><span class="o">;</span>
    <span class="k">this</span><span class="o">.</span><span class="na">deserializer</span> <span class="o">=</span> <span class="n">deserializer</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">TypeInformation</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="nf">getProducedType</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">deserializer</span><span class="o">.</span><span class="na">getProducedType</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">open</span><span class="o">(</span><span class="n">Configuration</span> <span class="n">parameters</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
    <span class="n">deserializer</span><span class="o">.</span><span class="na">open</span><span class="o">(()</span> <span class="o">-&gt;</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getMetricGroup</span><span class="o">());</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">(</span><span class="n">SourceContext</span><span class="o">&lt;</span><span class="n">RowData</span><span class="o">&gt;</span> <span class="n">ctx</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">isRunning</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// open and consume from socket
</span><span class="c1"></span>      <span class="k">try</span> <span class="o">(</span><span class="kd">final</span> <span class="n">Socket</span> <span class="n">socket</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Socket</span><span class="o">())</span> <span class="o">{</span>
        <span class="n">currentSocket</span> <span class="o">=</span> <span class="n">socket</span><span class="o">;</span>
        <span class="n">socket</span><span class="o">.</span><span class="na">connect</span><span class="o">(</span><span class="k">new</span> <span class="n">InetSocketAddress</span><span class="o">(</span><span class="n">hostname</span><span class="o">,</span> <span class="n">port</span><span class="o">),</span> <span class="n">0</span><span class="o">);</span>
        <span class="k">try</span> <span class="o">(</span><span class="n">InputStream</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">socket</span><span class="o">.</span><span class="na">getInputStream</span><span class="o">())</span> <span class="o">{</span>
          <span class="n">ByteArrayOutputStream</span> <span class="n">buffer</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ByteArrayOutputStream</span><span class="o">();</span>
          <span class="kt">int</span> <span class="n">b</span><span class="o">;</span>
          <span class="k">while</span> <span class="o">((</span><span class="n">b</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="na">read</span><span class="o">())</span> <span class="o">&gt;=</span> <span class="n">0</span><span class="o">)</span> <span class="o">{</span>
            <span class="c1">// buffer until delimiter
</span><span class="c1"></span>            <span class="k">if</span> <span class="o">(</span><span class="n">b</span> <span class="o">!=</span> <span class="n">byteDelimiter</span><span class="o">)</span> <span class="o">{</span>
              <span class="n">buffer</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">b</span><span class="o">);</span>
            <span class="o">}</span>
            <span class="c1">// decode and emit record
</span><span class="c1"></span>            <span class="k">else</span> <span class="o">{</span>
              <span class="n">ctx</span><span class="o">.</span><span class="na">collect</span><span class="o">(</span><span class="n">deserializer</span><span class="o">.</span><span class="na">deserialize</span><span class="o">(</span><span class="n">buffer</span><span class="o">.</span><span class="na">toByteArray</span><span class="o">()));</span>
              <span class="n">buffer</span><span class="o">.</span><span class="na">reset</span><span class="o">();</span>
            <span class="o">}</span>
          <span class="o">}</span>
        <span class="o">}</span>
      <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Throwable</span> <span class="n">t</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">t</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span> <span class="c1">// print and continue
</span><span class="c1"></span>      <span class="o">}</span>
      <span class="n">Thread</span><span class="o">.</span><span class="na">sleep</span><span class="o">(</span><span class="n">1000</span><span class="o">);</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">cancel</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">isRunning</span> <span class="o">=</span> <span class="kc">false</span><span class="o">;</span>
    <span class="k">try</span> <span class="o">{</span>
      <span class="n">currentSocket</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
    <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Throwable</span> <span class="n">t</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// ignore
</span><span class="c1"></span>    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sourceSinks.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sourceSinks.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[数据类型和序列化]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-datatypes-serialization/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-custom-serializer/?utm_source=atom_feed" rel="related" type="text/html" title="Custom Serializer" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Read and Write" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Streaming" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 方言" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-datatypes-serialization/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Datatypes Serialization</blockquote><h1 id="数据类型和序列化">数据类型和序列化</h1>
<p>Apache Flink 以独特的方式处理数据类型和序列化，包含自己的类型描述符、通用类型提取和类型序列化框架。本文档描述了这些概念和它们背后的原理。</p>
<h2 id="支持的数据类型">支持的数据类型</h2>
<p>Flink 对 DataSet 或 DataStream 中的元素类型进行了一些限制。这样做的原因是系统分析类型以确定高效的执行策略。</p>
<p>有七种不同类别的数据类型。</p>
<ol>
<li>Java Tuples 和 Scala Case 类</li>
<li>Java POJOs</li>
<li>Primitive Types</li>
<li>Regular Classes</li>
<li>Values</li>
<li>Hadoop Writables</li>
<li>特殊类型</li>
</ol>
<h3 id="tuples-和-case-类">Tuples 和 Case 类</h3>
<p>Scala case 类（以及 Scala tuples，它是 case 类的一种特殊类型），是包含固定数量的各种类型的字段的复合类型。元组字段由它们的 1-offset 名称寻址，例如第一个字段为 <code>_1</code>。case 类字段用它们的名字来访问。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">WordCount</span><span class="o">(</span><span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
<span class="k">val</span> <span class="n">input</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span>
    <span class="nc">WordCount</span><span class="o">(</span><span class="s">&#34;hello&#34;</span><span class="o">,</span> <span class="mi">1</span><span class="o">),</span>
    <span class="nc">WordCount</span><span class="o">(</span><span class="s">&#34;world&#34;</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span> <span class="c1">// Case Class Data Set
</span><span class="c1"></span>
<span class="n">input</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">)</span><span class="c1">// key by field expression &#34;word&#34;
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">input2</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">((</span><span class="s">&#34;hello&#34;</span><span class="o">,</span> <span class="mi">1</span><span class="o">),</span> <span class="o">(</span><span class="s">&#34;world&#34;</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span> <span class="c1">// Tuple2 Data Set
</span><span class="c1"></span>
<span class="n">input2</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="c1">// key by field positions 0 and 1
</span></code></pre></div><h3 id="pojos">POJOs</h3>
<p>如果 Java 和 Scala 类满足以下要求，Flink 会将其作为一种特殊的 POJO 数据类型。</p>
<ul>
<li>
<p>类必须是公共的。</p>
</li>
<li>
<p>它必须有一个没有参数的公共构造函数（默认构造函数）。</p>
</li>
<li>
<p>所有字段要么是公共的，要么必须通过 getter 和 setter 函数来访问。对于一个名为 foo 的字段，getter 和 setter 方法必须命名为 <code>getFoo()</code> 和 <code> setFoo()</code>。</p>
</li>
<li>
<p>字段的类型必须由注册的序列器支持。</p>
</li>
</ul>
<p>POJOs 通常用 PojoTypeInfo 表示，并用 PojoSerializer 序列化（使用 <a href="https://github.com/EsotericSoftware/kryo">Kryo</a> 作为可配置的回退）。例外的情况是当 POJOs 实际上是 Avro 类型（Avro 特定记录）或作为 &ldquo;Avro 反射类型 &ldquo;产生。在这种情况下，POJO 由 AvroTypeInfo 表示，并通过 AvroSerializer 序列化。如果需要，您也可以注册您自己的自定义序列化器；更多信息请参见<a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/types_serialization.html#serialization-of-pojo-types">序列化</a>。</p>
<p>Flink 分析 POJO 类型的结构，也就是说，它学习 POJO 的字段。因此，POJO 类型比一般类型更容易使用。此外，Flink 可以比一般类型更有效地处理 POJO。</p>
<p>下面的例子显示了一个简单的 POJO，它有两个公共字段。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">WordWithCount</span><span class="o">(</span><span class="k">var</span> <span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="k">var</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span>
      <span class="k">this</span><span class="o">(</span><span class="kc">null</span><span class="o">,</span> <span class="o">-</span><span class="mi">1</span><span class="o">)</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">input</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span>
    <span class="k">new</span> <span class="nc">WordWithCount</span><span class="o">(</span><span class="s">&#34;hello&#34;</span><span class="o">,</span> <span class="mi">1</span><span class="o">),</span>
    <span class="k">new</span> <span class="nc">WordWithCount</span><span class="o">(</span><span class="s">&#34;world&#34;</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span> <span class="c1">// Case Class Data Set
</span><span class="c1"></span>
<span class="n">input</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">)</span><span class="c1">// key by field expression &#34;word&#34;
</span></code></pre></div><h3 id="原始类型">原始类型</h3>
<p>Flink 支持所有的 Java 和 Scala 基元类型，如 Integer, String 和 Double。</p>
<h3 id="通用类类型">通用类类型</h3>
<p>Flink 支持大多数 Java 和 Scala 类（API 和自定义）。限制适用于包含不能序列化的字段的类，如文件指针、I/O 流或其他本地资源。遵循 Java Beans 约定的类一般都能很好地工作。</p>
<p>所有没有被确定为 POJO 类型的类（见上面的 POJO 要求）都被 Flink 作为一般类类型处理。Flink 将这些数据类型视为黑盒，无法访问它们的内容（例如，为了有效的排序）。一般类型使用序列化框架 <a href="https://github.com/EsotericSoftware/kryo">Kryo</a> 进行去/序列化。</p>
<h3 id="值">值</h3>
<p>Value 类型手动描述它们的序列化和反序列化。它们不需要通过一个通用的序列化框架，而是通过实现 org.apache.flinktypes.Value 接口的读写方法，为这些操作提供自定义代码。当通用序列化会非常低效时，使用 Value 类型是合理的。一个例子是一个数据类型，它将一个稀疏的元素向量实现为一个数组。知道数组大部分是零，就可以对非零元素使用特殊的编码，而通用序列化会简单地写入所有数组元素。</p>
<p>org.apache.flinktypes.CopyableValue 接口以类似的方式支持手动内部克隆逻辑。</p>
<p>Flink 自带了预先定义的 Value 类型，对应基本数据类型。ByteValue、ShortValue、IntValue、LongValue、FloatValue、DoubleValue、StringValue、CharValue、BooleanValue）。这些 Value 类型作为基本数据类型的可变体。它们的值可以被改变，允许程序员重用对象并减轻垃圾收集器的压力。</p>
<h3 id="hadoop-可写类型">Hadoop 可写类型</h3>
<p>你可以使用实现 org.apache.hadoop.Writable 接口的类型。在 <code>write()</code> 和 <code>readFields()</code> 方法中定义的序列化逻辑将被用于序列化。</p>
<h3 id="特殊类型">特殊类型</h3>
<p>你可以使用特殊类型，包括 Scala 的 Either、Option 和 Try。Java API 对 Either 有自己的自定义实现。类似于 Scala 的 Either，它代表了两种可能的类型的值，左或右。Either 对于错误处理或需要输出两种不同类型记录的操作符来说非常有用。</p>
<h3 id="类型擦除和类型推断">类型擦除和类型推断。</h3>
<p>注意：本节只与 Java 相关。</p>
<p>Java 编译器在编译后会丢弃很多通用类型信息。这在 Java 中被称为类型清除。这意味着在运行时，一个对象的实例不再知道它的通用类型。例如，<code>DataStream&lt;String&gt;</code> 和 <code>DataStream&lt;Long&gt;</code> 的实例在 JVM 看来是一样的。</p>
<p>Flink 在准备执行程序时（调用程序的主方法时）需要类型信息。Flink Java API 试图重建以各种方式扔掉的类型信息，并将其显式存储在数据集和运算符中。你可以通过 <code>DataStream.getType()</code> 来检索类型。该方法返回一个 TypeInformation 的实例，这是 Flink 内部表示类型的方式。</p>
<p>类型推理有其局限性，在某些情况下需要程序员的 &ldquo;配合&rdquo;。例如从集合中创建数据集的方法，如 <code>ExecutionEnvironment.fromCollection()</code>，你可以传递一个描述类型的参数。但是像 <code>MapFunction&lt;I, O&gt;</code> 这样的通用函数也可能需要额外的类型信息。</p>
<p><a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/java/typeutils/ResultTypeQueryable.java">ResultTypeQueryable</a> 接口可以由输入格式和函数实现，以明确地告诉 API 它们的返回类型。函数被调用的输入类型通常可以通过前面操作的结果类型来推断。</p>
<h2 id="flink-中的类型处理">Flink 中的类型处理</h2>
<p>Flink 试图推断出很多关于分布式计算过程中交换和存储的数据类型的信息。把它想象成一个数据库，推断表的模式。在大多数情况下，Flink 自己就能无缝地推断出所有必要的信息。有了类型信息，Flink 就可以做一些很酷的事情。</p>
<ul>
<li>
<p>Flink 对数据类型了解得越多，序列化和数据布局方案就越好。这对于 Flink 中的内存使用范式相当重要（尽可能在堆内/堆外对序列化数据进行工作，并使序列化非常便宜）。</p>
</li>
<li>
<p>最后，在大多数情况下，这也免去了用户对序列化框架的担心，也免去了对类型的注册。</p>
</li>
</ul>
<p>一般来说，关于数据类型的信息是在飞行前阶段需要的&ndash;也就是说，当程序对 DataStream 和 DataSet 进行调用时，以及在对 <code>execute()</code>、<code>print()</code>、<code>count()</code> 或 <code>collect()</code> 进行任何调用之前。</p>
<h2 id="最常见的问题">最常见的问题</h2>
<p>用户最经常需要与 Flink 的数据类型处理进行交互的问题是。</p>
<ul>
<li>
<p>注册子类型。如果函数签名只描述了超类型，但它们在执行过程中实际使用了这些子类型，那么让 Flink 意识到这些子类型可能会提高很多性能。为此，可以在 StreamExecutionEnvironment 或 ExecutionEnvironment 上为每个子类型调用 <code>.registerType(clazz)</code>。</p>
</li>
<li>
<p>注册自定义序列器。Flink 对于那些自己不透明处理的类型又回到了 <a href="https://github.com/EsotericSoftware/kryo">Kryo</a>。并非所有类型都能被 Kryo 无缝处理（因此也能被 Flink 处理）。例如，许多 Google Guava 集合类型在默认情况下不能很好地工作。解决的办法是为导致问题的类型注册额外的序列器。在 StreamExecutionEnvironment 或 ExecutionEnvironment 上调用.getConfig().addDefaultKryoSerializer( clazz, serializer)。许多库中都有额外的 Kryo 序列化器。有关使用自定义序列器的更多细节，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/custom_serializers.html">自定义序列器</a>。</p>
</li>
<li>
<p>添加类型提示。有时，当 Flink 尽管使用了所有技巧也无法推断出通用类型时，用户必须传递一个类型提示。这一般只在 Java API 中才需要。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html#type-hints-in-the-java-api">类型提示</a>一节对此进行了更详细的描述。</p>
</li>
<li>
<p>手动创建一个 TypeInformation。对于一些 API 调用来说，这可能是必要的，由于 Java 的通用类型擦除，Flink 不可能推断出数据类型。详情请看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html#creating-a-typeinformation-or-typeserializer">创建 TypeInformation 或 TypeSerializer</a>。</p>
</li>
</ul>
<h2 id="flink-的-typeinformation-类">Flink 的 TypeInformation 类</h2>
<p><a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/typeinfo/TypeInformation.java">TypeInformation</a> 类是所有类型描述符的基础类。它揭示了类型的一些基本属性，并且可以生成序列器，在特殊化中，可以生成类型的比较器。(注意，Flink 中的比较器的作用远不止定义一个顺序&ndash;它们基本上是处理键的实用程序)</p>
<p>在内部，Flink 对类型做了如下区分。</p>
<ul>
<li>
<p>基本类型。所有的 Java 基元和它们的盒子形式，加上 void，String，Date，BigDecimal 和 BigInteger。</p>
</li>
<li>
<p>基元数组和对象数组：基本类型：所有的 Java 基元和它们的盒子形式，加上 void、String、Date、BigDecimal 和 BigInteger。</p>
</li>
<li>
<p>复合型</p>
<ul>
<li>Flink Java Tuples(Flink Java API 的一部分)：最多 25 个字段，不支持 null 字段。</li>
<li>Scala case 类（包括 Scala tuples）：不支持 null 字段。</li>
<li>Row：具有任意数量字段的元组，支持 null 字段。</li>
<li>POJOs：遵循某种 bean-like 模式的类。</li>
</ul>
</li>
<li>
<p>辅助类型(Option、Either、Lists、Maps&hellip;)</p>
</li>
<li>
<p>通用类型。这些类型不会由 Flink 本身序列化，而是由 Kryo 序列化。</p>
</li>
</ul>
<p>POJOs 特别值得关注，因为它们支持创建复杂类型和在键的定义中使用字段名：<code>dataSet.join(another).where(&quot;name&quot;).equalTo(&quot;personName&quot;)</code>。它们对运行时也是透明的，可以被 Flink 非常有效地处理。</p>
<h3 id="pojo-类型的规则">POJO 类型的规则</h3>
<p>如果满足以下条件，Flink 将数据类型识别为 POJO 类型（并允许 &ldquo;按名称 &ldquo;字段引用）。</p>
<ul>
<li>类是公共的和独立的（没有非静态的内部类）。</li>
<li>该类有一个公共的无参数构造函数。</li>
<li>该类（以及所有超级类）中所有非静态、非瞬态的字段要么是公共的（是非最终的），要么有一个公共的 <code>getter-</code> 和 <code>setter-</code> 方法，遵循 Java beans 中 getter 和 setter 的命名惯例。</li>
</ul>
<p>请注意，当用户定义的数据类型不能被识别为 POJO 类型时，它必须被处理为 GenericType 并通过 Kryo 进行序列化。</p>
<h3 id="创建一个-typeinformation-或-typeserializer">创建一个 TypeInformation 或 TypeSerializer。</h3>
<p>要为一个类型创建 TypeInformation 对象，请使用语言特定的方式。</p>
<p>在 Scala 中，Flink 使用在编译时运行的宏，并捕捉所有通用类型信息，而它仍然可用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// important: this import is needed to access the &#39;createTypeInformation&#39; macro function
</span><span class="c1"></span><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala._</span>

<span class="k">val</span> <span class="n">stringInfo</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">createTypeInformation</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span>

<span class="k">val</span> <span class="n">tupleInfo</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">createTypeInformation</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span>
</code></pre></div><p>你仍然可以使用与 Java 中相同的方法作为后备。</p>
<p>要创建一个 TypeSerializer，只需在 TypeInformation 对象上调用 typeInfo.createSerializer(config)。</p>
<p>config 参数的类型是 ExecutionConfig，并持有程序注册的自定义序列器的信息。在可能的情况下，尽量传递程序的正确的 ExecutionConfig。你通常可以通过调用 <code>getExecutionConfig()</code> 从 DataStream 或 DataSet 中获取它。在函数内部（比如 MapFunction），你可以通过将函数变成 Rich Function，然后调用 <code>getRuntimeContext().getExecutionConfig()</code> 来获取它。</p>
<h2 id="scala-api-中的类型信息">Scala API 中的类型信息</h2>
<p>Scala 通过类型清单和类标签对运行时类型信息有非常详细的概念。一般来说，类型和方法可以访问其通用参数的类型&ndash;因此，Scala 程序不会像 Java 程序那样受到类型擦除的影响。</p>
<p>此外，Scala 允许通过 Scala Macros 在 Scala 编译器中运行自定义代码&ndash;这意味着每当你编译一个针对 Flink 的 Scala API 编写的 Scala 程序时，一些 Flink 代码就会被执行。</p>
<p>我们在编译过程中使用宏来查看所有用户函数的参数类型和返回类型&ndash;这时当然所有的类型信息都是完全可用的。在宏中，我们为函数的返回类型（或参数类型）创建一个 TypeInformation，并将其作为操作的一部分。</p>
<h3 id="没有隐式值的证据参数错误">没有隐式值的证据参数错误</h3>
<p>在 TypeInformation 不能被创建的情况下，程序编译失败，并出现 &ldquo;could not find implicit value for evidence parameter of type TypeInformation&rdquo; 的错误。</p>
<p>一个常见的原因是生成 TypeInformation 的代码没有被导入。请确保导入整个 <code>flink.api.scala</code> 包。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala._</span>
</code></pre></div><p>另一个常见的原因是通用方法，它可以在下面的章节中进行修复。</p>
<h3 id="通用方法">通用方法</h3>
<p>请考虑以下案例。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">def</span> <span class="n">selectFirst</span><span class="o">[</span><span class="kt">T</span><span class="o">](</span><span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">T</span>, <span class="k">_</span><span class="o">)])</span> <span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="n">input</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">v</span> <span class="k">=&gt;</span> <span class="n">v</span><span class="o">.</span><span class="n">_1</span> <span class="o">}</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">data</span> <span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)</span> <span class="kt">=</span> <span class="kt">...</span>

<span class="kt">val</span> <span class="kt">result</span> <span class="kt">=</span> <span class="kt">selectFirst</span><span class="o">(</span><span class="kt">data</span><span class="o">)</span>
</code></pre></div><p>对于这样的通用方法，每次调用时，函数参数和返回类型的数据类型可能不一样，在定义方法的站点不知道。上面的代码会导致一个错误，即没有足够的隐含证据。</p>
<p>在这种情况下，必须在调用站点生成类型信息并传递给方法。Scala 为此提供了隐式参数。</p>
<p>下面的代码告诉 Scala 将 T 的类型信息带入函数中。然后，类型信息将在方法被调用的站点生成，而不是在方法被定义的站点生成。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">def</span> <span class="n">selectFirst</span><span class="o">[</span><span class="kt">T</span> <span class="kt">:</span> <span class="kt">TypeInformation</span><span class="o">](</span><span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">T</span>, <span class="k">_</span><span class="o">)])</span> <span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="n">input</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">v</span> <span class="k">=&gt;</span> <span class="n">v</span><span class="o">.</span><span class="n">_1</span> <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h2 id="java-api-中的类型信息">Java API 中的类型信息</h2>
<p>在一般情况下，Java 会擦除通用类型信息，而 Flink 试图通过反射来重建尽可能多的类型信息，使用 Java 保留的少量信息（主要是函数签名和子类信息）。Flink 试图通过反射来重建尽可能多的类型信息，使用 Java 保留的少量信息（主要是函数签名和子类信息）。这个逻辑还包含了一些简单的类型推理，用于函数的返回类型取决于其输入类型的情况。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">AppendOne</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="kd">implements</span> <span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">T</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">T</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="o">{</span>

    <span class="kd">public</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">T</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="nf">map</span><span class="o">(</span><span class="n">T</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">T</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;(</span><span class="n">value</span><span class="o">,</span> <span class="n">1L</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>在有些情况下，Flink 无法重建所有的通用类型信息。在这种情况下，用户必须通过类型提示来帮忙。</p>
<h3 id="java-api-中的类型提示">Java API 中的类型提示</h3>
<p>在 Flink 无法重建被擦除的通用类型信息的情况下，Java API 提供了所谓的类型提示。类型提示告诉系统一个函数产生的数据流或数据集的类型。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">SomeType</span><span class="o">&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">dataSet</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="k">new</span> <span class="n">MyGenericNonInferrableFunction</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">SomeType</span><span class="o">&gt;())</span>
        <span class="o">.</span><span class="na">returns</span><span class="o">(</span><span class="n">SomeType</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</code></pre></div><p><code>returns</code> 语句指定产生的类型，在本例中是通过一个类。提示支持类型定义，通过:</p>
<ul>
<li>类，适用于非参数化类型(无属类)</li>
<li>TypeHint 以 <code>returns(new TypeHint&lt;Tuple2&lt;Integer, SomeType&gt;&gt;(){})</code> 的形式存在。TypeHint 类可以捕获通用类型信息，并将其保存到运行时（通过匿名子类）。</li>
</ul>
<h3 id="java-8-lambdas-的类型提取">Java 8 lambdas 的类型提取。</h3>
<p>Java 8 lambdas 的类型提取与非 lambdas 的工作方式不同，因为 lambdas 不与扩展函数接口的实现类相关联。</p>
<p>目前，Flink 试图找出哪个方法实现了 lambda，并使用 Java 的通用签名来确定参数类型和返回类型。然而，并不是所有的编译器都能为 lambdas 生成这些签名（在写这篇文档时，只有 4.5 以后的 Eclipse JDT 编译器能可靠地生成）。</p>
<h3 id="pojo-类型的序列化">POJO 类型的序列化</h3>
<p>PojoTypeInfo 正在为 POJO 内部的所有字段创建序列器。标准类型，如 int、long、String 等，由 Flink 附带的序列器处理。对于所有其他类型，我们回到了 <a href="https://github.com/EsotericSoftware/kryo">Kryo</a>。</p>
<p>如果 Kryo 不能处理类型，你可以要求 PojoTypeInfo 使用 <a href="https://avro.apache.org/">Avro</a> 来序列化 POJO。要做到这一点，你必须调用</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>
<span class="n">env</span><span class="o">.</span><span class="na">getConfig</span><span class="o">().</span><span class="na">enableForceAvro</span><span class="o">();</span>
</code></pre></div><p>请注意，Flink 是用 Avro 序列化器自动序列化 Avro 生成的 POJO。</p>
<p>如果您想让整个 POJO 类型被 Kryo 序列化器处理，请设置</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>
<span class="n">env</span><span class="o">.</span><span class="na">getConfig</span><span class="o">().</span><span class="na">enableForceKryo</span><span class="o">();</span>
</code></pre></div><p>如果 Kryo 无法序列化你的 POJO，你可以在 Kryo 中添加一个自定义序列化器，使用</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">env</span><span class="o">.</span><span class="na">getConfig</span><span class="o">().</span><span class="na">addDefaultKryoSerializer</span><span class="o">(</span><span class="n">Class</span><span class="o">&lt;?&gt;</span> <span class="n">type</span><span class="o">,</span> <span class="n">Class</span><span class="o">&lt;?</span> <span class="kd">extends</span> <span class="n">Serializer</span><span class="o">&lt;?&gt;&gt;</span> <span class="n">serializerClass</span><span class="o">)</span>
</code></pre></div><p>这些方法有不同的变体。</p>
<h2 id="禁用-kryo-fallback">禁用 Kryo Fallback</h2>
<p>有些情况下，程序可能希望明确避免使用 Kryo 作为通用类型的后备。最常见的情况是希望确保所有类型都能通过 Flink 自己的序列化器或通过用户定义的自定义序列化器进行有效序列化。</p>
<p>下面的设置会在遇到会通过 Kryo 的数据类型时引发异常。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">env</span><span class="o">.</span><span class="n">getConfig</span><span class="o">().</span><span class="n">disableGenericTypes</span><span class="o">();</span>
</code></pre></div><h2 id="使用工厂定义类型信息">使用工厂定义类型信息</h2>
<p>类型信息工厂允许将用户定义的类型信息插入到 Flink 类型系统中。你必须实现 <code>org.apache.flink.api.common.typeinfo.TypeInfoFactory</code> 来返回你的自定义类型信息。如果相应的类型已经被 <code>@org.apache.flink.api.common.typeinfo.TypeInfo</code> 注解，那么在类型提取阶段就会调用这个工厂。</p>
<p>类型信息工厂可以在 Java 和 Scala API 中使用。</p>
<p>在类型的层次结构中，在向上遍历时将选择最接近的工厂，然而，内置工厂具有最高的优先级。工厂也比 Flink 的内置类型有更高的优先级，因此你应该知道你在做什么。</p>
<p>下面的例子展示了如何在 Java 中使用工厂来注释一个自定义类型 MyTuple 并为其提供自定义类型信息。</p>
<p>注解的自定义类型:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="nd">@TypeInfo</span><span class="o">(</span><span class="n">MyTupleTypeInfoFactory</span><span class="o">.</span><span class="na">class</span><span class="o">)</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">MyTuple</span><span class="o">&lt;</span><span class="n">T0</span><span class="o">,</span> <span class="n">T1</span><span class="o">&gt;</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">T0</span> <span class="n">myfield0</span><span class="o">;</span>
  <span class="kd">public</span> <span class="n">T1</span> <span class="n">myfield1</span><span class="o">;</span>
<span class="o">}</span>
</code></pre></div><p>工厂供应自定义类型信息:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">MyTupleTypeInfoFactory</span> <span class="kd">extends</span> <span class="n">TypeInfoFactory</span><span class="o">&lt;</span><span class="n">MyTuple</span><span class="o">&gt;</span> <span class="o">{</span>

  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">TypeInformation</span><span class="o">&lt;</span><span class="n">MyTuple</span><span class="o">&gt;</span> <span class="nf">createTypeInfo</span><span class="o">(</span><span class="n">Type</span> <span class="n">t</span><span class="o">,</span> <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">TypeInformation</span><span class="o">&lt;?&gt;&gt;</span> <span class="n">genericParameters</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="k">new</span> <span class="n">MyTupleTypeInfo</span><span class="o">(</span><span class="n">genericParameters</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="s">&#34;T0&#34;</span><span class="o">),</span> <span class="n">genericParameters</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="s">&#34;T1&#34;</span><span class="o">));</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>方法 <code>createTypeInfo(Type, Map&lt;String, TypeInformation&lt;?&gt;)</code> 为工厂的目标类型创建类型信息。参数提供了关于类型本身的附加信息，以及类型的通用类型参数（如果可用）。</p>
<p>如果你的类型包含了可能需要从 Flink 函数的输入类型中导出的通用参数，请确保同时实现 <code>org.apache.flink.api.common.typeinfo.TypeInformation#getGenericParameters</code> 来实现通用参数到类型信息的双向映射。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/serialization" term="serialization" label="Serialization" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[配置]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-25-configuration/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Read and Write" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-streaming/?utm_source=atom_feed" rel="related" type="text/html" title="Hive Streaming" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-functions/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 方言" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Hive 集成 - 概览" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-25-configuration/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-25T00:00:00+08:00</published>
            <updated>2020-08-25T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Configuration</blockquote><h1 id="配置">配置</h1>
<p>默认情况下，Table &amp; SQL API 是预先配置的，可以在可接受的性能下产生准确的结果。</p>
<p>根据表程序的要求，可能需要调整某些参数进行优化。例如，无约束的流程序可能需要确保所需的状态大小是有上限的（参见流概念）。</p>
<h2 id="概述">概述</h2>
<p>在每个表环境中，TableConfig 都提供了配置当前会话的选项。</p>
<p>对于常见或重要的配置选项，TableConfig 提供了 getter 和 setter 方法，并提供了详细的内联文档。</p>
<p>对于更高级的配置，用户可以直接访问底层的键值映射。下面的章节列出了所有可用的选项，可以用来调整 Flink Table &amp; SQL API 程序。</p>
<p>注意: 由于在执行操作时，选项会在不同的时间点被读取，因此建议在实例化表环境后尽早设置配置选项。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// instantiate table environment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tEnv</span><span class="k">:</span> <span class="kt">TableEnvironment</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1">// access flink configuration
</span><span class="c1"></span><span class="k">val</span> <span class="n">configuration</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">getConfig</span><span class="o">().</span><span class="n">getConfiguration</span><span class="o">()</span>
<span class="c1">// set low-level key-value options
</span><span class="c1"></span><span class="n">configuration</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.exec.mini-batch.enabled&#34;</span><span class="o">,</span> <span class="s">&#34;true&#34;</span><span class="o">)</span>
<span class="n">configuration</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.exec.mini-batch.allow-latency&#34;</span><span class="o">,</span> <span class="s">&#34;5 s&#34;</span><span class="o">)</span>
<span class="n">configuration</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;table.exec.mini-batch.size&#34;</span><span class="o">,</span> <span class="s">&#34;5000&#34;</span><span class="o">)</span>
</code></pre></div><p>注意：目前，键值选项只支持 Blink 计划器。目前，键值选项只支持 Blink 计划器。</p>
<h3 id="执行选项">执行选项</h3>
<p>以下选项可以用来调整查询执行的性能。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Key</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">table.exec.async-lookup.buffer-capacity(Batch/Streaming)</td>
<td style="text-align:left">100</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">异步查找连接可以触发的最大异步 i/o 操作数。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.async-lookup.timeout(Batch/Streaming)</td>
<td style="text-align:left">&ldquo;3 min&rdquo;</td>
<td style="text-align:left">String</td>
<td style="text-align:left">异步操作完成的异步超时时间。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.disabled-operators(Batch)</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">String</td>
<td style="text-align:left">主要用于测试。一个以逗号分隔的操作符名称列表，每个名称代表一种被禁用的操作符。可以禁用的操作符包括 &ldquo;NestedLoopJoin&rdquo;、&ldquo;ShuffleHashJoin&rdquo;、&ldquo;BroadcastHashJoin&rdquo;、&ldquo;SortMergeJoin&rdquo;、&ldquo;HashAgg&rdquo;、&ldquo;SortAgg&rdquo;。默认情况下，没有任何操作符被禁用。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.mini-batch.allow-latency(Streaming)</td>
<td style="text-align:left">&ldquo;-1 ms&rdquo;</td>
<td style="text-align:left">String</td>
<td style="text-align:left">最大延迟可以用于 MiniBatch 来缓冲输入记录。MiniBatch 是一种优化，用于缓冲输入记录以减少状态访问。MiniBatch 会在允许的延迟间隔和达到最大缓冲记录数时触发。注意：如果 table.exec.mini-batch.enabled 被设置为 true，其值必须大于零。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.mini-batch.enabled(Streaming)</td>
<td style="text-align:left">false</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">指定是否启用 MiniBatch 优化。MiniBatch 是对输入记录进行缓冲以减少状态访问的优化。默认情况下，这个配置是禁用的。要启用这个功能，用户应该将这个配置设置为 true。注意：如果启用了 Mini-batch，必须设置&rsquo;table.exec.mini-batch.allow-latency&rsquo;和&rsquo;table.exec.mini-batch.size'。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.mini-batch.size(Streaming)</td>
<td style="text-align:left">-1</td>
<td style="text-align:left">Long</td>
<td style="text-align:left">MiniBatch 可以缓冲的输入记录的最大数量。MiniBatch 是对输入记录进行缓冲的优化，以减少状态访问。MiniBatch 会在允许的延迟间隔和达到最大缓冲记录数时触发。注意：MiniBatch 目前只适用于非窗口聚合。如果 table.exec.mini-batch.enabled 被设置为 true，其值必须为正。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.resource.default-parallelism(Batch/Streaming)</td>
<td style="text-align:left">-1</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">为所有操作符（如 aggregation、join、filter）设置默认的并行性，以便与并行实例一起运行。这个配置的优先级高于 StreamExecutionEnvironment 的并行性（实际上，这个配置覆盖了 StreamExecutionEnvironment 的并行性）。值为-1 表示没有设置默认的并行性，那么它将回落到使用 StreamExecutionEnvironment 的并行性。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.shuffle-mode(Batch)</td>
<td style="text-align:left">&ldquo;ALL_EDGES_BLOCKING&rdquo;</td>
<td style="text-align:left">String</td>
<td style="text-align:left">设置执行 shuffle 的模式。接受的值是, ALL_EDGES_BLOCKING: 所有边缘都将使用阻塞洗牌。FORWARD_EDGES_PIPELINED: 正向边缘将使用流水线洗牌，其他边缘将使用阻塞洗牌。POINTWISE_EDGES_PIPELINED: POINTWISE_EDGES_PIPELINED: 点向边缘将使用管道式洗牌，其他边缘将被阻挡。POINTWISE_EDGES_PIPELINED: 点向边缘包括前向和重新缩放边缘。ALL_EDGES_PIPELINED: 所有的边缘都将使用 pipelined shuffle，其他的边缘则使用 blocks。所有边缘都将使用流水线洗牌。batch: 与 ALL_EDGES_BLOCKING 相同。已废弃。pipelined: 与 ALL_EDGES_PIPELINED 相同。已被弃用。注意：Blocking shuffle 意味着数据将在发送到消费者任务之前被完全生成。Pipelined shuffle 意味着数据一旦被生产出来，就会被发送到消费者任务中。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.sink.not-null-enforcer(Batch/Streaming)</td>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">Enum,可能的值: [ERROR, DROP]</td>
<td style="text-align:left">表上的 NOT NULL 列约束强制要求不能将空值插入到表中。Flink 支持 &ldquo;错误&rdquo;（默认）和 &ldquo;放弃 &ldquo;执行行为。默认情况下，当 NOT NULL 列中写入空值时，Flink 会检查值并抛出运行时异常。用户可以将行为改为&rsquo;drop'，在不出现异常的情况下默默地删除这些记录。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.sort.async-merge-enabled(Batch)</td>
<td style="text-align:left">true</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">是否异步合并排序后的 spill 文件。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.sort.default-limit(Batch)</td>
<td style="text-align:left">-1</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">当用户在下单后没有设置限价时，默认限价。-1 表示该配置被忽略。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.sort.max-num-file-handles(Batch)</td>
<td style="text-align:left">128</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">外部合并排序的最大扇入量。它限制了每个操作者的文件句柄数。如果太小，可能会造成中间合并。但如果太大，会造成同时打开的文件太多，消耗内存，导致随机读取。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.source.idle-timeout(Streaming)</td>
<td style="text-align:left">&ldquo;-1 ms&rdquo;</td>
<td style="text-align:left">String</td>
<td style="text-align:left">当一个源在超时时间内没有收到任何元素时，它将被标记为暂时空闲。这样下游任务就可以提前打水印，而不需要在这个源空闲时等待它的水印。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.spill-compression.block-size(Batch)</td>
<td style="text-align:left">&ldquo;64 kb&rdquo;</td>
<td style="text-align:left">String</td>
<td style="text-align:left">溢出数据时做压缩时使用的内存大小。内存越大，压缩比越高，但作业会消耗更多的内存资源。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.spill-compression.enabled(Batch)</td>
<td style="text-align:left">true</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">是否压缩溢出数据。目前我们只支持压缩溢出数据的排序和哈希-agg 和哈希-join 操作符。</td>
</tr>
<tr>
<td style="text-align:left">table.exec.window-agg.buffer-size-limit(Batch)</td>
<td style="text-align:left">100000</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">设置组窗口 agg 运算符中使用的窗口元素缓冲区大小限制。</td>
</tr>
</tbody>
</table>
<h2 id="优化选项">优化选项</h2>
<p>以下选项可以用来调整查询优化器的行为，以获得更好的执行计划。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Key</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">table.optimizer.agg-phase-strategy(Batch/Streaming)</td>
<td style="text-align:left">&ldquo;AUTO&rdquo;</td>
<td style="text-align:left">String</td>
<td style="text-align:left">聚合阶段的策略。只能设置 AUTO、TWO_PHASE 或 ONE_PHASE。AUTO：集合阶段无特殊执行器。选择两阶段聚合还是一阶段聚合取决于成本。TWO_PHASE: 强制使用两级聚合，其中包括 localAggregate 和 globalAggregate。请注意，如果聚合调用不支持优化为两阶段，我们仍将使用一个阶段的聚合。ONE_PHASE: 强制使用只有 CompleteGlobalAggregate 的单阶段聚合。</td>
</tr>
<tr>
<td style="text-align:left">table.optimizer.distinct-agg.split.bucket-num(Streaming)</td>
<td style="text-align:left">1024</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">配置拆分不同聚合时的桶数。这个数字在一级聚合中用于计算一个桶键&rsquo;hash_code(distinct_key) % BUCKET_NUM'，这个桶键在拆分后作为一个额外的组键使用。</td>
</tr>
<tr>
<td style="text-align:left">table.optimizer.distinct-agg.split.enabled(Streaming)</td>
<td style="text-align:left">false</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">指示优化器是否将 distinct aggregation(例如 COUNT(DISTINCT col)，SUM(DISTINCT col))分成两级。第一层聚合由一个额外的 key 进行洗牌，这个 key 是用 distinct_key 和 buckets 数量的 hashcode 计算出来的。当 distinct aggregation 中存在数据倾斜时，这种优化是非常有用的，并提供了扩展作业的能力。默认为 false。</td>
</tr>
<tr>
<td style="text-align:left">table.optimizer.join-reorder-enabled(Batch/Streaming)</td>
<td style="text-align:left">false</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">在优化器中启用连接重排序。默认为禁用。</td>
</tr>
<tr>
<td style="text-align:left">table.optimizer.join.broadcast-threshold(Batch)</td>
<td style="text-align:left">1048576</td>
<td style="text-align:left">Long</td>
<td style="text-align:left">配置在执行连接时将向所有工作节点广播的表的最大字节数。将此值设置为-1，则禁用广播。</td>
</tr>
<tr>
<td style="text-align:left">table.optimizer.reuse-source-enabled(Batch/Streaming)</td>
<td style="text-align:left">true</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">当它为真时，优化器将尝试找出重复的表源并重用它们。这只有在 table.optimizer.reuse-sub-plan-enabled 为真时才会生效。</td>
</tr>
<tr>
<td style="text-align:left">table.optimizer.reuse-sub-plan-enabled(Batch/Streaming)</td>
<td style="text-align:left">true</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">当它为真时，优化器将尝试找出重复的子计划并重用它们。</td>
</tr>
<tr>
<td style="text-align:left">table.optimizer.source.predicate-pushdown-enabled(Batch/Streaming)</td>
<td style="text-align:left">true</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">当该值为真时，优化器将向下推送谓词到 FilterableTableSource 中。默认值为 true。</td>
</tr>
</tbody>
</table>
<h2 id="table-选项">Table 选项</h2>
<p>以下选项可用于调整表计划器(planner)的行为:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Key</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">table.dynamic-table-options.enabled(Batch/Streaming)</td>
<td style="text-align:left">false</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">启用或禁用 OPTIONS 提示，用于动态指定表选项，如果禁用，则如果指定了任何 OPTIONS 提示，就会产生异常。</td>
</tr>
<tr>
<td style="text-align:left">table.sql-dialect(Batch/Streaming)</td>
<td style="text-align:left">&ldquo;default&rdquo;</td>
<td style="text-align:left">String</td>
<td style="text-align:left">SQL 方言定义了如何解析一个 SQL 查询。不同的 SQL 方言可能支持不同的 SQL 语法。目前支持的方言有：默认和 hive。</td>
</tr>
</tbody>
</table>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/config.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/config.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Alter 语句]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Insert 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-sql-hints/?utm_source=atom_feed" rel="related" type="text/html" title="SQL 提示" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-show-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Show 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>ALTER Statements</blockquote><h1 id="alter-语句">ALTER 语句</h1>
<p>ALTER 语句用于修改目录中注册的表/视图/函数定义。</p>
<p>Flink SQL 目前支持以下 ALTER 语句。</p>
<ul>
<li>ALTER TABLE</li>
<li>ALTER DATABASE</li>
<li>ALTER FUNCTION</li>
</ul>
<h2 id="运行-alter-语句">运行 ALTER 语句</h2>
<p>ALTER 语句可以用 TableEnvironment 的 executeSql()方法执行，也可以在 SQL CLI 中执行。executeSql()方法在 ALTER 操作成功时返回 &ldquo;OK&rdquo;，否则将抛出一个异常。</p>
<p>下面的例子展示了如何在 TableEnvironment 和 SQL CLI 中运行 ALTER 语句。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">settings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">()...</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">settings</span><span class="o">)</span>

<span class="c1">// register a table named &#34;Orders&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)&#34;</span><span class="o">);</span>

<span class="c1">// a string array: [&#34;Orders&#34;]
</span><span class="c1"></span><span class="k">val</span> <span class="n">tables</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">listTables</span><span class="o">()</span>
<span class="c1">// or tableEnv.executeSql(&#34;SHOW TABLES&#34;).print()
</span><span class="c1"></span>
<span class="c1">// rename &#34;Orders&#34; to &#34;NewOrders&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;ALTER TABLE Orders RENAME TO NewOrders;&#34;</span><span class="o">)</span>

<span class="c1">// a string array: [&#34;NewOrders&#34;]
</span><span class="c1"></span><span class="k">val</span> <span class="n">tables</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">listTables</span><span class="o">()</span>
<span class="c1">// or tableEnv.executeSql(&#34;SHOW TABLES&#34;).print()
</span></code></pre></div><h2 id="alter-table">ALTER TABLE</h2>
<ul>
<li>Rename Table</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="k">table_name</span><span class="w"> </span><span class="k">RENAME</span><span class="w"> </span><span class="k">TO</span><span class="w"> </span><span class="n">new_table_name</span><span class="w">
</span></code></pre></div><p>将给定的表名重命名为另一个新表名。</p>
<ul>
<li>设置或更改表属性</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="k">table_name</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="n">val1</span><span class="p">,</span><span class="w"> </span><span class="n">key2</span><span class="o">=</span><span class="n">val2</span><span class="p">,</span><span class="w"> </span><span class="p">...)</span><span class="w">
</span></code></pre></div><p>设置指定表格中的一个或多个属性。如果某个属性已经在表中被设置，则用新的属性覆盖旧的值。</p>
<h2 id="alter-database">ALTER DATABASE</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="k">DATABASE</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.]</span><span class="n">db_name</span><span class="w"> </span><span class="k">SET</span><span class="w"> </span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="n">val1</span><span class="p">,</span><span class="w"> </span><span class="n">key2</span><span class="o">=</span><span class="n">val2</span><span class="p">,</span><span class="w"> </span><span class="p">...)</span><span class="w">
</span></code></pre></div><p>在指定的数据库中设置一个或多个属性。如果某个属性已经在数据库中被设置，则用新的属性覆盖旧的值。</p>
<h2 id="alter-function">ALTER FUNCTION</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">ALTER</span><span class="w"> </span><span class="p">[</span><span class="k">TEMPORARY</span><span class="o">|</span><span class="k">TEMPORARY</span><span class="w"> </span><span class="k">SYSTEM</span><span class="p">]</span><span class="w"> </span><span class="k">FUNCTION</span><span class="w"> 
</span><span class="w">  </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="n">function_name</span><span class="w"> 
</span><span class="w">  </span><span class="k">AS</span><span class="w"> </span><span class="n">identifier</span><span class="w"> </span><span class="p">[</span><span class="k">LANGUAGE</span><span class="w"> </span><span class="n">JAVA</span><span class="o">|</span><span class="n">SCALA</span><span class="o">|</span><span class="n">PYTHON</span><span class="p">]</span><span class="w">
</span></code></pre></div><p>用新的标识符和可选的语言标签改变一个目录函数。如果一个函数在目录中不存在，就会抛出一个异常。</p>
<p>如果语言标签是 JAVA/SCALA，标识符是 UDF 的完整 classpath。关于 Java/Scala UDF 的实现，请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html">User-defined Functions</a> 了解详情。</p>
<p>如果语言标签是 PYTHON，标识符是 UDF 的完全限定名，例如 pyflink.table.test.test_udf.add。关于 Python UDF 的实现，更多细节请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/python/user-guide/table/udfs/python_udfs.html">Python UDFs</a>。</p>
<p><strong>TEMPORARY</strong></p>
<p>改变具有目录和数据库命名空间的临时目录功能，并覆盖目录功能。</p>
<p><strong>TEMPORARY SYSTEM</strong></p>
<p>更改没有命名空间的临时系统函数，并覆盖内置函数。</p>
<p><strong>IF EXISTS</strong></p>
<p>如果函数不存在，就不会发生任何事情。</p>
<p><strong>LANGUAGE JAVA|SCALA|PYTHON</strong></p>
<p>语言标签，用于指导 flink 运行时如何执行函数。目前只支持 JAVA、SCALA 和 PYTHON，函数的默认语言是 JAVA。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/alter.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/alter.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/sql" term="sql" label="SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Catalogs]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Insert 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-catalogs/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Catalogs</blockquote><h1 id="catalogs">Catalogs</h1>
<p>目录提供了元数据，如数据库、表、分区、视图以及访问数据库或其他外部系统中存储的数据所需的功能和信息。</p>
<p>数据处理中最关键的一个方面是管理元数据。它可能是短暂的元数据，如临时表，或针对表环境注册的 UDF。或者是永久性的元数据，比如 Hive Metastore 中的元数据。目录为管理元数据提供了统一的 API，并使其可以从表 API 和 SQL 查询中访问。</p>
<p>Catalog 使用户能够引用数据系统中现有的元数据，并自动将它们映射到 Flink 的相应元数据中。例如，Flink 可以将 JDBC 表自动映射到 Flink 表，用户不必在 Flink 中手动重新编写 DDL。Catalog 大大简化了用户现有系统上手 Flink 所需的步骤，大大提升了用户体验。</p>
<h2 id="catalog-类型">Catalog 类型</h2>
<h3 id="genericinmemorycatalog">GenericInMemoryCatalog</h3>
<p>GenericInMemoryCatalog 是一个目录的内存实现。所有对象只在会话的生命周期内可用。</p>
<h3 id="jdbccatalog">JdbcCatalog</h3>
<p>JdbcCatalog 使用户能够通过 JDBC 协议将 Flink 与关系型数据库连接起来。PostgresCatalog 是目前 JDBC Catalog 的唯一实现。关于设置目录的更多细节，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html">JdbcCatalog 文档</a>。</p>
<h3 id="hivecatalog">HiveCatalog</h3>
<p>HiveCatalog 有两个目的，一是作为纯 Flink 元数据的持久化存储，二是作为读写现有 Hive 元数据的接口。Flink 的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/index.html">Hive 文档</a>提供了设置目录和与现有 Hive 安装接口的完整细节。</p>
<p>警告: Hive Metastore 将所有的元对象名称都存储为小写。这与 GenericInMemoryCatalog 不同，后者是区分大小写的。</p>
<h3 id="用户定义的-catalog">用户定义的 Catalog</h3>
<p>目录是可插拔的，用户可以通过实现 Catalog 接口来开发自定义目录。要在 SQL CLI 中使用自定义目录，用户应该通过实现 CatalogFactory 接口同时开发目录和它对应的目录工厂。</p>
<p>目录工厂定义了一组属性，用于在 SQL CLI 引导时配置目录。该属性集将被传递给发现服务，服务会尝试将属性与 CatalogFactory 匹配，并启动相应的目录实例。</p>
<h2 id="如何创建和注册-flink-table-到目录上">如何创建和注册 Flink Table 到目录上</h2>
<h3 id="使用-sql-ddl">使用 SQL DDL</h3>
<p>用户可以使用 SQL DDL 在 Table API 和 SQL 中创建目录中的表。</p>
<ul>
<li>Flink SQL</li>
</ul>
<pre><code>// the catalog should have been registered via yaml file
Flink SQL&gt; CREATE DATABASE mydb WITH (...);

Flink SQL&gt; CREATE TABLE mytable (name STRING, age INT) WITH (...);

Flink SQL&gt; SHOW TABLES;
mytable
</code></pre><p>详细信息，请查看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/create.html">Flink SQL CREATE DDL</a>。</p>
<ul>
<li>Scala</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// Create a HiveCatalog 
</span><span class="c1"></span><span class="k">val</span> <span class="n">catalog</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HiveCatalog</span><span class="o">(</span><span class="s">&#34;myhive&#34;</span><span class="o">,</span> <span class="kc">null</span><span class="o">,</span> <span class="s">&#34;&lt;path_of_hive_conf&gt;&#34;</span><span class="o">)</span>

<span class="c1">// Register the catalog
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">registerCatalog</span><span class="o">(</span><span class="s">&#34;myhive&#34;</span><span class="o">,</span> <span class="n">catalog</span><span class="o">)</span>

<span class="c1">// Create a catalog database
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE DATABASE mydb WITH (...)&#34;</span><span class="o">)</span>

<span class="c1">// Create a catalog table
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE mytable (name STRING, age INT) WITH (...)&#34;</span><span class="o">)</span>

<span class="n">tableEnv</span><span class="o">.</span><span class="n">listTables</span><span class="o">()</span> <span class="c1">// should return the tables in current catalog and database.
</span></code></pre></div><p>For detailed information, please check out Flink SQL CREATE DDL.</p>
<h3 id="使用-java-scala-或-python">使用 Java, Scala 或 Python</h3>
<p>用户可以使用 Java、Scala 或 Python 来编程创建目录表。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.table.api._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.catalog._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.catalog.hive.HiveCatalog</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.descriptors.Kafka</span>

<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">.</span><span class="n">build</span><span class="o">)</span>

<span class="c1">// Create a HiveCatalog 
</span><span class="c1"></span><span class="k">val</span> <span class="n">catalog</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HiveCatalog</span><span class="o">(</span><span class="s">&#34;myhive&#34;</span><span class="o">,</span> <span class="kc">null</span><span class="o">,</span> <span class="s">&#34;&lt;path_of_hive_conf&gt;&#34;</span><span class="o">)</span>

<span class="c1">// Register the catalog
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">registerCatalog</span><span class="o">(</span><span class="s">&#34;myhive&#34;</span><span class="o">,</span> <span class="n">catalog</span><span class="o">)</span>

<span class="c1">// Create a catalog database 
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">createDatabase</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">CatalogDatabaseImpl</span><span class="o">(...))</span>

<span class="c1">// Create a catalog table
</span><span class="c1"></span><span class="k">val</span> <span class="n">schema</span> <span class="k">=</span> <span class="nc">TableSchema</span><span class="o">.</span><span class="n">builder</span><span class="o">()</span>
    <span class="o">.</span><span class="n">field</span><span class="o">(</span><span class="s">&#34;name&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">STRING</span><span class="o">())</span>
    <span class="o">.</span><span class="n">field</span><span class="o">(</span><span class="s">&#34;age&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">INT</span><span class="o">())</span>
    <span class="o">.</span><span class="n">build</span><span class="o">()</span>

<span class="n">catalog</span><span class="o">.</span><span class="n">createTable</span><span class="o">(</span>
        <span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> 
        <span class="k">new</span> <span class="nc">CatalogTableImpl</span><span class="o">(</span>
            <span class="n">schema</span><span class="o">,</span>
            <span class="k">new</span> <span class="nc">Kafka</span><span class="o">()</span>
                <span class="o">.</span><span class="n">version</span><span class="o">(</span><span class="s">&#34;0.11&#34;</span><span class="o">)</span>
                <span class="o">....</span>
                <span class="o">.</span><span class="n">startFromEarlist</span><span class="o">()</span>
                <span class="o">.</span><span class="n">toProperties</span><span class="o">(),</span>
            <span class="s">&#34;my comment&#34;</span>
        <span class="o">),</span>
        <span class="kc">false</span>
    <span class="o">)</span>
    
<span class="k">val</span> <span class="n">tables</span> <span class="k">=</span> <span class="n">catalog</span><span class="o">.</span><span class="n">listTables</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">)</span> <span class="c1">// tables should contain &#34;mytable&#34;
</span></code></pre></div><h2 id="catalog-api">Catalog API</h2>
<p>注意：这里只列出了目录程序的 API，用户可以通过 SQL DDL 实现许多相同的功能。用户可以通过 SQL DDL 实现许多相同的功能。详细的 DDL 信息，请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/create.html">SQL CREATE DDL</a>。</p>
<h3 id="数据库操作">数据库操作</h3>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// create database
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">createDatabase</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">CatalogDatabaseImpl</span><span class="o">(...),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// drop database
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">dropDatabase</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// alter database
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">alterDatabase</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">CatalogDatabaseImpl</span><span class="o">(...),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// get database
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">getDatabase</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">);</span>

<span class="c1">// check if a database exist
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">databaseExists</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">);</span>

<span class="c1">// list databases in a catalog
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">listDatabases</span><span class="o">();</span>
</code></pre></div><h3 id="table-操作">Table 操作</h3>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// create table
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">createTable</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">CatalogTableImpl</span><span class="o">(...),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// drop table
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">dropTable</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// alter table
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">alterTable</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">CatalogTableImpl</span><span class="o">(...),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// rename table
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">renameTable</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> <span class="s">&#34;my_new_table&#34;</span><span class="o">);</span>

<span class="c1">// get table
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">getTable</span><span class="o">(</span><span class="s">&#34;mytable&#34;</span><span class="o">);</span>

<span class="c1">// check if a table exist or not
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">tableExists</span><span class="o">(</span><span class="s">&#34;mytable&#34;</span><span class="o">);</span>

<span class="c1">// list tables in a database
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">listTables</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">);</span>
</code></pre></div><h3 id="视图操作">视图操作</h3>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// create view
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">createTable</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;myview&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">CatalogViewImpl</span><span class="o">(...),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// drop view
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">dropTable</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;myview&#34;</span><span class="o">),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// alter view
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">alterTable</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">CatalogViewImpl</span><span class="o">(...),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// rename view
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">renameTable</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;myview&#34;</span><span class="o">),</span> <span class="s">&#34;my_new_view&#34;</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// get view
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">getTable</span><span class="o">(</span><span class="s">&#34;myview&#34;</span><span class="o">);</span>

<span class="c1">// check if a view exist or not
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">tableExists</span><span class="o">(</span><span class="s">&#34;mytable&#34;</span><span class="o">);</span>

<span class="c1">// list views in a database
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">listViews</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">);</span>
</code></pre></div><h3 id="partition-操作">Partition 操作</h3>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// create view
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">createPartition</span><span class="o">(</span>
    <span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span>
    <span class="k">new</span> <span class="nc">CatalogPartitionSpec</span><span class="o">(...),</span>
    <span class="k">new</span> <span class="nc">CatalogPartitionImpl</span><span class="o">(...),</span>
    <span class="kc">false</span><span class="o">);</span>

<span class="c1">// drop partition
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">dropPartition</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">CatalogPartitionSpec</span><span class="o">(...),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// alter partition
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">alterPartition</span><span class="o">(</span>
    <span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span>
    <span class="k">new</span> <span class="nc">CatalogPartitionSpec</span><span class="o">(...),</span>
    <span class="k">new</span> <span class="nc">CatalogPartitionImpl</span><span class="o">(...),</span>
    <span class="kc">false</span><span class="o">);</span>

<span class="c1">// get partition
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">getPartition</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">CatalogPartitionSpec</span><span class="o">(...));</span>

<span class="c1">// check if a partition exist or not
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">partitionExists</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">CatalogPartitionSpec</span><span class="o">(...));</span>

<span class="c1">// list partitions of a table
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">listPartitions</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">));</span>

<span class="c1">// list partitions of a table under a give partition spec
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">listPartitions</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">CatalogPartitionSpec</span><span class="o">(...));</span>

<span class="c1">// list partitions of a table by expression filter
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">listPartitionsByFilter</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;mytable&#34;</span><span class="o">),</span> <span class="nc">Arrays</span><span class="o">.</span><span class="n">asList</span><span class="o">(</span><span class="n">epr1</span><span class="o">,</span> <span class="o">...));</span>
</code></pre></div><h3 id="function-操作">Function 操作</h3>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// create function
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">createFunction</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;myfunc&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">CatalogFunctionImpl</span><span class="o">(...),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// drop function
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">dropFunction</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;myfunc&#34;</span><span class="o">),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// alter function
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">alterFunction</span><span class="o">(</span><span class="k">new</span> <span class="nc">ObjectPath</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">,</span> <span class="s">&#34;myfunc&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">CatalogFunctionImpl</span><span class="o">(...),</span> <span class="kc">false</span><span class="o">);</span>

<span class="c1">// get function
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">getFunction</span><span class="o">(</span><span class="s">&#34;myfunc&#34;</span><span class="o">);</span>

<span class="c1">// check if a function exist or not
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">functionExists</span><span class="o">(</span><span class="s">&#34;myfunc&#34;</span><span class="o">);</span>

<span class="c1">// list functions in a database
</span><span class="c1"></span><span class="n">catalog</span><span class="o">.</span><span class="n">listFunctions</span><span class="o">(</span><span class="s">&#34;mydb&#34;</span><span class="o">);</span>
</code></pre></div><h2 id="目录的-table-api-和-sql">目录的 Table API 和 SQL</h2>
<h3 id="注册目录">注册目录</h3>
<p>用户可以访问一个名为 default_catalog 的默认内存目录，这个目录总是默认创建的。该目录默认有一个名为 default_database 的单一数据库。用户也可以在现有的 Flink 会话中注册额外的目录。</p>
<ul>
<li>Scala</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">tableEnv</span><span class="o">.</span><span class="n">registerCatalog</span><span class="o">(</span><span class="k">new</span> <span class="nc">CustomCatalog</span><span class="o">(</span><span class="s">&#34;myCatalog&#34;</span><span class="o">));</span>
</code></pre></div><ul>
<li>YAML</li>
</ul>
<p>所有使用 YAML 定义的目录必须提供一个 <code>type</code> 属性，指定目录的类型。以下类型是开箱即用的。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Catalog</th>
<th style="text-align:left">Type Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">GenericInMemory</td>
<td style="text-align:left">generic_in_memory</td>
</tr>
<tr>
<td style="text-align:left">Hive</td>
<td style="text-align:left">hive</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">catalogs</span><span class="p">:</span><span class="w">
</span><span class="w">   </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myCatalog</span><span class="w">
</span><span class="w">     </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">custom_catalog</span><span class="w">
</span><span class="w">     </span><span class="nt">hive-conf-dir</span><span class="p">:</span><span class="w"> </span><span class="l">...</span><span class="w">
</span></code></pre></div><h3 id="更改当前目录和数据库">更改当前目录和数据库</h3>
<p>Flink 将始终搜索当前目录和数据库中的表、视图和 UDF。</p>
<ul>
<li>Scala</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">tableEnv</span><span class="o">.</span><span class="n">useCatalog</span><span class="o">(</span><span class="s">&#34;myCatalog&#34;</span><span class="o">);</span>
<span class="n">tableEnv</span><span class="o">.</span><span class="n">useDatabase</span><span class="o">(</span><span class="s">&#34;myDb&#34;</span><span class="o">);</span>
</code></pre></div><ul>
<li>Flink SQL</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; USE CATALOG myCatalog<span class="p">;</span>
Flink SQL&gt; USE myDB<span class="p">;</span>
</code></pre></div><p>通过提供 catalog.database.object 形式的完全限定名称，可以访问非当前目录的元数据。</p>
<ul>
<li>Scala</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;not_the_current_catalog.not_the_current_db.my_table&#34;</span><span class="o">);</span>
</code></pre></div><ul>
<li>Flink SQL</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; SELECT * FROM not_the_current_catalog.not_the_current_db.my_table<span class="p">;</span>
</code></pre></div><h3 id="列出可用的目录">列出可用的目录</h3>
<ul>
<li>Scala</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">tableEnv</span><span class="o">.</span><span class="n">listCatalogs</span><span class="o">();</span>
</code></pre></div><ul>
<li>Flink SQL</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; show catalogs<span class="p">;</span>
</code></pre></div><h3 id="列出可用的数据库">列出可用的数据库</h3>
<ul>
<li>Scala</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">tableEnv</span><span class="o">.</span><span class="n">listDatabases</span><span class="o">();</span>
</code></pre></div><ul>
<li>Flink SQL</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; show databases<span class="p">;</span>
</code></pre></div><h3 id="列出可用的表">列出可用的表</h3>
<ul>
<li>Scala</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">tableEnv</span><span class="o">.</span><span class="n">listTables</span><span class="o">();</span>
</code></pre></div><ul>
<li>Flink SQL</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; show tables<span class="p">;</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/catalogs.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/catalogs.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/catalogs" term="catalogs" label="Catalogs" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Create 语句]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Insert 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-create-statements/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Create Statements</blockquote><h1 id="create-语句">CREATE 语句</h1>
<p>CREATE 语句用于在当前或指定的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/catalogs.html">目录</a>中注册一个表/视图/函数。注册的表/视图/函数可以在 SQL 查询中使用。</p>
<p>Flink SQL 目前支持以下 CREATE 语句。</p>
<ul>
<li>CREATE TABLE</li>
<li>CREATE DATABASE</li>
<li>CREATE VIEW</li>
<li>CREATE FUNCTION</li>
</ul>
<h2 id="运行一条-create-语句">运行一条 CREATE 语句</h2>
<p>CREATE 语句可以用 TableEnvironment 的 executeSql()方法执行，也可以在 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html">SQL CLI</a> 中执行。executeSql()方法对于一个成功的 CREATE 操作会返回&rsquo;OK'，否则会抛出一个异常。</p>
<p>下面的例子展示了如何在 TableEnvironment 和 SQL CLI 中运行 CREATE 语句。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Scala
</span><span class="c1"></span><span class="k">val</span> <span class="n">settings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">()...</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">settings</span><span class="o">)</span>

<span class="c1">// SQL query with a registered table
</span><span class="c1">// register a table named &#34;Orders&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)&#34;</span><span class="o">);</span>
<span class="c1">// run a SQL query on the Table and retrieve the result as a new Table
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
  <span class="s">&#34;SELECT product, amount FROM Orders WHERE product LIKE &#39;%Rubber%&#39;&#34;</span><span class="o">);</span>

<span class="c1">// Execute insert SQL with a registered table
</span><span class="c1">// register a TableSink
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (&#39;connector.path&#39;=&#39;/path/to/file&#39; ...)&#34;</span><span class="o">);</span>
<span class="c1">// run an insert SQL on the Table and emit the result to the TableSink
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span>
  <span class="s">&#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE &#39;%Rubber%&#39;&#34;</span><span class="o">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; CREATE TABLE Orders <span class="o">(</span><span class="sb">`</span>user<span class="sb">`</span> BIGINT, product STRING, amount INT<span class="o">)</span> WITH <span class="o">(</span>...<span class="o">)</span><span class="p">;</span>
<span class="o">[</span>INFO<span class="o">]</span> Table has been created.

Flink SQL&gt; CREATE TABLE RubberOrders <span class="o">(</span>product STRING, amount INT<span class="o">)</span> WITH <span class="o">(</span>...<span class="o">)</span><span class="p">;</span>
<span class="o">[</span>INFO<span class="o">]</span> Table has been created.

Flink SQL&gt; INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE <span class="s1">&#39;%Rubber%&#39;</span><span class="p">;</span>
<span class="o">[</span>INFO<span class="o">]</span> Submitting SQL update statement to the cluster...
</code></pre></div><h2 id="create-table">CREATE TABLE</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="k">table_name</span><span class="w">
</span><span class="w">  </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="err">{</span><span class="w"> </span><span class="o">&lt;</span><span class="n">column_definition</span><span class="o">&gt;</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="o">&lt;</span><span class="n">computed_column_definition</span><span class="o">&gt;</span><span class="w"> </span><span class="err">}</span><span class="p">[</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="p">...</span><span class="n">n</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="p">[</span><span class="w"> </span><span class="o">&lt;</span><span class="n">watermark_definition</span><span class="o">&gt;</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="p">[</span><span class="w"> </span><span class="o">&lt;</span><span class="n">table_constraint</span><span class="o">&gt;</span><span class="w"> </span><span class="p">][</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="p">...</span><span class="n">n</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">table_comment</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="n">PARTITIONED</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="p">(</span><span class="n">partition_column_name1</span><span class="p">,</span><span class="w"> </span><span class="n">partition_column_name2</span><span class="p">,</span><span class="w"> </span><span class="p">...)]</span><span class="w">
</span><span class="w">  </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="n">val1</span><span class="p">,</span><span class="w"> </span><span class="n">key2</span><span class="o">=</span><span class="n">val2</span><span class="p">,</span><span class="w"> </span><span class="p">...)</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="w"> </span><span class="k">LIKE</span><span class="w"> </span><span class="n">source_table</span><span class="w"> </span><span class="p">[(</span><span class="w"> </span><span class="o">&lt;</span><span class="n">like_options</span><span class="o">&gt;</span><span class="w"> </span><span class="p">)]</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="w">   
</span><span class="w"></span><span class="o">&lt;</span><span class="n">column_definition</span><span class="o">&gt;</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">column_name</span><span class="w"> </span><span class="n">column_type</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="o">&lt;</span><span class="n">column_constraint</span><span class="o">&gt;</span><span class="w"> </span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">column_comment</span><span class="p">]</span><span class="w">
</span><span class="w">  
</span><span class="w"></span><span class="o">&lt;</span><span class="n">column_constraint</span><span class="o">&gt;</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">CONSTRAINT</span><span class="w"> </span><span class="k">constraint_name</span><span class="p">]</span><span class="w"> </span><span class="k">PRIMARY</span><span class="w"> </span><span class="k">KEY</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="n">ENFORCED</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="o">&lt;</span><span class="n">table_constraint</span><span class="o">&gt;</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">CONSTRAINT</span><span class="w"> </span><span class="k">constraint_name</span><span class="p">]</span><span class="w"> </span><span class="k">PRIMARY</span><span class="w"> </span><span class="k">KEY</span><span class="w"> </span><span class="p">(</span><span class="k">column_name</span><span class="p">,</span><span class="w"> </span><span class="p">...)</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="n">ENFORCED</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="o">&lt;</span><span class="n">computed_column_definition</span><span class="o">&gt;</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">column_name</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">computed_column_expression</span><span class="w"> </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">column_comment</span><span class="p">]</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="o">&lt;</span><span class="n">watermark_definition</span><span class="o">&gt;</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="n">WATERMARK</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">rowtime_column_name</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">watermark_strategy_expression</span><span class="w">
</span><span class="w">  
</span><span class="w"></span><span class="o">&lt;</span><span class="n">like_options</span><span class="o">&gt;</span><span class="p">:</span><span class="w">
</span><span class="w"></span><span class="err">{</span><span class="w">
</span><span class="w">   </span><span class="err">{</span><span class="w"> </span><span class="k">INCLUDING</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="k">EXCLUDING</span><span class="w"> </span><span class="err">}</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="k">ALL</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="k">CONSTRAINTS</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">PARTITIONS</span><span class="w"> </span><span class="err">}</span><span class="w">
</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="k">INCLUDING</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="k">EXCLUDING</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">OVERWRITING</span><span class="w"> </span><span class="err">}</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="k">GENERATED</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="k">OPTIONS</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">WATERMARKS</span><span class="w"> </span><span class="err">}</span><span class="w"> 
</span><span class="w"></span><span class="err">}</span><span class="p">[,</span><span class="w"> </span><span class="p">...]</span><span class="w">
</span></code></pre></div><p>用给定的名称创建一个表。如果目录中已经存在同名表，则抛出一个异常。</p>
<p><strong>计算列</strong></p>
<p>计算列是使用 &ldquo;column_name AS computed_column_expression&rdquo; 语法生成的虚拟列。它是由一个非查询表达式生成的，这个表达式使用同一张表中的其他列，而不是实际存储在表中。例如，计算列可以定义为 <code>cost AS price * quantity</code>。表达式可以包含物理列、常量、函数或变量的任意组合。表达式不能包含子查询。</p>
<p>计算列在 Flink 中通常用于在 CREATE TABLE 语句中定义<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html">时间属性</a>。可以通过 <code>proc AS PROCTIME()</code> 使用系统 <code>proctime()</code> 函数轻松定义一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html#processing-time">处理时间属性</a>。另一方面，计算列可以用来派生事件时间列，因为事件时间列可能需要从现有的字段中派生出来，比如原来的字段不是 TIMESTAMP(3)类型，或者嵌套在 JSON 字符串中。</p>
<p>注意：</p>
<ul>
<li>在源表上定义的计算列是在从源表读取后计算出来的，它可以用在下面的 SELECT 查询语句中。</li>
<li>计算列不能作为 INSERT 语句的目标。在 INSERT 语句中，SELECT 子句的模式应该与没有计算列的目标表的模式相匹配。</li>
</ul>
<p><strong>WATERMARK</strong></p>
<p>WATERMARK 定义了表的事件时间属性，其形式为 <code>WATERMARK FOR rowtime_column_name AS watermark_strategy_expression</code>。</p>
<p><code>rowtime_column_name</code> 定义了一个现有的列，该列被标记为表的事件时间属性。这个列的类型必须是 TIMESTAMP(3)，并且是模式中的顶层列。它可以是一个计算列。</p>
<p><code>watermark_strategy_expression</code> 定义了水印生成策略。它允许任意的非查询表达式，包括计算列，来计算水印。表达式的返回类型必须是 TIMESTAMP(3)，它表示自 Epoch 以来的时间戳。只有当返回的水印是非空的，并且它的值大于之前发出的本地水印时，才会发出水印（以保留升水印的契约）。水印生成表达式由框架对每条记录进行评估。框架将定期发射最大的生成水印。如果当前的水印仍然与上一个水印相同，或者是空的，或者返回的水印值小于上一次发射的水印值，那么将不会发射新的水印。水印是在 <code>pipeline.auto-watermark-interval</code> 配置定义的时间间隔内发出的。如果水印间隔为 0ms，如果生成的水印不是空的，并且大于最后一个水印，则每条记录都会发出水印。</p>
<p>当使用事件时间语义时，表必须包含事件时间属性和水印策略。</p>
<p>Flink 提供了几种常用的水印策略。</p>
<ul>
<li>
<p>严格的升序时间戳。WATERMARK FOR rowtime_column AS rowtime_column。</p>
</li>
<li>
<p>发出迄今为止观察到的最大时间戳的水印。时间戳小于最大时间戳的行不会迟到。</p>
</li>
<li>
<p>升序时间戳。WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL &lsquo;0.001&rsquo; SECOND.</p>
</li>
<li>
<p>发出迄今为止观察到的最大时间戳的水印减 1。时间戳等于或小于最大时间戳的行不会迟到。</p>
</li>
<li>
<p>绑定出顺序性的时间戳。WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL &lsquo;string&rsquo; timeUnit.</p>
</li>
</ul>
<p>发出水印，水印是最大观察到的时间戳减去指定的延迟，例如：WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL &lsquo;5&rsquo; SECOND 是 5 秒的延迟水印策略。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="k">user</span><span class="w"> </span><span class="nb">BIGINT</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">product</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">order_time</span><span class="w"> </span><span class="k">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span><span class="w">
</span><span class="w">    </span><span class="n">WATERMARK</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">order_time</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">order_time</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;5&#39;</span><span class="w"> </span><span class="k">SECOND</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="p">);</span><span class="w">
</span><span class="w"></span><span class="k">PRIMARY</span><span class="w"> </span><span class="k">KEY</span><span class="w">
</span></code></pre></div><p><strong>PRIMARY KEY</strong></p>
<p>Flink 利用优化的一个提示。它告诉我们一个表或视图的一列或一组列是唯一的，它们不包含空值。主键中的两列都不能为空。因此，主键可以唯一地识别表中的某一行。</p>
<p>主键约束既可以和列定义一起声明（列约束），也可以作为单行（表约束）。对于这两种情况，只能将其声明为一个单子。如果你同时定义了多个主键约束，就会抛出一个异常。</p>
<p><strong>有效性检查</strong></p>
<p>SQL 标准规定，一个约束可以是 ENFORCED 或 NOT ENFORCED。这控制了约束检查是否会在输入/输出数据上执行。Flink 并不拥有数据，因此我们要支持的唯一模式是 NOT ENFORCED 模式。用户要确保查询强制执行密钥的完整性。</p>
<p>Flink 会假设主键的正确性，假设列的空性与主键的列对齐。连接器应该确保这些是对齐的。</p>
<p>注意事项: 在 CREATE TABLE 语句中，创建主键约束会改变列的可空性，也就是说，有主键约束的列是不可空的。</p>
<p><strong>PARTITIONED BY</strong></p>
<p>按指定的列对创建的表进行分区。如果该表被用作文件系统汇，则会为每个分区创建一个目录。</p>
<p><strong>WITH OPTIONS</strong></p>
<p>表属性用于创建表源/接收器。这些属性通常用于查找和创建底层连接器。</p>
<p>表达式 key1=val1 的键和值都应该是字符串文字。关于不同连接器的所有支持的表属性，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connect.html">连接到外部系统</a>中的详细信息。</p>
<p>注释：表名可以有三种格式。表名可以有三种格式。</p>
<ol>
<li>catalog_name.db_name.table_name</li>
<li>db_name.table_name</li>
<li>table_name。</li>
</ol>
<p>对于 catalog_name.db_name.table_name，表将被注册到元存储中，目录名为 &ldquo;catalog_name&rdquo;，数据库名为 &ldquo;db_name&rdquo;；对于 db_name.table_name，表将被注册到执行表环境的当前目录中，数据库名为 &ldquo;db_name&rdquo;；对于 table_name，表将被注册到执行表环境的当前目录和数据库中。</p>
<p>注意事项: 用 CREATE TABLE 语句注册的表既可以作为表源，也可以作为表汇，在 DMLs 中没有引用之前，我们不能决定它是作为表源还是表汇使用。</p>
<p><strong>LIKE 子句</strong></p>
<p>LIKE 子句是 SQL 特征的变体/组合（特征 T171，&ldquo;表定义中的 LIKE 子句&quot;和特征 T173，&ldquo;表定义中的扩展 LIKE 子句&rdquo;）。该子句可用于根据现有表的定义创建一个表。此外，用户还可以扩展原表或排除其中的某些部分。与 SQL 标准不同的是，该子句必须在 CREATE 语句的顶层定义。这是因为该子句适用于定义的多个部分，而不仅仅是模式部分。</p>
<p>你可以使用该子句来重用（并可能覆盖）某些连接器属性，或者为外部定义的表添加水印。例如，您可以为 Apache Hive 中定义的表添加水印。</p>
<p>请考虑下面的示例语句。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="k">user</span><span class="w"> </span><span class="nb">BIGINT</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">product</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">order_time</span><span class="w"> </span><span class="k">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="w"> 
</span><span class="w">    </span><span class="s1">&#39;connector&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;kafka&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="s1">&#39;scan.startup.mode&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;earliest-offset&#39;</span><span class="w">
</span><span class="w"></span><span class="p">);</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">Orders_with_watermark</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="c1">-- Add watermark definition
</span><span class="c1"></span><span class="w">    </span><span class="n">WATERMARK</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">order_time</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">order_time</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;5&#39;</span><span class="w"> </span><span class="k">SECOND</span><span class="w"> 
</span><span class="w"></span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="c1">-- Overwrite the startup-mode
</span><span class="c1"></span><span class="w">    </span><span class="s1">&#39;scan.startup.mode&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;latest-offset&#39;</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">LIKE</span><span class="w"> </span><span class="n">Orders</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>由此产生的 Orders_with_watermark 表将等同于用以下语句创建的表。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">Orders_with_watermark</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="k">user</span><span class="w"> </span><span class="nb">BIGINT</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">product</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">order_time</span><span class="w"> </span><span class="k">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span><span class="w">
</span><span class="w">    </span><span class="n">WATERMARK</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">order_time</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">order_time</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;5&#39;</span><span class="w"> </span><span class="k">SECOND</span><span class="w"> 
</span><span class="w"></span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="s1">&#39;connector&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;kafka&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="s1">&#39;scan.startup.mode&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;latest-offset&#39;</span><span class="w">
</span><span class="w"></span><span class="p">);</span><span class="w">
</span></code></pre></div><p>可以用同类选项控制表功能的合并逻辑。</p>
<p>您可以控制以下的合并行为:</p>
<ul>
<li>CONSTRAINTS - 主键和唯一键等约束条件。</li>
<li>GENERATED-计算列</li>
<li>OPTIONS - 描述连接器和格式属性的连接器选项。</li>
<li>PARTITIONS - 表的分区</li>
<li>WATERMARKS - 水印声明</li>
</ul>
<p>有三种不同的合并策略。</p>
<ul>
<li>INCLUDING - 包括源表的特征，对重复的条目失败，例如，如果两个表中都存在相同键的选项。</li>
<li>EXCLUDING - 不包含源表的给定特征。</li>
<li>OVERWRITING - 包括源表的特征，用新表的属性覆盖源表的重复条目，例如，如果两个表中都存在具有相同键的选项，则将使用当前语句中的选项。</li>
</ul>
<p>此外，可以使用 INCLUDING/EXCLUDING ALL 选项来指定如果没有定义特定的策略应该是什么，即如果使用 EXCLUDING ALL INCLUDING WATERMARKS，则只从源表中包含水印。</p>
<p>例子：</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="c1">-- A source table stored in a filesystem
</span><span class="c1"></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">Orders_in_file</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="k">user</span><span class="w"> </span><span class="nb">BIGINT</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">product</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">order_time_string</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">order_time</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">to_timestamp</span><span class="p">(</span><span class="n">order_time</span><span class="p">)</span><span class="w">
</span><span class="w">    
</span><span class="w"></span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="n">PARTITIONED</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">user</span><span class="w"> 
</span><span class="w"></span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="w"> 
</span><span class="w">    </span><span class="s1">&#39;connector&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;filesystem&#39;</span><span class="w">
</span><span class="w">    </span><span class="s1">&#39;path&#39;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;...&#39;</span><span class="w">
</span><span class="w"></span><span class="p">);</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c1">-- A corresponding table we want to store in kafka
</span><span class="c1"></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">Orders_in_kafka</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="c1">-- Add watermark definition
</span><span class="c1"></span><span class="w">    </span><span class="n">WATERMARK</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">order_time</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">order_time</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;5&#39;</span><span class="w"> </span><span class="k">SECOND</span><span class="w"> 
</span><span class="w"></span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="s1">&#39;connector&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;kafka&#39;</span><span class="w">
</span><span class="w">    </span><span class="p">...</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">LIKE</span><span class="w"> </span><span class="n">Orders_in_file</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="c1">-- Exclude everything besides the computed columns which we need to generate the watermark for.
</span><span class="c1"></span><span class="w">    </span><span class="c1">-- We do not want to have the partitions or filesystem options as those do not apply to kafka. 
</span><span class="c1"></span><span class="w">    </span><span class="k">EXCLUDING</span><span class="w"> </span><span class="k">ALL</span><span class="w">
</span><span class="w">    </span><span class="k">INCLUDING</span><span class="w"> </span><span class="k">GENERATED</span><span class="w">
</span><span class="w"></span><span class="p">);</span><span class="w">
</span></code></pre></div><p>如果您没有提供同类选项，则默认使用 <code>INCLUDING ALL OVERWRITING OPTIONS</code>。</p>
<p>注意: 您无法控制合并物理字段的行为。这些字段将被合并，就像您应用 <strong>INCLUDING</strong> 策略一样。</p>
<h2 id="create-catalog">CREATE CATALOG</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">CATALOG</span><span class="w"> </span><span class="k">catalog_name</span><span class="w">
</span><span class="w">  </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="n">val1</span><span class="p">,</span><span class="w"> </span><span class="n">key2</span><span class="o">=</span><span class="n">val2</span><span class="p">,</span><span class="w"> </span><span class="p">...)</span><span class="w">
</span></code></pre></div><p>用给定的目录属性创建一个目录。如果已经存在同名的目录，则会产生异常。</p>
<p><strong>WITH OPTIONS</strong></p>
<p>目录属性，用于存储与本目录相关的额外信息。表达式 key1=val1 的键和值都应该是字符串文字。</p>
<p>更多详情请查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/catalogs.html">目录</a>。</p>
<h2 id="create-database">CREATE DATABASE</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">DATABASE</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.]</span><span class="n">db_name</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">database_comment</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="n">val1</span><span class="p">,</span><span class="w"> </span><span class="n">key2</span><span class="o">=</span><span class="n">val2</span><span class="p">,</span><span class="w"> </span><span class="p">...)</span><span class="w">
</span></code></pre></div><p>用给定的数据库属性创建一个数据库，如果目录中已经存在同名的数据库，则抛出异常。如果目录中已经存在相同名称的数据库，则会抛出异常。</p>
<p><strong>IF NOT EXISTS</strong></p>
<p>如果数据库已经存在，则不会发生任何事情。</p>
<p><strong>WITH OPTIONS</strong></p>
<p>数据库属性，用于存储与本数据库相关的额外信息。表达式 key1=val1 的键和值都应该是字符串文字。</p>
<h2 id="create-view">CREATE VIEW</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="p">[</span><span class="k">TEMPORARY</span><span class="p">]</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="n">view_name</span><span class="w">
</span><span class="w">  </span><span class="p">[</span><span class="err">{</span><span class="n">columnName</span><span class="w"> </span><span class="p">[,</span><span class="w"> </span><span class="n">columnName</span><span class="w"> </span><span class="p">]</span><span class="o">*</span><span class="w"> </span><span class="err">}</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">COMMENT</span><span class="w"> </span><span class="n">view_comment</span><span class="p">]</span><span class="w">
</span><span class="w">  </span><span class="k">AS</span><span class="w"> </span><span class="n">query_expression</span><span class="w">
</span></code></pre></div><p>用给定的查询表达式创建一个视图，如果目录中已经存在同名的视图，则抛出异常。如果目录中已经存在同名的视图，则会抛出异常。</p>
<p><strong>TEMPORARY</strong></p>
<p>创建具有目录和数据库命名空间并覆盖视图的临时视图。</p>
<p><strong>IF NOT EXISTS</strong></p>
<p>如果视图已经存在，则不会发生任何事情。</p>
<h2 id="create-function">CREATE FUNCTION</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="p">[</span><span class="k">TEMPORARY</span><span class="o">|</span><span class="k">TEMPORARY</span><span class="w"> </span><span class="k">SYSTEM</span><span class="p">]</span><span class="w"> </span><span class="k">FUNCTION</span><span class="w"> 
</span><span class="w">  </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="n">function_name</span><span class="w"> 
</span><span class="w">  </span><span class="k">AS</span><span class="w"> </span><span class="n">identifier</span><span class="w"> </span><span class="p">[</span><span class="k">LANGUAGE</span><span class="w"> </span><span class="n">JAVA</span><span class="o">|</span><span class="n">SCALA</span><span class="o">|</span><span class="n">PYTHON</span><span class="p">]</span><span class="w">
</span></code></pre></div><p>创建一个目录函数，该函数具有目录和数据库的名称空间，并带有标识符和可选的语言标签。如果目录中已经存在同名函数，则会抛出一个异常。</p>
<p>如果语言标签是 JAVA/SCALA，标识符是 UDF 的完整 classpath。关于 Java/Scala UDF 的实现，请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html">User-defined Functions</a> 了解详情。</p>
<p>如果语言标签是 PYTHON，标识符是 UDF 的完全限定名，例如 pyflink.table.test.test_udf.add。关于 Python UDF 的实现，更多细节请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/python/user-guide/table/udfs/python_udfs.html">Python UDFs</a>。</p>
<p><strong>TEMPORARY</strong></p>
<p>创建具有目录和数据库命名空间并覆盖目录功能的临时目录功能。</p>
<p><strong>TEMPORARY SYSTEM</strong></p>
<p>创建没有命名空间并覆盖内置函数的临时系统函数。</p>
<p><strong>IF NOT EXISTS</strong></p>
<p>如果函数已经存在，则不会发生任何事情。</p>
<p><strong>LANGUAGE JAVA|SCALA|PYTHON</strong></p>
<p>语言标签，用于指示 Flink 运行时如何执行函数。目前只支持 JAVA、SCALA 和 PYTHON，函数的默认语言是 JAVA。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/create.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/create.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Dataset 变换]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Dataset Transformations</blockquote><h1 id="dataset-转换">DataSet 转换</h1>
<p>本文档深入介绍了 DataSets 上可用的转换。关于 Flink Java API 的一般介绍，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">编程指南</a>。</p>
<p>对于密集索引的数据集中的压缩元素，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/zip_elements_guide.html">压缩元素指南</a>。</p>
<h2 id="map">Map</h2>
<p>Map 转换将用户定义的映射函数应用于 DataSet 的每个元素。它实现了一对一的映射，也就是说，函数必须准确地返回一个元素。</p>
<p>下面的代码将一个由整数对组成的 DataSet 转化为一个由整数组成的 DataSet。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">intPairs</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">intSums</span> <span class="k">=</span> <span class="n">intPairs</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">pair</span> <span class="k">=&gt;</span> <span class="n">pair</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="n">pair</span><span class="o">.</span><span class="n">_2</span> <span class="o">}</span>
</code></pre></div><h2 id="flatmap">FlatMap</h2>
<p>FlatMap 转换在 DataSet 的每个元素上应用了一个用户定义的 <code>flat-map</code> 函数。这种映射函数的变体可以为每个输入元素返回任意多个结果元素（包括没有）。</p>
<p>下面的代码将一个文本行的 DataSet 转换为一个单词的 DataSet。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">textLines</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">words</span> <span class="k">=</span> <span class="n">textLines</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34; &#34;</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><h2 id="mappartition">MapPartition</h2>
<p>MapPartition 在一次函数调用中转换一个并行分区。map-partition 函数以 Iterable 的形式获取分区，并可以产生任意数量的结果值。每个分区中元素的数量取决于平行度和之前的操作。</p>
<p>下面的代码将文本行的 DataSet 转换为每个分区的计数 DataSet。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">textLines</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// Some is required because the return value must be a Collection.
</span><span class="c1">// There is an implicit conversion from Option to a Collection.
</span><span class="c1"></span><span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">texLines</span><span class="o">.</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="n">in</span> <span class="k">=&gt;</span> <span class="nc">Some</span><span class="o">(</span><span class="n">in</span><span class="o">.</span><span class="n">size</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><h2 id="filter">Filter</h2>
<p>过滤器转换将用户定义的过滤器函数应用于 DataSet 的每个元素，并且只保留那些函数返回为真的元素。</p>
<p>以下代码从数据集中删除所有小于零的整数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">intNumbers</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">naturalNumbers</span> <span class="k">=</span> <span class="n">intNumbers</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">_</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">}</span>
</code></pre></div><p>重要：系统假设函数不会修改应用谓词的元素。违反这个假设会导致错误的结果。</p>
<h2 id="元组数据集的投影projection">元组数据集的投影(Projection)</h2>
<p><code>Project</code> 转换删除或移动 Tuple DataSet 的 Tuple 字段。<code>project(int...)</code> 方法通过其索引选择应该保留的 Tuple 字段，并定义它们在输出 Tuple 中的顺序。</p>
<p>投影(Projection)不需要定义用户函数。</p>
<p>下面的代码显示了在 DataSet 上应用 <code>Project</code> 转换的不同方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple3</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">in</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1">// converts Tuple3&lt;Integer, Double, String&gt; into Tuple2&lt;String, Integer&gt;
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="na">project</span><span class="o">(</span><span class="n">2</span><span class="o">,</span><span class="n">0</span><span class="o">);</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">#</span> <span class="n">scala</span>
<span class="nc">Not</span> <span class="n">supported</span><span class="o">.</span>
</code></pre></div><h3 id="分组数据集上的变换">分组数据集上的变换</h3>
<p><code>reduce</code> 操作可以对分组的数据集进行操作。指定用于分组的键可以通过多种方式进行。</p>
<ul>
<li>键表达式</li>
<li>键选择器函数</li>
<li>一个或多个字段位置键（仅限元组数据集）。</li>
<li>case 类字段(仅 case 类)</li>
</ul>
<p>请看一下 <code>reduce</code> 的例子，看看如何指定分组键。</p>
<h3 id="换算分组数据集">换算分组数据集</h3>
<p>应用于分组数据集的 <code>Reduce</code> 转换，使用用户定义的 <code>Reduce</code> 函数将每个分组换算为一个元素。对于每一组输入元素，一个 Reduce 函数将成对的元素连续组合成一个元素，直到每组只剩下一个元素。</p>
<p>请注意，对于一个 <code>ReduceFunction</code>，返回对象的键字段应该与输入值相匹配。这是因为 <code>reduce</code> 是隐式可组合的，当传递给 <code>reduce</code> 运算符时，从 <code>combine</code> 运算符发出的对象又是按键分组的。</p>
<h4 id="在按键表达式分组的数据集上进行-reduce-操作">在按键表达式分组的数据集上进行 Reduce 操作</h4>
<p>键表达式指定了 DataSet 中每个元素的一个或多个字段。每个键表达式都是一个公共字段的名称或一个 getter 方法。点号可以用来深入到对象中。键表达式 <code>&quot;*&quot;</code> 可以选择所有字段。下面的代码展示了如何使用键表达式对 POJO 数据集进行分组，并使用 <code>reduce</code> 函数对其进行换算。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO
</span><span class="c1"></span><span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="k">val</span> <span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="k">val</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">(</span><span class="kc">null</span><span class="o">,</span> <span class="o">-</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="c1">// [...]
</span><span class="c1"></span><span class="o">}</span>

<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">).</span><span class="n">reduce</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">w1</span><span class="o">,</span> <span class="n">w2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="k">new</span> <span class="nc">WC</span><span class="o">(</span><span class="n">w1</span><span class="o">.</span><span class="n">word</span><span class="o">,</span> <span class="n">w1</span><span class="o">.</span><span class="n">count</span> <span class="o">+</span> <span class="n">w2</span><span class="o">.</span><span class="n">count</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="对按键选择器分组的数据集进行换算">对按键选择器分组的数据集进行换算</h4>
<p>键选择器函数从数据集的每个元素中提取一个键值。提取的键值用于对 DataSet 进行分组。下面的代码展示了如何使用键选择器函数对 POJO 数据集进行分组，并使用 reduce 函数对其进行换算。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO
</span><span class="c1"></span><span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="k">val</span> <span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="k">val</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">(</span><span class="kc">null</span><span class="o">,</span> <span class="o">-</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="c1">// [...]
</span><span class="c1"></span><span class="o">}</span>

<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">word</span> <span class="o">}</span> <span class="n">reduce</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">w1</span><span class="o">,</span> <span class="n">w2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="k">new</span> <span class="nc">WC</span><span class="o">(</span><span class="n">w1</span><span class="o">.</span><span class="n">word</span><span class="o">,</span> <span class="n">w1</span><span class="o">.</span><span class="n">count</span> <span class="o">+</span> <span class="n">w2</span><span class="o">.</span><span class="n">count</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="对按字段位置键分组的数据集进行换算仅元组数据集">对按字段位置键分组的数据集进行换算（仅元组数据集）</h4>
<p>字段位置键指定了一个 Tuple DataSet 的一个或多个字段，这些字段被用作分组键。下面的代码显示了如何使用字段位置键和应用 reduce 函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">tuples</span> <span class="k">=</span> <span class="nc">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// group on the first and second Tuple field
</span><span class="c1"></span><span class="k">val</span> <span class="n">reducedTuples</span> <span class="k">=</span> <span class="n">tuples</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1</span><span class="o">).</span><span class="n">reduce</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h4 id="对按-case-类字段分组的数据集进行换算">对按 case 类字段分组的数据集进行换算</h4>
<p>当使用 Case Classes 时，你也可以使用字段的名称来指定分组键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">MyClass</span><span class="o">(</span><span class="k">val</span> <span class="n">a</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">c</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>
<span class="k">val</span> <span class="n">tuples</span> <span class="k">=</span> <span class="nc">DataSet</span><span class="o">[</span><span class="kt">MyClass</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// group on the first and second field
</span><span class="c1"></span><span class="k">val</span> <span class="n">reducedTuples</span> <span class="k">=</span> <span class="n">tuples</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="s">&#34;b&#34;</span><span class="o">).</span><span class="n">reduce</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h3 id="在分组数据集上进行分组换算">在分组数据集上进行分组换算</h3>
<p>应用在分组 DataSet 上的 GroupReduce 转换，会对每个组调用用户定义的 <code>group-reduce</code> 函数。这与 Reduce 之间的区别在于，用户定义的函数可以一次性获得整个组。该函数是在一个组的所有元素上用一个 <code>Iterable</code> 调用的，并且可以返回任意数量的结果元素。</p>
<h4 id="在按字段位置键分组的数据集上进行分组-reduce只适用于元组数据集">在按字段位置键分组的数据集上进行分组 Reduce(只适用于元组数据集)</h4>
<p>下面的代码显示了如何从一个按 Integer 分组的 DataSet 中删除重复的字符串。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">reduceGroup</span> <span class="o">{</span>
      <span class="o">(</span><span class="n">in</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)])</span> <span class="k">=&gt;</span>
        <span class="n">in</span><span class="o">.</span><span class="n">toSet</span> <span class="n">foreach</span> <span class="o">(</span><span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">)</span>
    <span class="o">}</span>
</code></pre></div><h4 id="对按键表达式键选择器函数或-case-类字段分组的数据集进行分组换算">对按键表达式、键选择器函数或 case 类字段分组的数据集进行分组换算</h4>
<p>类似于 Reduce 变换中的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-dataset-grouped-by-key-expression">键表达式</a>、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-dataset-grouped-by-keyselector-function">键选择器函数</a>和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-dataset-grouped-by-case-class-fields">case 类字段</a>的工作。</p>
<h4 id="对排序组进行-groupreduce">对排序组进行 GroupReduce</h4>
<p>一个 <code>group-reduce</code> 函数使用一个 Iterable 访问一个组的元素。可选地，Iterable 可以按照指定的顺序输出一个组的元素。在许多情况下，这有助于降低用户定义的 <code>group-reduce</code> 函数的复杂性，并提高其效率。</p>
<p>下面的代码显示了另一个例子，如何在一个由整数分组并按 String 排序的 DataSet 中删除重复的 String。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">sortGroup</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">reduceGroup</span> <span class="o">{</span>
      <span class="o">(</span><span class="n">in</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)])</span> <span class="k">=&gt;</span>
        <span class="k">var</span> <span class="n">prev</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=</span> <span class="kc">null</span>
        <span class="k">for</span> <span class="o">(</span><span class="n">t</span> <span class="k">&lt;-</span> <span class="n">in</span><span class="o">)</span> <span class="o">{</span>
          <span class="k">if</span> <span class="o">(</span><span class="n">prev</span> <span class="o">==</span> <span class="kc">null</span> <span class="o">||</span> <span class="n">prev</span> <span class="o">!=</span> <span class="n">t</span><span class="o">)</span>
            <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">t</span><span class="o">)</span>
            <span class="n">prev</span> <span class="k">=</span> <span class="n">t</span>
        <span class="o">}</span>
    <span class="o">}</span>
</code></pre></div><p>注意：如果在 <code>reduce</code> 操作之前，使用运算符的基于排序的执行策略建立了分组，那么 GroupSort 通常是免费的。</p>
<h4 id="可组合的-groupreducefunctions">可组合的 GroupReduceFunctions</h4>
<p>与 reduce 函数不同，<code>group-reduce</code> 函数是不可隐式组合的。为了使一个分组换算函数可以组合，它必须实现 <code>GroupCombineFunction</code> 接口。</p>
<p>重要：<code>GroupCombineFunction</code> 接口的通用输入和输出类型必须等于 <code>GroupReduceFunction</code> 的通用输入类型，如下例所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Combinable GroupReduceFunction that computes two sums.
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyCombinableGroupReducer</span>
  <span class="k">extends</span> <span class="nc">GroupReduceFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="kt">String</span><span class="o">]</span>
  <span class="k">with</span> <span class="nc">GroupCombineFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span>
<span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">reduce</span><span class="o">(</span>
    <span class="n">in</span><span class="k">:</span> <span class="kt">java.lang.Iterable</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)],</span>
    <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span>
  <span class="o">{</span>
    <span class="k">val</span> <span class="n">r</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span>
      <span class="n">in</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">asScala</span><span class="o">.</span><span class="n">reduce</span><span class="o">(</span> <span class="o">(</span><span class="n">a</span><span class="o">,</span><span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">a</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">a</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">)</span>
    <span class="c1">// concat key and sum and emit
</span><span class="c1"></span>    <span class="n">out</span><span class="o">.</span><span class="n">collect</span> <span class="o">(</span><span class="n">r</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="s">&#34;-&#34;</span> <span class="o">+</span> <span class="n">r</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">combine</span><span class="o">(</span>
    <span class="n">in</span><span class="k">:</span> <span class="kt">java.lang.Iterable</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)],</span>
    <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span>
  <span class="o">{</span>
    <span class="k">val</span> <span class="n">r</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span>
      <span class="n">in</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">asScala</span><span class="o">.</span><span class="n">reduce</span><span class="o">(</span> <span class="o">(</span><span class="n">a</span><span class="o">,</span><span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">a</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">a</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">)</span>
    <span class="c1">// emit tuple with key and sum
</span><span class="c1"></span>    <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">r</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="在分组数据集上进行分组合并">在分组数据集上进行分组合并</h3>
<p><code>GroupCombine</code> 变换是可组合的 <code>GroupReduceFunction</code> 中 <code>combine</code> 步骤的泛化形式。与此相反，<code>GroupReduce</code> 函数中的 <code>combine</code> 步骤只允许从输入类型 I 到输出类型 I 的组合。这是因为 <code>GroupReduce</code> 函数中的 <code>reduce</code> 步骤期望输入类型 I。</p>
<p>在某些应用中，希望在执行额外的转换（例如减少数据大小）之前，将一个数据集合并成中间格式。这可以通过一个 <code>CombineGroup</code> 转换来实现，而且成本很低。</p>
<p>注意：对分组数据集的 <code>GroupCombine</code> 是在内存中以贪婪的策略执行的，它可能不会一次处理所有数据，而是分多个步骤进行。它也是在各个分区上执行的，而不像 <code>GroupReduce</code> 变换那样进行数据交换。这可能会导致部分结果。</p>
<p>下面的例子演示了如何使用 <code>CombineGroup</code> 变换来实现另一种 <code>WordCount</code>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">[</span><span class="kt">..</span><span class="o">]</span> <span class="c1">// The words received as input
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">combinedWords</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="n">combineGroup</span> <span class="o">{</span>
    <span class="o">(</span><span class="n">words</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)])</span> <span class="k">=&gt;</span>
        <span class="k">var</span> <span class="n">key</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="kc">null</span>
        <span class="k">var</span> <span class="n">count</span> <span class="k">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="o">(</span><span class="n">word</span> <span class="k">&lt;-</span> <span class="n">words</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">key</span> <span class="k">=</span> <span class="n">word</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="o">}</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">key</span><span class="o">,</span> <span class="n">count</span><span class="o">))</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="n">combinedWords</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="n">reduceGroup</span> <span class="o">{</span>
    <span class="o">(</span><span class="n">words</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)])</span> <span class="k">=&gt;</span>
        <span class="k">var</span> <span class="n">key</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="kc">null</span>
        <span class="k">var</span> <span class="n">sum</span> <span class="k">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="o">((</span><span class="n">word</span><span class="o">,</span> <span class="n">sum</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">words</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">key</span> <span class="k">=</span> <span class="n">word</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">count</span>
        <span class="o">}</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">key</span><span class="o">,</span> <span class="n">sum</span><span class="o">))</span>
<span class="o">}</span>
</code></pre></div><p>上面的另一种 <code>WordCount</code> 实现演示了 <code>GroupCombine</code> 如何在执行 <code>GroupReduce</code> 转换之前组合单词。上面的例子只是一个概念证明。请注意，组合步骤如何改变 DataSet 的类型，通常在执行 <code>GroupReduce</code> 之前需要进行额外的 Map 转换。</p>
<h4 id="在分组元组数据集上进行聚合">在分组元组数据集上进行聚合</h4>
<p>有一些常用的聚合操作是经常使用的。Aggregate 转换提供了以下内置的聚合函数。</p>
<ul>
<li>Sum,</li>
<li>Min,</li>
<li>Max.</li>
</ul>
<p>Aggregate 变换只能应用在 Tuple 数据集上，并且只支持字段位置键进行分组。</p>
<p>下面的代码显示了如何在按字段位置键分组的数据集上应用&quot;聚合&quot;变换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">SUM</span><span class="o">,</span> <span class="mi">0</span><span class="o">).</span><span class="n">and</span><span class="o">(</span><span class="nc">MIN</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
</code></pre></div><p>要在一个 DataSet 上应用多个聚合，必须在第一个聚合之后使用 <code>.and()</code> 函数，也就是说 <code>.aggregary(SUM, 0).and(MIN, 2)</code> 会产生原始 DataSet 的字段 0 和字段 2 的最小值之和。与此相反，<code>.aggregary(SUM，0).aggregary(MIN，2)</code> 将在一个聚合上应用一个聚合。在给定的示例中，它将在计算字段 0 与字段 1 分组后产生字段 2 的最小值。</p>
<p>注意：聚合函数集将在未来得到扩展。</p>
<h4 id="对分组元组数据集的-minby--maxby-函数">对分组元组数据集的 MinBy / MaxBy 函数</h4>
<p><code>MinBy (MaxBy)</code> 转换为每组元组选择一个元组。被选择的元组是一个或多个指定字段的值是最小（最大）的元组。用于比较的字段必须是有效的关键字段，即可比较的字段。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。</p>
<p>下面的代码显示了如何从 <code>DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt;</code> 中选择具有相同 String 值的每组元组的 Integer 和 Double 字段最小值的元组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span>
                                   <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>  <span class="c1">// group DataSet on second field
</span><span class="c1"></span>                                   <span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span> <span class="c1">// select tuple with minimum values for first and third field.
</span></code></pre></div><h3 id="换算整个数据集">换算整个数据集</h3>
<p>Reduce 转换将用户定义的 <code>reduce</code> 函数应用于一个数据集的所有元素。随后，<code>reduce</code> 函数将元素对组合成一个元素，直到只剩下一个元素。</p>
<p>下面的代码显示了如何对一个整数数据集的所有元素进行求和。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">intNumbers</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">)</span>
<span class="k">val</span> <span class="n">sum</span> <span class="k">=</span> <span class="n">intNumbers</span><span class="o">.</span><span class="n">reduce</span> <span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">)</span>
</code></pre></div><p>使用 Reduce 转换换算一个完整的 DataSet 意味着最后的 Reduce 操作不能并行完成。然而，<code>reduce</code> 函数是可以自动组合的，因此 Reduce 转换不会限制大多数用例的可扩展性。</p>
<h3 id="对整个数据集进行分组换算">对整个数据集进行分组换算</h3>
<p><code>GroupReduce</code> 转换将用户定义的 <code>group-reduce</code> 函数应用于 DataSet 的所有元素。<code>group-reduce</code> 可以遍历 DataSet 的所有元素，并返回任意数量的结果元素。</p>
<p>下面的示例展示了如何在一个完整的 DataSet 上应用 <code>GroupReduce</code> 转换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyGroupReducer</span><span class="o">())</span>
</code></pre></div><p>注意：如果 <code>group-reduce</code> 函数不可组合，那么在一个完整的 DataSet 上的 <code>GroupReduce</code> 转换不能并行完成。因此，这可能是一个非常耗费计算的操作。请参阅上面的&quot;可组合的 GroupReduceFunctions&quot; 部分，了解如何实现可组合的 <code>group-reduce</code> 函数。</p>
<h3 id="在完整的数据集上进行分组合并groupcombine">在完整的数据集上进行分组合并(GroupCombine)</h3>
<p>在一个完整的 DataSet 上的 GroupCombine 的工作原理类似于在一个分组的 DataSet 上的 GroupCombine。在所有节点上对数据进行分区，然后以贪婪的方式进行合并（即只有适合内存的数据才会一次性合并）。</p>
<h3 id="在完整的-tuple-数据集上进行聚合">在完整的 Tuple 数据集上进行聚合</h3>
<p>有一些常用的聚合操作是经常使用的。Aggregate 转换提供了以下内置的聚合函数。</p>
<ul>
<li>Sum,</li>
<li>Min, 和</li>
<li>Max.</li>
</ul>
<p>Aggregate 变换只能应用于 Tuple 数据集。</p>
<p>下面的代码显示了如何在一个完整的数据集上应用聚合转换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">SUM</span><span class="o">,</span> <span class="mi">0</span><span class="o">).</span><span class="n">and</span><span class="o">(</span><span class="nc">MIN</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
</code></pre></div><p>注意：扩展支持的聚合函数集是我们的路线图。</p>
<h3 id="在完整的元组数据集上实现-minby--maxby">在完整的元组数据集上实现 MinBy / MaxBy</h3>
<p><code>MinBy (MaxBy)</code> 转换从一个元组数据集中选择一个元组。被选择的元组是一个或多个指定字段的值是最小（最大）的元组。用于比较的字段必须是有效的键字段，即可比较的字段。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。</p>
<p>以下代码显示了如何从 <code>DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt;</code> 中选择具有 Integer 和 Double 字段最大值的元组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span>                          
                                   <span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span> <span class="c1">// select tuple with maximum values for first and third field.
</span></code></pre></div><h3 id="distinct">Distinct</h3>
<p>Distinct 转换计算源 DataSet 中不同元素的 DataSet。下面的代码从 DataSet 中删除所有重复的元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">distinct</span><span class="o">()</span>
</code></pre></div><p>也可以使用以下方法改变 DataSet 中元素的区分方式。</p>
<ul>
<li>一个或多个字段位置键（仅元组数据集）。</li>
<li>一个键选择器函数，或</li>
<li>一个键表达式</li>
</ul>
<h4 id="用字段位置键去重distinct">用字段位置键去重(Distinct)</h4>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">distinct</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">2</span><span class="o">)</span>
</code></pre></div><h4 id="用-keyselector-函数去重distinct">用 KeySelector 函数去重(Distinct)</h4>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">distinct</span> <span class="o">{</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nc">Math</span><span class="o">.</span><span class="n">abs</span><span class="o">(</span><span class="n">x</span><span class="o">)}</span>
</code></pre></div><h4 id="用键表达式去重distinct">用键表达式去重(Distinct)</h4>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">CustomType</span><span class="o">(</span><span class="n">aName</span> <span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">aNumber</span> <span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span> <span class="o">}</span>

<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">CustomType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">distinct</span><span class="o">(</span><span class="s">&#34;aName&#34;</span><span class="o">,</span> <span class="s">&#34;aNumber&#34;</span><span class="o">)</span>
</code></pre></div><p>也可以用通配符表示使用所有字段:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO
</span><span class="c1"></span><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">CustomType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">distinct</span><span class="o">(</span><span class="s">&#34;_&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="join">Join</h3>
<p>Join 转换将两个 DataSets 连接成一个 DataSet。两个数据集的元素在一个或多个键上进行连接(join)，这些键可以通过使用</p>
<ul>
<li>键选择器函数</li>
<li>一个或多个字段位置键（仅限 Tuple DataSet）。</li>
<li>case 类字段</li>
</ul>
<p>有几种不同的方法来执行 Join 转换，如下所示。</p>
<h4 id="默认的-join-join-into-tuple2">默认的 Join (Join into Tuple2)</h4>
<p>默认的 Join 变换会产生一个新的 Tuple DataSet，它有两个字段。每个元组在第一个元组字段中持有第一个输入 DataSet 的 join 元素，在第二个字段中持有第二个输入 DataSet 的匹配元素。</p>
<p>下面的代码显示了一个使用字段位置键的默认 Join 转换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">input2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Double</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">input2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><h4 id="用-join-函数连接">用 Join 函数连接</h4>
<p>Join 转换也可以调用用户定义的 <code>join</code> 函数来处理连接(joining)元组。<code>join</code> 函数接收第一个输入 DataSet 的一个元素和第二个输入 DataSet 的一个元素，并准确返回一个元素。</p>
<p>下面的代码使用键选择器函数执行了一个带有自定义 java 对象的 DataSet 和一个 Tuple DataSet 的连接，并展示了如何使用用户定义的连接(join)函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">Rating</span><span class="o">(</span><span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">category</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">points</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>

<span class="k">val</span> <span class="n">ratings</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Ratings</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">weights</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">weightedRatings</span> <span class="k">=</span> <span class="n">ratings</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">weights</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;category&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">rating</span><span class="o">,</span> <span class="n">weight</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">rating</span><span class="o">.</span><span class="n">name</span><span class="o">,</span> <span class="n">rating</span><span class="o">.</span><span class="n">points</span> <span class="o">*</span> <span class="n">weight</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="用-flat-join-函数连接">用 Flat-Join 函数连接</h4>
<p>类似于 Map 和 FlatMap，<code>FlatJoin</code> 的行为方式与 Join 相同，但它不是返回一个元素，而是可以返回（收集）、零个、一个或多个元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">Rating</span><span class="o">(</span><span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">category</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">points</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>

<span class="k">val</span> <span class="n">ratings</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Ratings</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">weights</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">weightedRatings</span> <span class="k">=</span> <span class="n">ratings</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">weights</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;category&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">rating</span><span class="o">,</span> <span class="n">weight</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)])</span> <span class="k">=&gt;</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">weight</span><span class="o">.</span><span class="n">_2</span> <span class="o">&gt;</span> <span class="mf">0.1</span><span class="o">)</span> <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">rating</span><span class="o">.</span><span class="n">name</span><span class="o">,</span> <span class="n">rating</span><span class="o">.</span><span class="n">points</span> <span class="o">*</span> <span class="n">weight</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="用-projection-java-only-连接">用 Projection (Java Only) 连接</h4>
<p>Join 变换可以使用投影(projection)构造结果元组，如下所示:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple3</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Byte</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">input1</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;&gt;</span> <span class="n">input2</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple4</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">String</span><span class="o">,</span> <span class="n">Double</span><span class="o">,</span> <span class="n">Byte</span><span class="o">&gt;&gt;</span>
            <span class="n">result</span> <span class="o">=</span>
            <span class="n">input1</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">input2</span><span class="o">)</span>
                  <span class="c1">// key definition on first DataSet using a field position key
</span><span class="c1"></span>                  <span class="o">.</span><span class="na">where</span><span class="o">(</span><span class="n">0</span><span class="o">)</span>
                  <span class="c1">// key definition of second DataSet using a field position key
</span><span class="c1"></span>                  <span class="o">.</span><span class="na">equalTo</span><span class="o">(</span><span class="n">0</span><span class="o">)</span>
                  <span class="c1">// select and reorder fields of matching tuples
</span><span class="c1"></span>                  <span class="o">.</span><span class="na">projectFirst</span><span class="o">(</span><span class="n">0</span><span class="o">,</span><span class="n">2</span><span class="o">).</span><span class="na">projectSecond</span><span class="o">(</span><span class="n">1</span><span class="o">).</span><span class="na">projectFirst</span><span class="o">(</span><span class="n">1</span><span class="o">);</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// scala
</span><span class="c1"></span><span class="nc">Not</span> <span class="n">supported</span><span class="o">.</span>
</code></pre></div><h4 id="用数据集大小提示-join">用数据集大小提示 Join</h4>
<p>为了引导优化器选择正确的执行策略，你可以提示要连接(join)的 DataSet 的大小，如下所示:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">input2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// hint that the second DataSet is very small
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">joinWithTiny</span><span class="o">(</span><span class="n">input2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="c1">// hint that the second DataSet is very large
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">joinWithHuge</span><span class="o">(</span><span class="n">input2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
</code></pre></div><h4 id="join-算法提示">Join 算法提示</h4>
<p>Flink 运行时可以以各种方式执行连接(join)。每一种可能的方式在不同的情况下都会优于其他方式。系统会尝试自动选择一种合理的方式，但也允许你手动选择一种策略，以防你想强制执行特定的连接(join)方式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">SomeType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">input2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">AnotherType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// hint that the second DataSet is very small
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">input2</span><span class="o">,</span> <span class="nc">JoinHint</span><span class="o">.</span><span class="nc">BROADCAST_HASH_FIRST</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;id&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
</code></pre></div><p>有以下提示:</p>
<ul>
<li>
<p>OPTIMIZER_CHOOSES: 相当于完全不给提示，让系统来选择。</p>
</li>
<li>
<p>BROADCAST_HASH_FIRST：广播第一个输入，并据此建立一个哈希表，由第二个输入探测。如果第一个输入的数据非常小，这是一个很好的策略。</p>
</li>
<li>
<p>BROADCAST_HASH_SECOND: 广播第二个输入，并从中建立一个哈希表，由第一个输入探测。如果第二个输入非常小，是一个很好的策略。</p>
</li>
<li>
<p>REPARTITION_HASH_FIRST：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第一个输入建立一个哈希表。如果第一个输入比第二个输入小，但两个输入都很大，这个策略就很好。注意：如果无法估计大小，也无法重新使用已有的分区和排序，系统就会使用这个默认的后备策略。</p>
</li>
<li>
<p>REPARTITION_HASH_SECOND：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第二个输入建立一个哈希表。如果第二个输入比第一个输入小，但两个输入仍然很大，这个策略就很好。</p>
</li>
<li>
<p>REPARTITION_SORT_MERGE：系统对每个输入进行分区（洗牌）（除非输入已经分区），并对每个输入进行排序（除非已经排序）。通过对排序后的输入进行流式合并来连接(join)这些输入。如果一个或两个输入都已经被排序，这个策略就很好。</p>
</li>
</ul>
<h3 id="外连接">外连接</h3>
<p><code>OuterJoin</code> 转换在两个数据集上执行左、右或全外连接。外连接与常规（内连接）类似，创建所有键值相等的元素对。此外，如果在另一侧没有找到匹配的键，&ldquo;外侧&quot;的记录（左、右，或者在完全的情况下两者都有）将被保留。匹配的一对元素（或一个元素和另一个输入的空值）被交给 JoinFunction 将这对元素变成一个元素，或交给 FlatJoinFunction 将这对元素变成任意多个（包括无）元素。</p>
<p>两个 DataSets 的元素都是在一个或多个键上连接的，这些键可以通过使用</p>
<ul>
<li>键选择器函数</li>
<li>一个或多个字段位置键（仅限 Tuple DataSet）。</li>
<li>case 类字段</li>
</ul>
<p>OuterJoins 只支持 Java 和 Scala DataSet API。</p>
<h4 id="用-join-函数进行外连接">用 Join 函数进行外连接</h4>
<p><code>OuterJoin</code> 转换调用一个用户定义的 <code>join</code> 函数来处理连接元组。<code>join</code> 函数接收第一个输入 DataSet 的一个元素和第二个输入 DataSet 的一个元素，并准确地返回一个元素。根据外连接的类型（左、右、全），连接函数的两个输入元素中可以有一个是空的。</p>
<p>下面的代码使用键选择器函数执行 DataSet 与自定义 java 对象和 Tuple DataSet 的左外连接，并展示了如何使用用户定义的连接函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">Rating</span><span class="o">(</span><span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">category</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">points</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>

<span class="k">val</span> <span class="n">movies</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">ratings</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Ratings</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">moviesWithPoints</span> <span class="k">=</span> <span class="n">movies</span><span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span><span class="n">ratings</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;name&#34;</span><span class="o">)</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">movie</span><span class="o">,</span> <span class="n">rating</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">movie</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="k">if</span> <span class="o">(</span><span class="n">rating</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">rating</span><span class="o">.</span><span class="n">points</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="使用-flat-join-函数进行外连接">使用 Flat-Join 函数进行外连接</h4>
<p>类似于 Map 和 FlatMap，一个带有 <code>flat-join</code> 函数的 OuterJoin 的行为与带有 <code>join</code> 函数的 OuterJoin 相同，但它不是返回一个元素，而是可以返回（收集）、零个、一个或多个元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">Not</span> <span class="n">supported</span><span class="o">.</span>
</code></pre></div><h4 id="join-算法提示-1">Join 算法提示</h4>
<p>Flink 运行时可以以各种方式执行外连接。每一种可能的方式在不同的情况下都会优于其他方式。系统试图自动选择一种合理的方式，但允许你手动选择一种策略，以防你想强制执行特定的外连接方式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">SomeType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">input2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">AnotherType</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// hint that the second DataSet is very small
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span><span class="n">input2</span><span class="o">,</span> <span class="nc">JoinHint</span><span class="o">.</span><span class="nc">REPARTITION_SORT_MERGE</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;id&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">result2</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">rightOuterJoin</span><span class="o">(</span><span class="n">input2</span><span class="o">,</span> <span class="nc">JoinHint</span><span class="o">.</span><span class="nc">BROADCAST_HASH_FIRST</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;id&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
</code></pre></div><p>有以下提示:</p>
<ul>
<li>
<p>OPTIMIZER_CHOOSES: 相当于完全不给提示，让系统来选择。</p>
</li>
<li>
<p>BROADCAST_HASH_FIRST：广播第一个输入，并据此建立一个哈希表，由第二个输入探测。如果第一个输入的数据非常小，这是一个很好的策略。</p>
</li>
<li>
<p>BROADCAST_HASH_SECOND: 广播第二个输入，并从中建立一个哈希表，由第一个输入探测。如果第二个输入非常小，是一个很好的策略。</p>
</li>
<li>
<p>REPARTITION_HASH_FIRST：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第一个输入建立一个哈希表。如果第一个输入比第二个输入小，但两个输入仍然很大，这个策略就很好。</p>
</li>
<li>
<p>REPARTITION_HASH_SECOND：系统对每个输入进行分区（洗牌）（除非输入已经被分区），并从第二个输入建立一个哈希表。如果第二个输入比第一个输入小，但两个输入仍然很大，这个策略就很好。</p>
</li>
<li>
<p>REPARTITION_SORT_MERGE：系统对每个输入进行分区（洗牌）（除非输入已经分区），并对每个输入进行排序（除非已经排序）。通过对排序后的输入进行流式合并来连接(join)这些输入。如果一个或两个输入都已经被排序，这个策略就很好。</p>
</li>
</ul>
<p>注意：目前还不是所有的外连接类型都支持所有的执行策略。</p>
<ul>
<li>
<p>LeftOuterJoin 支持:</p>
<ul>
<li>OPTIMIZER_CHOOSES</li>
<li>BROADCAST_HASH_SECOND</li>
<li>REPARTITION_HASH_SECOND</li>
<li>REPARTITION_SORT_MERGE</li>
</ul>
</li>
<li>
<p>RightOuterJoin 支持:</p>
<ul>
<li>OPTIMIZER_CHOOSES</li>
<li>BROADCAST_HASH_FIRST</li>
<li>REPARTITION_HASH_FIRST</li>
<li>REPARTITION_SORT_MERGE</li>
</ul>
</li>
<li>
<p>FullOuterJoin 支持:</p>
<ul>
<li>OPTIMIZER_CHOOSES</li>
<li>REPARTITION_SORT_MERGE</li>
</ul>
</li>
</ul>
<h3 id="cross">Cross</h3>
<p>Cross 变换将两个 DataSets 组合成一个 DataSet。它建立了两个输入数据集元素的所有 pairwise 组合，即建立了一个笛卡尔积。Cross 变换要么在每对元素上调用用户定义的 <code>cross</code> 函数，要么输出一个 Tuple2。这两种模式如下所示。</p>
<p>注意：Cross 是一个潜在的计算密集型操作，甚至可以挑战大型计算集群。</p>
<h4 id="使用用户定义函数进行交叉运算">使用用户定义函数进行交叉运算</h4>
<p>Cross 变换可以调用一个用户定义的 <code>cross</code> 函数。<code>cross</code> 函数接收第一个输入的一个元素和第二个输入的一个元素，并正好返回一个结果元素。</p>
<p>下面的代码展示了如何使用 <code>cross</code> 函数对两个 DataSets 进行交叉变换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">Coord</span><span class="o">(</span><span class="n">id</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">x</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">y</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>

<span class="k">val</span> <span class="n">coords1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Coord</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">coords2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Coord</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">distances</span> <span class="k">=</span> <span class="n">coords1</span><span class="o">.</span><span class="n">cross</span><span class="o">(</span><span class="n">coords2</span><span class="o">)</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">c1</span><span class="o">,</span> <span class="n">c2</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">dist</span> <span class="k">=</span> <span class="n">sqrt</span><span class="o">(</span><span class="n">pow</span><span class="o">(</span><span class="n">c1</span><span class="o">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">c2</span><span class="o">.</span><span class="n">x</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span> <span class="o">+</span> <span class="n">pow</span><span class="o">(</span><span class="n">c1</span><span class="o">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">c2</span><span class="o">.</span><span class="n">y</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
    <span class="o">(</span><span class="n">c1</span><span class="o">.</span><span class="n">id</span><span class="o">,</span> <span class="n">c2</span><span class="o">.</span><span class="n">id</span><span class="o">,</span> <span class="n">dist</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><h4 id="用数据集大小提示交叉">用数据集大小提示交叉</h4>
<p>为了引导优化器选择正确的执行策略，你可以提示要交叉的 DataSet 的大小，如下所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">input2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// hint that the second DataSet is very small
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">crossWithTiny</span><span class="o">(</span><span class="n">input2</span><span class="o">)</span>

<span class="c1">// hint that the second DataSet is very large
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">crossWithHuge</span><span class="o">(</span><span class="n">input2</span><span class="o">)</span>
</code></pre></div><h3 id="cogroup">CoGroup</h3>
<p>CoGroup 转换联合(jointly)处理两个 DataSets 的组。两个 DataSets 根据定义的键进行分组，共享同一键的两个 DataSets 的组被一起交给用户定义的共组(co-group)函数。如果对于一个特定的键来说，只有一个 DataSet 有一个组，那么 <code>co-group</code> 函数就会和这个组以及一个空组一起被调用。共组(co-group)函数可以分别迭代两个组的元素，并返回任意数量的结果元素。</p>
<p>与 Reduce、GroupReduce 和 Join 类似，可以使用不同的键选择器方法来定义键。</p>
<h4 id="数据集上的-cogroup">数据集上的 CoGroup</h4>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">iVals</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">dVals</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="n">iVals</span><span class="o">.</span><span class="n">coGroup</span><span class="o">(</span><span class="n">dVals</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">iVals</span><span class="o">,</span> <span class="n">dVals</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">ints</span> <span class="k">=</span> <span class="n">iVals</span> <span class="n">map</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">_2</span> <span class="o">}</span> <span class="n">toSet</span>

    <span class="k">for</span> <span class="o">(</span><span class="n">dVal</span> <span class="k">&lt;-</span> <span class="n">dVals</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">for</span> <span class="o">(</span><span class="n">i</span> <span class="k">&lt;-</span> <span class="n">ints</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">dVal</span><span class="o">.</span><span class="n">_2</span> <span class="o">*</span> <span class="n">i</span><span class="o">)</span>
      <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="union">Union</h3>
<p>产生两个 DataSets 的联合(union)，这两个 DataSets 必须是同一类型。两个以上 DataSets 的联合(union)可以通过多个联合(union)调用来实现，如下所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">vals1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">vals2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">vals3</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">unioned</span> <span class="k">=</span> <span class="n">vals1</span><span class="o">.</span><span class="n">union</span><span class="o">(</span><span class="n">vals2</span><span class="o">).</span><span class="n">union</span><span class="o">(</span><span class="n">vals3</span><span class="o">)</span>
</code></pre></div><h3 id="rebalance">Rebalance</h3>
<p>均匀地重新平衡 DataSet 的并行分区，以消除数据倾斜。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// rebalance DataSet and apply a Map transformation.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">rebalance</span><span class="o">().</span><span class="n">map</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h3 id="hash-partition">Hash-Partition</h3>
<p>在给定的键上对 DataSet 进行散列分割。键可以被指定为位置键、表达式键和键选择器函数（关于如何指定键，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-grouped-dataset">Reduce 示例</a>）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// hash-partition DataSet by String value and apply a MapPartition transformation.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">partitionByHash</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h3 id="range-partition">Range-Partition</h3>
<p>在给定的键上 Range-partitions 一个 DataSet。键可以被指定为位置键、表达式键和键选择器函数（关于如何指定键，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-grouped-dataset">Reduce 示例</a>）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// range-partition DataSet by String value and apply a MapPartition transformation.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">partitionByRange</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h3 id="sort-partition">Sort Partition</h3>
<p>按照指定的顺序，在指定的字段上对 DataSet 的所有分区进行本地排序。字段可以被指定为字段表达式或字段位置（关于如何指定键，请参阅 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-grouped-dataset">Reduce 示例</a>）。通过链式 <code>sortPartition()</code> 调用，可以在多个字段上对分区进行排序。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// Locally sort partitions in ascending order on the second String field and
</span><span class="c1">// in descending order on the first String field.
</span><span class="c1">// Apply a MapPartition transformation on the sorted partitions.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">)</span>
            <span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">)</span>
            <span class="o">.</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><h3 id="first-n">First-n</h3>
<p>返回一个 DataSet 的前 n 个（任意）元素。First-n 可以应用于一个常规的 DataSet、一个分组的 DataSet 或一个分组排序的 DataSet。分组键可以被指定为键选择器函数或字段位置键（关于如何指定键，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#reduce-on-grouped-dataset">Reduce 示例</a>）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// Return the first five (arbitrary) elements of the DataSet
</span><span class="c1"></span><span class="k">val</span> <span class="n">out1</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">first</span><span class="o">(</span><span class="mi">5</span><span class="o">)</span>

<span class="c1">// Return the first two (arbitrary) elements of each String group
</span><span class="c1"></span><span class="k">val</span> <span class="n">out2</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">first</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>

<span class="c1">// Return the first three elements of each String group ordered by the Integer field
</span><span class="c1"></span><span class="k">val</span> <span class="n">out3</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">sortGroup</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Drop 语句]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Insert 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-sql-hints/?utm_source=atom_feed" rel="related" type="text/html" title="SQL 提示" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-show-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Show 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Drop Statements</blockquote><h1 id="drop-语句">DROP 语句</h1>
<p>DROP 语句用于从当前或指定的目录中删除一个注册的表/视图/函数。</p>
<p>Flink SQL 目前支持以下 DROP 语句。</p>
<ul>
<li>DROP TABLE</li>
<li>DROP DATABASE</li>
<li>DROP VIEW</li>
<li>DROP FUNCTION</li>
</ul>
<h2 id="运行一个-drop-语句">运行一个 DROP 语句</h2>
<p>DROP 语句可以用 TableEnvironment 的 executeSql()方法执行，也可以在 SQL CLI 中执行。executeSql()方法对于一个成功的 DROP 操作会返回&rsquo;OK'，否则会抛出一个异常。</p>
<p>下面的例子展示了如何在 TableEnvironment 和 SQL CLI 中运行 DROP 语句。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">settings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">()...</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">settings</span><span class="o">)</span>

<span class="c1">// register a table named &#34;Orders&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)&#34;</span><span class="o">)</span>

<span class="c1">// a string array: [&#34;Orders&#34;]
</span><span class="c1"></span><span class="k">val</span> <span class="n">tables</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">listTables</span><span class="o">()</span>
<span class="c1">// or tableEnv.executeSql(&#34;SHOW TABLES&#34;).print()
</span><span class="c1"></span>
<span class="c1">// drop &#34;Orders&#34; table from catalog
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;DROP TABLE Orders&#34;</span><span class="o">)</span>

<span class="c1">// an empty string array
</span><span class="c1"></span><span class="k">val</span> <span class="n">tables</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">listTables</span><span class="o">()</span>
<span class="c1">// or tableEnv.executeSql(&#34;SHOW TABLES&#34;).print()
</span></code></pre></div><h2 id="drop-table">DROP TABLE</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">DROP</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="k">table_name</span><span class="w">
</span></code></pre></div><p>删除一个给定表名的表。如果要删除的表不存在，则抛出一个异常。</p>
<p><strong>IF EXISTS</strong></p>
<p>如果该表不存在，就不会发生任何事情。</p>
<h2 id="drop-database">DROP DATABASE</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">DROP</span><span class="w"> </span><span class="k">DATABASE</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.]</span><span class="n">db_name</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="p">(</span><span class="k">RESTRICT</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="k">CASCADE</span><span class="p">)</span><span class="w"> </span><span class="p">]</span><span class="w">
</span></code></pre></div><p>删除一个给定数据库名称的数据库，如果要删除的数据库不存在，会产生异常。如果要删除的数据库不存在，则抛出一个异常。</p>
<p><strong>IF EXISTS</strong></p>
<p>如果数据库不存在，则不会发生任何事情。</p>
<p><strong>RESTRICT</strong></p>
<p>丢弃非空数据库会触发异常。默认为启用。</p>
<p><strong>CASCADE</strong></p>
<p>丢弃一个非空的数据库也会丢弃所有相关的表和函数。</p>
<h2 id="drop-view">DROP VIEW</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">DROP</span><span class="w"> </span><span class="p">[</span><span class="k">TEMPORARY</span><span class="p">]</span><span class="w"> </span><span class="k">VIEW</span><span class="w">  </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="n">view_name</span><span class="w">
</span></code></pre></div><p>丢弃一个有目录和数据库命名空间的视图。如果要删除的视图不存在，则会产生一个异常。</p>
<p><strong>TEMPORARY</strong></p>
<p>删除具有目录和数据库命名空间的临时视图。</p>
<p><strong>IF EXISTS</strong></p>
<p>如果视图不存在，则不会发生任何事情。</p>
<p>维护依赖关系 Flink 没有通过 CASCADE/RESTRICT 关键字来维护视图的依赖关系，当前的方式是当用户试图在诸如视图的底层表被删除的情况下使用视图时，会产生延迟错误消息。</p>
<h2 id="drop-function">DROP FUNCTION</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">DROP</span><span class="w"> </span><span class="p">[</span><span class="k">TEMPORARY</span><span class="o">|</span><span class="k">TEMPORARY</span><span class="w"> </span><span class="k">SYSTEM</span><span class="p">]</span><span class="w"> </span><span class="k">FUNCTION</span><span class="w"> </span><span class="p">[</span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="n">function_name</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>删除一个有目录和数据库命名空间的目录函数。如果要放弃的函数不存在，则会产生一个异常。</p>
<p><strong>TEMPORARY</strong></p>
<p>丢弃具有目录和数据库命名空间的临时目录功能。</p>
<p><strong>TEMPORARY SYSTEM</strong></p>
<p>删除没有命名空间的临时系统函数。</p>
<p><strong>IF EXISTS</strong></p>
<p>如果函数不存在，就不会发生任何事情。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/drop.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/drop.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/sql" term="sql" label="SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Explan 语句]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Insert 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-sql-hints/?utm_source=atom_feed" rel="related" type="text/html" title="SQL 提示" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-show-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Show 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Explan Statements</blockquote><h1 id="explain-语句">EXPLAIN 语句</h1>
<p>EXPLAIN 语句用于解释一个查询或 INSERT 语句的逻辑和优化查询计划。</p>
<h2 id="运行-explain-语句">运行 EXPLAIN 语句</h2>
<p>EXPLAIN 语句可以用 <code>TableEnvironment 的 executeSql()</code> 方法执行，也可以在 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html">SQL CLI</a> 中执行。<code>executeSql()</code> 方法在 EXPLAIN 操作成功后返回解释结果，否则将抛出一个异常。</p>
<p>下面的例子展示了如何在 TableEnvironment 和 SQL CLI 中运行 EXPLAIN 语句。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>
<span class="k">val</span> <span class="n">tEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// register a table named &#34;Orders&#34;
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE MyTable1 (count bigint, work VARCHAR(256) WITH (...)&#34;</span><span class="o">)</span>
<span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE MyTable2 (count bigint, work VARCHAR(256) WITH (...)&#34;</span><span class="o">)</span>

<span class="c1">// explain SELECT statement through TableEnvironment.explainSql()
</span><span class="c1"></span><span class="k">val</span> <span class="n">explanation</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">explainSql</span><span class="o">(</span>
  <span class="s">&#34;SELECT count, word FROM MyTable1 WHERE word LIKE &#39;F%&#39; &#34;</span> <span class="o">+</span>
  <span class="s">&#34;UNION ALL &#34;</span> <span class="o">+</span> 
  <span class="s">&#34;SELECT count, word FROM MyTable2&#34;</span><span class="o">)</span>
<span class="n">println</span><span class="o">(</span><span class="n">explanation</span><span class="o">)</span>

<span class="c1">// explain SELECT statement through TableEnvironment.executeSql()
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableResult</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span>
  <span class="s">&#34;EXPLAIN PLAN FOR &#34;</span> <span class="o">+</span> 
  <span class="s">&#34;SELECT count, word FROM MyTable1 WHERE word LIKE &#39;F%&#39; &#34;</span> <span class="o">+</span>
  <span class="s">&#34;UNION ALL &#34;</span> <span class="o">+</span> 
  <span class="s">&#34;SELECT count, word FROM MyTable2&#34;</span><span class="o">)</span>
<span class="n">tableResult</span><span class="o">.</span><span class="n">print</span><span class="o">()</span>
</code></pre></div><p>EXPLAIN 的结果是：</p>
<pre><code>== Abstract Syntax Tree ==
LogicalUnion(all=[true])
  LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')])
    FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word])
  FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word])
  

== Optimized Logical Plan ==
DataStreamUnion(all=[true], union all=[count, word])
  DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')])
    TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word])
  TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word])

== Physical Execution Plan ==
Stage 1 : Data Source
	content : collect elements with CollectionInputFormat

Stage 2 : Data Source
	content : collect elements with CollectionInputFormat

	Stage 3 : Operator
		content : from: (count, word)
		ship_strategy : REBALANCE

		Stage 4 : Operator
			content : where: (LIKE(word, _UTF-16LE'F%')), select: (count, word)
			ship_strategy : FORWARD

			Stage 5 : Operator
				content : from: (count, word)
				ship_strategy : REBALANCE
</code></pre><h2 id="语法">语法</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">EXPLAIN</span><span class="w"> </span><span class="n">PLAN</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="o">&lt;</span><span class="n">query_statement_or_insert_statement</span><span class="o">&gt;</span><span class="w">
</span></code></pre></div><p>关于查询语法，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#supported-syntax">查询</a>页面。关于 INSERT，请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/insert.html">INSERT</a> 页面。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/sql" term="sql" label="SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Flink Dataset API 编程指南]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Flink Dataset Api Programming Guide</blockquote><h2 id="flink-dataset-api-编程指南">Flink DataSet API 编程指南</h2>
<p>Flink 中的数据集程序是对数据集实现转换（如过滤、映射、加入、分组）的常规程序。数据集最初是从某些来源创建的（例如，通过读取文件，或从本地集合中创建）。结果通过汇返回，例如可以将数据写入（分布式）文件，或标准输出（例如命令行终端）。Flink 程序可以在各种环境下运行，独立运行，或者嵌入其他程序中。执行可以发生在本地 JVM 中，也可以发生在许多机器的集群中。</p>
<p>请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">DataStream API 概述</a>，了解 Flink API 的基本概念。该概述是针对 DataStream API 的，但这两个 API 的基本概念是一样的。</p>
<p>为了创建你自己的 Flink DataSet 程序，我们鼓励你从 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html#anatomy-of-a-flink-program">Flink 程序的骨架</a>开始，并逐步添加你自己的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#dataset-transformations">转换</a>。其余部分作为附加操作和高级功能的参考。</p>
<h3 id="程序示例">程序示例</h3>
<p>下面的程序是一个完整的、可以使用的 WordCount 的例子，你可以复制和粘贴代码在本地运行。你可以复制和粘贴代码在本地运行它。你只需要在你的项目中加入正确的 Flink 的库（参见与 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html">Flink 的链接</a>部分）并指定导入。然后你就可以开始了</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala._</span>

<span class="k">object</span> <span class="nc">WordCount</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
    <span class="k">val</span> <span class="n">text</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span>
      <span class="s">&#34;Who&#39;s there?&#34;</span><span class="o">,</span>
      <span class="s">&#34;I think I hear them. Stand, ho! Who&#39;s there?&#34;</span><span class="o">)</span>

    <span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">text</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">toLowerCase</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34;\\W+&#34;</span><span class="o">)</span> <span class="n">filter</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">nonEmpty</span> <span class="o">}</span> <span class="o">}</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">}</span>
      <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
      <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

    <span class="n">counts</span><span class="o">.</span><span class="n">print</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="dataset-转换">DataSet 转换</h3>
<p>数据转换将一个或多个 DataSet 转换为一个新的 DataSet。程序可以将多个转换组合成复杂的集合。</p>
<p>本节简要介绍了可用的转换。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html">转换文档</a>中有所有变换的完整描述和示例。</p>
<ul>
<li>Map</li>
</ul>
<p>接受一个元素，产生一个元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">toInt</span> <span class="o">}</span>
</code></pre></div><ul>
<li>FlatMap</li>
</ul>
<p>接受一个元素并产生零、一个或多个元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="n">str</span> <span class="k">=&gt;</span> <span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34; &#34;</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><ul>
<li>MapPartition</li>
</ul>
<p>在一个函数调用中转换一个并行分区。该函数以&quot;迭代器&quot;的形式获取分区，并可产生任意数量的结果值。每个分区的元素数量取决于平行度和之前的操作。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="n">in</span> <span class="k">=&gt;</span> <span class="n">in</span> <span class="n">map</span> <span class="o">{</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">}</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Filter</li>
</ul>
<p>对每个元素进行布尔函数评估，并保留那些函数返回真的元素。
重要：系统假设函数不会修改应用谓词的元素。违反这个假设会导致错误的结果。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">_</span> <span class="o">&gt;</span> <span class="mi">1000</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Reduce</li>
</ul>
<p>通过重复将两个元素合并为一个元素，将一组元素合并为一个元素。换算可以应用于一个完整的数据集，也可以应用于一个分组的数据集。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="k">_</span> <span class="o">+</span> <span class="k">_</span> <span class="o">}</span>
</code></pre></div><ul>
<li>ReduceGroup</li>
</ul>
<p>将一组元素合并成一个或多个元素。<code>ReduceGroup</code> 可以应用在一个完整的数据集上，也可以应用在一个分组的数据集上。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduceGroup</span> <span class="o">{</span> <span class="n">elements</span> <span class="k">=&gt;</span> <span class="n">elements</span><span class="o">.</span><span class="n">sum</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Aggregate</li>
</ul>
<p>将一组值聚合成一个值。<code>Aggregation</code> 函数可以被认为是内置的 <code>reduce</code> 函数。<code>Aggregate</code> 可以应用于一个完整的数据集，也可以应用于一个分组的数据集。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">SUM</span><span class="o">,</span> <span class="mi">0</span><span class="o">).</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">MIN</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
</code></pre></div><p>你也可以使用简写语法来进行 <code>minimum</code>, <code>maximum</code> 和 <code>sum</code> 的聚合。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">output</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">min</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
</code></pre></div><ul>
<li>Distinct</li>
</ul>
<p>返回数据集的不同元素。它从输入的 DataSet 中删除元素的所有字段或字段子集的重复条目。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">distinct</span><span class="o">()</span>
</code></pre></div><ul>
<li>Join</li>
</ul>
<p>通过创建所有键值相等的元素对来连接两个数据集。可以选择使用 <code>JoinFunction</code> 将一对元素变成一个元素，或者使用 <code>FlatJoinFunction</code> 将一对元素变成任意多个（包括无）元素。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#specifying-keys">键</a>部分了解如何定义连接键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// In this case tuple fields are used as keys. &#34;0&#34; is the join field on the first tuple
</span><span class="c1">// &#34;1&#34; is the join field on the second tuple.
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">input2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>你可以通过 Join Hints 指定运行时执行连接的方式。这些提示描述了连接是通过分区还是广播进行的，以及它是使用基于排序还是基于散列的算法。请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#join-algorithm-hints">转换指南</a>，了解可能的提示列表和示例。
如果没有指定提示，系统将尝试对输入大小进行估计，并根据这些估计选择最佳策略。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// This executes a join by broadcasting the first data set
</span><span class="c1">// using a hash table for the broadcast data
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">input2</span><span class="o">,</span> <span class="nc">JoinHint</span><span class="o">.</span><span class="nc">BROADCAST_HASH_FIRST</span><span class="o">)</span>
                   <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>请注意，连接转换只适用于等价连接。其他的连接类型需要使用 OuterJoin 或 CoGroup 来表达。</p>
<ul>
<li>OuterJoin</li>
</ul>
<p>在两个数据集上执行左联接、右联接或完全外联接。外联接与常规（内联接）类似，创建所有键值相同的元素对。此外，如果在另一侧没有找到匹配的键，&ldquo;外侧&quot;的记录（左、右或全联接时两者都有）将被保留。匹配的元素对（或一个元素和另一个输入的 <code>null</code> 值）被交给 <code>JoinFunction</code> 将这对元素变成单个元素，或交给 <code>FlatJoinFunction</code> 将这对元素变成任意多个（包括无）元素。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#specifying-keys">键</a>部分，了解如何定义连接键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">joined</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span><span class="n">right</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="o">{</span>
   <span class="o">(</span><span class="n">left</span><span class="o">,</span> <span class="n">right</span><span class="o">)</span> <span class="k">=&gt;</span>
     <span class="k">val</span> <span class="n">a</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">left</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="s">&#34;none&#34;</span> <span class="k">else</span> <span class="n">left</span><span class="o">.</span><span class="n">_1</span>
     <span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">right</span><span class="o">)</span>
  <span class="o">}</span>
</code></pre></div><ul>
<li>CoGroup</li>
</ul>
<p>减少操作的二维变体。在一个或多个字段上对每个输入进行分组，然后将分组合并。每一对组都会调用转换函数。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#specifying-keys">键</a>部分，了解如何定义 <code>coGroup</code> 键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data1</span><span class="o">.</span><span class="n">coGroup</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><ul>
<li>Cross</li>
</ul>
<p>建立两个输入的笛卡尔乘积（交叉乘积），创建所有元素对。可选择使用交叉函数将一对元素变成一个单一元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">data2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">data1</span><span class="o">.</span><span class="n">cross</span><span class="o">(</span><span class="n">data2</span><span class="o">)</span>
</code></pre></div><p>注意：<code>Cross</code> 可能是一个非常耗费计算的操作，甚至可以挑战大型计算集群！建议使用 <code>crossWithTiny()</code> 和 <code>crossWithHuge()</code> 来提示系统数据集的大小。</p>
<ul>
<li>
<p>Union</p>
</li>
<li>
<p>产生两个数据集的并集。</p>
</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-raku" data-lang="raku"><span class="n">data</span><span class="o">.</span><span class="nf">union</span><span class="p">(</span><span class="n">data2</span><span class="p">)</span>
</code></pre></div><ul>
<li>Rebalance</li>
</ul>
<p>均匀地重新平衡数据集的并行分区，以消除数据倾斜。只有类似于 Map 的变换才可以跟随重新平衡(rebalance)变换。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data1</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">data1</span><span class="o">.</span><span class="n">rebalance</span><span class="o">().</span><span class="n">map</span><span class="o">(...)</span>
</code></pre></div><ul>
<li>Hash-Partition</li>
</ul>
<p>在给定的键上对数据集进行散列分区。键可以被指定为位置键、表达式键和键选择函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">partitionByHash</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Range-Partition</li>
</ul>
<p>在给定的键上按照范围分割数据集。键可以被指定为位置键、表达式键和键选择函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">partitionByRange</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>自定义分区</li>
</ul>
<p>使用自定义的 <code>Partitioner</code> 函数，根据键将记录分配到特定的分区。键可以指定为位置键、表达式键和键选择函数。
注意：此方法仅适用于单个字段键。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span>
  <span class="o">.</span><span class="n">partitionCustom</span><span class="o">(</span><span class="n">partitioner</span><span class="o">,</span> <span class="n">key</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Sort Partition</li>
</ul>
<p>按照指定的顺序对数据集的所有分区进行本地排序。字段可以指定为元组位置或字段表达式。对多个字段的排序是通过链式 <code>sortPartition()</code> 调用完成的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">mapPartition</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>First-n</li>
</ul>
<p>返回一个数据集的前 n 个（任意）元素。First-n 可以应用于一个常规数据集、一个分组数据集或一个分组排序数据集。分组键可以指定为键选择函数、元组位置或 case 类字段。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// regular data set
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
<span class="c1">// grouped data set
</span><span class="c1"></span><span class="k">val</span> <span class="n">result2</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
<span class="c1">// grouped-sorted data set
</span><span class="c1"></span><span class="k">val</span> <span class="n">result3</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">sortGroup</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">first</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
</code></pre></div><p>以下转换可用于元组的数据集。</p>
<ul>
<li>MinBy / MaxBy</li>
</ul>
<p>从一组元组中选择一个元组，这些元组的一个或多个字段的值是最小的（最大的）。用于比较的字段必须是有效的键字段，即可比较。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。MinBy (MaxBy)可以应用于一个完整的数据集或一个分组数据集。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1">// a data set with a single tuple with minimum values for the Int and String fields.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
<span class="c1">// a data set with one tuple for each group with the minimum value for the Double field.
</span><span class="c1"></span><span class="k">val</span> <span class="n">out2</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Double</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
                                             <span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>通过匿名模式匹配从 tuple、case 类和集合中提取，比如下面。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">temperature</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="o">}</span>
</code></pre></div><p>不受 API 开箱即用的支持。要使用这个功能，你应该使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/scala_api_extensions.html">Scala API 扩展</a>。</p>
<p>变换的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/parallel.html">并行度</a>可以通过 <code>setParallelism(int)</code> 来定义，而 <code>name(String)</code> 可以给变换指定一个自定义的名称，这对调试很有帮助。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#data-sources">数据源</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#data-sinks">数据接收器</a>也是如此。</p>
<p><code>withParameters(Configuration)</code> 传递 Configuration 对象，这些对象可以从用户函数里面的 <code>open()</code> 方法访问。</p>
<h2 id="指定键">指定键</h2>
<p>一些转换（join、coGroup、groupBy）需要在元素集合上定义一个键。其他转换（Reduce、GroupReduce、Aggregate）允许在应用之前将数据按键分组。</p>
<p>一个 DataSet 被分组为：</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;...&gt;</span> <span class="n">input</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;...&gt;</span> <span class="n">reduced</span> <span class="o">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="cm">/*define key here*/</span><span class="o">)</span>
  <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="cm">/*do something*/</span><span class="o">);</span>
</code></pre></div><p>Flink 的数据模型不是基于键值对的。因此，你不需要将数据集类型物理地打包成键和值。键是&quot;虚拟的&rdquo;：它们被定义为实际数据上的函数，以指导分组操作符。</p>
<h3 id="为元组定义键">为元组定义键</h3>
<p>最简单的情况是对 Tuple 的一个或多个字段进行分组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">keyed</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
</code></pre></div><p>元组在第一个字段（整数类型的字段）上进行分组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">grouped</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div><p>在这里，我们将元组放在一个由第一个字段和第二个字段组成的复合键上。</p>
<p>关于嵌套 Tuple 的说明。如果你的 DataSet 有一个嵌套的元组，比如：</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple3</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Float</span><span class="o">&gt;,</span><span class="n">String</span><span class="o">,</span><span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">ds</span><span class="o">;</span>
</code></pre></div><p>指定 <code>groupBy(0)</code> 将使系统使用完整的 Tuple2 作为键（以 Integer 和 Float 为键）。如果要&quot;导航&quot;到嵌套的 Tuple2 中，就必须使用字段表达式键，下面将对其进行说明。</p>
<h3 id="使用字段表达式定义键">使用字段表达式定义键</h3>
<p>你可以使用基于字符串的字段表达式来引用嵌套的字段，并为分组、排序、连接(join)或 coGrouping 定义键。</p>
<p>字段表达式可以非常容易地选择（嵌套的）复合类型中的字段，如 Tuple 和 POJO 类型。</p>
<p>在下面的例子中，我们有一个有两个字段 &ldquo;word&rdquo; 和 &ldquo;count&rdquo; 的 WC POJO。要按字段 <code>word</code> 进行分组，我们只需将其名称传递给 <code>groupBy()</code> 函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary POJO (Plain old Java Object)
</span><span class="c1"></span><span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="k">var</span> <span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="k">var</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span> <span class="k">this</span><span class="o">(</span><span class="s">&#34;&#34;</span><span class="o">,</span> <span class="mi">0L</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">)</span>

<span class="c1">// or, as a case class, which is less typing
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="字段表达式语法">字段表达式语法</h3>
<ul>
<li>
<p>通过字段名选择 POJO 字段。例如 &ldquo;user&rdquo; 指的是 POJO 类型的 &ldquo;user&rdquo; 字段。</p>
</li>
<li>
<p>通过 1-offset 字段名或 0-offset 字段索引来选择 Tuple 字段。例如 &ldquo;_1&rdquo; 和 &ldquo;5&rdquo; 分别指 Scala Tuple 类型的第一和第六字段。</p>
</li>
</ul>
<p>你可以在 POJO 和 Tuple 中选择嵌套字段。例如 &ldquo;user.zip&rdquo; 指的是 POJO 的 &ldquo;zip&rdquo; 字段，它存储在 POJO 类型的 &ldquo;user&rdquo; 字段中。支持 POJO 和 Tuple 的任意嵌套和混合，如 &ldquo;_2.user.zip&rdquo; 或 &ldquo;user._4.1.zip&rdquo;。</p>
<p>你可以使用 &ldquo;_&rdquo; 通配符表达式选择完整的类型。这也适用于不是 Tuple 或 POJO 类型的类型。</p>
<p>字段表达式示例:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="k">var</span> <span class="n">complex</span><span class="k">:</span> <span class="kt">ComplexNestedClass</span><span class="o">,</span> <span class="k">var</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span> <span class="k">this</span><span class="o">(</span><span class="kc">null</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span>

<span class="k">class</span> <span class="nc">ComplexNestedClass</span><span class="o">(</span>
    <span class="k">var</span> <span class="n">someNumber</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>
    <span class="n">someFloat</span><span class="k">:</span> <span class="kt">Float</span><span class="o">,</span>
    <span class="n">word</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">,</span> <span class="nc">String</span><span class="o">),</span>
    <span class="n">hadoopCitizen</span><span class="k">:</span> <span class="kt">IntWritable</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">def</span> <span class="k">this</span><span class="o">()</span> <span class="o">{</span> <span class="k">this</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="s">&#34;&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">IntWritable</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span> <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>这些都是上面例子代码的有效字段表达式。</p>
<ul>
<li>
<p>&ldquo;count&rdquo;: WC 类中的计数字段</p>
</li>
<li>
<p>&ldquo;complex&rdquo;: 递归选择 POJO 类型 <code>ComplexNestedClass</code> 的 <code>complex</code> 字段的所有字段。</p>
</li>
<li>
<p>&ldquo;complex.word._3&rdquo;: 选择嵌套的 Tuple3 的最后一个字段。</p>
</li>
<li>
<p>&ldquo;complex.hadoopCitizen&rdquo;: 选择 Hadoop <code>IntWritable</code> 类型。</p>
</li>
</ul>
<h3 id="使用键选择函数定义键">使用键选择函数定义键</h3>
<p>另一种定义键的方法是&quot;键选择器&quot;函数。键选择器函数将一个元素作为输入，并返回该元素的键。键可以是任何类型的，并且可以从确定性计算中得到。</p>
<p>下面的例子显示了一个简单返回对象字段的键选择函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// some ordinary case class
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">keyed</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span> <span class="k">_</span><span class="o">.</span><span class="n">word</span> <span class="o">)</span>
</code></pre></div><h2 id="数据源">数据源</h2>
<p>数据源创建初始数据集，例如从文件或 Java 集合中创建。创建数据集的一般机制是在 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/io/InputFormat.java">InputFormat</a> 后面抽象出来的。Flink 自带了几种内置的格式来从常见的文件格式创建数据集。其中许多格式在 ExecutionEnvironment 上有快捷方法。</p>
<p>基于文件的:</p>
<ul>
<li>
<p><code>readTextFile(path) / TextInputFormat</code> - 读取文件并以字符串形式返回。</p>
</li>
<li>
<p><code>readTextFileWithValue(path) / TextValueInputFormat</code> - 以行的方式读取文件并以 <code>StringValues</code> 的形式返回。<code>StringValues</code> 是可变字符串。</p>
</li>
<li>
<p><code>readCsvFile(path) / CsvInputFormat</code> - 解析以逗号（或其他字符）分隔的文件。返回一个由 tuple、case 类对象或 POJOs 组成的 DataSet。支持基本的 java 类型及其对应的 Value 类型作为字段类型。</p>
</li>
<li>
<p><code>readFileOfPrimitives(path, delimiter) / PrimitiveInputFormat</code> - 使用给定的定界符，解析新行（或其他字符序列）定界的基元数据类型的文件，如 String 或 Integer。</p>
</li>
<li>
<p><code>readSequenceFile(Key, Value, path) / SequenceFileInputFormat</code> - 创建一个 JobConf 并从指定的路径读取文件，文件类型为 <code>SequenceFileInputFormat</code>，Key 类和 Value 类，并以 <code>Tuple2&lt;Key, Value&gt;</code> 的形式返回。</p>
</li>
</ul>
<p>基于集合的:</p>
<ul>
<li>
<p><code>fromCollection(Iterable)</code> - 从一个 Iterable 创建一个数据集。Iterable 返回的所有元素必须是相同的类型。</p>
</li>
<li>
<p><code>fromCollection(Iterator)</code> - 从一个 Iterator 创建一个数据集。该类指定了迭代器返回的元素的数据类型。</p>
</li>
<li>
<p><code>fromElements(elements: _*)</code> - 从给定的对象序列中创建一个数据集。所有对象必须是相同的类型。</p>
</li>
<li>
<p><code>fromParallelCollection(SplittableIterator)</code> - 从迭代器中并行创建一个数据集。该类指定了迭代器返回的元素的数据类型。</p>
</li>
<li>
<p><code>generateSequence(from, to)</code> - 在给定的区间内并行生成数字序列。</p>
</li>
</ul>
<p>通用的:</p>
<ul>
<li>
<p><code>readFile(inputFormat, path) / FileInputFormat</code> - 接受一个文件输入格式。</p>
</li>
<li>
<p><code>createInput(inputFormat) / InputFormat</code> - 接受一个通用的输入格式。</p>
</li>
</ul>
<p>示例:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span>  <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// read text file from local files system
</span><span class="c1"></span><span class="k">val</span> <span class="n">localLines</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;file:///path/to/my/textfile&#34;</span><span class="o">)</span>

<span class="c1">// read text file from an HDFS running at nnHost:nnPort
</span><span class="c1"></span><span class="k">val</span> <span class="n">hdfsLines</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;hdfs://nnHost:nnPort/path/to/my/textfile&#34;</span><span class="o">)</span>

<span class="c1">// read a CSV file with three fields
</span><span class="c1"></span><span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)](</span><span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">)</span>

<span class="c1">// read a CSV file with five fields, taking only two of them
</span><span class="c1"></span><span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)](</span>
  <span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">,</span>
  <span class="n">includedFields</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">3</span><span class="o">))</span> <span class="c1">// take the first and the fourth field
</span><span class="c1"></span>
<span class="c1">// CSV input can also be used with Case Classes
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">MyCaseClass</span><span class="o">(</span><span class="n">str</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">dbl</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>
<span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[</span><span class="kt">MyCaseClass</span><span class="o">](</span>
  <span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">,</span>
  <span class="n">includedFields</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">3</span><span class="o">))</span> <span class="c1">// take the first and the fourth field
</span><span class="c1"></span>
<span class="c1">// read a CSV file with three fields into a POJO (Person) with corresponding fields
</span><span class="c1"></span><span class="k">val</span> <span class="n">csvInput</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[</span><span class="kt">Person</span><span class="o">](</span>
  <span class="s">&#34;hdfs:///the/CSV/file&#34;</span><span class="o">,</span>
  <span class="n">pojoFields</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">&#34;name&#34;</span><span class="o">,</span> <span class="s">&#34;age&#34;</span><span class="o">,</span> <span class="s">&#34;zipcode&#34;</span><span class="o">))</span>

<span class="c1">// create a set from some given elements
</span><span class="c1"></span><span class="k">val</span> <span class="n">values</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="s">&#34;Foo&#34;</span><span class="o">,</span> <span class="s">&#34;bar&#34;</span><span class="o">,</span> <span class="s">&#34;foobar&#34;</span><span class="o">,</span> <span class="s">&#34;fubar&#34;</span><span class="o">)</span>

<span class="c1">// generate a number sequence
</span><span class="c1"></span><span class="k">val</span> <span class="n">numbers</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">generateSequence</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">10000000</span><span class="o">)</span>

<span class="c1">// read a file from the specified path of type SequenceFileInputFormat
</span><span class="c1"></span><span class="k">val</span> <span class="n">tuples</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">createInput</span><span class="o">(</span><span class="nc">HadoopInputs</span><span class="o">.</span><span class="n">readSequenceFile</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">IntWritable</span><span class="o">],</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">Text</span><span class="o">],</span>
 <span class="s">&#34;hdfs://nnHost:nnPort/path/to/file&#34;</span><span class="o">))</span>
</code></pre></div><h3 id="配置-csv-解析">配置 CSV 解析</h3>
<p>Flink 为 CSV 解析提供了许多配置选项。</p>
<ul>
<li>
<p>lineDelimiter: 字符串指定单个记录的定界符。默认的行定界符是新行字符 <code>'/n'</code>。</p>
</li>
<li>
<p>fieldDelimiter: 字符串指定分隔记录字段的定界符。默认的字段定界符是逗号字符 <code>','</code>。</p>
</li>
<li>
<p>includeFields: <code>Array[Int]</code> 定义从输入文件中读取哪些字段（以及忽略哪些字段）。默认情况下，前 n 个字段（由 <code>type()</code> 调用中的类型数定义）会被解析。</p>
</li>
<li>
<p>pojoFields: <code>Array[String]</code> 指定 POJO 的字段，这些字段被映射到 CSV 字段。CSV 字段的解析器会根据 POJO 字段的类型和顺序自动初始化。</p>
</li>
<li>
<p>parseQuotedStrings: 启用引号字符串解析的字符。如果字符串字段的第一个字符是引号字符，那么字符串将被解析为引号字符串（前导或尾部的空白不被修剪）。引号字符串中的字段定界符会被忽略。如果引号字符串字段的最后一个字符不是引号字符，则引号字符串解析失败。如果启用了引号字符串解析，且字段的第一个字符不是引号字符串，则该字符串将被解析为未引号字符串。默认情况下，引号字符串解析被禁用。</p>
</li>
<li>
<p>ignoreComments: 字符串指定一个注解前缀。所有以指定注解前缀开始的行都不会被解析和忽略。默认情况下，没有行被忽略。</p>
</li>
<li>
<p>lenient：布尔值，启用宽松解析。也就是说，不能正确解析的行会被忽略。默认情况下，禁用宽松解析，无效行会引发异常。</p>
</li>
<li>
<p>ignoreFirstLine: Boolean 配置 InputFormat 忽略输入文件的第一行。默认情况下，没有行被忽略。</p>
</li>
</ul>
<h3 id="input-path-的递归遍历">Input Path 的递归遍历</h3>
<p>对于基于文件的输入，当输入路径是一个目录时，默认情况下不会枚举嵌套文件。取而代之的是，只读取基础目录内的文件，而忽略嵌套文件。嵌套文件的递归枚举可以通过 <code>recursive.file.enumeration</code> 配置参数启用，就像下面的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// enable recursive enumeration of nested input files
</span><span class="c1"></span><span class="k">val</span> <span class="n">env</span>  <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// create a configuration object
</span><span class="c1"></span><span class="k">val</span> <span class="n">parameters</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span>

<span class="c1">// set the recursive enumeration parameter
</span><span class="c1"></span><span class="n">parameters</span><span class="o">.</span><span class="n">setBoolean</span><span class="o">(</span><span class="s">&#34;recursive.file.enumeration&#34;</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>

<span class="c1">// pass the configuration to the data source
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;file:///path/with.nested/files&#34;</span><span class="o">).</span><span class="n">withParameters</span><span class="o">(</span><span class="n">parameters</span><span class="o">)</span>
</code></pre></div><h3 id="读取压缩文件">读取压缩文件</h3>
<p>Flink 目前支持输入文件的透明解压，如果这些文件被标记为适当的文件扩展名。特别是，这意味着无需进一步配置输入格式，任何 <code>FileInputFormat</code> 都支持压缩，包括自定义输入格式。请注意，压缩文件可能不会被并行读取，从而影响作业的可扩展性。</p>
<p>下表列出了当前支持的压缩方法。</p>
<table>
<thead>
<tr>
<th style="text-align:left">压缩方法</th>
<th style="text-align:left">文件后缀</th>
<th style="text-align:left">并行性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">DEFLATE</td>
<td style="text-align:left">.deflate</td>
<td style="text-align:left">no</td>
</tr>
<tr>
<td style="text-align:left">GZip</td>
<td style="text-align:left">.gz, .gzip</td>
<td style="text-align:left">no</td>
</tr>
<tr>
<td style="text-align:left">Bzip2</td>
<td style="text-align:left">.bz2</td>
<td style="text-align:left">no</td>
</tr>
<tr>
<td style="text-align:left">XZ</td>
<td style="text-align:left">.xz</td>
<td style="text-align:left">no</td>
</tr>
</tbody>
</table>
<h2 id="数据接收器">数据接收器</h2>
<p>数据接收器消费 DataSet 并用于存储或返回它们。数据接收器的操作是用 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/io/OutputFormat.java">OutputFormat</a> 来描述的。Flink 带有各种内置的输出格式，这些格式被封装在对 DataSet 的操作后面。</p>
<ul>
<li><code>writeAsText() / TextOutputFormat</code> &ndash;将元素逐行写成 Strings。通过调用每个元素的 <code>toString()</code> 方法获得字符串。</li>
<li><code>writeAsCsv(...) / CsvOutputFormat</code> - 将元组写成逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的 <code>toString()</code> 方法。</li>
<li><code>print() / printToErr()</code> - 在标准输出/标准错误流上打印每个元素的 <code>toString()</code> 值。</li>
<li><code>write() / FileOutputFormat</code> - 用于自定义文件输出的方法和基类。支持自定义对象到字节的转换。</li>
<li><code>output()/ OutputFormat</code> - 最通用的输出方法，用于非基于文件的数据接收器（如将结果存储在数据库中）。</li>
</ul>
<p>一个 DataSet 可以被输入到多个操作中。程序可以写入或打印一个数据集，同时还可以对其进行额外的转换。</p>
<p><strong>示例</strong></p>
<p>标准数据接收器方法:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// text data
</span><span class="c1"></span><span class="k">val</span> <span class="n">textData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// write DataSet to a file on the local file system
</span><span class="c1"></span><span class="n">textData</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///my/result/on/localFS&#34;</span><span class="o">)</span>

<span class="c1">// write DataSet to a file on an HDFS with a namenode running at nnHost:nnPort
</span><span class="c1"></span><span class="n">textData</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;hdfs://nnHost:nnPort/my/result/on/localFS&#34;</span><span class="o">)</span>

<span class="c1">// write DataSet to a file and overwrite the file if it exists
</span><span class="c1"></span><span class="n">textData</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///my/result/on/localFS&#34;</span><span class="o">,</span> <span class="nc">WriteMode</span><span class="o">.</span><span class="nc">OVERWRITE</span><span class="o">)</span>

<span class="c1">// tuples as lines with pipe as the separator &#34;a|b|c&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">values</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">values</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="s">&#34;file:///path/to/the/result/file&#34;</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34;|&#34;</span><span class="o">)</span>

<span class="c1">// this writes tuples in the text formatting &#34;(a, b, c)&#34;, rather than as CSV lines
</span><span class="c1"></span><span class="n">values</span><span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///path/to/the/result/file&#34;</span><span class="o">)</span>

<span class="c1">// this writes values as strings using a user-defined formatting
</span><span class="c1"></span><span class="n">values</span> <span class="n">map</span> <span class="o">{</span> <span class="n">tuple</span> <span class="k">=&gt;</span> <span class="n">tuple</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="s">&#34; - &#34;</span> <span class="o">+</span> <span class="n">tuple</span><span class="o">.</span><span class="n">_2</span> <span class="o">}</span>
  <span class="o">.</span><span class="n">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///path/to/the/result/file&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="本地排序输出">本地排序输出</h3>
<p>数据接收器的输出可以使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-for-tuples">元组字段位置</a>或<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>对指定字段按指定顺序进行本地排序。这适用于每一种输出格式。</p>
<p>下面的示例展示了如何使用该功能。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">tData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">pData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">BookPojo</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">sData</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="c1">// sort output on String field in ascending order
</span><span class="c1"></span><span class="n">tData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>

<span class="c1">// sort output on Double field in descending and Int field in ascending order
</span><span class="c1"></span><span class="n">tData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">).</span><span class="n">sortPartition</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>

<span class="c1">// sort output on the &#34;author&#34; field of nested BookPojo in descending order
</span><span class="c1"></span><span class="n">pData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="s">&#34;_1.author&#34;</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">).</span><span class="n">writeAsText</span><span class="o">(...)</span>

<span class="c1">// sort output on the full tuple in ascending order
</span><span class="c1"></span><span class="n">tData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="s">&#34;_&#34;</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">).</span><span class="n">writeAsCsv</span><span class="o">(...)</span>

<span class="c1">// sort atomic type (String) output in descending order
</span><span class="c1"></span><span class="n">sData</span><span class="o">.</span><span class="n">sortPartition</span><span class="o">(</span><span class="s">&#34;_&#34;</span><span class="o">,</span> <span class="nc">Order</span><span class="o">.</span><span class="nc">DESCENDING</span><span class="o">).</span><span class="n">writeAsText</span><span class="o">(...)</span>
</code></pre></div><p>目前还不支持全局排序输出。</p>
<h2 id="迭代运算符">迭代运算符</h2>
<p>迭代在  Flink 程序中实现了循环。迭代运算符封装了程序的一部分，并反复执行，将一次迭代的结果（部分解）反馈到下一次迭代中。Flink 中的迭代有两种类型。<code>BulkIteration</code> 和 <code>DeltaIteration</code>。</p>
<p>本节提供了如何使用这两种运算符的快速示例。查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">迭代介绍页面</a>可以获得更详细的介绍。</p>
<h3 id="批量迭代">批量迭代</h3>
<p>要创建一个 <code>BulkIteration</code>，调用迭代开始的 DataSet 的 <code>iterate(int)</code> 方法，同时指定一个 <code>step</code> 函数。<code>step</code> 函数获取当前迭代的输入 DataSet，并且必须返回一个新的 DataSet。迭代调用的参数是最大的迭代次数，迭代过后要停止。</p>
<p>还有 <code>iterateWithTermination(int)</code> 函数，接受 <code>step</code> 函数，返回两个 DataSets。迭代步骤的结果和一个终止标准。一旦终止准则 DataSet 为空，就会停止迭代。</p>
<p>下面的例子是迭代估计数字 Pi。目标是计算随机点的数量，这些随机点落入单位圆中。在每一次迭代中，都会挑选一个随机点。如果这个点位于单位圆内，我们就递增计数。然后，Pi 的估计值是所得到的计数除以迭代次数乘以 4。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>

<span class="c1">// Create initial DataSet
</span><span class="c1"></span><span class="k">val</span> <span class="n">initial</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="n">count</span> <span class="k">=</span> <span class="n">initial</span><span class="o">.</span><span class="n">iterate</span><span class="o">(</span><span class="mi">10000</span><span class="o">)</span> <span class="o">{</span> <span class="n">iterationInput</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">iterationInput</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">i</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">x</span> <span class="k">=</span> <span class="nc">Math</span><span class="o">.</span><span class="n">random</span><span class="o">()</span>
    <span class="k">val</span> <span class="n">y</span> <span class="k">=</span> <span class="nc">Math</span><span class="o">.</span><span class="n">random</span><span class="o">()</span>
    <span class="n">i</span> <span class="o">+</span> <span class="o">(</span><span class="k">if</span> <span class="o">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="o">)</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="n">result</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">count</span> <span class="n">map</span> <span class="o">{</span> <span class="n">c</span> <span class="k">=&gt;</span> <span class="n">c</span> <span class="o">/</span> <span class="mf">10000.0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">}</span>

<span class="n">result</span><span class="o">.</span><span class="n">print</span><span class="o">()</span>

<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">(</span><span class="s">&#34;Iterative Pi Example&#34;</span><span class="o">)</span>
</code></pre></div><p>你也可以查看 <a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/clustering/KMeans.scala">K-Means</a> 的例子，它使用 <code>BulkIteration</code> 来聚类一组未标记的点。</p>
<h3 id="增量迭代">增量迭代</h3>
<p>增量迭代利用了某些算法在每次迭代中不改变解的每个数据点的事实。</p>
<p>除了在每次迭代中反馈的部分解（称为 workset），delta 迭代还保持着跨迭代的状态（称为解集），可以通过 delta 更新。迭代计算的结果是最后一次迭代后的状态。关于 delta 迭代的基本原理，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">迭代简介</a>。</p>
<p>定义 <code>DeltaIteration</code> 与定义 <code>BulkIteration</code> 类似。对于 delta 迭代，两个数据集构成了每次迭代的输入（工作集和解集），并且在每次迭代中产生两个数据集作为结果（新工作集，解集 delta）。</p>
<p>要创建一个 DeltaIteration 在初始解集上调用 <code>iterateDelta(initialWorkset，maxIterations，key)</code>。<code>step</code> 函数需要两个参数。(solutionSet, workset), 并且必须返回两个值: (solutionSetDelta, newWorkset).</p>
<p>下面是一个 delta 迭代语法的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// read the initial data sets
</span><span class="c1"></span><span class="k">val</span> <span class="n">initialSolutionSet</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">initialWorkset</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">maxIterations</span> <span class="k">=</span> <span class="mi">100</span>
<span class="k">val</span> <span class="n">keyPosition</span> <span class="k">=</span> <span class="mi">0</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">initialSolutionSet</span><span class="o">.</span><span class="n">iterateDelta</span><span class="o">(</span><span class="n">initialWorkset</span><span class="o">,</span> <span class="n">maxIterations</span><span class="o">,</span> <span class="nc">Array</span><span class="o">(</span><span class="n">keyPosition</span><span class="o">))</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">solution</span><span class="o">,</span> <span class="n">workset</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">candidateUpdates</span> <span class="k">=</span> <span class="n">workset</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="nc">ComputeCandidateChanges</span><span class="o">())</span>
    <span class="k">val</span> <span class="n">deltas</span> <span class="k">=</span> <span class="n">candidateUpdates</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">solution</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)(</span><span class="k">new</span> <span class="nc">CompareChangesToCurrent</span><span class="o">())</span>

    <span class="k">val</span> <span class="n">nextWorkset</span> <span class="k">=</span> <span class="n">deltas</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">new</span> <span class="nc">FilterByThreshold</span><span class="o">())</span>

    <span class="o">(</span><span class="n">deltas</span><span class="o">,</span> <span class="n">nextWorkset</span><span class="o">)</span>
<span class="o">}</span>

<span class="n">result</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">outputPath</span><span class="o">)</span>

<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><h2 id="在函数中对数据对象进行操作">在函数中对数据对象进行操作</h2>
<p>Flink 的运行时以 Java 对象的形式与用户函数交换数据。函数从运行时接收输入对象作为方法参数，并返回输出对象作为结果。因为这些对象是由用户函数和运行时代码访问的，所以理解和遵循用户代码如何访问，即读取和修改这些对象的规则是非常重要的。</p>
<p>用户函数以常规方法参数（如 MapFunction）或通过 Iterable 参数（如 GroupReduceFunction）从 Flink 的运行时接收对象。我们把运行时传递给用户函数的对象称为输入对象。用户函数可以将对象作为方法返回值（像 MapFunction）或通过 Collector（像 FlatMapFunction）发射给 Flink 运行时。我们将用户函数向运行时发射的对象称为输出对象。</p>
<p>Flink 的 DataSet API 具有两种模式，它们在 Flink 的运行时如何创建或重用输入对象方面有所不同。这种行为会影响用户函数如何与输入和输出对象交互的保证和约束。下面的章节定义了这些规则，并给出了编写安全用户函数代码的编码指南。</p>
<h3 id="禁用对象重用default">禁用对象重用(DEFAULT)</h3>
<p>默认情况下，Flink 在禁用对象重用模式下运行。这种模式可以保证函数在函数调用中总是接收新的输入对象。对象重用禁用模式能提供更好的保证，使用起来也更安全。但是，它有一定的处理开销，可能会引起较高的 Java 垃圾收集活动。下表解释了在禁用对象重用模式下，用户函数如何访问输入和输出对象。</p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">保证和限制</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">读取输入对象</td>
<td style="text-align:left">在一个方法调用中，保证输入对象的值不会改变。这包括由 Iterable 服务的对象。例如，在 List 或 Map 中收集由 Iterable 服务的输入对象是安全的。请注意，在方法调用离开后，对象可能会被修改。跨函数调用记忆对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">修改输入对象</td>
<td style="text-align:left">你可以修改输入对象。</td>
</tr>
<tr>
<td style="text-align:left">发射输入对象</td>
<td style="text-align:left">你可以发射输入对象。输入对象的值可能在发射后发生变化。读取发射后的输入对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">读取输出对象</td>
<td style="text-align:left">给予收集器的对象或作为方法结果返回的对象可能已经改变了其值。读取输出对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">修改输出对象</td>
<td style="text-align:left">你可以在对象被发射后对其进行修改，然后再次发射。</td>
</tr>
</tbody>
</table>
<p>对象重用禁用（默认）模式的编码准则。</p>
<ul>
<li>不要跨方法调用记忆和读取输入对象。</li>
<li>不要在发出对象后读取对象。</li>
</ul>
<h3 id="启用对象重用">启用对象重用</h3>
<p>在启用对象重用模式下，Flink 的运行时会尽量减少对象实例化的数量。这可以提高性能，并且可以减少 Java 垃圾收集的压力。通过调用 <code>ExecutionConfig.enableObjectReuse()</code> 激活对象重用启用模式。下表解释了在启用对象重用模式下，用户函数如何访问输入和输出对象。</p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">保证和限制</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">读取作为常规方法参数接收的输入对象</td>
<td style="text-align:left">作为常规方法参数接收的输入对象在一次函数调用中不被修改。对象可能在方法调用结束后被修改。跨函数调用记忆对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">读取从 Iterable 参数中接收到的输入对象</td>
<td style="text-align:left">从 Iterable 中接收到的输入对象只在调用 next()方法之前有效。一个 Iterable 或 Iterator 可以多次服务于同一个对象实例。记住从 Iterable 接收的输入对象是不安全的，例如，把它们放在 List 或 Map 中。</td>
</tr>
<tr>
<td style="text-align:left">修改输入对象</td>
<td style="text-align:left">除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(reuse)的输入对象外，你不得修改输入对象。</td>
</tr>
<tr>
<td style="text-align:left">发射输入对象</td>
<td style="text-align:left">除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(重用)的输入对象外，你不得发射输入对象。</td>
</tr>
<tr>
<td style="text-align:left">读取输出对象</td>
<td style="text-align:left">一个被交给 Collector 或作为方法结果返回的对象可能已经改变了它的值。读取输出对象是不安全的。</td>
</tr>
<tr>
<td style="text-align:left">修改输出对象</td>
<td style="text-align:left">你可以修改一个输出对象并再次发出它。</td>
</tr>
</tbody>
</table>
<p>启用对象重用的编码准则。</p>
<ul>
<li>不记忆从 Iterable 接收的输入对象。</li>
<li>不记忆和读取跨方法调用的输入对象。</li>
<li>除了 MapFunction、FlatMapFunction、MapPartitionFunction、GroupReduceFunction、GroupCombineFunction、CoGroupFunction 和 InputFormat.next(reuse)的输入对象外，不要修改或发出输入对象。</li>
<li>为了减少对象实例化，你总是可以发出一个专门的输出对象，这个对象被反复修改，但从不读取。</li>
</ul>
<h2 id="调试">调试</h2>
<p>在分布式集群中的大型数据集上运行数据分析程序之前，最好确保所实现的算法能够按照预期的方式运行。因此，实现数据分析程序通常是一个检查结果、调试和改进的渐进过程。</p>
<p>Flink 提供了一些不错的功能，通过支持 IDE 内的本地调试、注入测试数据和收集结果数据，大大简化了数据分析程序的开发过程。本节给大家一些提示，如何简化 Flink 程序的开发。</p>
<h3 id="本地执行环境">本地执行环境</h3>
<p>LocalEnvironment 在它创建的同一个 JVM 进程中启动 Flink 系统。如果你从 IDE 中启动 LocalEnvironment，你可以在代码中设置断点，轻松调试你的程序。</p>
<p>LocalEnvironment 的创建和使用方法如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">createLocalEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="n">pathToTextFile</span><span class="o">)</span>
<span class="c1">// build your program
</span><span class="c1"></span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><h3 id="收集数据源和接收器">收集数据源和接收器</h3>
<p>为分析程序提供输入并检查其输出，如果通过创建输入文件和读取输出文件来完成，是很麻烦的。Flink 具有特殊的数据源和接收器，这些数据源和接收器由 Java 集合支持，以方便测试。一旦程序经过测试，源和接收器可以很容易地被从 HDFS 等外部数据存储中读取/写入的源和接收器所替代。</p>
<p>集合数据源可以使用以下方式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">createLocalEnvironment</span><span class="o">()</span>

<span class="c1">// Create a DataSet from a list of elements
</span><span class="c1"></span><span class="k">val</span> <span class="n">myInts</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">)</span>

<span class="c1">// Create a DataSet from any Collection
</span><span class="c1"></span><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">myTuples</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="c1">// Create a DataSet from an Iterator
</span><span class="c1"></span><span class="k">val</span> <span class="n">longIt</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">myLongs</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="n">longIt</span><span class="o">)</span>
</code></pre></div><p>注：目前，集合数据源要求数据类型和迭代器实现 Serializable。此外，集合数据源不能并行执行（ parallelism = 1）。</p>
<h2 id="语义注解">语义注解</h2>
<p>语义注解可以用来给 Flink 提供关于函数行为的提示。它们告诉系统，函数读取并评估了函数输入的哪些字段，以及它将哪些字段从输入转发到输出，而没有进行修改。语义注解是加快执行速度的有力手段，因为它们允许系统推理出在多个操作中重复使用排序顺序或分区的问题。使用语义注解最终可能会使程序免于不必要的数据洗牌或不必要的排序，并显著提高程序的性能。</p>
<p>注意：语义注解的使用是可选的。然而，在提供语义注解时，保守地使用语义注解是绝对关键的! 不正确的语义注解将导致 Flink 对你的程序做出不正确的假设，并可能最终导致不正确的结果。如果一个操作符的行为不是明确可预测的，就不应该提供注解。请仔细阅读文档。</p>
<p>目前支持以下语义注解。</p>
<p><strong>转发字段注解</strong></p>
<p>转发字段信息声明了未被修改的输入字段被函数转发到输出中的同一位置或另一位置。该信息被优化器用来推断数据属性（如排序或分区）是否被函数保留。对于对输入元素组进行操作的函数，如 GroupReduce、GroupCombine、CoGroup 和 MapPartition，所有被定义为转发字段的字段必须总是从同一个输入元素联合转发。由组智函数发出的每个元素的转发字段可能来源于函数的输入组的不同元素。</p>
<p>字段转发信息使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>来指定。在输出中转发到同一位置的字段可以通过其位置来指定。指定的位置必须对输入和输出的数据类型有效，并具有相同的类型。例如字符串 &ldquo;f2 &ldquo;声明 Java 输入元组的第三个字段总是等于输出元组中的第三个字段。</p>
<p>将输入中的源字段和输出中的目标字段指定为字段表达式，就可以声明未修改的字段转发到输出中的另一个位置。字符串 <code>&quot;f0-&gt;f2&quot;</code> 表示将 Java 输入元组的第一个字段不变的复制到 Java 输出元组的第三个字段。通配符表达式 <code>*</code> 可以用来指代整个输入或输出类型，即 <code>&quot;f0-&gt;*&quot;</code> 表示一个函数的输出总是等于其 Java 输入元组的第一个字段。</p>
<p>多个转发字段可以在一个字符串中用分号隔开声明为 <code>&quot;f0; f2-&gt;f1; f3-&gt;f2&quot;</code>，也可以在单独的字符串中声明为 &ldquo;f0&rdquo;、&ldquo;f2-&gt;f1&rdquo;、&ldquo;f3-&gt;f2&rdquo;。当指定转发字段时，不要求所有的转发字段都声明，但所有的声明必须正确。</p>
<p>转发字段信息可以通过在函数类定义上附加 Java 注解来声明，或者在调用 DataSet 上的函数后将其作为操作符参数传递，如下图所示。</p>
<p><strong>函数类注解</strong></p>
<ul>
<li><code>@ForwardedFields</code> 用于单输入的函数，如 Map 和 Reduce。</li>
<li><code>@ForwardedFieldsFirst</code> 代表有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>@ForwardedFieldsSecond</code> 代表有两个输入的函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p><strong>操作符参数</strong></p>
<ul>
<li><code>data.map(myMapFnc).withForwardedFields()</code> 用于单输入的函数，如 Map 和 Reduce。</li>
<li><code>data1.join(data2).where().equalTo().with(myJoinFnc).withForwardFieldsFirst()</code> 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>data1.join(data2).where().equalTo().with(myJoinFnc).withForwardFieldsSecond()</code> 用于有两个输入的函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p>请注意，不可能覆盖通过操作符参数指定为类注解的字段前向信息。</p>
<p>例子：在函数的第二个输入端，如 Join 和 CoGroup，请注意不能覆盖通过运算符参数指定的类注解的字段前向信息。</p>
<p>下面的例子显示了如何使用函数类注解来声明转发的字段信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nd">@ForwardedFields</span><span class="o">(</span><span class="s">&#34;_1-&gt;_3&#34;</span><span class="o">)</span>
<span class="k">class</span> <span class="nc">MyMap</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]{</span>
   <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">))</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">,</span> <span class="nc">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">return</span> <span class="o">(</span><span class="s">&#34;foo&#34;</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span> <span class="o">/</span> <span class="mi">2</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>非转发字段</strong></p>
<p>非转发字段信息声明了所有在函数输出中不保留在同一位置的字段。所有其他字段的值都被认为保留在输出的同一位置。因此，非转发字段信息与转发字段信息是相反的。分组运算符（如 GroupReduce、GroupCombine、CoGroup 和 MapPartition）的非转发字段信息必须满足与转发字段信息相同的要求。</p>
<p>重要：非转发字段信息的规范是可选的。但是如果使用，必须指定 <strong>ALL!</strong> 非转发字段，因为所有其他字段都被认为是原地转发的。将一个转发字段声明为非转发字段是安全的。</p>
<p>非转发字段被指定为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>的列表。这个列表既可以是由分号分隔的字段表达式组成的单个字符串，也可以是多个字符串。例如 &ldquo;f1; f3&rdquo; 和 &ldquo;f1&rdquo;、&ldquo;f3&rdquo; 都声明 Java 元组的第二个和第四个字段不保留在原地，其他所有字段都保留在原地。非前向字段信息只能为输入和输出类型相同的函数指定。</p>
<p>非转发字段信息是作为函数类注解使用以下注解来指定的。</p>
<ul>
<li><code>@NonForwardedFields</code> 用于单个输入函数，如 Map 和 Reduce。</li>
<li><code>@NonForwardedFieldsFirst</code> 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>@NonForwardedFieldsSecond</code> 用于函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p>例子</p>
<p>下面的例子显示了如何声明非转发字段信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nd">@NonForwardedFields</span><span class="o">(</span><span class="s">&#34;_2&#34;</span><span class="o">)</span> <span class="c1">// second field is not forwarded
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyMap</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]{</span>
  <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">))</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">return</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span> <span class="o">/</span> <span class="mi">2</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>读取字段</strong></p>
<p>读取字段信息声明了所有被函数访问和评估的字段，也就是说，所有被函数用来计算结果的字段。例如，在条件语句中被评估的字段或用于计算的字段必须在指定读取字段信息时被标记为读取。仅仅是未经修改就转发到输出而不评估其值的字段，或者根本没有被访问的字段都不被认为是读。</p>
<p>重要：读取字段信息的指定是可选的。但是如果使用，必须指定 <strong>ALL!</strong> 读取字段。将一个非读字段声明为读字段是安全的。</p>
<p>读取字段被指定为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/#define-keys-using-field-expressions">字段表达式</a>的列表。这个列表可以是一个由分号分隔的字段表达式组成的单个字符串，也可以是多个字符串。例如 &ldquo;f1; f3&rdquo; 和 &ldquo;f1&rdquo;、&ldquo;f3&rdquo; 都声明 Java 元组的第二和第四字段被函数读取和评估。</p>
<p>读取字段信息是以函数类注解的形式指定的，使用以下注解。</p>
<ul>
<li><code>@ReadFields</code> 用于单输入函数，如 Map 和 Reduce。</li>
<li><code>@ReadFieldsFirst</code> 用于有两个输入的函数的第一个输入，如 Join 和 CoGroup。</li>
<li><code>@ReadFieldsSecond</code> 用于有两个输入的函数的第二个输入，如 Join 和 CoGroup。</li>
</ul>
<p>示例：</p>
<p>下面的例子显示了如何声明读取字段信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nd">@ReadFields</span><span class="o">(</span><span class="s">&#34;_1; _4&#34;</span><span class="o">)</span> <span class="c1">// _1 and _4 are read and evaluated by the function.
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyMap</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]{</span>
   <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">,</span> <span class="nc">Int</span><span class="o">,</span> <span class="nc">Int</span><span class="o">))</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_1</span> <span class="o">==</span> <span class="mi">42</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">return</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="k">return</span> <span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_4</span> <span class="o">+</span> <span class="mi">10</span><span class="o">,</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h2 id="广播变量">广播变量</h2>
<p>广播变量允许你在操作的常规输入之外，将一个数据集提供给操作的所有并行实例。这对辅助数据集或数据依赖性参数化很有用。然后，该数据集将作为一个集合在操作者处被访问。</p>
<ul>
<li>广播：广播集通过 <code>withBroadcastSet(DataSet，String)</code> 按名称注册，并通过</li>
<li>访问方式：通过目标操作者处的 <code>getRuntimeContext().getBroadcastVariable(String)</code> 访问。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// 1. The DataSet to be broadcast
</span><span class="c1"></span><span class="k">val</span> <span class="n">toBroadcast</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>

<span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="s">&#34;b&#34;</span><span class="o">)</span>

<span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">new</span> <span class="nc">RichMapFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]()</span> <span class="o">{</span>
    <span class="k">var</span> <span class="n">broadcastSet</span><span class="k">:</span> <span class="kt">Traversable</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">config</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="c1">// 3. Access the broadcast DataSet as a Collection
</span><span class="c1"></span>      <span class="n">broadcastSet</span> <span class="k">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="n">getBroadcastVariable</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;broadcastSetName&#34;</span><span class="o">).</span><span class="n">asScala</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">in</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">{</span>
        <span class="o">...</span>
    <span class="o">}</span>
<span class="o">}).</span><span class="n">withBroadcastSet</span><span class="o">(</span><span class="n">toBroadcast</span><span class="o">,</span> <span class="s">&#34;broadcastSetName&#34;</span><span class="o">)</span> <span class="c1">// 2. Broadcast the DataSet
</span></code></pre></div><p>在注册和访问广播数据集时，确保名称（前面例子中的 <code>broadcastSetName</code>）匹配。关于完整的示例程序，可以看一下 <a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/clustering/KMeans.scala">KMeans 算法</a>。</p>
<p>注意：由于广播变量的内容在每个节点上都保存在内存中，所以它不应该变得太大。对于像标量值这样简单的东西，你可以简单地将参数作为函数闭包的一部分，或者使用 <code>withParameters(...)</code> 方法来传递配置。</p>
<h2 id="分布式缓存">分布式缓存</h2>
<p>Flink 提供了一个类似于 Apache Hadoop 的分布式缓存，以使用户函数的并行实例可以在本地访问文件。该功能可用于共享包含静态外部数据的文件，如字典或机器学习的回归模型。</p>
<p>缓存的工作原理如下。程序在其 <code>ExecutionEnvironment</code> 中以特定的名称将<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/connectors.html#reading-from-file-systems">本地或远程文件系统（如 HDFS 或 S3）</a>的文件或目录注册为缓存文件。当程序执行时，Flink 会自动将该文件或目录复制到所有工作者的本地文件系统中。用户函数可以查找指定名称下的文件或目录，并从工作者的本地文件系统中访问它。</p>
<p>分布式缓存的使用方法如下。</p>
<p>在 <code>ExecutionEnvironment</code> 中注册文件或目录。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// register a file from HDFS
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">registerCachedFile</span><span class="o">(</span><span class="s">&#34;hdfs:///path/to/your/file&#34;</span><span class="o">,</span> <span class="s">&#34;hdfsFile&#34;</span><span class="o">)</span>

<span class="c1">// register a local executable file (script, executable, ...)
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">registerCachedFile</span><span class="o">(</span><span class="s">&#34;file:///path/to/exec/file&#34;</span><span class="o">,</span> <span class="s">&#34;localExecFile&#34;</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>

<span class="c1">// define your program and execute
</span><span class="c1"></span><span class="o">...</span>
<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyMapper</span><span class="o">())</span>
<span class="o">...</span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><p>在一个用户函数（这里是 MapFunction）中访问缓存文件。该函数必须扩展一个 RichFunction 类，因为它需要访问 RuntimeContext。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// extend a RichFunction to have access to the RuntimeContext
</span><span class="c1"></span><span class="k">class</span> <span class="nc">MyMapper</span> <span class="k">extends</span> <span class="nc">RichMapFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">config</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>

    <span class="c1">// access cached file via RuntimeContext and DistributedCache
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">myFile</span><span class="k">:</span> <span class="kt">File</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">.</span><span class="n">getDistributedCache</span><span class="o">.</span><span class="n">getFile</span><span class="o">(</span><span class="s">&#34;hdfsFile&#34;</span><span class="o">)</span>
    <span class="c1">// read the file (or navigate the directory)
</span><span class="c1"></span>    <span class="o">...</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// use content of cached file
</span><span class="c1"></span>    <span class="o">...</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h2 id="向函数传递参数">向函数传递参数</h2>
<p>可以使用构造函数或 <code>withParameters(Configuration)</code> 方法将参数传递给函数。参数会被序列化为函数对象的一部分，并传送给所有并行任务实例。</p>
<p><strong>通过构造函数</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">toFilter</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>

<span class="n">toFilter</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyFilter</span><span class="o">(</span><span class="mi">2</span><span class="o">))</span>

<span class="k">class</span> <span class="nc">MyFilter</span><span class="o">(</span><span class="n">limit</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">FilterFunction</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">filter</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">value</span> <span class="o">&gt;</span> <span class="n">limit</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p><strong>通过 withParameters(配置)</strong></p>
<p>本方法以一个 Configuration 对象作为参数，它将被传递给<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html#rich-functions">富函数</a>的 <code>open()</code> 方法。配置对象是一个从 String 键到不同值类型的 Map。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">toFilter</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>

<span class="k">val</span> <span class="n">c</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">()</span>
<span class="n">c</span><span class="o">.</span><span class="n">setInteger</span><span class="o">(</span><span class="s">&#34;limit&#34;</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>

<span class="n">toFilter</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">new</span> <span class="nc">RichFilterFunction</span><span class="o">[</span><span class="kt">Int</span><span class="o">]()</span> <span class="o">{</span>
    <span class="k">var</span> <span class="n">limit</span> <span class="k">=</span> <span class="mi">0</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">config</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="n">limit</span> <span class="k">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getInteger</span><span class="o">(</span><span class="s">&#34;limit&#34;</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">filter</span><span class="o">(</span><span class="n">in</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">in</span> <span class="o">&gt;</span> <span class="n">limit</span>
    <span class="o">}</span>
<span class="o">}).</span><span class="n">withParameters</span><span class="o">(</span><span class="n">c</span><span class="o">)</span>
</code></pre></div><p><strong>在全局范围内通过 ExecutionConfig</strong></p>
<p>Flink 还允许将自定义配置值传递到环境的 ExecutionConfig 接口。由于执行配置可以在所有（丰富的）用户函数中访问，因此自定义配置将在所有函数中全局可用。</p>
<p>设置一个自定义的全局配置：</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">()</span>
<span class="n">conf</span><span class="o">.</span><span class="n">setString</span><span class="o">(</span><span class="s">&#34;mykey&#34;</span><span class="o">,</span> <span class="s">&#34;myvalue&#34;</span><span class="o">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">getConfig</span><span class="o">.</span><span class="n">setGlobalJobParameters</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span>
</code></pre></div><p>请注意，你也可以传递一个扩展 <code>ExecutionConfig.GlobalJobParameters</code> 类的自定义类作为全局作业参数给执行配置。该接口允许实现 <code>Map&lt;String, String&gt; toMap()</code> 方法，该方法将在 web 前端显示来自配置的值。</p>
<p><strong>从全局配置中访问值</strong></p>
<p>全局工作参数中的对象在系统中的很多地方都可以访问。所有实现 RichFunction 接口的用户函数都可以通过运行时上下文访问。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kd">class</span> <span class="nc">Tokenizer</span> <span class="kd">extends</span> <span class="n">RichFlatMapFunction</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="o">{</span>

    <span class="kd">private</span> <span class="n">String</span> <span class="n">mykey</span><span class="o">;</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">open</span><span class="o">(</span><span class="n">Configuration</span> <span class="n">parameters</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
      <span class="kd">super</span><span class="o">.</span><span class="na">open</span><span class="o">(</span><span class="n">parameters</span><span class="o">);</span>
      <span class="n">ExecutionConfig</span><span class="o">.</span><span class="na">GlobalJobParameters</span> <span class="n">globalParams</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getExecutionConfig</span><span class="o">().</span><span class="na">getGlobalJobParameters</span><span class="o">();</span>
      <span class="n">Configuration</span> <span class="n">globConf</span> <span class="o">=</span> <span class="o">(</span><span class="n">Configuration</span><span class="o">)</span> <span class="n">globalParams</span><span class="o">;</span>
      <span class="n">mykey</span> <span class="o">=</span> <span class="n">globConf</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="s">&#34;mykey&#34;</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>
    <span class="o">}</span>
    <span class="c1">// ... more here ...
</span></code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/guide" term="guide" label="Guide" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Hadoop 的兼容性]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Hadoop Compatibility Beta</blockquote><h2 id="hadoop-兼容性测试版">Hadoop 兼容性测试版</h2>
<p>Flink 与 Apache Hadoop MapReduce 接口兼容，因此允许重用为 Hadoop MapReduce 实现的代码。</p>
<p>您可以:</p>
<ul>
<li>在 Flink 程序中使用 Hadoop 的可写<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html#supported-data-types">数据类型</a>。</li>
<li>使用任何 Hadoop InputFormat 作为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#data-sources">数据源</a>。</li>
<li>使用任何 Hadoop 输出格式作为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#data-sinks">数据接收器</a>。</li>
<li>将 Hadoop Mapper 用作 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#flatmap">FlatMapFunction</a>。</li>
<li>使用 Hadoop Reducer 作为 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#groupreduce-on-grouped-dataset">GroupReduceFunction</a>。</li>
</ul>
<p>本文档展示了如何将现有的 Hadoop MapReduce 代码与 Flink 一起使用。从 Hadoop 支持的文件系统读取代码，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/filesystems/index.html#hadoop-file-system-hdfs-and-its-other-implementations">连接到其他系统</a>指南。</p>
<h3 id="项目配置">项目配置</h3>
<p>对 Hadoop 输入/输出格式的支持是 flink-java 和 flink-scala Maven 模块的一部分，这些模块在编写 Flink 作业时总是需要的。这些代码位于 <code>org.apache.flink.api.java.hadoop</code> 和 <code>org.apache.flink.api.scala.hadoop</code> 中的 mapred 和 mapreduce API 的附加子包中。</p>
<p>对 Hadoop Mappers 和 Reducers 的支持包含在 <code>flink-hadoop-compatibility</code> Maven 模块中。这段代码位于 <code>org.apache.flink.hadoopcompatibility</code> 包中。</p>
<p>如果您想重用 Mappers 和 Reducers，请在 pom.xml 中添加以下依赖关系。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
	<span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
	<span class="nt">&lt;artifactId&gt;</span>flink-hadoop-compatibility_2.11<span class="nt">&lt;/artifactId&gt;</span>
	<span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>另请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html#add-hadoop-classpaths">如何配置 hadoop 依赖关系</a>。</p>
<h3 id="使用-hadoop-输入格式">使用 Hadoop 输入格式</h3>
<p>要使用 Flink 的 Hadoop InputFormats，必须先使用 HadoopInputs 实用程序类的 readHadoopFile 或 createHadoopInput 来包装格式。前者用于从 FileInputFormat 派生的输入格式，而后者必须用于通用的输入格式。通过使用 <code>ExecutionEnvironmen#createInput</code>，产生的 InputFormat 可以用来创建数据源。</p>
<p>生成的 DataSet 包含 2 个元组，其中第一个字段是键，第二个字段是从 Hadoop InputFormat 中检索的值。</p>
<p>下面的示例展示了如何使用 Hadoop 的 TextInputFormat。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">LongWritable</span>, <span class="kt">Text</span><span class="o">)]</span> <span class="k">=</span>
  <span class="n">env</span><span class="o">.</span><span class="n">createInput</span><span class="o">(</span><span class="nc">HadoopInputs</span><span class="o">.</span><span class="n">readHadoopFile</span><span class="o">(</span>
                    <span class="k">new</span> <span class="nc">TextInputFormat</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">LongWritable</span><span class="o">],</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">Text</span><span class="o">],</span> <span class="n">textPath</span><span class="o">))</span>

<span class="c1">// Do something with the data.
</span><span class="c1"></span><span class="o">[</span><span class="kt">...</span><span class="o">]</span>
</code></pre></div><h3 id="使用-hadoop-输出格式">使用 Hadoop 输出格式</h3>
<p>Flink 为 Hadoop OutputFormat 提供了一个兼容性封装器，它支持任何实现 org.apache.hadoop.mapred.OutputFormat 或扩展 org.apache.hadoop.mapreduce.OutputFormat 的类。OutputFormat 包装器希望它的输入数据是一个包含2个key和value的 DataSet。这些数据将由 Hadoop OutputFormat 处理。</p>
<p>下面的示例展示了如何使用 Hadoop 的 TextOutputFormat。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Obtain your result to emit.
</span><span class="c1"></span><span class="k">val</span> <span class="n">hadoopResult</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Text</span>, <span class="kt">IntWritable</span><span class="o">)]</span> <span class="k">=</span> <span class="o">[</span><span class="kt">...</span><span class="o">]</span>

<span class="k">val</span> <span class="n">hadoopOF</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HadoopOutputFormat</span><span class="o">[</span><span class="kt">Text</span>,<span class="kt">IntWritable</span><span class="o">](</span>
  <span class="k">new</span> <span class="nc">TextOutputFormat</span><span class="o">[</span><span class="kt">Text</span>, <span class="kt">IntWritable</span><span class="o">],</span>
  <span class="k">new</span> <span class="nc">JobConf</span><span class="o">)</span>

<span class="n">hadoopOF</span><span class="o">.</span><span class="n">getJobConf</span><span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&#34;mapred.textoutputformat.separator&#34;</span><span class="o">,</span> <span class="s">&#34; &#34;</span><span class="o">)</span>
<span class="nc">FileOutputFormat</span><span class="o">.</span><span class="n">setOutputPath</span><span class="o">(</span><span class="n">hadoopOF</span><span class="o">.</span><span class="n">getJobConf</span><span class="o">,</span> <span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">resultPath</span><span class="o">))</span>

<span class="n">hadoopResult</span><span class="o">.</span><span class="n">output</span><span class="o">(</span><span class="n">hadoopOF</span><span class="o">)</span>
</code></pre></div><h3 id="使用-hadoop-mappers-和-reducers">使用 Hadoop Mappers 和 Reducers</h3>
<p>Hadoop Mappers 在语义上等同于 Flink 的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#flatmap">FlatMapFunctions</a>，Hadoop Reducers 等同于 Flink 的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#groupreduce-on-grouped-dataset">GroupReduceFunctions</a>。Flink 为 Hadoop MapReduce 的 Mapper 和 Reducer 接口的实现提供了封装器，也就是说，你可以在常规的 Flink 程序中重用你的 Hadoop Mapper 和 Reducer。目前，只支持 Hadoop 的 mapred API（org.apache.hadoop.mapred）的 Mapper 和 Reduce 接口。</p>
<p>包装器将一个 <code>DataSet&lt;Tuple2&lt;KEYIN,VALUEIN&gt;</code> 作为输入，并产生一个 <code>DataSet&lt;Tuple2&lt;KEYOUT,VALUEOUT&gt;</code> 作为输出，其中 KEYIN 和 KEYOUT 是键，VALUEIN 和 VALUEOUT 是 Hadoop 函数处理的 Hadoop 键值对的值。对于 Reducers，Flink 提供了一个包装器，用于带（HadoopReduceCombineFunction）和不带 Combiner（HadoopReduceFunction）的 GroupReduceFunction。包装器接受一个可选的 JobConf 对象来配置 Hadoop Mapper 或 Reducer。</p>
<p>Flink 的函数包装器有:</p>
<ul>
<li>sorg.apache.flink.hadoopcompatibility.mapred.HadoopMapFunction,</li>
<li>sorg.apache.flink.hadoopcompatibility.mapred.HadoopReduceFunction, 和</li>
<li>sorg.apache.flink.hadoopcompatibility.mapred.HadoopReduceCombineFunction.</li>
</ul>
<p>并可作为常规的 Flink <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#flatmap">FlatMapFunctions</a> 或 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#groupreduce-on-grouped-dataset">GroupReduceFunctions</a> 使用。</p>
<p>下面的例子展示了如何使用 Hadoop Mapper 和 Reducer 函数:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="c1">// Obtain data to process somehow.
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="o">[...]</span>

<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">text</span>
  <span class="c1">// use Hadoop Mapper (Tokenizer) as MapFunction
</span><span class="c1"></span>  <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopMapFunction</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">Tokenizer</span><span class="o">()</span>
  <span class="o">))</span>
  <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="n">0</span><span class="o">)</span>
  <span class="c1">// use Hadoop Reducer (Counter) as Reduce- and CombineFunction
</span><span class="c1"></span>  <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopReduceCombineFunction</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">Counter</span><span class="o">(),</span> <span class="k">new</span> <span class="n">Counter</span><span class="o">()</span>
  <span class="o">));</span>
</code></pre></div><p>请注意：Reducer 包装器工作在 Flink 的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/dataset_transformations.html#transformations-on-grouped-dataset">groupBy()</a> 操作所定义的组上。它不考虑您在 JobConf 中设置的任何自定义分区器、排序或分组比较器。</p>
<h3 id="完整的-hadoop-wordcount-示例">完整的 Hadoop WordCount 示例</h3>
<p>下面的示例展示了使用 Hadoop 数据类型、Input-和 OutputFormats 以及 Mapper 和 Reducer 实现的完整 WordCount 实现。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

<span class="c1">// Set up the Hadoop TextInputFormat.
</span><span class="c1"></span><span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="n">Job</span><span class="o">.</span><span class="na">getInstance</span><span class="o">();</span>
<span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="n">hadoopIF</span> <span class="o">=</span>
  <span class="k">new</span> <span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">TextInputFormat</span><span class="o">(),</span> <span class="n">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">job</span>
  <span class="o">);</span>
<span class="n">TextInputFormat</span><span class="o">.</span><span class="na">addInputPath</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="n">inputPath</span><span class="o">));</span>

<span class="c1">// Read data using the Hadoop TextInputFormat.
</span><span class="c1"></span><span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">createInput</span><span class="o">(</span><span class="n">hadoopIF</span><span class="o">);</span>

<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">text</span>
  <span class="c1">// use Hadoop Mapper (Tokenizer) as MapFunction
</span><span class="c1"></span>  <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopMapFunction</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">Tokenizer</span><span class="o">()</span>
  <span class="o">))</span>
  <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="n">0</span><span class="o">)</span>
  <span class="c1">// use Hadoop Reducer (Counter) as Reduce- and CombineFunction
</span><span class="c1"></span>  <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopReduceCombineFunction</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">Counter</span><span class="o">(),</span> <span class="k">new</span> <span class="n">Counter</span><span class="o">()</span>
  <span class="o">));</span>

<span class="c1">// Set up the Hadoop TextOutputFormat.
</span><span class="c1"></span><span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;</span> <span class="n">hadoopOF</span> <span class="o">=</span>
  <span class="k">new</span> <span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
    <span class="k">new</span> <span class="n">TextOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(),</span> <span class="n">job</span>
  <span class="o">);</span>
<span class="n">hadoopOF</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">set</span><span class="o">(</span><span class="s">&#34;mapreduce.output.textoutputformat.separator&#34;</span><span class="o">,</span> <span class="s">&#34; &#34;</span><span class="o">);</span>
<span class="n">TextOutputFormat</span><span class="o">.</span><span class="na">setOutputPath</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="n">outputPath</span><span class="o">));</span>

<span class="c1">// Emit data using the Hadoop TextOutputFormat.
</span><span class="c1"></span><span class="n">result</span><span class="o">.</span><span class="na">output</span><span class="o">(</span><span class="n">hadoopOF</span><span class="o">);</span>

<span class="c1">// Execute Program
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="s">&#34;Hadoop WordCount&#34;</span><span class="o">);</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/hadoop_compatibility.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/hadoop_compatibility.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Insert 语句]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-sql-hints/?utm_source=atom_feed" rel="related" type="text/html" title="SQL 提示" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-show-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Show 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Insert Statements</blockquote><h1 id="insert-语句">INSERT 语句</h1>
<p>INSERT 语句用于向表中添加行。</p>
<h2 id="运行-insert-语句">运行 INSERT 语句</h2>
<p>单条 INSERT 语句可以通过 TableEnvironment 的 <code>executeSql()</code> 方法执行，也可以在 SQL CLI 中执行。INSERT 语句的 <code>executeSql()</code> 方法会立即提交一个 Flink 作业，并返回一个与提交的作业相关联的 TableResult 实例。多个 INSERT 语句可以通过 StatementSet 的 <code>addInsertSql()</code> 方法执行，StatementSet 可以由 <code>TableEnvironment.createStatementSet()</code> 方法创建。<code>addInsertSql()</code> 方法是一种懒惰的执行方式，它们只有在调用 <code>StatementSet.execute()</code> 时才会被执行。</p>
<p>下面的例子展示了如何在 TableEnvironment 中运行一条 INSERT 语句，以及在 SQL CLI 中，在 StatementSet 中运行多条 INSERT 语句。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; CREATE TABLE Orders <span class="o">(</span><span class="sb">`</span>user<span class="sb">`</span> BIGINT, product STRING, amount INT<span class="o">)</span> WITH <span class="o">(</span>...<span class="o">)</span><span class="p">;</span>
<span class="o">[</span>INFO<span class="o">]</span> Table has been created.

Flink SQL&gt; CREATE TABLE RubberOrders<span class="o">(</span>product STRING, amount INT<span class="o">)</span> WITH <span class="o">(</span>...<span class="o">)</span><span class="p">;</span>

Flink SQL&gt; SHOW TABLES<span class="p">;</span>
Orders
RubberOrders

Flink SQL&gt; INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE <span class="s1">&#39;%Rubber%&#39;</span><span class="p">;</span>
<span class="o">[</span>INFO<span class="o">]</span> Submitting SQL update statement to the cluster...
<span class="o">[</span>INFO<span class="o">]</span> Table update statement has been successfully submitted to the cluster:
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">settings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">()...</span>
<span class="k">val</span> <span class="n">tEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">settings</span><span class="o">)</span>

<span class="c1">// register a source table named &#34;Orders&#34; and a sink table named &#34;RubberOrders&#34;
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)&#34;</span><span class="o">)</span>
<span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)&#34;</span><span class="o">)</span>

<span class="c1">// run a single INSERT query on the registered source table and emit the result to registered sink table
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableResult1</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span>
  <span class="s">&#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE &#39;%Rubber%&#39;&#34;</span><span class="o">)</span>
<span class="c1">// get job status through TableResult
</span><span class="c1"></span><span class="n">println</span><span class="o">(</span><span class="n">tableResult1</span><span class="o">.</span><span class="n">getJobClient</span><span class="o">().</span><span class="n">get</span><span class="o">().</span><span class="n">getJobStatus</span><span class="o">())</span>

<span class="c1">//----------------------------------------------------------------------------
</span><span class="c1">// register another sink table named &#34;GlassOrders&#34; for multiple INSERT queries
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE GlassOrders(product VARCHAR, amount INT) WITH (...)&#34;</span><span class="o">)</span>

<span class="c1">// run multiple INSERT queries on the registered source table and emit the result to registered sink tables
</span><span class="c1"></span><span class="k">val</span> <span class="n">stmtSet</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">createStatementSet</span><span class="o">()</span>
<span class="c1">// only single INSERT query can be accepted by `addInsertSql` method
</span><span class="c1"></span><span class="n">stmtSet</span><span class="o">.</span><span class="n">addInsertSql</span><span class="o">(</span>
  <span class="s">&#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE &#39;%Rubber%&#39;&#34;</span><span class="o">)</span>
<span class="n">stmtSet</span><span class="o">.</span><span class="n">addInsertSql</span><span class="o">(</span>
  <span class="s">&#34;INSERT INTO GlassOrders SELECT product, amount FROM Orders WHERE product LIKE &#39;%Glass%&#39;&#34;</span><span class="o">)</span>
<span class="c1">// execute all statements together
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableResult2</span> <span class="k">=</span> <span class="n">stmtSet</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
<span class="c1">// get job status through TableResult
</span><span class="c1"></span><span class="n">println</span><span class="o">(</span><span class="n">tableResult2</span><span class="o">.</span><span class="n">getJobClient</span><span class="o">().</span><span class="n">get</span><span class="o">().</span><span class="n">getJobStatus</span><span class="o">())</span>
</code></pre></div><h2 id="insert-from-select-queries">Insert from select queries</h2>
<p>查询结果可以通过使用插入子句插入到表中。</p>
<h3 id="语法">语法</h3>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">INSERT</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">OVERWRITE</span><span class="w"> </span><span class="err">}</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="k">table_name</span><span class="w"> </span><span class="p">[</span><span class="n">PARTITION</span><span class="w"> </span><span class="n">part_spec</span><span class="p">]</span><span class="w"> </span><span class="n">select_statement</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">part_spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="p">(</span><span class="n">part_col_name1</span><span class="o">=</span><span class="n">val1</span><span class="w"> </span><span class="p">[,</span><span class="w"> </span><span class="n">part_col_name2</span><span class="o">=</span><span class="n">val2</span><span class="p">,</span><span class="w"> </span><span class="p">...])</span><span class="w">
</span></code></pre></div><p><strong>OVERWRITE</strong></p>
<p>INSERT OVERWRITE 将覆盖表或分区中的任何现有数据。否则，将追加新数据。</p>
<p><strong>PARTITION</strong></p>
<p>PARTITION 子句应包含本次插入的静态分区列。</p>
<h3 id="例子">例子</h3>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="c1">-- Creates a partitioned table
</span><span class="c1"></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">country_page_view</span><span class="w"> </span><span class="p">(</span><span class="k">user</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w"> </span><span class="n">cnt</span><span class="w"> </span><span class="nb">INT</span><span class="p">,</span><span class="w"> </span><span class="nb">date</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w"> </span><span class="n">country</span><span class="w"> </span><span class="n">STRING</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="n">PARTITIONED</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="p">(</span><span class="nb">date</span><span class="p">,</span><span class="w"> </span><span class="n">country</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">WITH</span><span class="w"> </span><span class="p">(...)</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c1">-- Appends rows into the static partition (date=&#39;2019-8-30&#39;, country=&#39;China&#39;)
</span><span class="c1"></span><span class="k">INSERT</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="n">country_page_view</span><span class="w"> </span><span class="n">PARTITION</span><span class="w"> </span><span class="p">(</span><span class="nb">date</span><span class="o">=</span><span class="s1">&#39;2019-8-30&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">country</span><span class="o">=</span><span class="s1">&#39;China&#39;</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="p">,</span><span class="w"> </span><span class="n">cnt</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">page_view_source</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c1">-- Appends rows into partition (date, country), where date is static partition with value &#39;2019-8-30&#39;,
</span><span class="c1">-- country is dynamic partition whose value is dynamic determined by each row.
</span><span class="c1"></span><span class="k">INSERT</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="n">country_page_view</span><span class="w"> </span><span class="n">PARTITION</span><span class="w"> </span><span class="p">(</span><span class="nb">date</span><span class="o">=</span><span class="s1">&#39;2019-8-30&#39;</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="p">,</span><span class="w"> </span><span class="n">cnt</span><span class="p">,</span><span class="w"> </span><span class="n">country</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">page_view_source</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c1">-- Overwrites rows into static partition (date=&#39;2019-8-30&#39;, country=&#39;China&#39;)
</span><span class="c1"></span><span class="k">INSERT</span><span class="w"> </span><span class="n">OVERWRITE</span><span class="w"> </span><span class="n">country_page_view</span><span class="w"> </span><span class="n">PARTITION</span><span class="w"> </span><span class="p">(</span><span class="nb">date</span><span class="o">=</span><span class="s1">&#39;2019-8-30&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">country</span><span class="o">=</span><span class="s1">&#39;China&#39;</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="p">,</span><span class="w"> </span><span class="n">cnt</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">page_view_source</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c1">-- Overwrites rows into partition (date, country), where date is static partition with value &#39;2019-8-30&#39;,
</span><span class="c1">-- country is dynamic partition whose value is dynamic determined by each row.
</span><span class="c1"></span><span class="k">INSERT</span><span class="w"> </span><span class="n">OVERWRITE</span><span class="w"> </span><span class="n">country_page_view</span><span class="w"> </span><span class="n">PARTITION</span><span class="w"> </span><span class="p">(</span><span class="nb">date</span><span class="o">=</span><span class="s1">&#39;2019-8-30&#39;</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="p">,</span><span class="w"> </span><span class="n">cnt</span><span class="p">,</span><span class="w"> </span><span class="n">country</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">page_view_source</span><span class="p">;</span><span class="w">
</span></code></pre></div><h2 id="insert-values-into-tables">Insert values into tables</h2>
<p>INSERT&hellip;VALUES 语句可以用来直接从 SQL 中向表中插入数据。</p>
<h3 id="语法-1">语法</h3>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">INSERT</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">OVERWRITE</span><span class="w"> </span><span class="err">}</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="k">table_name</span><span class="w"> </span><span class="k">VALUES</span><span class="w"> </span><span class="n">values_row</span><span class="w"> </span><span class="p">[,</span><span class="w"> </span><span class="n">values_row</span><span class="w"> </span><span class="p">...]</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">values_row</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="n">val1</span><span class="w"> </span><span class="p">[,</span><span class="w"> </span><span class="n">val2</span><span class="p">,</span><span class="w"> </span><span class="p">...])</span><span class="w">
</span></code></pre></div><p><strong>OVERWRITE</strong></p>
<p>INSERT OVERWRITE 将覆盖表中的任何现有数据。否则，将追加新数据。</p>
<h3 id="例子-1">例子</h3>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">students</span><span class="w"> </span><span class="p">(</span><span class="n">name</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w"> </span><span class="n">age</span><span class="w"> </span><span class="nb">INT</span><span class="p">,</span><span class="w"> </span><span class="n">gpa</span><span class="w"> </span><span class="nb">DECIMAL</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">))</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(...);</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">INSERT</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="n">students</span><span class="w">
</span><span class="w">  </span><span class="k">VALUES</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;fred flintstone&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">35</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">.</span><span class="mi">28</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;barney rubble&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">.</span><span class="mi">32</span><span class="p">);</span><span class="w">
</span></code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/insert.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/insert.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/sql" term="sql" label="SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Java Lambda 表达式]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/?utm_source=atom_feed" rel="related" type="text/html" title="可查询状态" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Java Lambda Expressions</blockquote><h2 id="java-lambda-表达式">Java Lambda 表达式</h2>
<p>Java 8 引入了一些新的语言功能，旨在实现更快、更清晰的编码。其中最重要的功能是所谓的&quot;Lambda 表达式&quot;，它打开了函数式编程的大门。Lambda 表达式允许以一种直接的方式实现和传递函数，而无需声明额外的（匿名）类。</p>
<p>注意 Flink 支持对 Java API 的所有操作符使用 lambda 表达式，但是，每当 lambda 表达式使用 Java 属的时候，你需要明确地声明类型信息。</p>
<p>本文档展示了如何使用 lambda 表达式并描述了当前的限制。关于 Flink API 的一般介绍，请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">DataSteam API 概述</a>。</p>
<h3 id="例子和限制">例子和限制</h3>
<p>下面的例子说明了如何实现一个简单的内联 <code>map()</code> 函数，该函数使用 lambda 表达式对其输入进行平方化。<code>map()</code> 函数的输入 <code>i</code> 和输出参数的类型不需要声明，因为它们是由 Java 编译器推断的。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
<span class="c1">// returns the squared i
</span><span class="c1"></span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">i</span> <span class="o">-&gt;</span> <span class="n">i</span><span class="o">*</span><span class="n">i</span><span class="o">)</span>
<span class="o">.</span><span class="na">print</span><span class="o">();</span>
</code></pre></div><p>Flink 可以从方法签名 <code>OUT map(IN value)</code> 的实现中自动提取结果类型信息，因为 OUT 不是通用的，而是 Integer。</p>
<p>遗憾的是，像 <code>flatMap()</code> 这样签名为 <code>void flatMap(IN value, Collector&lt;OUT&gt; out)</code> 的函数被 Java 编译器编译成 <code>void flatMap(IN value, Collector out)</code>。这使得 Flink 无法自动推断输出类型的类型信息。</p>
<p>Flink 很可能会抛出一个类似下面的异常。</p>
<pre><code>org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of 'Collector' are missing.
    In many cases lambda methods don't provide enough information for automatic type extraction when Java generics are involved.
    An easy workaround is to use an (anonymous) class instead that implements the 'org.apache.flink.api.common.functions.FlatMapFunction' interface.
    Otherwise the type has to be specified explicitly using type information.
</code></pre><p>在这种情况下，需要明确指定类型信息，否则输出将被视为类型为 Object，导致序列化效率低下。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.typeinfo.Types</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.api.java.DataSet</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.util.Collector</span><span class="o">;</span>

<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">input</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">);</span>

<span class="c1">// collector type must be declared
</span><span class="c1"></span><span class="n">input</span><span class="o">.</span><span class="na">flatMap</span><span class="o">((</span><span class="n">Integer</span> <span class="n">number</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">StringBuilder</span> <span class="n">builder</span> <span class="o">=</span> <span class="k">new</span> <span class="n">StringBuilder</span><span class="o">();</span>
    <span class="k">for</span><span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">number</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
        <span class="n">builder</span><span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">);</span>
        <span class="n">out</span><span class="o">.</span><span class="na">collect</span><span class="o">(</span><span class="n">builder</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
    <span class="o">}</span>
<span class="o">})</span>
<span class="c1">// provide type information explicitly
</span><span class="c1"></span><span class="o">.</span><span class="na">returns</span><span class="o">(</span><span class="n">Types</span><span class="o">.</span><span class="na">STRING</span><span class="o">)</span>
<span class="c1">// prints &#34;a&#34;, &#34;a&#34;, &#34;aa&#34;, &#34;a&#34;, &#34;aa&#34;, &#34;aaa&#34;
</span><span class="c1"></span><span class="o">.</span><span class="na">print</span><span class="o">();</span>
</code></pre></div><p>当使用具有通用返回类型的 <code>map()</code> 函数时，也会出现类似的问题。在下面的例子中，一个方法签名 <code>Tuple2&lt;Integer, Integer&gt; map(Integer value)</code> 被擦除为 <code>Tuple2 map(Integer value)</code>。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.functions.MapFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.api.java.tuple.Tuple2</span><span class="o">;</span>

<span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">i</span> <span class="o">-&gt;</span> <span class="n">Tuple2</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span><span class="o">))</span>    <span class="c1">// no information about fields of Tuple2
</span><span class="c1"></span>    <span class="o">.</span><span class="na">print</span><span class="o">();</span>
</code></pre></div><p>一般来说，这些问题可以通过多种方式解决。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.typeinfo.Types</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.api.java.tuple.Tuple2</span><span class="o">;</span>

<span class="c1">// use the explicit &#34;.returns(...)&#34;
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">i</span> <span class="o">-&gt;</span> <span class="n">Tuple2</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span><span class="o">))</span>
    <span class="o">.</span><span class="na">returns</span><span class="o">(</span><span class="n">Types</span><span class="o">.</span><span class="na">TUPLE</span><span class="o">(</span><span class="n">Types</span><span class="o">.</span><span class="na">INT</span><span class="o">,</span> <span class="n">Types</span><span class="o">.</span><span class="na">INT</span><span class="o">))</span>
    <span class="o">.</span><span class="na">print</span><span class="o">();</span>

<span class="c1">// use a class instead
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="k">new</span> <span class="n">MyTuple2Mapper</span><span class="o">())</span>
    <span class="o">.</span><span class="na">print</span><span class="o">();</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">MyTuple2Mapper</span> <span class="kd">extends</span> <span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="o">{</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="nf">map</span><span class="o">(</span><span class="n">Integer</span> <span class="n">i</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="n">Tuple2</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// use an anonymous class instead
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="k">new</span> <span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="o">{</span>
        <span class="nd">@Override</span>
        <span class="kd">public</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="nf">map</span><span class="o">(</span><span class="n">Integer</span> <span class="n">i</span><span class="o">)</span> <span class="o">{</span>
            <span class="k">return</span> <span class="n">Tuple2</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span><span class="o">);</span>
        <span class="o">}</span>
    <span class="o">})</span>
    <span class="o">.</span><span class="na">print</span><span class="o">();</span>

<span class="c1">// or in this example use a tuple subclass instead
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="na">fromElements</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">2</span><span class="o">,</span> <span class="n">3</span><span class="o">)</span>
    <span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">i</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">DoubleTuple</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">i</span><span class="o">))</span>
    <span class="o">.</span><span class="na">print</span><span class="o">();</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">DoubleTuple</span> <span class="kd">extends</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="nf">DoubleTuple</span><span class="o">(</span><span class="kt">int</span> <span class="n">f0</span><span class="o">,</span> <span class="kt">int</span> <span class="n">f1</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">this</span><span class="o">.</span><span class="na">f0</span> <span class="o">=</span> <span class="n">f0</span><span class="o">;</span>
        <span class="k">this</span><span class="o">.</span><span class="na">f1</span> <span class="o">=</span> <span class="n">f1</span><span class="o">;</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/java_lambdas.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/java_lambdas.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Joining]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-asynchronous-io-for-external-data-access/?utm_source=atom_feed" rel="related" type="text/html" title="用于外部数据访问的异步 I/O" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-windows/?utm_source=atom_feed" rel="related" type="text/html" title="窗口" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-joining/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Joining</blockquote><h1 id="窗口连接join">窗口连接(Join)</h1>
<p>窗口连接(window join)将两个流的元素连接起来，这两个流有一个共同的键，并且位于同一个窗口中。这些窗口可以通过使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-assigners">窗口分配器</a>来定义，并对两个流的元素进行评估。</p>
<p>然后，来自双方的元素被传递到一个用户定义的 JoinFunction 或 FlatJoinFunction 中，用户可以发出符合加入标准的结果。</p>
<p>一般的用法可以归纳为以下几点。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">stream</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">otherStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(&lt;</span><span class="nc">KeySelector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">equalTo</span><span class="o">(&lt;</span><span class="nc">KeySelector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="nc">WindowAssigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">apply</span><span class="o">(&lt;</span><span class="nc">JoinFunction</span><span class="o">&gt;)</span>
</code></pre></div><p>关于语义的一些说明:</p>
<ul>
<li>两个流中元素的成对组合的创建就像一个内部连接，这意味着一个流中的元素如果没有另一个流中的相应元素与之连接，就不会发出。</li>
<li>那些被加入的元素将以各自窗口中最大的时间戳作为它们的时间戳。例如，一个以 <code>[5, 10)</code> 为边界的窗口将导致加入的元素以9作为它们的时间戳。</li>
</ul>
<p>在下面的章节中，我们将使用一些示例性的场景来概述不同类型的窗口连接是如何进行的。</p>
<h2 id="滚动窗口连接">滚动窗口连接</h2>
<p>当执行滚动窗口连接时，所有具有共同的键和共同的滚动窗口的元素都会以成对组合的方式进行连接，并传递给 <code>JoinFunction</code> 或 <code>FlatJoinFunction</code>。因为这表现得像一个内连接，所以一个流的元素如果在其滚动窗口中没有来自另一个流的元素，就不会被发出去！这就是为什么我们要把一个流的元素加入到滚动窗口中。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/tumbling-window-join.svg" alt="img"></p>
<p>如图所示，我们定义了一个大小为2毫秒的滚动窗口，其结果是 <code>[0,1]</code>，<code>[2,3]</code>，&hellip;形式的窗口。图中显示了每个窗口中所有元素的配对组合，这些元素将被传递给 <code>JoinFunction</code>。请注意，在翻滚窗口 <code>[6,7]</code> 中，没有任何元素发出，因为绿色流中没有元素存在，要与橙色元素⑥和⑦连接。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.time.Time</span><span class="o">;</span>

<span class="o">...</span>

<span class="k">val</span> <span class="n">orangeStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">greenStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">orangeStream</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">greenStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">equalTo</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(</span><span class="mi">2</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="o">(</span><span class="n">e1</span><span class="o">,</span> <span class="n">e2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">e1</span> <span class="o">+</span> <span class="s">&#34;,&#34;</span> <span class="o">+</span> <span class="n">e2</span> <span class="o">}</span>
</code></pre></div><h2 id="滑动窗连接">滑动窗连接</h2>
<p>在执行滑动窗口连接时，所有具有共同键和共同滑动窗口的元素都会以成对组合的方式连接，并传递给 <code>JoinFunction</code> 或 <code>FlatJoinFunction</code>。一个流的元素如果在当前的滑动窗口中没有来自另一个流的元素，则不会被发出! 请注意，有些元素可能在一个滑动窗口中被加入，但在另一个滑动窗口中却没有!</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/sliding-window-join.svg" alt="img"></p>
<p>在这个例子中，我们使用的是大小为两毫秒的滑动窗口，并将它们滑动一毫秒，结果是滑动窗口 <code>[-1，0]，[0，1]，[1，2]，[2，3]</code>，&hellip;。x轴下面的加入元素就是每个滑动窗口传递给 <code>JoinFunction</code> 的元素。这里你也可以看到，例如橙色的②与绿色的③在窗口 <code>[2,3]</code> 中连接，但与窗口 <code>[1,2]</code> 中的任何元素都没有连接。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.time.Time</span><span class="o">;</span>

<span class="o">...</span>

<span class="k">val</span> <span class="n">orangeStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">greenStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">orangeStream</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">greenStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">equalTo</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">SlidingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span> <span class="cm">/* size */</span><span class="o">,</span> <span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="cm">/* slide */</span><span class="o">))</span>
    <span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="o">(</span><span class="n">e1</span><span class="o">,</span> <span class="n">e2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">e1</span> <span class="o">+</span> <span class="s">&#34;,&#34;</span> <span class="o">+</span> <span class="n">e2</span> <span class="o">}</span>
</code></pre></div><h2 id="会议窗口连接">会议窗口连接</h2>
<p>当执行会话窗口连接时，所有具有相同键的元素，当&quot;组合&quot;满足会话标准时，将以成对组合的方式连接，并传递给 <code>JoinFunction</code> 或 <code>FlatJoinFunction</code>。同样，这也是执行内部连接，所以如果有一个会话窗口只包含来自一个流的元素，就不会有输出。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/session-window-join.svg" alt="img"></p>
<p>在这里，我们定义了一个会话窗口加入，其中每个会话被至少1ms的间隙所分割。有三个会话，在前两个会话中，两个流中的加入元素都会传递给 <code>JoinFunction</code>。在第三个会话中，绿色流中没有元素，所以⑧和⑨没有加入！在第三个会话中，绿色流中没有元素。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.time.Time</span><span class="o">;</span>
 
<span class="o">...</span>

<span class="k">val</span> <span class="n">orangeStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">greenStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">orangeStream</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">greenStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">equalTo</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">EventTimeSessionWindows</span><span class="o">.</span><span class="n">withGap</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(</span><span class="mi">1</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="o">(</span><span class="n">e1</span><span class="o">,</span> <span class="n">e2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">e1</span> <span class="o">+</span> <span class="s">&#34;,&#34;</span> <span class="o">+</span> <span class="n">e2</span> <span class="o">}</span>
</code></pre></div><h2 id="间隔连接">间隔连接</h2>
<p>间隔连接将两个流的元素（我们暂且称它们为A和B）用一个共同的键连接起来，流B中的元素的时间戳与流A中元素的时间戳处于一个相对的时间间隔。</p>
<p>这也可以更正式地表达为 <code>b.timestamp∈[a.timestamp + lowerBound; a.timestamp + upperBound]</code> 或 <code>a.timestamp + lowerBound &lt;= b.timestamp &lt;= a.timestamp + upperBound</code>。</p>
<p>其中a和b是A和B的元素，它们有一个共同的键。下界和上界都可以是负的或正的，只要下界总是小于或等于上界。区间连接目前只执行内连接。</p>
<p>当一对元素传递给 <code>ProcessJoinFunction</code> 时，它们将被分配为两个元素中较大的时间戳（可以通过 <code>ProcessJoinFunction.Context</code> 访问）。</p>
<p>注意：间隔连接目前只支持事件时间。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/interval-join.svg" alt="img"></p>
<p>在上面的例子中，我们将两个流&quot;橙色&quot;和&quot;绿色&quot;连接起来，下界为-2毫秒，上界为+1毫秒。默认情况下，这些边界是包容的，但可以应用 <code>.lowerBoundExclusive()</code> 和 <code>.upperBoundExclusive</code> 来改变行为。</p>
<p>再次使用更正式的符号，这将被翻译为:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">orangeElem</span><span class="o">.</span><span class="n">ts</span> <span class="o">+</span> <span class="n">lowerBound</span> <span class="o">&lt;=</span> <span class="n">greenElem</span><span class="o">.</span><span class="n">ts</span> <span class="o">&lt;=</span> <span class="n">orangeElem</span><span class="o">.</span><span class="n">ts</span> <span class="o">+</span> <span class="n">upperBound</span>
</code></pre></div><p>as indicated by the triangles.</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.functions.co.ProcessJoinFunction</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.windowing.time.Time</span><span class="o">;</span>

<span class="o">...</span>

<span class="k">val</span> <span class="n">orangeStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">greenStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">orangeStream</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">)</span>
    <span class="o">.</span><span class="n">intervalJoin</span><span class="o">(</span><span class="n">greenStream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="n">elem</span> <span class="k">=&gt;</span> <span class="cm">/* select key */</span><span class="o">))</span>
    <span class="o">.</span><span class="n">between</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(-</span><span class="mi">2</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">milliseconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">ProcessJoinFunction</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">Integer</span>, <span class="kt">String</span><span class="o">]</span> <span class="o">{</span>
        <span class="k">override</span> <span class="k">def</span> <span class="n">processElement</span><span class="o">(</span><span class="n">left</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">,</span> <span class="n">right</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">,</span> <span class="n">ctx</span><span class="k">:</span> <span class="kt">ProcessJoinFunction</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">Integer</span>, <span class="kt">String</span><span class="o">]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
         <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">left</span> <span class="o">+</span> <span class="s">&#34;,&#34;</span> <span class="o">+</span> <span class="n">right</span><span class="o">);</span> 
        <span class="o">}</span>
      <span class="o">});</span>
    <span class="o">});</span>
</code></pre></div><p>原文连接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/joining.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/joining.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/operators" term="operators" label="Operators" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/joining" term="joining" label="Joining" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Operators]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/?utm_source=atom_feed" rel="related" type="text/html" title="可查询状态" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-operators/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Operators</blockquote><h2 id="操作符">操作符</h2>
<p>操作符将一个或多个 DataStream 转换为一个新的 DataStream。程序可以将多个变换组合成复杂的数据流拓扑。</p>
<p>本节给出了基本变换的描述，应用这些变换后的有效物理分区，以及对 Flink 的操作符链的见解。</p>
<h2 id="datastream-转换">DataStream 转换</h2>
<ul>
<li>Map</li>
</ul>
<p>DataStream → DataStream</p>
<p>接受一个元素并产生一个元素。一个将输入流的值翻倍的 <code>map</code> 函数:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">}</span>
</code></pre></div><ul>
<li>FlatMap</li>
</ul>
<p>DataStream → DataStream</p>
<p>接受一个元素并产生零个、一个或多个元素。一个将句子分割成单词的 <code>flatMap</code> 函数:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="n">str</span> <span class="k">=&gt;</span> <span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34; &#34;</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Filter</li>
</ul>
<p>DataStream → DataStream</p>
<p>评估每个元素的布尔函数，并保留那些函数返回值为真的元素。一个过滤掉零值的 <code>filter</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">_</span> <span class="o">!=</span> <span class="mi">0</span> <span class="o">}</span>
</code></pre></div><ul>
<li>KeyBy</li>
</ul>
<p>DataStream → KeyedStream</p>
<p>在逻辑上将一个流划分为互斥的分区，每个分区包含相同键的元素。在内部，这是通过哈希分区实现的。关于如何指定键，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html#keyed-state">keys</a>。这个转换会返回一个 <code>KeyedStream</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="s">&#34;someKey&#34;</span><span class="o">)</span> <span class="c1">// Key by field &#34;someKey&#34;
</span><span class="c1"></span><span class="n">dataStream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>         <span class="c1">// Key by the first element of a Tuple
</span></code></pre></div><ul>
<li>Reduce</li>
</ul>
<p>KeyedStream → DataStream</p>
<p>在 keyed 数据流上进行&quot;滚动&quot;换算(reduce)。将当前元素与最后一个换算的值合并，并发出新的值。</p>
<p>一个创建部分和(sum)流的 <code>reduce</code> 函数:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">keyedStream</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="k">_</span> <span class="o">+</span> <span class="k">_</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Fold</li>
</ul>
<p>KeyedStream → DataStream</p>
<p>在一个带有初始值的 keyed 数据流上进行&quot;滚动&quot;折叠。将当前元素与最后一个折叠的值结合起来，并发出新的值。</p>
<p>一个折叠函数，当应用于序列(1,2,3,4,5)时，发出序列 &ldquo;start-1&rdquo;、&ldquo;start-1-2&rdquo;、&ldquo;start-1-2-3&rdquo;、&hellip;</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span>
    <span class="n">keyedStream</span><span class="o">.</span><span class="n">fold</span><span class="o">(</span><span class="s">&#34;start&#34;</span><span class="o">)((</span><span class="n">str</span><span class="o">,</span> <span class="n">i</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span> <span class="n">str</span> <span class="o">+</span> <span class="s">&#34;-&#34;</span> <span class="o">+</span> <span class="n">i</span> <span class="o">})</span>
</code></pre></div><ul>
<li>Aggregations</li>
</ul>
<p>KeyedStream → DataStream</p>
<p>在 keyed 数据流上进行滚动聚合。<code>min</code> 和 <code>minBy</code> 的区别在于 <code>min</code> 返回最小值，而 <code>minBy</code> 则返回该字段中具有最小值的元素（<code>max</code> 和 <code>maxBy</code> 也一样）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">keyedStream</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">min</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">min</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">keyedStream</span><span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Window</li>
</ul>
<p>KeyedStream → WindowedStream</p>
<p>可以在已经分区的 <code>KeyedStream</code> 上定义 <code>Window</code>。窗口根据一些特征（例如，最近5秒内到达的数据）对每个键中的数据进行分组。关于窗口的描述，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html">窗口</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span> <span class="c1">// Last 5 seconds of data
</span></code></pre></div><ul>
<li>WindowAll</li>
</ul>
<p>DataStream → AllWindowedStream</p>
<p>可以在常规的 DataStream 上定义窗口。窗口根据一些特征（例如，在过去5秒内到达的数据）对所有流事件进行分组。关于窗口的完整描述，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html">窗口</a>。</p>
<p>警告：在许多情况下，这是一个非并行的转换。所有的记录将被收集在 <code>windowAll</code> 操作符的一个任务(task)中。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">windowAll</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span> <span class="c1">// Last 5 seconds of data
</span></code></pre></div><ul>
<li>Window Apply</li>
</ul>
<p>WindowedStream → DataStream</p>
<p>AllWindowedStream → DataStream</p>
<p>将一般函数应用于整个窗口。下面是一个手动求和窗口元素的函数。</p>
<p>注意：如果您使用的是 <code>windowAll</code> 转换，您需要使用 <code>AllWindowFunction</code> 来代替。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">windowedStream</span><span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="nc">WindowFunction</span> <span class="o">}</span>

<span class="c1">// applying an AllWindowFunction on non-keyed window stream
</span><span class="c1"></span><span class="n">allWindowedStream</span><span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="nc">AllWindowFunction</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Window Reduce</li>
</ul>
<p>WindowedStream → DataStream</p>
<p>对窗口应用函数式的 <code>reduce </code> 函数，并返回换算后的值:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">windowedStream</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="k">_</span> <span class="o">+</span> <span class="k">_</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Window Fold</li>
</ul>
<p>WindowedStream → DataStream</p>
<p>对窗口应用功能 <code>fold</code> 函数并返回折叠后的值。示例函数应用于序列 <code>(1,2,3,4,5)</code> 时，将序列折叠成字符串 &ldquo;start-1-2-3-4-5&rdquo;:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span>
    <span class="n">windowedStream</span><span class="o">.</span><span class="n">fold</span><span class="o">(</span><span class="s">&#34;start&#34;</span><span class="o">,</span> <span class="o">(</span><span class="n">str</span><span class="o">,</span> <span class="n">i</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span> <span class="n">str</span> <span class="o">+</span> <span class="s">&#34;-&#34;</span> <span class="o">+</span> <span class="n">i</span> <span class="o">})</span>
</code></pre></div><ul>
<li>窗口上的聚合</li>
</ul>
<p>WindowedStream → DataStream</p>
<p>聚合一个窗口的内容。<code>min</code> 和 <code>minBy</code> 的区别在于 <code>min</code> 返回最小值，而 <code>minBy</code> 返回在该字段中具有最小值的元素（<code>max</code> 和 <code>maxBy</code> 相同）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">windowedStream</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">min</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">min</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">windowedStream</span><span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Union</li>
</ul>
<p>DataStream* → DataStream</p>
<p>联合两个或多个数据流，创建一个新的流，包含所有流的所有元素。注意：如果你把一个数据流和它自己联合起来，你将在生成的数据流中得到每个元素两次。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">union</span><span class="o">(</span><span class="n">otherStream1</span><span class="o">,</span> <span class="n">otherStream2</span><span class="o">,</span> <span class="o">...)</span>
</code></pre></div><ul>
<li>Window Join</li>
</ul>
<p>DataStream,DataStream → DataStream</p>
<p>在一个给定的键和一个公共窗口上连接(join)两个数据流。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">otherStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;).</span><span class="n">equalTo</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">3</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">apply</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
</code></pre></div><ul>
<li>Window CoGroup</li>
</ul>
<p>DataStream,DataStream → DataStream</p>
<p>在一个给定的键和一个共同的窗口上将两个数据流串联(Cogroups)起来。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">coGroup</span><span class="o">(</span><span class="n">otherStream</span><span class="o">)</span>
    <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">3</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">apply</span> <span class="o">{}</span>
</code></pre></div><ul>
<li>Connect</li>
</ul>
<p>DataStream,DataStream → ConnectedStreams</p>
<p>&ldquo;连接&rdquo;(connect)两个数据流，保留其类型，允许两个数据流之间共享状态。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">someStream</span> <span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="n">otherStream</span> <span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">connectedStreams</span> <span class="k">=</span> <span class="n">someStream</span><span class="o">.</span><span class="n">connect</span><span class="o">(</span><span class="n">otherStream</span><span class="o">)</span>
</code></pre></div><ul>
<li>CoMap, CoFlatMap</li>
</ul>
<p>ConnectedStreams → DataStream</p>
<p>类似于连接(connected)数据流上的 <code>map</code> 和 <code>flatMap</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">connectedStreams</span><span class="o">.</span><span class="n">map</span><span class="o">(</span>
    <span class="o">(</span><span class="k">_</span> <span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="kc">true</span><span class="o">,</span>
    <span class="o">(</span><span class="k">_</span> <span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="kc">false</span>
<span class="o">)</span>
<span class="n">connectedStreams</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span>
    <span class="o">(</span><span class="k">_</span> <span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="kc">true</span><span class="o">,</span>
    <span class="o">(</span><span class="k">_</span> <span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="kc">false</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>Split</li>
</ul>
<p>DataStream → SplitStream</p>
<p>根据某种标准，将流分成两个或多个流。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">split</span> <span class="k">=</span> <span class="n">someDataStream</span><span class="o">.</span><span class="n">split</span><span class="o">(</span>
  <span class="o">(</span><span class="n">num</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="o">(</span><span class="n">num</span> <span class="o">%</span> <span class="mi">2</span><span class="o">)</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="mi">0</span> <span class="k">=&gt;</span> <span class="nc">List</span><span class="o">(</span><span class="s">&#34;even&#34;</span><span class="o">)</span>
      <span class="k">case</span> <span class="mi">1</span> <span class="k">=&gt;</span> <span class="nc">List</span><span class="o">(</span><span class="s">&#34;odd&#34;</span><span class="o">)</span>
    <span class="o">}</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>Select</li>
</ul>
<p>SplitStream → DataStream</p>
<p>从分割流中选择一个或多个流。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">even</span> <span class="k">=</span> <span class="n">split</span> <span class="n">select</span> <span class="s">&#34;even&#34;</span>
<span class="k">val</span> <span class="n">odd</span> <span class="k">=</span> <span class="n">split</span> <span class="n">select</span> <span class="s">&#34;odd&#34;</span>
<span class="k">val</span> <span class="n">all</span> <span class="k">=</span> <span class="n">split</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="s">&#34;even&#34;</span><span class="o">,</span><span class="s">&#34;odd&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Iterate</li>
</ul>
<p>DataStream → IterativeStream → DataStream</p>
<p>在流(flow)中创建一个&quot;反馈&quot;循环，将一个操作符的输出重定向到之前的某个操作符。这对于定义持续更新模型的算法特别有用。下面的代码从一个流(stream)开始，连续应用迭代体。大于0的元素被送回反馈通道，其余元素被转发到下游。参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/#iterations">迭代</a>的完整描述。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">initialStream</span><span class="o">.</span><span class="n">iterate</span> <span class="o">{</span>
  <span class="n">iteration</span> <span class="k">=&gt;</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">iterationBody</span> <span class="k">=</span> <span class="n">iteration</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span><span class="cm">/*do something*/</span><span class="o">}</span>
    <span class="o">(</span><span class="n">iterationBody</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="o">),</span> <span class="n">iterationBody</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="o">))</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>通过匿名模式匹配从 tuple、case 类和集合中提取，比如下面:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">temperature</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="o">}</span>
</code></pre></div><p>不受 API 开箱即用的支持。要使用这个功能，你应该使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/scala_api_extensions.html">Scala API 扩展</a>。</p>
<p>以下转换可用于 Tuples 的数据流:</p>
<ul>
<li>Project</li>
</ul>
<p>DataStream → DataStream</p>
<p>从元组中选择一个字段的子集。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataStream</span><span class="o">&lt;</span><span class="n">Tuple3</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">in</span> <span class="o">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">DataStream</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="na">project</span><span class="o">(</span><span class="n">2</span><span class="o">,</span><span class="n">0</span><span class="o">);</span>
</code></pre></div><h2 id="物理分区">物理分区</h2>
<p>Flink 还可以通过以下函数对转换后的准确流分区进行低级控制（如果需要）。</p>
<ul>
<li>自定义分区</li>
</ul>
<p>DataStream → DataStream</p>
<p>使用用户定义的 Partitioner 为每个元素选择目标任务。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">partitionCustom</span><span class="o">(</span><span class="n">partitioner</span><span class="o">,</span> <span class="s">&#34;someKey&#34;</span><span class="o">)</span>
<span class="n">dataStream</span><span class="o">.</span><span class="n">partitionCustom</span><span class="o">(</span><span class="n">partitioner</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span>
</code></pre></div><ul>
<li>随机分区</li>
</ul>
<p>DataStream → DataStream</p>
<p>将元素按照均匀分布随机分区。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">shuffle</span><span class="o">()</span>
</code></pre></div><ul>
<li>Rebalancing (循环分区)</li>
</ul>
<p>DataStream → DataStream</p>
<p>对元素进行循环分区，使每个分区的负载相等。在数据倾斜的情况下，对性能优化很有用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">rebalance</span><span class="o">()</span>
</code></pre></div><ul>
<li>Rescaling</li>
</ul>
<p>DataStream → DataStream</p>
<p>将元素，轮回分区到下游操作的子集。如果你想拥有管道，例如，从源的每个并行实例向几个映射器(mappers)的子集扇出，以分配负载，但又不想进行 <code>rebalance()</code> 会引起的完全再平衡，那么这就很有用。这将只需要本地数据传输，而不是通过网络传输数据，这取决于其他配置值，如 TaskManagers 的槽数(slots)。</p>
<p>上游操作向其发送元素的下游操作子集取决于上游和下游操作的并行程度。例如，如果上游操作的并行度为2，下游操作的并行度为4，那么一个上游操作将向两个下游操作分发元素，而另一个上游操作将向另外两个下游操作分发。另一方面，如果下游操作具有并行度2，而上游操作具有并行度4，那么两个上游操作将分配给一个下游操作，而其他两个上游操作将分配给其他下游操作。</p>
<p>在不同的并行度不是彼此的倍数的情况下，一个或几个下游操作将从上游操作中获得不同数量的输入。</p>
<p>请看此图，可以直观地看到上例中的连接(connection)模式。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/rescale.svg" alt="img"></p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">rescale</span><span class="o">()</span>
</code></pre></div><ul>
<li>Broadcasting</li>
</ul>
<p>DataStream → DataStream</p>
<p>将元素广播到每个分区。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">dataStream</span><span class="o">.</span><span class="n">broadcast</span><span class="o">()</span>
</code></pre></div><h2 id="任务链和资源组">任务链和资源组</h2>
<p>链式的两个后续变换意味着将它们共同放置在同一个线程中以获得更好的性能。如果可能的话，Flink 默认会将操作符链起来（例如，两个后续的 map 变换）。如果需要的话，API 提供了对链式操作的精细控制。</p>
<p>如果你想在整个作业(job)中禁用链，请使用 <code>StreamExecutionEnvironment.disableOperatorChaining()</code>。对于更细粒度的控制，以下函数是可用的。请注意，这些函数只能在 DataStream 转换之后使用，因为它们引用了之前的转换。例如，你可以使用 <code>someStream.map(...).startNewChain()</code>，但你不能使用 <code>someStream.startNewChain()</code>。</p>
<p>资源组是 Flink 中的一个槽，参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html#configuring-taskmanager-processing-slots">slots</a>。如果需要，你可以在单独的槽中手动隔离操作符。</p>
<ul>
<li>Start new chain</li>
</ul>
<p>开始一个新的链，从这个操作符开始。两个映射器(mappers)将被连锁，<code>filter</code> 将不会被连锁到第一个映射器(mapper)。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">someStream</span><span class="o">.</span><span class="n">filter</span><span class="o">(...).</span><span class="n">map</span><span class="o">(...).</span><span class="n">startNewChain</span><span class="o">().</span><span class="n">map</span><span class="o">(...)</span>
</code></pre></div><ul>
<li>Disable chaining</li>
</ul>
<p>不将 <code>map</code> 运算符连锁化。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">someStream</span><span class="o">.</span><span class="n">map</span><span class="o">(...).</span><span class="n">disableChaining</span><span class="o">()</span>
</code></pre></div><ul>
<li>Set slot sharing group</li>
</ul>
<p>设置操作的槽位共享组。Flink 会将具有相同槽位共享组的操作放入同一个槽位，而将没有槽位共享组的操作保留在其他槽位。这可以用来隔离槽位。如果所有的输入操作都在同一个槽共享组中，槽共享组就会从输入操作中继承。缺省槽共享组的名称是 &ldquo;default&rdquo;，操作可以通过调用 <code>slotSharingGroup(&quot;default&quot;)</code> 来明确地放入这个组。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">someStream</span><span class="o">.</span><span class="n">filter</span><span class="o">(...).</span><span class="n">slotSharingGroup</span><span class="o">(</span><span class="s">&#34;name&#34;</span><span class="o">)</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/operator" term="operator" label="Operator" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Process Function]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-process-function/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-process-function/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Process Function</blockquote><h2 id="processfunction">ProcessFunction</h2>
<p><code>ProcessFunction</code> 是一种低级的流处理操作，可以访问所有（非循环）流应用的基本构件。</p>
<ul>
<li>事件（流元素）</li>
<li>状态（容错，一致，只在 keyed 流上）。</li>
<li>定时器(事件时间和处理时间，仅在 keyed 流上)</li>
</ul>
<p><code>ProcessFunction</code> 可以被认为是一个 <code>FlatMapFunction</code>，它可以访问 keyed 状态和定时器。它通过对输入流中收到的每个事件进行调用来处理事件。</p>
<p>对于容错状态，<code>ProcessFunction</code> 提供了对 Flink 的 keyed 状态的访问，通过 <code>RuntimeContext</code> 访问，类似于其他有状态函数访问 keyed 状态的方式。</p>
<p>定时器允许应用程序对处理时间和事件时间的变化做出反应。每次调用函数 <code>processElement(...)</code> 都会得到一个 <code>Context</code> 对象，它可以访问元素的事件时间时间戳，以及 <code>TimerService</code>。<code>TimerService</code> 可以用来为未来的事件/处理时间实例注册回调。对于事件时间定时器，当当前水印提前到或超过定时器的时间戳时，就会调用 <code>onTimer(...)</code> 方法；而对于处理时间定时器，当挂钟时间达到指定时间时，就会调用 <code>onTimer(...)</code>。在该调用过程中，所有的状态又会被限定在创建定时器的键上，允许定时器对 keyed 状态进行操作。</p>
<p>注意：如果你想访问 keyed 状态和定时器，你必须在 keyed 流上应用 <code>ProcessFunction</code>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">stream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(...).</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyProcessFunction</span><span class="o">())</span>
</code></pre></div><h2 id="低级连接join">低级连接(join)</h2>
<p>为了实现对两个输入的低级操作，应用程序可以使用 <code>CoProcessFunction</code> 或 <code>KeyedCoProcessFunction</code>。该函数与两个不同的输入绑定，并从两个不同的输入中获取对 <code>processElement1(...)</code> 和 <code>processElement2(...)</code> 记录的单独调用。</p>
<p>实现低级联接(join)通常遵循这种模式。</p>
<ul>
<li>为一个输入（或两个输入）创建一个状态对象。</li>
<li>从其输入中接收到元素时更新状态</li>
<li>在接收到另一个输入的元素后，探测状态并产生加入的结果。</li>
</ul>
<p>例如，您可能会将客户数据加入到金融交易中，同时为客户数据保留状态。如果你关心在面对失序事件时是否有完整的、确定性的加入，你可以使用一个定时器，当客户数据流的水印已经超过该交易的时间时，你可以为该交易评估并发出加入。</p>
<h2 id="例子">例子</h2>
<p>在下面的例子中，<code>KeyedProcessFunction</code> 维护每个键的计数，并且每当一分钟过去（以事件时间为单位），该键没有更新时，就会发出一个键/计数对。</p>
<ul>
<li>计数、键和最后修改时间戳都存储在一个 <code>ValueState</code> 中，这个 <code>ValueState</code> 隐式地被键所限定。</li>
<li>对于每条记录，<code>KeyedProcessFunction</code> 都会递增计数器并设置最后修改时间戳。</li>
<li>该函数还将在未来一分钟内（以事件时间为单位）安排一次回调。</li>
<li>在每次回调时，它都会将回调的事件时间戳与存储的计数的最后修改时间进行核对，如果两者匹配（即在该分钟内没有进一步的更新发生），则发出 <code>key/count</code>。</li>
</ul>
<p>注意: 这个简单的例子可以用会话窗口来实现。我们在这里使用 <code>KeyedProcessFunction</code> 来说明它提供的基本模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.ValueState</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.ValueStateDescriptor</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.java.tuple.Tuple</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.functions.KeyedProcessFunction</span>
<span class="k">import</span> <span class="nn">org.apache.flink.util.Collector</span>

<span class="c1">// the source data stream
</span><span class="c1"></span><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Tuple2</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// apply the process function onto a keyed stream
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Tuple2</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">]]</span> <span class="k">=</span> <span class="n">stream</span>
  <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">CountWithTimeoutFunction</span><span class="o">())</span>

<span class="cm">/**
</span><span class="cm">  * The data type stored in the state
</span><span class="cm">  */</span>
<span class="k">case</span> <span class="k">class</span> <span class="nc">CountWithTimestamp</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">lastModified</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span>

<span class="cm">/**
</span><span class="cm">  * The implementation of the ProcessFunction that maintains the count and timeouts
</span><span class="cm">  */</span>
<span class="k">class</span> <span class="nc">CountWithTimeoutFunction</span> <span class="k">extends</span> <span class="nc">KeyedProcessFunction</span><span class="o">[</span><span class="kt">Tuple</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="o">{</span>

  <span class="cm">/** The state that is maintained by this process function */</span>
  <span class="k">lazy</span> <span class="k">val</span> <span class="n">state</span><span class="k">:</span> <span class="kt">ValueState</span><span class="o">[</span><span class="kt">CountWithTimestamp</span><span class="o">]</span> <span class="k">=</span> <span class="n">getRuntimeContext</span>
    <span class="o">.</span><span class="n">getState</span><span class="o">(</span><span class="k">new</span> <span class="nc">ValueStateDescriptor</span><span class="o">[</span><span class="kt">CountWithTimestamp</span><span class="o">](</span><span class="s">&#34;myState&#34;</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">CountWithTimestamp</span><span class="o">]))</span>


  <span class="k">override</span> <span class="k">def</span> <span class="n">processElement</span><span class="o">(</span>
      <span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">String</span><span class="o">),</span> 
      <span class="n">ctx</span><span class="k">:</span> <span class="kt">KeyedProcessFunction</span><span class="o">[</span><span class="kt">Tuple</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span> 
      <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>

    <span class="c1">// initialize or retrieve/update the state
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">current</span><span class="k">:</span> <span class="kt">CountWithTimestamp</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">value</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="kc">null</span> <span class="k">=&gt;</span>
        <span class="nc">CountWithTimestamp</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">timestamp</span><span class="o">)</span>
      <span class="k">case</span> <span class="nc">CountWithTimestamp</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">count</span><span class="o">,</span> <span class="n">lastModified</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="nc">CountWithTimestamp</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">timestamp</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="c1">// write the state back
</span><span class="c1"></span>    <span class="n">state</span><span class="o">.</span><span class="n">update</span><span class="o">(</span><span class="n">current</span><span class="o">)</span>

    <span class="c1">// schedule the next timer 60 seconds from the current event time
</span><span class="c1"></span>    <span class="n">ctx</span><span class="o">.</span><span class="n">timerService</span><span class="o">.</span><span class="n">registerEventTimeTimer</span><span class="o">(</span><span class="n">current</span><span class="o">.</span><span class="n">lastModified</span> <span class="o">+</span> <span class="mi">60000</span><span class="o">)</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">onTimer</span><span class="o">(</span>
      <span class="n">timestamp</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> 
      <span class="n">ctx</span><span class="k">:</span> <span class="kt">KeyedProcessFunction</span><span class="o">[</span><span class="kt">Tuple</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span><span class="k">#</span><span class="nc">OnTimerContext</span><span class="o">,</span> 
      <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>

    <span class="n">state</span><span class="o">.</span><span class="n">value</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="nc">CountWithTimestamp</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">count</span><span class="o">,</span> <span class="n">lastModified</span><span class="o">)</span> <span class="k">if</span> <span class="o">(</span><span class="n">timestamp</span> <span class="o">==</span> <span class="n">lastModified</span> <span class="o">+</span> <span class="mi">60000</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">key</span><span class="o">,</span> <span class="n">count</span><span class="o">))</span>
      <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>注意：在 Flink 1.4.0 之前，当从处理时间计时器调用时，<code>ProcessFunction.onTimer()</code> 方法将当前处理时间设置为事件时间戳。这种行为非常微妙，可能不会被用户发现。嗯，这是有害的，因为处理时间时间戳是不确定的，而且不与水印对齐。此外，用户实现的逻辑依赖于这个错误的时间戳极有可能是无意中的错误。所以我们决定修复它。升级到 1.4.0 后，使用这个错误的事件时间时间戳的 Flink 作业将失败，用户应该将他们的作业调整为正确的逻辑。</p>
<h2 id="keyedprocessfunction">KeyedProcessFunction</h2>
<p><code>KeyedProcessFunction</code> 作为 <code>ProcessFunction</code> 的扩展，在其 <code>onTimer(...)</code> 方法中提供了对定时器键的访问。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">override</span> <span class="k">def</span> <span class="n">onTimer</span><span class="o">(</span><span class="n">timestamp</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">ctx</span><span class="k">:</span> <span class="kt">OnTimerContext</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">OUT</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">var</span> <span class="n">key</span> <span class="k">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">getCurrentKey</span>
  <span class="c1">// ...
</span><span class="c1"></span><span class="o">}</span>
</code></pre></div><h2 id="定时器">定时器</h2>
<p>两种类型的定时器（处理时间和事件时间）都由 <code>TimerService</code> 内部维护，并排队执行。</p>
<p><code>TimerService</code> 对每个键和时间戳的定时器进行重复复制，即每个键和时间戳最多只有一个定时器。如果同一个时间戳注册了多个定时器，<code>onTimer()</code> 方法将只被调用一次。</p>
<p>注意 Flink 同步调用 <code>onTimer()</code> 和 <code>processElement()</code>。因此，用户不必担心状态的并发修改。</p>
<h2 id="容错性">容错性</h2>
<p>定时器是容错的，并与应用程序的状态一起检查点。在故障恢复或从保存点启动应用程序时，定时器将被恢复。</p>
<p>注意: 在恢复之前应该启动的检查点处理时间定时器将立即启动。这可能发生在应用程序从故障中恢复或从保存点启动时。</p>
<p>注意: 定时器总是异步检查点，除了 RocksDB 后端/与增量快照/与基于堆的定时器的组合（将用FLINK-10026解决）。注意，大量的定时器会增加检查点时间，因为定时器是检查点状态的一部分。请参阅 &ldquo;定时器凝聚 &ldquo;部分，了解如何减少定时器数量的建议。</p>
<h2 id="定时器凝聚">定时器凝聚</h2>
<p>由于 Flink 对每个键和时间戳只维护一个定时器，您可以通过降低定时器分辨率来凝聚定时器的数量。</p>
<p>对于1秒的定时器分辨率（事件或处理时间），您可以将目标时间四舍五入到整秒。定时器最多会提前1秒发射，但不会晚于要求的毫秒精度。因此，每个键和秒最多有一个定时器。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">coalescedTime</span> <span class="k">=</span> <span class="o">((</span><span class="n">ctx</span><span class="o">.</span><span class="n">timestamp</span> <span class="o">+</span> <span class="n">timeout</span><span class="o">)</span> <span class="o">/</span> <span class="mi">1000</span><span class="o">)</span> <span class="o">*</span> <span class="mi">1000</span>
<span class="n">ctx</span><span class="o">.</span><span class="n">timerService</span><span class="o">.</span><span class="n">registerProcessingTimeTimer</span><span class="o">(</span><span class="n">coalescedTime</span><span class="o">)</span>
</code></pre></div><p>由于事件时间定时器只在有水印出现时才会启动，你也可以通过使用当前的水印来安排和凝聚这些定时器与下一个水印。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">coalescedTime</span> <span class="k">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">timerService</span><span class="o">.</span><span class="n">currentWatermark</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">ctx</span><span class="o">.</span><span class="n">timerService</span><span class="o">.</span><span class="n">registerEventTimeTimer</span><span class="o">(</span><span class="n">coalescedTime</span><span class="o">)</span>
</code></pre></div><p>定时器也可以通过以下方式停止或删除。</p>
<p>停止一个处理时间的定时器。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">timestampOfTimerToStop</span> <span class="k">=</span> <span class="o">...</span>
<span class="n">ctx</span><span class="o">.</span><span class="n">timerService</span><span class="o">.</span><span class="n">deleteProcessingTimeTimer</span><span class="o">(</span><span class="n">timestampOfTimerToStop</span><span class="o">)</span>
</code></pre></div><p>停止事件时间定时器。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">timestampOfTimerToStop</span> <span class="k">=</span> <span class="o">...</span>
<span class="n">ctx</span><span class="o">.</span><span class="n">timerService</span><span class="o">.</span><span class="n">deleteEventTimeTimer</span><span class="o">(</span><span class="n">timestampOfTimerToStop</span><span class="o">)</span>
</code></pre></div><p>注意: 如果没有注册给定时间戳的定时器，停止定时器没有效果。</p>
<p>原文连接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/process_function.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/process_function.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Scala API 扩展]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/?utm_source=atom_feed" rel="related" type="text/html" title="可查询状态" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Scala API Extensions</blockquote><h2 id="scala-api-扩展">Scala API 扩展</h2>
<p>为了在 Scala 和 Java API 之间保持相当程度的一致性，一些允许在 Scala 中进行高级表达的功能被从标准 API 中省略了，包括批处理和流式处理。</p>
<p>如果你想享受完整的 Scala 体验，你可以选择加入通过隐式转换来增强 Scala API 的扩展。</p>
<p>要使用所有可用的扩展，您只需为 DataSet API 添加一个简单的导入即可。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala.extensions._</span>
</code></pre></div><p>或者 DataStream API:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala.extensions._</span>
</code></pre></div><p>另外，你也可以按顺序导入单个扩展，只使用你喜欢的扩展。</p>
<h2 id="接受部分函数">接受部分函数</h2>
<p>通常情况下，DataSet 和 DataStream API 都不接受匿名模式匹配函数来解构 tuple、case 类或集合，比如下面。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">String</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">temperature</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="c1">// [...]
</span><span class="c1"></span>  <span class="c1">// The previous line causes the following compilation error:
</span><span class="c1"></span>  <span class="c1">// &#34;The argument types of an anonymous function must be fully known. (SLS 8.5)&#34;
</span><span class="c1"></span><span class="o">}</span>
</code></pre></div><p>该扩展在 DataSet 和 DataStream Scala API 中引入了新的方法，这些方法在扩展的 API 中具有一对一的对应关系。这些代理方法确实支持匿名模式匹配函数。</p>
<h3 id="dataset-api">DataSet API</h3>
<ul>
<li>mapWith	方法和原来的 map (DataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">value</span><span class="o">.</span><span class="n">toString</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>mapPartitionWith 方法和原来的	mapPartition (DataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapPartitionWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="n">head</span> <span class="o">#</span><span class="k">:</span><span class="kt">:</span> <span class="k">_</span> <span class="o">=&gt;</span> <span class="n">head</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>flatMapWith	方法和原来的 flatMap (DataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">flatMapWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">visitTimes</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">visitTimes</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">name</span> <span class="o">-&gt;</span> <span class="k">_</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>filterWith 方法和原来的	filter (DataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">filterWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="nc">Train</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">isOnTime</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">isOnTime</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>reduceWith 方法和原来的	reduce (DataSet, GroupedDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduceWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">((</span><span class="k">_</span><span class="o">,</span> <span class="n">amount1</span><span class="o">),</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">amount2</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="n">amount1</span> <span class="o">+</span> <span class="n">amount2</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>reduceGroupWith	方法和原来的 reduceGroup (GroupedDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduceGroupWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="n">id</span> <span class="o">#</span><span class="k">:</span><span class="kt">:</span> <span class="kt">value</span> <span class="k">#</span><span class="kt">::</span> <span class="k">_</span> <span class="o">=&gt;</span> <span class="n">id</span> <span class="o">-&gt;</span> <span class="n">value</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>groupingBy 方法和原来的	groupBy (DataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">groupingBy</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">id</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>sortGroupWith	方法和原来的 sortGroup (GroupedDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">grouped</span><span class="o">.</span><span class="n">sortGroupWith</span><span class="o">(</span><span class="nc">Order</span><span class="o">.</span><span class="nc">ASCENDING</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">case</span> <span class="nc">House</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">value</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>combineGroupWith 方法和原来的	combineGroup (GroupedDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">grouped</span><span class="o">.</span><span class="n">combineGroupWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="n">header</span> <span class="o">#</span><span class="k">:</span><span class="kt">:</span> <span class="kt">amounts</span> <span class="o">=&gt;</span> <span class="n">amounts</span><span class="o">.</span><span class="n">sum</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>projecting 方法和原来的	apply (JoinDataSet, CrossDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span>
  <span class="n">whereClause</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="n">pk</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">pk</span><span class="o">).</span>
  <span class="n">isEqualTo</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">fk</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">fk</span><span class="o">).</span>
  <span class="n">projecting</span> <span class="o">{</span>
    <span class="k">case</span> <span class="o">((</span><span class="n">pk</span><span class="o">,</span> <span class="n">tx</span><span class="o">),</span> <span class="o">(</span><span class="n">products</span><span class="o">,</span> <span class="n">fk</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="n">tx</span> <span class="o">-&gt;</span> <span class="n">products</span>
  <span class="o">}</span>

<span class="n">data1</span><span class="o">.</span><span class="n">cross</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span><span class="n">projecting</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="k">_</span><span class="o">),</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">-&gt;</span> <span class="n">b</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>projecting 方法和原来的	apply (CoGroupDataSet)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data1</span><span class="o">.</span><span class="n">coGroup</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span>
  <span class="n">whereClause</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="n">pk</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">pk</span><span class="o">).</span>
  <span class="n">isEqualTo</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">fk</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">fk</span><span class="o">).</span>
  <span class="n">projecting</span> <span class="o">{</span>
    <span class="k">case</span> <span class="o">(</span><span class="n">head1</span> <span class="o">#</span><span class="k">:</span><span class="kt">:</span> <span class="k">_</span><span class="o">,</span> <span class="n">head2</span> <span class="o">#</span><span class="k">:</span><span class="kt">:</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">head1</span> <span class="o">-&gt;</span> <span class="n">head2</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="datastream-api">DataStream API</h3>
<ul>
<li>mapWith	方法和原来的 map (DataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">value</span><span class="o">.</span><span class="n">toString</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>flatMapWith	方法和原来的 flatMap (DataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">flatMapWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">name</span><span class="o">,</span> <span class="n">visits</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">visits</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">name</span> <span class="o">-&gt;</span> <span class="k">_</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>filterWith 方法和原来的	filter (DataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">filterWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="nc">Train</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">isOnTime</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">isOnTime</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>keyingBy 方法和原来的	keyBy (DataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">keyingBy</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">id</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>mapWith 方法和原来的 map (ConnectedDataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">mapWith</span><span class="o">(</span>
  <span class="n">map1</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">value</span><span class="o">.</span><span class="n">toString</span><span class="o">,</span>
  <span class="n">map2</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">,</span> <span class="n">value</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">value</span> <span class="o">+</span> <span class="mi">1</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>flatMapWith	方法和原来的 flatMap (ConnectedDataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">flatMapWith</span><span class="o">(</span>
  <span class="n">flatMap1</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">json</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">parse</span><span class="o">(</span><span class="n">json</span><span class="o">),</span>
  <span class="n">flatMap2</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">,</span> <span class="n">json</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">parse</span><span class="o">(</span><span class="n">json</span><span class="o">)</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>keyingBy 方法和原来的	keyBy (ConnectedDataStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">keyingBy</span><span class="o">(</span>
  <span class="n">key1</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">timestamp</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">timestamp</span><span class="o">,</span>
  <span class="n">key2</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">id</span>
<span class="o">)</span>
</code></pre></div><ul>
<li>reduceWith 方法和原来的 reduce (KeyedStream, WindowedStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">reduceWith</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">((</span><span class="k">_</span><span class="o">,</span> <span class="n">sum1</span><span class="o">),</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">sum2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">sum1</span> <span class="o">+</span> <span class="n">sum2</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>foldWith 方法和原来的	fold (KeyedStream, WindowedStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">foldWith</span><span class="o">(</span><span class="nc">User</span><span class="o">(</span><span class="n">bought</span> <span class="k">=</span> <span class="mi">0</span><span class="o">))</span> <span class="o">{</span>
  <span class="k">case</span> <span class="o">(</span><span class="nc">User</span><span class="o">(</span><span class="n">b</span><span class="o">),</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">items</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="nc">User</span><span class="o">(</span><span class="n">b</span> <span class="o">+</span> <span class="n">items</span><span class="o">.</span><span class="n">size</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><ul>
<li>applyWith	方法和原来的 apply (WindowedStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">applyWith</span><span class="o">(</span><span class="mi">0</span><span class="o">)(</span>
  <span class="n">foldFunction</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="n">sum</span><span class="o">,</span> <span class="n">amount</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">sum</span> <span class="o">+</span> <span class="n">amount</span>
  <span class="n">windowFunction</span> <span class="k">=</span> <span class="k">case</span> <span class="o">(</span><span class="n">k</span><span class="o">,</span> <span class="n">w</span><span class="o">,</span> <span class="n">sum</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="o">)</span>
</code></pre></div><ul>
<li>projecting 方法和原来的	apply (JoinedStream)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">data2</span><span class="o">).</span>
  <span class="n">whereClause</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="n">pk</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">pk</span><span class="o">).</span>
  <span class="n">isEqualTo</span><span class="o">(</span><span class="k">case</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">fk</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">fk</span><span class="o">).</span>
  <span class="n">projecting</span> <span class="o">{</span>
    <span class="k">case</span> <span class="o">((</span><span class="n">pk</span><span class="o">,</span> <span class="n">tx</span><span class="o">),</span> <span class="o">(</span><span class="n">products</span><span class="o">,</span> <span class="n">fk</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="n">tx</span> <span class="o">-&gt;</span> <span class="n">products</span>
  <span class="o">}</span>
</code></pre></div><p>关于每个方法的语义的更多信息，请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">DataSet</a> 和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">DataStream API</a> 文档。</p>
<p>要专门使用这个扩展，可以添加以下导入。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala.extensions.acceptPartialFunctions</span>
</code></pre></div><p>对于 DataSet 扩展和</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions</span>
</code></pre></div><p>下面的代码段展示了一个最小的例子，说明如何一起使用这些扩展方法（与 DataSet API 一起）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">object</span> <span class="nc">Main</span> <span class="o">{</span>
  <span class="k">import</span> <span class="nn">org.apache.flink.api.scala.extensions._</span>
  <span class="k">case</span> <span class="k">class</span> <span class="nc">Point</span><span class="o">(</span><span class="n">x</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">y</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>
  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
    <span class="k">val</span> <span class="n">ds</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="nc">Point</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="nc">Point</span><span class="o">(</span><span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">),</span> <span class="nc">Point</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">6</span><span class="o">))</span>
    <span class="n">ds</span><span class="o">.</span><span class="n">filterWith</span> <span class="o">{</span>
      <span class="k">case</span> <span class="nc">Point</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="o">}.</span><span class="n">reduceWith</span> <span class="o">{</span>
      <span class="k">case</span> <span class="o">(</span><span class="nc">Point</span><span class="o">(</span><span class="n">x1</span><span class="o">,</span> <span class="n">y1</span><span class="o">),</span> <span class="o">(</span><span class="nc">Point</span><span class="o">(</span><span class="n">x2</span><span class="o">,</span> <span class="n">y2</span><span class="o">)))</span> <span class="k">=&gt;</span> <span class="nc">Point</span><span class="o">(</span><span class="n">x1</span> <span class="o">+</span> <span class="n">y1</span><span class="o">,</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">y2</span><span class="o">)</span>
    <span class="o">}.</span><span class="n">mapWith</span> <span class="o">{</span>
      <span class="k">case</span> <span class="nc">Point</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">y</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">y</span><span class="o">)</span>
    <span class="o">}.</span><span class="n">flatMapWith</span> <span class="o">{</span>
      <span class="k">case</span> <span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">y</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&#34;x&#34;</span> <span class="o">-&gt;</span> <span class="n">x</span><span class="o">,</span> <span class="s">&#34;y&#34;</span> <span class="o">-&gt;</span> <span class="n">y</span><span class="o">)</span>
    <span class="o">}.</span><span class="n">groupingBy</span> <span class="o">{</span>
      <span class="k">case</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">id</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/scala_api_extensions.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/scala_api_extensions.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Show 语句]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-show-statements/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Insert 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-sql-hints/?utm_source=atom_feed" rel="related" type="text/html" title="SQL 提示" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-show-statements/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Show Statements</blockquote><h1 id="show-语句">SHOW 语句</h1>
<p>SHOW 语句用于列出所有目录，或列出当前目录中的所有数据库，或列出当前目录和当前数据库中的所有表/视图，或列出当前目录和当前数据库中的所有函数，包括临时系统函数、系统函数、临时目录函数和目录函数。</p>
<p>Flink SQL 目前支持以下 SHOW 语句。</p>
<ul>
<li>SHOW CATALOGS</li>
<li>SHOW DATABASES</li>
<li>SHOW TABLES</li>
<li>SHOW VIEWS</li>
<li>SHOW FUNCTIONS</li>
</ul>
<h2 id="运行-show-语句">运行 SHOW 语句</h2>
<p>SHOW 语句可以用 TableEnvironment 的 executeSql()方法执行，也可以在 SQL CLI 中执行。executeSql()方法会对成功的 SHOW 操作返回对象，否则会抛出一个异常。</p>
<p>下面的例子展示了如何在 TableEnvironment 和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html">SQL CLI</a> 中运行 SHOW 语句。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Flink SQL&gt; SHOW CATALOGS<span class="p">;</span>
default_catalog

Flink SQL&gt; SHOW DATABASES<span class="p">;</span>
default_database

Flink SQL&gt; CREATE TABLE my_table <span class="o">(</span>...<span class="o">)</span> WITH <span class="o">(</span>...<span class="o">)</span><span class="p">;</span>
<span class="o">[</span>INFO<span class="o">]</span> Table has been created.

Flink SQL&gt; SHOW TABLES<span class="p">;</span>
my_table

Flink SQL&gt; CREATE VIEW my_view AS ...<span class="p">;</span>
<span class="o">[</span>INFO<span class="o">]</span> View has been created.

Flink SQL&gt; SHOW VIEWS<span class="p">;</span>
my_view

Flink SQL&gt; SHOW FUNCTIONS<span class="p">;</span>
mod
sha256
...
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>
<span class="k">val</span> <span class="n">tEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// show catalogs
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;SHOW CATALOGS&#34;</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>
<span class="c1">// +-----------------+
</span><span class="c1">// |    catalog name |
</span><span class="c1">// +-----------------+
</span><span class="c1">// | default_catalog |
</span><span class="c1">// +-----------------+
</span><span class="c1"></span>
<span class="c1">// show databases
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;SHOW DATABASES&#34;</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>
<span class="c1">// +------------------+
</span><span class="c1">// |    database name |
</span><span class="c1">// +------------------+
</span><span class="c1">// | default_database |
</span><span class="c1">// +------------------+
</span><span class="c1"></span>
<span class="c1">// create a table
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE my_table (...) WITH (...)&#34;</span><span class="o">)</span>
<span class="c1">// show tables
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;SHOW TABLES&#34;</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>
<span class="c1">// +------------+
</span><span class="c1">// | table name |
</span><span class="c1">// +------------+
</span><span class="c1">// |   my_table |
</span><span class="c1">// +------------+
</span><span class="c1"></span>
<span class="c1">// create a view
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE VIEW my_view AS ...&#34;</span><span class="o">)</span>
<span class="c1">// show views
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;SHOW VIEWS&#34;</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>
<span class="c1">// +-----------+
</span><span class="c1">// | view name |
</span><span class="c1">// +-----------+
</span><span class="c1">// |   my_view |
</span><span class="c1">// +-----------+
</span><span class="c1"></span>
<span class="c1">// show functions
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;SHOW FUNCTIONS&#34;</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>
<span class="c1">// +---------------+
</span><span class="c1">// | function name |
</span><span class="c1">// +---------------+
</span><span class="c1">// |           mod |
</span><span class="c1">// |        sha256 |
</span><span class="c1">// |           ... |
</span><span class="c1">// +---------------+
</span></code></pre></div><h2 id="show-catalogs">SHOW CATALOGS</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SHOW</span><span class="w"> </span><span class="n">CATALOGS</span><span class="w">
</span></code></pre></div><p>显示所有目录。</p>
<h2 id="show-databases">SHOW DATABASES</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SHOW</span><span class="w"> </span><span class="n">DATABASES</span><span class="w">
</span></code></pre></div><p>显示当前目录中的所有数据库。</p>
<h2 id="show-tables">SHOW TABLES</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SHOW</span><span class="w"> </span><span class="n">TABLES</span><span class="w">
</span></code></pre></div><p>显示当前目录和当前数据库中的所有表。</p>
<h2 id="show-views">SHOW VIEWS</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SHOW</span><span class="w"> </span><span class="n">VIEWS</span><span class="w">
</span></code></pre></div><p>显示当前目录和当前数据库中的所有视图。</p>
<h2 id="show-functions">SHOW FUNCTIONS</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SHOW</span><span class="w"> </span><span class="n">FUNCTIONS</span><span class="w">
</span></code></pre></div><p>显示当前目录和当前数据库中的所有功能，包括临时系统功能、系统功能、临时目录功能和目录功能。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/show.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/show.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/sql" term="sql" label="SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SQL]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-sql-overview/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-sql-overview/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>SQL Overview</blockquote><h1 id="sql">SQL</h1>
<p>本页介绍了 Flink 支持的 SQL 语言，包括数据定义语言（DDL）、数据操作语言（DML）和查询语言。Flink 的 SQL 支持是基于 <a href="https://calcite.apache.org/">Apache Calcite</a>，它实现了 SQL 标准。</p>
<p>本页列出了目前 Flink SQL 支持的所有语句。</p>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html">SELECT (Queries)</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/create.html">CREATE TABLE, DATABASE, VIEW, FUNCTION</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/drop.html">DROP TABLE, DATABASE, VIEW, FUNCTION</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/alter.html">ALTER TABLE, DATABASE, FUNCTION</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/insert.html">INSERT</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/hints.html">SQL HINTS</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/describe.html">DESCRIBE</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/explain.html">EXPLAIN</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/use.html">USE</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/show.html">SHOW</a></li>
</ul>
<h2 id="数据类型">数据类型</h2>
<p>请看关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/types.html">数据类型</a>的专门页面。</p>
<p>通用类型和(嵌套的)复合类型(例如 POJOs、tuple、行、Scala case 类)也可以是行的字段。</p>
<p>具有任意嵌套的复合类型的字段可以用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/systemFunctions.html#value-access-functions">值访问函数</a>来访问。</p>
<p>通用类型被当作一个黑盒子，可以通过<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html">用户定义的函数</a>进行传递或处理。</p>
<p>对于 DDL，我们支持页面数据类型中定义的完整<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/types.html">数据类型</a>。</p>
<p>注意事项: 有些数据类型在 SQL 查询中还不支持（即在投递表达式或字元中）。例如：STRING, BYTES, RAW, WITHOUT TIME ZONE 的 TIME(p), WITH LOCAL TIME ZONE 的 TIME(p), WITHOUT TIME ZONE 的 TIMESTAMP(p), WITH LOCAL TIME ZONE 的 TIMESTAMP(p), ARRAY, MULTISET, ROW。</p>
<h2 id="保留关键词">保留关键词</h2>
<p>虽然并不是每一个 SQL 功能都已实现，但有些字符串组合已经被保留为关键字，供将来使用。如果你想使用下面的一个字符串作为字段名，请确保在它们周围加上反引号（例如<code>value</code>，<code>count</code>）。</p>
<pre><code>A, ABS, ABSOLUTE, ACTION, ADA, ADD, ADMIN, AFTER, ALL, ALLOCATE, ALLOW, ALTER, ALWAYS, AND, ANY, ARE, ARRAY, AS, ASC, ASENSITIVE, ASSERTION, ASSIGNMENT, ASYMMETRIC, AT, ATOMIC, ATTRIBUTE, ATTRIBUTES, AUTHORIZATION, AVG, BEFORE, BEGIN, BERNOULLI, BETWEEN, BIGINT, BINARY, BIT, BLOB, BOOLEAN, BOTH, BREADTH, BY, BYTES, C, CALL, CALLED, CARDINALITY, CASCADE, CASCADED, CASE, CAST, CATALOG, CATALOG_NAME, CEIL, CEILING, CENTURY, CHAIN, CHAR, CHARACTER, CHARACTERISTICS, CHARACTERS, CHARACTER_LENGTH, CHARACTER_SET_CATALOG, CHARACTER_SET_NAME, CHARACTER_SET_SCHEMA, CHAR_LENGTH, CHECK, CLASS_ORIGIN, CLOB, CLOSE, COALESCE, COBOL, COLLATE, COLLATION, COLLATION_CATALOG, COLLATION_NAME, COLLATION_SCHEMA, COLLECT, COLUMN, COLUMN_NAME, COMMAND_FUNCTION, COMMAND_FUNCTION_CODE, COMMIT, COMMITTED, CONDITION, CONDITION_NUMBER, CONNECT, CONNECTION, CONNECTION_NAME, CONSTRAINT, CONSTRAINTS, CONSTRAINT_CATALOG, CONSTRAINT_NAME, CONSTRAINT_SCHEMA, CONSTRUCTOR, CONTAINS, CONTINUE, CONVERT, CORR, CORRESPONDING, COUNT, COVAR_POP, COVAR_SAMP, CREATE, CROSS, CUBE, CUME_DIST, CURRENT, CURRENT_CATALOG, CURRENT_DATE, CURRENT_DEFAULT_TRANSFORM_GROUP, CURRENT_PATH, CURRENT_ROLE, CURRENT_SCHEMA, CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_TRANSFORM_GROUP_FOR_TYPE, CURRENT_USER, CURSOR, CURSOR_NAME, CYCLE, DATA, DATABASE, DATE, DATETIME_INTERVAL_CODE, DATETIME_INTERVAL_PRECISION, DAY, DEALLOCATE, DEC, DECADE, DECIMAL, DECLARE, DEFAULT, DEFAULTS, DEFERRABLE, DEFERRED, DEFINED, DEFINER, DEGREE, DELETE, DENSE_RANK, DEPTH, DEREF, DERIVED, DESC, DESCRIBE, DESCRIPTION, DESCRIPTOR, DETERMINISTIC, DIAGNOSTICS, DISALLOW, DISCONNECT, DISPATCH, DISTINCT, DOMAIN, DOUBLE, DOW, DOY, DROP, DYNAMIC, DYNAMIC_FUNCTION, DYNAMIC_FUNCTION_CODE, EACH, ELEMENT, ELSE, END, END-EXEC, EPOCH, EQUALS, ESCAPE, EVERY, EXCEPT, EXCEPTION, EXCLUDE, EXCLUDING, EXEC, EXECUTE, EXISTS, EXP, EXPLAIN, EXTEND, EXTERNAL, EXTRACT, FALSE, FETCH, FILTER, FINAL, FIRST, FIRST_VALUE, FLOAT, FLOOR, FOLLOWING, FOR, FOREIGN, FORTRAN, FOUND, FRAC_SECOND, FREE, FROM, FULL, FUNCTION, FUSION, G, GENERAL, GENERATED, GET, GLOBAL, GO, GOTO, GRANT, GRANTED, GROUP, GROUPING, HAVING, HIERARCHY, HOLD, HOUR, IDENTITY, IMMEDIATE, IMPLEMENTATION, IMPORT, IN, INCLUDING, INCREMENT, INDICATOR, INITIALLY, INNER, INOUT, INPUT, INSENSITIVE, INSERT, INSTANCE, INSTANTIABLE, INT, INTEGER, INTERSECT, INTERSECTION, INTERVAL, INTO, INVOKER, IS, ISOLATION, JAVA, JOIN, K, KEY, KEY_MEMBER, KEY_TYPE, LABEL, LANGUAGE, LARGE, LAST, LAST_VALUE, LATERAL, LEADING, LEFT, LENGTH, LEVEL, LIBRARY, LIKE, LIMIT, LN, LOCAL, LOCALTIME, LOCALTIMESTAMP, LOCATOR, LOWER, M, MAP, MATCH, MATCHED, MAX, MAXVALUE, MEMBER, MERGE, MESSAGE_LENGTH, MESSAGE_OCTET_LENGTH, MESSAGE_TEXT, METHOD, MICROSECOND, MILLENNIUM, MIN, MINUTE, MINVALUE, MOD, MODIFIES, MODULE, MONTH, MORE, MULTISET, MUMPS, NAME, NAMES, NATIONAL, NATURAL, NCHAR, NCLOB, NESTING, NEW, NEXT, NO, NONE, NORMALIZE, NORMALIZED, NOT, NULL, NULLABLE, NULLIF, NULLS, NUMBER, NUMERIC, OBJECT, OCTETS, OCTET_LENGTH, OF, OFFSET, OLD, ON, ONLY, OPEN, OPTION, OPTIONS, OR, ORDER, ORDERING, ORDINALITY, OTHERS, OUT, OUTER, OUTPUT, OVER, OVERLAPS, OVERLAY, OVERRIDING, PAD, PARAMETER, PARAMETER_MODE, PARAMETER_NAME, PARAMETER_ORDINAL_POSITION, PARAMETER_SPECIFIC_CATALOG, PARAMETER_SPECIFIC_NAME, PARAMETER_SPECIFIC_SCHEMA, PARTIAL, PARTITION, PASCAL, PASSTHROUGH, PATH, PERCENTILE_CONT, PERCENTILE_DISC, PERCENT_RANK, PLACING, PLAN, PLI, POSITION, POWER, PRECEDING, PRECISION, PREPARE, PRESERVE, PRIMARY, PRIOR, PRIVILEGES, PROCEDURE, PUBLIC, QUARTER, RANGE, RANK, RAW, READ, READS, REAL, RECURSIVE, REF, REFERENCES, REFERENCING, REGR_AVGX, REGR_AVGY, REGR_COUNT, REGR_INTERCEPT, REGR_R2, REGR_SLOPE, REGR_SXX, REGR_SXY, REGR_SYY, RELATIVE, RELEASE, REPEATABLE, RESET, RESTART, RESTRICT, RESULT, RETURN, RETURNED_CARDINALITY, RETURNED_LENGTH, RETURNED_OCTET_LENGTH, RETURNED_SQLSTATE, RETURNS, REVOKE, RIGHT, ROLE, ROLLBACK, ROLLUP, ROUTINE, ROUTINE_CATALOG, ROUTINE_NAME, ROUTINE_SCHEMA, ROW, ROWS, ROW_COUNT, ROW_NUMBER, SAVEPOINT, SCALE, SCHEMA, SCHEMA_NAME, SCOPE, SCOPE_CATALOGS, SCOPE_NAME, SCOPE_SCHEMA, SCROLL, SEARCH, SECOND, SECTION, SECURITY, SELECT, SELF, SENSITIVE, SEQUENCE, SERIALIZABLE, SERVER, SERVER_NAME, SESSION, SESSION_USER, SET, SETS, SIMILAR, SIMPLE, SIZE, SMALLINT, SOME, SOURCE, SPACE, SPECIFIC, SPECIFICTYPE, SPECIFIC_NAME, SQL, SQLEXCEPTION, SQLSTATE, SQLWARNING, SQL_TSI_DAY, SQL_TSI_FRAC_SECOND, SQL_TSI_HOUR, SQL_TSI_MICROSECOND, SQL_TSI_MINUTE, SQL_TSI_MONTH, SQL_TSI_QUARTER, SQL_TSI_SECOND, SQL_TSI_WEEK, SQL_TSI_YEAR, SQRT, START, STATE, STATEMENT, STATIC, STDDEV_POP, STDDEV_SAMP, STREAM, STRING, STRUCTURE, STYLE, SUBCLASS_ORIGIN, SUBMULTISET, SUBSTITUTE, SUBSTRING, SUM, SYMMETRIC, SYSTEM, SYSTEM_USER, TABLE, TABLESAMPLE, TABLE_NAME, TEMPORARY, THEN, TIES, TIME, TIMESTAMP, TIMESTAMPADD, TIMESTAMPDIFF, TIMEZONE_HOUR, TIMEZONE_MINUTE, TINYINT, TO, TOP_LEVEL_COUNT, TRAILING, TRANSACTION, TRANSACTIONS_ACTIVE, TRANSACTIONS_COMMITTED, TRANSACTIONS_ROLLED_BACK, TRANSFORM, TRANSFORMS, TRANSLATE, TRANSLATION, TREAT, TRIGGER, TRIGGER_CATALOG, TRIGGER_NAME, TRIGGER_SCHEMA, TRIM, TRUE, TYPE, UESCAPE, UNBOUNDED, UNCOMMITTED, UNDER, UNION, UNIQUE, UNKNOWN, UNNAMED, UNNEST, UPDATE, UPPER, UPSERT, USAGE, USER, USER_DEFINED_TYPE_CATALOG, USER_DEFINED_TYPE_CODE, USER_DEFINED_TYPE_NAME, USER_DEFINED_TYPE_SCHEMA, USING, VALUE, VALUES, VARBINARY, VARCHAR, VARYING, VAR_POP, VAR_SAMP, VERSION, VIEW, WEEK, WHEN, WHENEVER, WHERE, WIDTH_BUCKET, WINDOW, WITH, WITHIN, WITHOUT, WORK, WRAPPER, WRITE, XML, YEAR, ZONE
</code></pre>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SQL 客户端]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-sql-client/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-sql-client/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>SQL Client</blockquote><h2 id="sql-client">SQL Client</h2>
<p>Flink 的 Table &amp; SQL API 使得它可以使用 SQL 语言编写的查询，但是这些查询需要嵌入到一个用 Java 或 Scala 编写的表程序中。而且，这些程序在提交给集群之前需要用构建工具打包。这或多或少限制了 Flink 对 Java/Scala 程序员的使用。</p>
<p>SQL Client 旨在提供一种简单的方式来编写、调试和提交表程序到 Flink 集群，而不需要任何一行 Java 或 Scala 代码。SQL Client CLI 允许在命令行上从运行的分布式应用中检索和可视化实时结果。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/sql_client_demo.gif" alt="img"></p>
<h2 id="入门">入门</h2>
<p>本节介绍如何从命令行设置和运行第一个 Flink SQL 程序。</p>
<p>SQL Client 被捆绑在常规的 Flink 发行版中，因此可以开箱即运行。它只需要一个正在运行的 Flink 集群，在那里可以执行表程序。关于设置 Flink 集群的更多信息，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/cluster_setup.html">集群和部署</a>部分。如果你只是想试用 SQL Client，也可以使用下面的命令用一个 worker 启动一个本地集群。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">./bin/start-cluster.sh
</code></pre></div><h3 id="启动-sql-客户端-cli">启动 SQL 客户端 CLI</h3>
<p>SQL Client 脚本也位于 Flink 的二进制目录中。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html#limitations--future">在未来</a>，用户将有两种启动 SQL Client CLI 的可能性，一是通过启动一个嵌入式的独立进程，二是通过连接到一个远程 SQL Client 网关。目前只支持嵌入式模式。你可以通过调用来启动 CLI。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">./bin/sql-client.sh embedded
</code></pre></div><p>默认情况下，SQL 客户端将从位于 <code>./conf/sql-client-defaults.yaml</code> 的环境文件中读取其配置。有关环境文件结构的更多信息，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html#environment-files">配置部分</a>。</p>
<h3 id="运行-sql-查询">运行 SQL 查询</h3>
<p>一旦启动 CLI，您可以使用 HELP 命令列出所有可用的 SQL 语句。为了验证你的设置和集群连接，你可以输入第一个 SQL 查询，然后按回车键执行。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="s1">&#39;Hello World&#39;</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>这个查询不需要表源(table source)，并产生一个单行结果。CLI 将从集群中检索结果，并将其可视化。您可以按Q键关闭结果视图。</p>
<p>CLI 支持三种模式来维护和可视化结果。</p>
<p>table 模式将结果在内存中具体化，并以常规的、分页的表格表示方式将其可视化。可以通过在 CLI 中执行以下命令启用该模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">SET execution.result-mode<span class="o">=</span>table<span class="p">;</span>
</code></pre></div><p><strong>changelog</strong> 模式不将结果具体化，而是将由插入(+)和收回(-)组成的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/dynamic_tables.html#continuous-queries">连续查询</a>所产生的结果流可视化。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">SET execution.result-mode<span class="o">=</span>changelog<span class="p">;</span>
</code></pre></div><p><strong>tableau</strong> 模式更像是传统的方式，将结果以 tableau 的形式直接显示在屏幕上。显示内容会受到查询执行类型(execute.type)的影响。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">SET execution.result-mode<span class="o">=</span>tableau<span class="p">;</span>
</code></pre></div><p>请注意，当您使用此模式进行流式查询时，结果将在控制台中连续打印。如果这个查询的输入数据是有边界的，那么在Flink处理完所有输入数据后，作业就会终止，打印也会自动停止。否则，如果你想终止一个正在运行的查询，在这种情况下只要输入CTRL-C键，作业和打印就会停止。</p>
<p>你可以使用下面的查询来查看所有的结果模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">cnt</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="p">(</span><span class="k">VALUES</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;Bob&#39;</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;Alice&#39;</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;Greg&#39;</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;Bob&#39;</span><span class="p">))</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">NameTable</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="w"> </span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">name</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>这个查询执行一个有界字数的例子。</p>
<p>在 changelog 模式下，可视化的 changelog 应该类似于。</p>
<pre><code>+ Bob, 1
+ Alice, 1
+ Greg, 1
- Bob, 1
+ Bob, 2
</code></pre><p>在 table 模式下，可视化的结果表会持续更新，直到表程序结束。</p>
<pre><code>Bob, 2
Alice, 1
Greg, 1
</code></pre><p>在tableau模式下，如果你在流模式下运行查询，显示的结果将是。</p>
<pre><code>+-----+----------------------+----------------------+
| +/- |                 name |                  cnt |
+-----+----------------------+----------------------+
|   + |                  Bob |                    1 |
|   + |                Alice |                    1 |
|   + |                 Greg |                    1 |
|   - |                  Bob |                    1 |
|   + |                  Bob |                    2 |
+-----+----------------------+----------------------+
Received a total of 5 rows
</code></pre><p>而如果你在批处理模式下运行查询，显示的结果将是。</p>
<pre><code>+-------+-----+
|  name | cnt |
+-------+-----+
| Alice |   1 |
|   Bob |   2 |
|  Greg |   1 |
+-------+-----+
3 rows in set
</code></pre><p>在 SQL 查询的原型设计过程中，所有这些结果模式都是有用的。在所有这些模式中，结果都存储在 SQL 客户端的 Java 堆内存中。为了保持CLI界面的响应性，changelog 模式只显示最新的1000个变化。表模式允许浏览更大的结果，这些结果仅受可用主内存和配置的最大行数（<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html#configuration">max-table-result-rows</a>）的限制。</p>
<p>注意：在批处理环境中执行的查询，只能使用table或tableau结果模式进行检索。</p>
<p>在定义了一个查询后，可以将其作为一个长期运行的、分离的 Flink 作业提交给集群。为此，需要使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html#detached-sql-queries">INSERT INTO 语句</a>指定存储结果的目标系统。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html#configurations">配置部分</a>解释了如何声明 table source 以读取数据，如何声明 table sink 以写入数据，以及如何配置其他表程序属性。</p>
<h2 id="配置">配置</h2>
<p>SQL Client 可以通过以下可选的CLI命令来启动。这些命令将在后面的段落中详细讨论。</p>
<pre><code>./bin/sql-client.sh embedded --help

Mode &quot;embedded&quot; submits Flink jobs from the local machine.

  Syntax: embedded [OPTIONS]
  &quot;embedded&quot; mode options:
     -d,--defaults &lt;environment file&gt;      The environment properties with which
                                           every new session is initialized.
                                           Properties might be overwritten by
                                           session properties.
     -e,--environment &lt;environment file&gt;   The environment properties to be
                                           imported into the session. It might
                                           overwrite default environment
                                           properties.
     -h,--help                             Show the help message with
                                           descriptions of all options.
     -hist,--history &lt;History file path&gt;   The file which you want to save the
                                           command history into. If not
                                           specified, we will auto-generate one
                                           under your user's home directory.
     -j,--jar &lt;JAR file&gt;                   A JAR file to be imported into the
                                           session. The file might contain
                                           user-defined classes needed for the
                                           execution of statements such as
                                           functions, table sources, or sinks.
                                           Can be used multiple times.
     -l,--library &lt;JAR directory&gt;          A JAR file directory with which every
                                           new session is initialized. The files
                                           might contain user-defined classes
                                           needed for the execution of
                                           statements such as functions, table
                                           sources, or sinks. Can be used
                                           multiple times.
     -pyarch,--pyArchives &lt;arg&gt;            Add python archive files for job. The
                                           archive files will be extracted to
                                           the working directory of python UDF
                                           worker. Currently only zip-format is
                                           supported. For each archive file, a
                                           target directory be specified. If the
                                           target directory name is specified,
                                           the archive file will be extracted to
                                           a name can directory with the
                                           specified name. Otherwise, the
                                           archive file will be extracted to a
                                           directory with the same name of the
                                           archive file. The files uploaded via
                                           this option are accessible via
                                           relative path. '#' could be used as
                                           the separator of the archive file
                                           path and the target directory name.
                                           Comma (',') could be used as the
                                           separator to specify multiple archive
                                           files. This option can be used to
                                           upload the virtual environment, the
                                           data files used in Python UDF (e.g.:
                                           --pyArchives
                                           file:///tmp/py37.zip,file:///tmp/data
                                           .zip#data --pyExecutable
                                           py37.zip/py37/bin/python). The data
                                           files could be accessed in Python
                                           UDF, e.g.: f = open('data/data.txt',
                                           'r').
     -pyexec,--pyExecutable &lt;arg&gt;          Specify the path of the python
                                           interpreter used to execute the
                                           python UDF worker (e.g.:
                                           --pyExecutable
                                           /usr/local/bin/python3). The python
                                           UDF worker depends on Python 3.5+,
                                           Apache Beam (version == 2.19.0), Pip
                                           (version &gt;= 7.1.0) and SetupTools
                                           (version &gt;= 37.0.0). Please ensure
                                           that the specified environment meets
                                           the above requirements.
     -pyfs,--pyFiles &lt;pythonFiles&gt;         Attach custom python files for job.
                                           These files will be added to the
                                           PYTHONPATH of both the local client
                                           and the remote python UDF worker. The
                                           standard python resource file
                                           suffixes such as .py/.egg/.zip or
                                           directory are all supported. Comma
                                           (',') could be used as the separator
                                           to specify multiple files (e.g.:
                                           --pyFiles
                                           file:///tmp/myresource.zip,hdfs:///$n
                                           amenode_address/myresource2.zip).
     -pyreq,--pyRequirements &lt;arg&gt;         Specify a requirements.txt file which
                                           defines the third-party dependencies.
                                           These dependencies will be installed
                                           and added to the PYTHONPATH of the
                                           python UDF worker. A directory which
                                           contains the installation packages of
                                           these dependencies could be specified
                                           optionally. Use '#' as the separator
                                           if the optional parameter exists
                                           (e.g.: --pyRequirements
                                           file:///tmp/requirements.txt#file:///
                                           tmp/cached_dir).
     -s,--session &lt;session identifier&gt;     The identifier for a session.
                                           'default' is the default identifier.
     -u,--update &lt;SQL update statement&gt;    Experimental (for testing only!):
                                           Instructs the SQL Client to
                                           immediately execute the given update
                                           statement after starting up. The
                                           process is shut down after the
                                           statement has been submitted to the
                                           cluster and returns an appropriate
                                           return code. Currently, this feature
                                           is only supported for INSERT INTO
                                           statements that declare the target
                                           sink table.

</code></pre><h3 id="环境文件">环境文件</h3>
<p>一个SQL查询需要一个配置环境来执行。所谓的环境文件定义了可用的目录(catalogs)、table source 和 sink、用户定义的函数以及执行和部署所需的其他属性。</p>
<p>每个环境文件都是一个常规的 <a href="http://yaml.org/">YAML 文件</a>。下面是这样一个文件的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="c"># Define tables here such as sources, sinks, views, or temporal tables.</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">tables</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">MyTableSource</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">source-table</span><span class="w">
</span><span class="w">    </span><span class="nt">update-mode</span><span class="p">:</span><span class="w"> </span><span class="l">append</span><span class="w">
</span><span class="w">    </span><span class="nt">connector</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">filesystem</span><span class="w">
</span><span class="w">      </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/path/to/something.csv&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">format</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">csv</span><span class="w">
</span><span class="w">      </span><span class="nt">fields</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">MyField1</span><span class="w">
</span><span class="w">          </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">INT</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">MyField2</span><span class="w">
</span><span class="w">          </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">VARCHAR</span><span class="w">
</span><span class="w">      </span><span class="nt">line-delimiter</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;\n&#34;</span><span class="w">
</span><span class="w">      </span><span class="nt">comment-prefix</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;#&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">schema</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">MyField1</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">INT</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">MyField2</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">VARCHAR</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">MyCustomView</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">view</span><span class="w">
</span><span class="w">    </span><span class="nt">query</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;SELECT MyField2 FROM MyTableSource&#34;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># Define user-defined functions here.</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">functions</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myUDF</span><span class="w">
</span><span class="w">    </span><span class="nt">from</span><span class="p">:</span><span class="w"> </span><span class="l">class</span><span class="w">
</span><span class="w">    </span><span class="nt">class</span><span class="p">:</span><span class="w"> </span><span class="l">foo.bar.AggregateUDF</span><span class="w">
</span><span class="w">    </span><span class="nt">constructor</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="m">7.6</span><span class="w">
</span><span class="w">      </span>- <span class="kc">false</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># Define available catalogs</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">catalogs</span><span class="p">:</span><span class="w">
</span><span class="w">   </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">catalog_1</span><span class="w">
</span><span class="w">     </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">hive</span><span class="w">
</span><span class="w">     </span><span class="nt">property-version</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">     </span><span class="nt">hive-conf-dir</span><span class="p">:</span><span class="w"> </span><span class="l">...</span><span class="w">
</span><span class="w">   </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">catalog_2</span><span class="w">
</span><span class="w">     </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">hive</span><span class="w">
</span><span class="w">     </span><span class="nt">property-version</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">     </span><span class="nt">default-database</span><span class="p">:</span><span class="w"> </span><span class="l">mydb2</span><span class="w">
</span><span class="w">     </span><span class="nt">hive-conf-dir</span><span class="p">:</span><span class="w"> </span><span class="l">...</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># Properties that change the fundamental execution behavior of a table program.</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">execution</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">planner: blink                    # optional</span><span class="p">:</span><span class="w"> </span><span class="l">either &#39;blink&#39; (default) or &#39;old&#39;</span><span class="w">
</span><span class="w">  </span><span class="nt">type: streaming                   # required</span><span class="p">:</span><span class="w"> </span><span class="l">execution mode either &#39;batch&#39; or &#39;streaming&#39;</span><span class="w">
</span><span class="w">  </span><span class="nt">result-mode: table                # required</span><span class="p">:</span><span class="w"> </span><span class="l">either &#39;table&#39; or &#39;changelog&#39;</span><span class="w">
</span><span class="w">  </span><span class="nt">max-table-result-rows: 1000000    # optional</span><span class="p">:</span><span class="w"> </span><span class="l">maximum number of maintained rows in</span><span class="w">
</span><span class="w">                                    </span><span class="c">#   &#39;table&#39; mode (1000000 by default, smaller 1 means unlimited)</span><span class="w">
</span><span class="w">  </span><span class="nt">time-characteristic: event-time   # optional</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;processing-time&#39;</span><span class="w"> </span><span class="l">or &#39;event-time&#39; (default)</span><span class="w">
</span><span class="w">  </span><span class="nt">parallelism: 1                    # optional</span><span class="p">:</span><span class="w"> </span><span class="l">Flink&#39;s parallelism (1 by default)</span><span class="w">
</span><span class="w">  </span><span class="nt">periodic-watermarks-interval: 200 # optional</span><span class="p">:</span><span class="w"> </span><span class="l">interval for periodic watermarks (200 ms by default)</span><span class="w">
</span><span class="w">  </span><span class="nt">max-parallelism: 16               # optional</span><span class="p">:</span><span class="w"> </span><span class="l">Flink&#39;s maximum parallelism (128 by default)</span><span class="w">
</span><span class="w">  </span><span class="nt">min-idle-state-retention: 0       # optional</span><span class="p">:</span><span class="w"> </span><span class="l">table program&#39;s minimum idle state time</span><span class="w">
</span><span class="w">  </span><span class="nt">max-idle-state-retention: 0       # optional</span><span class="p">:</span><span class="w"> </span><span class="l">table program&#39;s maximum idle state time</span><span class="w">
</span><span class="w">  </span><span class="nt">current-catalog: catalog_1        # optional</span><span class="p">:</span><span class="w"> </span><span class="l">name of the current catalog of the session (&#39;default_catalog&#39; by default)</span><span class="w">
</span><span class="w">  </span><span class="nt">current-database: mydb1           # optional</span><span class="p">:</span><span class="w"> </span><span class="l">name of the current database of the current catalog</span><span class="w">
</span><span class="w">                                    </span><span class="c">#   (default database of the current catalog by default)</span><span class="w">
</span><span class="w">  </span><span class="nt">restart-strategy:                 # optional</span><span class="p">:</span><span class="w"> </span><span class="l">restart strategy</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">fallback                 </span><span class="w"> </span><span class="c">#   &#34;fallback&#34; to global restart strategy by default</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># Configuration options for adjusting and tuning table programs.</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># A full list of options and their default values can be found</span><span class="w">
</span><span class="w"></span><span class="c"># on the dedicated &#34;Configuration&#34; page.</span><span class="w">
</span><span class="w"></span><span class="nt">configuration</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">table.optimizer.join-reorder-enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">table.exec.spill-compression.enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">table.exec.spill-compression.block-size</span><span class="p">:</span><span class="w"> </span><span class="l">128kb</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># Properties that describe the cluster to which table programs are submitted to.</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">deployment</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">response-timeout</span><span class="p">:</span><span class="w"> </span><span class="m">5000</span><span class="w">
</span></code></pre></div><p>这份配置:</p>
<ul>
<li>定义了一个表源 <code>MyTableSource</code> 的环境，该表源从 CSV 文件中读取。</li>
<li>定义了一个视图 <code>MyCustomView</code>，该视图使用 SQL 查询声明了一个虚拟表。</li>
<li>定义了一个用户定义的函数 <code>myUDF</code>，可以使用类名和两个构造函数参数来实例化。</li>
<li>连接两个 Hive 目录，并使用 catalog_1 作为当前目录，mydb1 作为目录的当前数据库。</li>
<li>在流式模式下使用 blink planner，运行具有事件时间特性和并行度为1的语句。</li>
<li>在 table 结果模式下运行探索性查询。</li>
<li>并通过配置选项围绕连接重排序和溢出进行一些 planner 调整。</li>
</ul>
<p>根据使用情况，一个配置可以被分割成多个文件。因此，可以为一般目的创建环境文件（使用 <code>--defaults</code> 创建默认环境文件），也可以为每个会话创建环境文件（使用 <code>--environment</code> 创建会话环境文件）。每一个 CLI 会话都会用默认属性和会话属性来初始化。例如，<code>defaults</code> 环境文件可以指定在每个会话中应该可以查询的所有 table source，而 <code>session</code> 环境文件只声明特定的状态保留时间和并行度。在启动CLI应用程序时，可以同时传递默认环境文件和会话环境文件。如果没有指定默认环境文件，SQL Client 会在 Flink 的配置目录下搜索 <code>./conf/sql-client-defaults.yaml</code>。</p>
<p>注意：在 CLI 会话中设置的属性（例如使用SET命令）具有最高优先权。</p>
<pre><code>CLI commands &gt; session environment file &gt; defaults environment file
</code></pre><h3 id="重启策略">重启策略</h3>
<p>重启策略控制 Flink 作业在发生故障时如何重启。与 Flink 集群的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/restart_strategies.html">全局重启策略</a>类似，可以在环境文件中声明一个更精细的重启配置。</p>
<p>支持以下策略。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">execution</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># falls back to the global strategy defined in flink-conf.yaml</span><span class="w">
</span><span class="w">  </span><span class="nt">restart-strategy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">fallback</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># job fails directly and no restart is attempted</span><span class="w">
</span><span class="w">  </span><span class="nt">restart-strategy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">none</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># attempts a given number of times to restart the job</span><span class="w">
</span><span class="w">  </span><span class="nt">restart-strategy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">fixed-delay</span><span class="w">
</span><span class="w">    </span><span class="nt">attempts: 3      # retries before job is declared as failed (default</span><span class="p">:</span><span class="w"> </span><span class="l">Integer.MAX_VALUE)</span><span class="w">
</span><span class="w">    </span><span class="nt">delay: 10000     # delay in ms between retries (default</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="l">s)</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># attempts as long as the maximum number of failures per time interval is not exceeded</span><span class="w">
</span><span class="w">  </span><span class="nt">restart-strategy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">failure-rate</span><span class="w">
</span><span class="w">    </span><span class="nt">max-failures-per-interval: 1   # retries in interval until failing (default</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="l">)</span><span class="w">
</span><span class="w">    </span><span class="nt">failure-rate-interval</span><span class="p">:</span><span class="w"> </span><span class="m">60000</span><span class="w">   </span><span class="c"># measuring interval in ms for failure rate</span><span class="w">
</span><span class="w">    </span><span class="nt">delay: 10000                   # delay in ms between retries (default</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="l">s)</span><span class="w">
</span></code></pre></div><h3 id="依赖性">依赖性</h3>
<p>SQL Client 不需要使用 Maven 或 SBT 设置一个 Java 项目。相反，你可以将依赖关系作为常规的 JAR 文件传递给集群。你可以单独指定每个 JAR 文件（使用 <code>--jar</code>）或定义整个库目录（使用 <code>--library</code>）。对于连接到外部系统（如 Apache Kafka）的连接器和相应的数据格式（如JSON），Flink 提供了现成的 JAR bundles。这些 JAR 文件可以从 Maven 中央仓库为每个版本下载。</p>
<p>所提供的 SQL JAR 的完整列表和关于如何使用它们的文档可以在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connect.html">连接到外部系统页面</a>上找到。</p>
<p>下面的例子显示了一个环境文件，它定义了一个从 Apache Kafka 读取 JSON 数据的 table source。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">tables</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">TaxiRides</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">source-table</span><span class="w">
</span><span class="w">    </span><span class="nt">update-mode</span><span class="p">:</span><span class="w"> </span><span class="l">append</span><span class="w">
</span><span class="w">    </span><span class="nt">connector</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">property-version</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">kafka</span><span class="w">
</span><span class="w">      </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;0.11&#34;</span><span class="w">
</span><span class="w">      </span><span class="nt">topic</span><span class="p">:</span><span class="w"> </span><span class="l">TaxiRides</span><span class="w">
</span><span class="w">      </span><span class="nt">startup-mode</span><span class="p">:</span><span class="w"> </span><span class="l">earliest-offset</span><span class="w">
</span><span class="w">      </span><span class="nt">properties</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">bootstrap.servers</span><span class="p">:</span><span class="w"> </span><span class="l">localhost:9092</span><span class="w">
</span><span class="w">        </span><span class="nt">group.id</span><span class="p">:</span><span class="w"> </span><span class="l">testGroup</span><span class="w">
</span><span class="w">    </span><span class="nt">format</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">property-version</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">json</span><span class="w">
</span><span class="w">      </span><span class="nt">schema</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;ROW&lt;rideId LONG, lon FLOAT, lat FLOAT, rideTime TIMESTAMP&gt;&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">schema</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">rideId</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">BIGINT</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">lon</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">FLOAT</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">lat</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">FLOAT</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">rowTime</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">TIMESTAMP(3)</span><span class="w">
</span><span class="w">        </span><span class="nt">rowtime</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">timestamps</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;from-field&#34;</span><span class="w">
</span><span class="w">            </span><span class="nt">from</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;rideTime&#34;</span><span class="w">
</span><span class="w">          </span><span class="nt">watermarks</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;periodic-bounded&#34;</span><span class="w">
</span><span class="w">            </span><span class="nt">delay</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;60000&#34;</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">procTime</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">TIMESTAMP(3)</span><span class="w">
</span><span class="w">        </span><span class="nt">proctime</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></code></pre></div><p>由此产生的 <code>TaxiRide</code> 表的模式(schema)包含了 JSON 模式(schema)的大部分字段。此外，它还增加了一个行时间属性 <code>rowTime</code> 和处理时间属性 <code>procTime</code>。</p>
<p><code>connector</code> 和 <code>format</code> 都允许定义一个属性版本（目前是版本1），以便于将来向后兼容。</p>
<h3 id="用户定义的函数">用户定义的函数</h3>
<p>SQL Client 允许用户创建自定义的、用户定义的函数，以便在 SQL 查询中使用。目前，这些函数被限制在 Java/Scala 类或 Python  文件中以编程方式定义。</p>
<p>为了提供一个 Java/Scala 用户定义的函数，你需要首先实现和编译一个扩展 ScalarFunction、AggregateFunction 或 TableFunction 的函数类（见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html">用户定义的函数</a>）。然后可以将一个或多个函数打包到 SQL Client 的依赖 JAR 中。</p>
<p>为了提供一个 Python 用户定义函数，你需要编写一个 Python 函数，并用 <code>pyflink.table.udf.udf</code> 或 <code>pyflink.table.udf.udtf</code> 装饰器来装饰它(见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/python/user-guide/table/udfs/python_udfs.html">Python UDFs</a>)。然后可以将一个或多个函数放入一个 Python 文件中。Python 文件和相关的依赖关系需要通过环境文件中的配置 (参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/python/user-guide/table/python_config.html">Python 配置</a>) 或命令行选项 (参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/cli.html#usage">命令行用法</a>) 来指定。</p>
<p>所有函数在被调用之前必须在环境文件中声明。对于函数列表中的每一项，必须指定</p>
<ul>
<li>注册该函数的名称。</li>
<li>函数的源，使用 <code>from</code> (暂时限制为 class (Java/Scala UDF) 或 python (Python UDF))。</li>
</ul>
<p>Java/Scala UDF 必须指定:</p>
<ul>
<li><code>class</code>，表示函数的完全限定类名和一个可选的实例化构造参数列表。</li>
</ul>
<p>Python 的 UDF 必须指定:</p>
<ul>
<li><code>fully-qualified-name</code>: 表示完全限定的名称，即函数的&quot;[模块名].[对象名]&quot;。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">functions</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name: java_udf               # required</span><span class="p">:</span><span class="w"> </span><span class="l">name of the function</span><span class="w">
</span><span class="w">    </span><span class="nt">from: class                  # required</span><span class="p">:</span><span class="w"> </span><span class="l">source of the function</span><span class="w">
</span><span class="w">    </span><span class="nt">class: ...                   # required</span><span class="p">:</span><span class="w"> </span><span class="l">fully qualified class name of the function</span><span class="w">
</span><span class="w">    </span><span class="nt">constructor:                 # optional</span><span class="p">:</span><span class="w"> </span><span class="l">constructor parameters of the function class</span><span class="w">
</span><span class="w">      </span>- <span class="nt">...                      # optional</span><span class="p">:</span><span class="w"> </span><span class="l">a literal parameter with implicit type</span><span class="w">
</span><span class="w">      </span>- <span class="nt">class: ...               # optional</span><span class="p">:</span><span class="w"> </span><span class="l">full class name of the parameter</span><span class="w">
</span><span class="w">        </span><span class="nt">constructor:             # optional</span><span class="p">:</span><span class="w"> </span><span class="l">constructor parameters of the parameter&#39;s class</span><span class="w">
</span><span class="w">          </span>- <span class="nt">type: ...            # optional</span><span class="p">:</span><span class="w"> </span><span class="l">type of the literal parameter</span><span class="w">
</span><span class="w">            </span><span class="nt">value: ...           # optional</span><span class="p">:</span><span class="w"> </span><span class="l">value of the literal parameter</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name: python_udf             # required</span><span class="p">:</span><span class="w"> </span><span class="l">name of the function</span><span class="w">
</span><span class="w">    </span><span class="nt">from: python                 # required</span><span class="p">:</span><span class="w"> </span><span class="l">source of the function </span><span class="w">
</span><span class="w">    </span><span class="nt">fully-qualified-name: ...    # required</span><span class="p">:</span><span class="w"> </span><span class="l">fully qualified class name of the function      </span><span class="w">
</span></code></pre></div><p>对于 Java/Scala UDF，请确保指定参数的顺序和类型严格匹配你的函数类的一个构造函数。</p>
<h3 id="构造函数参数">构造函数参数</h3>
<p>根据用户定义的函数，在 SQL 语句中使用它之前，可能需要对实现进行参数化。</p>
<p>如前面的例子所示，在声明用户定义函数时，可以通过以下三种方式之一使用构造函数参数来配置类。</p>
<p>一个隐含类型的字面值。SQL Client 会根据字面值本身自动推导出类型。目前，这里只支持 BOOLEAN、INT、DOUBLE 和 VARCHAR 的值。如果自动推导没有达到预期的效果（例如，你需要一个 VARCHAR false），请使用显式类型代替。</p>
<pre><code>- true         # -&gt; BOOLEAN (case sensitive)
- 42           # -&gt; INT
- 1234.222     # -&gt; DOUBLE
- foo          # -&gt; VARCHAR
</code></pre><p>一个具有明确类型的字面值。明确声明参数的 <code>type</code> 和 <code>value</code> 属性，以保证类型安全。</p>
<pre><code>- type: DECIMAL
  value: 11111111111111111
</code></pre><p>下表说明了支持的 Java 参数类型和相应的 SQL 类型字符串。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Java type</th>
<th style="text-align:left">SQL type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">java.math.BigDecimal</td>
<td style="text-align:left">DECIMAL</td>
</tr>
<tr>
<td style="text-align:left">java.lang.Boolean</td>
<td style="text-align:left">BOOLEAN</td>
</tr>
<tr>
<td style="text-align:left">java.lang.Byte</td>
<td style="text-align:left">TINYINT</td>
</tr>
<tr>
<td style="text-align:left">java.lang.Double</td>
<td style="text-align:left">DOUBLE</td>
</tr>
<tr>
<td style="text-align:left">java.lang.Float</td>
<td style="text-align:left">REAL, FLOAT</td>
</tr>
<tr>
<td style="text-align:left">java.lang.Integer</td>
<td style="text-align:left">INTEGER, INT</td>
</tr>
<tr>
<td style="text-align:left">java.lang.Long</td>
<td style="text-align:left">BIGINT</td>
</tr>
<tr>
<td style="text-align:left">java.lang.Short</td>
<td style="text-align:left">SMALLINT</td>
</tr>
<tr>
<td style="text-align:left">java.lang.String</td>
<td style="text-align:left">VARCHAR</td>
</tr>
</tbody>
</table>
<p>目前还不支持更多的类型（如 TIMESTAMP 或 ARRAY）、原语类型和 null。</p>
<p>一个（嵌套的）类实例。除了字面值，你还可以通过指定 <code>class</code> 和 <code>constructor</code> 函数属性，为构造函数参数创建（嵌套）类实例。这个过程可以递归执行，直到所有的构造参数都用字面值表示。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml">- <span class="nt">class</span><span class="p">:</span><span class="w"> </span><span class="l">foo.bar.paramClass</span><span class="w">
</span><span class="w">  </span><span class="nt">constructor</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">StarryName</span><span class="w">
</span><span class="w">    </span>- <span class="nt">class</span><span class="p">:</span><span class="w"> </span><span class="l">java.lang.Integer</span><span class="w">
</span><span class="w">      </span><span class="nt">constructor</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">class</span><span class="p">:</span><span class="w"> </span><span class="l">java.lang.String</span><span class="w">
</span><span class="w">          </span><span class="nt">constructor</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">VARCHAR</span><span class="w">
</span><span class="w">              </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span></code></pre></div><h3 id="catalogs">Catalogs</h3>
<p>Catalogs 可以定义为一组 YAML 属性，在启动 SQL Client 时自动注册到环境中。</p>
<p>用户可以在 SQL CLI 中指定要使用哪个目录(catalog)作为当前目录(catalog)，以及要使用该目录(catalog)的哪个数据库作为当前数据库。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">catalogs</span><span class="p">:</span><span class="w">
</span><span class="w">   </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">catalog_1</span><span class="w">
</span><span class="w">     </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">hive</span><span class="w">
</span><span class="w">     </span><span class="nt">property-version</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">     </span><span class="nt">default-database</span><span class="p">:</span><span class="w"> </span><span class="l">mydb2</span><span class="w">
</span><span class="w">     </span><span class="nt">hive-conf-dir</span><span class="p">:</span><span class="w"> </span><span class="l">&lt;path of Hive conf directory&gt;</span><span class="w">
</span><span class="w">   </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">catalog_2</span><span class="w">
</span><span class="w">     </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">hive</span><span class="w">
</span><span class="w">     </span><span class="nt">property-version</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">     </span><span class="nt">hive-conf-dir</span><span class="p">:</span><span class="w"> </span><span class="l">&lt;path of Hive conf directory&gt;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">execution</span><span class="p">:</span><span class="w">
</span><span class="w">   </span><span class="l">...</span><span class="w">
</span><span class="w">   </span><span class="nt">current-catalog</span><span class="p">:</span><span class="w"> </span><span class="l">catalog_1</span><span class="w">
</span><span class="w">   </span><span class="nt">current-database</span><span class="p">:</span><span class="w"> </span><span class="l">mydb1</span><span class="w">
</span></code></pre></div><p>关于目录的更多信息，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/catalogs.html">目录</a>。</p>
<h2 id="分离式-sql-查询">分离式 SQL 查询</h2>
<p>为了定义端到端 SQL 管道，SQL 的 INSERT INTO 语句可以用于向 Flink 集群提交长期运行的、分离的查询。这些查询将其结果产生到外部系统中，而不是 SQL Client。这允许处理更高的并行性和更大的数据量。CLI 本身对提交后的分离查询没有任何控制。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">INSERT</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="n">MyTableSink</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">MyTableSource</span><span class="w">
</span></code></pre></div><p>表接收器 <code>MyTableSink</code> 必须在环境文件中声明。有关支持的外部系统及其配置的更多信息，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connect.html">连接页面</a>。下面是一个 Apache Kafka 表接收器的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">tables</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">MyTableSink</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">sink-table</span><span class="w">
</span><span class="w">    </span><span class="nt">update-mode</span><span class="p">:</span><span class="w"> </span><span class="l">append</span><span class="w">
</span><span class="w">    </span><span class="nt">connector</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">property-version</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">kafka</span><span class="w">
</span><span class="w">      </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;0.11&#34;</span><span class="w">
</span><span class="w">      </span><span class="nt">topic</span><span class="p">:</span><span class="w"> </span><span class="l">OutputTopic</span><span class="w">
</span><span class="w">      </span><span class="nt">properties</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">bootstrap.servers</span><span class="p">:</span><span class="w"> </span><span class="l">localhost:9092</span><span class="w">
</span><span class="w">        </span><span class="nt">group.id</span><span class="p">:</span><span class="w"> </span><span class="l">testGroup</span><span class="w">
</span><span class="w">    </span><span class="nt">format</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">property-version</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">json</span><span class="w">
</span><span class="w">      </span><span class="nt">derive-schema</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">schema</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">rideId</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">BIGINT</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">lon</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">FLOAT</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">lat</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">FLOAT</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">rideTime</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">TIMESTAMP(3)</span><span class="w">
</span></code></pre></div><p>SQL 客户端确保语句成功提交到集群。一旦提交查询，CLI 将显示有关 Flink 作业的信息。</p>
<pre><code>[INFO] Table update statement has been successfully submitted to the cluster:
Cluster ID: StandaloneClusterId
Job ID: 6f922fe5cba87406ff23ae4a7bb79044
Web interface: http://localhost:8081
</code></pre><p>注意: SQL 客户端在提交后不会跟踪正在运行的 Flink 作业的状态。CLI 进程可以在提交后被关闭，而不影响分离查询。Flink 的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/restart_strategies.html">重启策略</a>照顾到了容错。可以使用 Flink 的 Web 界面、命令行或 REST API 来取消查询。</p>
<h2 id="sql-视图">SQL 视图</h2>
<p>视图允许从 SQL 查询中定义虚拟表。视图定义会被立即解析和验证。然而，实际的执行发生在提交一般的 INSERT INTO 或 SELECT 语句期间访问视图时。</p>
<p>视图可以在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html#environment-files">环境文件</a>中定义，也可以在 CLI 会话中定义。</p>
<p>下面的例子显示了如何在一个文件中定义多个视图。视图是按照它们在环境文件中定义的顺序注册的。支持诸如视图 A 依赖于视图 B 依赖于视图 C 的引用链。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">tables</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">MyTableSource</span><span class="w">
</span><span class="w">    </span><span class="c"># ...</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">MyRestrictedView</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">view</span><span class="w">
</span><span class="w">    </span><span class="nt">query</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;SELECT MyField2 FROM MyTableSource&#34;</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">MyComplexView</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">view</span><span class="w">
</span><span class="w">    </span><span class="nt">query</span><span class="p">:</span><span class="w"> </span><span class="p">&gt;</span><span class="sd">
</span><span class="sd">      SELECT MyField2 + 42, CAST(MyField1 AS VARCHAR)
</span><span class="sd">      FROM MyTableSource
</span><span class="sd">      WHERE MyField2 &gt; 200</span><span class="w">      
</span></code></pre></div><p>与 table source 和 sink 类似，会话环境文件中定义的视图具有最高优先级。</p>
<p>视图也可以在 CLI 会话中使用 <code>CREATE VIEW</code> 语句创建。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="n">MyNewView</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="n">MyField2</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">MyTableSource</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>在 CLI 会话中创建的视图也可以使用 <code>DROP VIEW</code> 语句再次删除。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">DROP</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="n">MyNewView</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>注意: CLI 中视图的定义仅限于上述语法。在未来的版本中，将支持为视图定义模式或在表名中转义空格。</p>
<h2 id="临时表">临时表</h2>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">临时表</a>允许对变化的历史表进行（参数化的）查看，该表在特定的时间点返回一个表的内容。这对于将一个表与另一个表在特定时间戳的内容连接起来特别有用。更多信息可以在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html#join-with-a-temporal-table">临时表连接</a>页面中找到。</p>
<p>下面的示例展示了如何定义一个临时表 <code>SourceTemporalTable</code>。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">tables</span><span class="p">:</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># Define the table source (or view) that contains updates to a temporal table</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">HistorySource</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">source-table</span><span class="w">
</span><span class="w">    </span><span class="nt">update-mode</span><span class="p">:</span><span class="w"> </span><span class="l">append</span><span class="w">
</span><span class="w">    </span><span class="nt">connector</span><span class="p">:</span><span class="w"> </span><span class="c"># ...</span><span class="w">
</span><span class="w">    </span><span class="nt">format</span><span class="p">:</span><span class="w"> </span><span class="c"># ...</span><span class="w">
</span><span class="w">    </span><span class="nt">schema</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">integerField</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">INT</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">stringField</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">STRING</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">rowtimeField</span><span class="w">
</span><span class="w">        </span><span class="nt">data-type</span><span class="p">:</span><span class="w"> </span><span class="l">TIMESTAMP(3)</span><span class="w">
</span><span class="w">        </span><span class="nt">rowtime</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">timestamps</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">from-field</span><span class="w">
</span><span class="w">            </span><span class="nt">from</span><span class="p">:</span><span class="w"> </span><span class="l">rowtimeField</span><span class="w">
</span><span class="w">          </span><span class="nt">watermarks</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">from-source</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># Define a temporal table over the changing history table with time attribute and primary key</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">SourceTemporalTable</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">temporal-table</span><span class="w">
</span><span class="w">    </span><span class="nt">history-table</span><span class="p">:</span><span class="w"> </span><span class="l">HistorySource</span><span class="w">
</span><span class="w">    </span><span class="nt">primary-key</span><span class="p">:</span><span class="w"> </span><span class="l">integerField</span><span class="w">
</span><span class="w">    </span><span class="nt">time-attribute</span><span class="p">:</span><span class="w"> </span><span class="l">rowtimeField </span><span class="w"> </span><span class="c"># could also be a proctime field</span><span class="w">
</span></code></pre></div><p>如示例中所示，table source、视图和临时表的定义可以相互混合。它们按照在环境文件中定义的顺序进行注册。例如，一个临时表可以引用一个视图，该视图可以依赖于另一个视图或 table source。</p>
<h2 id="限制和未来">限制和未来</h2>
<p>目前的 SQL Client 只支持嵌入式模式。未来，社区计划通过提供基于 REST 的 SQL Client 网关来扩展其功能，详见 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-24+-+SQL+Client">FLIP-24</a> 和 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-91%3A+Support+SQL+Client+Gateway">FLIP-91</a>。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SQL 提示]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-sql-hints/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Insert 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-show-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Show 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-sql-hints/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>SQL Hints</blockquote><h1 id="sql-提示">SQL 提示</h1>
<p>SQL 提示可以与 SQL 语句一起使用，以改变执行计划。本章解释了如何使用提示来强制各种方法。</p>
<p>一般来说，一个提示可以用来。</p>
<p>强制执行计划器：没有完美的计划器，所以实现提示让用户更好地控制执行是有意义的。
Append meta data(或统计)：一些统计，比如&quot;扫描的表索引&quot;和 &ldquo;一些 shuffle 键的 skew info&rdquo;，对于查询来说是有些动态的，用提示来配置它们会非常方便，因为我们从 planner 得到的规划元数据往往不是那么准确。
运算符资源约束：对于很多情况，我们会给执行运算符一个默认的资源配置，比如最小并行或管理内存（耗费资源的 UDF）或特殊的资源需求（GPU 或 SSD 磁盘）等等，用提示对每个查询（而不是 Job）的资源进行配置会非常灵活。</p>
<h2 id="动态表选项">动态表选项</h2>
<p>动态表选项允许动态指定或覆盖表选项，与 SQL DDL 或连接 API 定义的静态表选项不同，这些选项可以在每个查询中的每个表范围内灵活指定。</p>
<p>因此，它非常适用于交互式终端的临时查询，例如，在 SQL-CLI 中，只需添加一个动态选项 <code>/*+ OPTIONS('csv.ignore-parse-errors'='true') */</code>，就可以指定忽略 CSV 源的解析错误。</p>
<p>注意：动态表选项默认是禁止使用的，因为它可能会改变查询的语义。您需要将配置选项 table.dynamic-table-options.enabled 显式地设置为 true（默认为 false），有关如何设置配置选项的详细信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/config.html">配置</a>。</p>
<h3 id="语法">语法</h3>
<p>为了不破坏 SQL 的兼容性，我们使用 Oracle 风格的 SQL 提示语法。</p>
<pre><code>table_path /*+ OPTIONS(key=val [, key=val]*) */

key:
    stringLiteral
val:
    stringLiteral
</code></pre><h3 id="例子">例子</h3>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">kafka_table1</span><span class="w"> </span><span class="p">(</span><span class="n">id</span><span class="w"> </span><span class="nb">BIGINT</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w"> </span><span class="n">age</span><span class="w"> </span><span class="nb">INT</span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(...);</span><span class="w">
</span><span class="w"></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">kafka_table2</span><span class="w"> </span><span class="p">(</span><span class="n">id</span><span class="w"> </span><span class="nb">BIGINT</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w"> </span><span class="n">age</span><span class="w"> </span><span class="nb">INT</span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(...);</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c1">-- override table options in query source
</span><span class="c1"></span><span class="k">select</span><span class="w"> </span><span class="n">id</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">kafka_table1</span><span class="w"> </span><span class="cm">/*+ OPTIONS(&#39;scan.startup.mode&#39;=&#39;earliest-offset&#39;) */</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c1">-- override table options in join
</span><span class="c1"></span><span class="k">select</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">from</span><span class="w">
</span><span class="w">    </span><span class="n">kafka_table1</span><span class="w"> </span><span class="cm">/*+ OPTIONS(&#39;scan.startup.mode&#39;=&#39;earliest-offset&#39;) */</span><span class="w"> </span><span class="n">t1</span><span class="w">
</span><span class="w">    </span><span class="k">join</span><span class="w">
</span><span class="w">    </span><span class="n">kafka_table2</span><span class="w"> </span><span class="cm">/*+ OPTIONS(&#39;scan.startup.mode&#39;=&#39;earliest-offset&#39;) */</span><span class="w"> </span><span class="n">t2</span><span class="w">
</span><span class="w">    </span><span class="k">on</span><span class="w"> </span><span class="n">t1</span><span class="p">.</span><span class="n">id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t2</span><span class="p">.</span><span class="n">id</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c1">-- override table options for INSERT target table
</span><span class="c1"></span><span class="k">insert</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">kafka_table1</span><span class="w"> </span><span class="cm">/*+ OPTIONS(&#39;sink.partitioner&#39;=&#39;round-robin&#39;) */</span><span class="w"> </span><span class="k">select</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">kafka_table2</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/hints.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/hints.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/sql" term="sql" label="SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Table API]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-table-api/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-table-api/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Table API</blockquote><h1 id="table-api">Table API</h1>
<p>表 API 是一个统一的关系型 API，用于流处理和批处理。Table API 查询可以在批处理或流处理输入上运行，无需修改。Table API 是 SQL 语言的超级集，是专门为 Apache Flink 工作而设计的。Table API 是 Scala、Java 和 Python 的语言集成 API。Table API 查询不是像 SQL 那样以字符串值的方式指定查询，而是以语言嵌入的方式在 Java、Scala 或 Python 中定义查询，并支持自动完成和语法验证等 IDE。</p>
<p>Table API 与 Flink 的 SQL 集成共享许多概念和部分 API。请看<a href="https://ohmyweekly.github.io/notes/2020-08-22-streaming-concepts/">通用概念</a>和 API，了解如何注册表或创建表对象。<a href="https://ohmyweekly.github.io/notes/2020-08-22-streaming-concepts/">流概念</a>页面讨论了流的具体概念，如动态表和<a href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes">时间属性</a>。</p>
<p>下面的例子假设一个名为 Orders 的注册表具有属性（a, b, c, rowtime）。rowtime 字段在流式中是一个逻辑<a href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes">时间属性</a>，在批处理中是一个常规的时间戳字段。</p>
<h2 id="概述和示例">概述和示例</h2>
<p>Table API 可用于 Scala、Java 和 Python。Scala Table API 利用的是 Scala 表达式，Java Table API 既支持 Expression DSL，也支持解析并转换为等价表达式的字符串，Python Table API 目前只支持解析并转换为等价表达式的字符串。</p>
<p>下面的例子显示了 Scala、Java 和 Python Table API 之间的差异。表程序是在批处理环境中执行的。它扫描 Orders 表，按字段 a 分组，并计算每组的结果行。</p>
<p>通过导入 <code>org.apache.flink.table.api._</code>、<code>org.apache.flink.api.scala._</code> 和 <code>org.apache.flink.table.api.bridge.scala._</code>（用于桥接到/来自 DataStream）来启用 Scala Table API。</p>
<p>下面的例子展示了 Scala Table API 程序是如何构造的。表字段使用 Scala 的字符串插值，使用美元字符（<code>$</code>）引用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.bridge.scala._</span>

<span class="c1">// environment configuration
</span><span class="c1"></span><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">tEnv</span> <span class="k">=</span> <span class="nc">BatchTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// register Orders table in table environment
</span><span class="c1">// ...
</span><span class="c1"></span>
<span class="c1">// specify table program
</span><span class="c1"></span><span class="k">val</span> <span class="n">orders</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span> <span class="c1">// schema (a, b, c, rowtime)
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span>
               <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">)</span>
               <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">count</span> <span class="n">as</span> <span class="s">&#34;cnt&#34;</span><span class="o">)</span>
               <span class="o">.</span><span class="n">toDataSet</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="c1">// conversion to DataSet
</span><span class="c1"></span>               <span class="o">.</span><span class="n">print</span><span class="o">()</span>
</code></pre></div><p>下一个例子显示了一个更复杂的 Table API 程序。该程序再次扫描 Orders 表，过滤空值，对类型为 String 的字段 a 进行归一化处理，并为每个小时和产品 a 计算平均计费金额 b。它过滤空值，对类型为 String 的字段 a 进行标准化，并为每个小时和产品 a 计算平均账单金额 b。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// environment configuration
</span><span class="c1">// ...
</span><span class="c1"></span>
<span class="c1">// specify table program
</span><span class="c1"></span><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span> <span class="c1">// schema (a, b, c, rowtime)
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">orders</span>
        <span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">.</span><span class="n">isNotNull</span> <span class="o">&amp;&amp;</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">isNotNull</span> <span class="o">&amp;&amp;</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">.</span><span class="n">isNotNull</span><span class="o">)</span>
        <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">.</span><span class="n">lowerCase</span><span class="o">()</span> <span class="n">as</span> <span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span><span class="o">)</span>
        <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">1.</span><span class="n">hour</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">as</span> <span class="s">&#34;hourlyWindow&#34;</span><span class="o">)</span>
        <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;hourlyWindow&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">)</span>
        <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;hourlyWindow&#34;</span><span class="o">.</span><span class="n">end</span> <span class="n">as</span> <span class="s">&#34;hour&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">avg</span> <span class="n">as</span> <span class="s">&#34;avgBillingAmount&#34;</span><span class="o">)</span>
</code></pre></div><p>由于表 API 是针对批处理和流数据的统一 API，所以这两个示例程序都可以在批处理和流输入上执行，而不需要对表程序本身进行任何修改。在这两种情况下，考虑到流式记录不会迟到，程序会产生相同的结果（详见<a href="https://ohmyweekly.github.io/notes/2020-08-22-streaming-concepts/">流概念</a>）。</p>
<h2 id="操作">操作</h2>
<p>表 API 支持以下操作。请注意，并不是所有的操作都能在批处理和流式处理中使用，它们都有相应的标签。</p>
<h3 id="scan-projection-和-filter">Scan, Projection 和 Filter</h3>
<ul>
<li>From(Batch/Streaming)</li>
</ul>
<p>类似于 SQL 查询中的 FROM 子句。执行对注册的表的扫描。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Values(Batch/Streaming)</li>
</ul>
<p>类似于 SQL 查询中的 VALUES 子句。从提供的行中产生一个内联表。</p>
<p>你可以使用 <code>row(...)</code> 表达式来创建复合行。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">fromValues</span><span class="o">(</span>
   <span class="n">row</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="s">&#34;ABC&#34;</span><span class="o">),</span>
   <span class="n">row</span><span class="o">(</span><span class="mi">2L</span><span class="o">,</span> <span class="s">&#34;ABCDE&#34;</span><span class="o">)</span>
<span class="o">)</span>
</code></pre></div><p>将产生一个模式(schema)如下的表。</p>
<pre><code>root
|-- f0: BIGINT NOT NULL     // original types INT and BIGINT are generalized to BIGINT
|-- f1: VARCHAR(5) NOT NULL // original types CHAR(3) and CHAR(5) are generalized
                            // to VARCHAR(5). VARCHAR is used instead of CHAR so that
                            // no padding is applied
</code></pre><p>该方法将从输入的表达式中自动得出类型，如果某个位置的类型不同，该方法将尝试为所有类型找到共同的超级类型。如果某个位置的类型不同，方法将尝试为所有类型找到一个共同的超级类型。如果一个共同的超级类型不存在，将抛出一个异常。</p>
<p>您也可以明确地指定请求的类型。这可能对分配更多的通用类型（如 DECIMAL）或为列命名很有帮助。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">fromValues</span><span class="o">(</span>
    <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">ROW</span><span class="o">(</span>
        <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">FIELD</span><span class="o">(</span><span class="s">&#34;id&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">DECIMAL</span><span class="o">(</span><span class="mi">10</span><span class="o">,</span> <span class="mi">2</span><span class="o">)),</span>
        <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">FIELD</span><span class="o">(</span><span class="s">&#34;name&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">STRING</span><span class="o">())</span>
    <span class="o">),</span>
    <span class="n">row</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="s">&#34;ABC&#34;</span><span class="o">),</span>
    <span class="n">row</span><span class="o">(</span><span class="mi">2L</span><span class="o">,</span> <span class="s">&#34;ABCDE&#34;</span><span class="o">)</span>
<span class="o">)</span>
</code></pre></div><p>将产生一个模式(schema)如下的表。</p>
<pre><code>root
|-- id: DECIMAL(10, 2)
|-- name: STRING
</code></pre><ul>
<li>Select(Batch/Streaming)</li>
</ul>
<p>类似于 SQL SELECT 语句。执行选择操作。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span> <span class="n">as</span> <span class="s">&#34;d&#34;</span><span class="o">)</span>
</code></pre></div><p>你可以使用星号（<code>*</code>）作为通配符，选择表中所有的列。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;*&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>As(Batch/Streaming)</li>
</ul>
<p>重新命名字段。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;x&#34;</span><span class="o">,</span> <span class="s">&#34;y&#34;</span><span class="o">,</span> <span class="s">&#34;z&#34;</span><span class="o">,</span> <span class="s">&#34;t&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Where / Filter(Batch/Streaming)</li>
</ul>
<p>类似于 SQL WHERE 子句。过滤掉没有通过过滤谓词的记录。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">===</span> <span class="mi">0</span><span class="o">)</span>
</code></pre></div><p>或:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span><span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;b&#34;</span> <span class="o">===</span> <span class="s">&#34;red&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="列操作">列操作</h3>
<ul>
<li>AddColumns(Batch/Streaming)</li>
</ul>
<p>执行字段添加操作。如果添加的字段已经存在，它将抛出一个异常。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">);</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span><span class="o">.</span><span class="n">addColumns</span><span class="o">(</span><span class="n">concat</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">,</span> <span class="s">&#34;Sunny&#34;</span><span class="o">))</span>
</code></pre></div><ul>
<li>AddOrReplaceColumns(Batch/Streaming)</li>
</ul>
<p>执行字段添加操作。如果添加的列名与现有的列名相同，那么现有的字段将被替换。此外，如果添加的字段名与现有字段名重复，则使用最后一个字段名。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">);</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span><span class="o">.</span><span class="n">addOrReplaceColumns</span><span class="o">(</span><span class="n">concat</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">,</span> <span class="s">&#34;Sunny&#34;</span><span class="o">)</span> <span class="n">as</span> <span class="s">&#34;desc&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>DropColumns(Batch/Streaming)</li>
</ul>
<p>执行字段删除操作。字段表达式应该是字段引用表达式，并且只能删除现有的字段。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">);</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span><span class="o">.</span><span class="n">dropColumns</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>RenameColumns(Batch/Streaming)</li>
</ul>
<p>执行字段重命名操作。字段表达式应为别名表达式，且只能对现有字段进行重命名。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">);</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span><span class="o">.</span><span class="n">renameColumns</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;b&#34;</span> <span class="n">as</span> <span class="s">&#34;b2&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span> <span class="n">as</span> <span class="s">&#34;c2&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="聚合aggregations">聚合(Aggregations)</h3>
<ul>
<li>GroupBy 聚合(Batch/Streaming/Result Updating)</li>
</ul>
<p>类似于 SQL 的 GROUP BY 子句。将分组键上的行与下面的运行聚合操作符进行分组，以分组方式聚合行。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">sum</span><span class="o">().</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;d&#34;</span><span class="o">))</span>
</code></pre></div><p>注意：对于流式查询，计算查询结果所需的状态可能会无限增长，这取决于聚合的类型和不同分组键的数量。请提供有效的保留时间间隔的查询配置，以防止状态大小过大。详见<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<ul>
<li>GroupBy 窗口聚合(Batch/Streaming)</li>
</ul>
<p>在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#group-windows">分组窗口</a>和可能的一个或多个分组键上对一个表进行分组和聚合。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">orders</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">5.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span> <span class="c1">// define window
</span><span class="c1"></span>    <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span> <span class="c1">// group by key and window
</span><span class="c1"></span>    <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">.</span><span class="n">start</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">.</span><span class="n">end</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">.</span><span class="n">rowtime</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">sum</span> <span class="n">as</span> <span class="s">&#34;d&#34;</span><span class="o">)</span> <span class="c1">// access window properties and aggregate
</span></code></pre></div><ul>
<li>Over 窗口聚合(Streaming)</li>
</ul>
<p>类似于 SQL OVER 子句。根据前后记录的窗口(范围)，为每条记录计算 OVER 窗口汇总。更多细节请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#over-windows">over 窗口</a>部分。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">orders</span>
    <span class="c1">// define window
</span><span class="c1"></span>    <span class="o">.</span><span class="n">window</span><span class="o">(</span>
        <span class="nc">Over</span>
          <span class="n">partitionBy</span> <span class="n">$</span><span class="s">&#34;a&#34;</span>
          <span class="n">orderBy</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span>
          <span class="n">preceding</span> <span class="nc">UNBOUNDED_RANGE</span>
          <span class="n">following</span> <span class="nc">CURRENT_RANGE</span>
          <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">avg</span> <span class="n">over</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">max</span><span class="o">().</span><span class="n">over</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">),</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">min</span><span class="o">().</span><span class="n">over</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">))</span> <span class="c1">// sliding aggregate
</span></code></pre></div><p>注意：所有的聚合必须定义在同一个窗口上，即相同的分区、排序和范围。目前，只支持对 CURRENT ROW 范围的 PRECEDING（UNBOUNDED 和 bounded）窗口。还不支持带 FOLLOWING 的范围。ORDER BY 必须在单个<a href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes">时间属性</a>上指定。</p>
<ul>
<li>Distinct 聚合(Batch Streaming/Result Updating)</li>
</ul>
<p>类似于 SQL 的 DISTINCT AGGREGATION 子句，如 <code>COUNT(DISTINCT a)</code>。Distinct 聚合声明一个聚合函数（内置的或用户定义的）只应用在不同的输入值上，Distinct 可以应用于 GroupBy 聚合，GroupBy 窗口聚合和 Over 窗口聚合。Distinct 可以应用于 GroupBy 聚合、GroupBy 窗口聚合和 Over 窗口聚合。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">);</span>
<span class="c1">// Distinct aggregation on group by
</span><span class="c1"></span><span class="k">val</span> <span class="n">groupByDistinctResult</span> <span class="k">=</span> <span class="n">orders</span>
    <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">distinct</span> <span class="n">as</span> <span class="s">&#34;d&#34;</span><span class="o">)</span>
<span class="c1">// Distinct aggregation on time window group by
</span><span class="c1"></span><span class="k">val</span> <span class="n">groupByWindowDistinctResult</span> <span class="k">=</span> <span class="n">orders</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">5.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">).</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">distinct</span> <span class="n">as</span> <span class="s">&#34;d&#34;</span><span class="o">)</span>
<span class="c1">// Distinct aggregation on over window
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Over</span>
        <span class="n">partitionBy</span> <span class="n">$</span><span class="s">&#34;a&#34;</span>
        <span class="n">orderBy</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span>
        <span class="n">preceding</span> <span class="nc">UNBOUNDED_RANGE</span>
        <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">avg</span><span class="o">.</span><span class="n">distinct</span> <span class="n">over</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">max</span> <span class="n">over</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">min</span> <span class="n">over</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>
</code></pre></div><p>用户自定义的聚合函数也可以与 DISTINCT 修饰符一起使用。如果只计算不同值的聚合结果，只需在聚合函数中添加 distinct 修饰符即可。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">);</span>

<span class="c1">// Use distinct aggregation for user-defined aggregate functions
</span><span class="c1"></span><span class="k">val</span> <span class="n">myUdagg</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MyUdagg</span><span class="o">();</span>
<span class="n">orders</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;users&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;users&#34;</span><span class="o">,</span> <span class="n">myUdagg</span><span class="o">.</span><span class="n">distinct</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;points&#34;</span><span class="o">)</span> <span class="n">as</span> <span class="s">&#34;myDistinctResult&#34;</span><span class="o">);</span>
</code></pre></div><p>注意：对于流式查询，计算查询结果所需的状态可能会根据不同字段的数量而无限增长。请提供一个有效的保留时间间隔的查询配置，以防止过大的状态大小。详情请看<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<ul>
<li>Distinct(Batch Streaming/Result Updating)</li>
</ul>
<p>类似于 SQL 的 DISTINCT 子句。返回具有不同值组合的记录。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span><span class="o">.</span><span class="n">distinct</span><span class="o">()</span>
</code></pre></div><p>注意：对于流式查询，计算查询结果所需的状态可能会根据不同字段的数量而无限增长。请提供一个有效的保留时间间隔的查询配置，以防止过大的状态大小。如果启用了状态清洗功能，Distinct 必须发出消息，以防止下游操作者过早地驱逐状态，从而使 Distinct 包含结果更新。详见<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<h3 id="joins">Joins</h3>
<ul>
<li>Inner Join(Batch/Streaming)</li>
</ul>
<p>类似于 SQL JOIN 子句。连接两个表。两个表必须有不同的字段名，并且必须通过 join 操作符或使用 where 或 filter 操作符定义至少一个平等连接谓词。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">left</span> <span class="k">=</span> <span class="n">ds1</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="n">ds2</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;d&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;e&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;f&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">right</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="o">===</span> <span class="n">$</span><span class="s">&#34;d&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;e&#34;</span><span class="o">)</span>
</code></pre></div><p>注意：对于流式查询，计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供一个具有有效保留时间间隔的查询配置，以防止状态大小过大。详情请看<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<ul>
<li>Outer Join(Batch/Streaming/Result Updating)</li>
</ul>
<p>类似于 SQL LEFT/RIGHT/FULL OUTER JOIN 子句。连接两个表。两个表必须有不同的字段名，并且必须定义至少一个平等连接谓词。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">left</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataSet</span><span class="o">(</span><span class="n">ds1</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataSet</span><span class="o">(</span><span class="n">ds2</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;d&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;e&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;f&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">leftOuterResult</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span><span class="n">right</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="o">===</span> <span class="n">$</span><span class="s">&#34;d&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;e&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">rightOuterResult</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">rightOuterJoin</span><span class="o">(</span><span class="n">right</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="o">===</span> <span class="n">$</span><span class="s">&#34;d&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;e&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">fullOuterResult</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">fullOuterJoin</span><span class="o">(</span><span class="n">right</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="o">===</span> <span class="n">$</span><span class="s">&#34;d&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;e&#34;</span><span class="o">)</span>
</code></pre></div><p>注意：对于流式查询，计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供一个具有有效保留时间间隔的查询配置，以防止状态大小过大。详情请看<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<ul>
<li>Interval Join(Batch/Streaming)</li>
</ul>
<p>注：区间连接是常规连接的一个子集，可以用流式处理。</p>
<p>一个区间连接至少需要一个等价连接谓词和一个连接条件，以限制双方的时间。这样的条件可以由两个合适的范围谓词（&lt;，&lt;=，&gt;=，&gt;）或一个比较两个输入表的相同类型的<a href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes">时间属性</a>（即处理时间或事件时间）的单一平等谓词来定义。</p>
<p>例如，以下谓词是有效的区间连接条件。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">$</span><span class="s">&#34;ltime&#34;</span> <span class="o">===</span> <span class="n">$</span><span class="s">&#34;rtime&#34;</span>
<span class="n">$</span><span class="s">&#34;ltime&#34;</span> <span class="o">&gt;=</span> <span class="n">$</span><span class="s">&#34;rtime&#34;</span> <span class="o">&amp;&amp;</span> <span class="n">$</span><span class="s">&#34;ltime&#34;</span> <span class="o">&lt;</span> <span class="n">$</span><span class="s">&#34;rtime&#34;</span> <span class="o">+</span> <span class="mf">10.</span><span class="n">minutes</span>
<span class="k">val</span> <span class="n">left</span> <span class="k">=</span> <span class="n">ds1</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;ltime&#34;</span><span class="o">.</span><span class="n">rowtime</span><span class="o">)</span>
<span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="n">ds2</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;d&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;e&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;f&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;rtime&#34;</span><span class="o">.</span><span class="n">rowtime</span><span class="o">)</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">right</span><span class="o">)</span>
  <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="o">===</span> <span class="n">$</span><span class="s">&#34;d&#34;</span> <span class="o">&amp;&amp;</span> <span class="n">$</span><span class="s">&#34;ltime&#34;</span> <span class="o">&gt;=</span> <span class="n">$</span><span class="s">&#34;rtime&#34;</span> <span class="o">-</span> <span class="mf">5.</span><span class="n">minutes</span> <span class="o">&amp;&amp;</span> <span class="n">$</span><span class="s">&#34;ltime&#34;</span> <span class="o">&lt;</span> <span class="n">$</span><span class="s">&#34;rtime&#34;</span> <span class="o">+</span> <span class="mf">10.</span><span class="n">minutes</span><span class="o">)</span>
  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;e&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;ltime&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Inner Join with Table Function (UDTF)(Batch/Streaming)</li>
</ul>
<p>用表格函数的结果连接一个表格。左表（外表）的每条记录都与相应的表函数调用所产生的所有记录合并。如果左（外）表的表函数调用返回的结果是空的，则放弃该表的某行。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// instantiate User-Defined Table Function
</span><span class="c1"></span><span class="k">val</span> <span class="n">split</span><span class="k">:</span> <span class="kt">TableFunction</span><span class="o">[</span><span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MySplitUDTF</span><span class="o">()</span>

<span class="c1">// join
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">table</span>
    <span class="o">.</span><span class="n">joinLateral</span><span class="o">(</span><span class="n">split</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span> <span class="n">as</span> <span class="o">(</span><span class="s">&#34;s&#34;</span><span class="o">,</span> <span class="s">&#34;t&#34;</span><span class="o">,</span> <span class="s">&#34;v&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;s&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;t&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;v&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Left Outer Join with Table Function (UDTF)(Batch/Streaming)</li>
</ul>
<p>用表格函数的结果连接一个表格。左表（外表）的每一行都与相应的表函数调用所产生的所有行相连接。如果表函数调用返回的结果为空，则保留相应的外侧行，并将结果用空值填充。</p>
<p>注意：目前，表函数左外侧连接的谓词只能是空或字面为真。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// instantiate User-Defined Table Function
</span><span class="c1"></span><span class="k">val</span> <span class="n">split</span><span class="k">:</span> <span class="kt">TableFunction</span><span class="o">[</span><span class="k">_</span><span class="o">]</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MySplitUDTF</span><span class="o">()</span>

<span class="c1">// join
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">table</span>
    <span class="o">.</span><span class="n">leftOuterJoinLateral</span><span class="o">(</span><span class="n">split</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span> <span class="n">as</span> <span class="o">(</span><span class="s">&#34;s&#34;</span><span class="o">,</span> <span class="s">&#34;t&#34;</span><span class="o">,</span> <span class="s">&#34;v&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;s&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;t&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;v&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Join with Temporal Table(Streaming)</li>
</ul>
<p><a href="https://ohmyweekly.github.io/notes/2020-08-22-temporal-tables">时间表</a>是跟踪其随时间变化的表。</p>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table-functions">时间表函数</a>提供了对时间表在特定时间点的状态的访问。用时态表函数连接表的语法与带表函数的内部连接中的语法相同。</p>
<p>目前只支持与时态表的内联接。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">ratesHistory</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;RatesHistory&#34;</span><span class="o">)</span>

<span class="c1">// register temporal table function with a time attribute and primary key
</span><span class="c1"></span><span class="k">val</span> <span class="n">rates</span> <span class="k">=</span> <span class="n">ratesHistory</span><span class="o">.</span><span class="n">createTemporalTableFunction</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;r_proctime&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;r_currency&#34;</span><span class="o">)</span>

<span class="c1">// join with &#34;Orders&#34; based on the time attribute and key
</span><span class="c1"></span><span class="k">val</span> <span class="n">orders</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span>
    <span class="o">.</span><span class="n">joinLateral</span><span class="o">(</span><span class="n">rates</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;o_rowtime&#34;</span><span class="o">),</span> <span class="n">$</span><span class="s">&#34;r_currency&#34;</span> <span class="o">===</span> <span class="n">$</span><span class="s">&#34;o_currency&#34;</span><span class="o">)</span>
</code></pre></div><p>更多信息请查看更详细的<a href="https://ohmyweekly.github.io/notes/2020-08-22-temporal-tables">时间表概念说明</a>。</p>
<h3 id="集合操作">集合操作</h3>
<ul>
<li>Union(Batch)</li>
</ul>
<p>类似于 SQL UNION 子句。将两个表联合起来，去除重复记录，两个表必须有相同的字段类型。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">left</span> <span class="k">=</span> <span class="n">ds1</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="n">ds2</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">union</span><span class="o">(</span><span class="n">right</span><span class="o">)</span>
</code></pre></div><ul>
<li>UnionAll(Batch/Streaming)</li>
</ul>
<p>类似于 SQL UNION ALL 子句。联合两个表，两个表的字段类型必须相同。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">left</span> <span class="k">=</span> <span class="n">ds1</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="n">ds2</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">unionAll</span><span class="o">(</span><span class="n">right</span><span class="o">)</span>
</code></pre></div><ul>
<li>Intersect(Batch)</li>
</ul>
<p>类似于 SQL INTERSECT 子句。Intersect 子句返回的是两个表中都存在的记录。如果一条记录在一个表或两个表中存在一次以上，则只返回一次，即结果表没有重复记录。两个表的字段类型必须相同。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">left</span> <span class="k">=</span> <span class="n">ds1</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="n">ds2</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;e&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;f&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;g&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">intersect</span><span class="o">(</span><span class="n">right</span><span class="o">)</span>
</code></pre></div><ul>
<li>IntersectAll(Batch)</li>
</ul>
<p>类似于 SQL 的 INTERSECT ALL 子句。IntersectAll 子句返回两个表中都存在的记录。如果一条记录在两张表中都存在一次以上，那么就会按照它在两张表中存在的次数来返回，也就是说，得到的表可能有重复的记录。两个表的字段类型必须相同。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">left</span> <span class="k">=</span> <span class="n">ds1</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="n">ds2</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;e&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;f&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;g&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">intersectAll</span><span class="o">(</span><span class="n">right</span><span class="o">)</span>
</code></pre></div><ul>
<li>Minus(Batch)</li>
</ul>
<p>类似于 SQL EXCEPT 子句。Minus 返回左表中不存在于右表中的记录。左表中的重复记录只返回一次，即删除重复记录。两个表的字段类型必须相同。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">left</span> <span class="k">=</span> <span class="n">ds1</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="n">ds2</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">minus</span><span class="o">(</span><span class="n">right</span><span class="o">)</span>
</code></pre></div><ul>
<li>MinusAll(Batch)</li>
</ul>
<p>类似于 SQL EXCEPT ALL 子句。MinusAll 子句返回右表中不存在的记录。一条记录在左表中出现 n 次，在右表中出现 m 次，则返回(n - m)次，即删除右表中存在的重复记录。两个表的字段类型必须相同。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">left</span> <span class="k">=</span> <span class="n">ds1</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="n">ds2</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">minusAll</span><span class="o">(</span><span class="n">right</span><span class="o">)</span>
</code></pre></div><ul>
<li>In(Batch/Streaming)</li>
</ul>
<p>类似于 SQL 的 IN 子句。如果一个表达式存在于给定的表子查询中，In 子句返回 true。子查询表必须由一列组成。该列必须与表达式具有相同的数据类型。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">left</span> <span class="k">=</span> <span class="n">ds1</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="n">ds2</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">.</span><span class="n">in</span><span class="o">(</span><span class="n">right</span><span class="o">))</span>
</code></pre></div><p>注意：对于流式查询，该操作被重写为加入和分组操作。计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供有效的保留时间间隔的查询配置，以防止状态大小过大。详情请看<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<h3 id="orderby-offset-和-fetch">OrderBy, Offset 和 Fetch</h3>
<ul>
<li>Order By(Batch)</li>
</ul>
<p>类似于 SQL ORDER BY 子句。返回所有平行分区的全局排序记录。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span> <span class="k">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">in</span><span class="o">.</span><span class="n">orderBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">.</span><span class="n">asc</span><span class="o">)</span>
</code></pre></div><ul>
<li>Offset 和 Fetch(Batch)</li>
</ul>
<p>类似于 SQL 的 OFFSET 和 FETCH 子句。Offset 和 Fetch 限制了排序结果中返回的记录数量。Offset 和 Fetch 在技术上是 Order By 操作符的一部分，因此必须在它前面。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">in</span> <span class="k">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)</span>

<span class="c1">// returns the first 5 records from the sorted result
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="n">orderBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">.</span><span class="n">asc</span><span class="o">).</span><span class="n">fetch</span><span class="o">(</span><span class="mi">5</span><span class="o">)</span>

<span class="c1">// skips the first 3 records and returns all following records from the sorted result
</span><span class="c1"></span><span class="k">val</span> <span class="n">result2</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="n">orderBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">.</span><span class="n">asc</span><span class="o">).</span><span class="n">offset</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>

<span class="c1">// skips the first 10 records and returns the next 5 records from the sorted result
</span><span class="c1"></span><span class="k">val</span> <span class="n">result3</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">in</span><span class="o">.</span><span class="n">orderBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">.</span><span class="n">asc</span><span class="o">).</span><span class="n">offset</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">fetch</span><span class="o">(</span><span class="mi">5</span><span class="o">)</span>
</code></pre></div><h3 id="insert">Insert</h3>
<ul>
<li>Insert Into(Batch/Streaming)</li>
</ul>
<p>类似于 SQL 查询中的 <code>INSERT INTO</code> 子句，该方法执行插入到一个注册的输出表中。<code>executeInsert()</code> 方法将立即提交一个执行插入操作的 Flink 作业。</p>
<p>输出表必须在 TableEnvironment 中注册（见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#connector-tables">连接器表</a>）。此外，注册表的模式必须与查询的模式相匹配。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="n">orders</span><span class="o">.</span><span class="n">executeInsert</span><span class="o">(</span><span class="s">&#34;OutOrders&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="group-窗口">Group 窗口</h3>
<p>分组窗口根据时间或行数间隔将分组行汇总成有限的组，每组评估一次汇总函数。对于批处理表来说，窗口是按时间间隔对记录进行分组的便捷捷径。</p>
<p>窗口是使用 window(w: GroupWindow)子句定义的，并且需要一个别名，这个别名是使用 as 子句指定的。为了通过窗口对表进行分组，必须在 groupBy(&hellip;)子句中像常规分组属性一样引用窗口别名。下面的例子展示了如何在表上定义窗口聚合。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">window</span><span class="o">([</span><span class="kt">w:</span> <span class="kt">GroupWindow</span><span class="o">]</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>  <span class="c1">// define window with alias w
</span><span class="c1"></span>  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>   <span class="c1">// group the table by window w
</span><span class="c1"></span>  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">sum</span><span class="o">)</span>  <span class="c1">// aggregate
</span></code></pre></div><p>在流式环境中，只有当窗口聚合除窗口外还对一个或多个属性进行分组时，才能并行计算，即 groupBy(&hellip;)子句引用了一个窗口别名和至少一个附加属性。仅引用窗口别名的 groupBy(&hellip;) 子句（如上面的例子）只能由单个非并行任务来评估。下面的示例显示了如何定义具有附加分组属性的窗口聚合。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">window</span><span class="o">([</span><span class="kt">w:</span> <span class="kt">GroupWindow</span><span class="o">]</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span> <span class="c1">// define window with alias w
</span><span class="c1"></span>  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">)</span>  <span class="c1">// group the table by attribute a and window w
</span><span class="c1"></span>  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">sum</span><span class="o">)</span>  <span class="c1">// aggregate
</span></code></pre></div><p>窗口属性，如时间窗口的开始、结束或行时间戳，可以在选择语句中作为窗口别名的属性，分别添加为 w.start、w.end 和 w.rowtime。窗口开始时间和行时间时间戳是包含的窗口下界和上界。相反，窗口结束时间戳是专属的上层窗口边界。例如一个从下午 2 点开始的 30 分钟的翻滚窗口，其起始时间戳为 14:00:00.000，行时时间戳为 14:29:59.999，结束时间戳为 14:30:00.000。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">window</span><span class="o">([</span><span class="kt">w:</span> <span class="kt">GroupWindow</span><span class="o">]</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>  <span class="c1">// define window with alias w
</span><span class="c1"></span>  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">)</span>  <span class="c1">// group the table by attribute a and window w
</span><span class="c1"></span>  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">.</span><span class="n">start</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">.</span><span class="n">end</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">.</span><span class="n">rowtime</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">count</span><span class="o">)</span> <span class="c1">// aggregate and add window start, end, and rowtime timestamps
</span></code></pre></div><p>窗口参数定义了如何将行映射到窗口。Window 不是一个用户可以实现的接口。相反，Table API 提供了一组具有特定语义的预定义 Window 类，它们被翻译成底层的 DataStream 或 DataSet 操作。下面列出了支持的窗口定义。</p>
<h4 id="滚动窗口">滚动窗口</h4>
<p>滚动窗口将行分配到固定长度的非重叠的连续窗口。例如，5 分钟的滚动窗口以 5 分钟的间隔将行分组。滚动窗口可以在事件时间、处理时间或行数上定义。</p>
<p>滚动窗口可以通过使用 Tumble 类来定义，具体如下。</p>
<table>
<thead>
<tr>
<th style="text-align:left">方法</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">over</td>
<td style="text-align:left">定义窗口的长度，可以是时间或行数间隔。</td>
</tr>
<tr>
<td style="text-align:left">on</td>
<td style="text-align:left">要对其进行分组（时间间隔）或排序（行数）的时间属性。对于批处理查询，这可能是任何 Long 或 Timestamp 属性。对于流式查询，这必须是一个<a href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes">声明的事件时间或处理时间时间属性</a>。</td>
</tr>
<tr>
<td style="text-align:left">as</td>
<td style="text-align:left">为窗口指定一个别名。该别名用于在下面的 groupBy()子句中引用窗口，并在 select()子句中选择窗口属性，如窗口开始、结束或行时间戳。</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Tumbling Event-time Window
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">10.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>

<span class="c1">// Tumbling Processing-time Window (assuming a processing-time attribute &#34;proctime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">10.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>

<span class="c1">// Tumbling Row-count Window (assuming a processing-time attribute &#34;proctime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">10.</span><span class="n">rows</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>
</code></pre></div><h4 id="slide-滑动窗口">Slide (滑动窗口)</h4>
<p>滑动窗口有一个固定的尺寸，并按指定的滑动间隔滑动，如果滑动间隔小于窗口尺寸，滑动窗口就会重叠。如果滑动间隔小于窗口大小，滑动窗口就会重叠。因此，行可以分配给多个窗口。例如，一个 15 分钟大小和 5 分钟滑动间隔的滑动窗口将每行分配到 3 个不同的 15 分钟大小的窗口，这些窗口以 5 分钟的间隔进行评估。滑动窗口可以在事件时间、处理时间或行数上定义。</p>
<p>滑动窗口是通过使用 Slide 类来定义的，具体如下。</p>
<table>
<thead>
<tr>
<th style="text-align:left">方法</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">over</td>
<td style="text-align:left">定义窗口的长度，可以是时间或行数间隔。</td>
</tr>
<tr>
<td style="text-align:left">every</td>
<td style="text-align:left">定义滑动间隔，可以是时间间隔或行数间隔。缩放间隔的类型必须与尺寸间隔相同。</td>
</tr>
<tr>
<td style="text-align:left">on</td>
<td style="text-align:left">要对其进行分组（时间间隔）或排序（行数）的时间属性。对于批处理查询，这可能是任何 Long 或 Timestamp 属性。对于流式查询，这必须是一个<a href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes">声明的事件时间或处理时间时间属性</a>。</td>
</tr>
<tr>
<td style="text-align:left">as</td>
<td style="text-align:left">为窗口指定一个别名。该别名用于在下面的 groupBy()子句中引用窗口，并在 select()子句中选择窗口属性，如窗口开始、结束或行时间戳。</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Sliding Event-time Window
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Slide</span> <span class="n">over</span> <span class="mf">10.</span><span class="n">minutes</span> <span class="n">every</span> <span class="mf">5.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>

<span class="c1">// Sliding Processing-time window (assuming a processing-time attribute &#34;proctime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Slide</span> <span class="n">over</span> <span class="mf">10.</span><span class="n">minutes</span> <span class="n">every</span> <span class="mf">5.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>

<span class="c1">// Sliding Row-count window (assuming a processing-time attribute &#34;proctime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Slide</span> <span class="n">over</span> <span class="mf">10.</span><span class="n">rows</span> <span class="n">every</span> <span class="mf">5.</span><span class="n">rows</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>
</code></pre></div><h4 id="session-会话窗口">Session (会话窗口)</h4>
<p>会话窗口没有固定的大小，但它们的界限是由不活动的时间间隔来定义的，也就是说，如果在定义的间隙期没有事件出现，会话窗口就会被关闭。例如，有 30 分钟间隔的会话窗口在 30 分钟不活动后观察到一行时开始（否则该行将被添加到现有的窗口中），如果在 30 分钟内没有行被添加，则关闭。会话窗口可以在事件时间或处理时间工作。</p>
<p>通过使用 Session 类定义会话窗口，如下所示。</p>
<table>
<thead>
<tr>
<th style="text-align:left">方法</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">withGap</td>
<td style="text-align:left">将两个窗口之间的间隔定义为时间间隔。</td>
</tr>
<tr>
<td style="text-align:left">on</td>
<td style="text-align:left">要对其进行分组（时间间隔）或排序（行数）的时间属性。对于批处理查询，这可能是任何 Long 或 Timestamp 属性。对于流式查询，这必须是一个<a href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes">声明的事件时间或处理时间时间属性</a>。</td>
</tr>
<tr>
<td style="text-align:left">as</td>
<td style="text-align:left">为窗口指定一个别名。该别名用于在下面的 groupBy()子句中引用窗口，并在 select()子句中选择窗口属性，如窗口开始、结束或行时间戳。</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Session Event-time Window
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Session</span> <span class="n">withGap</span> <span class="mf">10.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>

<span class="c1">// Session Processing-time Window (assuming a processing-time attribute &#34;proctime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Session</span> <span class="n">withGap</span> <span class="mf">10.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="over-窗口">Over 窗口</h3>
<p>Over 窗口聚合是从标准 SQL（over 子句）中得知的，并在查询的 SELECT 子句中定义。与组窗口不同的是，组窗口是在 GROUP BY 子句中指定的，over 窗口不折叠行。相反，over 窗口聚合计算的是每条输入行在其相邻行的范围内的聚合。</p>
<p>Over 窗口是使用 window(w: OverWindow*)子句来定义的(在 Python API 中使用 over_window(*OverWindow))，并且在 select()方法中通过别名来引用。下面的例子展示了如何在表上定义一个 <code>over</code> 窗口聚合。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">window</span><span class="o">([</span><span class="kt">w:</span> <span class="kt">OverWindow</span><span class="o">]</span> <span class="n">as</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span>              <span class="c1">// define over window with alias w
</span><span class="c1"></span>  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">.</span><span class="n">sum</span> <span class="n">over</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">.</span><span class="n">min</span> <span class="n">over</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span> <span class="c1">// aggregate over the over window w
</span></code></pre></div><p>OverWindow 定义了计算汇总的行的范围。OverWindow 不是一个用户可以实现的接口。相反，Table API 提供了 Over 类来配置 over 窗口的属性。Over 窗口可以在事件时间或处理时间上定义，也可以在指定为时间间隔或行数的范围上定义。支持的 over 窗口定义是以 Over（和其他类）上的方法暴露出来的，下面列出了这些方法。</p>
<table>
<thead>
<tr>
<th style="text-align:left">方法</th>
<th style="text-align:left">Required</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">partitionBy</td>
<td style="text-align:left">Optional</td>
<td style="text-align:left">定义输入的一个或多个属性的分区。每个分区都被单独排序，聚合函数被分别应用到每个分区。注意：在流环境中，只有当窗口包含 partitionBy 子句时，才能并行计算 over window aggregates。如果没有 partitionBy(&hellip;)，流就会被一个单一的、非并行的任务处理。</td>
</tr>
<tr>
<td style="text-align:left">orderBy</td>
<td style="text-align:left">Required</td>
<td style="text-align:left">定义每个分区中行的顺序，从而定义聚合函数应用到行的顺序。注意：对于流式查询，这必须是一个<a href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes">声明的事件时间或处理时间时间属性</a>。目前，只支持单个排序属性。</td>
</tr>
<tr>
<td style="text-align:left">preceding</td>
<td style="text-align:left">Optional</td>
<td style="text-align:left">定义包含在窗口中并在当前行之前的行的间隔。这个间隔可以指定为时间间隔或行数间隔。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#bounded-over-windows">有边界的窗口</a>用间隔的大小来指定，例如，时间间隔为 10.分钟，行数间隔为 10.行。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#unbounded-over-windows">无边界的窗口</a>用一个常数来指定，例如，时间间隔为 UNBOUNDED_RANGE，行数间隔为 UNBOUNDED_ROW。未绑定的窗口从分区的第一行开始。如果省略了前面的子句，UNBOUNDED_RANGE 和 CURRENT_RANGE 被用作窗口的默认前后。</td>
</tr>
<tr>
<td style="text-align:left">following</td>
<td style="text-align:left">Optional</td>
<td style="text-align:left">定义包含在窗口中并跟随当前行的行的窗口间隔。这个间隔必须与前一个间隔的单位（时间或行数）相同。目前，不支持在当前行之后添加行的窗口。您可以指定两个常量中的一个。CURRENT_ROW 将窗口的上界设置为当前行。CURRENT_RANGE 将窗口的上界设置为当前行的排序键，也就是说，所有与当前行具有相同排序键的行都包含在窗口中。如果省略下面的子句，时间间隔窗口的上界定义为 CURRENT_RANGE，行数间隔窗口的上界定义为 CURRENT_ROW。</td>
</tr>
<tr>
<td style="text-align:left">as</td>
<td style="text-align:left">Required</td>
<td style="text-align:left">为 over 窗口指定一个别名。该别名用于在下面的 select()子句中引用 over 窗口。</td>
</tr>
</tbody>
</table>
<p>注意：目前，在同一个 select()调用中，所有的聚合函数都必须在同一个 over 窗口中计算。</p>
<h4 id="unbounded-over-windows">Unbounded Over Windows</h4>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Unbounded Event-time over window (assuming an event-time attribute &#34;rowtime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Over</span> <span class="n">partitionBy</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="n">orderBy</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">preceding</span> <span class="nc">UNBOUNDED_RANGE</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span>

<span class="c1">// Unbounded Processing-time over window (assuming a processing-time attribute &#34;proctime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Over</span> <span class="n">partitionBy</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="n">orderBy</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span> <span class="n">preceding</span> <span class="nc">UNBOUNDED_RANGE</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span>

<span class="c1">// Unbounded Event-time Row-count over window (assuming an event-time attribute &#34;rowtime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Over</span> <span class="n">partitionBy</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="n">orderBy</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">preceding</span> <span class="nc">UNBOUNDED_ROW</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span>
 
<span class="c1">// Unbounded Processing-time Row-count over window (assuming a processing-time attribute &#34;proctime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Over</span> <span class="n">partitionBy</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="n">orderBy</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span> <span class="n">preceding</span> <span class="nc">UNBOUNDED_ROW</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span>
</code></pre></div><h4 id="bounded-over-windows">Bounded Over Windows</h4>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Bounded Event-time over window (assuming an event-time attribute &#34;rowtime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Over</span> <span class="n">partitionBy</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="n">orderBy</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">preceding</span> <span class="mf">1.</span><span class="n">minutes</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span>

<span class="c1">// Bounded Processing-time over window (assuming a processing-time attribute &#34;proctime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Over</span> <span class="n">partitionBy</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="n">orderBy</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span> <span class="n">preceding</span> <span class="mf">1.</span><span class="n">minutes</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span>

<span class="c1">// Bounded Event-time Row-count over window (assuming an event-time attribute &#34;rowtime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Over</span> <span class="n">partitionBy</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="n">orderBy</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">preceding</span> <span class="mf">10.</span><span class="n">rows</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span>
  
<span class="c1">// Bounded Processing-time Row-count over window (assuming a processing-time attribute &#34;proctime&#34;)
</span><span class="c1"></span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Over</span> <span class="n">partitionBy</span> <span class="n">$</span><span class="s">&#34;a&#34;</span> <span class="n">orderBy</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span> <span class="n">preceding</span> <span class="mf">10.</span><span class="n">rows</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="基于行的操作">基于行的操作</h3>
<p>基于行的操作产生多列的输出。</p>
<ul>
<li>Map(Batch/Streaming)</li>
</ul>
<p>使用用户定义的标量函数或内置的标量函数执行 map 操作。如果输出类型是复合类型，则输出将被扁平化。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">MyMapFunction</span> <span class="k">extends</span> <span class="nc">ScalarFunction</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Row</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">Row</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="s">&#34;pre-&#34;</span> <span class="o">+</span> <span class="n">a</span><span class="o">)</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getResultType</span><span class="o">(</span><span class="n">signature</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Class</span><span class="o">[</span><span class="k">_</span><span class="o">]])</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[</span><span class="k">_</span><span class="o">]</span> <span class="k">=</span>
    <span class="nc">Types</span><span class="o">.</span><span class="nc">ROW</span><span class="o">(</span><span class="nc">Types</span><span class="o">.</span><span class="nc">STRING</span><span class="o">,</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">STRING</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">func</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MyMapFunction</span><span class="o">()</span>
<span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">func</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="s">&#34;b&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>FlatMap(Batch/Streaming)</li>
</ul>
<p>用表格函数执行 <code>flatMap</code> 操作。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">MyFlatMapFunction</span> <span class="k">extends</span> <span class="nc">TableFunction</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">str</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="s">&#34;#&#34;</span><span class="o">))</span> <span class="o">{</span>
      <span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34;#&#34;</span><span class="o">).</span><span class="n">foreach</span><span class="o">({</span> <span class="n">s</span> <span class="k">=&gt;</span>
        <span class="k">val</span> <span class="n">row</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Row</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
        <span class="n">row</span><span class="o">.</span><span class="n">setField</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">s</span><span class="o">)</span>
        <span class="n">row</span><span class="o">.</span><span class="n">setField</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="n">s</span><span class="o">.</span><span class="n">length</span><span class="o">)</span>
        <span class="n">collect</span><span class="o">(</span><span class="n">row</span><span class="o">)</span>
      <span class="o">})</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getResultType</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="nc">Types</span><span class="o">.</span><span class="nc">ROW</span><span class="o">(</span><span class="nc">Types</span><span class="o">.</span><span class="nc">STRING</span><span class="o">,</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">INT</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">func</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MyFlatMapFunction</span>
<span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">func</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;c&#34;</span><span class="o">)).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="s">&#34;b&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Aggregate(Batch/Streaming/Result Updating)</li>
</ul>
<p>用一个聚合函数执行一个聚合操作。必须用 select 语句关闭&quot;聚合&quot;，select 语句不支持聚合函数。如果输出类型是复合类型，聚合的输出将被扁平化。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">MyMinMaxAcc</span><span class="o">(</span><span class="k">var</span> <span class="n">min</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="k">var</span> <span class="n">max</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>

<span class="k">class</span> <span class="nc">MyMinMax</span> <span class="k">extends</span> <span class="nc">AggregateFunction</span><span class="o">[</span><span class="kt">Row</span>, <span class="kt">MyMinMaxAcc</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">def</span> <span class="n">accumulate</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">MyMinMaxAcc</span><span class="o">,</span> <span class="n">value</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">value</span> <span class="o">&lt;</span> <span class="n">acc</span><span class="o">.</span><span class="n">min</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">min</span> <span class="k">=</span> <span class="n">value</span>
    <span class="o">}</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">value</span> <span class="o">&gt;</span> <span class="n">acc</span><span class="o">.</span><span class="n">max</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">max</span> <span class="k">=</span> <span class="n">value</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span><span class="k">:</span> <span class="kt">MyMinMaxAcc</span> <span class="o">=</span> <span class="nc">MyMinMaxAcc</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span>

  <span class="k">def</span> <span class="n">resetAccumulator</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">MyMinMaxAcc</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">min</span> <span class="k">=</span> <span class="mi">0</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">max</span> <span class="k">=</span> <span class="mi">0</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getValue</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">MyMinMaxAcc</span><span class="o">)</span><span class="k">:</span> <span class="kt">Row</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">Row</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Integer</span><span class="o">.</span><span class="n">valueOf</span><span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">min</span><span class="o">),</span> <span class="nc">Integer</span><span class="o">.</span><span class="n">valueOf</span><span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">max</span><span class="o">))</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getResultType</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">new</span> <span class="nc">RowTypeInfo</span><span class="o">(</span><span class="nc">Types</span><span class="o">.</span><span class="nc">INT</span><span class="o">,</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">INT</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">myAggFunc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MyMinMax</span>
<span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="n">myAggFunc</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">)</span> <span class="n">as</span> <span class="o">(</span><span class="s">&#34;x&#34;</span><span class="o">,</span> <span class="s">&#34;y&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;key&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;x&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;y&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>Group 窗口聚合(Batch/Streaming)</li>
</ul>
<p>在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#group-windows">分组窗口</a>和可能的一个或多个分组键上对一个表进行分组和聚合。你必须用 select 语句关闭&quot;聚合&quot;。而且选择语句不支持 &ldquo;*&rdquo; 或聚合函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">myAggFunc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MyMinMax</span>
<span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">input</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">5.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span> <span class="c1">// define window
</span><span class="c1"></span>    <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;key&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span> <span class="c1">// group by key and window
</span><span class="c1"></span>    <span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="n">myAggFunc</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">)</span> <span class="n">as</span> <span class="o">(</span><span class="s">&#34;x&#34;</span><span class="o">,</span> <span class="s">&#34;y&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;key&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;x&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;y&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">.</span><span class="n">start</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">.</span><span class="n">end</span><span class="o">)</span> <span class="c1">// access window properties and aggregate results
</span></code></pre></div><ul>
<li>FlatAggregate(Streaming/Result Updating)</li>
</ul>
<p>类似于 GroupBy 聚合。将分组键上的行与下面的运行表聚合运算符进行分组，将行进行分组。与 AggregateFunction 的不同之处在于，TableAggregateFunction 可以为一个组返回 0 条或多条记录。你必须用 select 语句关闭 &ldquo;flatAggregate&rdquo;。而 select 语句不支持聚合函数。</p>
<p>不使用 emitValue 输出结果，还可以使用 emitUpdateWithRetract 方法。与 emitValue 不同的是，emitUpdateWithRetract 用于输出已经更新的值。这个方法以回缩模式增量输出数据，也就是说，一旦有更新，我们必须在发送新的更新记录之前回缩旧的记录。如果在表聚合函数中定义了 emitUpdateWithRetract 方法，那么 emitUpdateWithRetract 方法将优先于 emitValue 方法使用，因为该方法被视为比 emitValue 更有效，因为它可以增量输出值。详见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#table-aggregation-functions">表聚合函数</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">java.lang.</span><span class="o">{</span><span class="nc">Integer</span> <span class="k">=&gt;</span> <span class="nc">JInteger</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.Types</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.TableAggregateFunction</span>

<span class="cm">/**
</span><span class="cm"> * Accumulator for top2.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">Top2Accum</span> <span class="o">{</span>
  <span class="k">var</span> <span class="n">first</span><span class="k">:</span> <span class="kt">JInteger</span> <span class="o">=</span> <span class="k">_</span>
  <span class="k">var</span> <span class="n">second</span><span class="k">:</span> <span class="kt">JInteger</span> <span class="o">=</span> <span class="k">_</span>
<span class="o">}</span>

<span class="cm">/**
</span><span class="cm"> * The top2 user-defined table aggregate function.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">Top2</span> <span class="k">extends</span> <span class="nc">TableAggregateFunction</span><span class="o">[</span><span class="kt">JTuple2</span><span class="o">[</span><span class="kt">JInteger</span>, <span class="kt">JInteger</span><span class="o">]</span>, <span class="kt">Top2Accum</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span><span class="k">:</span> <span class="kt">Top2Accum</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">acc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Top2Accum</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">first</span> <span class="k">=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="k">=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span>
    <span class="n">acc</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">accumulate</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">Top2Accum</span><span class="o">,</span> <span class="n">v</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">v</span> <span class="o">&gt;</span> <span class="n">acc</span><span class="o">.</span><span class="n">first</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="k">=</span> <span class="n">acc</span><span class="o">.</span><span class="n">first</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">first</span> <span class="k">=</span> <span class="n">v</span>
    <span class="o">}</span> <span class="k">else</span> <span class="k">if</span> <span class="o">(</span><span class="n">v</span> <span class="o">&gt;</span> <span class="n">acc</span><span class="o">.</span><span class="n">second</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="k">=</span> <span class="n">v</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">Top2Accum</span><span class="o">,</span> <span class="n">its</span><span class="k">:</span> <span class="kt">JIterable</span><span class="o">[</span><span class="kt">Top2Accum</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">iter</span> <span class="k">=</span> <span class="n">its</span><span class="o">.</span><span class="n">iterator</span><span class="o">()</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="n">hasNext</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">top2</span> <span class="k">=</span> <span class="n">iter</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
      <span class="n">accumulate</span><span class="o">(</span><span class="n">acc</span><span class="o">,</span> <span class="n">top2</span><span class="o">.</span><span class="n">first</span><span class="o">)</span>
      <span class="n">accumulate</span><span class="o">(</span><span class="n">acc</span><span class="o">,</span> <span class="n">top2</span><span class="o">.</span><span class="n">second</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">emitValue</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">Top2Accum</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">JTuple2</span><span class="o">[</span><span class="kt">JInteger</span>, <span class="kt">JInteger</span><span class="o">]])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// emit the value and rank
</span><span class="c1"></span>    <span class="k">if</span> <span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">first</span> <span class="o">!=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">JTuple2</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">first</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
    <span class="o">}</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="o">!=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">JTuple2</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">second</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">top2</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Top2</span>
<span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span>
    <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;key&#34;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">flatAggregate</span><span class="o">(</span><span class="n">top2</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">)</span> <span class="n">as</span> <span class="o">(</span><span class="n">$</span><span class="s">&#34;v&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;rank&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;key&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;v&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;rank&#34;</span><span class="o">)</span>
</code></pre></div><p>注意：对于流式查询，计算查询结果所需的状态可能会无限增长，这取决于聚合的类型和不同分组键的数量。请提供具有有效保留时间间隔的查询配置，以防止状态大小过大。详情请参见<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<ul>
<li>Group Window FlatAggregate(Streaming)</li>
</ul>
<p>在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#group-windows">分组窗口</a>和可能一个或多个分组键上对一个表进行分组和聚合。你必须用 select 语句关闭 &ldquo;flatAggregate&rdquo;。而 select 语句不支持聚合函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">top2</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Top2</span>
<span class="k">val</span> <span class="n">orders</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">5.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span> <span class="n">as</span> <span class="s">&#34;w&#34;</span><span class="o">)</span> <span class="c1">// define window
</span><span class="c1"></span>    <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">)</span> <span class="c1">// group by key and window
</span><span class="c1"></span>    <span class="o">.</span><span class="n">flatAggregate</span><span class="o">(</span><span class="n">top2</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;b&#34;</span><span class="o">)</span> <span class="n">as</span> <span class="o">(</span><span class="n">$</span><span class="s">&#34;v&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;rank&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="n">w</span><span class="o">.</span><span class="n">start</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">.</span><span class="n">end</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;w&#34;</span><span class="o">.</span><span class="n">rowtime</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;v&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;rank&#34;</span><span class="o">)</span> <span class="c1">// access window properties and aggregate results
</span></code></pre></div><h2 id="数据类型">数据类型</h2>
<p>请看关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/types.html">数据类型</a>的专门页面。</p>
<p>通用类型和(嵌套的)复合类型(例如 POJOs、tuple、行、Scala case 类)也可以是行的字段。</p>
<p>具有任意嵌套的复合类型的字段可以用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/systemFunctions.html#value-access-functions">值访问函数</a>来访问。</p>
<p>通用类型被视为一个黑盒，可以通过<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html">用户定义的函数</a>进行传递或处理。</p>
<h2 id="表达式语法">表达式语法</h2>
<p>前面几节中的一些操作符都期望有一个或多个表达式。表达式可以使用内嵌的 Scala DSL 或作为字符串来指定。请参考上面的例子来了解如何指定表达式。</p>
<p>这是表达式的 EBNF 语法。</p>
<pre><code>expressionList = expression , { &quot;,&quot; , expression } ;

expression = overConstant | alias ;

alias = logic | ( logic , &quot;as&quot; , fieldReference ) | ( logic , &quot;as&quot; , &quot;(&quot; , fieldReference , { &quot;,&quot; , fieldReference } , &quot;)&quot; ) ;

logic = comparison , [ ( &quot;&amp;&amp;&quot; | &quot;||&quot; ) , comparison ] ;

comparison = term , [ ( &quot;=&quot; | &quot;==&quot; | &quot;===&quot; | &quot;!=&quot; | &quot;!==&quot; | &quot;&gt;&quot; | &quot;&gt;=&quot; | &quot;&lt;&quot; | &quot;&lt;=&quot; ) , term ] ;

term = product , [ ( &quot;+&quot; | &quot;-&quot; ) , product ] ;

product = unary , [ ( &quot;*&quot; | &quot;/&quot; | &quot;%&quot;) , unary ] ;

unary = [ &quot;!&quot; | &quot;-&quot; | &quot;+&quot; ] , composite ;

composite = over | suffixed | nullLiteral | prefixed | atom ;

suffixed = interval | suffixAs | suffixCast | suffixIf | suffixDistinct | suffixFunctionCall | timeIndicator ;

prefixed = prefixAs | prefixCast | prefixIf | prefixDistinct | prefixFunctionCall ;

interval = timeInterval | rowInterval ;

timeInterval = composite , &quot;.&quot; , (&quot;year&quot; | &quot;years&quot; | &quot;quarter&quot; | &quot;quarters&quot; | &quot;month&quot; | &quot;months&quot; | &quot;week&quot; | &quot;weeks&quot; | &quot;day&quot; | &quot;days&quot; | &quot;hour&quot; | &quot;hours&quot; | &quot;minute&quot; | &quot;minutes&quot; | &quot;second&quot; | &quot;seconds&quot; | &quot;milli&quot; | &quot;millis&quot;) ;

rowInterval = composite , &quot;.&quot; , &quot;rows&quot; ;

suffixCast = composite , &quot;.cast(&quot; , dataType , &quot;)&quot; ;

prefixCast = &quot;cast(&quot; , expression , dataType , &quot;)&quot; ;

dataType = &quot;BYTE&quot; | &quot;SHORT&quot; | &quot;INT&quot; | &quot;LONG&quot; | &quot;FLOAT&quot; | &quot;DOUBLE&quot; | &quot;BOOLEAN&quot; | &quot;STRING&quot; | &quot;DECIMAL&quot; | &quot;SQL_DATE&quot; | &quot;SQL_TIME&quot; | &quot;SQL_TIMESTAMP&quot; | &quot;INTERVAL_MONTHS&quot; | &quot;INTERVAL_MILLIS&quot; | ( &quot;MAP&quot; , &quot;(&quot; , dataType , &quot;,&quot; , dataType , &quot;)&quot; ) | ( &quot;PRIMITIVE_ARRAY&quot; , &quot;(&quot; , dataType , &quot;)&quot; ) | ( &quot;OBJECT_ARRAY&quot; , &quot;(&quot; , dataType , &quot;)&quot; ) ;

suffixAs = composite , &quot;.as(&quot; , fieldReference , &quot;)&quot; ;

prefixAs = &quot;as(&quot; , expression, fieldReference , &quot;)&quot; ;

suffixIf = composite , &quot;.?(&quot; , expression , &quot;,&quot; , expression , &quot;)&quot; ;

prefixIf = &quot;?(&quot; , expression , &quot;,&quot; , expression , &quot;,&quot; , expression , &quot;)&quot; ;

suffixDistinct = composite , &quot;distinct.()&quot; ;

prefixDistinct = functionIdentifier , &quot;.distinct&quot; , [ &quot;(&quot; , [ expression , { &quot;,&quot; , expression } ] , &quot;)&quot; ] ;

suffixFunctionCall = composite , &quot;.&quot; , functionIdentifier , [ &quot;(&quot; , [ expression , { &quot;,&quot; , expression } ] , &quot;)&quot; ] ;

prefixFunctionCall = functionIdentifier , [ &quot;(&quot; , [ expression , { &quot;,&quot; , expression } ] , &quot;)&quot; ] ;

atom = ( &quot;(&quot; , expression , &quot;)&quot; ) | literal | fieldReference ;

fieldReference = &quot;*&quot; | identifier ;

nullLiteral = &quot;nullOf(&quot; , dataType , &quot;)&quot; ;

timeIntervalUnit = &quot;YEAR&quot; | &quot;YEAR_TO_MONTH&quot; | &quot;MONTH&quot; | &quot;QUARTER&quot; | &quot;WEEK&quot; | &quot;DAY&quot; | &quot;DAY_TO_HOUR&quot; | &quot;DAY_TO_MINUTE&quot; | &quot;DAY_TO_SECOND&quot; | &quot;HOUR&quot; | &quot;HOUR_TO_MINUTE&quot; | &quot;HOUR_TO_SECOND&quot; | &quot;MINUTE&quot; | &quot;MINUTE_TO_SECOND&quot; | &quot;SECOND&quot; ;

timePointUnit = &quot;YEAR&quot; | &quot;MONTH&quot; | &quot;DAY&quot; | &quot;HOUR&quot; | &quot;MINUTE&quot; | &quot;SECOND&quot; | &quot;QUARTER&quot; | &quot;WEEK&quot; | &quot;MILLISECOND&quot; | &quot;MICROSECOND&quot; ;

over = composite , &quot;over&quot; , fieldReference ;

overConstant = &quot;current_row&quot; | &quot;current_range&quot; | &quot;unbounded_row&quot; | &quot;unbounded_row&quot; ;

timeIndicator = fieldReference , &quot;.&quot; , ( &quot;proctime&quot; | &quot;rowtime&quot; ) ;
</code></pre><p>字符。在这里，literal 是一个有效的 Java 字元。字符串的字元可以使用单引号或双引号来指定。复制引号进行转义（例如 &lsquo;It&rsquo;s me.&rsquo; 或 &ldquo;I &ldquo;&ldquo;like &ldquo;dog.&quot;）。</p>
<p>空符。空符必须有一个类型。使用 nullOf(type)(例如 nullOf(INT))来创建一个空值。</p>
<p>字段引用。fieldReference 指定数据中的一列（如果使用 <code>*</code>，则指定所有列），functionIdentifier 指定一个支持的标量函数。列名和函数名遵循 Java 标识符语法。</p>
<p>函数调用。作为字符串指定的表达式也可以使用前缀符号代替后缀符号来调用运算符和函数。</p>
<p>小数。如果需要处理精确的数值或大的小数，Table API 也支持 Java 的 BigDecimal 类型。在 Scala Table API 中，小数可以通过 BigDecimal(&ldquo;123456&rdquo;)来定义，而在 Java 中，可以通过附加一个 &ldquo;p&rdquo; 来表示精确，例如 123456p。</p>
<p>时间表示法。为了处理时间值，Table API 支持 Java SQL 的 Date, Time 和 Timestamp 类型。在 Scala Table API 中，可以通过使用 java.sql.Date.valueOf(&ldquo;2016-06-27&rdquo;)、java.sql.Time.valueOf(&ldquo;10:10:42&rdquo;) 或 java.sql.Timestamp.valueOf(&ldquo;2016-06-27 10:10:42.123&rdquo;) 来定义字符。Java 和 Scala Table API 还支持调用 &ldquo;2016-06-27&rdquo;.toDate()、&ldquo;10:10:42&rdquo;.toTime() 和  &ldquo;2016-06-27 10:10:42.123&rdquo;.toTimestamp() 来将 Strings 转换为时间类型。注意：由于 Java 的时态 SQL 类型依赖于时区，请确保 Flink 客户端和所有的 TaskManagers 使用相同的时区。</p>
<p>时间间隔。时间间隔可以用月数（Types.INTERVAL_MONTHS）或毫秒数（Types.INTERVAL_MILLIS）表示。同一类型的时间间隔可以加减(例如：1.小时+10.分钟)。可以将毫秒的时间间隔加到时间点上（如 &ldquo;2016-08-10&rdquo;.toDate + 5.days）。</p>
<p>Scala 表达式。Scala 表达式使用隐式转换。因此，确保在你的程序中添加通配符 <code>import org.apache.flink.table.api._</code>。如果一个字词没有被当作表达式，可以使用.toExpr 如 3.toExpr 来强制转换一个字词。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Table API 和 SQL]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-table-api-and-sql/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-table-api-and-sql/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Table API &amp; SQL</blockquote><h2 id="table-api-和-sql">Table API 和 SQL</h2>
<p>Apache Flink 具有两个关系型 API - Table API 和 SQL - 用于统一的流和批处理。Table API 是 Scala 和 Java 的语言集成查询 API，它允许用非常直观的方式从关系运算符（如选择、过滤和连接）组成查询。Flink 的 SQL 支持是基于 <a href="https://calcite.apache.org/">Apache Calcite</a>，它实现了 SQL 标准。无论输入是批处理输入（DataSet）还是流输入（DataStream），在任一接口中指定的查询都具有相同的语义，并指定相同的结果。</p>
<p>表 API 和 SQL 接口与 Flink 的 DataStream 和 DataSet API 紧密集成。你可以很容易地在所有 API 和建立在 API 基础上的库之间切换。例如，您可以使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/cep.html">CEP 库</a> 从 DataStream 中提取模式，随后使用 Table API 来分析模式，或者您可能会在预处理数据上运行 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/gelly">Gelly 图算法</a>之前，使用 SQL 查询扫描、过滤和聚合一个批处理表。</p>
<p>请注意，Table API 和 SQL 的功能还不完善，正在积极开发中。并非所有的操作都被 [Table API, SQL] 和 [stream, batch] 输入的每个组合所支持。</p>
<h3 id="依赖结构">依赖结构</h3>
<p>从 Flink 1.9 开始，Flink 为评估 Table &amp; SQL API 程序提供了两种不同的规划器实现：Blink planner 和 Flink 1.9 之前的旧 planner。Planner 负责将关系运算符转化为可执行的、优化的 Flink 作业。这两种 planner 都有不同的优化规则和运行时类。它们在支持的功能集上也可能有所不同。</p>
<p>注意: 对于生产用例，我们推荐 blink planner，它从 1.11 开始成为默认 planner。</p>
<p>所有的 Table API 和 SQL 组件都捆绑在 flink-table 或 flink-table-blink Maven 构件中。</p>
<p>以下是与大多数项目相关的依赖关系。</p>
<ul>
<li>flink-table-common: 一个通用模块，用于通过自定义函数、格式等扩展表生态系统。</li>
<li>flink-table-api-java: 使用 Java 编程语言的纯表程序的 Table &amp; SQL API（处于早期开发阶段，不推荐！）。</li>
<li>flink-table-api-scala: Table 和 SQL API，用于使用 Java 编程语言的纯表程序（处于早期开发阶段，不推荐）。</li>
<li>flink-table-api-java-bridge: 使用 Java 编程语言支持 DataStream/DataSet API 的 Table &amp; SQL API。</li>
<li>flink-table-api-scala-bridge: 使用 Scala 编程语言，支持 DataStream/DataSet API 的表和 SQL API。</li>
<li>flink-table-planner: 表程序 planner 和运行时。这是在 1.9 版本之前 Flink 唯一的 planner。从 Flink 1.11 开始，不再推荐使用它。</li>
<li>flink-table-planner-link: 新的 Blink 计划器，从 Flink 1.11 开始成为默认的。</li>
<li>flink-table-runtim-blink: 新的 Blink 运行时。</li>
<li>flink-table-uber: 将上面的 API 模块加上旧的规划器打包成一个适用于大多数 Table &amp; SQL API 使用案例的发行版。uber JAR 文件 <code>flink-table-*.jar</code> 默认位于 Flink 版本的 <code>/lib</code> 目录下。</li>
<li>flink-table-uber-blink: 将上面的 API 模块加上 Blink 的特定模块打包成一个适用于大多数 Table &amp; SQL API 用例的发行版。uber JAR 文件 <code>flink-table-blink-*.jar</code> 默认位于 Flink 版本的 <code>/lib</code> 目录下。</li>
</ul>
<p>关于如何在表程序中切换新旧 Blink planner，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html">通用 API</a> 页面。</p>
<h3 id="表程序依赖">表程序依赖</h3>
<p>根据目标编程语言的不同，您需要将 Java 或 Scala API 添加到项目中，以便使用 Table API 和 SQL 来定义管道。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="c">&lt;!-- Either... --&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-table-api-java-bridge_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="c">&lt;!-- or... --&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-table-api-scala-bridge_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>此外，如果你想在 IDE 中本地运行 Table API 和 SQL 程序，你必须添加以下一组模块，这取决于你想使用的计划器。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="c">&lt;!-- Either... (for the old planner that was available before Flink 1.9) --&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-table-planner_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="c">&lt;!-- or.. (for the new Blink planner) --&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-table-planner-blink_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>在内部，表生态系统的部分内容是在 Scala 中实现的。因此，请确保为批处理和流应用添加以下依赖关系。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-streaming-scala_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><h3 id="扩展依赖性">扩展依赖性</h3>
<p>如果你想实现与 Kafka 交互的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sourceSinks.html#define-a-tablefactory">自定义格式</a>或一组<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/systemFunctions.html">用户定义的函数</a>，下面的依赖就足够了，可以用于 SQL 客户端的 JAR 文件。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-table-common<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>目前，该模块包括以下扩展点：</p>
<ul>
<li>SerializationSchemaFactory</li>
<li>DeserializationSchemaFactory</li>
<li>ScalarFunction</li>
<li>TableFunction</li>
<li>AggregateFunction</li>
</ul>
<h3 id="下一步怎么走">下一步怎么走？</h3>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html">概念与通用 API</a>: Table API 和 SQL 的共享概念和 API。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/types.html">数据类型</a>: 列出了预先定义的数据类型及其属性。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming">流概念</a>: 表 API 或 SQL 的流特定文档，如时间属性的配置和更新结果的处理。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connect.html">连接到外部系统</a>: 可用的连接器和格式，用于向外部系统读写数据。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html">Table API</a>。支持的操作和表 API 的 API。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/index.html">SQL</a>。支持 SQL 的操作和语法。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/systemFunctions.html">内置函数</a>: 表 API 和 SQL 中支持的函数。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html">SQL 客户端</a>: 玩转 Flink SQL，并向集群提交表格程序，无需编程知识。</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use 语句]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-use-statements/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Insert 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-sql-hints/?utm_source=atom_feed" rel="related" type="text/html" title="SQL 提示" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-use-statements/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Use Statements</blockquote><h1 id="use-语句">USE 语句</h1>
<p>USE 语句用于设置当前数据库或目录。</p>
<h2 id="运行-use-语句">运行 USE 语句</h2>
<p>USE 语句可以通过 TableEnvironment 的 executeSql() 方法执行，也可以在 SQL CLI 中执行。executeSql() 方法会对一个成功的 USE 操作返回 &lsquo;OK&rsquo;， 否则会抛出一个异常。</p>
<p>下面的例子展示了如何在 TableEnvironment 和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html">SQL CLI</a> 中运行一条 USE 语句。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>
<span class="k">val</span> <span class="n">tEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// create a catalog
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE CATALOG cat1 WITH (...)&#34;</span><span class="o">)</span>
<span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;SHOW CATALOGS&#34;</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>
<span class="c1">// +-----------------+
</span><span class="c1">// |    catalog name |
</span><span class="c1">// +-----------------+
</span><span class="c1">// | default_catalog |
</span><span class="c1">// | cat1            |
</span><span class="c1">// +-----------------+
</span><span class="c1"></span>
<span class="c1">// change default catalog
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;USE CATALOG cat1&#34;</span><span class="o">)</span>

<span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;SHOW DATABASES&#34;</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>
<span class="c1">// databases are empty
</span><span class="c1">// +---------------+
</span><span class="c1">// | database name |
</span><span class="c1">// +---------------+
</span><span class="c1">// +---------------+
</span><span class="c1"></span>
<span class="c1">// create a database
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE DATABASE db1 WITH (...)&#34;</span><span class="o">)</span>
<span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;SHOW DATABASES&#34;</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>
<span class="c1">// +---------------+
</span><span class="c1">// | database name |
</span><span class="c1">// +---------------+
</span><span class="c1">// |        db1    |
</span><span class="c1">// +---------------+
</span><span class="c1"></span>
<span class="c1">// change default database
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;USE db1&#34;</span><span class="o">)</span>
</code></pre></div><h2 id="use-catloag">USE CATLOAG</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="n">USE</span><span class="w"> </span><span class="k">CATALOG</span><span class="w"> </span><span class="k">catalog_name</span><span class="w">
</span></code></pre></div><p>设置当前目录。所有没有明确指定目录的后续命令将使用这个目录。如果所提供的目录不存在，则会抛出一个异常。默认的当前目录是default_catalog。</p>
<h2 id="use">USE</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="n">USE</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.]</span><span class="n">database_name</span><span class="w">
</span></code></pre></div><p>设置当前数据库。所有没有明确指定数据库的后续命令将使用这个数据库。如果提供的数据库不存在，则会抛出一个异常。默认的当前数据库是default_database。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/use.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/use.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/sql" term="sql" label="SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[临时表]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-temporal-tables/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-temporal-tables/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Temporal Tables</blockquote><h1 id="时间表">时间表</h1>
<p>时间表代表了对变化表的（参数化）视图的概念，该视图返回一个表在特定时间点的内容。</p>
<p>变化表可以是跟踪变化的变化历史表（如数据库变化日志），也可以是将变化具体化的变化维度表（如数据库表）。</p>
<p>对于变化的历史表，Flink 可以跟踪变化，并允许在查询中的某个时间点访问表的内容。在 Flink 中，这种表用 Temporal Table Function 来表示。</p>
<p>对于变化的维度表，Flink 允许在查询内的处理时间点访问表的内容。在 Flink 中，这种表是由一个 Temporal Table 来表示的。</p>
<h2 id="动机">动机</h2>
<h3 id="与不断变化的历史表相关联">与不断变化的历史表相关联</h3>
<p>假设我们有以下表格 RatesHistory。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">RatesHistory</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">rowtime</span><span class="w"> </span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">=======</span><span class="w"> </span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">Euro</span><span class="w">        </span><span class="mi">114</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span><span class="w"></span><span class="mi">10</span><span class="p">:</span><span class="mi">45</span><span class="w">   </span><span class="n">Euro</span><span class="w">        </span><span class="mi">116</span><span class="w">
</span><span class="w"></span><span class="mi">11</span><span class="p">:</span><span class="mi">15</span><span class="w">   </span><span class="n">Euro</span><span class="w">        </span><span class="mi">119</span><span class="w">
</span><span class="w"></span><span class="mi">11</span><span class="p">:</span><span class="mi">49</span><span class="w">   </span><span class="n">Pounds</span><span class="w">      </span><span class="mi">108</span><span class="w">
</span></code></pre></div><p>RatesHistory 代表了一个不断增长的对日元（汇率为 1）的货币汇率附加表。例如，从 09:00 到 10:45，欧元对日元的汇率是 114，从 10:45 到 11:15 是 116。从 10:45 到 11:15 是 116。</p>
<p>考虑到我们希望输出 10:58 时的所有当前汇率，我们将需要以下 SQL 查询来计算结果表。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">RatesHistory</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">r</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">rowtime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="k">SELECT</span><span class="w"> </span><span class="k">MAX</span><span class="p">(</span><span class="n">rowtime</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="k">FROM</span><span class="w"> </span><span class="n">RatesHistory</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">r2</span><span class="w">
</span><span class="w">  </span><span class="k">WHERE</span><span class="w"> </span><span class="n">r2</span><span class="p">.</span><span class="n">currency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">currency</span><span class="w">
</span><span class="w">  </span><span class="k">AND</span><span class="w"> </span><span class="n">r2</span><span class="p">.</span><span class="n">rowtime</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">TIME</span><span class="w"> </span><span class="s1">&#39;10:58&#39;</span><span class="p">);</span><span class="w">
</span></code></pre></div><p>相关子查询确定相应货币的最大时间低于或等于期望时间。外层查询列出具有最大时间戳的汇率。</p>
<p>下表显示了这种计算的结果。在我们的例子中，10:45 的欧元更新被考虑在内，然而，11:15 的欧元更新和新输入的英镑在 10:58 的表格中没有被考虑。</p>
<pre><code>rowtime currency   rate
======= ======== ======
09:00   US Dollar   102
09:00   Yen           1
10:45   Euro        116
</code></pre><p>Temporal Tables 的概念旨在简化此类查询，加快其执行速度，并减少 Flink 的状态使用。Temporal Table 是一个关于 append-only 表的参数化视图，它将 append-only 表的行解释为表的 changelog，并提供该表在特定时间点的版本。将 append-only 表解释为变更日志需要指定一个主键属性和一个时间戳属性。主键决定哪些行会被覆盖，时间戳决定行的有效时间。</p>
<p>在上面的例子中，currency 是 RatesHistory 表的主键，rowtime 是时间戳属性。</p>
<p>在 Flink 中，这是由一个 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table-function">Temporal Table Function</a> 来表示的。</p>
<h3 id="与变化的维度表相关联">与变化的维度表相关联</h3>
<p>另一方面，有些用例需要加入一个不断变化的维度表，而这个表是一个外部数据库表。</p>
<p>让我们假设 LatestRates 是一张表（例如存储在），它是以最新的速率来物化的。LatestRates 是物化的历史 RatesHistory。那么 LatestRates 表在时间 10:58 时的内容将是。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="mi">10</span><span class="p">:</span><span class="mi">58</span><span class="o">&gt;</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">LatestRates</span><span class="p">;</span><span class="w">
</span><span class="w"></span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span><span class="w"></span><span class="n">Euro</span><span class="w">        </span><span class="mi">116</span><span class="w">
</span></code></pre></div><p>12:00 时 LatestRates 表的内容将是：</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="mi">12</span><span class="p">:</span><span class="mi">00</span><span class="o">&gt;</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">LatestRates</span><span class="p">;</span><span class="w">
</span><span class="w"></span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span><span class="w"></span><span class="n">Euro</span><span class="w">        </span><span class="mi">119</span><span class="w">
</span><span class="w"></span><span class="n">Pounds</span><span class="w">      </span><span class="mi">108</span><span class="w">
</span></code></pre></div><p>在 Flink 中，这用一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table">时间表</a>来表示。</p>
<h2 id="时间表函数">时间表函数</h2>
<p>为了访问时态表中的数据，必须传递一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html">时间属性</a>，该属性决定了将返回的表的版本。Flink 使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#table-functions">表函数</a>的 SQL 语法来提供一种表达方式。</p>
<p>一旦定义好，一个时态表函数就会接受一个单一的时间参数 timeAttribute，并返回一组行。这个集合包含了所有现有主键相对于给定时间属性的最新版本的行。</p>
<p>假设我们基于 RatesHistory 表定义了一个时态表函数 <code>Rates(timeAttribute)</code>，我们可以用下面的方式查询这样的函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Rates</span><span class="p">(</span><span class="s1">&#39;10:15&#39;</span><span class="p">);</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">rowtime</span><span class="w"> </span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">=======</span><span class="w"> </span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">Euro</span><span class="w">        </span><span class="mi">114</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Rates</span><span class="p">(</span><span class="s1">&#39;11:00&#39;</span><span class="p">);</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">rowtime</span><span class="w"> </span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">=======</span><span class="w"> </span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="mi">10</span><span class="p">:</span><span class="mi">45</span><span class="w">   </span><span class="n">Euro</span><span class="w">        </span><span class="mi">116</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span></code></pre></div><p>每次查询 <code>Rates(timeAttribute)</code> 都会返回给定时间属性的 Rates 的状态。</p>
<p>注意：目前，Flink 不支持直接查询带有恒定时间属性参数的时态表函数。目前，时态表函数只能用于连接。上面的例子是用来提供对函数 <code>Rates(timeAttribute)</code> 返回内容的直观认识。</p>
<p>关于如何使用时态表进行联接的更多信息，还请参见关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html">连续查询的连接</a>页面。</p>
<h2 id="定义时态表函数">定义时态表函数</h2>
<p>下面的代码片段说明了如何从一个仅有追加的表创建一个时态表函数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Get the stream and table environments.
</span><span class="c1"></span><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">tEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// Provide a static data set of the rates history table.
</span><span class="c1"></span><span class="k">val</span> <span class="n">ratesHistoryData</span> <span class="k">=</span> <span class="k">new</span> <span class="n">mutable</span><span class="o">.</span><span class="nc">MutableList</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span>
<span class="n">ratesHistoryData</span><span class="o">.+=((</span><span class="s">&#34;US Dollar&#34;</span><span class="o">,</span> <span class="mi">102L</span><span class="o">))</span>
<span class="n">ratesHistoryData</span><span class="o">.+=((</span><span class="s">&#34;Euro&#34;</span><span class="o">,</span> <span class="mi">114L</span><span class="o">))</span>
<span class="n">ratesHistoryData</span><span class="o">.+=((</span><span class="s">&#34;Yen&#34;</span><span class="o">,</span> <span class="mi">1L</span><span class="o">))</span>
<span class="n">ratesHistoryData</span><span class="o">.+=((</span><span class="s">&#34;Euro&#34;</span><span class="o">,</span> <span class="mi">116L</span><span class="o">))</span>
<span class="n">ratesHistoryData</span><span class="o">.+=((</span><span class="s">&#34;Euro&#34;</span><span class="o">,</span> <span class="mi">119L</span><span class="o">))</span>

<span class="c1">// Create and register an example table using above data set.
</span><span class="c1">// In the real setup, you should replace this with your own table.
</span><span class="c1"></span><span class="k">val</span> <span class="n">ratesHistory</span> <span class="k">=</span> <span class="n">env</span>
  <span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="n">ratesHistoryData</span><span class="o">)</span>
  <span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tEnv</span><span class="o">,</span> &#39;r_currency<span class="o">,</span> &#39;r_rate<span class="o">,</span> &#39;r_proctime<span class="o">.</span><span class="n">proctime</span><span class="o">)</span>

<span class="n">tEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;RatesHistory&#34;</span><span class="o">,</span> <span class="n">ratesHistory</span><span class="o">)</span>

<span class="c1">// Create and register TemporalTableFunction.
</span><span class="c1">// Define &#34;r_proctime&#34; as the time attribute and &#34;r_currency&#34; as the primary key.
</span><span class="c1"></span><span class="k">val</span> <span class="n">rates</span> <span class="k">=</span> <span class="n">ratesHistory</span><span class="o">.</span><span class="n">createTemporalTableFunction</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;r_proctime&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;r_currency&#34;</span><span class="o">)</span> <span class="c1">// &lt;==== (1)
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">registerFunction</span><span class="o">(</span><span class="s">&#34;Rates&#34;</span><span class="o">,</span> <span class="n">rates</span><span class="o">)</span>                                          <span class="c1">// &lt;==== (2)
</span></code></pre></div><p>第(1)行创建了一个 Rates <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table-functions">时态表函数</a>，这使得我们可以使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#joins">Table API</a> 中的函数 Rates。</p>
<p>第(2)行在我们的表环境中以 Rates 的名义注册这个函数，这使得我们可以在 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#joins">SQL</a> 中使用 Rates 函数。</p>
<h2 id="时态表">时态表</h2>
<p>注意: 这只在 Blink planner 中支持。</p>
<p>为了访问时间表中的数据，目前必须定义一个 LookupableTableSource 的 TableSource。Flink 使用 SQL:2011 中提出的 SQL 语法 FOR SYSTEM_TIME AS OF 来查询时间表。</p>
<p>假设我们定义了一个名为 LatestRates 的时态表，我们可以用下面的方式查询这样的表。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">LatestRates</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">SYSTEM_TIME</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">OF</span><span class="w"> </span><span class="n">TIME</span><span class="w"> </span><span class="s1">&#39;10:15&#39;</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="n">Euro</span><span class="w">        </span><span class="mi">114</span><span class="w">
</span><span class="w"></span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">LatestRates</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">SYSTEM_TIME</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">OF</span><span class="w"> </span><span class="n">TIME</span><span class="w"> </span><span class="s1">&#39;11:00&#39;</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="n">Euro</span><span class="w">        </span><span class="mi">116</span><span class="w">
</span><span class="w"></span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span></code></pre></div><p>注意：目前，Flink 不支持直接查询时间恒定的时态表。目前，时态表只能用在 join 中。上面的例子是用来提供一个直观的时间表 LatestRates 返回的内容。</p>
<p>更多关于如何使用时态表进行连接的信息，请参见关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html">连续查询的连接</a>页面。</p>
<h2 id="定义时态表">定义时态表</h2>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Get the stream and table environments.
</span><span class="c1"></span><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">settings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">().</span><span class="n">build</span><span class="o">()</span>
<span class="k">val</span> <span class="n">tEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">,</span> <span class="n">settings</span><span class="o">)</span>
<span class="c1">// or val tEnv = TableEnvironment.create(settings)
</span><span class="c1"></span>
<span class="c1">// Define an HBase table with DDL, then we can use it as a temporal table in sql
</span><span class="c1">// Column &#39;currency&#39; is the rowKey in HBase table
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span>
    <span class="s">s&#34;&#34;&#34;
</span><span class="s">       |CREATE TABLE LatestRates (
</span><span class="s">       |    currency STRING,
</span><span class="s">       |    fam1 ROW&lt;rate DOUBLE&gt;
</span><span class="s">       |) WITH (
</span><span class="s">       |    &#39;connector&#39; = &#39;hbase-1.4&#39;,
</span><span class="s">       |    &#39;table-name&#39; = &#39;Rates&#39;,
</span><span class="s">       |    &#39;zookeeper.quorum&#39; = &#39;localhost:2181&#39;
</span><span class="s">       |)
</span><span class="s">       |&#34;&#34;&#34;</span><span class="o">.</span><span class="n">stripMargin</span><span class="o">)</span>
</code></pre></div><p>也请参见如何定义 LookupableTableSource 的页面。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[侧输出]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/?utm_source=atom_feed" rel="related" type="text/html" title="可查询状态" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Side Outputs</blockquote><h2 id="side-output">Side Output</h2>
<p>除了 DataStream 操作产生的主流(main stream)外，还可以产生任意数量的附加侧输出结果流。结果流中的数据类型不必与主流中的数据类型相匹配，不同侧输出的类型也可以不同。当您要分割数据流时，这种操作非常有用，通常您必须复制数据流，然后从每个数据流中过滤掉您不想要的数据。</p>
<p>在使用侧输出时，首先需要定义一个 <code>OutputTag</code>，用来识别侧输出流。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">outputTag</span> <span class="k">=</span> <span class="nc">OutputTag</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;side-output&#34;</span><span class="o">)</span>
</code></pre></div><p>请注意 <code>OutputTag</code> 是如何根据侧输出流所包含的元素类型进行类型化的。</p>
<p>可以通过以下函数向侧输出发送数据。</p>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/process_function.html">ProcessFunction</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/process_function.html#the-keyedprocessfunction">KeyedProcessFunction</a></li>
<li>CoProcessFunction</li>
<li>KeyedCoProcessFunction</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#processwindowfunction">ProcessWindowFunction</a></li>
<li>ProcessAllWindowFunction</li>
</ul>
<p>你可以使用 <code>Context</code> 参数（在上面的函数中暴露给用户）向一个由 <code>OutputTag</code> 标识的侧输出发送数据。下面是一个从 <code>ProcessFunction</code> 中发射侧输出数据的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">outputTag</span> <span class="k">=</span> <span class="nc">OutputTag</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;side-output&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">mainDataStream</span> <span class="k">=</span> <span class="n">input</span>
  <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">ProcessFunction</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">processElement</span><span class="o">(</span>
        <span class="n">value</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>
        <span class="n">ctx</span><span class="k">:</span> <span class="kt">ProcessFunction</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span>
        <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="c1">// emit data to regular output
</span><span class="c1"></span>      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">value</span><span class="o">)</span>

      <span class="c1">// emit data to side output
</span><span class="c1"></span>      <span class="n">ctx</span><span class="o">.</span><span class="n">output</span><span class="o">(</span><span class="n">outputTag</span><span class="o">,</span> <span class="s">&#34;sideout-&#34;</span> <span class="o">+</span> <span class="nc">String</span><span class="o">.</span><span class="n">valueOf</span><span class="o">(</span><span class="n">value</span><span class="o">))</span>
    <span class="o">}</span>
  <span class="o">})</span>
</code></pre></div><p>为了检索侧输出流，你可以在 DataStream 操作的结果上使用 <code>getSideOutput(OutputTag)</code>。这将给你一个 DataStream，它的类型是侧输出流的结果。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">outputTag</span> <span class="k">=</span> <span class="nc">OutputTag</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;side-output&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">mainDataStream</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">sideOutputStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">mainDataStream</span><span class="o">.</span><span class="n">getSideOutput</span><span class="o">(</span><span class="n">outputTag</span><span class="o">)</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/side_output.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/side_output.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/side-outputs" term="side-outputs" label="Side Outputs" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[函数]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-functions/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-table-api-user-defined-functions/?utm_source=atom_feed" rel="related" type="text/html" title="用户定义函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-functions/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Functions</blockquote><h1 id="函数">函数</h1>
<p>Flink Table API &amp; SQL 使用户能够通过函数进行数据转换。</p>
<h2 id="函数的类型">函数的类型</h2>
<p>Flink 中的函数有两个维度来分类。</p>
<p>一个维度是系统（或内置）函数 v.s. 目录函数。系统函数没有命名空间，可以只用名字来引用。目录函数属于目录和数据库，因此它们有目录和数据库的命名空间，它们可以用完全/部分限定名（<code>catalog.db.func</code> 或 <code>db.func</code>）或者只用函数名来引用。</p>
<p>另一个维度是临时函数 v.s. 持久化函数。临时函数是不稳定的，只存在于一个会话的生命周期内，它们总是由用户创建的。而持久性函数则是在会话的生命周期内存在的，它们要么是由系统提供的，要么是在目录中持久存在的。</p>
<p>这两个维度给 Flink 用户提供了4类函数。</p>
<ol>
<li>临时系统函数</li>
<li>系统函数</li>
<li>临时目录函数</li>
<li>目录函数</li>
</ol>
<h2 id="引用函数">引用函数</h2>
<p>在 Flink 中，用户有两种引用函数的方式 - 精确引用函数或模棱两可的引用函数。</p>
<h3 id="精确的函数引用">精确的函数引用</h3>
<p>精确的函数引用使用户能够专门使用目录函数，并且跨目录和跨数据库，例如从 <code>mytable</code> 中选择 <code>mycatalog.mydb.myfunc(x)</code>，从 <code>mytable</code> 中选择 <code>mydb.myfunc(x)</code>。</p>
<p>这只从 Flink 1.10 开始支持。</p>
<h3 id="模棱两可的函数引用">模棱两可的函数引用</h3>
<p>在模棱两可的函数引用中，用户只需在 SQL 查询中指定函数名称即可，例如：<code>select myfunc(x) from mytable</code>。</p>
<h2 id="函数解析顺序">函数解析顺序</h2>
<p>只有当有不同类型但名称相同的函数时，解析顺序才是重要的，比如有三个函数都名为 &ldquo;myfunc&rdquo;，但分别是临时目录、目录和系统函数。如果没有函数名冲突，则函数将被解析为唯一的一个。</p>
<h3 id="精确的函数引用-1">精确的函数引用</h3>
<p>因为系统函数没有命名空间，所以 Flink 中的精确函数引用必须指向临时目录函数或目录函数。</p>
<p>其解析顺序是：</p>
<ol>
<li>临时目录函数</li>
<li>目录函数</li>
</ol>
<h3 id="含糊不清的函数参考">含糊不清的函数参考</h3>
<p>解析顺序是:</p>
<ol>
<li>临时系统函数</li>
<li>系统函数</li>
<li>临时目录函数，在当前目录和当前数据库中的会话。</li>
<li>目录函数，在当前目录和当前数据库中的会话。</li>
</ol>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/function" term="function" label="Function" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[动态表]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-dynamic-tables/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-dynamic-tables/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Dynamic Tables</blockquote><h1 id="动态表">动态表</h1>
<p>SQL 和关系代数在设计时并没有考虑到流数据。因此，关系代数（和 SQL）和流处理之间几乎没有概念上的差距。</p>
<p>本页讨论了这些差异，并解释了 Flink 如何在无界数据上实现与常规数据库引擎在有界数据上相同的语义。</p>
<h2 id="数据流的关系查询">数据流的关系查询</h2>
<p>下表比较了传统的关系代数和流处理在输入数据、执行和输出结果方面的情况。</p>
<table>
<thead>
<tr>
<th style="text-align:left">关系代数/SQL</th>
<th style="text-align:left">流处理</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">关系（或表）是有界（多）元组的集合。</td>
<td style="text-align:left">流是一个无限的元组序列。</td>
</tr>
<tr>
<td style="text-align:left">在批数据上执行的查询（如关系数据库中的表）可以访问完整的输入数据。</td>
<td style="text-align:left">流式查询在启动时不能访问所有的数据，必须&quot;等待&quot;数据流进来。</td>
</tr>
<tr>
<td style="text-align:left">批量查询在产生一个固定大小的结果后就会终止。</td>
<td style="text-align:left">流式查询根据接收到的记录不断地更新其结果，并且永远不会完成。</td>
</tr>
</tbody>
</table>
<p>尽管存在这些差异，但用关系查询和 SQL 处理流并不是不可能的。先进的关系数据库系统提供了一种叫做物化视图的功能。物化视图被定义为一个 SQL 查询，就像一个普通的虚拟视图一样。与虚拟视图不同，物化视图会缓存查询的结果，这样在访问视图时就不需要对查询进行评估。缓存的一个常见挑战是防止缓存提供过时的结果。当其定义查询的基表被修改时，一个物化视图就会过时。急切的视图维护是一种技术，它可以在更新基表时立即更新一个物化视图。</p>
<p>如果我们考虑以下几点，急切的视图维护和流上的 SQL 查询之间的联系就会变得很明显。</p>
<ul>
<li>数据库表是一个 INSERT、UPDATE 和 DELETE DML 语句流的结果，通常称为 changelog 流。</li>
<li>物化视图被定义为一个 SQL 查询。为了更新视图，查询不断处理视图的基础关系的 changelog 流。</li>
<li>物化视图是流式 SQL 查询的结果。</li>
</ul>
<p>考虑到这些要点，我们在下一节介绍以下动态表的概念。</p>
<h2 id="动态表与连续查询">动态表与连续查询</h2>
<p>动态表是 Flink 的表 API 和 SQL 支持流数据的核心概念。与代表批处理数据的静态表相比，动态表是随时间变化的。它们可以像静态批处理表一样被查询。查询动态表会产生一个连续查询。一个连续查询永远不会终止，并产生一个动态表作为结果。查询不断地更新它的（动态）结果表，以反映其（动态）输入表的变化。本质上，对动态表的连续查询与定义物化视图的查询非常相似。</p>
<p>需要注意的是，连续查询的结果在语义上总是等同于在输入表的快照上以批处理模式执行相同查询的结果。</p>
<p>下图直观地展示了流、动态表和连续查询的关系。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/table-streaming/stream-query-stream.png" alt="img"></p>
<ol>
<li>一个流被转换为一个动态表。</li>
<li>对动态表进行连续查询，得到一个新的动态表。</li>
<li>产生的动态表又被转换回流。</li>
</ol>
<p>注意：动态表首先是一个逻辑概念。动态表在查询执行过程中不一定（完全）实体化。</p>
<p>在下文中，我们将解释动态表和连续查询的概念，其点击事件流的模式如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">[</span>
  <span class="err">user:</span>  <span class="err">VARCHAR</span><span class="p">,</span>   <span class="err">//</span> <span class="err">the</span> <span class="err">name</span> <span class="err">of</span> <span class="err">the</span> <span class="err">user</span>
  <span class="err">cTime:</span> <span class="err">TIMESTAMP</span><span class="p">,</span> <span class="err">//</span> <span class="err">the</span> <span class="err">time</span> <span class="err">when</span> <span class="err">the</span> <span class="err">URL</span> <span class="err">was</span> <span class="err">accessed</span>
  <span class="err">url:</span>   <span class="err">VARCHAR</span>    <span class="err">//</span> <span class="err">the</span> <span class="err">URL</span> <span class="err">that</span> <span class="err">was</span> <span class="err">accessed</span> <span class="err">by</span> <span class="err">the</span> <span class="err">user</span>
<span class="p">]</span>
</code></pre></div><h2 id="在流上定义一个表">在流上定义一个表</h2>
<p>为了用关系查询来处理一个流，必须把它转换成一个表。从概念上讲，流的每一条记录都被解释为对生成的表进行 INSERT 修改。从本质上讲，我们是从一个仅有 INSERT 的 changelog 流建立一个表。</p>
<p>下图直观地展示了点击事件流（左手边）是如何转换为表（右手边）的。随着更多的点击流记录被插入，生成的表在不断增长。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/table-streaming/append-mode.png" alt="img"></p>
<p>注意：一个定义在流上的表在内部是不被实现的。</p>
<h3 id="连续查询">连续查询</h3>
<p>连续查询是在动态表上进行评估，并生成一个新的动态表作为结果。与批处理查询不同，连续查询永远不会终止，并根据输入表的更新更新其结果表。在任何时间点上，连续查询的结果在语义上等同于在输入表的快照上以批处理模式执行相同查询的结果。</p>
<p>在下面我们展示了在点击事件流上定义的点击表上的两个查询示例。</p>
<p>第一个查询是一个简单的 GROUP-BY COUNT 聚合查询。它对用户字段的点击表进行分组，并统计访问的 URL 数量。下图显示了当点击表更新了更多的行时，查询是如何随着时间的推移进行评估的。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/table-streaming/query-groupBy-cnt.png" alt="img"></p>
<p>查询开始时，点击表（左侧）为空。当第一条记录被插入到 clicks 表中时，查询开始计算结果表。插入第一行 <code>[Mary, ./home]</code> 后，结果表（右侧，顶部）由一条行 <code>[Mary, 1]</code> 组成。当第二条记录 <code>[Bob, ./cart]</code> 插入点击表后，查询更新结果表，插入一条新的记录 <code>[Bob, 1]</code>。第三条记录 <code>[Mary, ./prod?id=1]</code> 产生对已经计算好的结果行的更新，这样 <code>[Mary, 1]</code> 就更新为 <code>[Mary, 2]</code>。最后，查询将第三条记录 <code>[Liz，1]</code> 插入到结果表中，这时第四条记录被追加到点击表中。</p>
<p>第二个查询与第一个查询类似，但将点击表除了用户属性也分组在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#group-windows">一个小时滚动窗口</a>上，然后再统计 URL 的数量（窗口等基于时间的计算是基于特殊的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html">时间属性</a>，后面会讨论）。同样，图中显示了不同时间点的输入和输出，以直观地显示动态表的变化性质。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/table-streaming/query-groupBy-window-cnt.png" alt="img"></p>
<p>和以前一样，输入表的点击率在左边显示。查询每隔一小时持续计算结果并更新结果表。clicks 表包含四条记录，时间戳（cTime）在 12:00:00 和 12:59:59 之间。查询从这个输入中计算出两条结果行（每个用户一条），并将它们追加到结果表中。对于 13:00:00 和 13:59:59 之间的下一个窗口，点击表包含三条记录，结果是另外两条记录被追加到结果表中。随着时间的推移，更多的行被追加到点击表中，结果表会被更新。</p>
<h3 id="更新和追加查询">更新和追加查询</h3>
<p>虽然这两个例子查询看起来很相似（都是计算一个分组计数合计），但它们在一个重要方面有所不同。</p>
<ul>
<li>第一个查询更新了之前发出的结果，即定义结果表的 changelog 流包含了 INSERT 和 UPDATE 变化。</li>
<li>第二个查询只对结果表进行追加，即结果表的 changelog 流只包含 insert 更改。</li>
</ul>
<p>查询产生的是只追加表还是更新表有一定的影响。</p>
<ul>
<li>产生更新变化的查询通常要维护更多的状态（见下节）。</li>
<li>将仅有附录的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/dynamic_tables.html#table-to-stream-conversion">表转换为流</a>与更新表的转换是不同的（参见表到流的转换部分）。</li>
</ul>
<h3 id="查询限制">查询限制</h3>
<p>许多（但不是全部）语义有效的查询可以作为流上的连续查询来评估。有些查询的计算成本太高，要么是由于它们需要维护的状态大小，要么是由于计算更新太贵。</p>
<ul>
<li>状态大小。连续查询是在无边界的流上进行评估的，通常应该运行数周或数月。因此，一个连续查询处理的数据总量可能非常大。必须更新之前发出的结果的查询需要维护所有发出的行，以便能够更新它们。例如，第一个示例查询需要存储每个用户的 URL 计数，以便能够增加计数，并在输入表收到新行时发出新结果。如果只跟踪注册用户，需要维护的计数数量可能不会太高。但是，如果非注册用户被分配了一个唯一的用户名，那么需要维护的次数会随着时间的推移而增加，最终可能会导致查询失败。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="p">,</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">clicks</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">user</span><span class="p">;</span><span class="w">
</span></code></pre></div><ul>
<li>计算更新。有些查询需要重新计算和更新大部分发出的结果行，即使只增加或更新一条输入记录。显然，这种查询并不适合作为连续查询来执行。一个例子是下面的查询，它根据最后一次点击的时间为每个用户计算一个 rank。只要点击表收到一条新的记录，该用户的 lastAction 就会被更新，必须计算新的 rank。但是由于两行不能有相同的 rank，所以所有排名较低的行也需要更新。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="p">,</span><span class="w"> </span><span class="n">RANK</span><span class="p">()</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="p">(</span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">lastLogin</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="p">,</span><span class="w"> </span><span class="k">MAX</span><span class="p">(</span><span class="n">cTime</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">lastAction</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">clicks</span><span class="w"> </span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">user</span><span class="w">
</span><span class="w"></span><span class="p">);</span><span class="w">
</span></code></pre></div><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/query_configuration.html">查询配置</a>页面讨论了控制连续查询执行的参数。有些参数可用于以维护状态的大小来换取结果的准确性。</p>
<h2 id="表到流的转换">表到流的转换</h2>
<p>动态表可以像普通的数据库表一样，通过 INSERT、UPDATE 和 DELETE 的修改不断地进行修改。它可能是一张只有一行的表，不断地更新，也可能是一张只有插入的表，没有 UPDATE 和 DELETE 的修改，或者是介于两者之间的任何表。</p>
<p>当把动态表转换为流或写入外部系统时，需要对这些变化进行编码。Flink 的表 API 和 SQL 支持三种方式来编码动态表的变化。</p>
<ul>
<li>
<p>只添加流。一个只被 INSERT 修改的动态表，可以通过发出插入的行来转换成流。</p>
</li>
<li>
<p>撤回流。缩回流是指有两种消息的流，即添加消息和缩回消息。通过将 INSERT 变更编码为添加消息，将 DELETE 变更编码为回撤消息，将 UPDATE 变更编码为更新（上一条）行的回撤消息和更新（新一条）行的添加消息，就可以将一张动态表转换为回撤流。下图直观地展示了动态表转换为回撤流的过程。</p>
</li>
</ul>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/table-streaming/undo-redo-mode.png" alt="img"></p>
<ul>
<li>Upsert 流。Upsert 流是一个有两种消息类型的流，即 upsert 消息和删除消息。一个动态表被转换为 upsert 流需要一个（可能是复合的）唯一键。通过将 INSERT 和 UPDATE 更改编码为 upsert 消息，将 DELETE 更改编码为 delete 消息，将具有唯一键的动态表转换为流。消耗流的操作者需要知道唯一键属性，以便正确应用消息。与 retract 流的主要区别在于 update 变更用一条消息进行编码，因此效率更高。下图直观地展示了动态表转换为 update 流的过程。</li>
</ul>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/table-streaming/redo-mode.png" alt="img"></p>
<p>将动态表转换为 DataStream 的 API 在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#convert-a-table-into-a-datastream">通用概念</a>页面上讨论。请注意，在将动态表转换为 DataStream 时，只支持追加和收回流。在 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sourceSinks.html#define-a-tablesink">TableSources 和 TableSinks</a> 页面上讨论了将动态表发射到外部系统的 TableSink 接口。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/dynamic_tables.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/dynamic_tables.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[可查询状态]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-queryable-state-beta/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Queryable State Beta</blockquote><p>注意：可查询状态的客户端 API 目前处于不断发展的状态，对所提供接口的稳定性不做保证。在即将到来的 Flink 版本中，客户端的 API 很可能会有突破性的变化。</p>
<p>简而言之，这个功能将 Flink 的 managed keyed (partitioned) state（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">Working with State</a>）暴露给外界，并允许用户从 Flink 外部查询作业的状态。对于某些场景来说，可查询状态消除了与外部系统（如键值存储）进行分布式操作/交易的需求，而这往往是实践中的瓶颈。此外，该功能对于调试目的可能特别有用。</p>
<p>注意事项: 当查询一个状态对象时，该对象是在没有任何同步或复制的情况下从一个并发线程访问的。这是一个设计上的选择，因为上述任何一种情况都会导致作业延迟的增加，这是我们想要避免的。因为任何使用 Java 堆空间的状态后端，如 MemoryStateBackend 或 FsStateBackend，在检索值时都不会使用副本，而是直接引用存储的值，所以读-修改-写模式是不安全的，可能会导致可查询状态服务器因并发修改而失败。RocksDBStateBackend 则可以避免这些问题。</p>
<h2 id="架构">架构</h2>
<p>在展示如何使用可查询状态之前，先简单介绍一下构成它的实体。Queryable State 功能由三个主要实体组成。</p>
<ol>
<li>QueryableStateClient，它（可能）运行在 Flink 集群之外，并提交用户查询。</li>
<li>QueryableStateClientProxy，它运行在每个 TaskManager 上（即 Flink 集群内部），负责接收客户端的查询，代表他从负责的 TaskManager 中获取所请求的状态，并将其返回给客户端，以及</li>
<li>QueryableStateServer，它运行在每个 TaskManager 上，负责为本地存储的状态提供服务。</li>
</ol>
<p>客户端连接到其中一个代理，并发送一个与特定键 <em>k</em> 相关联的状态的请求。正如在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">使用状态</a>中所述，keyed state 被组织在键组(Key Groups)中，每个 TaskManager 都被分配了一些这样的键组(Key Groups)。为了发现哪个 TaskManager 负责持有 <em>k</em> 的键组，代理将询问 JobManager。根据答案，代理将查询运行在该 TaskManager 上的 QueryableStateServer，以获取与 <em>k</em> 相关联的状态，并将响应转发到客户端。</p>
<h2 id="激活可查询状态">激活可查询状态</h2>
<p>要在 Flink 集群上启用可查询状态，你需要做以下工作。</p>
<ol>
<li>将 <code>flink-queryable-state-runtime_2.11-1.11.0.jar</code> 从 <a href="https://flink.apache.org/downloads.html">Flink 发行版</a>的 <code>opt/</code> 文件夹中复制到 <code>lib/</code>  文件夹中。</li>
<li>设置属性 <code>queryable-state.enable</code> 为 <code>true</code>。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html#queryable-state">配置</a>文档了解详情和附加参数。</li>
</ol>
<p>要验证您的群集是否在启用可查询状态后运行，请检查任何 TaskManager 的日志中的行。&ldquo;Started the Queryable State Proxy Server @ &hellip;&quot;。</p>
<h3 id="使状态可查询">使状态可查询</h3>
<p>现在你已经在集群上激活了可查询状态，现在是时候看看如何使用它了。为了使一个状态对外界可见，它需要通过使用以下方式明确地成为可查询状态。</p>
<ul>
<li>QueryableStateStream, 一个方便的对象，它作为一个接收器(sink)，并把它的传入值作为可查询的状态提供，或者是</li>
<li>stateDescriptor.setQueryable(String queryableStateName) 方法，使得状态描述符所代表的 keyed state，可以查询。</li>
</ul>
<p>下面的章节将解释这两种方法的使用。</p>
<h3 id="可查询的状态流">可查询的状态流</h3>
<p>在 KeyedStream 上调用 <code>.asQueryableState(stateName, stateDescriptor)</code> 会返回一个 <code>QueryableStateStream</code>，它将其值作为可查询状态提供。根据状态的类型，<code>asQueryableState()</code> 方法有以下几种变体。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="c1">// ValueState
</span><span class="c1"></span><span class="n">QueryableStateStream</span> <span class="nf">asQueryableState</span><span class="o">(</span>
    <span class="n">String</span> <span class="n">queryableStateName</span><span class="o">,</span>
    <span class="n">ValueStateDescriptor</span> <span class="n">stateDescriptor</span><span class="o">)</span>

<span class="c1">// Shortcut for explicit ValueStateDescriptor variant
</span><span class="c1"></span><span class="n">QueryableStateStream</span> <span class="nf">asQueryableState</span><span class="o">(</span><span class="n">String</span> <span class="n">queryableStateName</span><span class="o">)</span>

<span class="c1">// FoldingState
</span><span class="c1"></span><span class="n">QueryableStateStream</span> <span class="nf">asQueryableState</span><span class="o">(</span>
    <span class="n">String</span> <span class="n">queryableStateName</span><span class="o">,</span>
    <span class="n">FoldingStateDescriptor</span> <span class="n">stateDescriptor</span><span class="o">)</span>

<span class="c1">// ReducingState
</span><span class="c1"></span><span class="n">QueryableStateStream</span> <span class="nf">asQueryableState</span><span class="o">(</span>
    <span class="n">String</span> <span class="n">queryableStateName</span><span class="o">,</span>
    <span class="n">ReducingStateDescriptor</span> <span class="n">stateDescriptor</span><span class="o">)</span>
</code></pre></div><p>注意：没有可查询的 <code>ListState</code> 接收器，因为这会导致一个不断增长的列表，可能无法清理，因此最终会消耗过多的内存。</p>
<p>返回的 <code>QueryableStateStream</code> 可以被看作是一个接收器(sink)，不能被进一步转换。在内部，一个 <code>QueryableStateStream</code> 会被翻译成一个操作符，它使用所有传入的记录来更新可查询状态实例。更新逻辑是由 <code>asQueryableState</code> 调用中提供的 <code>StateDescriptor</code> 的类型暗示的。在像下面这样的程序中，keyed stream 的所有记录将通过 <code>ValueState.update(value)</code> 来更新状态实例:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">stream</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">asQueryableState</span><span class="o">(</span><span class="s">&#34;query-name&#34;</span><span class="o">)</span>
</code></pre></div><p>这就像 Scala API 的 <code>flatMapWithState</code> 一样。</p>
<h3 id="管理的-keyed-state">管理的 Keyed State</h3>
<p>通过 <code>StateDescriptor.setQueryable(String queryableStateName)</code> 使相应的状态描述符成为可查询的状态，可以使操作符的托管键控状态(参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html#using-managed-keyed-state">使用 Managed Keyed State)</a>)成为可查询的状态，如下面的例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">ValueStateDescriptor</span><span class="o">&lt;</span><span class="nc">Tuple2</span><span class="o">&lt;</span><span class="nc">Long</span><span class="o">,</span> <span class="nc">Long</span><span class="o">&gt;&gt;</span> <span class="n">descriptor</span> <span class="k">=</span>
        <span class="k">new</span> <span class="nc">ValueStateDescriptor</span><span class="o">&lt;&gt;(</span>
                <span class="s">&#34;average&#34;</span><span class="o">,</span> <span class="c1">// the state name
</span><span class="c1"></span>                <span class="nc">TypeInformation</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="k">new</span> <span class="nc">TypeHint</span><span class="o">&lt;</span><span class="nc">Tuple2</span><span class="o">&lt;</span><span class="nc">Long</span><span class="o">,</span> <span class="nc">Long</span><span class="o">&gt;&gt;()</span> <span class="o">{}));</span> <span class="c1">// type information
</span><span class="c1"></span>
<span class="n">descriptor</span><span class="o">.</span><span class="n">setQueryable</span><span class="o">(</span><span class="s">&#34;query-name&#34;</span><span class="o">);</span> <span class="c1">// queryable state name
</span></code></pre></div><p>注意：<code>queryableStateName</code> 参数可以任意选择，并且只用于查询。它不一定要与状态本身的名称相同。</p>
<p>这个变体对于哪种类型的状态可以被查询没有限制。这意味着它可以用于任何 ValueState、ReduceState、ListState、MapState、AggregatingState 以及目前已被废弃的 FoldingState。</p>
<h3 id="查询状态">查询状态</h3>
<p>到目前为止，你已经设置了你的集群以可查询的状态运行，并且你已经将你的（部分）状态声明为可查询。现在是时候看看如何查询这个状态了。</p>
<p>为此，你可以使用 <code>QueryableStateClient</code> 辅助类。它可以在 <code>flink-queryable-state-client jar</code> 中找到，它必须和 flink-core 一起被显式地包含在项目的 pom.xml 中作为依赖，如下所示。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-core<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-queryable-state-client-java<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>更多的内容，可以查看如何<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html">设置 Flink 程序</a>。</p>
<p><code>QueryableStateClient</code> 会将你的查询提交给内部代理，然后代理会处理你的查询并返回最终结果。初始化客户端的唯一要求是提供一个有效的 TaskManager 主机名（记住每个 TaskManager 上都有一个可查询状态代理运行）和代理监听的端口。更多关于如何配置代理和状态服务器端口的信息请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/queryable_state.html#configuration">配置部分</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">QueryableStateClient</span> <span class="n">client</span> <span class="o">=</span> <span class="k">new</span> <span class="n">QueryableStateClient</span><span class="o">(</span><span class="n">tmHostname</span><span class="o">,</span> <span class="n">proxyPort</span><span class="o">)</span>
</code></pre></div><p>客户端准备好后，要查询一个类型为 V 的状态，与类型为 K 的键相关联，可以使用该方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">CompletableFuture</span><span class="o">&lt;</span><span class="n">S</span><span class="o">&gt;</span> <span class="nf">getKvState</span><span class="o">(</span>
    <span class="n">JobID</span> <span class="n">jobId</span><span class="o">,</span>
    <span class="n">String</span> <span class="n">queryableStateName</span><span class="o">,</span>
    <span class="n">K</span> <span class="n">key</span><span class="o">,</span>
    <span class="n">TypeInformation</span><span class="o">&lt;</span><span class="n">K</span><span class="o">&gt;</span> <span class="n">keyTypeInfo</span><span class="o">,</span>
    <span class="n">StateDescriptor</span><span class="o">&lt;</span><span class="n">S</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="n">stateDescriptor</span><span class="o">)</span>
</code></pre></div><p>以上返回一个 CompletableFuture，最终持有 ID 为 jobID 的作业的 <code>queryableStateName</code> 所标识的可查询状态实例的状态值。key 是你对其状态感兴趣的键，keyTypeInfo 将告诉 Flink 如何序列化/解序列化它。最后，<code>stateDescriptor</code> 包含了关于所请求的状态的必要信息，即它的类型（Value、Reduce 等）和如何序列化/解序列化它的必要信息。</p>
<p>细心的读者会注意到，返回的 future 包含一个 S 类型的值，即一个包含实际值的 <code>State</code> 对象。这可以是 Flink 支持的任何一种状态类型。ValueState，ReduceState，ListState，MapState，AggregatingState，以及目前已经废弃的 FoldingState。</p>
<p>注意：这些状态对象不允许对包含的状态进行修改。您可以使用它们来获取状态的实际值，例如使用 <code>valueState.get()</code>，或者迭代包含的 <code>&lt;K，V&gt;</code> 条目，例如使用 <code>mapState.entry()</code>，但您不能修改它们。举个例子，在返回的列表状态上调用 <code>add()</code> 方法会抛出一个 <code>UnsupportedOperationException</code>。</p>
<p>注意：客户端是异步的，可以被多个线程共享。在未使用时需要通过 <code>QueryableStateClient.shutdown()</code> 来关闭它，以释放资源。</p>
<h3 id="例子">例子</h3>
<p>下面的例子扩展了 <code>CountWindowAverage</code> 的例子(请看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html#using-managed-keyed-state">使用 Managed Keyed State</a>)，使其可查询，并展示了如何查询这个值。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">CountWindowAverage</span> <span class="kd">extends</span> <span class="n">RichFlatMapFunction</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="o">{</span>

    <span class="kd">private</span> <span class="kd">transient</span> <span class="n">ValueState</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">sum</span><span class="o">;</span> <span class="c1">// a tuple containing the count and the sum
</span><span class="c1"></span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">flatMap</span><span class="o">(</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="n">input</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
        <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="n">currentSum</span> <span class="o">=</span> <span class="n">sum</span><span class="o">.</span><span class="na">value</span><span class="o">();</span>
        <span class="n">currentSum</span><span class="o">.</span><span class="na">f0</span> <span class="o">+=</span> <span class="n">1</span><span class="o">;</span>
        <span class="n">currentSum</span><span class="o">.</span><span class="na">f1</span> <span class="o">+=</span> <span class="n">input</span><span class="o">.</span><span class="na">f1</span><span class="o">;</span>
        <span class="n">sum</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="n">currentSum</span><span class="o">);</span>

        <span class="k">if</span> <span class="o">(</span><span class="n">currentSum</span><span class="o">.</span><span class="na">f0</span> <span class="o">&gt;=</span> <span class="n">2</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">out</span><span class="o">.</span><span class="na">collect</span><span class="o">(</span><span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;&gt;(</span><span class="n">input</span><span class="o">.</span><span class="na">f0</span><span class="o">,</span> <span class="n">currentSum</span><span class="o">.</span><span class="na">f1</span> <span class="o">/</span> <span class="n">currentSum</span><span class="o">.</span><span class="na">f0</span><span class="o">));</span>
            <span class="n">sum</span><span class="o">.</span><span class="na">clear</span><span class="o">();</span>
        <span class="o">}</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">open</span><span class="o">(</span><span class="n">Configuration</span> <span class="n">config</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">ValueStateDescriptor</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">descriptor</span> <span class="o">=</span>
                <span class="k">new</span> <span class="n">ValueStateDescriptor</span><span class="o">&lt;&gt;(</span>
                        <span class="s">&#34;average&#34;</span><span class="o">,</span> <span class="c1">// the state name
</span><span class="c1"></span>                        <span class="n">TypeInformation</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="k">new</span> <span class="n">TypeHint</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;()</span> <span class="o">{}));</span> <span class="c1">// type information
</span><span class="c1"></span>        <span class="n">descriptor</span><span class="o">.</span><span class="na">setQueryable</span><span class="o">(</span><span class="s">&#34;query-name&#34;</span><span class="o">);</span>
        <span class="n">sum</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getState</span><span class="o">(</span><span class="n">descriptor</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
<span class="n">Once</span> <span class="n">used</span> <span class="n">in</span> <span class="n">a</span> <span class="n">job</span><span class="o">,</span> <span class="n">you</span> <span class="n">can</span> <span class="n">retrieve</span> <span class="n">the</span> <span class="n">job</span> <span class="n">ID</span> <span class="n">and</span> <span class="n">then</span> <span class="n">query</span> <span class="n">any</span> <span class="n">key</span><span class="err">’</span><span class="n">s</span> <span class="n">current</span> <span class="n">state</span> <span class="n">from</span> <span class="k">this</span> <span class="n">operator</span><span class="o">:</span>

<span class="n">QueryableStateClient</span> <span class="n">client</span> <span class="o">=</span> <span class="k">new</span> <span class="n">QueryableStateClient</span><span class="o">(</span><span class="n">tmHostname</span><span class="o">,</span> <span class="n">proxyPort</span><span class="o">);</span>

<span class="c1">// the state descriptor of the state to be fetched.
</span><span class="c1"></span><span class="n">ValueStateDescriptor</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;</span> <span class="n">descriptor</span> <span class="o">=</span>
        <span class="k">new</span> <span class="n">ValueStateDescriptor</span><span class="o">&lt;&gt;(</span>
          <span class="s">&#34;average&#34;</span><span class="o">,</span>
          <span class="n">TypeInformation</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="k">new</span> <span class="n">TypeHint</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;()</span> <span class="o">{}));</span>

<span class="n">CompletableFuture</span><span class="o">&lt;</span><span class="n">ValueState</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;&gt;&gt;</span> <span class="n">resultFuture</span> <span class="o">=</span>
        <span class="n">client</span><span class="o">.</span><span class="na">getKvState</span><span class="o">(</span><span class="n">jobId</span><span class="o">,</span> <span class="s">&#34;query-name&#34;</span><span class="o">,</span> <span class="n">key</span><span class="o">,</span> <span class="n">BasicTypeInfo</span><span class="o">.</span><span class="na">LONG_TYPE_INFO</span><span class="o">,</span> <span class="n">descriptor</span><span class="o">);</span>

<span class="c1">// now handle the returned value
</span><span class="c1"></span><span class="n">resultFuture</span><span class="o">.</span><span class="na">thenAccept</span><span class="o">(</span><span class="n">response</span> <span class="o">-&gt;</span> <span class="o">{</span>
        <span class="k">try</span> <span class="o">{</span>
            <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="n">res</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="na">get</span><span class="o">();</span>
        <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
        <span class="o">}</span>
<span class="o">});</span>
</code></pre></div><h2 id="配置">配置</h2>
<p>以下配置参数会影响可查询状态服务器和客户端的行为，它们被定义在 <code>QueryableStateOptions</code> 中。</p>
<h3 id="状态服务器">状态服务器</h3>
<ul>
<li><code>queryable-state.server.ports</code>：可查询状态服务器的服务器端口范围。如果在同一台机器上运行多个 task manager，这对避免端口冲突很有用。指定的范围可以是：一个端口: &ldquo;9123&rdquo;，一个端口范围: &ldquo;50100-50200&rdquo;，或者一个范围和或点的列表: &ldquo;50100-50200,50300-50400,51234&rdquo;。默认端口为 9067。</li>
<li><code>queryable-state.server.network-threads</code>: 接收状态服务器传入请求的网络（事件循环）线程数（0 =&gt; #slots）。</li>
<li><code>queryable-state.server.query-threads</code>: 为状态服务器处理/服务传入请求的线程数（0 =&gt; #slots）。</li>
</ul>
<h3 id="代理">代理</h3>
<ul>
<li><code>queryable-state.proxy.ports</code>：可查询状态代理服务器的端口范围。如果在同一台机器上运行多个 task manager，这对避免端口冲突很有用。指定的范围可以是：一个端口: &ldquo;9123&rdquo;，一个端口范围: &ldquo;50100-50200&rdquo;，或者一个范围和或点的列表: &ldquo;50100-50200,50300-50400,51234&rdquo;。默认端口为 9069。</li>
<li><code>queryable-state.proxy.network-threads</code>：为客户端代理接收传入请求的网络（事件循环）线程数（0 =&gt; #slots）。</li>
<li><code>queryable-state.proxy.query-threads</code>：为客户端代理处理/服务传入请求的线程数（0 =&gt; #slots）。</li>
</ul>
<h3 id="限制条件">限制条件</h3>
<ul>
<li>可查询状态的生命周期与任务的生命周期绑定，例如，任务在启动时注册可查询状态，在处置时取消注册。在未来的版本中，我们希望将其解耦，以便在任务完成后允许查询，并通过状态复制加快恢复速度。</li>
<li>关于可用 KvState 的通知是通过一个简单的告诉发生的。将来应该改进这个功能，使其更加强大，包括询问和确认。</li>
<li>服务器和客户端会跟踪查询的统计数据。目前默认情况下，这些数据是被禁用的，因为它们不会暴露在任何地方。一旦有更好的支持通过 Metrics 系统发布这些数字，我们应该启用统计。</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/queryable_state.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/queryable_state.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[处理应用程序参数]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-handling-application-parameters/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-handling-application-parameters/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Handling Application Parameters</blockquote><h1 id="处理应用程序参数">处理应用程序参数</h1>
<p>几乎所有的 Flink 应用，包括批处理和流式应用，都依赖于外部配置参数，它们用于指定输入和输出源（如路径或地址）、系统参数（并行性、运行时配置）和应用特定参数（通常在用户函数中使用）。它们用于指定输入和输出源（如路径或地址）、系统参数（并行性、运行时配置）和应用程序特定参数（通常在用户函数中使用）。</p>
<p>Flink 提供了一个名为 ParameterTool 的简单工具，为解决这些问题提供一些基本的工具。请注意，你不一定要使用这里描述的 ParameterTool。其他框架如 Commons CLI和argparse4j 也能很好地与 Flink 一起工作。</p>
<p>将你的配置值导入 ParameterTool 之中</p>
<p>ParameterTool 提供了一组预定义的静态方法来读取配置。该工具内部期待的是一个 <code>Map&lt;String，String&gt;</code>，所以很容易将其与自己的配置风格整合在一起。</p>
<p>从 <code>.properties</code> 文件中</p>
<p>下面的方法将读取一个属性文件并提供键/值对。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">String</span> <span class="n">propertiesFilePath</span> <span class="o">=</span> <span class="s">&#34;/home/sam/flink/myjob.properties&#34;</span><span class="o">;</span>
<span class="n">ParameterTool</span> <span class="n">parameter</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromPropertiesFile</span><span class="o">(</span><span class="n">propertiesFilePath</span><span class="o">);</span>

<span class="n">File</span> <span class="n">propertiesFile</span> <span class="o">=</span> <span class="k">new</span> <span class="n">File</span><span class="o">(</span><span class="n">propertiesFilePath</span><span class="o">);</span>
<span class="n">ParameterTool</span> <span class="n">parameter</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromPropertiesFile</span><span class="o">(</span><span class="n">propertiesFile</span><span class="o">);</span>

<span class="n">InputStream</span> <span class="n">propertiesFileInputStream</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FileInputStream</span><span class="o">(</span><span class="n">file</span><span class="o">);</span>
<span class="n">ParameterTool</span> <span class="n">parameter</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromPropertiesFile</span><span class="o">(</span><span class="n">propertiesFileInputStream</span><span class="o">);</span>
</code></pre></div><p>从命令行参数来看</p>
<p>这就允许从命令行中获取 <code>--input hdfs://mydata --elements 42</code> 这样的参数。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">ParameterTool</span> <span class="n">parameter</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromArgs</span><span class="o">(</span><span class="n">args</span><span class="o">);</span>
    <span class="c1">// .. regular code ..
</span></code></pre></div><p>从系统属性</p>
<p>当启动 JVM 时，你可以将系统属性传递给它。<code>-Dinput=hdfs://mydata</code>。你也可以从这些系统属性中初始化 ParameterTool。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ParameterTool</span> <span class="n">parameter</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromSystemProperties</span><span class="o">();</span>
</code></pre></div><p>在 Flink 程序中使用参数</p>
<p>现在我们已经从某个地方得到了参数（见上文），我们可以以各种方式使用它们。</p>
<p>直接从 ParameterTool 中使用</p>
<p>ParameterTool 本身有访问值的方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ParameterTool</span> <span class="n">parameters</span> <span class="o">=</span> <span class="c1">// ...
</span><span class="c1"></span><span class="n">parameter</span><span class="o">.</span><span class="na">getRequired</span><span class="o">(</span><span class="s">&#34;input&#34;</span><span class="o">);</span>
<span class="n">parameter</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="s">&#34;output&#34;</span><span class="o">,</span> <span class="s">&#34;myDefaultValue&#34;</span><span class="o">);</span>
<span class="n">parameter</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="s">&#34;expectedCount&#34;</span><span class="o">,</span> <span class="o">-</span><span class="n">1L</span><span class="o">);</span>
<span class="n">parameter</span><span class="o">.</span><span class="na">getNumberOfParameters</span><span class="o">()</span>
<span class="c1">// .. there are more methods available.
</span></code></pre></div><p>你可以在客户端提交应用程序的 <code>main()</code> 方法中直接使用这些方法的返回值。例如，你可以这样设置一个操作符的并行性。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ParameterTool</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromArgs</span><span class="o">(</span><span class="n">args</span><span class="o">);</span>
<span class="kt">int</span> <span class="n">parallelism</span> <span class="o">=</span> <span class="n">parameters</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="s">&#34;mapParallelism&#34;</span><span class="o">,</span> <span class="n">2</span><span class="o">);</span>
<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">Tokenizer</span><span class="o">()).</span><span class="na">setParallelism</span><span class="o">(</span><span class="n">parallelism</span><span class="o">);</span>
</code></pre></div><p>由于 ParameterTool 是可序列化的，所以你可以把它传递给函数本身。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ParameterTool</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromArgs</span><span class="o">(</span><span class="n">args</span><span class="o">);</span>
<span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">Tokenizer</span><span class="o">(</span><span class="n">parameters</span><span class="o">));</span>
</code></pre></div><p>然后在函数内部使用它从命令行获取值。</p>
<p>全局注册参数</p>
<p>在 ExecutionConfig 中注册为全局作业参数的参数可以作为配置值从 JobManager Web 界面和用户定义的所有功能中访问。</p>
<p>全局注册参数。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ParameterTool</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">ParameterTool</span><span class="o">.</span><span class="na">fromArgs</span><span class="o">(</span><span class="n">args</span><span class="o">);</span>

<span class="c1">// set up the execution environment
</span><span class="c1"></span><span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>
<span class="n">env</span><span class="o">.</span><span class="na">getConfig</span><span class="o">().</span><span class="na">setGlobalJobParameters</span><span class="o">(</span><span class="n">parameters</span><span class="o">);</span>
</code></pre></div><p>在任何丰富的用户功能中访问它们。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kd">class</span> <span class="nc">Tokenizer</span> <span class="kd">extends</span> <span class="n">RichFlatMapFunction</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="o">{</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">flatMap</span><span class="o">(</span><span class="n">String</span> <span class="n">value</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="o">{</span>
	<span class="n">ParameterTool</span> <span class="n">parameters</span> <span class="o">=</span> <span class="o">(</span><span class="n">ParameterTool</span><span class="o">)</span>
	    <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getExecutionConfig</span><span class="o">().</span><span class="na">getGlobalJobParameters</span><span class="o">();</span>
	<span class="n">parameters</span><span class="o">.</span><span class="na">getRequired</span><span class="o">(</span><span class="s">&#34;input&#34;</span><span class="o">);</span>
	<span class="c1">// .. do more ..
</span></code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/application_parameters.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/application_parameters.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/parameters" term="parameters" label="Parameters" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[实验特性]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-experimental-features/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-experimental-features/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Experimental Features</blockquote><h2 id="实验特性">实验特性</h2>
<p>本节介绍 DataStream API 中的实验性功能。实验性功能仍在不断发展，可能是不稳定的、不完整的，或者在未来的版本中会有很大的变化。</p>
<h3 id="将预先分割的数据流重新解释为-keyed-流">将预先分割的数据流重新解释为 keyed 流</h3>
<p>我们可以将一个预分区的数据流重新解释为一个 keyed 流，以避免洗牌。</p>
<p>警告：重新解释的数据流必须已经被预分区了，其方式与 Flink 的 keyBy 在洗牌中对数据的分区方式完全相同，即键组分配。</p>
<p>一个用例是两个作业之间的物化洗牌：第一个作业执行 keyBy 洗牌，并将每个输出物化为一个分区。第二个作业有源，对于每个并行实例，从第一个作业创建的相应分区中读取。现在可以将这些源重新解释为 keyed 流，例如应用窗口化。请注意，这个技巧使得第二个作业的并行性很尴尬，这对细粒度的恢复方案很有帮助。</p>
<p>这个重新解释的功能是通过 DataStreamUtils 暴露的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">static</span> <span class="o">&lt;</span><span class="n">T</span><span class="o">,</span> <span class="n">K</span><span class="o">&gt;</span> <span class="nc">KeyedStream</span><span class="o">&lt;</span><span class="n">T</span><span class="o">,</span> <span class="n">K</span><span class="o">&gt;</span> <span class="n">reinterpretAsKeyedStream</span><span class="o">(</span>
    <span class="nc">DataStream</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="n">stream</span><span class="o">,</span>
    <span class="nc">KeySelector</span><span class="o">&lt;</span><span class="n">T</span><span class="o">,</span> <span class="n">K</span><span class="o">&gt;</span> <span class="n">keySelector</span><span class="o">,</span>
    <span class="nc">TypeInformation</span><span class="o">&lt;</span><span class="n">K</span><span class="o">&gt;</span> <span class="n">typeInfo</span><span class="o">)</span>
</code></pre></div><p>给定一个基流(base stream)、一个键选择器和类型信息，该方法从基流创建一个 keyed 流。</p>
<p>代码示例:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
<span class="k">val</span> <span class="n">source</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">new</span> <span class="nc">DataStreamUtils</span><span class="o">(</span><span class="n">source</span><span class="o">).</span><span class="n">reinterpretAsKeyedStream</span><span class="o">((</span><span class="n">in</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">in</span><span class="o">)</span>
  <span class="o">.</span><span class="n">timeWindow</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
  <span class="o">.</span><span class="n">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span>
  <span class="o">.</span><span class="n">addSink</span><span class="o">(</span><span class="k">new</span> <span class="nc">DiscardingSink</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/experimental.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/experimental.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[广播状态模式]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-the-broadcast-state-pattern/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-the-broadcast-state-pattern/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>The Broadcast State Pattern</blockquote><p>在本节中，您将了解如何在实践中使用广播状态。请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/concepts/stateful-stream-processing.html">Stateful Stream Processing</a> 来了解有状态流处理背后的概念。</p>
<h2 id="提供的-api">提供的 API</h2>
<p>为了展示所提供的 API，我们将在介绍它们的全部功能之前先举一个例子。作为我们的运行示例，我们将使用这样的情况：我们有一个不同颜色和形状的对象流，我们希望找到相同颜色的对象对，并遵循特定的模式，例如，一个矩形和一个三角形。我们假设有趣的模式集会随着时间的推移而演变。</p>
<p>在这个例子中，第一个流将包含具有 <code>Color</code> 和 <code>Shape</code> 属性的 <code>Item</code> 类型的元素。另一个流将包含 <code>Rules</code>。</p>
<p>从 <code>Items</code> 流开始，我们只需要按 <code>Color</code> keyBy，因为我们想要相同颜色的对。这将确保相同颜色的元素最终会出现在同一个物理机上。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="c1">// key the items by color
</span><span class="c1"></span><span class="n">KeyedStream</span><span class="o">&lt;</span><span class="n">Item</span><span class="o">,</span> <span class="n">Color</span><span class="o">&gt;</span> <span class="n">colorPartitionedStream</span> <span class="o">=</span> <span class="n">itemStream</span>
                        <span class="o">.</span><span class="na">keyBy</span><span class="o">(</span><span class="k">new</span> <span class="n">KeySelector</span><span class="o">&lt;</span><span class="n">Item</span><span class="o">,</span> <span class="n">Color</span><span class="o">&gt;(){...});</span>
</code></pre></div><p>继续讨论规则，包含规则的流应该被广播到所有下游任务，这些任务应该将它们存储在本地，以便它们可以根据所有传入的项目评估它们。下面的代码段将i)广播规则流，ii)使用提供的 <code>MapStateDescriptor</code>，它将创建规则将被存储的广播状态。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// a map descriptor to store the name of the rule (string) and the rule itself.
</span><span class="c1"></span><span class="nc">MapStateDescriptor</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">Rule</span><span class="o">&gt;</span> <span class="n">ruleStateDescriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MapStateDescriptor</span><span class="o">&lt;&gt;(</span>
			<span class="s">&#34;RulesBroadcastState&#34;</span><span class="o">,</span>
			<span class="nc">BasicTypeInfo</span><span class="o">.</span><span class="nc">STRING_TYPE_INFO</span><span class="o">,</span>
			<span class="nc">TypeInformation</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="k">new</span> <span class="nc">TypeHint</span><span class="o">&lt;</span><span class="nc">Rule</span><span class="o">&gt;()</span> <span class="o">{}));</span>
		
<span class="c1">// broadcast the rules and create the broadcast state
</span><span class="c1"></span><span class="nc">BroadcastStream</span><span class="o">&lt;</span><span class="nc">Rule</span><span class="o">&gt;</span> <span class="n">ruleBroadcastStream</span> <span class="k">=</span> <span class="n">ruleStream</span>
                        <span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="n">ruleStateDescriptor</span><span class="o">);</span>
</code></pre></div><p>最后，为了根据从 <code>Item</code> 流传入的元素来评估 <code>Rules</code>，我们需要。</p>
<ul>
<li>连接(connect)两个流，并且</li>
<li>指定我们的匹配检测逻辑。</li>
</ul>
<p>将一个流（keyed or non-keyed）与 <code>BroadcastStream</code> 连接起来，可以通过在非广播流上调用 <code>connect()</code> 来完成，并将 <code>BroadcastStream</code> 作为一个参数。这将返回一个 <code>BroadcastConnectedStream</code>，我们可以在这个 Stream 上调用一个特殊类型的 <code>CoProcessFunction</code> 来处理。该函数将包含我们的匹配逻辑。该函数的具体类型取决于非广播流的类型。</p>
<ul>
<li>如果它是 <strong>keyed</strong>，那么这个函数就是 <code>KeyedBroadcastProcessFunction</code>。</li>
<li>如果是 <strong>non-keyed,</strong>，那么该函数就是一个 <code>BroadcastProcessFunction</code>。</li>
</ul>
<p>鉴于我们的非广播流是 keyed 的，下面的代码段包含了上述调用。</p>
<p>注意： 连接(connect)应该被调用在非广播流上， 以 <code>BroadcastStream</code> 作为参数。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">DataStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">colorPartitionedStream</span>
                 <span class="o">.</span><span class="na">connect</span><span class="o">(</span><span class="n">ruleBroadcastStream</span><span class="o">)</span>
                 <span class="o">.</span><span class="na">process</span><span class="o">(</span>
                     
                     <span class="c1">// type arguments in our KeyedBroadcastProcessFunction represent: 
</span><span class="c1"></span>                     <span class="c1">//   1. the key of the keyed stream
</span><span class="c1"></span>                     <span class="c1">//   2. the type of elements in the non-broadcast side
</span><span class="c1"></span>                     <span class="c1">//   3. the type of elements in the broadcast side
</span><span class="c1"></span>                     <span class="c1">//   4. the type of the result, here a string
</span><span class="c1"></span>                     
                     <span class="k">new</span> <span class="n">KeyedBroadcastProcessFunction</span><span class="o">&lt;</span><span class="n">Color</span><span class="o">,</span> <span class="n">Item</span><span class="o">,</span> <span class="n">Rule</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;()</span> <span class="o">{</span>
                         <span class="c1">// my matching logic
</span><span class="c1"></span>                     <span class="o">}</span>
                 <span class="o">);</span>
</code></pre></div><h3 id="broadcastprocessfunction-和-keyedbroadcastprocessfunction">BroadcastProcessFunction 和 KeyedBroadcastProcessFunction</h3>
<p>与 <code>CoProcessFunction</code> 一样，这些函数有两个处理方法要实现；<code>processBroadcastElement()</code> 负责处理广播流中的传入元素，<code>processElement()</code> 用于处理非广播流。这些方法的完整签名如下:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">abstract</span> <span class="kd">class</span> <span class="nc">BroadcastProcessFunction</span><span class="o">&lt;</span><span class="n">IN1</span><span class="o">,</span> <span class="n">IN2</span><span class="o">,</span> <span class="n">OUT</span><span class="o">&gt;</span> <span class="kd">extends</span> <span class="n">BaseBroadcastProcessFunction</span> <span class="o">{</span>

    <span class="kd">public</span> <span class="kd">abstract</span> <span class="kt">void</span> <span class="nf">processElement</span><span class="o">(</span><span class="n">IN1</span> <span class="n">value</span><span class="o">,</span> <span class="n">ReadOnlyContext</span> <span class="n">ctx</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>

    <span class="kd">public</span> <span class="kd">abstract</span> <span class="kt">void</span> <span class="nf">processBroadcastElement</span><span class="o">(</span><span class="n">IN2</span> <span class="n">value</span><span class="o">,</span> <span class="n">Context</span> <span class="n">ctx</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>
<span class="o">}</span>
<span class="kd">public</span> <span class="kd">abstract</span> <span class="kd">class</span> <span class="nc">KeyedBroadcastProcessFunction</span><span class="o">&lt;</span><span class="n">KS</span><span class="o">,</span> <span class="n">IN1</span><span class="o">,</span> <span class="n">IN2</span><span class="o">,</span> <span class="n">OUT</span><span class="o">&gt;</span> <span class="o">{</span>

    <span class="kd">public</span> <span class="kd">abstract</span> <span class="kt">void</span> <span class="nf">processElement</span><span class="o">(</span><span class="n">IN1</span> <span class="n">value</span><span class="o">,</span> <span class="n">ReadOnlyContext</span> <span class="n">ctx</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>

    <span class="kd">public</span> <span class="kd">abstract</span> <span class="kt">void</span> <span class="nf">processBroadcastElement</span><span class="o">(</span><span class="n">IN2</span> <span class="n">value</span><span class="o">,</span> <span class="n">Context</span> <span class="n">ctx</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onTimer</span><span class="o">(</span><span class="kt">long</span> <span class="n">timestamp</span><span class="o">,</span> <span class="n">OnTimerContext</span> <span class="n">ctx</span><span class="o">,</span> <span class="n">Collector</span><span class="o">&lt;</span><span class="n">OUT</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>
<span class="o">}</span>
</code></pre></div><p>首先需要注意的是，这两个函数在处理广播端元素时都需要实现 <code>processBroadcastElement()</code> 方法，在处理非广播端元素时需要实现 <code>processElement()</code> 方法。</p>
<p>这两个方法在提供的上下文中有所不同。非广播侧有一个 <code>ReadOnlyContext</code>，而广播侧有一个 <code>Context</code>。</p>
<p>这两个上下文（以下枚举中的 <code>ctx</code>）:</p>
<ol>
<li>提供对广播状态的访问：<code>ctx.getBroadcastState(MapStateDescriptor&lt;K, V&gt; stateDescriptor)</code>。</li>
<li>允许查询元素的时间戳：<code>ctx.timestamp()</code>。</li>
<li>获取当前水印：<code>ctx.currentWatermark()</code>。</li>
<li>获取当前处理时间：<code>ctx.currentProcessingTime()</code>，以及</li>
<li>将元素发射到侧输出：<code>ctx.output(OutputTag&lt;X&gt; outputTag, X value)</code>。</li>
</ol>
<p><code>getBroadcastState()</code> 中的 <code>stateDescriptor</code> 应该和上面的 <code>.broadcast(ruleStateDescriptor)</code> 中的 <code>stateDescriptor</code> 是一样的。</p>
<p>区别在于各自对广播状态的访问类型。广播端对其有读写访问权，而非广播端则只有读的访问权（因此才有这些名字）。原因是在 Flink 中，不存在跨任务通信。所以，为了保证广播状态中的内容在我们操作符的所有并行实例中都是相同的，我们只给广播侧读写访问权，而广播侧在所有任务中看到的元素都是相同的，并且我们要求该侧每个传入元素的计算在所有任务中都是相同的。忽略这个规则会打破状态的一致性保证，导致结果不一致，而且往往难以调试。</p>
<p>注意 <code>processBroadcastElement()</code> 中实现的逻辑必须在所有并行实例中具有相同的确定性行为!</p>
<p>最后，由于 <code>KeyedBroadcastProcessFunction</code> 是在 keyed stream 上运行的，它暴露了一些 <code>BroadcastProcessFunction</code> 无法实现的功能。那就是</p>
<ol>
<li><code>processElement()</code> 方法中的 <code>ReadOnlyContext</code> 允许访问 Flink 的底层定时器服务，它允许注册事件和/或处理时间定时器。当一个定时器发射时， <code>onTimer()</code> (如上所示)被调用一个 <code>OnTimerContext</code>，它暴露了与 <code>ReadOnlyContext</code> 相同的功能，再加上</li>
</ol>
<ul>
<li>能够询问发射的定时器是事件还是处理时间, 和</li>
<li>来查询与定时器相关联的键。</li>
</ul>
<ol start="2">
<li><code>processBroadcastElement()</code> 方法中的 <code>Context</code> 包含 <code>applyToKeyedState(StateDescriptor&lt;S, VS&gt; stateDescriptor, KeyedStateFunction&lt;KS, S&gt; function)</code> 方法。这允许注册一个 <code>KeyedStateFunction</code>，以应用于与提供的 <code>stateDescriptor</code> 相关联的所有键的所有状态。</li>
</ol>
<p>注意。注册定时器只能在 <code>KeyedBroadcastProcessFunction</code> 的 <code>processElement()</code> 处进行，而且只能在那里进行。在 <code>processBroadcastElement()</code> 方法中是不可能的，因为没有键与广播元素相关联。</p>
<p>回到我们原来的例子，我们的 <code>KeyedBroadcastProcessFunction</code> 可以是如下的样子。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="k">new</span> <span class="n">KeyedBroadcastProcessFunction</span><span class="o">&lt;</span><span class="n">Color</span><span class="o">,</span> <span class="n">Item</span><span class="o">,</span> <span class="n">Rule</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;()</span> <span class="o">{</span>

    <span class="c1">// store partial matches, i.e. first elements of the pair waiting for their second element
</span><span class="c1"></span>    <span class="c1">// we keep a list as we may have many first elements waiting
</span><span class="c1"></span>    <span class="kd">private</span> <span class="kd">final</span> <span class="n">MapStateDescriptor</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Item</span><span class="o">&gt;&gt;</span> <span class="n">mapStateDesc</span> <span class="o">=</span>
        <span class="k">new</span> <span class="n">MapStateDescriptor</span><span class="o">&lt;&gt;(</span>
            <span class="s">&#34;items&#34;</span><span class="o">,</span>
            <span class="n">BasicTypeInfo</span><span class="o">.</span><span class="na">STRING_TYPE_INFO</span><span class="o">,</span>
            <span class="k">new</span> <span class="n">ListTypeInfo</span><span class="o">&lt;&gt;(</span><span class="n">Item</span><span class="o">.</span><span class="na">class</span><span class="o">));</span>

    <span class="c1">// identical to our ruleStateDescriptor above
</span><span class="c1"></span>    <span class="kd">private</span> <span class="kd">final</span> <span class="n">MapStateDescriptor</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Rule</span><span class="o">&gt;</span> <span class="n">ruleStateDescriptor</span> <span class="o">=</span> 
        <span class="k">new</span> <span class="n">MapStateDescriptor</span><span class="o">&lt;&gt;(</span>
            <span class="s">&#34;RulesBroadcastState&#34;</span><span class="o">,</span>
            <span class="n">BasicTypeInfo</span><span class="o">.</span><span class="na">STRING_TYPE_INFO</span><span class="o">,</span>
            <span class="n">TypeInformation</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="k">new</span> <span class="n">TypeHint</span><span class="o">&lt;</span><span class="n">Rule</span><span class="o">&gt;()</span> <span class="o">{}));</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">processBroadcastElement</span><span class="o">(</span><span class="n">Rule</span> <span class="n">value</span><span class="o">,</span>
                                        <span class="n">Context</span> <span class="n">ctx</span><span class="o">,</span>
                                        <span class="n">Collector</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
        <span class="n">ctx</span><span class="o">.</span><span class="na">getBroadcastState</span><span class="o">(</span><span class="n">ruleStateDescriptor</span><span class="o">).</span><span class="na">put</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="na">name</span><span class="o">,</span> <span class="n">value</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">processElement</span><span class="o">(</span><span class="n">Item</span> <span class="n">value</span><span class="o">,</span>
                               <span class="n">ReadOnlyContext</span> <span class="n">ctx</span><span class="o">,</span>
                               <span class="n">Collector</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>

        <span class="kd">final</span> <span class="n">MapState</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Item</span><span class="o">&gt;&gt;</span> <span class="n">state</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getMapState</span><span class="o">(</span><span class="n">mapStateDesc</span><span class="o">);</span>
        <span class="kd">final</span> <span class="n">Shape</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="na">getShape</span><span class="o">();</span>
    
        <span class="k">for</span> <span class="o">(</span><span class="n">Map</span><span class="o">.</span><span class="na">Entry</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Rule</span><span class="o">&gt;</span> <span class="n">entry</span> <span class="o">:</span>
                <span class="n">ctx</span><span class="o">.</span><span class="na">getBroadcastState</span><span class="o">(</span><span class="n">ruleStateDescriptor</span><span class="o">).</span><span class="na">immutableEntries</span><span class="o">())</span> <span class="o">{</span>
            <span class="kd">final</span> <span class="n">String</span> <span class="n">ruleName</span> <span class="o">=</span> <span class="n">entry</span><span class="o">.</span><span class="na">getKey</span><span class="o">();</span>
            <span class="kd">final</span> <span class="n">Rule</span> <span class="n">rule</span> <span class="o">=</span> <span class="n">entry</span><span class="o">.</span><span class="na">getValue</span><span class="o">();</span>
    
            <span class="n">List</span><span class="o">&lt;</span><span class="n">Item</span><span class="o">&gt;</span> <span class="n">stored</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">ruleName</span><span class="o">);</span>
            <span class="k">if</span> <span class="o">(</span><span class="n">stored</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">stored</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;();</span>
            <span class="o">}</span>
    
            <span class="k">if</span> <span class="o">(</span><span class="n">shape</span> <span class="o">==</span> <span class="n">rule</span><span class="o">.</span><span class="na">second</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">stored</span><span class="o">.</span><span class="na">isEmpty</span><span class="o">())</span> <span class="o">{</span>
                <span class="k">for</span> <span class="o">(</span><span class="n">Item</span> <span class="n">i</span> <span class="o">:</span> <span class="n">stored</span><span class="o">)</span> <span class="o">{</span>
                    <span class="n">out</span><span class="o">.</span><span class="na">collect</span><span class="o">(</span><span class="s">&#34;MATCH: &#34;</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="s">&#34; - &#34;</span> <span class="o">+</span> <span class="n">value</span><span class="o">);</span>
                <span class="o">}</span>
                <span class="n">stored</span><span class="o">.</span><span class="na">clear</span><span class="o">();</span>
            <span class="o">}</span>
    
            <span class="c1">// there is no else{} to cover if rule.first == rule.second
</span><span class="c1"></span>            <span class="k">if</span> <span class="o">(</span><span class="n">shape</span><span class="o">.</span><span class="na">equals</span><span class="o">(</span><span class="n">rule</span><span class="o">.</span><span class="na">first</span><span class="o">))</span> <span class="o">{</span>
                <span class="n">stored</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">value</span><span class="o">);</span>
            <span class="o">}</span>
    
            <span class="k">if</span> <span class="o">(</span><span class="n">stored</span><span class="o">.</span><span class="na">isEmpty</span><span class="o">())</span> <span class="o">{</span>
                <span class="n">state</span><span class="o">.</span><span class="na">remove</span><span class="o">(</span><span class="n">ruleName</span><span class="o">);</span>
            <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
                <span class="n">state</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="n">ruleName</span><span class="o">,</span> <span class="n">stored</span><span class="o">);</span>
            <span class="o">}</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="重要的考虑因素">重要的考虑因素</h3>
<p>在介绍完提供的 API 之后，本节重点介绍使用广播状态时需要注意的重要事项。这些事项是</p>
<ul>
<li>
<p>没有跨任务通信。如前所述，这就是为什么只有 (Keyed)-BroadcastProcessFunction 的广播端可以修改广播状态的内容的原因。此外，用户必须确保所有的任务对每一个传入元素都以同样的方式修改广播状态的内容。否则，不同的任务可能有不同的内容，导致结果不一致。</p>
</li>
<li>
<p>不同任务的广播状态中事件的顺序可能不同。虽然广播流的元素保证了所有元素将（最终）进入所有下游任务，但元素可能会以不同的顺序到达每个任务。因此，每个传入元素的状态更新必须不依赖于传入事件的顺序。</p>
</li>
<li>
<p>所有的任务都会对其广播状态进行 checkpoint。虽然当 checkpoint 发生时，所有任务的广播状态中都有相同的元素（checkpoint 屏障不会超过元素），但所有任务都会 checkpoint 他们的广播状态，而不仅仅是其中一个。这是一个设计决定，以避免在还原过程中让所有任务从同一个文件中读取（从而避免热点），尽管它的代价是将检查点状态的大小增加了p的系数（=并行性）。Flink 保证在恢复/缩放时，不会有重复和丢失的数据。在以相同或更小的并行度进行恢复时，每个任务读取其检查点状态。扩容后，每个任务读取自己的状态，其余任务（p_new-p_old）以循环的方式读取之前任务的检查点。</p>
</li>
<li>
<p>没有 RocksDB 状态后端。广播状态在运行时保存在内存中，内存供应也应相应进行。这对所有的操作符状态都适用。</p>
</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[批处理例子]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Batch Examples</blockquote><h2 id="batch-示例">Batch 示例</h2>
<p>下面的示例程序展示了 Flink 的不同应用，从简单的单词计数到图形算法。这些代码样本说明了 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">Flink 的 DataSet API</a> 的使用。</p>
<p>以下和更多例子的完整源代码可以在 Flink 源码库的 <a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-batch">flink-examples-batch</a> 模块中找到。</p>
<h3 id="运行一个例子">运行一个例子</h3>
<p>为了运行一个 Flink 实例，我们假设你有一个正在运行的 Flink 实例。导航中的 &ldquo;Quickstart&rdquo; 和 &ldquo;Setup&rdquo; 选项卡描述了启动 Flink 的各种方法。</p>
<p>最简单的方法是运行 <code>./bin/start-cluster.sh</code>，默认情况下，它用一个 JobManager 和一个 TaskManager 启动一个本地集群。</p>
<p>Flink 的每个二进制版本都包含一个例子目录，其中有本页每个例子的 jar 文件。</p>
<p>要运行 WordCount 示例，请发出以下命令。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">./bin/flink run ./examples/batch/WordCount.jar
</code></pre></div><p>其他的例子也可以用类似的方式启动。</p>
<p>请注意，许多例子在运行时没有传递任何参数，而是使用内置的数据。要使用真实数据运行 WordCount，你必须传递数据的路径。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">./bin/flink run ./examples/batch/WordCount.jar --input /path/to/some/text/data --output /path/to/result
</code></pre></div><p>请注意，非本地文件系统需要一个模式前缀，如 <code>hdfs://</code>。</p>
<h3 id="wordcount">WordCount</h3>
<p>WordCount 是大数据处理系统中的 &ldquo;Hello World&rdquo;。它计算文本集合中的单词频率。该算法分两步工作。首先，文本被分割成单个单词。第二，对单词进行分组和计数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// get input data
</span><span class="c1"></span><span class="k">val</span> <span class="n">text</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readTextFile</span><span class="o">(</span><span class="s">&#34;/path/to/file&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">text</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">toLowerCase</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34;\\W+&#34;</span><span class="o">)</span> <span class="n">filter</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">nonEmpty</span> <span class="o">}</span> <span class="o">}</span>
  <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="o">}</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

<span class="n">counts</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">outputPath</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34; &#34;</span><span class="o">)</span>
</code></pre></div><p><a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/wordcount/WordCount.scala">WordCount 的例子</a>实现了上面描述的算法，输入参数：<code>--input &lt;path&gt; --output &lt;path&gt;</code>。作为测试数据，任何文本文件都可以。</p>
<h3 id="页面排名">页面排名</h3>
<p>PageRank 算法计算由链接定义的图中页面的&quot;重要性&quot;，这些链接从一个页面指向另一个页面。它是一种迭代图算法，这意味着它反复应用相同的计算。在每一次迭代中，每个页面将其当前的排名分布在所有的邻居上，并计算其新的排名，作为它从邻居那里得到的排名的累加和。PageRank 算法是由 Google 搜索引擎推广的，它利用网页的重要性来对搜索查询的结果进行排名。</p>
<p>在这个简单的例子中，PageRank 的实现方式是<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">批量迭代</a>和固定的迭代次数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// User-defined types
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">Link</span><span class="o">(</span><span class="n">sourceId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">targetId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span>
<span class="k">case</span> <span class="k">class</span> <span class="nc">Page</span><span class="o">(</span><span class="n">pageId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">rank</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>
<span class="k">case</span> <span class="k">class</span> <span class="nc">AdjacencyList</span><span class="o">(</span><span class="n">sourceId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">targetIds</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Long</span><span class="o">])</span>

<span class="c1">// set up execution environment
</span><span class="c1"></span><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// read the pages and initial ranks by parsing a CSV file
</span><span class="c1"></span><span class="k">val</span> <span class="n">pages</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[</span><span class="kt">Page</span><span class="o">](</span><span class="n">pagesInputPath</span><span class="o">)</span>

<span class="c1">// the links are encoded as an adjacency list: (page-id, Array(neighbor-ids))
</span><span class="c1"></span><span class="k">val</span> <span class="n">links</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">readCsvFile</span><span class="o">[</span><span class="kt">Link</span><span class="o">](</span><span class="n">linksInputPath</span><span class="o">)</span>

<span class="c1">// assign initial ranks to pages
</span><span class="c1"></span><span class="k">val</span> <span class="n">pagesWithRanks</span> <span class="k">=</span> <span class="n">pages</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="nc">Page</span><span class="o">(</span><span class="n">p</span><span class="o">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">numPages</span><span class="o">))</span>

<span class="c1">// build adjacency list from link input
</span><span class="c1"></span><span class="k">val</span> <span class="n">adjacencyLists</span> <span class="k">=</span> <span class="n">links</span>
  <span class="c1">// initialize lists
</span><span class="c1"></span>  <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">e</span> <span class="k">=&gt;</span> <span class="nc">AdjacencyList</span><span class="o">(</span><span class="n">e</span><span class="o">.</span><span class="n">sourceId</span><span class="o">,</span> <span class="nc">Array</span><span class="o">(</span><span class="n">e</span><span class="o">.</span><span class="n">targetId</span><span class="o">)))</span>
  <span class="c1">// concatenate lists
</span><span class="c1"></span>  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;sourceId&#34;</span><span class="o">).</span><span class="n">reduce</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">l1</span><span class="o">,</span> <span class="n">l2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="nc">AdjacencyList</span><span class="o">(</span><span class="n">l1</span><span class="o">.</span><span class="n">sourceId</span><span class="o">,</span> <span class="n">l1</span><span class="o">.</span><span class="n">targetIds</span> <span class="o">++</span> <span class="n">l2</span><span class="o">.</span><span class="n">targetIds</span><span class="o">)</span>
  <span class="o">}</span>

<span class="c1">// start iteration
</span><span class="c1"></span><span class="k">val</span> <span class="n">finalRanks</span> <span class="k">=</span> <span class="n">pagesWithRanks</span><span class="o">.</span><span class="n">iterateWithTermination</span><span class="o">(</span><span class="n">maxIterations</span><span class="o">)</span> <span class="o">{</span>
  <span class="n">currentRanks</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">newRanks</span> <span class="k">=</span> <span class="n">currentRanks</span>
      <span class="c1">// distribute ranks to target pages
</span><span class="c1"></span>      <span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">adjacencyLists</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;pageId&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;sourceId&#34;</span><span class="o">)</span> <span class="o">{</span>
        <span class="o">(</span><span class="n">page</span><span class="o">,</span> <span class="n">adjacent</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Page</span><span class="o">])</span> <span class="k">=&gt;</span>
        <span class="k">for</span> <span class="o">(</span><span class="n">targetId</span> <span class="k">&lt;-</span> <span class="n">adjacent</span><span class="o">.</span><span class="n">targetIds</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">Page</span><span class="o">(</span><span class="n">targetId</span><span class="o">,</span> <span class="n">page</span><span class="o">.</span><span class="n">rank</span> <span class="o">/</span> <span class="n">adjacent</span><span class="o">.</span><span class="n">targetIds</span><span class="o">.</span><span class="n">length</span><span class="o">))</span>
        <span class="o">}</span>
      <span class="o">}</span>
      <span class="c1">// collect ranks and sum them up
</span><span class="c1"></span>      <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&#34;pageId&#34;</span><span class="o">).</span><span class="n">aggregate</span><span class="o">(</span><span class="nc">SUM</span><span class="o">,</span> <span class="s">&#34;rank&#34;</span><span class="o">)</span>
      <span class="c1">// apply dampening factor
</span><span class="c1"></span>      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">p</span> <span class="k">=&gt;</span>
        <span class="nc">Page</span><span class="o">(</span><span class="n">p</span><span class="o">.</span><span class="n">pageId</span><span class="o">,</span> <span class="o">(</span><span class="n">p</span><span class="o">.</span><span class="n">rank</span> <span class="o">*</span> <span class="nc">DAMPENING_FACTOR</span><span class="o">)</span> <span class="o">+</span> <span class="o">((</span><span class="mi">1</span> <span class="o">-</span> <span class="nc">DAMPENING_FACTOR</span><span class="o">)</span> <span class="o">/</span> <span class="n">numPages</span><span class="o">))</span>
      <span class="o">}</span>

    <span class="c1">// terminate if no rank update was significant
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">termination</span> <span class="k">=</span> <span class="n">currentRanks</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">newRanks</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&#34;pageId&#34;</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="s">&#34;pageId&#34;</span><span class="o">)</span> <span class="o">{</span>
      <span class="o">(</span><span class="n">current</span><span class="o">,</span> <span class="n">next</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span> <span class="k">=&gt;</span>
        <span class="c1">// check for significant update
</span><span class="c1"></span>        <span class="k">if</span> <span class="o">(</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="o">(</span><span class="n">current</span><span class="o">.</span><span class="n">rank</span> <span class="o">-</span> <span class="n">next</span><span class="o">.</span><span class="n">rank</span><span class="o">)</span> <span class="o">&gt;</span> <span class="nc">EPSILON</span><span class="o">)</span> <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="o">(</span><span class="n">newRanks</span><span class="o">,</span> <span class="n">termination</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">finalRanks</span>

<span class="c1">// emit result
</span><span class="c1"></span><span class="n">result</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">outputPath</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34; &#34;</span><span class="o">)</span>
</code></pre></div><p><a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/graph/PageRankBasic.scala">PageRank 程序</a>实现了上述示例。它需要以下参数才能运行。<code>--pages &lt;path&gt; --links &lt;path&gt; --output &lt;path&gt; --numPages &lt;n&gt; --iterations &lt;n&gt;</code>。</p>
<p>输入文件是纯文本文件，必须按以下格式进行。</p>
<ul>
<li>页数用一个（长）ID 表示，用换行字符分隔。
<ul>
<li>例如 &ldquo;1/n2/n12/n42/n63/n&rdquo; 给出了 5 个 ID 为 1、2、12、42 和 63 的页面。</li>
</ul>
</li>
<li>链接用页面 ID 对表示，用空格分隔。链接用换行符分隔。
<ul>
<li>例如 &ldquo;1 2\n2 12\n1 12\n42 63\n&rdquo; 给出了四个(定向)链接(1)-&gt;(2)，(2)-&gt;(12)，(1)-&gt;(12)和(42)-&gt;(63)。</li>
</ul>
</li>
</ul>
<p>对于这个简单的实现，要求每个页面至少有一个入站链接和一个出站链接（一个页面可以指向自己）。</p>
<h3 id="连接的组件">连接的组件</h3>
<p>Connected Components 算法通过给同一连接部分中的所有顶点分配相同的组件 ID，来识别较大图中相互连接的部分。与 PageRank 类似，Connected Components 是一种迭代算法。在每一步中，每个顶点将其当前的组件 ID 传播给所有的邻居。如果一个顶点接受来自邻居的组件 ID，如果它小于自己的组件 ID。</p>
<p>本实现使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">增量迭代</a>。没有改变组件 ID 的顶点不参与下一步。这产生了更好的性能，因为后面的迭代通常只处理一些离群的顶点。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// set up execution environment
</span><span class="c1"></span><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="c1">// read vertex and edge data
</span><span class="c1">// assign the initial components (equal to the vertex id)
</span><span class="c1"></span><span class="k">val</span> <span class="n">vertices</span> <span class="k">=</span> <span class="n">getVerticesDataSet</span><span class="o">(</span><span class="n">env</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="n">id</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">id</span><span class="o">)</span> <span class="o">}</span>

<span class="c1">// undirected edges by emitting for each input edge the input edges itself and an inverted
</span><span class="c1">// version
</span><span class="c1"></span><span class="k">val</span> <span class="n">edges</span> <span class="k">=</span> <span class="n">getEdgesDataSet</span><span class="o">(</span><span class="n">env</span><span class="o">).</span><span class="n">flatMap</span> <span class="o">{</span> <span class="n">edge</span> <span class="k">=&gt;</span> <span class="nc">Seq</span><span class="o">(</span><span class="n">edge</span><span class="o">,</span> <span class="o">(</span><span class="n">edge</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">edge</span><span class="o">.</span><span class="n">_1</span><span class="o">))</span> <span class="o">}</span>

<span class="c1">// open a delta iteration
</span><span class="c1"></span><span class="k">val</span> <span class="n">verticesWithComponents</span> <span class="k">=</span> <span class="n">vertices</span><span class="o">.</span><span class="n">iterateDelta</span><span class="o">(</span><span class="n">vertices</span><span class="o">,</span> <span class="n">maxIterations</span><span class="o">,</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span> <span class="o">{</span>
  <span class="o">(</span><span class="n">s</span><span class="o">,</span> <span class="n">ws</span><span class="o">)</span> <span class="k">=&gt;</span>

    <span class="c1">// apply the step logic: join with the edges
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">allNeighbors</span> <span class="k">=</span> <span class="n">ws</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">edges</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">{</span> <span class="o">(</span><span class="n">vertex</span><span class="o">,</span> <span class="n">edge</span><span class="o">)</span> <span class="k">=&gt;</span>
      <span class="o">(</span><span class="n">edge</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">vertex</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="c1">// select the minimum neighbor
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">minNeighbors</span> <span class="k">=</span> <span class="n">allNeighbors</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">min</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

    <span class="c1">// update if the component of the candidate is smaller
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">updatedComponents</span> <span class="k">=</span> <span class="n">minNeighbors</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">s</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">equalTo</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
      <span class="o">(</span><span class="n">newVertex</span><span class="o">,</span> <span class="n">oldVertex</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)])</span> <span class="k">=&gt;</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">newVertex</span><span class="o">.</span><span class="n">_2</span> <span class="o">&lt;</span> <span class="n">oldVertex</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">newVertex</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="c1">// delta and new workset are identical
</span><span class="c1"></span>    <span class="o">(</span><span class="n">updatedComponents</span><span class="o">,</span> <span class="n">updatedComponents</span><span class="o">)</span>
<span class="o">}</span>

<span class="n">verticesWithComponents</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">outputPath</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34; &#34;</span><span class="o">)</span>
</code></pre></div><p><a href="https://github.com/apache/flink/blob/master//flink-examples/flink-examples-batch/src/main/scala/org/apache/flink/examples/scala/graph/ConnectedComponents.scala">ConnectedComponents 程序</a>实现了上面的例子。它需要以下参数才能运行: <code>--vertices &lt;path&gt; --edges &lt;path&gt; --output &lt;path&gt; --iterations &lt;n&gt;</code>。</p>
<p>输入文件是纯文本文件，必须按如下格式编写。</p>
<ul>
<li>顶点用 ID 表示，并用换行符隔开。
<ul>
<li>例如 &ldquo;1/n2/n12/n42/n63/n&rdquo; 给出了五个顶点，分别是(1)、(2)、(12)、(42)和(63)。</li>
</ul>
</li>
<li>边缘用一对顶点 ID 表示，这些顶点 ID 用空格字符分隔。边缘用换行符隔开。
<ul>
<li>例如，&ldquo;1 2/n2 12/n1 12/n42 63/n&rdquo; 给出了四个(非直接)联系(1)-(2)、(2)-(12)、(1)-(12)和(42)-(63)。</li>
</ul>
</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/examples.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/examples.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[数据源]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-data-sources/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-data-sources/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Data Sources</blockquote><p>注：这描述了新的数据源 API ，作为 FLIP-27 的一部分在 Flink 1.11 中引入。这个新的 API 目前处于 BETA 状态。
大多数现有的源连接器还没有（截至 Flink 1.11 ）使用这个新的 API 实现，而是使用以前的 API ，基于 SourceFunction 。
本页介绍了 Flink 的数据源 API 及其背后的概念和架构。如果你对 Flink 中的数据源是如何工作的，或者你想实现一个新的数据源，请阅读本页面。</p>
<p>如果您正在寻找预定义的源连接器，请查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/">连接器文档</a>。</p>
<h2 id="数据源概念">数据源概念</h2>
<p><strong>核心部件</strong></p>
<p>一个数据源有三个核心组件。<code>Split</code>、<code>SplitEnumerator</code> 和 <code>SourceReader</code>。</p>
<ul>
<li>
<p><code>Split</code> 是数据源所消耗的一部分数据，就像一个文件或一个日志分区。<code>Split</code> 是源分配工作和并行读取数据的粒度。</p>
</li>
<li>
<p><code>SourceReader</code> 请求 <code>Split</code> 并进行处理，例如读取 <code>Split</code> 所代表的文件或日志分区。<code>SourceReader</code> 在 <code>SourceOperators</code> 的 Task Manager 上并行运行，并产生事件/记录的并行流。</p>
</li>
<li>
<p><code>SplitEnumerator</code> 生成 <code>Split</code> 并将它们分配给 <code>SourceReader</code> 。它作为单个实例在任务管理器上运行，负责维护待处理的 <code>Split</code> 的积压，并以平衡的方式将它们分配给读者。</p>
</li>
</ul>
<p><code>Source</code> 类是将上述三个组件联系在一起的 API 入口点。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/source_components.svg" alt="img"></p>
<p><strong>统一的跨流和批处理</strong></p>
<p>数据源 API 以统一的方式支持无界流源和有界批处理源。</p>
<p>这两种情况的区别很小：在有界/批处理的情况下，枚举器生成一组固定的 split ，而且每个 split 必然是有限的。在无界流的情况下，这两种情况中的一种是不正确的（ split 不是有限的，或者枚举器不断产生新的 split ）。</p>
<h3 id="例子">例子</h3>
<p>下面是一些简化的概念性例子，以说明在流式和批处理情况下，数据源组件如何交互。</p>
<p>请注意，这并不能准确地描述 Kafka 和 File 源的实现是如何工作的；部分内容是简化的，用于说明目的。</p>
<p><strong>绑定的文件源</strong></p>
<p>源有一个要读取的目录的 URI/路径，以及一个定义如何解析文件的格式。</p>
<ul>
<li><code>Split</code> 是一个文件，或者一个文件的一个区域（如果数据格式支持分割文件）。</li>
<li><code>SplitEnumerator</code> 列出了给定目录路径下的所有文件。它将 <code>Split</code> 分配给下一个请求 <code>Split</code> 的读者。一旦所有的 <code>Split</code> 都分配完毕，它就会用 <code>NoMoreSplits</code> 来响应请求。</li>
<li>SourceReader 请求一个 <code>Split</code> ，并读取被分配的 <code>Split</code> （文件或文件区域），并使用给定的格式进行解析。如果它没有得到另一个 <code>Split</code> ，而是得到一个 <code>NoMoreSplits</code> 消息，它就结束了。</li>
</ul>
<p><strong>非绑定流文件源</strong></p>
<p>这个源的工作方式和上面描述的一样，除了 <code>SplitEnumerator</code> 从不响应 <code>NoMoreSplits</code> ，而是周期性地列出给定 <code>URI/Path</code> 下的内容以检查新文件。一旦发现新文件，它就会为它们生成新的 <code>Splits</code> ，并可以将它们分配给可用的 <code>SourceReaders</code>。</p>
<p><strong>无界流 Kafka 源</strong></p>
<p>该源有一个 Kafka Topic （或 Topic 列表或 Topic regex ）和一个 Deserializer 来解析记录。</p>
<ul>
<li>一个 Split 就是一个 Kafka Topic 分区。</li>
<li>SplitEnumerator 连接到 brokers ，以列出所有涉及订阅的主题分区。枚举器可以选择重复这个操作来发现新添加的主题/分区。</li>
<li>SourceReader 使用 KafkaConsumer 读取分配的 split （主题分区），并使用提供的 Deserializer 反序列化记录。分割(Topic Partitions) 没有终点，所以读取器永远不会到达数据的终点。</li>
</ul>
<p><strong>绑定的 Kafka 源</strong></p>
<p>和上面一样，只是每个 Split （主题分区）有一个定义的结束偏移量。一旦 SourceReader 达到一个 Split 的结束偏移量，它就会完成该 Split 。一旦所有分配的 Split 结束， SourceReader 就结束了。</p>
<h2 id="数据源-api">数据源 API</h2>
<p>本节介绍了 FLIP-27 中新引入的 Source API 的主要接口，并为开发者提供了 Source 开发的技巧。</p>
<h3 id="source">Source</h3>
<p><a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/Source.java">Source</a> API 是一个工厂风格的接口，用于创建以下组件。</p>
<ul>
<li>Split Enumerator</li>
<li>源读取器</li>
<li>分离式序列器</li>
<li>枚举器检查点序列器</li>
</ul>
<p>除此之外， Source 还提供了源的<a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/Boundedness.java">边界</a>属性，这样 Flink 可以选择合适的模式来运行 Flink 作业。</p>
<p>Source 的实现应该是可序列化的，因为 Source 实例在运行时被序列化并上传到 Flink 集群。</p>
<h3 id="splitenumerator">SplitEnumerator</h3>
<p><a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/SplitEnumerator.java">SplitEnumerator</a> 有望成为 Source 的&quot;大脑&quot;。SplitEnumerator 的典型实现会做以下工作。</p>
<ul>
<li>SourceReader 注册处理</li>
<li>SourceReader 失败处理
<ul>
<li>当 SourceReader 失败时，将调用 <code>addSplitsBack()</code> 方法。SplitEnumerator 应该收回未被失败的 SourceReader 承认的分割分配。</li>
</ul>
</li>
<li>SourceEvent 处理
<ul>
<li>SourceEvents 是在 SplitEnumerator 和 SourceReader 之间发送的自定义事件。实现可以利用这种机制来进行复杂的协调。</li>
</ul>
</li>
<li>分割发现和分配
<ul>
<li>SplitEnumerator 可以根据各种事件将 split 分配给 SourceReaders ，包括发现新的 split 、新的 SourceReader 注册、 SourceReader 失败等。</li>
</ul>
</li>
</ul>
<p>SplitEnumerator 可以借助 <a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/SplitEnumeratorContext.java">SplitEnumeratorContext</a> 完成上述工作， SplitEnumeratorContext 是在创建或恢复 SplitEnumerator 时提供给 Source 的。SplitEnumeratorContext 允许 SplitEnumerator 检索读取器的必要信息并执行协调动作。Source 实现应该将 SplitEnumeratorContext 传递给 SplitEnumerator 实例。</p>
<p>虽然 SplitEnumerator 实现可以通过只在它的方法被调用时采取协调动作的被动方式很好地工作，但一些 SplitEnumerator 实现可能希望主动采取行动。例如，一个 SplitEnumerator 可能希望定期运行 split discovery ，并将新的 split 分配给 SourceReaders 。这样的实现可能会发现调用 Async() 方法 SplitEnumeratorContext 很方便。下面的代码片段展示了 SplitEnumerator 实现如何在不维护自己的线程的情况下实现这一点。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">MySplitEnumerator</span> <span class="n">implements</span> <span class="nc">SplitEnumerator</span><span class="o">&lt;</span><span class="nc">MySplit</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="k">private</span> <span class="k">final</span> <span class="n">long</span> <span class="nc">DISCOVER_INTERVAL</span> <span class="k">=</span> <span class="mi">60</span><span class="n">_000L</span><span class="o">;</span>

    <span class="cm">/**
</span><span class="cm">     * A method to discover the splits.
</span><span class="cm">     */</span>
    <span class="k">private</span> <span class="nc">List</span><span class="o">&lt;</span><span class="nc">MySplit</span><span class="o">&gt;</span> <span class="n">discoverSplits</span><span class="o">()</span> <span class="o">{...}</span>
    
    <span class="nd">@Override</span>
    <span class="n">public</span> <span class="n">void</span> <span class="n">start</span><span class="o">()</span> <span class="o">{</span>
        <span class="o">...</span>
        <span class="n">enumContext</span><span class="o">.</span><span class="n">callAsync</span><span class="o">(</span><span class="k">this:</span><span class="kt">:discoverSplits</span><span class="o">,</span> <span class="n">splits</span> <span class="o">-&gt;</span> <span class="o">{</span>
            <span class="nc">Map</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">,</span> <span class="nc">List</span><span class="o">&lt;</span><span class="nc">MockSourceSplit</span><span class="o">&gt;&gt;</span> <span class="n">assignments</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">HashMap</span><span class="o">&lt;&gt;();</span>
            <span class="n">int</span> <span class="n">parallelism</span> <span class="k">=</span> <span class="n">enumContext</span><span class="o">.</span><span class="n">currentParallelism</span><span class="o">();</span>
            <span class="k">for</span> <span class="o">(</span><span class="nc">MockSourceSplit</span> <span class="n">split</span> <span class="k">:</span> <span class="kt">splits</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">int</span> <span class="n">owner</span> <span class="k">=</span> <span class="n">split</span><span class="o">.</span><span class="n">splitId</span><span class="o">().</span><span class="n">hashCode</span><span class="o">()</span> <span class="o">%</span> <span class="n">parallelism</span><span class="o">;</span>
                <span class="n">assignments</span><span class="o">.</span><span class="n">computeIfAbsent</span><span class="o">(</span><span class="n">owner</span><span class="o">,</span> <span class="k">new</span> <span class="nc">ArrayList</span><span class="o">&lt;&gt;()).</span><span class="n">add</span><span class="o">(</span><span class="n">split</span><span class="o">);</span>
            <span class="o">}</span>
            <span class="n">enumContext</span><span class="o">.</span><span class="n">assignSplits</span><span class="o">(</span><span class="k">new</span> <span class="nc">SplitsAssignment</span><span class="o">&lt;&gt;(</span><span class="n">assignments</span><span class="o">));</span>
        <span class="o">},</span> <span class="mi">0L</span><span class="o">,</span> <span class="nc">DISCOVER_INTERVAL</span><span class="o">);</span>
        <span class="o">...</span>
    <span class="o">}</span>
    <span class="o">...</span>
<span class="o">}</span>
</code></pre></div><h2 id="sourcereader">SourceReader</h2>
<p>SourceReader 是一个运行在 Task Manager 中的组件，用于消耗来自 Splits 的记录。</p>
<p>SourceReader 暴露了一个基于拉的消费接口。一个 Flink 任务在循环中不断调用 pollNext(ReaderOutput) 来轮询 SourceReader 的记录。pollNext(ReaderOutput) 方法的返回值表示源阅读器的状态。</p>
<ul>
<li>MORE_AVAILABLE - SourceReader 立即有更多的记录可用。</li>
<li>NOTHING_AVAILABLE - SourceReader 此时没有更多的记录可用，但将来可能会有更多的记录。</li>
<li>END_OF_INPUT - SourceReader 已经用完了所有的记录，达到了数据的终点。这意味着 SourceReader 可以被关闭。</li>
</ul>
<p>为了保证性能，会给 pollNext(ReaderOutput) 方法提供一个 ReaderOutput ，所以如果有必要， SourceReader 可以在一次调用 pollNext() 的过程中发出多条记录。例如，有时外部系统的工作粒度是块。一个块可能包含多条记录，但源码只能在块的边界处进行检查点。在这种情况下， SourceReader 可以一次将一个块中的所有记录排放到 ReaderOutput 。但是，除非必要， SourceReader 的实现应该避免在一次 pollNext(ReaderOutput) 的调用中发射多条记录。这是因为从 SourceReader 中进行轮询的任务线程是在事件循环中工作的，不能阻塞。</p>
<p>SourceReader 的所有状态都应该维护在 SourceSplits 里面，这些状态在 snapshotState() 调用时返回。这样做可以在需要时将 SourceSplits 重新分配给其他 SourceReaders 。</p>
<p>在创建 SourceReader 时，会向 Source 提供一个 SourceReaderContext 。预计 Source 将把上下文传递给 SourceReader 实例。SourceReader 可以通过 SourceReaderContext 向其 SplitEnumerator 发送 SourceEvent 。Source 的一个典型的设计模式是让 SourceReaders 向 SplitEnumerator 报告它们的本地信息， SplitEnumerator 有一个全局视图来做决策。</p>
<p>SourceReader API 是一个低级的 API ，它允许用户手动处理 split ，并有自己的线程模型来获取和交接记录。为了方便 SourceReader 的实现， Flink 提供了一个 <a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/SourceReaderBase.java">SourceReaderBase</a> 类，大大减少了编写 SourceReader 的工作量。强烈建议连接器开发人员利用 SourceReaderBase ，而不是从头开始编写 SourceReaders 。更多细节请查看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/sources.html#the-split-reader-api">Split Reader API</a> 部分。</p>
<h3 id="使用-source">使用 Source</h3>
<p>为了从 Source 创建 DataStream ，需要将 Source 传递给 StreamExecutionEnvironment。例如:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="n">mySource</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MySource</span><span class="o">(...)</span>

<span class="k">val</span> <span class="n">stream</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromSource</span><span class="o">(</span>
      <span class="n">mySource</span><span class="o">,</span>
      <span class="nc">WatermarkStrategy</span><span class="o">.</span><span class="n">noWatermarks</span><span class="o">(),</span>
      <span class="s">&#34;MySourceName&#34;</span><span class="o">)</span>
<span class="o">...</span>
</code></pre></div><h2 id="split-读取器-api">Split 读取器 API</h2>
<p>核心的 SourceReader API 是完全异步的，需要实现者手动管理异步拆分读取。然而，在实践中，大多数 Source 使用执行阻塞操作，比如在客户端（例如 KafkaConsumer ）上阻塞 poll() 调用，或者在分布式文件系统（ HDFS ， S3 ，&hellip;）上阻塞 I/O 操作。为了与异步的 Source API 兼容，这些阻塞（同步）操作需要发生在单独的线程中，线程将数据交给异步部分的阅读器。</p>
<p><a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/splitreader/SplitReader.java">SplitReader</a> 是用于简单的基于同步读取/轮询的源码实现的高级 API ，比如文件读取、 Kafka 等。</p>
<p>核心是 SourceReaderBase 类，它接收一个 SplitReader 并创建运行 SplitReader 的 fetcher 线程，支持不同的消费线程模型。</p>
<h3 id="splitreader">SplitReader</h3>
<p>SplitReader API 只有三个方法。</p>
<ul>
<li>一个阻塞获取方法，返回一个 <a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/RecordsWithSplitIds.java">RecordsWithSplitIds</a> 。</li>
<li>一种非阻塞方法，用于处理拆分变化。</li>
<li>一个非阻塞的唤醒方法，用于唤醒阻塞的获取操作。</li>
</ul>
<p>SplitReader 只专注于从外部系统读取记录，因此比 SourceReader 简单得多。详情请查看该类的 Java 文档。</p>
<h3 id="sourcereaderbase">SourceReaderBase</h3>
<p>SourceReader 的实现很常见，它做了以下工作。</p>
<ul>
<li>拥有一个线程池，以阻塞的方式从外部系统的分割处获取数据。</li>
<li>处理内部获取线程和其他方法调用之间的同步，如 pollNext(ReaderOutput) 。</li>
<li>维护每个 split 的水印，以便进行水印对齐。</li>
<li>维护每个分身的状态，以便检查点。</li>
</ul>
<p>为了减少编写一个新的 SourceReader 的工作， Flink 提供了一个 <a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/SourceReaderBase.java">SourceReaderBase</a> 类作为 SourceReader 的基础实现。SourceReaderBase 开箱即完成了上述所有工作。如果要编写一个新的 SourceReader ，只需要让 SourceReader 实现继承 SourceReaderBase ，填充一些方法，然后实现一个高级的 <a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/splitreader/SplitReader.java">SplitReader</a> 就可以了。</p>
<h3 id="splitfetchermanager">SplitFetcherManager</h3>
<p>SourceReaderBase 支持一些开箱即用的线程模型，这取决于与之合作的 <a href="https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/fetcher/SplitFetcherManager.java">SplitFetcherManager</a> 的行为。SplitFetcherManager 帮助创建和维护一个 SplitFetcher 池，每个 SplitFetcher 用一个 SplitReader 来获取。它还决定了如何将 split 分配给每个 split fetcher 。</p>
<p>举个例子，如下图所示，一个 SplitFetcherManager 可能有固定数量的线程，每个线程从分配给 SourceReader 的一些 split 中获取。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/source_reader.svg" alt="img"></p>
<p>下面的代码片段实现了这个线程模型。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="cm">/**
</span><span class="cm"> * A SplitFetcherManager that has a fixed size of split fetchers and assign splits 
</span><span class="cm"> * to the split fetchers based on the hash code of split IDs.
</span><span class="cm"> */</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">FixedSizeSplitFetcherManager</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">SplitT</span> <span class="kd">extends</span> <span class="n">SourceSplit</span><span class="o">&gt;</span> 
        <span class="kd">extends</span> <span class="n">SplitFetcherManager</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">SplitT</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">private</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">numFetchers</span><span class="o">;</span>

    <span class="kd">public</span> <span class="nf">FixedSizeSplitFetcherManager</span><span class="o">(</span>
            <span class="kt">int</span> <span class="n">numFetchers</span><span class="o">,</span>
            <span class="n">FutureNotifier</span> <span class="n">futureNotifier</span><span class="o">,</span>
            <span class="n">FutureCompletingBlockingQueue</span><span class="o">&lt;</span><span class="n">RecordsWithSplitIds</span><span class="o">&lt;</span><span class="n">E</span><span class="o">&gt;&gt;</span> <span class="n">elementsQueue</span><span class="o">,</span>
            <span class="n">Supplier</span><span class="o">&lt;</span><span class="n">SplitReader</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">SplitT</span><span class="o">&gt;&gt;</span> <span class="n">splitReaderSupplier</span><span class="o">)</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span><span class="n">futureNotifier</span><span class="o">,</span> <span class="n">elementsQueue</span><span class="o">,</span> <span class="n">splitReaderSupplier</span><span class="o">);</span>
        <span class="k">this</span><span class="o">.</span><span class="na">numFetchers</span> <span class="o">=</span> <span class="n">numFetchers</span><span class="o">;</span>
        <span class="c1">// Create numFetchers split fetchers.
</span><span class="c1"></span>        <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numFetchers</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
            <span class="n">startFetcher</span><span class="o">(</span><span class="n">createSplitFetcher</span><span class="o">());</span>
        <span class="o">}</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">addSplits</span><span class="o">(</span><span class="n">List</span><span class="o">&lt;</span><span class="n">SplitT</span><span class="o">&gt;</span> <span class="n">splitsToAdd</span><span class="o">)</span> <span class="o">{</span>
        <span class="c1">// Group splits by their owner fetchers.
</span><span class="c1"></span>        <span class="n">Map</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">SplitT</span><span class="o">&gt;&gt;</span> <span class="n">splitsByFetcherIndex</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashMap</span><span class="o">&lt;&gt;();</span>
        <span class="n">splitsToAdd</span><span class="o">.</span><span class="na">forEach</span><span class="o">(</span><span class="n">split</span> <span class="o">-&gt;</span> <span class="o">{</span>
            <span class="kt">int</span> <span class="n">ownerFetcherIndex</span> <span class="o">=</span> <span class="n">split</span><span class="o">.</span><span class="na">hashCode</span><span class="o">()</span> <span class="o">%</span> <span class="n">numFetchers</span><span class="o">;</span>
            <span class="n">splitsByFetcherIndex</span>
                    <span class="o">.</span><span class="na">computeIfAbsent</span><span class="o">(</span><span class="n">ownerFetcherIndex</span><span class="o">,</span> <span class="n">s</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;())</span>
                    <span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">split</span><span class="o">);</span>
        <span class="o">});</span>
        <span class="c1">// Assign the splits to their owner fetcher.
</span><span class="c1"></span>        <span class="n">splitsByFetcherIndex</span><span class="o">.</span><span class="na">forEach</span><span class="o">((</span><span class="n">fetcherIndex</span><span class="o">,</span> <span class="n">splitsForFetcher</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">{</span>
            <span class="n">fetchers</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">fetcherIndex</span><span class="o">).</span><span class="na">addSplits</span><span class="o">(</span><span class="n">splitsForFetcher</span><span class="o">);</span>
        <span class="o">});</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>而使用这个线程模型的 SourceReader 可以创建如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">FixedFetcherSizeSourceReader</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">T</span><span class="o">,</span> <span class="n">SplitT</span> <span class="kd">extends</span> <span class="n">SourceSplit</span><span class="o">,</span> <span class="n">SplitStateT</span><span class="o">&gt;</span>
        <span class="kd">extends</span> <span class="n">SourceReaderBase</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">T</span><span class="o">,</span> <span class="n">SplitT</span><span class="o">,</span> <span class="n">SplitStateT</span><span class="o">&gt;</span> <span class="o">{</span>

    <span class="kd">public</span> <span class="nf">FixedFetcherSizeSourceReader</span><span class="o">(</span>
            <span class="n">FutureNotifier</span> <span class="n">futureNotifier</span><span class="o">,</span>
            <span class="n">FutureCompletingBlockingQueue</span><span class="o">&lt;</span><span class="n">RecordsWithSplitIds</span><span class="o">&lt;</span><span class="n">E</span><span class="o">&gt;&gt;</span> <span class="n">elementsQueue</span><span class="o">,</span>
            <span class="n">Supplier</span><span class="o">&lt;</span><span class="n">SplitReader</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">SplitT</span><span class="o">&gt;&gt;</span> <span class="n">splitFetcherSupplier</span><span class="o">,</span>
            <span class="n">RecordEmitter</span><span class="o">&lt;</span><span class="n">E</span><span class="o">,</span> <span class="n">T</span><span class="o">,</span> <span class="n">SplitStateT</span><span class="o">&gt;</span> <span class="n">recordEmitter</span><span class="o">,</span>
            <span class="n">Configuration</span> <span class="n">config</span><span class="o">,</span>
            <span class="n">SourceReaderContext</span> <span class="n">context</span><span class="o">)</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span>
                <span class="n">futureNotifier</span><span class="o">,</span>
                <span class="n">elementsQueue</span><span class="o">,</span>
                <span class="k">new</span> <span class="n">FixedSizeSplitFetcherManager</span><span class="o">&lt;&gt;(</span>
                        <span class="n">config</span><span class="o">.</span><span class="na">getInteger</span><span class="o">(</span><span class="n">SourceConfig</span><span class="o">.</span><span class="na">NUM_FETCHERS</span><span class="o">),</span>
                        <span class="n">futureNotifier</span><span class="o">,</span>
                        <span class="n">elementsQueue</span><span class="o">,</span>
                        <span class="n">splitFetcherSupplier</span><span class="o">),</span>
                <span class="n">recordEmitter</span><span class="o">,</span>
                <span class="n">config</span><span class="o">,</span>
                <span class="n">context</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">onSplitFinished</span><span class="o">(</span><span class="n">Collection</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">finishedSplitIds</span><span class="o">)</span> <span class="o">{</span>
        <span class="c1">// Do something in the callback for the finished splits.
</span><span class="c1"></span>    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">SplitStateT</span> <span class="nf">initializedState</span><span class="o">(</span><span class="n">SplitT</span> <span class="n">split</span><span class="o">)</span> <span class="o">{</span>
        <span class="o">...</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">SplitT</span> <span class="nf">toSplitType</span><span class="o">(</span><span class="n">String</span> <span class="n">splitId</span><span class="o">,</span> <span class="n">SplitStateT</span> <span class="n">splitState</span><span class="o">)</span> <span class="o">{</span>
        <span class="o">...</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>显然， SourceReader 的实现也可以在 SplitFetcherManager 和 SourceReaderBase 之上轻松实现自己的线程模型。</p>
<h2 id="事件时间和水印">事件时间和水印</h2>
<p>事件时间分配和水印生成作为数据源的一部分发生。离开源读取器的事件流具有事件时间戳，并且（在流执行期间）包含水印。有关事件时间和水印的介绍，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/concepts/timely-stream-processing.html">及时流处理</a>。</p>
<p>重要事项 基于传统 <a href="https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java">SourceFunction</a> 的应用程序通常会在后面单独的步骤中通过 stream.assignTimestampsAndWatermarks(WatermarkStrategy) 生成时间戳和水印。这个函数不应该被用于新的源，因为时间戳将被分配，并且它将覆盖之前的分割感知水印。</p>
<h3 id="api">API</h3>
<p>在 DataStream API 创建期间， WatermarkStrategy 被传递给 Source，并创建 <a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/common/eventtime/TimestampAssigner.java">TimestampAssigner</a> 和 <a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/common/eventtime/WatermarkGenerator.java">WatermarkGenerator</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">environment</span><span class="o">.</span><span class="n">fromSource</span><span class="o">(</span>
    <span class="nc">Source</span><span class="o">&lt;</span><span class="nc">OUT</span><span class="o">,</span> <span class="o">?,</span> <span class="o">?&gt;</span> <span class="n">source</span><span class="o">,</span>
    <span class="nc">WatermarkStrategy</span><span class="o">&lt;</span><span class="nc">OUT</span><span class="o">&gt;</span> <span class="n">timestampsAndWatermarks</span><span class="o">,</span>
    <span class="nc">String</span> <span class="n">sourceName</span><span class="o">)</span>
</code></pre></div><p>TimestampAssigner 和 WatermarkGenerator 作为 ReaderOutput(或 SourceOutput) 的一部分透明地运行，因此源码实现者不必实现任何时间戳提取和水印生成代码。</p>
<h3 id="事件时间戳">事件时间戳</h3>
<p>事件时间戳的分配有两个步骤。</p>
<ol>
<li>
<p>SourceReader 可以通过调用 SourceOutput.collect(event, timestamp) 将源记录时间戳附加到事件上。这只与基于记录且有时间戳的数据源有关，如 Kafka 、 Kinesis 、 Pulsar 或 Pravega 。不基于记录且有时间戳的数据源（如文件）没有源记录时间戳。这一步是源连接器实现的一部分，而不是由使用源的应用程序参数化。</p>
</li>
<li>
<p>由应用程序配置的 TimestampAssigner 分配最终的时间戳。TimestampAssigner 看到原始源记录时间戳和事件。分配者可以使用源记录时间戳或访问事件的一个字段获得最终的事件时间戳。</p>
</li>
</ol>
<p>这种两步法允许用户同时引用源系统的时间戳和事件数据中的时间戳作为事件时间戳。</p>
<p>注意：当使用没有源记录时间戳的数据源（如文件），并选择源记录时间戳作为最终的事件时间戳时，事件将得到一个默认的时间戳，等于 LONG_MIN （=-9,223,372,036,854,775,808 ）。</p>
<h3 id="水印生成">水印生成</h3>
<p>水印生成器仅在流式执行期间激活。批量执行会停用水印生成器；下面描述的所有相关操作都将成为有效的无操作。</p>
<p>数据源 API 支持每次拆分单独运行水印生成器。这使得 Flink 可以单独观察每个分体的事件时间进度，这对于正确处理事件时间偏斜和防止空闲分区拖累整个应用的事件时间进度非常重要。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/per_split_watermarks.svg" alt="img"></p>
<p>当使用 Split Reader API 实现一个源连接器时，会自动处理这个问题。所有基于 Split Reader API 的实现都具有开箱即用的 split-aware 水印。</p>
<p>对于一个低级别的 SourceReader API 的实现来说，要使用 split-aware 水印的生成，该实现必须将不同的 split 事件输出到不同的输出中： Split-local SourceOutputs 。分割本地输出可以通过 createOutputForSplit(splitId) 和 releaseOutputForSplit(splitId) 方法在主 <a href="https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/source/ReaderOutput.java">ReaderOutput</a> 上创建和释放。详情请参考该类和方法的 JavaDocs 。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/sources.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/sources.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/data-sources" term="data-sources" label="Data Sources" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[数据集中的 zipping 元素]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="related" type="text/html" title="本地执行" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Zipping Elements in a Dataset</blockquote><h2 id="zipping-数据集中的元素">Zipping 数据集中的元素</h2>
<p>在某些算法中，人们可能需要为数据集元素分配唯一的标识符。本文档介绍了如何将 <a href="https://github.com/apache/flink/blob/master//flink-java/src/main/java/org/apache/flink/api/java/utils/DataSetUtils.java">DataSetUtils</a> 用于该目的。</p>
<h3 id="使用密集索引进行-zip">使用密集索引进行 Zip</h3>
<p><code>zipWithIndex</code> 给元素分配连续的标签，接收一个数据集作为输入，并返回一个新的（唯一id，初始值）2-tuples的数据集。这个过程需要两次传递，先计数再给元素贴标签，而且由于计数的同步性，不能采用流水线方式。备选的 <code>zipWithUniqueId</code> 以流水线的方式工作，当唯一的标签已经足够时，首选 <code>zip</code>。例如，下面的代码。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala._</span>

<span class="k">val</span> <span class="n">env</span><span class="k">:</span> <span class="kt">ExecutionEnvironment</span> <span class="o">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="s">&#34;A&#34;</span><span class="o">,</span> <span class="s">&#34;B&#34;</span><span class="o">,</span> <span class="s">&#34;C&#34;</span><span class="o">,</span> <span class="s">&#34;D&#34;</span><span class="o">,</span> <span class="s">&#34;E&#34;</span><span class="o">,</span> <span class="s">&#34;F&#34;</span><span class="o">,</span> <span class="s">&#34;G&#34;</span><span class="o">,</span> <span class="s">&#34;H&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">zipWithIndex</span>

<span class="n">result</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">resultPath</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34;,&#34;</span><span class="o">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><p>可以得到元组: (0,G), (1,H), (2,A), (3,B), (4,C), (5,D), (6,E), (7,F)</p>
<h3 id="带有唯一标识符的-zip">带有唯一标识符的 Zip</h3>
<p>在许多情况下，人们可能不需要分配连续的标签，<code>zipWithUniqueId</code> 以流水线的方式工作，加快了标签分配过程。该方法接收一个数据集作为输入，并返回一个由（唯一id，初始值）2-tuples组成的新数据集。例如，下面的代码。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.scala._</span>

<span class="k">val</span> <span class="n">env</span><span class="k">:</span> <span class="kt">ExecutionEnvironment</span> <span class="o">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="s">&#34;A&#34;</span><span class="o">,</span> <span class="s">&#34;B&#34;</span><span class="o">,</span> <span class="s">&#34;C&#34;</span><span class="o">,</span> <span class="s">&#34;D&#34;</span><span class="o">,</span> <span class="s">&#34;E&#34;</span><span class="o">,</span> <span class="s">&#34;F&#34;</span><span class="o">,</span> <span class="s">&#34;G&#34;</span><span class="o">,</span> <span class="s">&#34;H&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">zipWithUniqueId</span>

<span class="n">result</span><span class="o">.</span><span class="n">writeAsCsv</span><span class="o">(</span><span class="n">resultPath</span><span class="o">,</span> <span class="s">&#34;\n&#34;</span><span class="o">,</span> <span class="s">&#34;,&#34;</span><span class="o">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>
</code></pre></div><p>可以得到元组: (0,G), (1,A), (2,H), (3,B), (5,C), (7,D), (9,E), (11,F)</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/zip_elements_guide.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/zip_elements_guide.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[时间属性]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-time-attributes/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Time Attributes</blockquote><h1 id="时间属性">时间属性</h1>
<p>Flink 能够根据不同的时间概念来处理流数据。</p>
<ul>
<li>处理时间是指正在执行相应操作的机器的系统时间（也称为&quot;挂钟时间&quot;）。</li>
<li>事件时间指的是基于时间戳对流媒体数据的处理，时间戳附加在每一行上。时间戳可以编码事件发生的时间。</li>
<li>摄取时间是事件进入 Flink 的时间；在内部，它的处理方式与事件时间类似。</li>
</ul>
<p>关于 Flink 中时间处理的更多信息，请参见关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间和水印</a>的介绍。</p>
<p>本页解释了如何在 Flink 的表 API 和 SQL 中为基于时间的操作定义时间属性。</p>
<h2 id="时间属性介绍">时间属性介绍</h2>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#group-windows">Table API</a> 和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#group-windows">SQL</a> 中的窗口等基于时间的操作都需要时间概念及其来源的信息。因此，表可以提供逻辑时间属性，用于指示时间和在表程序中访问相应的时间戳。</p>
<p>时间属性可以成为每个表模式的一部分。它们是在从 CREATE TABLE DDL 或 DataStream 创建表时定义的，或者是在使用 TableSource 时预先定义的。一旦在开始时定义了时间属性，它就可以作为一个字段被引用，并且可以在基于时间的操作中使用。</p>
<p>只要时间属性没有被修改，只是从查询的一个部分转发到另一个部分，它仍然是一个有效的时间属性。时间属性的行为就像常规的时间戳一样，可以被访问进行计算。如果在计算中使用了时间属性，它将被具体化并成为常规时间戳。常规时间戳不与 Flink 的时间和水印系统合作，因此不能再用于基于时间的操作。</p>
<p>表程序要求已经为流环境指定了相应的时间特征。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

<span class="n">env</span><span class="o">.</span><span class="n">setStreamTimeCharacteristic</span><span class="o">(</span><span class="nc">TimeCharacteristic</span><span class="o">.</span><span class="nc">ProcessingTime</span><span class="o">)</span> <span class="c1">// default
</span><span class="c1"></span>
<span class="c1">// alternatively:
</span><span class="c1">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)
</span><span class="c1">// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
</span></code></pre></div><h2 id="处理时间">处理时间</h2>
<p>处理时间允许表程序根据本地机器的时间产生结果。它是最简单的时间概念，但不提供确定性。它既不需要提取时间戳，也不需要生成水印。</p>
<p>有三种方法可以定义处理时间属性。</p>
<h3 id="在创建表-ddl-中定义">在创建表 DDL 中定义</h3>
<p>处理时间属性是在创建表 DDL 中使用系统 PROCTIME()函数定义为计算列。关于计算列的更多信息请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/create.html#create-table">CREATE TABLE DDL</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">user_actions</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="n">user_name</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="k">data</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="n">user_action_time</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">PROCTIME</span><span class="p">()</span><span class="w"> </span><span class="c1">-- declare an additional field as a processing time attribute
</span><span class="c1"></span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="p">...</span><span class="w">
</span><span class="w"></span><span class="p">);</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">SELECT</span><span class="w"> </span><span class="n">TUMBLE_START</span><span class="p">(</span><span class="n">user_action_time</span><span class="p">,</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;10&#39;</span><span class="w"> </span><span class="k">MINUTE</span><span class="p">),</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="n">user_name</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">user_actions</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">TUMBLE</span><span class="p">(</span><span class="n">user_action_time</span><span class="p">,</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;10&#39;</span><span class="w"> </span><span class="k">MINUTE</span><span class="p">);</span><span class="w">
</span></code></pre></div><h3 id="在-datastream-to-table-转换期间">在 DataStream-to-Table 转换期间</h3>
<p>处理时间属性是在模式定义过程中用 <code>.proctime</code> 属性定义的。时间属性只能通过一个额外的逻辑字段来扩展物理模式。因此，它只能在模式定义的最后定义。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// declare an additional logical field as a processing time attribute
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;UserActionTimestamp&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;user_name&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;data&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;user_action_time&#34;</span><span class="o">.</span><span class="n">proctime</span><span class="o">)</span>

<span class="k">val</span> <span class="n">windowedTable</span> <span class="k">=</span> <span class="n">table</span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">10.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;user_action_time&#34;</span> <span class="n">as</span> <span class="s">&#34;userActionWindow&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="使用-table-source">使用 Table Source</h3>
<p>处理时间属性由实现 <code>DefinedProctimeAttribute</code> 接口的 <code>TableSource</code> 定义。逻辑时间属性附加到由 <code>TableSource</code> 的返回类型定义的物理模式中。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// define a table source with a processing attribute
</span><span class="c1"></span><span class="k">class</span> <span class="nc">UserActionSource</span> <span class="k">extends</span> <span class="nc">StreamTableSource</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">with</span> <span class="nc">DefinedProctimeAttribute</span> <span class="o">{</span>

	<span class="k">override</span> <span class="k">def</span> <span class="n">getReturnType</span> <span class="k">=</span> <span class="o">{</span>
		<span class="k">val</span> <span class="n">names</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;user_name&#34;</span> <span class="o">,</span> <span class="s">&#34;data&#34;</span><span class="o">)</span>
		<span class="k">val</span> <span class="n">types</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">[</span><span class="kt">TypeInformation</span><span class="o">[</span><span class="k">_</span><span class="o">]](</span><span class="nc">Types</span><span class="o">.</span><span class="nc">STRING</span><span class="o">,</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">STRING</span><span class="o">)</span>
		<span class="nc">Types</span><span class="o">.</span><span class="nc">ROW</span><span class="o">(</span><span class="n">names</span><span class="o">,</span> <span class="n">types</span><span class="o">)</span>
	<span class="o">}</span>

	<span class="k">override</span> <span class="k">def</span> <span class="n">getDataStream</span><span class="o">(</span><span class="n">execEnv</span><span class="k">:</span> <span class="kt">StreamExecutionEnvironment</span><span class="o">)</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
		<span class="c1">// create stream
</span><span class="c1"></span>		<span class="k">val</span> <span class="n">stream</span> <span class="k">=</span> <span class="o">...</span>
		<span class="n">stream</span>
	<span class="o">}</span>

	<span class="k">override</span> <span class="k">def</span> <span class="n">getProctimeAttribute</span> <span class="k">=</span> <span class="o">{</span>
		<span class="c1">// field with this name will be appended as a third field
</span><span class="c1"></span>		<span class="s">&#34;user_action_time&#34;</span>
	<span class="o">}</span>
<span class="o">}</span>

<span class="c1">// register table source
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">registerTableSource</span><span class="o">(</span><span class="s">&#34;user_actions&#34;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">UserActionSource</span><span class="o">)</span>

<span class="k">val</span> <span class="n">windowedTable</span> <span class="k">=</span> <span class="n">tEnv</span>
	<span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;user_actions&#34;</span><span class="o">)</span>
	<span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">10.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;user_action_time&#34;</span> <span class="n">as</span> <span class="s">&#34;userActionWindow&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="事件时间">事件时间</h3>
<p>事件时间允许表格程序根据每条记录中包含的时间产生结果。这使得即使在事件失序或事件迟到的情况下，也能得到一致的结果。当从持久存储中读取记录时，它还能保证表程序的结果可重放。</p>
<p>此外，事件时间允许在批处理和流环境中对表程序进行统一的语法。流式环境中的时间属性可以是批处理环境中记录的常规字段。</p>
<p>为了处理失序事件，区分流式环境中事件的准时和迟到，Flink 需要从事件中提取时间戳，并在时间上做出某种进展（所谓的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">水印</a>）。</p>
<p>事件时间属性既可以在创建表 DDL 中定义，也可以在 DataStream 到表的转换过程中定义，或者使用 TableSource 定义。</p>
<h3 id="在创建表-ddl-中定义-1">在创建表 DDL 中定义</h3>
<p>事件时间属性是在 CREATE TABLE DDL 中使用 WATERMARK 语句定义的。水印语句在现有的事件时间字段上定义了一个水印生成表达式，将事件时间字段标记为事件时间属性。关于水印语句和水印策略的更多信息，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/create.html#create-table">CREATE TABLE DDL</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">user_actions</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="n">user_name</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="k">data</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="n">user_action_time</span><span class="w"> </span><span class="k">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span><span class="w">
</span><span class="w">  </span><span class="c1">-- declare user_action_time as event time attribute and use 5 seconds delayed watermark strategy
</span><span class="c1"></span><span class="w">  </span><span class="n">WATERMARK</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">user_action_time</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">user_action_time</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;5&#39;</span><span class="w"> </span><span class="k">SECOND</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="p">...</span><span class="w">
</span><span class="w"></span><span class="p">);</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">SELECT</span><span class="w"> </span><span class="n">TUMBLE_START</span><span class="p">(</span><span class="n">user_action_time</span><span class="p">,</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;10&#39;</span><span class="w"> </span><span class="k">MINUTE</span><span class="p">),</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="n">user_name</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">user_actions</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">TUMBLE</span><span class="p">(</span><span class="n">user_action_time</span><span class="p">,</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;10&#39;</span><span class="w"> </span><span class="k">MINUTE</span><span class="p">);</span><span class="w">
</span></code></pre></div><h3 id="在-datastream-to-table-转换期间-1">在 DataStream-to-Table 转换期间</h3>
<p>事件时间属性是在模式定义期间用 <code>.rowtime</code> 属性定义的。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">时间戳和水印</a>必须在被转换的 DataStream 中被分配。</p>
<p>在将 DataStream 转换为表时，有两种方法可以定义时间属性。根据指定的.rowtime 字段名是否存在于 DataStream 的模式中，时间戳字段要么是</p>
<ul>
<li>作为一个新的字段添加到模式中，或</li>
<li>替换一个现有的字段。</li>
</ul>
<p>无论哪种情况，事件时间戳字段都将持有 DataStream 事件时间戳的值。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// Option 1:
</span><span class="c1"></span>
<span class="c1">// extract timestamp and assign watermarks based on knowledge of the stream
</span><span class="c1"></span><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">inputStream</span><span class="o">.</span><span class="n">assignTimestampsAndWatermarks</span><span class="o">(...)</span>

<span class="c1">// declare an additional logical field as an event time attribute
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;user_name&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;data&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;user_action_time&#34;</span><span class="o">.</span><span class="n">rowtime</span><span class="o">)</span>


<span class="c1">// Option 2:
</span><span class="c1"></span>
<span class="c1">// extract timestamp from first field, and assign watermarks based on knowledge of the stream
</span><span class="c1"></span><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="n">inputStream</span><span class="o">.</span><span class="n">assignTimestampsAndWatermarks</span><span class="o">(...)</span>

<span class="c1">// the first field has been used for timestamp extraction, and is no longer necessary
</span><span class="c1">// replace first field with a logical event time attribute
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;user_action_time&#34;</span><span class="o">.</span><span class="n">rowtime</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;user_name&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;data&#34;</span><span class="o">)</span>

<span class="c1">// Usage:
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">windowedTable</span> <span class="k">=</span> <span class="n">table</span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">10.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;user_action_time&#34;</span> <span class="n">as</span> <span class="s">&#34;userActionWindow&#34;</span><span class="o">)</span>
</code></pre></div><h2 id="使用-tablesource">使用 TableSource</h2>
<p>事件时间属性由一个实现 <code>DefinedRowtimeAttributes</code> 接口的 <code>TableSource</code> 定义。<code>getRowtimeAttributeDescriptors()</code> 方法返回一个 <code>RowtimeAttributeDescriptor</code> 列表，用于描述时间属性的最终名称，一个用于导出属性值的时间戳提取器，以及与属性相关的水印策略。</p>
<p>请确保 <code>getDataStream()</code> 方法返回的 DataStream 与定义的时间属性一致。只有当定义了 StreamRecordTimestamp 时间戳提取器时，才会考虑 DataStream 的时间戳（由 TimestampAssigner 分配的时间戳）。只有定义了 PreserveWatermarks 水印策略，DataStream 的水印才会被保留。否则，只有 TableSource 的 rowtime 属性的值是相关的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// define a table source with a rowtime attribute
</span><span class="c1"></span><span class="k">class</span> <span class="nc">UserActionSource</span> <span class="k">extends</span> <span class="nc">StreamTableSource</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">with</span> <span class="nc">DefinedRowtimeAttributes</span> <span class="o">{</span>

	<span class="k">override</span> <span class="k">def</span> <span class="n">getReturnType</span> <span class="k">=</span> <span class="o">{</span>
		<span class="k">val</span> <span class="n">names</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;user_name&#34;</span> <span class="o">,</span> <span class="s">&#34;data&#34;</span><span class="o">,</span> <span class="s">&#34;user_action_time&#34;</span><span class="o">)</span>
		<span class="k">val</span> <span class="n">types</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">[</span><span class="kt">TypeInformation</span><span class="o">[</span><span class="k">_</span><span class="o">]](</span><span class="nc">Types</span><span class="o">.</span><span class="nc">STRING</span><span class="o">,</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">STRING</span><span class="o">,</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">LONG</span><span class="o">)</span>
		<span class="nc">Types</span><span class="o">.</span><span class="nc">ROW</span><span class="o">(</span><span class="n">names</span><span class="o">,</span> <span class="n">types</span><span class="o">)</span>
	<span class="o">}</span>

	<span class="k">override</span> <span class="k">def</span> <span class="n">getDataStream</span><span class="o">(</span><span class="n">execEnv</span><span class="k">:</span> <span class="kt">StreamExecutionEnvironment</span><span class="o">)</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
		<span class="c1">// create stream
</span><span class="c1"></span>		<span class="c1">// ...
</span><span class="c1"></span>		<span class="c1">// assign watermarks based on the &#34;user_action_time&#34; attribute
</span><span class="c1"></span>		<span class="k">val</span> <span class="n">stream</span> <span class="k">=</span> <span class="n">inputStream</span><span class="o">.</span><span class="n">assignTimestampsAndWatermarks</span><span class="o">(...)</span>
		<span class="n">stream</span>
	<span class="o">}</span>

	<span class="k">override</span> <span class="k">def</span> <span class="n">getRowtimeAttributeDescriptors</span><span class="k">:</span> <span class="kt">util.List</span><span class="o">[</span><span class="kt">RowtimeAttributeDescriptor</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
		<span class="c1">// Mark the &#34;user_action_time&#34; attribute as event-time attribute.
</span><span class="c1"></span>		<span class="c1">// We create one attribute descriptor of &#34;user_action_time&#34;.
</span><span class="c1"></span>		<span class="k">val</span> <span class="n">rowtimeAttrDescr</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">RowtimeAttributeDescriptor</span><span class="o">(</span>
			<span class="s">&#34;user_action_time&#34;</span><span class="o">,</span>
			<span class="k">new</span> <span class="nc">ExistingField</span><span class="o">(</span><span class="s">&#34;user_action_time&#34;</span><span class="o">),</span>
			<span class="k">new</span> <span class="nc">AscendingTimestamps</span><span class="o">)</span>
		<span class="k">val</span> <span class="n">listRowtimeAttrDescr</span> <span class="k">=</span> <span class="nc">Collections</span><span class="o">.</span><span class="n">singletonList</span><span class="o">(</span><span class="n">rowtimeAttrDescr</span><span class="o">)</span>
		<span class="n">listRowtimeAttrDescr</span>
	<span class="o">}</span>
<span class="o">}</span>

<span class="c1">// register the table source
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">registerTableSource</span><span class="o">(</span><span class="s">&#34;user_actions&#34;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">UserActionSource</span><span class="o">)</span>

<span class="k">val</span> <span class="n">windowedTable</span> <span class="k">=</span> <span class="n">tEnv</span>
	<span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;user_actions&#34;</span><span class="o">)</span>
	<span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Tumble</span> <span class="n">over</span> <span class="mf">10.</span><span class="n">minutes</span> <span class="n">on</span> <span class="n">$</span><span class="s">&#34;user_action_time&#34;</span> <span class="n">as</span> <span class="s">&#34;userActionWindow&#34;</span><span class="o">)</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[本地执行]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-local-execution/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-local-execution/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Local Execution</blockquote><h2 id="本地执行">本地执行</h2>
<p>Flink 可以在一台机器上运行，甚至在一台 Java 虚拟机中运行。这使得用户可以在本地测试和调试 Flink 程序。本节将对本地执行机制进行概述。</p>
<p>本地环境和执行器允许你在本地 Java 虚拟机中运行 Flink 程序，或者作为现有程序的一部分在任何 JVM 中运行。大多数例子可以通过简单地点击 IDE 的&quot;运行&quot;按钮在本地启动。</p>
<p>Flink 中支持两种不同的本地执行。LocalExecutionEnvironment 是启动完整的 Flink 运行时，包括一个 JobManager 和一个 TaskManager。这些包括内存管理和所有在集群模式下执行的内部算法。</p>
<p>CollectionEnvironment 是在 Java 集合上执行 Flink 程序。这种模式不会启动完整的 Flink 运行时，所以执行的开销非常低，而且是轻量级的。例如，一个 <code>DataSet.map()</code> 转换将通过将 <code>map()</code> 函数应用于 Java 列表中的所有元素来执行。</p>
<h3 id="调试">调试</h3>
<p>如果你在本地运行 Flink 程序，你也可以像其他 Java 程序一样调试你的程序。你可以使用 <code>System.out.println()</code> 来写出一些内部变量，也可以使用调试器。可以在 <code>map()</code>、<code>reduce()</code> 和其他所有方法中设置断点。也请参考 Java API 文档中的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#debugging">调试部分</a>，了解 Java API 中的测试和本地调试工具的指南。</p>
<h3 id="maven-依赖">Maven 依赖</h3>
<p>如果你是在 Maven 项目中开发程序，你必须使用这个依赖关系添加 flink-clients 模块。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-clients_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><h3 id="本地环境">本地环境</h3>
<p>LocalEnvironment 是 Flink 程序本地执行的一个句柄。使用它可以在本地 JVM 中运行程序&ndash;独立或嵌入其他程序中。</p>
<p>本地环境是通过 <code>ExecutionEnvironment.createLocalEnvironment()</code> 方法实例化的。默认情况下，它将使用与你的机器有多少 CPU 核（硬件上下文）一样多的本地线程来执行。您也可以指定所需的并行度。本地环境可以配置为使用 <code>enableLogging()/disableLogging()</code> 将日志记录到控制台。</p>
<p>在大多数情况下，调用 <code>ExecutionEnvironment.getExecutionEnvironment()</code> 是更好的方法。当程序在本地（命令行接口之外）启动时，该方法会返回一个 <code>LocalEnvironment</code>，当程序被命令行接口调用时，该方法会返回一个预配置的集群执行环境。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
    <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">createLocalEnvironment</span><span class="o">();</span>

    <span class="n">DataSet</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">readTextFile</span><span class="o">(</span><span class="s">&#34;file:///path/to/file&#34;</span><span class="o">);</span>

    <span class="n">data</span>
        <span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="k">new</span> <span class="n">FilterFunction</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;()</span> <span class="o">{</span>
            <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">filter</span><span class="o">(</span><span class="n">String</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
                <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="na">startsWith</span><span class="o">(</span><span class="s">&#34;http://&#34;</span><span class="o">);</span>
            <span class="o">}</span>
        <span class="o">})</span>
        <span class="o">.</span><span class="na">writeAsText</span><span class="o">(</span><span class="s">&#34;file:///path/to/result&#34;</span><span class="o">);</span>

    <span class="n">JobExecutionResult</span> <span class="n">res</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">();</span>
<span class="o">}</span>
</code></pre></div><p>在执行结束后返回的 JobExecutionResult 对象，包含了程序运行时间和累加器结果。</p>
<p>LocalEnvironment 还允许向 Flink 传递自定义配置值。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">Configuration</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
<span class="n">conf</span><span class="o">.</span><span class="na">setFloat</span><span class="o">(</span><span class="n">ConfigConstants</span><span class="o">.</span><span class="na">TASK_MANAGER_MEMORY_FRACTION_KEY</span><span class="o">,</span> <span class="n">0</span><span class="o">.</span><span class="na">5f</span><span class="o">);</span>
<span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">createLocalEnvironment</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span>
</code></pre></div><p>注意：本地执行环境不启动任何 Web 前端来监控执行。</p>
<h3 id="收集环境">收集环境</h3>
<p>使用 CollectionEnvironment 在 Java 集合上执行是一种执行 Flink 程序的低开销方法。这种模式的典型用例是自动测试、调试和代码重用。</p>
<p>用户可以使用为批处理而实现的算法，也可以用于交互性更强的情况。Flink 程序的一个稍微改变的变体可以用于 Java 应用服务器中处理传入的请求。</p>
<p>基于集合执行的骨架:</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
    <span class="c1">// initialize a new Collection-based execution environment
</span><span class="c1"></span>    <span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CollectionEnvironment</span><span class="o">();</span>

    <span class="n">DataSet</span><span class="o">&lt;</span><span class="n">User</span><span class="o">&gt;</span> <span class="n">users</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">fromCollection</span><span class="o">(</span> <span class="cm">/* get elements from a Java Collection */</span><span class="o">);</span>

    <span class="cm">/* Data Set transformations ... */</span>

    <span class="c1">// retrieve the resulting Tuple2 elements into a ArrayList.
</span><span class="c1"></span>    <span class="n">Collection</span><span class="o">&lt;...&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;...&gt;();</span>
    <span class="n">resultDataSet</span><span class="o">.</span><span class="na">output</span><span class="o">(</span><span class="k">new</span> <span class="n">LocalCollectionOutputFormat</span><span class="o">&lt;...&gt;(</span><span class="n">result</span><span class="o">));</span>

    <span class="c1">// kick off execution.
</span><span class="c1"></span>    <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">();</span>

    <span class="c1">// Do some work with the resulting ArrayList (=Collection).
</span><span class="c1"></span>    <span class="k">for</span><span class="o">(...</span> <span class="n">t</span> <span class="o">:</span> <span class="n">result</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">System</span><span class="o">.</span><span class="na">err</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&#34;Result = &#34;</span><span class="o">+</span><span class="n">t</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>flink-examples-batch 模块包含一个完整的例子，叫做 CollectionExecutionExample。</p>
<p>请注意，基于集合的 Flink 程序的执行只可能在小数据上执行，小数据适合 JVM 堆。在集合上的执行不是多线程的，只使用一个线程。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/local_execution.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/local_execution.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[查询]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-queries/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-queries/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Queries</blockquote><h1 id="查询">查询</h1>
<p>SELECT 语句和 VALUES 语句是用 TableEnvironment 的 sqlQuery()方法指定的。该方法将 SELECT 语句（或 VALUES 语句）的结果作为一个表返回。表可以在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#mixing-table-api-and-sql">后续的 SQL 和 Table API 查询</a>中使用，可以<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#integration-with-datastream-and-dataset-api">转换为 DataSet 或 DataStream</a>，也可以<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#emit-a-table">写入 TableSink</a>。SQL 和 Table API 查询可以无缝混合，并进行整体优化，转化为一个程序。</p>
<p>为了在 SQL 查询中访问一个表，必须<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#register-tables-in-the-catalog">在 TableEnvironment 中注册</a>。表可以从 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#register-a-tablesource">TableSource</a>、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#register-a-table">Table</a>、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#create-table">CREATE TABLE 语句</a>、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#register-a-datastream-or-dataset-as-table">DataStream 或 DataSet</a> 中注册。另外，用户也可以<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/catalogs.html">在 TableEnvironment 中注册目录</a>来指定数据源的位置。</p>
<p>为了方便起见，Table.toString()会自动在其 TableEnvironment 中以唯一的名称注册表，并返回名称。所以，Table 对象可以直接内联到 SQL 查询中，如下例所示。</p>
<p>注意：包含不支持的 SQL 特性的查询会导致 TableException。批量表和流式表的 SQL 支持的功能在下面的章节中列出。</p>
<h2 id="指定查询">指定查询</h2>
<p>下面的例子显示了如何在注册表和内联表上指定 SQL 查询。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// read a DataStream from an external source
</span><span class="c1"></span><span class="k">val</span> <span class="n">ds</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span>, <span class="kt">Integer</span><span class="o">)]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">addSource</span><span class="o">(...)</span>

<span class="c1">// SQL query with an inlined (unregistered) table
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">toTable</span><span class="o">(</span><span class="n">tableEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;user&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;product&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;amount&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
  <span class="s">s&#34;SELECT SUM(amount) FROM </span><span class="si">$table</span><span class="s"> WHERE product LIKE &#39;%Rubber%&#39;&#34;</span><span class="o">)</span>

<span class="c1">// SQL query with a registered table
</span><span class="c1">// register the DataStream under the name &#34;Orders&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">,</span> <span class="n">ds</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;user&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;product&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;amount&#34;</span><span class="o">)</span>
<span class="c1">// run a SQL query on the Table and retrieve the result as a new Table
</span><span class="c1"></span><span class="k">val</span> <span class="n">result2</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
  <span class="s">&#34;SELECT product, amount FROM Orders WHERE product LIKE &#39;%Rubber%&#39;&#34;</span><span class="o">)</span>

<span class="c1">// create and register a TableSink
</span><span class="c1"></span><span class="k">val</span> <span class="n">schema</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Schema</span><span class="o">()</span>
    <span class="o">.</span><span class="n">field</span><span class="o">(</span><span class="s">&#34;product&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">STRING</span><span class="o">())</span>
    <span class="o">.</span><span class="n">field</span><span class="o">(</span><span class="s">&#34;amount&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">INT</span><span class="o">())</span>

<span class="n">tableEnv</span><span class="o">.</span><span class="n">connect</span><span class="o">(</span><span class="k">new</span> <span class="nc">FileSystem</span><span class="o">(</span><span class="s">&#34;/path/to/file&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">withFormat</span><span class="o">(...)</span>
    <span class="o">.</span><span class="n">withSchema</span><span class="o">(</span><span class="n">schema</span><span class="o">)</span>
    <span class="o">.</span><span class="n">createTemporaryTable</span><span class="o">(</span><span class="s">&#34;RubberOrders&#34;</span><span class="o">)</span>

<span class="c1">// run an INSERT SQL on the Table and emit the result to the TableSink
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span>
  <span class="s">&#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE &#39;%Rubber%&#39;&#34;</span><span class="o">)</span>
</code></pre></div><h2 id="执行查询">执行查询</h2>
<p>可以通过 TableEnvironment.executeSql()方法执行 SELECT 语句或 VALUES 语句，将内容收集到本地。该方法将 SELECT 语句（或 VALUES 语句）的结果作为 TableResult 返回。与 SELECT 语句类似，可以使用 Table.execute()方法执行 Table 对象，将查询的内容收集到本地客户端。TableResult.collect()方法返回一个可关闭的行迭代器。除非收集完所有的结果数据，否则选择作业不会结束。我们应该通过 CloseableIterator#close()方法主动关闭作业，避免资源泄露。我们也可以通过 TableResult.print()方法将选择结果打印到客户端控制台。TableResult 中的结果数据只能被访问一次。因此，collect()和 print()不能相继被调用。</p>
<p>对于流式作业，TableResult.collect()方法或 TableResult.print()方法可以保证端到端的精确一次记录传递。这需要启用检查点机制。默认情况下，检查点机制是被禁用的。要启用检查点，我们可以通过 TableConfig 设置检查点属性（详见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html#checkpointing">检查点配置</a>）。所以一条结果记录只有在其对应的检查点完成后才能被客户端访问。</p>
<p>注意事项 对于流媒体模式，现在只支持只追加查询。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">,</span> <span class="n">settings</span><span class="o">)</span>
<span class="c1">// enable checkpointing
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">getConfig</span><span class="o">.</span><span class="n">getConfiguration</span><span class="o">.</span><span class="n">set</span><span class="o">(</span>
  <span class="nc">ExecutionCheckpointingOptions</span><span class="o">.</span><span class="nc">CHECKPOINTING_MODE</span><span class="o">,</span> <span class="nc">CheckpointingMode</span><span class="o">.</span><span class="nc">EXACTLY_ONCE</span><span class="o">)</span>
<span class="n">tableEnv</span><span class="o">.</span><span class="n">getConfig</span><span class="o">.</span><span class="n">getConfiguration</span><span class="o">.</span><span class="n">set</span><span class="o">(</span>
  <span class="nc">ExecutionCheckpointingOptions</span><span class="o">.</span><span class="nc">CHECKPOINTING_INTERVAL</span><span class="o">,</span> <span class="nc">Duration</span><span class="o">.</span><span class="n">ofSeconds</span><span class="o">(</span><span class="mi">10</span><span class="o">))</span>

<span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)&#34;</span><span class="o">)</span>

<span class="c1">// execute SELECT statement
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableResult1</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;SELECT * FROM Orders&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">it</span> <span class="k">=</span> <span class="n">tableResult1</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
<span class="k">try</span> <span class="k">while</span> <span class="o">(</span><span class="n">it</span><span class="o">.</span><span class="n">hasNext</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">row</span> <span class="k">=</span> <span class="n">it</span><span class="o">.</span><span class="n">next</span>
  <span class="c1">// handle row
</span><span class="c1"></span><span class="o">}</span>
<span class="k">finally</span> <span class="n">it</span><span class="o">.</span><span class="n">close</span><span class="o">()</span> <span class="c1">// close the iterator to avoid resource leak
</span><span class="c1"></span>
<span class="c1">// execute Table
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableResult2</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span><span class="s">&#34;SELECT * FROM Orders&#34;</span><span class="o">).</span><span class="n">execute</span><span class="o">()</span>
<span class="n">tableResult2</span><span class="o">.</span><span class="n">print</span><span class="o">()</span>
</code></pre></div><h2 id="语法">语法</h2>
<p>Flink 使用 <a href="https://calcite.apache.org/docs/reference.html">Apache Calcite</a> 解析 SQL，它支持标准的 ANSI SQL。</p>
<p>下面的 BNF-语法描述了在批处理和流式查询中支持的 SQL 特性的超集。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#operations">操作</a>部分显示了支持的特性的例子，并指出哪些特性只支持批处理或流式查询。</p>
<pre><code>query:
  values
  | {
      select
      | selectWithoutFrom
      | query UNION [ ALL ] query
      | query EXCEPT query
      | query INTERSECT query
    }
    [ ORDER BY orderItem [, orderItem ]* ]
    [ LIMIT { count | ALL } ]
    [ OFFSET start { ROW | ROWS } ]
    [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ONLY]

orderItem:
  expression [ ASC | DESC ]

select:
  SELECT [ ALL | DISTINCT ]
  { * | projectItem [, projectItem ]* }
  FROM tableExpression
  [ WHERE booleanExpression ]
  [ GROUP BY { groupItem [, groupItem ]* } ]
  [ HAVING booleanExpression ]
  [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ]

selectWithoutFrom:
  SELECT [ ALL | DISTINCT ]
  { * | projectItem [, projectItem ]* }

projectItem:
  expression [ [ AS ] columnAlias ]
  | tableAlias . *

tableExpression:
  tableReference [, tableReference ]*
  | tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ]

joinCondition:
  ON booleanExpression
  | USING '(' column [, column ]* ')'

tableReference:
  tablePrimary
  [ matchRecognize ]
  [ [ AS ] alias [ '(' columnAlias [, columnAlias ]* ')' ] ]

tablePrimary:
  [ TABLE ] [ [ catalogName . ] schemaName . ] tableName [ dynamicTableOptions ]
  | LATERAL TABLE '(' functionName '(' expression [, expression ]* ')' ')'
  | UNNEST '(' expression ')'

dynamicTableOptions:
  /*+ OPTIONS(key=val [, key=val]*) */

key:
  stringLiteral

val:
  stringLiteral

values:
  VALUES expression [, expression ]*

groupItem:
  expression
  | '(' ')'
  | '(' expression [, expression ]* ')'
  | CUBE '(' expression [, expression ]* ')'
  | ROLLUP '(' expression [, expression ]* ')'
  | GROUPING SETS '(' groupItem [, groupItem ]* ')'

windowRef:
    windowName
  | windowSpec

windowSpec:
    [ windowName ]
    '('
    [ ORDER BY orderItem [, orderItem ]* ]
    [ PARTITION BY expression [, expression ]* ]
    [
        RANGE numericOrIntervalExpression {PRECEDING}
      | ROWS numericExpression {PRECEDING}
    ]
    ')'

matchRecognize:
      MATCH_RECOGNIZE '('
      [ PARTITION BY expression [, expression ]* ]
      [ ORDER BY orderItem [, orderItem ]* ]
      [ MEASURES measureColumn [, measureColumn ]* ]
      [ ONE ROW PER MATCH ]
      [ AFTER MATCH
            ( SKIP TO NEXT ROW
            | SKIP PAST LAST ROW
            | SKIP TO FIRST variable
            | SKIP TO LAST variable
            | SKIP TO variable )
      ]
      PATTERN '(' pattern ')'
      [ WITHIN intervalLiteral ]
      DEFINE variable AS condition [, variable AS condition ]*
      ')'

measureColumn:
      expression AS alias

pattern:
      patternTerm [ '|' patternTerm ]*

patternTerm:
      patternFactor [ patternFactor ]*

patternFactor:
      variable [ patternQuantifier ]

patternQuantifier:
      '*'
  |   '*?'
  |   '+'
  |   '+?'
  |   '?'
  |   '??'
  |   '{' { [ minRepeat ], [ maxRepeat ] } '}' ['?']
  |   '{' repeat '}'
</code></pre><p>Flink SQL 对标识符（表名、属性名、函数名）使用了类似 Java 的词汇策略。</p>
<p>无论标识符是否被引用，它们的大小写都会被保留。
之后，标识符会被大小写敏感地匹配。
与 Java 不同的是，回标允许标识符包含非字母数字字符（例如：&ldquo;SELECT a AS<code>my field</code>FROM t&rdquo;）。
字符串必须用单引号括起来（例如，SELECT &lsquo;Hello World&rsquo;）。重复一个单引号进行转义（例如，SELECT &lsquo;It&rsquo;s me.'）。字符串中支持 Unicode 字符。如果需要明确的 unicode 码点，请使用以下语法。</p>
<p>使用反斜杠（\）作为转义字符（默认）。SELECT U&amp;'\263A&rsquo;
使用自定义转义字符。SELECT U&amp;'#263A' UESCAPE &lsquo;#'。</p>
<h2 id="operations">Operations</h2>
<h3 id="scan-projection-和-filter">Scan, Projection 和 Filter</h3>
<ul>
<li>Scan / Select / As(Batch/Streaming)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">SELECT</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">c</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span></code></pre></div><ul>
<li>Where / Filter(Batch/Streaming)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;red&#39;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w">
</span></code></pre></div><ul>
<li>用户定义标量函数 (Scalar UDF)(Batch/Streaming)</li>
</ul>
<p>UDF 必须在 TableEnvironment 中注册。关于如何指定和注册标量 UDF 的详细信息，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html">UDF 文档</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">PRETTY_PRINT</span><span class="p">(</span><span class="k">user</span><span class="p">)</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span></code></pre></div><h3 id="聚合">聚合</h3>
<ul>
<li>GroupBy 聚合(Batch/Streaming/Result Updating)</li>
</ul>
<p>注意：流表上的 GroupBy 会产生更新结果。详情请参见<a href="https://ohmyweekly.github.io/notes/2020-08-22-dynamic-tables">动态表流</a>概念页面。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">SUM</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">d</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">a</span><span class="w">
</span></code></pre></div><ul>
<li>GroupBy 窗口聚合(Batch/Streaming)</li>
</ul>
<p>使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#group-windows">分组窗口</a>来计算每个组的单一结果行。更多细节请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#group-windows">分组窗口</a>部分。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="p">,</span><span class="w"> </span><span class="k">SUM</span><span class="p">(</span><span class="n">amount</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">TUMBLE</span><span class="p">(</span><span class="n">rowtime</span><span class="p">,</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;1&#39;</span><span class="w"> </span><span class="k">DAY</span><span class="p">),</span><span class="w"> </span><span class="k">user</span><span class="w">
</span></code></pre></div><ul>
<li>Over 窗口聚合(Streaming)</li>
</ul>
<p>注意：所有的聚合必须定义在同一个窗口上，即相同的分区、排序和范围。目前，只支持对 CURRENT ROW 范围的 PRECEDING（UNBOUNDED 和 bounded）窗口。还不支持带 FOLLOWING 的范围。ORDER BY 必须在单个<a href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes">时间属性</a>上指定。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="n">amount</span><span class="p">)</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">user</span><span class="w">
</span><span class="w">  </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">proctime</span><span class="w">
</span><span class="w">  </span><span class="k">ROWS</span><span class="w"> </span><span class="k">BETWEEN</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">PRECEDING</span><span class="w"> </span><span class="k">AND</span><span class="w"> </span><span class="k">CURRENT</span><span class="w"> </span><span class="k">ROW</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">SELECT</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="n">amount</span><span class="p">)</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="k">SUM</span><span class="p">(</span><span class="n">amount</span><span class="p">)</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="n">w</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="n">WINDOW</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">user</span><span class="w">
</span><span class="w">  </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">proctime</span><span class="w">
</span><span class="w">  </span><span class="k">ROWS</span><span class="w"> </span><span class="k">BETWEEN</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">PRECEDING</span><span class="w"> </span><span class="k">AND</span><span class="w"> </span><span class="k">CURRENT</span><span class="w"> </span><span class="k">ROW</span><span class="p">)</span><span class="w">
</span></code></pre></div><ul>
<li>Distinct(Batch/Streaming/Result Updating)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">DISTINCT</span><span class="w"> </span><span class="n">users</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span></code></pre></div><p>注意：对于流式查询，计算查询结果所需的状态可能会根据不同字段的数量而无限增长。请提供一个有效的保留时间间隔的查询配置，以防止过大的状态大小。详情请看<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<ul>
<li>Grouping sets, Rollup, Cube(Batch/Streaming/Result Updating)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">SUM</span><span class="p">(</span><span class="n">amount</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">GROUPING</span><span class="w"> </span><span class="k">SETS</span><span class="w"> </span><span class="p">((</span><span class="k">user</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="n">product</span><span class="p">))</span><span class="w">
</span></code></pre></div><p>注：流式模式分组集、Rollup 和 Cube 仅在 Blink 计划器中支持。</p>
<ul>
<li>Having(Batch/Streaming)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">SUM</span><span class="p">(</span><span class="n">amount</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">users</span><span class="w">
</span><span class="w"></span><span class="k">HAVING</span><span class="w"> </span><span class="k">SUM</span><span class="p">(</span><span class="n">amount</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">50</span><span class="w">
</span></code></pre></div><ul>
<li>用户定义聚合函数 (UDAGG)(Batch/Streaming)</li>
</ul>
<p>UDAGG 必须在 TableEnvironment 中注册。关于如何指定和注册 UDAGG 的细节，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html">UDF 文档</a>。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">MyAggregate</span><span class="p">(</span><span class="n">amount</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">users</span><span class="w">
</span></code></pre></div><h3 id="joins">Joins</h3>
<ul>
<li>Inner Equi-join(Batch/Streaming)</li>
</ul>
<p>目前，只支持等价连接，即至少有一个带有平等谓词的共轭条件的连接，不支持任意的交叉连接或θ连接。不支持任意的交叉连接或θ连接。</p>
<p>注意：连接的顺序没有被优化。表的连接顺序是按照 FROM 子句中指定的顺序进行的。确保指定表的顺序不会产生交叉连接（笛卡尔乘积），因为交叉连接不支持，会导致查询失败。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">INNER</span><span class="w"> </span><span class="k">JOIN</span><span class="w"> </span><span class="n">Product</span><span class="w"> </span><span class="k">ON</span><span class="w"> </span><span class="n">Orders</span><span class="p">.</span><span class="n">productId</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Product</span><span class="p">.</span><span class="n">id</span><span class="w">
</span></code></pre></div><p>注意：对于流式查询，计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供一个具有有效保留时间间隔的查询配置，以防止状态大小过大。详情请看<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<ul>
<li>Outer Equi-join(Batch/Streaming/Result Updating)</li>
</ul>
<p>目前，只支持 equi-joins 连接，即至少有一个带有平等谓词的共轭条件的连接，不支持任意的交叉连接或θ连接。不支持任意的交叉连接或θ连接。</p>
<p>注意：连接的顺序没有被优化。表的连接顺序是按照 FROM 子句中指定的顺序进行的。确保指定表的顺序不会产生交叉连接（笛卡尔乘积），因为交叉连接不支持，会导致查询失败。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">LEFT</span><span class="w"> </span><span class="k">JOIN</span><span class="w"> </span><span class="n">Product</span><span class="w"> </span><span class="k">ON</span><span class="w"> </span><span class="n">Orders</span><span class="p">.</span><span class="n">productId</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Product</span><span class="p">.</span><span class="n">id</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">RIGHT</span><span class="w"> </span><span class="k">JOIN</span><span class="w"> </span><span class="n">Product</span><span class="w"> </span><span class="k">ON</span><span class="w"> </span><span class="n">Orders</span><span class="p">.</span><span class="n">productId</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Product</span><span class="p">.</span><span class="n">id</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">FULL</span><span class="w"> </span><span class="k">OUTER</span><span class="w"> </span><span class="k">JOIN</span><span class="w"> </span><span class="n">Product</span><span class="w"> </span><span class="k">ON</span><span class="w"> </span><span class="n">Orders</span><span class="p">.</span><span class="n">productId</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Product</span><span class="p">.</span><span class="n">id</span><span class="w">
</span></code></pre></div><p>注意：对于流式查询，计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供一个具有有效保留时间间隔的查询配置，以防止状态大小过大。详情请看<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<ul>
<li>Interval Join(Batch/Streaming)</li>
</ul>
<p>注：区间连接是常规连接的一个子集，可以用流式处理。</p>
<p>一个区间连接至少需要一个等价连接谓词和一个连接条件，以限制双方的时间。这样的条件可以由两个合适的范围谓词（&lt;，&lt;=，&gt;=，&gt;）、一个 BETWEEN 谓词或一个比较两个输入表的相同类型的<a href="https://ohmyweekly.github.io/notes/2020-08-22-time-attributes">时间属性</a>（即处理时间或事件时间）的单一平等谓词来定义。</p>
<p>例如，以下谓词是有效的区间连接条件。</p>
<ul>
<li>ltime = rtime</li>
<li>ltime &gt;= rtime AND ltime &lt; rtime + INTERVAL &lsquo;10&rsquo; MINUTE</li>
<li>ltime BETWEEN rtime - INTERVAL &lsquo;10&rsquo; SECOND AND rtime + INTERVAL &lsquo;5&rsquo; SECOND</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="n">o</span><span class="p">,</span><span class="w"> </span><span class="n">Shipments</span><span class="w"> </span><span class="n">s</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">orderId</span><span class="w"> </span><span class="k">AND</span><span class="w">
</span><span class="w">      </span><span class="n">o</span><span class="p">.</span><span class="n">ordertime</span><span class="w"> </span><span class="k">BETWEEN</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">shiptime</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;4&#39;</span><span class="w"> </span><span class="n">HOUR</span><span class="w"> </span><span class="k">AND</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">shiptime</span><span class="w">
</span></code></pre></div><p>上面的例子中，如果在收到订单 4 小时后才发货，那么就会将所有的订单与其对应的货物加入。</p>
<ul>
<li>将数组扩展为关系(Batch/Streaming)</li>
</ul>
<p>还不支持 Unnesting With ORDINALITY。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">users</span><span class="p">,</span><span class="w"> </span><span class="n">tag</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">CROSS</span><span class="w"> </span><span class="k">JOIN</span><span class="w"> </span><span class="k">UNNEST</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="p">(</span><span class="n">tag</span><span class="p">)</span><span class="w">
</span></code></pre></div><ul>
<li>Join with Table Function (UDTF)(Batch/Streaming)</li>
</ul>
<p>用表格函数的结果连接一个表格。左表（外表）的每一行都与表函数的相应调用所产生的所有行相连接。</p>
<p>用户定义表函数（UDTF）必须在之前注册。关于如何指定和注册 UDTF 的细节，请参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html">UDF 文档</a>。</p>
<p><strong>Inner Join</strong></p>
<p>左表（外表）的一行，如果它的表函数调用返回一个空的结果，就会被删除。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">users</span><span class="p">,</span><span class="w"> </span><span class="n">tag</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="p">,</span><span class="w"> </span><span class="k">LATERAL</span><span class="w"> </span><span class="k">TABLE</span><span class="p">(</span><span class="n">unnest_udtf</span><span class="p">(</span><span class="n">tags</span><span class="p">))</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">tag</span><span class="w">
</span></code></pre></div><p><strong>Left Outer Join</strong></p>
<p>如果表函数调用返回的结果为空，则保留相应的外行，并将结果用空值填充。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">users</span><span class="p">,</span><span class="w"> </span><span class="n">tag</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">LEFT</span><span class="w"> </span><span class="k">JOIN</span><span class="w"> </span><span class="k">LATERAL</span><span class="w"> </span><span class="k">TABLE</span><span class="p">(</span><span class="n">unnest_udtf</span><span class="p">(</span><span class="n">tags</span><span class="p">))</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">tag</span><span class="w"> </span><span class="k">ON</span><span class="w"> </span><span class="k">TRUE</span><span class="w">
</span></code></pre></div><p>注意：目前，只有字面意义上的 &ldquo;TRUE &ldquo;被支持为针对横向表的左外连接的谓词。</p>
<ul>
<li>Join with Temporal Table Function(Streaming)</li>
</ul>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">时间表</a>是跟踪随时间变化的表。</p>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table-functions">时间表函数</a>提供了对时间表在特定时间点的状态的访问。使用时态表函数连接表的语法与使用表函数连接相同。</p>
<p>注意：目前只支持与时态表的内部连接。</p>
<p>假设 Rates 是一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table-functions">时间表函数</a>，连接可以用 SQL 表达如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w">
</span><span class="w">  </span><span class="n">o_amount</span><span class="p">,</span><span class="w"> </span><span class="n">r_rate</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w">
</span><span class="w">  </span><span class="n">Orders</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="k">LATERAL</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="p">(</span><span class="n">Rates</span><span class="p">(</span><span class="n">o_proctime</span><span class="p">))</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w">
</span><span class="w">  </span><span class="n">r_currency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">o_currency</span><span class="w">
</span></code></pre></div><p>更多信息请查看更详细的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">时间表概念</a>说明。</p>
<ul>
<li>Join with Temporal Table(Batch/Streaming)</li>
</ul>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">时间表</a>是跟踪随时间变化的表。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table">时间表</a>提供了对时间表在特定时间点的版本的访问。</p>
<p>只支持与处理时间的时态表进行内联和左联。</p>
<p>下面的例子假设 LatestRates 是一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table">时间表</a>，它是以最新的速率来具体化的。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w">
</span><span class="w">  </span><span class="n">o</span><span class="p">.</span><span class="n">amout</span><span class="p">,</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">currency</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">rate</span><span class="p">,</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">amount</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w">
</span><span class="w">  </span><span class="n">Orders</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">o</span><span class="w">
</span><span class="w">  </span><span class="k">JOIN</span><span class="w"> </span><span class="n">LatestRates</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">SYSTEM_TIME</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">OF</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">proctime</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">r</span><span class="w">
</span><span class="w">  </span><span class="k">ON</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">currency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">currency</span><span class="w">
</span></code></pre></div><p>更多信息请查看更详细的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">时间表</a>概念描述。</p>
<p>仅支持 Blink 计划器。</p>
<h3 id="集合运算">集合运算</h3>
<ul>
<li>Union(Batch)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="p">(</span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="k">UNION</span><span class="w">
</span><span class="w">    </span><span class="p">(</span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w">
</span></code></pre></div><ul>
<li>UnionAll(Batch/Streaming)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="p">(</span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="k">UNION</span><span class="w"> </span><span class="k">ALL</span><span class="w">
</span><span class="w">    </span><span class="p">(</span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w">
</span></code></pre></div><ul>
<li>Intersect / Except(Batch)</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="p">(</span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="k">INTERSECT</span><span class="w">
</span><span class="w">    </span><span class="p">(</span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w">
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="p">(</span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="k">EXCEPT</span><span class="w">
</span><span class="w">    </span><span class="p">(</span><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w">
</span></code></pre></div><ul>
<li>In(Batch/Streaming)</li>
</ul>
<p>如果给定表的子查询中存在表达式，则返回 true。子查询表必须由一列组成。该列必须与表达式具有相同的数据类型。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="p">,</span><span class="w"> </span><span class="n">amount</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="k">IN</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="k">SELECT</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">NewProducts</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w">
</span></code></pre></div><p>注意：对于流式查询，该操作被重写为加入和分组操作。计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供有效的保留时间间隔的查询配置，以防止状态大小过大。详情请看<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<ul>
<li>Exists(Batch/Streaming)</li>
</ul>
<p>如果子查询至少返回一条记录，则返回 true。只有当操作可以被重写成联接和分组操作时才支持。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="k">user</span><span class="p">,</span><span class="w"> </span><span class="n">amount</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">    </span><span class="k">SELECT</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">NewProducts</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w">
</span></code></pre></div><p>注意：对于流式查询，该操作被重写为加入和分组操作。计算查询结果所需的状态可能会根据不同输入行的数量而无限增长。请提供有效的保留时间间隔的查询配置，以防止状态大小过大。详情请看<a href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration">查询配置</a>。</p>
<h3 id="orderby-和-limit">OrderBy 和 Limit</h3>
<ul>
<li>Order By</li>
</ul>
<p>批量流注：流查询的结果必须主要按升序时间属性进行排序。支持其他排序属性。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">orderTime</span><span class="w">
</span></code></pre></div><ul>
<li>Limit(Batch)</li>
</ul>
<p>注意：LIMIT 子句需要一个 ORDER BY 子句。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">orderTime</span><span class="w">
</span><span class="w"></span><span class="k">LIMIT</span><span class="w"> </span><span class="mi">3</span><span class="w">
</span></code></pre></div><h3 id="top-n">Top-N</h3>
<p>注意 Top-N 只在 Blink planner 中支持。</p>
<p>Top-N 查询要求按列排序的 N 个最小或最大的值。最小值和最大值集都被认为是 Top-N 查询。当需要从批处理/流处理表中只显示 N 条最底层或最上层的记录时，Top-N 查询非常有用。这个结果集可以用于进一步分析。</p>
<p>Flink 使用 OVER 窗口子句和过滤条件的组合来表达 Top-N 查询。借助 OVER window PARTITION BY 子句的强大功能，Flink 还支持每组 Top-N。例如，每个类别中实时销售量最大的前五个产品。对于批处理表和流处理表的 SQL，都支持 Top-N 查询。</p>
<p>下面是 TOP-N 语句的语法。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="p">[</span><span class="n">column_list</span><span class="p">]</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">   </span><span class="k">SELECT</span><span class="w"> </span><span class="p">[</span><span class="n">column_list</span><span class="p">],</span><span class="w">
</span><span class="w">     </span><span class="n">ROW_NUMBER</span><span class="p">()</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="p">([</span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">col1</span><span class="p">[,</span><span class="w"> </span><span class="n">col2</span><span class="p">...]]</span><span class="w">
</span><span class="w">       </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">col1</span><span class="w"> </span><span class="p">[</span><span class="k">asc</span><span class="o">|</span><span class="k">desc</span><span class="p">][,</span><span class="w"> </span><span class="n">col2</span><span class="w"> </span><span class="p">[</span><span class="k">asc</span><span class="o">|</span><span class="k">desc</span><span class="p">]...])</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">rownum</span><span class="w">
</span><span class="w">   </span><span class="k">FROM</span><span class="w"> </span><span class="k">table_name</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w"> </span><span class="n">rownum</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="p">[</span><span class="k">AND</span><span class="w"> </span><span class="n">conditions</span><span class="p">]</span><span class="w">
</span></code></pre></div><p>参数说明:</p>
<ul>
<li>ROW_NUMBER()。根据分区内行的顺序，给每一行分配一个唯一的、连续的数字，从 1 开始。目前，我们只支持 ROW_NUMBER 作为 over window 函数。在未来，我们将支持 RANK()和 DENSE_RANK()。</li>
<li>PARTITION BY col1[，col2&hellip;]。指定分区列。每个分区将有一个 Top-N 的结果。</li>
<li>ORDER BY col1[asc|desc][，col2[asc|desc]&hellip;]：指定排序列。指定排序列。不同列的排序方向可以不同。</li>
<li>WHERE rownum &lt;= N：为了让 Flink 识别这个查询是 Top-N 查询，需要 rownum &lt;= N。N 代表将保留 N 条最小或最大的记录。</li>
<li>[AND 条件]。在 where 子句中可以自由添加其他条件，但其他条件只能与 rownum &lt;= N 使用 AND 连接组合。</li>
</ul>
<p>流模式下的注意点: TopN 查询是结果更新。Flink SQL 会根据顺序键对输入的数据流进行排序，所以如果前 N 条记录发生了变化，变化后的记录会作为回撤/更新记录发送到下游。建议使用支持更新的存储作为 Top-N 查询的汇。另外，如果 Top N 记录需要存储在外部存储中，结果表应该与 Top-N 查询的唯一键相同。</p>
<p>Top-N 查询的唯一键是分区列和 rownum 列的组合。Top-N 查询也可以得出上游的唯一键。以下面的工作为例，假设 product_id 是 ShopSales 的唯一键，那么 Top-N 查询的唯一键是[category，rownum]和[product_id]。</p>
<p>下面的例子展示了如何在流表上使用 Top-N 指定 SQL 查询。这个例子是为了得到我们上面提到的 &ldquo;每个类别实时销量最大的前五个产品&rdquo;。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">getTableEnvironment</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// read a DataStream from an external source
</span><span class="c1"></span><span class="k">val</span> <span class="n">ds</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">addSource</span><span class="o">(...)</span>
<span class="c1">// register the DataStream under the name &#34;ShopSales&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;ShopSales&#34;</span><span class="o">,</span> <span class="n">ds</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;product_id&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;category&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;product_name&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;sales&#34;</span><span class="o">)</span>


<span class="c1">// select top-5 products per category which have the maximum sales.
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
    <span class="s">&#34;&#34;&#34;
</span><span class="s">      |SELECT *
</span><span class="s">      |FROM (
</span><span class="s">      |   SELECT *,
</span><span class="s">      |       ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) as row_num
</span><span class="s">      |   FROM ShopSales)
</span><span class="s">      |WHERE row_num &lt;= 5
</span><span class="s">    &#34;&#34;&#34;</span><span class="o">.</span><span class="n">stripMargin</span><span class="o">)</span>
</code></pre></div><h4 id="无排名输出优化">无排名输出优化</h4>
<p>如上所述，rownum 字段将作为唯一键的一个字段写入结果表，这可能导致很多记录被写入结果表。例如，当排名 9 的记录（比如产品-1001）更新，其排名升级为 1 时，排名 1~9 的所有记录都会作为更新消息输出到结果表。如果结果表接收的数据过多，就会成为 SQL 作业的瓶颈。</p>
<p>优化的方式是在 Top-N 查询的外侧 SELECT 子句中省略 rownum 字段。这样做是合理的，因为 Top N 记录的数量通常不多，因此消费者可以自己快速排序。如果没有 rownum 字段，在上面的例子中，只需要将改变的记录（product-1001）发送到下游，这样可以减少很多结果表的 IO。</p>
<p>下面的例子展示了如何用这种方式优化上面的 Top-N 例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">getTableEnvironment</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// read a DataStream from an external source
</span><span class="c1"></span><span class="k">val</span> <span class="n">ds</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">addSource</span><span class="o">(...)</span>
<span class="c1">// register the DataStream under the name &#34;ShopSales&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;ShopSales&#34;</span><span class="o">,</span> <span class="n">ds</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;product_id&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;category&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;product_name&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;sales&#34;</span><span class="o">)</span>


<span class="c1">// select top-5 products per category which have the maximum sales.
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
    <span class="s">&#34;&#34;&#34;
</span><span class="s">      |SELECT product_id, category, product_name, sales  -- omit row_num field in the output
</span><span class="s">      |FROM (
</span><span class="s">      |   SELECT *,
</span><span class="s">      |       ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) as row_num
</span><span class="s">      |   FROM ShopSales)
</span><span class="s">      |WHERE row_num &lt;= 5
</span><span class="s">    &#34;&#34;&#34;</span><span class="o">.</span><span class="n">stripMargin</span><span class="o">)</span>
</code></pre></div><p>流模式下的注意点: 为了将上述查询输出到外部存储中，并得到正确的结果，外部存储必须与 Top-N 查询具有相同的唯一键，在上面的示例查询中，如果 product_id 是查询的唯一键，那么外部表也应该以 product_id 作为唯一键。在上面的示例查询中，如果 product_id 是查询的唯一键，那么外部表也应该以 product_id 作为唯一键。</p>
<h3 id="重复数据删除">重复数据删除</h3>
<p>注意 重复数据删除只在 Blink planner 中支持。</p>
<p>重复数据删除就是删除一组列上重复的行，只保留第一条或最后一条。在某些情况下，上游 ETL 作业并不是端到端完全对接的，这可能会导致在故障切换时，sink 中有重复的记录。但是，重复的记录会影响到下游分析作业（如 SUM、COUNT）的正确性。所以在进一步分析之前需要进行重复数据删除。</p>
<p>Flink 使用 ROW_NUMBER()来删除重复记录，就像 Top-N 查询的方式一样。理论上，重复数据删除是 Top-N 的一个特例，N 为 1，按处理时间或事件时间排序。</p>
<p>下面是重复数据删除语句的语法。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="p">[</span><span class="n">column_list</span><span class="p">]</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">   </span><span class="k">SELECT</span><span class="w"> </span><span class="p">[</span><span class="n">column_list</span><span class="p">],</span><span class="w">
</span><span class="w">     </span><span class="n">ROW_NUMBER</span><span class="p">()</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="p">([</span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">col1</span><span class="p">[,</span><span class="w"> </span><span class="n">col2</span><span class="p">...]]</span><span class="w">
</span><span class="w">       </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">time_attr</span><span class="w"> </span><span class="p">[</span><span class="k">asc</span><span class="o">|</span><span class="k">desc</span><span class="p">])</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">rownum</span><span class="w">
</span><span class="w">   </span><span class="k">FROM</span><span class="w"> </span><span class="k">table_name</span><span class="p">)</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w"> </span><span class="n">rownum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w">
</span></code></pre></div><p>参数说明:</p>
<ul>
<li>ROW_NUMBER()。为每一行指定一个唯一的、连续的编号，从 1 开始。</li>
<li>PARTITION BY col1[，col2&hellip;]: 指定分区列，即重复复制键。</li>
<li>ORDER BY time_attr[asc|desc]。指定排序列，必须是<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html">时间属性</a>。目前只支持 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html#processing-time">proctime 属性</a>。未来将支持 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html#event-time">Rowtime 属性</a>。用 ASC 排序表示保留第一行，用 DESC 排序表示保留最后一行。</li>
<li>WHERE rownum = 1：为了让 Flink 识别这个查询是重复数据删除，需要 rownum = 1。</li>
</ul>
<p>下面的例子展示了如何在流表上指定使用重复数据删除的 SQL 查询。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">getTableEnvironment</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// read a DataStream from an external source
</span><span class="c1"></span><span class="k">val</span> <span class="n">ds</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">addSource</span><span class="o">(...)</span>
<span class="c1">// register the DataStream under the name &#34;Orders&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">,</span> <span class="n">ds</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;order_id&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;user&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;product&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;number&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span><span class="o">.</span><span class="n">proctime</span><span class="o">)</span>

<span class="c1">// remove duplicate rows on order_id and keep the first occurrence row,
</span><span class="c1">// because there shouldn&#39;t be two orders with the same order_id.
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
    <span class="s">&#34;&#34;&#34;
</span><span class="s">      |SELECT order_id, user, product, number
</span><span class="s">      |FROM (
</span><span class="s">      |   SELECT *,
</span><span class="s">      |       ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY proctime DESC) as row_num
</span><span class="s">      |   FROM Orders)
</span><span class="s">      |WHERE row_num = 1
</span><span class="s">    &#34;&#34;&#34;</span><span class="o">.</span><span class="n">stripMargin</span><span class="o">)</span>
</code></pre></div><h3 id="group-windows">Group Windows</h3>
<p>组窗口是在 SQL 查询的 GROUP BY 子句中定义的。就像使用常规的 GROUP BY 子句的查询一样，使用包含组窗口函数的 GROUP BY 子句的查询是为每个组计算一条结果行。在批处理表和流式表上的 SQL 支持以下组窗口函数。</p>
<table>
<thead>
<tr>
<th style="text-align:left">分组窗口函数</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">TUMBLE(time_attr, interval)</td>
<td style="text-align:left">定义一个滚动时间窗口。滚动时间窗口将行分配到具有固定持续时间（间隔）的非重叠的连续窗口。例如，一个 5 分钟的时间窗口可以将行以 5 分钟的间隔进行分组。滚动窗口可以在事件时间（流+批次）或处理时间（流）上定义。</td>
</tr>
<tr>
<td style="text-align:left">HOP(time_attr, interval, interval)</td>
<td style="text-align:left">定义一个跳转时间窗口（在表 API 中称为滑动窗口）。跳跃时间窗口有一个固定的持续时间（第二个间隔参数），并按指定的跳跃间隔（第一个间隔参数）进行跳转。如果跳转间隔小于窗口大小，则跳转窗口是重叠的。因此，可以将行分配到多个窗口。例如，15 分钟大小的跳转窗口和 5 分钟的跳转间隔将每行分配给 3 个 15 分钟大小的不同窗口，这些窗口以 5 分钟的间隔进行评估。滚动窗口可以在事件时间（流+批处理）或处理时间（流）上定义。</td>
</tr>
<tr>
<td style="text-align:left">SESSION(time_attr, interval)</td>
<td style="text-align:left">定义一个会话时间窗口。会话时间窗口没有固定的持续时间，但其边界由不活动的时间间隔定义，即如果在定义的间隙期内没有事件出现，则会话窗口关闭。例如，有 30 分钟间隙的会话窗口在 30 分钟不活动后观察到一行时开始（否则该行将被添加到现有的窗口中），如果在 30 分钟内没有行被添加，则关闭。会话窗口可以在事件时间（流+批处理）或处理时间（流）上工作。</td>
</tr>
</tbody>
</table>
<h4 id="时间属性">时间属性</h4>
<p>对于流表的 SQL 查询，组窗口函数的 time_attr 参数必须引用一个有效的时间属性，该属性指定行的处理时间或事件时间。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html">时间属性的文档</a>，了解如何定义时间属性。</p>
<p>对于批处理表上的 SQL，组窗口函数的 time_attr 参数必须是类型为 TIMESTAMP 的属性。</p>
<h4 id="选择组窗口的开始和结束时间戳">选择组窗口的开始和结束时间戳</h4>
<p>可以通过以下辅助功能选择组窗口的开始和结束时间戳以及时间属性。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Auxiliary 函数</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">TUMBLE_START(time_attr, interval),HOP_START(time_attr, interval, interval),SESSION_START(time_attr, interval)</td>
<td style="text-align:left">返回对应的滚动、跳跃或会话窗口的包容下界的时间戳。</td>
</tr>
<tr>
<td style="text-align:left">TUMBLE_END(time_attr, interval),HOP_END(time_attr, interval, interval),SESSION_END(time_attr, interval)</td>
<td style="text-align:left">返回对应的翻滚、跳跃或会话窗口的专属上界的时间戳。注意：在后续的基于时间的操作中，如<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#joins">区间连接</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#aggregations">分组窗口或 over 窗口聚合</a>中，不能将专属上界时间戳作为<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html">行时间属性</a>使用。</td>
</tr>
<tr>
<td style="text-align:left">TUMBLE_ROWTIME(time_attr, interval),HOP_ROWTIME(time_attr, interval, interval),SESSION_ROWTIME(time_attr, interval)</td>
<td style="text-align:left">返回对应的翻滚、跳跃或会话窗口的包容上界的时间戳。产生的属性是一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html">行时间属性</a>，可以用于后续的基于时间的操作，如<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#joins">区间连接</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#aggregations">分组窗口或窗口聚合</a>。</td>
</tr>
<tr>
<td style="text-align:left">TUMBLE_PROCTIME(time_attr, interval),HOP_PROCTIME(time_attr, interval, interval),SESSION_PROCTIME(time_attr, interval)</td>
<td style="text-align:left">返回一个 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html#processing-time">proctime 属性</a>，该属性可用于后续基于时间的操作，如<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#joins">区间连接</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#aggregations">分组窗口或过窗口聚合</a>。</td>
</tr>
</tbody>
</table>
<p>注意：在调用辅助函数时，必须使用与 GROUP BY 子句中的组窗口函数完全相同的参数。</p>
<p>下面的例子展示了如何在流式表上使用组窗口指定 SQL 查询。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// read a DataStream from an external source
</span><span class="c1"></span><span class="k">val</span> <span class="n">ds</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">addSource</span><span class="o">(...)</span>
<span class="c1">// register the DataStream under the name &#34;Orders&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">,</span> <span class="n">ds</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;user&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;product&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;amount&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;proctime&#34;</span><span class="o">.</span><span class="n">proctime</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;rowtime&#34;</span><span class="o">.</span><span class="n">rowtime</span><span class="o">)</span>

<span class="c1">// compute SUM(amount) per day (in event-time)
</span><span class="c1"></span><span class="k">val</span> <span class="n">result1</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
    <span class="s">&#34;&#34;&#34;
</span><span class="s">      |SELECT
</span><span class="s">      |  user,
</span><span class="s">      |  TUMBLE_START(rowtime, INTERVAL &#39;1&#39; DAY) as wStart,
</span><span class="s">      |  SUM(amount)
</span><span class="s">      | FROM Orders
</span><span class="s">      | GROUP BY TUMBLE(rowtime, INTERVAL &#39;1&#39; DAY), user
</span><span class="s">    &#34;&#34;&#34;</span><span class="o">.</span><span class="n">stripMargin</span><span class="o">)</span>

<span class="c1">// compute SUM(amount) per day (in processing-time)
</span><span class="c1"></span><span class="k">val</span> <span class="n">result2</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
  <span class="s">&#34;SELECT user, SUM(amount) FROM Orders GROUP BY TUMBLE(proctime, INTERVAL &#39;1&#39; DAY), user&#34;</span><span class="o">)</span>

<span class="c1">// compute every hour the SUM(amount) of the last 24 hours in event-time
</span><span class="c1"></span><span class="k">val</span> <span class="n">result3</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
  <span class="s">&#34;SELECT product, SUM(amount) FROM Orders GROUP BY HOP(rowtime, INTERVAL &#39;1&#39; HOUR, INTERVAL &#39;1&#39; DAY), product&#34;</span><span class="o">)</span>

<span class="c1">// compute SUM(amount) per session with 12 hour inactivity gap (in event-time)
</span><span class="c1"></span><span class="k">val</span> <span class="n">result4</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
    <span class="s">&#34;&#34;&#34;
</span><span class="s">      |SELECT
</span><span class="s">      |  user,
</span><span class="s">      |  SESSION_START(rowtime, INTERVAL &#39;12&#39; HOUR) AS sStart,
</span><span class="s">      |  SESSION_END(rowtime, INTERVAL &#39;12&#39; HOUR) AS sEnd,
</span><span class="s">      |  SUM(amount)
</span><span class="s">      | FROM Orders
</span><span class="s">      | GROUP BY SESSION(rowtime(), INTERVAL &#39;12&#39; HOUR), user
</span><span class="s">    &#34;&#34;&#34;</span><span class="o">.</span><span class="n">stripMargin</span><span class="o">)</span>
</code></pre></div><h3 id="模式识别">模式识别</h3>
<ul>
<li>MATCH_RECOGNIZE(Streaming)</li>
</ul>
<p>根据 MATCH_RECOGNIZE <a href="https://standards.iso.org/ittf/PubliclyAvailableStandards/c065143_ISO_IEC_TR_19075-5_2016.zip">ISO 标准</a>在流表中搜索给定模式。这使得在 SQL 查询中表达复杂事件处理（CEP）逻辑成为可能。</p>
<p>更详细的描述，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/match_recognize.html">检测表中模式</a>的专门页面。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">T</span><span class="p">.</span><span class="n">aid</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="p">.</span><span class="n">bid</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="p">.</span><span class="n">cid</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">MyTable</span><span class="w">
</span><span class="w"></span><span class="n">MATCH_RECOGNIZE</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">userid</span><span class="w">
</span><span class="w">  </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">proctime</span><span class="w">
</span><span class="w">  </span><span class="n">MEASURES</span><span class="w">
</span><span class="w">    </span><span class="n">A</span><span class="p">.</span><span class="n">id</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">aid</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">B</span><span class="p">.</span><span class="n">id</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">bid</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="k">C</span><span class="p">.</span><span class="n">id</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">cid</span><span class="w">
</span><span class="w">  </span><span class="n">PATTERN</span><span class="w"> </span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="k">C</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="n">DEFINE</span><span class="w">
</span><span class="w">    </span><span class="n">A</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="n">B</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">    </span><span class="k">C</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;c&#39;</span><span class="w">
</span><span class="w"></span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">T</span><span class="w">
</span></code></pre></div>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[查询配置]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-query-configuration/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-query-configuration/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Query Configuration</blockquote><h1 id="查询配置">查询配置</h1>
<p>表 API 和 SQL 查询具有相同的语义，无论其输入是有限的行集还是无限制的表变化流。在许多情况下，对流输入的连续查询能够计算出与离线计算结果相同的准确结果。然而，对于一些连续查询，你必须限制它们所维持的状态的大小，以避免在摄取无约束的输入流时耗尽存储。这取决于输入数据的特性和查询本身是否需要限制状态大小，以及它是否和如何影响计算结果的准确性。</p>
<p>Flink 的 Table API 和 SQL 接口提供了参数来调整连续查询的准确性和资源消耗。这些参数是通过 TableConfig 对象指定的，可以从 TableEnvironment 中获得。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// obtain query configuration from TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tConfig</span><span class="k">:</span> <span class="kt">TableConfig</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">getConfig</span>
<span class="c1">// set query parameters
</span><span class="c1"></span><span class="n">tConfig</span><span class="o">.</span><span class="n">setIdleStateRetentionTime</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(</span><span class="mi">12</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(</span><span class="mi">24</span><span class="o">))</span>

<span class="c1">// define query
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="o">???</span>

<span class="c1">// create TableSink
</span><span class="c1"></span><span class="k">val</span> <span class="n">sink</span><span class="k">:</span> <span class="kt">TableSink</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">=</span> <span class="o">???</span>

<span class="c1">// register TableSink
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">registerTableSink</span><span class="o">(</span>
  <span class="s">&#34;outputTable&#34;</span><span class="o">,</span>                  <span class="c1">// table name
</span><span class="c1"></span>  <span class="nc">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">](...),</span>             <span class="c1">// field names
</span><span class="c1"></span>  <span class="nc">Array</span><span class="o">[</span><span class="kt">TypeInformation</span><span class="o">[</span><span class="k">_</span><span class="o">]](...),</span> <span class="c1">// field types
</span><span class="c1"></span>  <span class="n">sink</span><span class="o">)</span>                           <span class="c1">// table sink
</span><span class="c1"></span>
<span class="c1">// emit result Table via a TableSink
</span><span class="c1"></span><span class="n">result</span><span class="o">.</span><span class="n">executeInsert</span><span class="o">(</span><span class="s">&#34;outputTable&#34;</span><span class="o">)</span>

<span class="c1">// convert result Table into a DataStream[Row]
</span><span class="c1"></span><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">=</span> <span class="n">result</span><span class="o">.</span><span class="n">toAppendStream</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span>
</code></pre></div><p>下面我们介绍 TableConfig 的参数，以及它们如何影响查询的准确性和资源消耗。</p>
<h2 id="闲置状态保留时间">闲置状态保留时间</h2>
<p>许多查询在一个或多个键属性上聚合或连接记录。当这样的查询在一个流上执行时，连续查询需要收集记录或维护每个键的部分结果。如果输入流的键域是不断变化的，即活跃的键值是随着时间的推移而变化的，那么随着观察到越来越多不同的键，连续查询会积累越来越多的状态。然而，往往键在一段时间后就会变得不活跃，其相应的状态也就变得陈旧无用。</p>
<p>例如下面的查询计算每节课的点击次数。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">sessionId</span><span class="p">,</span><span class="w"> </span><span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">clicks</span><span class="w"> </span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">sessionId</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>sessionId 属性被用作分组键，连续查询会对它观察到的每个 sessionId 进行计数。sessionId 属性是随着时间的推移而不断变化的，sessionId 值只有在会话结束之前才是有效的，即在有限的时间内。然而，连续查询无法知道 sessionId 的这一属性，它期望每个 sessionId 值都能在任何时间点出现。它为每一个观察到的 sessionId 值维持一个计数。因此，随着观察到的 sessionId 值越来越多，查询的总状态大小也在不断增加。</p>
<p>闲置状态保留时间参数定义了一个键的状态在被移除之前不被更新的保留时间。对于前面的示例查询，只要在配置的时间段内没有更新，sessionId 的计数就会被删除。</p>
<p>通过删除一个键的状态，连续查询就会完全忘记它以前见过这个键。如果处理一条带有键的记录，其状态在之前已经被删除，则该记录将被视为带有相应键的第一条记录。对于上面的例子来说，这意味着一个 sessionId 的计数将重新开始为 0。</p>
<p>有两个参数可以配置空闲状态保留时间。</p>
<ul>
<li>最小空闲状态保留时间定义了一个非活动键的状态在被移除之前至少保留多长时间。</li>
<li>最大空闲状态保留时间定义了非活动键的状态在被删除前最多保留多长时间。</li>
</ul>
<p>参数指定如下:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">tConfig</span><span class="k">:</span> <span class="kt">TableConfig</span> <span class="o">=</span> <span class="o">???</span>

<span class="c1">// set idle state retention time: min = 12 hours, max = 24 hours
</span><span class="c1"></span><span class="n">tConfig</span><span class="o">.</span><span class="n">setIdleStateRetentionTime</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(</span><span class="mi">12</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(</span><span class="mi">24</span><span class="o">))</span>
</code></pre></div><p>清理状态需要额外的记账，对于 minTime 和 maxTime 的较大差异，记账成本较低。minTime 和 maxTime 之间的差异必须至少为 5 分钟。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/query_configuration.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/query_configuration.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[检查点]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-checkpointing/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-checkpointing/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Checkpointing</blockquote><p>Flink 中的每一个函数和操作符都可以是有状态的（详情请看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">使用状态</a>）。有状态的函数在单个元素/事件的处理过程中存储数据，使得状态成为任何类型的更复杂操作的关键构建模块。</p>
<p>为了使状态具有容错性，Flink 需要对状态进行 <strong>checkpoint</strong>。检查点允许 Flink 恢复流中的状态和位置，使应用程序具有与无故障执行相同的语义。</p>
<p>关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/learn-flink/fault_tolerance.html">流式容错的文档</a>详细描述了 Flink 的流式容错机制背后的技术。</p>
<h2 id="前提条件">前提条件</h2>
<p>Flink 的检查点机制与流和状态的持久存储交互。一般来说，它需要:</p>
<ul>
<li>一个能在一定时间内重放记录(replay records)的持久（或耐用）数据源。这种源的例子是持久性消息队列（如 Apache Kafka、RabbitMQ、Amazon Kinesis、Google PubSub）或文件系统（如 HDFS、S3、GFS、NFS、Ceph&hellip;）。</li>
<li>状态的持久性存储，通常是一个分布式文件系统（如 HDFS、S3、GFS、NFS、Ceph&hellip;）。</li>
</ul>
<h2 id="启用和配置检查点">启用和配置检查点</h2>
<p>默认情况下，检查点被禁用。要启用检查点，在 <code>StreamExecutionEnvironment</code> 上调用 <code>enableCheckpointing(n)</code>，其中 <em>n</em> 是检查点间隔，单位为毫秒。</p>
<p>检查点的其他参数包括:</p>
<ul>
<li>
<p>exactly-once vs. at-least-once：你可以选择向 <code>enableCheckpointing(n)</code> 方法传递一个模式，以便在两个保证级别之间进行选择。对于大多数应用来说，exactly-once 是比较好的。At-least-once 可能适用于某些超低延迟（持续几毫秒）的应用。</p>
</li>
<li>
<p>检查点超时。如果一个正在进行中的检查点没有完成，那么它被中止的时间。</p>
</li>
<li>
<p>检查点之间的最小时间。为了确保流应用在检查点之间有一定的进度，可以定义检查点之间需要经过多少时间。例如，如果这个值设置为5000，那么下一个检查点将在上一个检查点完成后不早于5秒开始，无论检查点持续时间和检查点间隔如何。请注意，这意味着检查点间隔永远不会小于这个参数。</p>
</li>
</ul>
<p>通过定义&quot;检查点之间的时间&quot;(time between checkpoints)通常比检查点间隔更容易配置应用程序，因为&quot;检查点之间的时间&quot;不容易受到检查点有时可能比平均时间长的事实的影响（例如，如果目标存储系统暂时缓慢）。</p>
<p>请注意，这个值也意味着并发检查点的数量为1。</p>
<ul>
<li>并发检查点的数量。默认情况下，当一个检查点仍在进行时，系统不会触发另一个检查点。这可以确保拓扑不会在检查点上花费太多时间，而使处理流的工作没有进展。可以允许多个重叠的检查点，这对于那些有一定处理延迟（例如因为函数调用外部服务，需要一些时间来响应），但仍然希望做非常频繁的检查点（100s毫秒），以便在故障时重新处理很少的管道来说是很有意思的。</li>
</ul>
<p>当定义了检查点之间的最小时间时，不能使用这个选项。</p>
<ul>
<li>
<p>外部化检查点。您可以配置周期性检查点，使其在外部持久化。外部化的检查点会将它们的元数据写入持久化存储中，当作业失败时不会自动清理。这样一来，如果你的工作失败了，你身边就会有一个检查点来恢复。关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/state/checkpoints.html#externalized-checkpoints">外部化检查点的部署说明</a>中有更多细节。</p>
</li>
<li>
<p>Fail/checkpoint 错误时继续执行任务。这决定了如果在执行任务的检查点过程中出现错误，任务是否会失败。这是默认行为。另外，当禁用该功能时，任务将简单地拒绝向检查点协调器提供检查点并继续运行。</p>
</li>
<li>
<p>更喜欢用于恢复的检查点。这决定了即使有更近的保存点可用时，任务是否会回退到最新的检查点，以减少恢复时间。</p>
</li>
<li>
<p>不对齐的检查点。你可以启用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/state/checkpoints.html#unaligned-checkpoints">不对齐的检查点</a>，以大大减少背压下的检查点时间。仅适用于精确的一次检查点，且并发检查点数量为1。</p>
</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>

<span class="c1">// start a checkpoint every 1000 ms
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">enableCheckpointing</span><span class="o">(</span><span class="mi">1000</span><span class="o">)</span>

<span class="c1">// 高级选项:
</span><span class="c1"></span>
<span class="c1">// 设置模式为 exactly-once (这是默认的)
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">setCheckpointingMode</span><span class="o">(</span><span class="nc">CheckpointingMode</span><span class="o">.</span><span class="nc">EXACTLY_ONCE</span><span class="o">)</span>

<span class="c1">// make sure 500 ms of progress happen between checkpoints
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">setMinPauseBetweenCheckpoints</span><span class="o">(</span><span class="mi">500</span><span class="o">)</span>

<span class="c1">// checkpoints have to complete within one minute, or are discarded
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">setCheckpointTimeout</span><span class="o">(</span><span class="mi">60000</span><span class="o">)</span>

<span class="c1">// prevent the tasks from failing if an error happens in their checkpointing, the checkpoint will just be declined.
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">setFailTasksOnCheckpointingErrors</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="c1">// allow only one checkpoint to be in progress at the same time
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">setMaxConcurrentCheckpoints</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

<span class="c1">// enables the experimental unaligned checkpoints
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getCheckpointConfig</span><span class="o">.</span><span class="n">enableUnalignedCheckpoints</span><span class="o">()</span>
</code></pre></div><h3 id="相关配置选项">相关配置选项</h3>
<p>更多的参数和/或默认值可以通过 <code>conf/flink-conf.yaml</code> 来设置（参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html">配置</a>的完整指南）。</p>
<table>
<thead>
<tr>
<th style="text-align:left">键</th>
<th style="text-align:left">默认值</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">state.backend</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">String</td>
<td style="text-align:left">用于存储和 checkpoint 状态的状态后端。</td>
</tr>
<tr>
<td style="text-align:left">state.backend.async</td>
<td style="text-align:left">true</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">状态后端是否应该在可能的情况下使用异步快照方法的选项，可配置。有些状态后端可能不支持异步快照，或者只支持异步快照，而忽略这个选项。</td>
</tr>
<tr>
<td style="text-align:left">state.backend.fs.memory-threshold</td>
<td style="text-align:left">20 kb</td>
<td style="text-align:left">MemorySize</td>
<td style="text-align:left">状态数据文件的最小尺寸。小于这个大小的所有状态块都内嵌存储在根检查点元数据文件中。该配置的最大内存阈值为1MB。</td>
</tr>
<tr>
<td style="text-align:left">state.backend.fs.write-buffer-size</td>
<td style="text-align:left">4096</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">写入文件系统的检查点流的默认写缓冲区大小。实际的写缓冲区大小是由这个选项和选项 &lsquo;state.backend.fs.memory-threshold&rsquo; 的最大值决定的。</td>
</tr>
<tr>
<td style="text-align:left">state.backend.incremental</td>
<td style="text-align:left">false</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">如果可能，状态后端是否应该创建增量检查点。对于增量检查点，只存储与前一个检查点的差异，而不是完整的检查点状态。一旦启用，在 Web UI 中显示的状态大小或从 rest API 中获取的状态大小只代表 delta 检查点大小，而不是完整的检查点大小。一些状态后端可能不支持增量检查点而忽略这个选项。</td>
</tr>
<tr>
<td style="text-align:left">state.backend.local-recovery</td>
<td style="text-align:left">false</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left">这个选项可以配置这个状态后端的本地恢复。默认情况下，本地恢复是被停用的。本地恢复目前只覆盖 keyed state 后端。目前，MemoryStateBackend 不支持本地恢复，忽略此选项。</td>
</tr>
<tr>
<td style="text-align:left">state.checkpoints.dir</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">String</td>
<td style="text-align:left">在 Flink 支持的文件系统中，用于存储检查点数据文件和元数据的默认目录。该存储路径必须可以从所有参与进程/节点（即所有 TaskManager 和 JobManager）访问。</td>
</tr>
<tr>
<td style="text-align:left">state.checkpoints.num-retained</td>
<td style="text-align:left">1</td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">保留已完成的检查点的最大数量。</td>
</tr>
<tr>
<td style="text-align:left">state.savepoints.dir</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">String</td>
<td style="text-align:left">保存点的默认目录。由将保存点写入文件系统的状态后端（MemoryStateBackend, FsStateBackend, RocksDBStateBackend）使用。</td>
</tr>
<tr>
<td style="text-align:left">taskmanager.state.local.root-dirs</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">String</td>
<td style="text-align:left">配置参数，定义本地恢复中存储基于文件的状态的根目录。本地恢复目前只覆盖 keyed state 后端。目前，MemoryStateBackend 不支持本地恢复，忽略这个选项。</td>
</tr>
</tbody>
</table>
<h3 id="选择状态后端">选择状态后端</h3>
<p>Flink 的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/learn-flink/fault_tolerance.html">检查点机制</a>在定时器和有状态的操作符中存储所有状态的一致快照，包括连接器、窗口和任何<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">用户定义的状态</a>。检查点存储的位置（例如，JobManager内存、文件系统、数据库）取决于配置的状态后端。</p>
<p>默认情况下，状态保存在 TaskManager 的内存中，检查点保存在 JobManager 的内存中。为了正确地持久化大状态，Flink 支持各种方法在其他状态后端存储和检查点状态。状态后端的选择可以通过 <code>StreamExecutionEnvironment.setStateBackend(...)</code> 进行配置。</p>
<p>有关可用的状态后端以及作业范围(job-wide)和集群范围(cluster-wide)配置选项的更多细节，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/state/state_backends.html">状态后端</a>。</p>
<h3 id="迭代作业中的状态检查点">迭代作业中的状态检查点</h3>
<p>Flink 目前只为没有迭代的作业提供处理保证。在迭代作业上启用检查点会导致异常。为了在迭代程序上强制检查点，用户需要在启用检查点时设置一个特殊标志：<code>env.enableCheckpointing(interval, CheckpointingMode.EXACTLY_ONCE, force = true)</code>。</p>
<p>请注意，循环边缘中飞行中的记录（以及与之相关的状态变化）将在失败时丢失。</p>
<h3 id="重新启动策略">重新启动策略</h3>
<p>Flink 支持不同的重启策略，这些策略可以控制作业(job)在发生故障时如何重启。更多信息，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/task_failure_recovery.html">重启策略</a>。</p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[检测表中的模式]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-detecting-patterns-in-tables/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-detecting-patterns-in-tables/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Detecting Patterns in Tables</blockquote><p>检测表格中的模式
搜索一组事件模式是一个常见的用例，特别是在数据流的情况下。Flink 自带复杂事件处理（CEP）库，可以在事件流中进行模式检测。此外，Flink 的 SQL API 提供了一种关系型的查询表达方式，有大量的内置函数和基于规则的优化，可以开箱即用。</p>
<p>2016 年 12 月，国际标准化组织（ISO）发布了新版本的 SQL 标准，其中包括 SQL 中的行模式识别（ISO/IEC TR 19075-5:2016）。它允许 Flink 使用 MATCH_RECOGNIZE 子句整合 CEP 和 SQL API，用于 SQL 中的复杂事件处理。</p>
<p>MATCH_RECOGNIZE 子句可以实现以下任务。</p>
<p>对使用 partition by 和 order by 子句的数据进行逻辑分区和排序。
使用 PATTERN 子句定义要寻找的行的模式。这些模式使用类似于正则表达式的语法。
行模式变量的逻辑成分在 DEFINE 子句中指定。
在 MEASURES 子句中定义措施，这些措施是在 SQL 查询的其他部分中可用的表达式。
下面的例子说明了基本模式识别的语法。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="n">T</span><span class="p">.</span><span class="n">aid</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="p">.</span><span class="n">bid</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="p">.</span><span class="n">cid</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">MyTable</span><span class="w">
</span><span class="w">    </span><span class="n">MATCH_RECOGNIZE</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">      </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">userid</span><span class="w">
</span><span class="w">      </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">proctime</span><span class="w">
</span><span class="w">      </span><span class="n">MEASURES</span><span class="w">
</span><span class="w">        </span><span class="n">A</span><span class="p">.</span><span class="n">id</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">aid</span><span class="p">,</span><span class="w">
</span><span class="w">        </span><span class="n">B</span><span class="p">.</span><span class="n">id</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">bid</span><span class="p">,</span><span class="w">
</span><span class="w">        </span><span class="k">C</span><span class="p">.</span><span class="n">id</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">cid</span><span class="w">
</span><span class="w">      </span><span class="n">PATTERN</span><span class="w"> </span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="k">C</span><span class="p">)</span><span class="w">
</span><span class="w">      </span><span class="n">DEFINE</span><span class="w">
</span><span class="w">        </span><span class="n">A</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">        </span><span class="n">B</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="w">
</span><span class="w">        </span><span class="k">C</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;c&#39;</span><span class="w">
</span><span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">T</span><span class="w">
</span></code></pre></div><p>本页将更详细地解释每个关键字，并将说明更复杂的例子。</p>
<p>注意 Flink 对 MATCH_RECOGNIZE 子句的实现是完整标准的一个子集。只有那些在下面的章节中记录的功能得到了支持。根据社区反馈，可能会支持更多的功能，也请看一下已知的限制。</p>
<p>介绍和示例
安装指南
模式识别功能内部使用了 Apache Flink 的 CEP 库。为了能够使用 MATCH_RECOGNIZE 子句，需要将该库作为一个依赖项添加到你的 Maven 项目中。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-cep_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>另外，你也可以将依赖关系添加到集群 classpath 中（更多信息请参见依赖关系部分）。</p>
<p>如果你想在 SQL 客户端中使用 MATCH_RECOGNIZE 子句，你不需要做任何事情，因为所有的依赖关系都是默认的。</p>
<p>SQL 语义
每个 MATCH_RECOGNIZE 查询都由以下子句组成。</p>
<p>PARTITION BY - 定义表的逻辑分区；类似于 GROUP BY 操作。</p>
<p>MEASURES - 定义子句的输出；类似于 SELECT 子句。
ONE ROW PER MATCH - 输出模式，定义每次匹配应该产生多少行。
AFTER MATCH SKIP&ndash;指定下一个匹配应该从哪里开始；这也是控制一个事件可以属于多少个不同匹配的方法。
PATTERN - 允许使用类似于正则表达式的语法来构建搜索的模式。
DEFINE - 这一部分定义了模式变量必须满足的条件。
注意 目前，MATCH_RECOGNIZE 子句只能应用于追加表。此外，它也总是产生一个追加表。</p>
<p>例子
在我们的例子中，我们假设已经注册了一个 Ticker 表。该表包含股票在某一特定时间点的价格。</p>
<p>该表的模式如下：</p>
<pre><code>Ticker
     |-- symbol: String                           # symbol of the stock
     |-- price: Long                              # price of the stock
     |-- tax: Long                                # tax liability of the stock
     |-- rowtime: TimeIndicatorTypeInfo(rowtime)  # point in time when the change to those values happened
</code></pre><p>为了简化，我们只考虑单只股票 ACME 的传入数据。一个行情可以类似于下表，其中行是连续追加的。</p>
<pre><code>symbol         rowtime         price    tax
======  ====================  ======= =======
'ACME'  '01-Apr-11 10:00:00'   12      1
'ACME'  '01-Apr-11 10:00:01'   17      2
'ACME'  '01-Apr-11 10:00:02'   19      1
'ACME'  '01-Apr-11 10:00:03'   21      3
'ACME'  '01-Apr-11 10:00:04'   25      2
'ACME'  '01-Apr-11 10:00:05'   18      1
'ACME'  '01-Apr-11 10:00:06'   15      1
'ACME'  '01-Apr-11 10:00:07'   14      2
'ACME'  '01-Apr-11 10:00:08'   24      2
'ACME'  '01-Apr-11 10:00:09'   25      2
'ACME'  '01-Apr-11 10:00:10'   19      1
</code></pre><p>现在的任务是寻找单一行情的价格不断下降的时期。为此，可以写一个类似的查询。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Ticker</span><span class="w">
</span><span class="w">    </span><span class="n">MATCH_RECOGNIZE</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">        </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">symbol</span><span class="w">
</span><span class="w">        </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">rowtime</span><span class="w">
</span><span class="w">        </span><span class="n">MEASURES</span><span class="w">
</span><span class="w">            </span><span class="n">START_ROW</span><span class="p">.</span><span class="n">rowtime</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">start_tstamp</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="k">LAST</span><span class="p">(</span><span class="n">PRICE_DOWN</span><span class="p">.</span><span class="n">rowtime</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">bottom_tstamp</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="k">LAST</span><span class="p">(</span><span class="n">PRICE_UP</span><span class="p">.</span><span class="n">rowtime</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">end_tstamp</span><span class="w">
</span><span class="w">        </span><span class="n">ONE</span><span class="w"> </span><span class="k">ROW</span><span class="w"> </span><span class="n">PER</span><span class="w"> </span><span class="k">MATCH</span><span class="w">
</span><span class="w">        </span><span class="k">AFTER</span><span class="w"> </span><span class="k">MATCH</span><span class="w"> </span><span class="n">SKIP</span><span class="w"> </span><span class="k">TO</span><span class="w"> </span><span class="k">LAST</span><span class="w"> </span><span class="n">PRICE_UP</span><span class="w">
</span><span class="w">        </span><span class="n">PATTERN</span><span class="w"> </span><span class="p">(</span><span class="n">START_ROW</span><span class="w"> </span><span class="n">PRICE_DOWN</span><span class="o">+</span><span class="w"> </span><span class="n">PRICE_UP</span><span class="p">)</span><span class="w">
</span><span class="w">        </span><span class="n">DEFINE</span><span class="w">
</span><span class="w">            </span><span class="n">PRICE_DOWN</span><span class="w"> </span><span class="k">AS</span><span class="w">
</span><span class="w">                </span><span class="p">(</span><span class="k">LAST</span><span class="p">(</span><span class="n">PRICE_DOWN</span><span class="p">.</span><span class="n">price</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">IS</span><span class="w"> </span><span class="k">NULL</span><span class="w"> </span><span class="k">AND</span><span class="w"> </span><span class="n">PRICE_DOWN</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">START_ROW</span><span class="p">.</span><span class="n">price</span><span class="p">)</span><span class="w"> </span><span class="k">OR</span><span class="w">
</span><span class="w">                    </span><span class="n">PRICE_DOWN</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="k">LAST</span><span class="p">(</span><span class="n">PRICE_DOWN</span><span class="p">.</span><span class="n">price</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">),</span><span class="w">
</span><span class="w">            </span><span class="n">PRICE_UP</span><span class="w"> </span><span class="k">AS</span><span class="w">
</span><span class="w">                </span><span class="n">PRICE_UP</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="k">LAST</span><span class="p">(</span><span class="n">PRICE_DOWN</span><span class="p">.</span><span class="n">price</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="n">MR</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>该查询按符号列对 Ticker 表进行分区，并按行时间属性进行排序。</p>
<p>PATTERN 子句指定我们感兴趣的模式是以 START_ROW 事件为起点，然后是一个或多个 PRICE_DOWN 事件，最后是 PRICE_UP 事件。如果能找到这样的模式，下一个模式匹配将在最后一个 PRICE_UP 事件中寻找，如 AFTER MATCH SKIP TO LAST 子句所示。</p>
<p>DEFINE 子句指定了 PRICE_DOWN 和 PRICE_UP 事件需要满足的条件。虽然 START_ROW 模式变量并不存在，但它有一个隐含的条件，这个条件总是被评估为 TRUE。</p>
<p>模式变量 PRICE_DOWN 被定义为价格小于满足 PRICE_DOWN 条件的最后一行的价格。对于初始情况或者没有满足 PRICE_DOWN 条件的最后一行，这一行的价格应该小于模式中前一行的价格（由 START_ROW 引用）。</p>
<p>模式变量 PRICE_UP 被定义为价格大于满足 PRICE_DOWN 条件的最后一行的价格的行。</p>
<p>该查询为股票价格连续下跌的每个时期产生一条汇总行。</p>
<p>输出行的具体表示方法在查询的 MEASURES 部分定义。输出行的数量由 ONE ROW PER MATCH 输出模式定义。</p>
<pre><code> symbol       start_tstamp       bottom_tstamp         end_tstamp
=========  ==================  ==================  ==================
ACME       01-APR-11 10:00:04  01-APR-11 10:00:07  01-APR-11 10:00:08
</code></pre><p>结果一行描述了从 01-APR-11 10:00:04 开始的价格下降期，在 01-APR-11 10:00:07 达到最低价，在 01-APR-11 10:00:08 再次上涨。</p>
<p>分割
可以在分区数据中寻找模式，例如，单个股票或特定用户的趋势。这可以使用 partition by 子句来表达。该子句类似于使用 GROUP BY 进行聚合。</p>
<p>注意 强烈建议对输入的数据进行分区，否则 MATCH_RECOGNIZE 子句将被翻译成一个非平行操作符，以确保全局排序。</p>
<p>事件的顺序
Apache Flink 允许根据时间来搜索模式；无论是处理时间还是事件时间。</p>
<p>在事件时间的情况下，事件在被传递到内部模式状态机之前会被排序。因此，产生的输出将是正确的，不管行被附加到表中的顺序如何。相反，模式是按照每行包含的时间所指定的顺序来评估的。</p>
<p>MATCH_RECOGNIZE 子句假设时间属性以升序作为 ORDER BY 子句的第一个参数。</p>
<p>对于 Ticker 表的例子，像 ORDER BY rowtime ASC, price DESC 这样的定义是有效的，但是 ORDER BY price, rowtime 或者 ORDER BY rowtime DESC, price ASC 是无效的。</p>
<p>定义和测量
DEFINE 和 MEASURES 关键字的含义类似于简单 SQL 查询中的 WHERE 和 SELECT 子句。</p>
<p>MEASURES 子句定义了匹配模式的输出中会包含哪些内容。它可以投射列和定义评估的表达式。产生的行数取决于输出模式的设置。</p>
<p>DEFINE 子句指定了行必须满足的条件，以便将其分类到相应的模式变量。如果没有为模式变量定义条件，那么将使用一个默认条件，该条件对每条记录的评价为真。</p>
<p>关于这些子句中可以使用的表达式的更详细解释，请看事件流导航部分。</p>
<p>聚合
聚合可以在 DEFINE 和 MEASURES 子句中使用。同时支持内置和自定义的用户定义函数。</p>
<p>聚合函数被应用于映射到匹配的行的每个子集。为了了解这些子集是如何被评估的，请看一下事件流导航部分。</p>
<p>下面这个例子的任务是找到一个股票平均价格不低于某个阈值的最长时间段。它显示了 MATCH_RECOGNIZE 可以如何通过聚合来表达。这个任务可以用下面的查询来执行。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Ticker</span><span class="w">
</span><span class="w">    </span><span class="n">MATCH_RECOGNIZE</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">        </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">symbol</span><span class="w">
</span><span class="w">        </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">rowtime</span><span class="w">
</span><span class="w">        </span><span class="n">MEASURES</span><span class="w">
</span><span class="w">            </span><span class="k">FIRST</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">rowtime</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">start_tstamp</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="k">LAST</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">rowtime</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">end_tstamp</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="k">AVG</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">avgPrice</span><span class="w">
</span><span class="w">        </span><span class="n">ONE</span><span class="w"> </span><span class="k">ROW</span><span class="w"> </span><span class="n">PER</span><span class="w"> </span><span class="k">MATCH</span><span class="w">
</span><span class="w">        </span><span class="k">AFTER</span><span class="w"> </span><span class="k">MATCH</span><span class="w"> </span><span class="n">SKIP</span><span class="w"> </span><span class="n">PAST</span><span class="w"> </span><span class="k">LAST</span><span class="w"> </span><span class="k">ROW</span><span class="w">
</span><span class="w">        </span><span class="n">PATTERN</span><span class="w"> </span><span class="p">(</span><span class="n">A</span><span class="o">+</span><span class="w"> </span><span class="n">B</span><span class="p">)</span><span class="w">
</span><span class="w">        </span><span class="n">DEFINE</span><span class="w">
</span><span class="w">            </span><span class="n">A</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">AVG</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">15</span><span class="w">
</span><span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="n">MR</span><span class="p">;</span><span class="w">
</span></code></pre></div><p>给定这个查询和以下输入值：</p>
<pre><code>symbol         rowtime         price    tax
======  ====================  ======= =======
'ACME'  '01-Apr-11 10:00:00'   12      1
'ACME'  '01-Apr-11 10:00:01'   17      2
'ACME'  '01-Apr-11 10:00:02'   13      1
'ACME'  '01-Apr-11 10:00:03'   16      3
'ACME'  '01-Apr-11 10:00:04'   25      2
'ACME'  '01-Apr-11 10:00:05'   2       1
'ACME'  '01-Apr-11 10:00:06'   4       1
'ACME'  '01-Apr-11 10:00:07'   10      2
'ACME'  '01-Apr-11 10:00:08'   15      2
'ACME'  '01-Apr-11 10:00:09'   25      2
'ACME'  '01-Apr-11 10:00:10'   25      1
'ACME'  '01-Apr-11 10:00:11'   30      1
</code></pre><p>只要事件的平均价格不超过 15，查询就会将事件累积为模式变量 A 的一部分。例如，这样的超限事件发生在 01-4-11 10:00:04。接下来的时期在 01-4-11 10:00:11 再次超过 15 的平均价格。因此，所述查询的结果将是：。</p>
<pre><code> symbol       start_tstamp       end_tstamp          avgPrice
=========  ==================  ==================  ============
ACME       01-APR-11 10:00:00  01-APR-11 10:00:03     14.5
ACME       01-APR-11 10:00:05  01-APR-11 10:00:10     13.5
</code></pre><p>注意 聚合可以应用于表达式，但只有当它们引用一个单一的模式变量时才可以。因此 SUM(A.price * A.tax)是有效的，但是 AVG(A.price * B.tax)不是。</p>
<p>注意不支持 DISTINCT 聚合。</p>
<p>定义一个模式
MATCH_RECOGNIZE 子句允许用户在事件流中搜索模式，使用一种强大的、富有表现力的语法，这种语法与广泛使用的正则表达式语法有些相似。</p>
<p>每个模式都是由基本的构件构成的，称为模式变量，可以对其应用运算符（量化符和其他修饰符）。整个模式必须用括号括起来。</p>
<p>一个模式的例子可以是这样的。</p>
<pre><code>PATTERN (A B+ C* D)
</code></pre><p>我们可以使用以下操作符。</p>
<p>并集 &ndash; 像(A B)这样的模式意味着 A 和 B 之间的相邻性是严格的，因此，中间不能有没有映射到 A 或 B 的行。
定量符&ndash;修改可以映射到模式变量的行数。</p>
<pre><code>* — 0 or more rows
+ — 1 or more rows
? — 0 or 1 rows
{ n } — exactly n rows (n &gt; 0)
{ n, } — n or more rows (n ≥ 0)
{ n, m } — between n and m (inclusive) rows (0 ≤ n ≤ m, 0 &lt; m)
{ , m } — between 0 and m (inclusive) rows (m &gt; 0)
</code></pre><p>注意 不支持可能产生空匹配的模式。这类模式的例子有 PATTERN (A*)、PATTERN (A?B*)、PATTERN (A{0,} B{0,} C*)等。</p>
<p>贪婪和不情愿的量化器
每个量化器可以是贪婪的（默认行为）或勉强的。贪婪的量化器试图匹配尽可能多的记录，而不情愿的量化器试图匹配尽可能少的记录。</p>
<p>为了说明两者的区别，我们可以查看下面的示例，在这个示例中，一个贪婪的量化器被应用于 B 变量。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Ticker</span><span class="w">
</span><span class="w">    </span><span class="n">MATCH_RECOGNIZE</span><span class="p">(</span><span class="w">
</span><span class="w">        </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">symbol</span><span class="w">
</span><span class="w">        </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">rowtime</span><span class="w">
</span><span class="w">        </span><span class="n">MEASURES</span><span class="w">
</span><span class="w">            </span><span class="k">C</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">lastPrice</span><span class="w">
</span><span class="w">        </span><span class="n">ONE</span><span class="w"> </span><span class="k">ROW</span><span class="w"> </span><span class="n">PER</span><span class="w"> </span><span class="k">MATCH</span><span class="w">
</span><span class="w">        </span><span class="k">AFTER</span><span class="w"> </span><span class="k">MATCH</span><span class="w"> </span><span class="n">SKIP</span><span class="w"> </span><span class="n">PAST</span><span class="w"> </span><span class="k">LAST</span><span class="w"> </span><span class="k">ROW</span><span class="w">
</span><span class="w">        </span><span class="n">PATTERN</span><span class="w"> </span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="n">B</span><span class="o">*</span><span class="w"> </span><span class="k">C</span><span class="p">)</span><span class="w">
</span><span class="w">        </span><span class="n">DEFINE</span><span class="w">
</span><span class="w">            </span><span class="n">A</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="n">B</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">B</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">15</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="k">C</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">C</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">12</span><span class="w">
</span><span class="w">    </span><span class="p">)</span><span class="w">
</span></code></pre></div><p>鉴于我们有以下输入。</p>
<pre><code> symbol  tax   price          rowtime
======= ===== ======== =====================
 XYZ     1     10       2018-09-17 10:00:02
 XYZ     2     11       2018-09-17 10:00:03
 XYZ     1     12       2018-09-17 10:00:04
 XYZ     2     13       2018-09-17 10:00:05
 XYZ     1     14       2018-09-17 10:00:06
 XYZ     2     16       2018-09-17 10:00:07
</code></pre><p>上述模式将产生以下输出。</p>
<pre><code> symbol   lastPrice
======== ===========
 XYZ      16
</code></pre><p>同样的查询，将 <code>B*</code> 修改为 <code>B*</code> 吗，即 B*应该是不愿意的，会产生。</p>
<pre><code> symbol   lastPrice
======== ===========
 XYZ      13
 XYZ      16
</code></pre><p>模式变量 B 只匹配到价格为 12 的行，而不是吞掉价格为 12、13、14 的行。</p>
<p>注意 对于一个模式的最后一个变量，不可能使用贪婪的量化符。因此，像（A B*）这样的模式是不允许的。这可以通过引入一个人为的状态（如 C）来轻松解决，这个状态具有 B 的否定条件，所以你可以使用这样的查询。</p>
<pre><code>PATTERN (A B* C)
DEFINE
    A AS condA(),
    B AS condB(),
    C AS NOT condB()
</code></pre><p>注意 目前不支持可选的勉强量化符(A??或 A{0,1}?)。</p>
<p>时间限制
特别是对于流式使用案例，通常要求一个模式在给定的时间内完成。这允许限制 Flink 必须在内部维护的整体状态大小，即使在贪婪的量化器的情况下。</p>
<p>因此，Flink SQL 支持额外的（非标准 SQL）WITHIN 子句来定义模式的时间约束。该子句可以定义在 PATTERN 子句之后，并以毫秒为间隔进行解析。</p>
<p>如果一个潜在匹配的第一个事件和最后一个事件之间的时间长于给定的值，这样的匹配将不会被追加到结果表中。</p>
<p>注意 一般鼓励使用 within 子句，因为它有助于 Flink 进行有效的内存管理。一旦达到阈值，底层状态可以被修剪。</p>
<p>注意 然而，WITHIN 子句不是 SQL 标准的一部分。推荐的处理时间限制的方式可能会在未来发生变化。</p>
<p>在下面的查询示例中说明了 WITHIN 子句的使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Ticker</span><span class="w">
</span><span class="w">    </span><span class="n">MATCH_RECOGNIZE</span><span class="p">(</span><span class="w">
</span><span class="w">        </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">symbol</span><span class="w">
</span><span class="w">        </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">rowtime</span><span class="w">
</span><span class="w">        </span><span class="n">MEASURES</span><span class="w">
</span><span class="w">            </span><span class="k">C</span><span class="p">.</span><span class="n">rowtime</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">dropTime</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="k">C</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">dropDiff</span><span class="w">
</span><span class="w">        </span><span class="n">ONE</span><span class="w"> </span><span class="k">ROW</span><span class="w"> </span><span class="n">PER</span><span class="w"> </span><span class="k">MATCH</span><span class="w">
</span><span class="w">        </span><span class="k">AFTER</span><span class="w"> </span><span class="k">MATCH</span><span class="w"> </span><span class="n">SKIP</span><span class="w"> </span><span class="n">PAST</span><span class="w"> </span><span class="k">LAST</span><span class="w"> </span><span class="k">ROW</span><span class="w">
</span><span class="w">        </span><span class="n">PATTERN</span><span class="w"> </span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="n">B</span><span class="o">*</span><span class="w"> </span><span class="k">C</span><span class="p">)</span><span class="w"> </span><span class="n">WITHIN</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;1&#39;</span><span class="w"> </span><span class="n">HOUR</span><span class="w">
</span><span class="w">        </span><span class="n">DEFINE</span><span class="w">
</span><span class="w">            </span><span class="n">B</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">B</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">10</span><span class="w">
</span><span class="w">            </span><span class="k">C</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">C</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">10</span><span class="w">
</span><span class="w">    </span><span class="p">)</span><span class="w">
</span></code></pre></div><p>查询检测到在 1 小时的时间间隔内发生的价格下跌 10。</p>
<p>假设该查询用于分析以下行情数据。</p>
<pre><code>symbol         rowtime         price    tax
======  ====================  ======= =======
'ACME'  '01-Apr-11 10:00:00'   20      1
'ACME'  '01-Apr-11 10:20:00'   17      2
'ACME'  '01-Apr-11 10:40:00'   18      1
'ACME'  '01-Apr-11 11:00:00'   11      3
'ACME'  '01-Apr-11 11:20:00'   14      2
'ACME'  '01-Apr-11 11:40:00'   9       1
'ACME'  '01-Apr-11 12:00:00'   15      1
'ACME'  '01-Apr-11 12:20:00'   14      2
'ACME'  '01-Apr-11 12:40:00'   24      2
'ACME'  '01-Apr-11 13:00:00'   1       2
'ACME'  '01-Apr-11 13:20:00'   19      1
</code></pre><p>查询将产生以下结果。</p>
<pre><code>symbol         dropTime         dropDiff
======  ====================  =============
'ACME'  '01-Apr-11 13:00:00'      14
</code></pre><p>结果行表示价格从 15（在 4 月 1 日 12:00:00）下降到 1（在 4 月 1 日 13:00:00）。dropDiff 列包含了价格差。</p>
<p>请注意，即使价格也以更高的数值下降，例如，下降 11（在 01-Apr-11 10:00:00 和 01-Apr-11 11:40:00 之间），这两个事件之间的时间差大于 1 小时。因此，它们不会产生匹配。</p>
<p>输出模式
输出模式描述了每找到一个匹配的记录应该发出多少行。SQL 标准描述了两种模式。</p>
<pre><code>ALL ROWS PER MATCH
ONE ROW PER MATCH.
</code></pre><p>目前，唯一支持的输出模式是 ONE ROW PER MATCH，对于每一个找到的匹配项，总会产生一个输出汇总行。</p>
<p>输出行的模式将是[分区列]+[措施列]按该特定顺序的连接。</p>
<p>下面的例子显示了一个定义为查询的输出。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Ticker</span><span class="w">
</span><span class="w">    </span><span class="n">MATCH_RECOGNIZE</span><span class="p">(</span><span class="w">
</span><span class="w">        </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">symbol</span><span class="w">
</span><span class="w">        </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">rowtime</span><span class="w">
</span><span class="w">        </span><span class="n">MEASURES</span><span class="w">
</span><span class="w">            </span><span class="k">FIRST</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">startPrice</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="k">LAST</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">topPrice</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="n">B</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">lastPrice</span><span class="w">
</span><span class="w">        </span><span class="n">ONE</span><span class="w"> </span><span class="k">ROW</span><span class="w"> </span><span class="n">PER</span><span class="w"> </span><span class="k">MATCH</span><span class="w">
</span><span class="w">        </span><span class="n">PATTERN</span><span class="w"> </span><span class="p">(</span><span class="n">A</span><span class="o">+</span><span class="w"> </span><span class="n">B</span><span class="p">)</span><span class="w">
</span><span class="w">        </span><span class="n">DEFINE</span><span class="w">
</span><span class="w">            </span><span class="n">A</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">LAST</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">IS</span><span class="w"> </span><span class="k">NULL</span><span class="w"> </span><span class="k">OR</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="k">LAST</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">),</span><span class="w">
</span><span class="w">            </span><span class="n">B</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">B</span><span class="p">.</span><span class="n">price</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="k">LAST</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="p">)</span><span class="w">
</span></code></pre></div><p>对于以下输入行：</p>
<pre><code> symbol   tax   price          rowtime
======== ===== ======== =====================
 XYZ      1     10       2018-09-17 10:00:02
 XYZ      2     12       2018-09-17 10:00:03
 XYZ      1     13       2018-09-17 10:00:04
 XYZ      2     11       2018-09-17 10:00:05
</code></pre><p>查询将产生以下输出。</p>
<pre><code> symbol   startPrice   topPrice   lastPrice
======== ============ ========== ===========
 XYZ      10           13         11
</code></pre><p>模式识别是按符号列进行分区的。尽管在 MEASURES 子句中没有明确提到，但在结果的开头会添加分区列。</p>
<p>模式导航
DEFINE 和 MEASURES 子句允许在（可能）匹配模式的行列表中进行导航。</p>
<p>本节将讨论这种用于声明条件或产生输出结果的导航。</p>
<p>模式变量引用
模式变量引用允许引用映射到 DEFINE 或 MEASURES 子句中特定模式变量的一组行。</p>
<p>例如，表达式 A.price 描述了迄今为止映射到 A 的一组行，再加上当前行，如果我们尝试将当前行与 A 进行匹配。如果 DEFINE/MEASURES 子句中的表达式需要单行（例如 A.price 或 A.price&gt;10），则选择属于相应集合的最后一个值。</p>
<p>如果没有指定模式变量（例如 SUM(price)），表达式会引用默认的模式变量*，它引用模式中的所有变量。换句话说，它创建了一个迄今为止映射到任何变量的所有行加上当前行的列表。</p>
<p>例子</p>
<p>要想了解更透彻的例子，可以看看下面的模式和相应的条件。</p>
<pre><code>PATTERN (A B+)
DEFINE
  A AS A.price &gt; 10,
  B AS B.price &gt; A.price AND SUM(price) &lt; 100 AND SUM(B.price) &lt; 80
</code></pre><p>下表描述了如何评估每个传入事件的这些条件。</p>
<p>该表由以下几栏组成：</p>
<pre><code># - the row identifier that uniquely identifies an incoming row in the lists [A.price]/[B.price]/[price].
price - the price of the incoming row.
[A.price]/[B.price]/[price] - describe lists of rows which are used in the DEFINE clause to evaluate conditions.
Classifier - the classifier of the current row which indicates the pattern variable the row is mapped to.
A.price/B.price/SUM(price)/SUM(B.price) - describes the result after those expressions have been evaluated.
#	price	Classifier	[A.price]	[B.price]	[price]	A.price	B.price	SUM(price)	SUM(B.price)
#1	10	-&gt; A	#1	-	-	10	-	-	-
#2	15	-&gt; B	#1	#2	#1, #2	10	15	25	15
#3	20	-&gt; B	#1	#2, #3	#1, #2, #3	10	20	45	35
#4	31	-&gt; B	#1	#2, #3, #4	#1, #2, #3, #4	10	31	76	66
#5	35		#1	#2, #3, #4, #5	#1, #2, #3, #4, #5	10	35	111	101
</code></pre><p>从表中可以看出，第一行被映射到模式变量 A，随后的行被映射到模式变量 B，但是最后一行不满足 B 的条件，因为所有映射行的 SUM(价格)和 B 中所有行的总和超过了指定的阈值。</p>
<p>逻辑偏移
逻辑偏移可以在映射到特定模式变量的事件中进行导航。这可以用两个相应的函数来表示。</p>
<p>偏移函数 描述
LAST(variable.field, n)
返回事件中被映射到变量第 n 个最后元素的字段的值。从映射到的最后一个元素开始计算。</p>
<p>FIRST(variable.field, n)
返回事件中被映射到变量第 n 个元素的字段值。从映射到的第一个元素开始计算。</p>
<p>示例</p>
<p>为了更透彻的举例，可以看看下面的模式和相应的条件。</p>
<pre><code>PATTERN (A B+)
DEFINE
  A AS A.price &gt; 10,
  B AS (LAST(B.price, 1) IS NULL OR B.price &gt; LAST(B.price, 1)) AND
       (LAST(B.price, 2) IS NULL OR B.price &gt; 2 * LAST(B.price, 2))
</code></pre><p>下表描述了如何评估每个传入事件的这些条件。</p>
<p>该表由以下几栏组成：</p>
<pre><code>price - the price of the incoming row.
Classifier - the classifier of the current row which indicates the pattern variable the row is mapped to.
LAST(B.price, 1)/LAST(B.price, 2) - describes the result after those expressions have been evaluated.
price	Classifier	LAST(B.price, 1)	LAST(B.price, 2)	Comment
10	-&gt; A			
15	-&gt; B	null	null	Notice that LAST(A.price, 1) is null because there is still nothing mapped to B.
20	-&gt; B	15	null	
31	-&gt; B	20	15	
35		31	20	Not mapped because 35 &lt; 2 * 20.
</code></pre><p>使用默认的模式变量与逻辑偏移量也可能是有意义的。</p>
<p>在这种情况下，偏移量会考虑到目前为止映射的所有行。</p>
<pre><code>PATTERN (A B? C)
DEFINE
  B AS B.price &lt; 20,
  C AS LAST(price, 1) &lt; C.price
price	Classifier	LAST(price, 1)	Comment
10	-&gt; A		
15	-&gt; B		
20	-&gt; C	15	LAST(price, 1) is evaluated as the price of the row mapped to the B variable.
</code></pre><p>如果第二行没有映射到 B 变量，我们会有以下结果。</p>
<pre><code>price	Classifier	LAST(price, 1)	Comment
10	-&gt; A		
20	-&gt; C	10	LAST(price, 1) is evaluated as the price of the row mapped to the A variable.
</code></pre><p>也可以在 first/last 函数的第一个参数中使用多个模式变量引用。这样，就可以写一个访问多列的表达式。但是，所有这些表达式必须使用同一个模式变量。换句话说，LAST/FIRST 函数的值必须在单行中计算。</p>
<p>因此，可以使用 LAST(A.price * A.tax)，但不允许使用 LAST(A.price * B.tax)这样的表达式。</p>
<p>匹配后策略
AFTER MATCH SKIP 子句指定了在找到完整匹配后，在哪里开始一个新的匹配过程。</p>
<p>有四种不同的策略。</p>
<p>SKIP PAST LAST ROW - 在当前匹配的最后一行之后的下一行恢复模式匹配。
SKIP TO NEXT ROW - 从匹配起始行后的下一行开始继续搜索新的匹配。
SKIP TO LAST 变量&ndash;在映射到指定模式变量的最后一行恢复模式匹配。
SKIP TO FIRST 变量&ndash;在被映射到指定模式变量的第一行恢复模式匹配。
这也是一种指定一个事件可以属于多少个匹配的方式。例如，使用 SKIP PAST LAST ROW 策略，每个事件最多只能属于一个匹配。</p>
<p>例子</p>
<p>为了更好地理解这些策略之间的差异，可以看一下下面的例子。</p>
<p>对于以下输入行。</p>
<pre><code> symbol   tax   price         rowtime
======== ===== ======= =====================
 XYZ      1     7       2018-09-17 10:00:01
 XYZ      2     9       2018-09-17 10:00:02
 XYZ      1     10      2018-09-17 10:00:03
 XYZ      2     5       2018-09-17 10:00:04
 XYZ      2     17      2018-09-17 10:00:05
 XYZ      2     14      2018-09-17 10:00:06
</code></pre><p>我们用不同的策略评估以下查询。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Ticker</span><span class="w">
</span><span class="w">    </span><span class="n">MATCH_RECOGNIZE</span><span class="p">(</span><span class="w">
</span><span class="w">        </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">symbol</span><span class="w">
</span><span class="w">        </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">rowtime</span><span class="w">
</span><span class="w">        </span><span class="n">MEASURES</span><span class="w">
</span><span class="w">            </span><span class="k">SUM</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">sumPrice</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="k">FIRST</span><span class="p">(</span><span class="n">rowtime</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">startTime</span><span class="p">,</span><span class="w">
</span><span class="w">            </span><span class="k">LAST</span><span class="p">(</span><span class="n">rowtime</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">endTime</span><span class="w">
</span><span class="w">        </span><span class="n">ONE</span><span class="w"> </span><span class="k">ROW</span><span class="w"> </span><span class="n">PER</span><span class="w"> </span><span class="k">MATCH</span><span class="w">
</span><span class="w">        </span><span class="p">[</span><span class="k">AFTER</span><span class="w"> </span><span class="k">MATCH</span><span class="w"> </span><span class="n">STRATEGY</span><span class="p">]</span><span class="w">
</span><span class="w">        </span><span class="n">PATTERN</span><span class="w"> </span><span class="p">(</span><span class="n">A</span><span class="o">+</span><span class="w"> </span><span class="k">C</span><span class="p">)</span><span class="w">
</span><span class="w">        </span><span class="n">DEFINE</span><span class="w">
</span><span class="w">            </span><span class="n">A</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">SUM</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">price</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">30</span><span class="w">
</span><span class="w">    </span><span class="p">)</span><span class="w">
</span></code></pre></div><p>查询返回映射到 A 的所有行的价格总和，以及整体匹配的第一个和最后一个时间戳。</p>
<p>根据使用的 AFTER MATCH 策略，查询会产生不同的结果。</p>
<p>AFTER MATCH SKIP PAST ROW(跳过最后一行)</p>
<pre><code> symbol   sumPrice        startTime              endTime
======== ========== ===================== =====================
 XYZ      26         2018-09-17 10:00:01   2018-09-17 10:00:04
 XYZ      17         2018-09-17 10:00:05   2018-09-17 10:00:06
</code></pre><p>第一个结果与 1 号，2 号，3 号，4 号行相匹配。</p>
<p>第二个结果与#5, #6 行相匹配。</p>
<p>匹配后跳转到下一行。</p>
<pre><code> symbol   sumPrice        startTime              endTime
======== ========== ===================== =====================
 XYZ      26         2018-09-17 10:00:01   2018-09-17 10:00:04
 XYZ      24         2018-09-17 10:00:02   2018-09-17 10:00:05
 XYZ      15         2018-09-17 10:00:03   2018-09-17 10:00:05
 XYZ      22         2018-09-17 10:00:04   2018-09-17 10:00:06
 XYZ      17         2018-09-17 10:00:05   2018-09-17 10:00:06
</code></pre><p>同样，第一个结果对 1 号、2 号、3 号、4 号行进行匹配。</p>
<p>与之前的策略相比，接下来的匹配中又包含了 2 号行的匹配。因此，第二个结果与行#2，#3，#4，#5 相匹配。</p>
<p>第三个结果与 3 号，4 号，5 号行相匹配。</p>
<p>第四个结果与行#4，#5，#6 相匹配。</p>
<p>最后一个结果与行#5，#6 匹配。</p>
<p>匹配后跳转到最后一行。</p>
<pre><code> symbol   sumPrice        startTime              endTime
======== ========== ===================== =====================
 XYZ      26         2018-09-17 10:00:01   2018-09-17 10:00:04
 XYZ      15         2018-09-17 10:00:03   2018-09-17 10:00:05
 XYZ      22         2018-09-17 10:00:04   2018-09-17 10:00:06
 XYZ      17         2018-09-17 10:00:05   2018-09-17 10:00:06
</code></pre><p>同样，第一个结果针对 1 号、2 号、3 号、4 号行进行匹配。</p>
<p>与之前的策略相比，接下来的匹配只包括 3 号行（映射到 A 行），再次进行匹配。因此，第二个结果与行#3，#4，#5 相匹配。</p>
<p>第三个结果与#4，#5，#6 行相匹配。</p>
<p>最后一个结果与行#5,#6 匹配，因此第三个结果与行#4,#5,#6 匹配。</p>
<p>匹配后跳转到第一行 A。</p>
<p>这个组合会产生一个运行时异常，因为我们总是试图在上一个比赛开始的地方开始一个新的比赛。这将产生一个无限循环，因此是被禁止的。</p>
<p>我们必须记住，在使用 SKIP TO FIRST/LAST 变量策略的情况下，有可能没有记录映射到该变量上（例如模式 A*）。在这种情况下，将抛出一个运行时异常，因为标准要求有一条有效的记录来继续匹配。</p>
<p>时间属性
为了在 MATCH_RECOGNIZE 之上应用一些后续的查询，可能需要使用时间属性。为了选择这些属性，有两个函数可用。</p>
<p>功能描述
MATCH_ROWTIME()
返回被映射到给定模式的最后一行的时间戳。</p>
<p>所得到的属性是一个 rowtime 属性，它可以被用于后续的基于时间的操作，如区间连接和组窗口或窗口聚合。</p>
<p>MATCH_PROCTIME()
返回一个 proctime 属性，该属性可用于后续基于时间的操作，如区间连接和组窗口或窗口聚合。</p>
<p>控制内存消耗
在编写 MATCH_RECOGNIZE 查询时，内存消耗是一个重要的考虑因素，因为潜在的匹配空间是以类似广度优先的方式建立的。考虑到这一点，必须确保模式能够完成。最好是有合理数量的行映射到匹配中，因为它们必须适应内存。</p>
<p>例如，模式不能有一个没有上限的量化器，接受每一行。这样的模式可以是这样的。</p>
<pre><code>PATTERN (A B+ C)
DEFINE
  A as A.price &gt; 10,
  C as C.price &gt; 20
</code></pre><p>该查询将把每一条进入的记录映射到 B 变量上，因此永远不会结束。这个查询可以通过否定 C 的条件来解决。</p>
<pre><code>PATTERN (A B+ C)
DEFINE
  A as A.price &gt; 10,
  B as B.price &lt;= 20,
  C as C.price &gt; 20
</code></pre><p>或者通过使用勉强的定量器。</p>
<pre><code>PATTERN (A B+? C)
DEFINE
  A as A.price &gt; 10,
  C as C.price &gt; 20
</code></pre><p>注意 请注意，MATCH_RECOGNIZE 子句不使用配置的状态保留时间。人们可能希望使用 WITHIN 子句来达到这个目的。</p>
<p>已知限制
Flink 对 MATCH_RECOGNIZE 子句的实现是一项持续的努力，目前还不支持 SQL 标准的一些功能。</p>
<p>不支持的功能包括</p>
<p>模式表达式。
模式组&ndash;这意味着，例如量化符不能应用于模式的子序列。因此，（A (B C)+）不是有效的模式。
改变&ndash;像 PATTERN((A B | C D) E)这样的模式，这意味着在寻找 E 行之前必须先找到一个子序列 A B 或 C D。
PERMUTE 运算符&ndash;相当于它所应用的所有变量的排列组合，例如 PATTERN(PERMUTE (A, B, C))=PATTERN(A B C | A C B | B A C | B A C | C B A | C B A)。
锚 - ^, $，表示一个分区的开始/结束，这些在流媒体环境中没有意义，将不被支持。
排除 - PATTERN ({- A -} B) 意味着 A 将被查找，但不会参与输出。这只对 ALL ROWS PER MATCH 模式有效。
不情愿的可选量化符&ndash;PATTERN A?? 只支持贪婪的可选量化符。
ALL ROWS PER MATCH 输出模式&ndash;它为每一条参与创建发现匹配的记录产生一条输出行。这也意味着。
MEASURES 子句唯一支持的语义是 FINAL。
CLASSIFIER 函数，该函数返回某行被映射到的模式变量，目前还不支持。
SUBSET - 允许创建模式变量的逻辑组，并在 DEFINE 和 MEASURES 子句中使用这些组。
物理偏移&ndash;PREV/NEXT，它索引所有看到的事件，而不是只索引那些被映射到模式变量的事件（如逻辑偏移情况）。
提取时间属性&ndash;目前没有可能为后续基于时间的操作获取时间属性。
MATCH_RECOGNIZE 只支持 SQL。在 Table API 中没有等价物。
聚合。
不支持不同的聚合。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/match_recognize.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/match_recognize.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[概念和通用 API]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-concepts-and-common-api/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-concepts-and-common-api/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Concepts and Common API</blockquote><h2 id="概念和通用-api">概念和通用 API</h2>
<p>Table API 和 SQL 被集成在一个联合 API 中。这个 API 的核心概念是一个 Table，作为查询的输入和输出。本文档介绍了具有 Table API 和 SQL 查询的程序的常用结构，如何注册 Table，如何查询 Table，如何发出 Table。</p>
<h2 id="两种-planners-的主要区别">两种 Planners 的主要区别</h2>
<ol>
<li>Blink 将批处理作业视为流式作业的一种特殊情况。因此，也不支持 Table 和 DataSet 之间的转换，批处理作业不会被翻译成 DateSet 程序，而是翻译成 DataStream 程序，和流作业一样。</li>
<li>Blink 计划器不支持 BatchTableSource，请使用有界的 StreamTableSource 代替。</li>
<li>旧计划器和 Blink 计划器的 FilterableTableSource 的实现是不兼容的。旧的规划者会将 PlannerExpressions 推送到 FilterableTableSource 中，而 Blink 规划者会将 Expressions 推送下去。</li>
<li>基于字符串的键值<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/config.html">配置</a>选项(详情请看配置文档)只用于 Blink 规划器。</li>
<li>PlannerConfig 在两个规划器中的实现(CalciteConfig)是不同的。</li>
<li>Blink 规划师将在 TableEnvironment 和 StreamTableEnvironment 上把多个汇优化成一个 DAG。旧的规划器总是会将每个汇优化成一个新的 DAG，其中所有的 DAG 是相互独立的。</li>
<li>现在老的计划器不支持目录统计，而 Blink 计划器支持。</li>
</ol>
<h2 id="table-api-和-sql-程序的结构">Table API 和 SQL 程序的结构</h2>
<p>所有用于批处理和流处理的 Table API 和 SQL 程序都遵循相同的模式。下面的代码示例显示了 Table API 和 SQL 程序的共同结构。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// create a TableEnvironment for specific planner batch or streaming
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="c1">// create a Table
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">connect</span><span class="o">(...).</span><span class="n">createTemporaryTable</span><span class="o">(</span><span class="s">&#34;table1&#34;</span><span class="o">)</span>
<span class="c1">// register an output Table
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">connect</span><span class="o">(...).</span><span class="n">createTemporaryTable</span><span class="o">(</span><span class="s">&#34;outputTable&#34;</span><span class="o">)</span>

<span class="c1">// create a Table from a Table API query
</span><span class="c1"></span><span class="k">val</span> <span class="n">tapiResult</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;table1&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(...)</span>
<span class="c1">// create a Table from a SQL query
</span><span class="c1"></span><span class="k">val</span> <span class="n">sqlResult</span>  <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span><span class="s">&#34;SELECT ... FROM table1 ...&#34;</span><span class="o">)</span>

<span class="c1">// emit a Table API result Table to a TableSink, same for SQL result
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableResult</span> <span class="k">=</span> <span class="n">tapiResult</span><span class="o">.</span><span class="n">executeInsert</span><span class="o">(</span><span class="s">&#34;outputTable&#34;</span><span class="o">)</span>
<span class="n">tableResult</span><span class="o">...</span>
</code></pre></div><p>注意：表 API 和 SQL 查询可以很容易地与 DataStream 或 DataSet 程序集成并嵌入其中。请查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#integration-with-datastream-and-dataset-api">与 DataStream 和 DataSet API 的集成</a>部分，了解如何将 DataStream 和 DataSets 转换为表，反之亦然。</p>
<h2 id="创建一个-tableenvironment">创建一个 TableEnvironment</h2>
<p>TableEnvironment 是 Table API 和 SQL 集成的核心概念。它负责</p>
<ul>
<li>在内部目录(catalog)中注册一个 Table</li>
<li>登记目录(catalog)</li>
<li>加载可插拔模块</li>
<li>执行 SQL 查询</li>
<li>注册一个用户定义的（标量、表或聚合）函数</li>
<li>将 DataStream 或 DataSet 转换为 Table</li>
<li>持有对 ExecutionEnvironment 或 StreamExecutionEnvironment 的引用。</li>
</ul>
<p>一个 Table 总是绑定在一个特定的 TableEnvironment 上。在同一个查询中，不可能将不同 TableEnvironments 的表组合起来，例如，将它们连接或联合起来。</p>
<p>通过调用静态的 <code>BatchTableEnvironment.create()</code> 或 <code>StreamTableEnvironment.create()</code> 方法创建一个 TableEnvironment，其中包含一个 StreamExecutionEnvironment 或 ExecutionEnvironment 和一个可选的 TableConfig。TableConfig 可以用来配置 TableEnvironment 或自定义查询优化和翻译过程（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#query-optimization">Query Optimization</a>）。</p>
<p>确保选择与你的编程语言相匹配的特定规划器 BatchTableEnvironment/StreamTableEnvironment。</p>
<p>如果这两个规划器 jar 都在 classpath 上（默认行为），你应该明确设置在当前程序中使用哪个规划器。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// **********************
</span><span class="c1">// FLINK STREAMING QUERY
</span><span class="c1">// **********************
</span><span class="c1"></span><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.EnvironmentSettings</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.bridge.scala.StreamTableEnvironment</span>

<span class="k">val</span> <span class="n">fsSettings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">().</span><span class="n">useOldPlanner</span><span class="o">().</span><span class="n">inStreamingMode</span><span class="o">().</span><span class="n">build</span><span class="o">()</span>
<span class="k">val</span> <span class="n">fsEnv</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">fsTableEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">fsEnv</span><span class="o">,</span> <span class="n">fsSettings</span><span class="o">)</span>
<span class="c1">// or val fsTableEnv = TableEnvironment.create(fsSettings)
</span><span class="c1"></span>
<span class="c1">// ******************
</span><span class="c1">// FLINK BATCH QUERY
</span><span class="c1">// ******************
</span><span class="c1"></span><span class="k">import</span> <span class="nn">org.apache.flink.api.scala.ExecutionEnvironment</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.bridge.scala.BatchTableEnvironment</span>

<span class="k">val</span> <span class="n">fbEnv</span> <span class="k">=</span> <span class="nc">ExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">fbTableEnv</span> <span class="k">=</span> <span class="nc">BatchTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">fbEnv</span><span class="o">)</span>

<span class="c1">// **********************
</span><span class="c1">// BLINK STREAMING QUERY
</span><span class="c1">// **********************
</span><span class="c1"></span><span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.EnvironmentSettings</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.bridge.scala.StreamTableEnvironment</span>

<span class="k">val</span> <span class="n">bsEnv</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">bsSettings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">().</span><span class="n">useBlinkPlanner</span><span class="o">().</span><span class="n">inStreamingMode</span><span class="o">().</span><span class="n">build</span><span class="o">()</span>
<span class="k">val</span> <span class="n">bsTableEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">bsEnv</span><span class="o">,</span> <span class="n">bsSettings</span><span class="o">)</span>
<span class="c1">// or val bsTableEnv = TableEnvironment.create(bsSettings)
</span><span class="c1"></span>
<span class="c1">// ******************
</span><span class="c1">// BLINK BATCH QUERY
</span><span class="c1">// ******************
</span><span class="c1"></span><span class="k">import</span> <span class="nn">org.apache.flink.table.api.</span><span class="o">{</span><span class="nc">EnvironmentSettings</span><span class="o">,</span> <span class="nc">TableEnvironment</span><span class="o">}</span>

<span class="k">val</span> <span class="n">bbSettings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">().</span><span class="n">useBlinkPlanner</span><span class="o">().</span><span class="n">inBatchMode</span><span class="o">().</span><span class="n">build</span><span class="o">()</span>
<span class="k">val</span> <span class="n">bbTableEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">bbSettings</span><span class="o">)</span>
</code></pre></div><p>注意：如果在 <code>/lib</code> 目录下只有一个 planner jar，可以使用 <code>AnyPlanner(python 的 use_any_planner)</code> 来创建特定的环境设置。</p>
<h2 id="在目录catalog中创建表">在目录(Catalog)中创建表</h2>
<p>一个 TableEnvironment 维护着一个表的目录图，这些表是用一个标识符创建的。每个标识符由 3 部分组成：目录名、数据库名和对象名。如果没有指定目录或数据库，将使用当前的默认值（参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#table-identifier-expanding">Table 标识符展开</a>部分的例子）。</p>
<p>表可以是虚拟的（VIEWS）或常规的（TABLES）。VIEWS 可以从现有的 Table 对象创建，通常是 Table API 或 SQL 查询的结果。TABLES 描述外部数据，如文件、数据库表或消息队列。</p>
<h3 id="临时表与永久表">临时表与永久表</h3>
<p>表可以是临时的，与单个 Flink 会话的生命周期挂钩，也可以是永久的，在多个 Flink 会话和集群中可见。</p>
<p>永久表需要一个<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/catalogs.html">目录</a>（如 Hive Metastore）来维护表的元数据。一旦创建了永久表，它对连接到目录的任何 Flink 会话都是可见的，并将继续存在，直到表被显式放弃。</p>
<p>另一方面，临时表总是存储在内存中，并且只在它们创建的 Flink 会话的持续时间内存在。这些表对其他会话不可见。它们不绑定到任何目录或数据库，但可以在一个目录或数据库的命名空间中创建。如果相应的数据库被删除，临时表不会被删除。</p>
<h3 id="shadowing">Shadowing</h3>
<p>可以用与现有永久表相同的标识符登记一个临时表。只要临时表存在，临时表就会对永久表产生遮盖，使永久表无法访问。所有使用该标识符的查询都将针对临时表执行。</p>
<p>这可能对实验很有用。它允许首先对临时表运行完全相同的查询，例如，只有一个数据子集，或者数据被混淆了。一旦验证了查询的正确性，就可以针对真正的生产表运行。</p>
<h2 id="创建一个-table">创建一个 Table</h2>
<h3 id="虚拟表">虚拟表</h3>
<p>表 API 对象对应于 SQL 术语中的 VIEW（虚拟表）。它封装了一个逻辑查询计划。它可以在一个目录中创建，具体如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="c1">// table is the result of a simple projection query 
</span><span class="c1"></span><span class="k">val</span> <span class="n">projTable</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;X&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(...)</span>

<span class="c1">// register the Table projTable as table &#34;projectedTable&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;projectedTable&#34;</span><span class="o">,</span> <span class="n">projTable</span><span class="o">)</span>
</code></pre></div><p>注意：Table 对象与关系型数据库系统中的 VIEW 类似，即定义 Table 的查询不进行优化，但当另一个查询引用注册的 Table 时，会被内联。如果多个查询引用同一个注册表，则会对每个引用查询进行内联，并执行多次，即注册表的结果不会被共享。</p>
<h3 id="连接器表">连接器表</h3>
<p>也可以从<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connect.html">连接器</a>声明中创建一个关系型数据库中已知的 TABLE。连接器描述的是存储表数据的外部系统。这里可以声明 Apacha Kafka 或普通文件系统等存储系统。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="nc">DDL</span>
<span class="n">tableEnvironment</span>
  <span class="o">.</span><span class="n">connect</span><span class="o">(...)</span>
  <span class="o">.</span><span class="n">withFormat</span><span class="o">(...)</span>
  <span class="o">.</span><span class="n">withSchema</span><span class="o">(...)</span>
  <span class="o">.</span><span class="n">inAppendMode</span><span class="o">()</span>
  <span class="o">.</span><span class="n">createTemporaryTable</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="扩展-table-标识符">扩展 Table 标识符</h3>
<p>表总是用目录(catalog)、数据库、表名三部分组成的标识符进行注册。</p>
<p>用户可以将其中的一个目录和一个数据库设置为&quot;当前目录&quot;和&quot;当前数据库&quot;。其中，上述 3 部分标识符中的前两部分可以选择，如果不提供，则引用当前目录和当前数据库。用户可以通过表 API 或 SQL 切换当前目录和当前数据库。</p>
<p>标识符遵循 SQL 的要求，这意味着它们可以用反引号符(`)进行转义。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tEnv</span><span class="k">:</span> <span class="kt">TableEnvironment</span> <span class="o">=</span> <span class="o">...;</span>
<span class="n">tEnv</span><span class="o">.</span><span class="n">useCatalog</span><span class="o">(</span><span class="s">&#34;custom_catalog&#34;</span><span class="o">)</span>
<span class="n">tEnv</span><span class="o">.</span><span class="n">useDatabase</span><span class="o">(</span><span class="s">&#34;custom_database&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="o">...;</span>

<span class="c1">// register the view named &#39;exampleView&#39; in the catalog named &#39;custom_catalog&#39;
</span><span class="c1">// in the database named &#39;custom_database&#39; 
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;exampleView&#34;</span><span class="o">,</span> <span class="n">table</span><span class="o">)</span>

<span class="c1">// register the view named &#39;exampleView&#39; in the catalog named &#39;custom_catalog&#39;
</span><span class="c1">// in the database named &#39;other_database&#39; 
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;other_database.exampleView&#34;</span><span class="o">,</span> <span class="n">table</span><span class="o">)</span>

<span class="c1">// register the view named &#39;example.View&#39; in the catalog named &#39;custom_catalog&#39;
</span><span class="c1">// in the database named &#39;custom_database&#39; 
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;`example.View`&#34;</span><span class="o">,</span> <span class="n">table</span><span class="o">)</span>

<span class="c1">// register the view named &#39;exampleView&#39; in the catalog named &#39;other_catalog&#39;
</span><span class="c1">// in the database named &#39;other_database&#39; 
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;other_catalog.other_database.exampleView&#34;</span><span class="o">,</span> <span class="n">table</span><span class="o">)</span>
</code></pre></div><h2 id="查询一个-table">查询一个 Table</h2>
<h3 id="table-api">Table API</h3>
<p>Table API 是 Scala 和 Java 的语言集成查询 API。与 SQL 不同的是，查询不是指定为 Strings，而是在宿主语言中一步步组成。</p>
<p>该 API 基于 Table 类，它表示一个表（流式或批处理），并提供了应用关系操作的方法。这些方法返回一个新的 Table 对象，该对象表示对输入的 Table 应用关系操作的结果。有些关系操作由多个方法调用组成，如 <code>table.groupBy(...).select()</code>，其中 <code>groupBy(...)</code> 指定表的分组，<code>select(...)</code> 是表的分组上的投影。</p>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html">Table API</a> 文档描述了流式表和批处理表上支持的所有 Table API 操作。</p>
<p>下面的示例显示了一个简单的 Table API 聚合查询。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="c1">// register Orders table
</span><span class="c1"></span>
<span class="c1">// scan registered Orders table
</span><span class="c1"></span><span class="k">val</span> <span class="n">orders</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;Orders&#34;</span><span class="o">)</span>
<span class="c1">// compute revenue for all customers from France
</span><span class="c1"></span><span class="k">val</span> <span class="n">revenue</span> <span class="k">=</span> <span class="n">orders</span>
  <span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;cCountry&#34;</span> <span class="o">===</span> <span class="s">&#34;FRANCE&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;cID&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;cName&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;cID&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;cName&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;revenue&#34;</span><span class="o">.</span><span class="n">sum</span> <span class="nc">AS</span> <span class="s">&#34;revSum&#34;</span><span class="o">)</span>

<span class="c1">// emit or convert Table
</span><span class="c1">// execute query
</span></code></pre></div><p>注意：Scala Table API 使用以美元符号（<code>$</code>）开头的 Scala 字符串插值来引用 Table 的属性。Table API 使用 Scala implicits。请确保导入</p>
<ul>
<li><code>org.apache.flink.table.api._</code> - 用于隐式表达式转换</li>
<li><code>org.apache.flink.api.scala._</code> 和 <code>org.apache.flink.table.api.bridge.scala._</code>，如果你想从 DataStream 转换到 DataStream。</li>
</ul>
<h2 id="sql">SQL</h2>
<p>Flink 的 SQL 集成是基于 <a href="https://calcite.apache.org/">Apache Calcite</a>，它实现了 SQL 标准。SQL 查询被指定为常规 Strings。</p>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/index.html">SQL</a> 文档描述了 Flink 对流和批处理表的 SQL 支持。</p>
<p>下面的例子展示了如何指定一个查询并将结果以表的形式返回。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="c1">// register Orders table
</span><span class="c1"></span>
<span class="c1">// compute revenue for all customers from France
</span><span class="c1"></span><span class="k">val</span> <span class="n">revenue</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span><span class="s">&#34;&#34;&#34;
</span><span class="s">  |SELECT cID, cName, SUM(revenue) AS revSum
</span><span class="s">  |FROM Orders
</span><span class="s">  |WHERE cCountry = &#39;FRANCE&#39;
</span><span class="s">  |GROUP BY cID, cName
</span><span class="s">  &#34;&#34;&#34;</span><span class="o">.</span><span class="n">stripMargin</span><span class="o">)</span>

<span class="c1">// emit or convert Table
</span><span class="c1">// execute query
</span></code></pre></div><p>下面的示例显示了如何指定一个更新查询，将其结果插入到注册表中。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="c1">// register &#34;Orders&#34; table
</span><span class="c1">// register &#34;RevenueFrance&#34; output table
</span><span class="c1"></span>
<span class="c1">// compute revenue for all customers from France and emit to &#34;RevenueFrance&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;&#34;&#34;
</span><span class="s">  |INSERT INTO RevenueFrance
</span><span class="s">  |SELECT cID, cName, SUM(revenue) AS revSum
</span><span class="s">  |FROM Orders
</span><span class="s">  |WHERE cCountry = &#39;FRANCE&#39;
</span><span class="s">  |GROUP BY cID, cName
</span><span class="s">  &#34;&#34;&#34;</span><span class="o">.</span><span class="n">stripMargin</span><span class="o">)</span>
</code></pre></div><h3 id="混合-table-api-和-sql">混合 Table API 和 SQL</h3>
<p>表 API 和 SQL 查询可以很容易地混合，因为两者都返回 Table 对象。</p>
<ul>
<li>可以在 SQL 查询返回的 Table 对象上定义 Table API 查询。</li>
<li>通过在 TableEnvironment 中<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#register-a-table">注册生成的 Table</a>并在 SQL 查询的 FROM 子句中引用它，可以在 Table API 查询的结果上定义一个 SQL 查询。</li>
</ul>
<h3 id="发出一个表">发出一个表</h3>
<p>一个 Table 是通过将其写入 TableSink 而发出的。TableSink 是一个通用接口，它支持多种文件格式（如 CSV、Apache Parquet、Apache Avro）、存储系统（如 JDBC、Apache HBase、Apache Cassandra、Elasticsearch）或消息系统（如 Apache Kafka、RabbitMQ）。</p>
<p>批量表只能写入 BatchTableSink，而流式表则需要 AppendStreamTableSink、RetractStreamTableSink 或 UpsertStreamTableSink。</p>
<p>请参阅有关 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sourceSinks.html">Table Sources &amp; Sink</a> 的文档，以了解可用的 Sink 的详细信息以及如何实现自定义 TableSink 的说明。</p>
<p><code>Table.executeInsert(String tableName)</code> 方法将 Table 排放到一个注册的 TableSink 中。该方法通过名称从目录中查找 TableSink，并验证 Table 的模式与 TableSink 的模式是否相同。</p>
<p>下面的示例展示了如何发射 Table。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="c1">// create an output Table
</span><span class="c1"></span><span class="k">val</span> <span class="n">schema</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Schema</span><span class="o">()</span>
    <span class="o">.</span><span class="n">field</span><span class="o">(</span><span class="s">&#34;a&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">INT</span><span class="o">())</span>
    <span class="o">.</span><span class="n">field</span><span class="o">(</span><span class="s">&#34;b&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">STRING</span><span class="o">())</span>
    <span class="o">.</span><span class="n">field</span><span class="o">(</span><span class="s">&#34;c&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">LONG</span><span class="o">())</span>

<span class="n">tableEnv</span><span class="o">.</span><span class="n">connect</span><span class="o">(</span><span class="k">new</span> <span class="nc">FileSystem</span><span class="o">(</span><span class="s">&#34;/path/to/file&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">withFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Csv</span><span class="o">().</span><span class="n">fieldDelimiter</span><span class="o">(</span><span class="sc">&#39;|&#39;</span><span class="o">).</span><span class="n">deriveSchema</span><span class="o">())</span>
    <span class="o">.</span><span class="n">withSchema</span><span class="o">(</span><span class="n">schema</span><span class="o">)</span>
    <span class="o">.</span><span class="n">createTemporaryTable</span><span class="o">(</span><span class="s">&#34;CsvSinkTable&#34;</span><span class="o">)</span>

<span class="c1">// compute a result Table using Table API operators and/or SQL queries
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1">// emit the result Table to the registered TableSink
</span><span class="c1"></span><span class="n">result</span><span class="o">.</span><span class="n">executeInsert</span><span class="o">(</span><span class="s">&#34;CsvSinkTable&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="翻译和执行查询">翻译和执行查询</h3>
<p>两个规划器翻译和执行查询的行为是不同的。</p>
<ul>
<li>Blink 计划器</li>
</ul>
<p>表 API 和 SQL 查询无论其输入是流式还是批处理，都会被翻译成 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">DataStream</a> 程序。一个查询在内部表示为一个逻辑查询计划，并分两个阶段进行翻译。</p>
<ol>
<li>逻辑计划的优化。</li>
<li>翻译成 DataStream 程序。</li>
</ol>
<p>Table API 或 SQL 查询被翻译时:</p>
<ul>
<li><code>TableEnvironment.executeSql()</code> 被调用。这个方法用于执行给定的语句，一旦这个方法被调用，sql 查询就会立即被翻译。</li>
<li><code>Table.executeInsert()</code> 被调用。该方法用于将表的内容插入到给定的 sink 路径中，一旦调用该方法，Table API 立即被翻译。</li>
<li>调用 <code>Table.execute()</code>。该方法用于将表内容收集到本地客户端，一旦调用该方法，Table API 立即被翻译。</li>
<li><code>StatementSet.execute()</code> 被调用。一个 Table（通过 <code>StatementSet.addInsert()</code> 向 sink 发出）或一个 INSERT 语句（通过  <code>StatementSet.addInsertSql()</code> 指定）将首先在 StatementSet 中被缓冲。一旦 <code>StatementSet.execute()</code> 被调用，它们就会被翻译。所有接收器将被优化成一个 DAG。</li>
<li>当一个表被转换为 DataStream 时，它就会被翻译（参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#integration-with-datastream-and-dataset-api">与 DataStream 和 DataSet API 的集成</a>）。一旦翻译完毕，它就是一个常规的 DataStream 程序，并在调用 StreamExecutionEnvironment.execut()时被执行。
注意: 从 1.11 版本开始，<code>sqlUpdate()</code> 方法和 <code>insertInto()</code> 方法已被废弃。如果 Table 程序是由这两个方法构建的，我们必须使用 <code>StreamTableEnvironment.execution()</code> 方法代替 <code>StreamExecutionEnvironment.execution()</code> 方法来执行。</li>
</ul>
<h2 id="与-datastream-和-dataset-api-的集成">与 DataStream 和 DataSet API 的集成</h2>
<p>两种流上的计划器都可以与 DataStream API 集成，只有老的计划器可以与 DataSet API 集成，批处理的 Blink 计划器不能与两者结合。只有旧的计划器可以与 DataSet API 集成，批处理的 Blink 计划器不能与两者结合。注：下面讨论的 DataSet API 只适用于批处理的旧版规划器。</p>
<p>Table API 和 SQL 查询可以很容易地与 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/datastream_api.html">DataStream</a> 和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch">DataSet</a> 程序集成并嵌入其中。例如，可以查询一个外部表（例如来自 RDBMS），做一些预处理，如过滤、投影、聚合或加入元数据，然后用 DataStream 或 DataSet API（以及建立在这些 API 之上的任何库，如 CEP 或 Gelly）进一步处理数据。反之，也可以在 DataStream 或 DataSet 程序的结果上应用 Table API 或 SQL 查询。</p>
<p>这种交互可以通过将 DataStream 或 DataSet 转换为表来实现，反之亦然。在本节中，我们将描述这些转换是如何完成的。</p>
<h3 id="scala-隐式转换">Scala 隐式转换</h3>
<p>Scala Table API 为 DataSet、DataStream 和 Table 类提供了隐式转换的功能。这些转换是通过导入包 <code>org.apache.flink.table.api.bridge.scala._</code> 来实现的，此外还可以导入 <code>org.apache.flink.api.scala._</code> 来实现 Scala DataStream API。</p>
<h3 id="从-datastream-或-dataset-创建视图">从 DataStream 或 DataSet 创建视图</h3>
<p>DataStream 或 DataSet 可以作为视图在 TableEnvironment 中注册。由此产生的视图的模式取决于注册的 DataStream 或 DataSet 的数据类型。请查看有关<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html#mapping-of-data-types-to-table-schema">数据类型到表模式的映射</a>部分以了解详情。</p>
<p>注意：从 DataStream 或 DataSet 创建的视图只能注册为临时视图。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get TableEnvironment 
</span><span class="c1">// registration of a DataSet is equivalent
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span><span class="k">:</span> <span class="kt">StreamTableEnvironment</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// register the DataStream as View &#34;myTable&#34; with fields &#34;f0&#34;, &#34;f1&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;myTable&#34;</span><span class="o">,</span> <span class="n">stream</span><span class="o">)</span>

<span class="c1">// register the DataStream as View &#34;myTable2&#34; with fields &#34;myLong&#34;, &#34;myString&#34;
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">createTemporaryView</span><span class="o">(</span><span class="s">&#34;myTable2&#34;</span><span class="o">,</span> <span class="n">stream</span><span class="o">,</span> &#39;myLong<span class="o">,</span> &#39;myString<span class="o">)</span>
</code></pre></div><h3 id="将-datastream-或-dataset-转换为-table">将 DataStream 或 DataSet 转换为 Table</h3>
<p>不需要在 TableEnvironment 中注册一个 DataStream 或 DataSet，也可以直接将其转换为 Table。如果你想在 Table API 查询中使用 Table，这很方便。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get TableEnvironment
</span><span class="c1">// registration of a DataSet is equivalent
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// convert the DataStream into a Table with default fields &#34;_1&#34;, &#34;_2&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">table1</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">)</span>

<span class="c1">// convert the DataStream into a Table with fields &#34;myLong&#34;, &#34;myString&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">table2</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myLong&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myString&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="将-table-转换为-datastream-或-dataset">将 Table 转换为 DataStream 或 DataSet</h3>
<p>Table 可以被转换为 DataStream 或 DataSet。通过这种方式，可以在表 API 或 SQL 查询的结果上运行自定义 DataStream 或 DataSet 程序。</p>
<p>当将 Table 转换为 DataStream 或 DataSet 时，您需要指定生成的 DataStream 或 DataSet 的数据类型，即表的行要转换为的数据类型。通常，最方便的转换类型是 Row。下面的列表给出了不同选项的功能概述。</p>
<ul>
<li>Row：字段按位置映射，字段数量任意，支持 null 值，无类型安全访问。</li>
<li>POJO：字段按名称映射（POJO 字段必须与表字段一样命名），任意数量的字段，支持 null 值，类型安全访问。</li>
<li>Case Class：字段按位置映射，不支持 null 值，类型安全访问。</li>
<li>Tuple：字段按位置映射，限制为 22 个（Scala）或 25 个（Java）字段，不支持 null 值，类型安全访问。</li>
<li>原子类型：表必须有一个字段，不支持空值，类型安全访问。表必须有一个字段，不支持 null 值，类型安全访问。</li>
</ul>
<h3 id="将-table-转换为-datastream">将 Table 转换为 DataStream</h3>
<p>作为流式查询结果的表将被动态更新，即随着查询输入流中新记录的到达而变化。因此，将这种动态查询转换成的 DataStream 需要对表的更新进行编码。</p>
<p>有两种模式可以将表转换为 DataStream。</p>
<ol>
<li>Append 模式。只有当动态 Table 只被 INSERT 修改时，才可以使用这种模式，即只进行追加，之前发出的结果永远不会更新。</li>
<li>收回模式。这种模式可以一直使用。它将 INSERT 和 DELETE 更改用布尔标志编码。</li>
</ol>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get TableEnvironment. 
</span><span class="c1">// registration of a DataSet is equivalent
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span><span class="k">:</span> <span class="kt">StreamTableEnvironment</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="c1">// Table with two fields (String name, Integer age)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1">// convert the Table into an append DataStream of Row
</span><span class="c1"></span><span class="k">val</span> <span class="n">dsRow</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">toAppendStream</span><span class="o">[</span><span class="kt">Row</span><span class="o">](</span><span class="n">table</span><span class="o">)</span>

<span class="c1">// convert the Table into an append DataStream of Tuple2[String, Int]
</span><span class="c1"></span><span class="k">val</span> <span class="n">dsTuple</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="n">dsTuple</span> <span class="k">=</span> 
  <span class="n">tableEnv</span><span class="o">.</span><span class="n">toAppendStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)](</span><span class="n">table</span><span class="o">)</span>

<span class="c1">// convert the Table into a retract DataStream of Row.
</span><span class="c1">//   A retract stream of type X is a DataStream[(Boolean, X)]. 
</span><span class="c1">//   The boolean field indicates the type of the change. 
</span><span class="c1">//   True is INSERT, false is DELETE.
</span><span class="c1"></span><span class="k">val</span> <span class="n">retractStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Boolean</span>, <span class="kt">Row</span><span class="o">)]</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">toRetractStream</span><span class="o">[</span><span class="kt">Row</span><span class="o">](</span><span class="n">table</span><span class="o">)</span>
</code></pre></div><p>注意：关于动态表及其属性的详细讨论在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/dynamic_tables.html">动态表</a>文档中给出。</p>
<p>注意: 一旦表转换为 DataStream，请使用 <code>StreamExecutionEnvironment.execute()</code> 方法来执行 DataStream 程序。</p>
<h3 id="将-table-转换为-dataset">将 Table 转换为 DataSet</h3>
<p>Table 转换为 DataStream 的过程如下:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get TableEnvironment 
</span><span class="c1">// registration of a DataSet is equivalent
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">BatchTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="c1">// Table with two fields (String name, Integer age)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1">// convert the Table into a DataSet of Row
</span><span class="c1"></span><span class="k">val</span> <span class="n">dsRow</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">toDataSet</span><span class="o">[</span><span class="kt">Row</span><span class="o">](</span><span class="n">table</span><span class="o">)</span>

<span class="c1">// convert the Table into a DataSet of Tuple2[String, Int]
</span><span class="c1"></span><span class="k">val</span> <span class="n">dsTuple</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">toDataSet</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)](</span><span class="n">table</span><span class="o">)</span>
</code></pre></div><p>注意: 一旦 Table 转换为 DataSet，我们必须使用 <code>ExecutionEnvironment.execute</code> 方法来执行 DataSet 程序。</p>
<h3 id="数据类型到-table-schema-的映射">数据类型到 Table Schema 的映射</h3>
<p>Flink 的 DataStream 和 DataSet API 支持非常多样化的类型。复合类型，如 Tuples（内置的 Scala 和 Flink Java tuples）、POJOs、Scala case 类和 Flink 的 Row 类型，允许嵌套具有多个字段的数据结构，这些字段可以在 Table 表达式中访问。其他类型被视为原子类型。在下文中，我们将描述 Table API 如何将这些类型转换为内部行表示，并展示将 DataStream 转换为 Table 的例子。</p>
<p>数据类型到 Table Schema 的映射可以通过两种方式进行：基于字段位置或基于字段名。</p>
<ul>
<li>基于位置的映射</li>
</ul>
<p>基于位置的映射可以用来给字段一个更有意义的名字，同时保持字段顺序。这种映射可用于具有定义字段顺序的复合数据类型以及原子类型。复合数据类型如元组、行和 case 类都有这样的字段顺序。然而，POJO 的字段必须根据字段名进行映射（见下一节）。字段可以被投影出来，但不能使用别名作为重命名。</p>
<p>当定义基于位置的映射时，指定的名称必须不存在于输入数据类型中，否则 API 将假设映射应该基于字段名发生。如果没有指定字段名，则使用复合类型的默认字段名和字段顺序，对于原子类型则使用 f0。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span><span class="k">:</span> <span class="kt">StreamTableEnvironment</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// convert DataStream into Table with default field names &#34;_1&#34; and &#34;_2&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with field &#34;myLong&#34; only
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myLong&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with field names &#34;myLong&#34; and &#34;myInt&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myLong&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myInt&#34;</span><span class="o">)</span>
</code></pre></div><ul>
<li>基于名称的映射</li>
</ul>
<p>基于名称的映射可以用于任何数据类型，包括 POJO。它是定义表模式映射的最灵活的方式。映射中的所有字段都是通过名称引用的，并可能使用别名重命名为。字段可以重新排序和投影出来。</p>
<p>如果没有指定字段名，则使用复合类型的默认字段名和字段顺序，对于原子类型则使用 f0。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span><span class="k">:</span> <span class="kt">StreamTableEnvironment</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// convert DataStream into Table with default field names &#34;_1&#34; and &#34;_2&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with field &#34;_2&#34; only
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;_2&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with swapped fields
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;_2&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;_1&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with swapped fields and field names &#34;myInt&#34; and &#34;myLong&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;_2&#34;</span> <span class="n">as</span> <span class="s">&#34;myInt&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;_1&#34;</span> <span class="n">as</span> <span class="s">&#34;myLong&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="原子类型">原子类型</h3>
<p>Flink 将原语（Integer、Double、String）或通用类型（不能分析和分解的类型）视为原子类型。原子类型的 DataStream 或 DataSet 会被转换为具有单一属性的 Table。属性的类型是从原子类型推断出来的，可以指定属性的名称。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span><span class="k">:</span> <span class="kt">StreamTableEnvironment</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// convert DataStream into Table with default field name &#34;f0&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with field name &#34;myLong&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myLong&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="tuplesscala-和-java和-case-类仅-scala">Tuples（Scala 和 Java）和 Case 类（仅 Scala）。</h3>
<p>Flink 支持 Scala 的内置元组，并为 Java 提供了自己的元组类。DataStreams 和 DataSets 这两种元组都可以转换为表。通过为所有字段提供名称（基于位置的映射），可以重命名字段。如果没有指定字段名，则使用默认的字段名。如果引用了原始的字段名（对于 Flink Tuples 来说是 f0, f1, &hellip;，对于 Scala Tuples 来说是 _1, _2, &hellip;），API 会假定映射是基于名称而不是基于位置的。基于名称的映射允许重新排序字段和用别名（as）进行投影。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span><span class="k">:</span> <span class="kt">StreamTableEnvironment</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// convert DataStream into Table with renamed default field names &#39;_1, &#39;_2
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with field names &#34;myLong&#34;, &#34;myString&#34; (position-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myLong&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myString&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with reordered fields &#34;_2&#34;, &#34;_1&#34; (name-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;_2&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;_1&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with projected field &#34;_2&#34; (name-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;_2&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with reordered and aliased fields &#34;myString&#34;, &#34;myLong&#34; (name-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;_2&#34;</span> <span class="n">as</span> <span class="s">&#34;myString&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;_1&#34;</span> <span class="n">as</span> <span class="s">&#34;myLong&#34;</span><span class="o">)</span>

<span class="c1">// define case class
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">Person</span><span class="o">(</span><span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">age</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
<span class="k">val</span> <span class="n">streamCC</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Person</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// convert DataStream into Table with default field names &#39;name, &#39;age
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">streamCC</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with field names &#39;myName, &#39;myAge (position-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">streamCC</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myName&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myAge&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with reordered and aliased fields &#34;myAge&#34;, &#34;myName&#34; (name-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;age&#34;</span> <span class="n">as</span> <span class="s">&#34;myAge&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;name&#34;</span> <span class="n">as</span> <span class="s">&#34;myName&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="pojojava-和-scala">POJO（Java 和 Scala）</h3>
<p>Flink 支持 POJO 作为复合类型。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html#pojos">这里</a>记录了确定 POJO 的规则。</p>
<p>当将 POJO DataStream 或 DataSet 转换为 Table 而不指定字段名时，会使用原始 POJO 字段的名称。名称映射需要原始名称，不能通过位置来完成。字段可以使用别名（使用 as 关键字）重命名，重新排序，并进行投影。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span><span class="k">:</span> <span class="kt">StreamTableEnvironment</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="c1">// Person is a POJO with field names &#34;name&#34; and &#34;age&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Person</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// convert DataStream into Table with default field names &#34;age&#34;, &#34;name&#34; (fields are ordered by name!)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with renamed fields &#34;myAge&#34;, &#34;myName&#34; (name-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;age&#34;</span> <span class="n">as</span> <span class="s">&#34;myAge&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;name&#34;</span> <span class="n">as</span> <span class="s">&#34;myName&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with projected field &#34;name&#34; (name-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;name&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with projected and renamed field &#34;myName&#34; (name-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;name&#34;</span> <span class="n">as</span> <span class="s">&#34;myName&#34;</span><span class="o">)</span>
</code></pre></div><h3 id="row">Row</h3>
<p>Row 数据类型支持任意数量的字段和具有 null 值的字段。字段名可以通过 RowTypeInfo 来指定，也可以在将 Row DataStream 或 DataSet 转换为 Table 时指定。Row 类型支持通过位置和名称对字段进行映射。可以通过为所有字段提供名称（基于位置的映射）或单独选择字段进行投影/排序/重命名（基于名称的映射）来重命名字段。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// get a TableEnvironment
</span><span class="c1"></span><span class="k">val</span> <span class="n">tableEnv</span><span class="k">:</span> <span class="kt">StreamTableEnvironment</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// see &#34;Create a TableEnvironment&#34; section
</span><span class="c1"></span>
<span class="c1">// DataStream of Row with two fields &#34;name&#34; and &#34;age&#34; specified in `RowTypeInfo`
</span><span class="c1"></span><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// convert DataStream into Table with default field names &#34;name&#34;, &#34;age&#34;
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with renamed field names &#34;myName&#34;, &#34;myAge&#34; (position-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myName&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myAge&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with renamed fields &#34;myName&#34;, &#34;myAge&#34; (name-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;name&#34;</span> <span class="n">as</span> <span class="s">&#34;myName&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;age&#34;</span> <span class="n">as</span> <span class="s">&#34;myAge&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with projected field &#34;name&#34; (name-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;name&#34;</span><span class="o">)</span>

<span class="c1">// convert DataStream into Table with projected and renamed field &#34;myName&#34; (name-based)
</span><span class="c1"></span><span class="k">val</span> <span class="n">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">fromDataStream</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;name&#34;</span> <span class="n">as</span> <span class="s">&#34;myName&#34;</span><span class="o">)</span>
</code></pre></div><h2 id="查询优化">查询优化</h2>
<ul>
<li>Blink 计划器</li>
</ul>
<p>Apache Flink 利用并扩展了 Apache Calcite 来执行复杂的查询优化。这包括一系列基于规则和成本的优化，如：</p>
<ul>
<li>基于 Apache Calcite 的子查询装饰相关。</li>
<li>投影修剪</li>
<li>分区修剪</li>
<li>过滤器下推</li>
<li>子计划重复复制，避免重复计算。</li>
<li>特殊子查询重写，包括两部分。
<ul>
<li>将 IN 和 EXISTS 转换为左半连接。</li>
<li>将 NOT IN 和 NOT EXISTS 转换为左反连接。</li>
</ul>
</li>
<li>可选的 join 重新排序
<ul>
<li>通过 <code>table.optimizer.join-reorder-enabled</code> 启用。</li>
</ul>
</li>
</ul>
<p>注：<code>IN/EXISTS/NOT IN/NOT EXISTS</code> 目前只支持子查询重写中的连词条件。</p>
<p>优化器做出智能决策，不仅基于计划，还基于数据源提供的丰富统计数据，以及每个操作符（如 io、cpu、网络和内存）的细粒度成本。</p>
<p>高级用户可以通过 CalciteConfig 对象提供自定义优化，该对象可以通过调用 <code>TableEnvironment#getConfig#setPlannerConfig</code> 提供给 table 环境。</p>
<h2 id="解释表">解释表</h2>
<p>Table API 提供了一种机制来解释计算 Table 的逻辑和优化查询计划。这是通过 <code>Table.explain()</code> 方法或 <code>StatementSet.explain()</code> 方法完成的。<code>Table.explain()</code> 返回一个 Table 的计划。<code>StatementSet.explain()</code> 返回多个接收器的计划。它返回一个描述三个计划的字符串。</p>
<ol>
<li>关系查询的抽象语法树，即未优化的逻辑查询计划。</li>
<li>优化的逻辑查询计划，以及</li>
<li>物理执行计划。</li>
</ol>
<p><code>TableEnvironment.explainSql()</code> 和 <code>TableEnvironment.executeSql()</code> 支持执行 EXPLAIN 语句来获取计划，请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/explain.html">EXPLAIN</a> 页面。</p>
<p>下面的代码显示了一个使用 <code>Table.explain()</code> 方法给定 Table 的例子和相应的输出。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>
<span class="k">val</span> <span class="n">tEnv</span> <span class="k">=</span> <span class="nc">StreamTableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">env</span><span class="o">)</span>

<span class="k">val</span> <span class="n">table1</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span> <span class="s">&#34;hello&#34;</span><span class="o">)).</span><span class="n">toTable</span><span class="o">(</span><span class="n">tEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;count&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;word&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">table2</span> <span class="k">=</span> <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span> <span class="s">&#34;hello&#34;</span><span class="o">)).</span><span class="n">toTable</span><span class="o">(</span><span class="n">tEnv</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;count&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;word&#34;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">table</span> <span class="k">=</span> <span class="n">table1</span>
  <span class="o">.</span><span class="n">where</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;word&#34;</span><span class="o">.</span><span class="n">like</span><span class="o">(</span><span class="s">&#34;F%&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">unionAll</span><span class="o">(</span><span class="n">table2</span><span class="o">)</span>
<span class="n">println</span><span class="o">(</span><span class="n">table</span><span class="o">.</span><span class="n">explain</span><span class="o">())</span>
</code></pre></div><p>上述例子的结果是:</p>
<pre><code>== Abstract Syntax Tree ==
LogicalUnion(all=[true])
  LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')])
    FlinkLogicalDataStreamScan(id=[1], fields=[count, word])
  FlinkLogicalDataStreamScan(id=[2], fields=[count, word])

== Optimized Logical Plan ==
DataStreamUnion(all=[true], union all=[count, word])
  DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')])
    DataStreamScan(id=[1], fields=[count, word])
  DataStreamScan(id=[2], fields=[count, word])

== Physical Execution Plan ==
Stage 1 : Data Source
	content : collect elements with CollectionInputFormat

Stage 2 : Data Source
	content : collect elements with CollectionInputFormat

	Stage 3 : Operator
		content : from: (count, word)
		ship_strategy : REBALANCE

		Stage 4 : Operator
			content : where: (LIKE(word, _UTF-16LE'F%')), select: (count, word)
			ship_strategy : FORWARD

			Stage 5 : Operator
				content : from: (count, word)
				ship_strategy : REBALANCE
</code></pre><p>下面的代码显示了使用 <code>StatementSet.explain()</code> 方法进行多重接收器计划的一个例子和相应的输出。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">settings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">.</span><span class="n">useBlinkPlanner</span><span class="o">.</span><span class="n">inStreamingMode</span><span class="o">.</span><span class="n">build</span>
<span class="k">val</span> <span class="n">tEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">settings</span><span class="o">)</span>

<span class="k">val</span> <span class="n">schema</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Schema</span><span class="o">()</span>
    <span class="o">.</span><span class="n">field</span><span class="o">(</span><span class="s">&#34;count&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">INT</span><span class="o">())</span>
    <span class="o">.</span><span class="n">field</span><span class="o">(</span><span class="s">&#34;word&#34;</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="nc">STRING</span><span class="o">())</span>

<span class="n">tEnv</span><span class="o">.</span><span class="n">connect</span><span class="o">(</span><span class="k">new</span> <span class="nc">FileSystem</span><span class="o">(</span><span class="s">&#34;/source/path1&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">withFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Csv</span><span class="o">().</span><span class="n">deriveSchema</span><span class="o">())</span>
    <span class="o">.</span><span class="n">withSchema</span><span class="o">(</span><span class="n">schema</span><span class="o">)</span>
    <span class="o">.</span><span class="n">createTemporaryTable</span><span class="o">(</span><span class="s">&#34;MySource1&#34;</span><span class="o">)</span>
<span class="n">tEnv</span><span class="o">.</span><span class="n">connect</span><span class="o">(</span><span class="k">new</span> <span class="nc">FileSystem</span><span class="o">(</span><span class="s">&#34;/source/path2&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">withFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Csv</span><span class="o">().</span><span class="n">deriveSchema</span><span class="o">())</span>
    <span class="o">.</span><span class="n">withSchema</span><span class="o">(</span><span class="n">schema</span><span class="o">)</span>
    <span class="o">.</span><span class="n">createTemporaryTable</span><span class="o">(</span><span class="s">&#34;MySource2&#34;</span><span class="o">)</span>
<span class="n">tEnv</span><span class="o">.</span><span class="n">connect</span><span class="o">(</span><span class="k">new</span> <span class="nc">FileSystem</span><span class="o">(</span><span class="s">&#34;/sink/path1&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">withFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Csv</span><span class="o">().</span><span class="n">deriveSchema</span><span class="o">())</span>
    <span class="o">.</span><span class="n">withSchema</span><span class="o">(</span><span class="n">schema</span><span class="o">)</span>
    <span class="o">.</span><span class="n">createTemporaryTable</span><span class="o">(</span><span class="s">&#34;MySink1&#34;</span><span class="o">)</span>
<span class="n">tEnv</span><span class="o">.</span><span class="n">connect</span><span class="o">(</span><span class="k">new</span> <span class="nc">FileSystem</span><span class="o">(</span><span class="s">&#34;/sink/path2&#34;</span><span class="o">))</span>
    <span class="o">.</span><span class="n">withFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Csv</span><span class="o">().</span><span class="n">deriveSchema</span><span class="o">())</span>
    <span class="o">.</span><span class="n">withSchema</span><span class="o">(</span><span class="n">schema</span><span class="o">)</span>
    <span class="o">.</span><span class="n">createTemporaryTable</span><span class="o">(</span><span class="s">&#34;MySink2&#34;</span><span class="o">)</span>
    
<span class="k">val</span> <span class="n">stmtSet</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">createStatementSet</span><span class="o">()</span>

<span class="k">val</span> <span class="n">table1</span> <span class="k">=</span> <span class="n">tEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MySource1&#34;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;word&#34;</span><span class="o">.</span><span class="n">like</span><span class="o">(</span><span class="s">&#34;F%&#34;</span><span class="o">))</span>
<span class="n">stmtSet</span><span class="o">.</span><span class="n">addInsert</span><span class="o">(</span><span class="s">&#34;MySink1&#34;</span><span class="o">,</span> <span class="n">table1</span><span class="o">)</span>

<span class="k">val</span> <span class="n">table2</span> <span class="k">=</span> <span class="n">table1</span><span class="o">.</span><span class="n">unionAll</span><span class="o">(</span><span class="n">tEnv</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MySource2&#34;</span><span class="o">))</span>
<span class="n">stmtSet</span><span class="o">.</span><span class="n">addInsert</span><span class="o">(</span><span class="s">&#34;MySink2&#34;</span><span class="o">,</span> <span class="n">table2</span><span class="o">)</span>

<span class="k">val</span> <span class="n">explanation</span> <span class="k">=</span> <span class="n">stmtSet</span><span class="o">.</span><span class="n">explain</span><span class="o">()</span>
<span class="n">println</span><span class="o">(</span><span class="n">explanation</span><span class="o">)</span>
</code></pre></div><p>多重接收器计划的结果是:</p>
<pre><code>== Abstract Syntax Tree ==
LogicalLegacySink(name=[MySink1], fields=[count, word])
+- LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')])
   +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]])

LogicalLegacySink(name=[MySink2], fields=[count, word])
+- LogicalUnion(all=[true])
   :- LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')])
   :  +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]])
   +- LogicalTableScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]])

== Optimized Logical Plan ==
Calc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')], reuse_id=[1])
+- TableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word])

LegacySink(name=[MySink1], fields=[count, word])
+- Reused(reference_id=[1])

LegacySink(name=[MySink2], fields=[count, word])
+- Union(all=[true], union=[count, word])
   :- Reused(reference_id=[1])
   +- TableSourceScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word])

== Physical Execution Plan ==
Stage 1 : Data Source
	content : collect elements with CollectionInputFormat

	Stage 2 : Operator
		content : CsvTableSource(read fields: count, word)
		ship_strategy : REBALANCE

		Stage 3 : Operator
			content : SourceConversion(table:Buffer(default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]), fields:(count, word))
			ship_strategy : FORWARD

			Stage 4 : Operator
				content : Calc(where: (word LIKE _UTF-16LE'F%'), select: (count, word))
				ship_strategy : FORWARD

				Stage 5 : Operator
					content : SinkConversionToRow
					ship_strategy : FORWARD

					Stage 6 : Operator
						content : Map
						ship_strategy : FORWARD

Stage 8 : Data Source
	content : collect elements with CollectionInputFormat

	Stage 9 : Operator
		content : CsvTableSource(read fields: count, word)
		ship_strategy : REBALANCE

		Stage 10 : Operator
			content : SourceConversion(table:Buffer(default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]), fields:(count, word))
			ship_strategy : FORWARD

			Stage 12 : Operator
				content : SinkConversionToRow
				ship_strategy : FORWARD

				Stage 13 : Operator
					content : Map
					ship_strategy : FORWARD

					Stage 7 : Data Sink
						content : Sink: CsvTableSink(count, word)
						ship_strategy : FORWARD

						Stage 14 : Data Sink
							content : Sink: CsvTableSink(count, word)
							ship_strategy : FORWARD
</code></pre><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[模块]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-modules/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-modules/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Modules</blockquote><h1 id="模块测试版">模块测试版</h1>
<p>模块允许用户扩展 Flink 的内置对象，比如定义一些行为类似 Flink 内置函数的功能。它们是可插拔的，虽然 Flink 提供了一些预建模块，但用户可以编写自己的模块。</p>
<p>例如，用户可以定义自己的地理函数，并将其作为内置函数插入 Flink，以便在 Flink SQL 和 Table API 中使用。又比如，用户可以加载一个现成的 Hive 模块，将 Hive 内置函数作为 Flink 内置函数使用。</p>
<h2 id="模块类型">模块类型</h2>
<h3 id="coremodule">CoreModule</h3>
<p>CoreModule 包含了 Flink 的所有系统（内置）功能，并且默认被加载。</p>
<h3 id="hivemodule">HiveModule</h3>
<p>HiveModule 作为 Flink 的系统函数，向 SQL 和 Table API 用户提供 Hive 内置函数。Flink 的 Hive 文档提供了设置该模块的全部细节。</p>
<h3 id="用户定义模块">用户定义模块</h3>
<p>用户可以通过实现 Module 接口来开发自定义模块。为了在 SQL CLI 中使用自定义模块，用户应该通过实现 ModuleFactory 接口同时开发一个模块和它对应的模块工厂。</p>
<p>模块工厂定义了一组属性，用于在 SQL CLI 引导时配置模块。属性被传递给发现服务，服务会尝试将属性与模块工厂进行匹配，并实例化一个相应的模块实例。</p>
<h2 id="命名空间和解析顺序">命名空间和解析顺序</h2>
<p>模块提供的对象被认为是 Flink 系统（内置）对象的一部分；因此，它们没有任何命名空间。</p>
<p>当有两个同名的对象存在于两个模块中时，Flink 总是将对象引用解析为第一个加载模块中的对象。</p>
<h2 id="模块-api">模块 API</h2>
<h3 id="装载和卸载模块">装载和卸载模块</h3>
<p>用户可以在现有的 Flink 会话中加载和卸载模块。</p>
<ul>
<li>Scala</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">tableEnv</span><span class="o">.</span><span class="n">loadModule</span><span class="o">(</span><span class="s">&#34;myModule&#34;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">CustomModule</span><span class="o">());</span>
<span class="n">tableEnv</span><span class="o">.</span><span class="n">unloadModule</span><span class="o">(</span><span class="s">&#34;myModule&#34;</span><span class="o">);</span>
</code></pre></div><ul>
<li>YAML</li>
</ul>
<p>所有使用 YAML 定义的模块都必须提供一个 <code>type</code> 属性来指定类型。以下类型是开箱即用的。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Catalog</th>
<th style="text-align:left">Type Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">CoreModule</td>
<td style="text-align:left">core</td>
</tr>
<tr>
<td style="text-align:left">HiveModule</td>
<td style="text-align:left">hive</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">modules</span><span class="p">:</span><span class="w">
</span><span class="w">   </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">core</span><span class="w">
</span><span class="w">     </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">core</span><span class="w">
</span><span class="w">   </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myhive</span><span class="w">
</span><span class="w">     </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">hive</span><span class="w">
</span></code></pre></div><h3 id="列出可用的模块">列出可用的模块</h3>
<ul>
<li>Scala</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">tableEnv</span><span class="o">.</span><span class="n">listModules</span><span class="o">();</span>
</code></pre></div><ul>
<li>SQL</li>
</ul>
<pre><code>Flink SQL&gt; SHOW MODULES;
</code></pre><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/modules.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/modules.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/module" term="module" label="Module" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[流的概念]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-streaming-concepts/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-streaming-concepts/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Streaming Concepts</blockquote><h2 id="流的概念">流的概念</h2>
<p>Flink 的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html">Table API</a>和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/index.html">SQL 支持</a>是批处理和流处理的统一 API。这意味着Table API 和 SQL 查询具有相同的语义，无论其输入是有界批处理输入还是无界流输入。由于关系代数和 SQL 最初是为批处理设计的，所以对无界流输入的关系查询不如对有界批输入的关系查询好理解。</p>
<p>下面几页解释了 Flink 的关系 API 在流数据上的概念、实际限制和特定流的配置参数。</p>
<h2 id="下一步该往哪里走">下一步该往哪里走？</h2>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/dynamic_tables.html">动态表</a>。描述动态表的概念。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html">时间属性</a>。解释时间属性，以及在表API和SQL中如何处理时间属性。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html">连续查询中的连接</a>。连续查询中支持的不同类型的连接。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">临时表</a>。描述临时表的概念。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/query_configuration.html">查询配置</a>。列出 Table API 和 SQL 特定配置选项。</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[测试]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-testing/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-testing/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Testing</blockquote><h1 id="测试">测试</h1>
<p>测试是每个软件开发过程中不可缺少的一部分，因此 Apache Flink 提供的工具可以在测试金字塔的多个层次上测试你的应用程序代码。</p>
<h2 id="测试用户自定义函数">测试用户自定义函数</h2>
<p>通常，我们可以假设 Flink 在用户定义的函数之外产生正确的结果。因此，建议尽可能用单元测试来测试那些包含主要业务逻辑的类。</p>
<h3 id="单元测试无状态timeless-udfs">单元测试无状态、Timeless UDFs。</h3>
<p>例如，我们来看看下面的无状态 MapFunction。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">IncrementMapFunction</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">]</span> <span class="o">{</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">record</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">record</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>通过传递合适的参数和验证输出，用你最喜欢的测试框架对这样的函数进行单元测试是非常容易的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">IncrementMapFunctionTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">Matchers</span> <span class="o">{</span>

    <span class="s">&#34;IncrementMapFunction&#34;</span> <span class="n">should</span> <span class="s">&#34;increment values&#34;</span> <span class="n">in</span> <span class="o">{</span>
        <span class="c1">// instantiate your function
</span><span class="c1"></span>        <span class="k">val</span> <span class="n">incrementer</span><span class="k">:</span> <span class="kt">IncrementMapFunction</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">IncrementMapFunction</span><span class="o">()</span>

        <span class="c1">// call the methods that you have implemented
</span><span class="c1"></span>        <span class="n">incremeter</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span> <span class="n">should</span> <span class="n">be</span> <span class="o">(</span><span class="mi">3</span><span class="o">)</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>同样，使用 org.apache.flink.util.Collector 的用户定义函数（例如 FlatMapFunction 或 ProcessFunction）可以通过提供一个模拟对象而不是真实的 Collector 来轻松测试。一个与 IncrementMapFunction 功能相同的 FlatMapFunction 可以进行如下单元测试。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">IncrementFlatMapFunctionTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">MockFactory</span> <span class="o">{</span>

    <span class="s">&#34;IncrementFlatMapFunction&#34;</span> <span class="n">should</span> <span class="s">&#34;increment values&#34;</span> <span class="n">in</span> <span class="o">{</span>
       <span class="c1">// instantiate your function
</span><span class="c1"></span>      <span class="k">val</span> <span class="n">incrementer</span> <span class="k">:</span> <span class="kt">IncrementFlatMapFunction</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">IncrementFlatMapFunction</span><span class="o">()</span>

      <span class="k">val</span> <span class="n">collector</span> <span class="k">=</span> <span class="n">mock</span><span class="o">[</span><span class="kt">Collector</span><span class="o">[</span><span class="kt">Integer</span><span class="o">]]</span>

      <span class="c1">//verify collector was called with the right output
</span><span class="c1"></span>      <span class="o">(</span><span class="n">collector</span><span class="o">.</span><span class="n">collect</span> <span class="k">_</span><span class="o">).</span><span class="n">expects</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>

      <span class="c1">// call the methods that you have implemented
</span><span class="c1"></span>      <span class="n">flattenFunction</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="n">collector</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="单元测试-有状态或及时的-udf-和自定义操作符">单元测试 有状态或及时的 UDF 和自定义操作符</h3>
<p>测试一个用户定义函数的功能是比较困难的，因为它涉及到测试用户代码和 Flink 运行时之间的交互。为此，Flink 提供了一个所谓的测试线束的集合，它可以用来测试这样的用户定义函数以及自定义操作符。</p>
<ul>
<li>OneInputStreamOperatorTestHarness(用于 DataStreams 上的操作符)</li>
<li>KeyedOneInputStreamOperatorTestHarness(用于 KeyedStreams 上的操作者)</li>
<li>TwoInputStreamOperatorTestHarness (适用于两个 DataStreams 的 ConnectedStreams 操作者)</li>
<li>KeyedTwoInputStreamOperatorTestHarness (用于两个 KeyedStream 的 ConnectedStreams 上的操作员)</li>
</ul>
<p>为了使用测试套件，需要一组额外的依赖关系（测试范围）。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-test-utils_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>test<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-runtime_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>test<span class="nt">&lt;/scope&gt;</span>
  <span class="nt">&lt;classifier&gt;</span>tests<span class="nt">&lt;/classifier&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-streaming-java_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>test<span class="nt">&lt;/scope&gt;</span>
  <span class="nt">&lt;classifier&gt;</span>tests<span class="nt">&lt;/classifier&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>现在，测试线束可以用来将记录和水印推送到你的用户定义函数或自定义运算符中，控制处理时间，最后对运算符的输出进行断言（包括侧输出）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">StatefulFlatMapFunctionTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">Matchers</span> <span class="k">with</span> <span class="nc">BeforeAndAfter</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">var</span> <span class="n">testHarness</span><span class="k">:</span> <span class="kt">OneInputStreamOperatorTestHarness</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span>
  <span class="k">private</span> <span class="k">var</span> <span class="n">statefulFlatMap</span><span class="k">:</span> <span class="kt">StatefulFlatMapFunction</span> <span class="o">=</span> <span class="kc">null</span>

  <span class="n">before</span> <span class="o">{</span>
    <span class="c1">//instantiate user-defined function
</span><span class="c1"></span>    <span class="n">statefulFlatMap</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StatefulFlatMap</span>

    <span class="c1">// wrap user defined function into a the corresponding operator
</span><span class="c1"></span>    <span class="n">testHarness</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">OneInputStreamOperatorTestHarness</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">](</span><span class="k">new</span> <span class="nc">StreamFlatMap</span><span class="o">(</span><span class="n">statefulFlatMap</span><span class="o">))</span>

    <span class="c1">// optionally configured the execution environment
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">getExecutionConfig</span><span class="o">().</span><span class="n">setAutoWatermarkInterval</span><span class="o">(</span><span class="mi">50</span><span class="o">);</span>

    <span class="c1">// open the test harness (will also call open() on RichFunctions)
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">open</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="s">&#34;StatefulFlatMap&#34;</span> <span class="n">should</span> <span class="s">&#34;do some fancy stuff with timers and state&#34;</span> <span class="n">in</span> <span class="o">{</span>


    <span class="c1">//push (timestamped) elements into the operator (and hence user defined function)
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">processElement</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">100</span><span class="o">);</span>

    <span class="c1">//trigger event time timers by advancing the event time of the operator with a watermark
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">processWatermark</span><span class="o">(</span><span class="mi">100</span><span class="o">);</span>

    <span class="c1">//trigger proccesign time timers by advancing the processing time of the operator directly
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">setProcessingTime</span><span class="o">(</span><span class="mi">100</span><span class="o">);</span>

    <span class="c1">//retrieve list of emitted records for assertions
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">getOutput</span> <span class="n">should</span> <span class="n">contain</span> <span class="o">(</span><span class="mi">3</span><span class="o">)</span>

    <span class="c1">//retrieve list of records emitted to a specific side output for assertions (ProcessFunction only)
</span><span class="c1"></span>    <span class="c1">//testHarness.getSideOutput(new OutputTag[Int](&#34;invalidRecords&#34;)) should have size 0
</span><span class="c1"></span>  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>KeyedOneInputStreamOperatorTestHarness 和 KeyedTwoInputStreamOperatorTestHarness 是通过额外提供一个包括键类的 TypeInformation 的 KeySelector 来实例化的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">StatefulFlatMapTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">Matchers</span> <span class="k">with</span> <span class="nc">BeforeAndAfter</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">var</span> <span class="n">testHarness</span><span class="k">:</span> <span class="kt">OneInputStreamOperatorTestHarness</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Long</span>, <span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span>
  <span class="k">private</span> <span class="k">var</span> <span class="n">statefulFlatMapFunction</span><span class="k">:</span> <span class="kt">FlattenFunction</span> <span class="o">=</span> <span class="kc">null</span>

  <span class="n">before</span> <span class="o">{</span>
    <span class="c1">//instantiate user-defined function
</span><span class="c1"></span>    <span class="n">statefulFlatMapFunction</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StateFulFlatMap</span>

    <span class="c1">// wrap user defined function into a the corresponding operator
</span><span class="c1"></span>    <span class="n">testHarness</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">KeyedOneInputStreamOperatorTestHarness</span><span class="o">(</span><span class="k">new</span> <span class="nc">StreamFlatMap</span><span class="o">(</span><span class="n">statefulFlatMapFunction</span><span class="o">),</span><span class="k">new</span> <span class="nc">MyStringKeySelector</span><span class="o">(),</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">STRING</span><span class="o">())</span>

    <span class="c1">// open the test harness (will also call open() on RichFunctions)
</span><span class="c1"></span>    <span class="n">testHarness</span><span class="o">.</span><span class="n">open</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="c1">//tests
</span><span class="c1"></span>
<span class="o">}</span>
</code></pre></div><p>在 Flink 代码库中还可以找到更多使用这些测试线束的例子，例如。</p>
<ul>
<li>org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest 是一个很好的例子，用于测试依赖于处理或事件时间的操作员和用户定义的函数。</li>
<li>org.apache.flink.streaming.api.function.sink.filesystem.LocalStreamingFileSinkTest 展示了如何使用 AbstractStreamOperatorTestHarness 测试自定义的 sink。具体来说，它使用 AbstractStreamOperatorTestHarness.snapshot 和 AbstractStreamOperatorTestHarness.initializeState 来测试它与 Flink 的检查点机制的交互。</li>
</ul>
<p>注意: AbstractStreamOperatorTestHarness 和它的派生类目前不是公共 API 的一部分，可能会发生变化。</p>
<h3 id="单元测试-processfunction">单元测试 ProcessFunction</h3>
<p>鉴于其重要性，除了之前的测试线束可以直接用于测试 ProcessFunction 外，Flink 还提供了一个名为 ProcessFunctionTestHarnesses 的测试线束工厂，可以更方便地进行测试线束实例化。考虑到这个例子。</p>
<p>注意: 要使用这个测试线束，你还需要引入上一节中提到的依赖关系。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">PassThroughProcessFunction</span> <span class="k">extends</span> <span class="nc">ProcessFunction</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">Integer</span><span class="o">]</span> <span class="o">{</span>

    <span class="nd">@throws</span><span class="o">[</span><span class="kt">Exception</span><span class="o">]</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">processElement</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">,</span> <span class="n">ctx</span><span class="k">:</span> <span class="kt">ProcessFunction</span><span class="o">[</span><span class="kt">Integer</span>, <span class="kt">Integer</span><span class="o">]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">Integer</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">value</span><span class="o">)</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>使用 ProcessFunctionTestHarnesses 对这样的函数进行单元测试是非常容易的，通过传递合适的参数并验证输出。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">PassThroughProcessFunctionTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">Matchers</span> <span class="o">{</span>

  <span class="s">&#34;PassThroughProcessFunction&#34;</span> <span class="n">should</span> <span class="s">&#34;forward values&#34;</span> <span class="n">in</span> <span class="o">{</span>

    <span class="c1">//instantiate user-defined function
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">processFunction</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">PassThroughProcessFunction</span>

    <span class="c1">// wrap user defined function into a the corresponding operator
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">harness</span> <span class="k">=</span> <span class="nc">ProcessFunctionTestHarnesses</span><span class="o">.</span><span class="n">forProcessFunction</span><span class="o">(</span><span class="n">processFunction</span><span class="o">)</span>

    <span class="c1">//push (timestamped) elements into the operator (and hence user defined function)
</span><span class="c1"></span>    <span class="n">harness</span><span class="o">.</span><span class="n">processElement</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">10</span><span class="o">)</span>

    <span class="c1">//retrieve list of emitted records for assertions
</span><span class="c1"></span>    <span class="n">harness</span><span class="o">.</span><span class="n">extractOutputValues</span><span class="o">()</span> <span class="n">should</span> <span class="n">contain</span> <span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>关于如何使用 ProcessFunctionTestHarnesses 来测试 ProcessFunction 的不同风味，如 KeyedProcessFunction、KeyedCoProcessFunction、BroadcastProcessFunction 等的更多例子，鼓励用户查看 ProcessFunctionTestHarnessesTest。</p>
<h2 id="测试-flink-作业">测试 Flink 作业</h2>
<h3 id="junit-规则-miniclusterwithclientresource">JUnit 规则 MiniClusterWithClientResource</h3>
<p>Apache Flink 提供了一个名为 MiniClusterWithClientResource 的 JUnit 规则，用于针对本地的、嵌入式的迷你集群测试完整的作业，名为 MiniClusterWithClientResource。</p>
<p>要使用 MiniClusterWithClientResource，需要一个额外的依赖（测试范围）。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-test-utils_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>让我们以前面几节中同样简单的 MapFunction 为例。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">IncrementMapFunction</span> <span class="k">extends</span> <span class="nc">MapFunction</span><span class="o">[</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">]</span> <span class="o">{</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">record</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">record</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>现在可以在本地 Flink 集群中测试使用该 MapFunction 的简单管道，具体如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">StreamingJobIntegrationTest</span> <span class="k">extends</span> <span class="nc">FlatSpec</span> <span class="k">with</span> <span class="nc">Matchers</span> <span class="k">with</span> <span class="nc">BeforeAndAfter</span> <span class="o">{</span>

  <span class="k">val</span> <span class="n">flinkCluster</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MiniClusterWithClientResource</span><span class="o">(</span><span class="k">new</span> <span class="nc">MiniClusterResourceConfiguration</span><span class="o">.</span><span class="nc">Builder</span><span class="o">()</span>
    <span class="o">.</span><span class="n">setNumberSlotsPerTaskManager</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
    <span class="o">.</span><span class="n">setNumberTaskManagers</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
    <span class="o">.</span><span class="n">build</span><span class="o">)</span>

  <span class="n">before</span> <span class="o">{</span>
    <span class="n">flinkCluster</span><span class="o">.</span><span class="n">before</span><span class="o">()</span>
  <span class="o">}</span>

  <span class="n">after</span> <span class="o">{</span>
    <span class="n">flinkCluster</span><span class="o">.</span><span class="n">after</span><span class="o">()</span>
  <span class="o">}</span>


  <span class="s">&#34;IncrementFlatMapFunction pipeline&#34;</span> <span class="n">should</span> <span class="s">&#34;incrementValues&#34;</span> <span class="n">in</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

    <span class="c1">// configure your test environment
</span><span class="c1"></span>    <span class="n">env</span><span class="o">.</span><span class="n">setParallelism</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>

    <span class="c1">// values are collected in a static variable
</span><span class="c1"></span>    <span class="nc">CollectSink</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">clear</span><span class="o">()</span>

    <span class="c1">// create a stream of custom elements and apply transformations
</span><span class="c1"></span>    <span class="n">env</span><span class="o">.</span><span class="n">fromElements</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">21</span><span class="o">,</span> <span class="mi">22</span><span class="o">)</span>
       <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">new</span> <span class="nc">IncrementMapFunction</span><span class="o">())</span>
       <span class="o">.</span><span class="n">addSink</span><span class="o">(</span><span class="k">new</span> <span class="nc">CollectSink</span><span class="o">())</span>

    <span class="c1">// execute
</span><span class="c1"></span>    <span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">()</span>

    <span class="c1">// verify your results
</span><span class="c1"></span>    <span class="nc">CollectSink</span><span class="o">.</span><span class="n">values</span> <span class="n">should</span> <span class="n">contain</span> <span class="n">allOf</span> <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">22</span><span class="o">,</span> <span class="mi">23</span><span class="o">)</span>
    <span class="o">}</span>
<span class="o">}</span>
<span class="c1">// create a testing sink
</span><span class="c1"></span><span class="k">class</span> <span class="nc">CollectSink</span> <span class="k">extends</span> <span class="nc">SinkFunction</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">invoke</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">synchronized</span> <span class="o">{</span>
      <span class="nc">CollectSink</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">value</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">object</span> <span class="nc">CollectSink</span> <span class="o">{</span>
    <span class="c1">// must be static
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">values</span><span class="k">:</span> <span class="kt">util.List</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="k">new</span> <span class="n">util</span><span class="o">.</span><span class="nc">ArrayList</span><span class="o">()</span>
<span class="o">}</span>
</code></pre></div><p>关于 MiniClusterWithClientResource 的集成测试的几点说明。</p>
<ul>
<li>
<p>为了不把你的整个流水线代码从生产中复制到测试中，请在你的生产代码中使源和汇可插拔，并在你的测试中注入特殊的测试源和测试汇。</p>
</li>
<li>
<p>这里使用了 CollectSink 中的静态变量，因为 Flink 在将所有操作符分布在集群中之前，会将它们序列化。通过静态变量与本地 Flink 迷你集群实例化的运算符进行通信是解决这个问题的一种方法。另外，你可以将数据写到与你的测试汇的临时目录中的文件中。</p>
</li>
<li>
<p>如果你的作业使用事件时间计时器，你可以实现一个自定义的并行源函数来发射水印。</p>
</li>
<li>
<p>建议始终以并行度 <code>&gt;1</code> 的方式在本地测试你的流水线，以识别只有并行执行的流水线才会出现的错误。</p>
</li>
<li>
<p>优先选择 <code>@ClassRule</code> 而不是 <code>@Rule</code>，这样多个测试可以共享同一个 Flink 集群。这样做可以节省大量的时间，因为 Flink 集群的启动和关闭通常会支配实际测试的执行时间。</p>
</li>
<li>
<p>如果你的管道包含自定义状态处理，你可以通过启用检查点并在迷你集群内重新启动作业来测试其正确性。为此，你需要通过从你的管道中的（仅测试的）用户定义函数中抛出一个异常来触发失败。</p>
</li>
</ul>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/testing.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/testing" term="testing" label="testing" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[状态后端]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-state-backends/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-state-backends/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>State Backends</blockquote><h2 id="状态后端">状态后端</h2>
<p>Flink 提供了不同的状态后端，指定状态的存储方式和位置。</p>
<p>状态可以位于 Java 的堆上或离堆(off-heap)。根据你的状态后端，Flink 还可以为应用程序管理状态，这意味着 Flink 处理内存管理（必要时可能会溢出到磁盘），以允许应用程序持有非常大的状态。默认情况下，配置文件 <em>flink-conf.yaml</em> 决定了所有 Flink 作业(job)的状态后端。</p>
<p>然而，默认的状态后端可以在每个作业(per-job)的基础上被重写，如下所示。</p>
<p>有关可用的状态后端、其优势、限制和配置参数的更多信息，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/state/state_backends.html">部署与操作</a>中的相应章节。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span><span class="o">()</span>
<span class="n">env</span><span class="o">.</span><span class="n">setStateBackend</span><span class="o">(...)</span>
</code></pre></div><p>状态后端: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state_backends.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state_backends.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[状态模式的演变]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-state-schema-evolution/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-state-schema-evolution/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>State Schema Evolution</blockquote><p>Apache Flink 流媒体应用通常被设计为无限期或长时间运行。与所有长期运行的服务一样，应用程序需要更新以适应不断变化的需求。这对于应用程序所针对的数据模式(data schema)也是一样的，它们会随着应用程序的发展而发展。</p>
<p>本页提供了关于如何演进状态类型的数据模式(data schema)的概述。当前的限制在不同的类型和状态结构（<code>ValueState</code>、<code>ListState</code> 等）中有所不同。</p>
<p>请注意，本页面上的信息仅在您使用由 Flink 自己的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html">类型序列化框架</a>生成的状态序列化器时相关。也就是说，在声明你的状态时，所提供的状态描述符并没有被配置为使用特定的 <code>TypeSerializer</code> 或 <code>TypeInformation</code>，在这种情况下，Flink 会推导出状态类型的信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">ListStateDescriptor</span><span class="o">&lt;</span><span class="n">MyPojoType</span><span class="o">&gt;</span> <span class="n">descriptor</span> <span class="o">=</span>
    <span class="k">new</span> <span class="n">ListStateDescriptor</span><span class="o">&lt;&gt;(</span>
        <span class="s">&#34;state-name&#34;</span><span class="o">,</span>
        <span class="n">MyPojoType</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

<span class="n">checkpointedState</span> <span class="o">=</span> <span class="n">getRuntimeContext</span><span class="o">().</span><span class="na">getListState</span><span class="o">(</span><span class="n">descriptor</span><span class="o">);</span>
</code></pre></div><p>在底层，状态的模式(schema)是否可以被演化取决于用于读取/写入持久化状态字节的序列化器。简单地说，只有当它的序列化器正确地支持时，一个注册状态的模式才能被演化。这是由 Flink 的类型序列化框架生成的序列化器透明地处理的（当前的支持范围列在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/schema_evolution.html#supported-data-types-for-schema-evolution">下面</a>）。</p>
<p>如果你打算为你的状态类型实现一个自定义的 <code>TypeSerializer</code>，并想了解如何实现序列化器以支持状态模式演化，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html">自定义状态序列化</a>。那里的文档还涵盖了关于状态序列化器和 Flink 的状态后端之间的相互作用的必要内部细节，以支持状态模式(state schema)演化。</p>
<h2 id="状态模式的演化">状态模式的演化</h2>
<p>要演化给定状态类型的模式，您需要采取以下步骤。</p>
<ol>
<li>保存你的 Flink 流作业(job)的保存点。</li>
<li>更新您的应用程序中的状态类型（例如，修改您的 Avro 类型模式）。</li>
<li>从保存点恢复作业(job)。当第一次访问状态时，Flink 将评估是否已经改变了状态的模式(schema)，并在必要时迁移状态模式。</li>
</ol>
<p>迁移状态以适应已更改的模式的过程是自动发生的，并且对每个状态都是独立的。这个过程由 Flink 内部执行，首先检查状态的新序列器是否与之前的序列器有不同的序列化模式，如果有，则用之前的序列器将状态读到对象，再用新的序列器写回字节。</p>
<p>关于迁移过程的进一步细节不在本文档的范围内，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html">这里</a>。</p>
<h2 id="支持的模式演化数据类型">支持的模式演化数据类型</h2>
<p>目前，模式演化只支持 POJO 和 Avro 类型。因此，如果你关心状态的模式演化，目前建议始终使用 POJO 或 Avro 作为状态数据类型。</p>
<p>有计划扩展对更多复合类型的支持；更多细节请参考 <a href="https://issues.apache.org/jira/browse/FLINK-10896">FLINK-10896</a>。</p>
<h3 id="pojo-类型">POJO 类型</h3>
<p>Flink 支持 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html#rules-for-pojo-types">POJO 类型</a>的演化模式，基于以下一组规则。</p>
<ol>
<li>字段可以被删除。一旦被删除，在未来的检查点和保存点中，被删除字段的之前值将被丢弃。</li>
<li>可以添加新字段。新字段将被初始化为其类型的默认值，正如 <a href="https://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html">Java 所定义的</a>那样。</li>
<li>已声明的字段类型不能改变。</li>
<li>POJO 类型的类名不能改变，包括类的命名空间。</li>
</ol>
<p>请注意，POJO 类型状态的模式只能在 Flink 版本大于 1.8.0 的情况下，从以前的保存点恢复时才能进化。当使用比 1.8.0 更老的 Flink 版本进行还原时，模式不能被改变。</p>
<h3 id="avro-类型">Avro 类型</h3>
<p>Flink 完全支持 Avro 类型状态的演变模式，只要模式变化被 <a href="http://avro.apache.org/docs/current/spec.html#Schema+Resolution">Avro 的模式解析规则</a>认为是兼容的。</p>
<p>一个限制是作为状态类型使用的 Avro 生成的类在恢复作业时不能被重新定位或具有不同的命名空间。</p>
<p>注意: 不支持键的模式演变。</p>
<p>举个例子。RocksDB 状态后端依赖于二进制对象的标识，而不是 hashCode 方法实现。对 keys 对象结构的任何改变都可能导致非确定性行为。</p>
<p>注意: Kryo 不能用于模式演化。</p>
<p>当使用 Kryo 时，框架没有可能验证是否有任何不兼容的变化。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/schema_evolution.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/schema_evolution.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[用于外部数据访问的异步 I/O]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-asynchronous-io-for-external-data-access/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-windows/?utm_source=atom_feed" rel="related" type="text/html" title="窗口" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-asynchronous-io-for-external-data-access/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Asynchronous Io for External Data Access</blockquote><p>本页解释了如何使用 Flink 的 API 与外部数据存储进行异步 I/O。对于不熟悉异步或事件驱动编程的用户来说，一篇关于 Futures 和事件驱动编程的文章可能是有用的准备。</p>
<p>注：关于异步 I/O 实用程序的设计和实现的细节可以在提案和设计文件 <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65870673">FLIP-12：异步I/O设计和实现中找到</a>。</p>
<h2 id="异步io操作的必要性">异步I/O操作的必要性</h2>
<p>在与外部系统交互时（例如用存储在数据库中的数据来丰富流事件时），需要注意与外部系统的通信延迟不会主导流应用的总工作。</p>
<p>奈何访问外部数据库中的数据，例如在 <code>MapFunction</code> 中，通常意味着同步交互。一个请求被发送到数据库，<code>MapFunction</code> 等待直到收到响应。在许多情况下，这种等待占据了函数的绝大部分时间。</p>
<p>与数据库的异步交互意味着一个并行函数实例可以同时处理许多请求，并同时接收响应。这样一来，等待时间就可以与发送其他请求和接收响应叠加起来。最起码，等待时间可以摊在多个请求上。这在大多数情况下会导致更高的流吞吐量。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/async_io.svg" alt="img"></p>
<p>注意：通过仅仅将 <code>MapFunction</code> 扩展到很高的并行度来提高吞吐量，在某些情况下也是可行的，但通常要付出很高的资源代价：拥有更多的并行 <code>MapFunction</code> 实例意味着更多的任务、线程、Flink 内部网络连接、与数据库的网络连接、缓冲区以及一般的内部记账开销。</p>
<h2 id="前提条件">前提条件</h2>
<p>如上节所述，要实现对数据库（或键/值存储）的适当异步 I/O，需要向该数据库提供一个支持异步请求的客户端。许多流行的数据库都提供了这样的客户端。</p>
<p>在没有这样的客户端的情况下，可以尝试通过创建多个客户端，并用线程池处理同步调用，将同步客户端变成有限的并发客户端。然而，这种方法通常比一个合适的异步客户端效率低。</p>
<h2 id="异步-io-api">异步 I/O API</h2>
<p>Flink 的 Async I/O API 允许用户使用异步请求客户端与数据流。该 API 处理与数据流的集成，以及处理顺序、事件时间、容错等。</p>
<p>假设自己有一个目标数据库的异步客户端，需要三个部分来实现对数据库的异步 I/O 的流转换。</p>
<ul>
<li>一个 AsyncFunction 的实现，用来调度请求。</li>
<li>一个回调，获取操作结果并将其交给 <code>ResultFuture</code>。</li>
<li>在 DataStream 上应用异步 I/O 操作作为转换。</li>
</ul>
<p>下面的代码示例说明了基本模式。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="cm">/**
</span><span class="cm"> * An implementation of the &#39;AsyncFunction&#39; that sends requests and sets the callback.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">AsyncDatabaseRequest</span> <span class="k">extends</span> <span class="nc">AsyncFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)]</span> <span class="o">{</span>

    <span class="cm">/** The database specific client that can issue concurrent requests with callbacks */</span>
    <span class="k">lazy</span> <span class="k">val</span> <span class="n">client</span><span class="k">:</span> <span class="kt">DatabaseClient</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">DatabaseClient</span><span class="o">(</span><span class="n">host</span><span class="o">,</span> <span class="n">post</span><span class="o">,</span> <span class="n">credentials</span><span class="o">)</span>

    <span class="cm">/** The context used for the future callbacks */</span>
    <span class="k">implicit</span> <span class="k">lazy</span> <span class="k">val</span> <span class="n">executor</span><span class="k">:</span> <span class="kt">ExecutionContext</span> <span class="o">=</span> <span class="nc">ExecutionContext</span><span class="o">.</span><span class="n">fromExecutor</span><span class="o">(</span><span class="nc">Executors</span><span class="o">.</span><span class="n">directExecutor</span><span class="o">())</span>


    <span class="k">override</span> <span class="k">def</span> <span class="n">asyncInvoke</span><span class="o">(</span><span class="n">str</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">resultFuture</span><span class="k">:</span> <span class="kt">ResultFuture</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>

        <span class="c1">// issue the asynchronous request, receive a future for the result
</span><span class="c1"></span>        <span class="k">val</span> <span class="n">resultFutureRequested</span><span class="k">:</span> <span class="kt">Future</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">client</span><span class="o">.</span><span class="n">query</span><span class="o">(</span><span class="n">str</span><span class="o">)</span>

        <span class="c1">// set the callback to be executed once the request by the client is complete
</span><span class="c1"></span>        <span class="c1">// the callback simply forwards the result to the result future
</span><span class="c1"></span>        <span class="n">resultFutureRequested</span><span class="o">.</span><span class="n">onSuccess</span> <span class="o">{</span>
            <span class="k">case</span> <span class="n">result</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="n">resultFuture</span><span class="o">.</span><span class="n">complete</span><span class="o">(</span><span class="nc">Iterable</span><span class="o">((</span><span class="n">str</span><span class="o">,</span> <span class="n">result</span><span class="o">)))</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// create the original stream
</span><span class="c1"></span><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// apply the async I/O transformation
</span><span class="c1"></span><span class="k">val</span> <span class="n">resultStream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">)]</span> <span class="k">=</span>
    <span class="nc">AsyncDataStream</span><span class="o">.</span><span class="n">unorderedWait</span><span class="o">(</span><span class="n">stream</span><span class="o">,</span> <span class="k">new</span> <span class="nc">AsyncDatabaseRequest</span><span class="o">(),</span> <span class="mi">1000</span><span class="o">,</span> <span class="nc">TimeUnit</span><span class="o">.</span><span class="nc">MILLISECONDS</span><span class="o">,</span> <span class="mi">100</span><span class="o">)</span>
</code></pre></div><p>重要提示：<code>ResultFuture.complete</code> 的第一次调用就完成了。所有后续的完成调用将被忽略。</p>
<p>以下两个参数控制异步操作。</p>
<ul>
<li>
<p>超时: 超时定义了异步请求在被认为失败之前可能需要的时间。这个参数可以防范死机/失败的请求。</p>
</li>
<li>
<p>Capacity（容量）：该参数定义了异步请求在被认为失败之前可能需要的时间。这个参数定义了多少个异步请求可以同时进行。尽管异步I/O方法通常会带来更好的吞吐量，但操作者仍然可以成为流应用的瓶颈。限制并发请求的数量可以确保操作者不会积累越来越多的待处理请求的积压，但一旦容量耗尽，就会触发背压。</p>
</li>
</ul>
<h3 id="超时处理">超时处理</h3>
<p>当一个异步 I/O 请求超时时，默认情况下会抛出一个异常并重新启动作业。如果你想处理超时，你可以重写 <code>AsyncFunction#timeout</code> 方法。</p>
<h3 id="结果的顺序">结果的顺序</h3>
<p><code>AsyncFunction</code> 发出的并发请求经常以某种未定义的顺序完成，基于哪个请求先完成。为了控制结果记录以何种顺序发出，Flink 提供了两种模式。</p>
<ul>
<li>
<p>Unordered: 异步请求一结束，结果记录就会被发出。在异步 I/O 操作符之后，流中记录的顺序与之前不同。这种模式以处理时间为基本时间特性时，延迟最低，开销最小。使用 <code>AsyncDataStream.unorderedWait(...)</code> 来实现这种模式。</p>
</li>
<li>
<p>Ordered: 在这种情况下，流的顺序被保留下来。结果记录的发出顺序与异步请求被触发的顺序相同（运算符输入记录的顺序）。为了达到这个目的，操作符会缓冲一个结果记录，直到它前面的所有记录都被发出来（或定时发出来）。这通常会在检查点中引入一些额外的延迟和一些开销，因为与无序模式相比，记录或结果在检查点状态下维持的时间更长。使用 <code>AsyncDataStream.orderedWait(...)</code> 来处理这种模式。</p>
</li>
</ul>
<h2 id="事件时间">事件时间</h2>
<p>当流媒体应用程序使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间</a>工作时，水印将由异步 I/O 操作符正确处理。具体来说，这意味着两种顺序模式的以下内容。</p>
<ul>
<li>无序的：水印不会超越记录，反之亦然，这意味着水印会建立一个顺序边界。只有在水印之间才会发出无序的记录。发生在某一水印之后的记录，只有在该水印被发射之后才会被发射。而水印则只有在该水印之前的所有输入的结果记录被发出之后才会被发出。</li>
</ul>
<p>这意味着在有水印的情况下，无序模式会引入一些与有序模式相同的延迟和管理开销。该开销的数量取决于水印的频率。</p>
<ul>
<li>有序的: 水印和记录的顺序被保留下来 就像记录之间的顺序被保留一样 与处理时间相比，开销没有明显变化。</li>
</ul>
<p>请记住，摄取时间是事件时间的一种特殊情况，其自动生成的水印是基于源处理时间的。</p>
<h2 id="容错保证">容错保证</h2>
<p>异步 I/O 操作符提供了完全精确的一次容错保证，它将飞行中的异步请求记录存储在检查点中，并在故障恢复时恢复/重新触发请求。它将飞行中的异步请求记录存储在检查点中，并在从故障中恢复时恢复/重新触发请求。</p>
<h2 id="实现技巧">实现技巧</h2>
<p>对于有 Executor（或 Scala 中的 ExecutionContext）用于回调的 Futures 实现，我们建议使用  DirectExecutor，因为回调通常只做最少的工作，而且DirectExecutor 避免了额外的线程间交接开销。回调通常只将结果交给 <code>ResultFuture</code>，后者将其添加到输出缓冲区。从那里开始，包括记录排放和与检查点记账的交互在内的繁重逻辑无论如何都发生在一个专用线程池中。</p>
<p>可以通过 <code>org.apache.flink.runtime.concurrent.Executors.directExecutor()</code> 或 <code>com.google.common.util.concurrent.MoreExecutors.directExecutor()</code> 获得 DirectExecutor。</p>
<h2 id="注意事项">注意事项</h2>
<p>AsyncFunction 不叫多线程。</p>
<p>我们在这里要明确指出的一个常见的困惑是，AsyncFunction 不是以多线程的方式调用的。AsyncFunction 只存在一个实例，并且对于流的各个分区中的每一条记录，它都会被依次调用。除非 <code>asyncInvoke(...)</code> 方法快速返回并依赖于回调（由客户端），否则不会导致正确的异步 I/O。</p>
<p>例如，以下模式会导致阻塞 <code>asyncInvoke(...)</code> 函数，从而使异步行为无效。</p>
<ul>
<li>
<p>使用一个数据库客户端，其查找/查询方法的调用会被阻塞，直到结果被接收回来为止</p>
</li>
<li>
<p>在 <code>asyncInvoke(...)</code> 方法中阻止/等待异步客户端返回的未来型对象。</p>
</li>
</ul>
<p>出于一致性的考虑，AsyncFunction 的操作符（AsyncWaitOperator）目前必须位于操作符链的头部。</p>
<p>由于在 FLINK-13063 问题中给出的原因，我们目前必须打破 AsyncWaitOperator 的操作符链，以防止潜在的一致性问题。这是对以前支持链的行为的改变。需要旧行为并接受潜在的违反一致性保证的用户可以手动实例化并将 AsyncWaitOperator 添加到作业图中，并通过 AsyncWaitOperator#setChainingStrategy(ChainingStrategy.ALWAYS) 将链式策略设置回链式。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/operators" term="operators" label="Operators" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/io" term="io" label="IO" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[用户定义函数]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-table-api-user-defined-functions/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-functions/?utm_source=atom_feed" rel="related" type="text/html" title="函数" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-table-api-user-defined-functions/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>User Defined Functions</blockquote><h1 id="用户自定义函数">用户自定义函数</h1>
<p>用户自定义函数(UDFs)是扩展点，用于调用常用的逻辑或自定义逻辑，这些逻辑无法在查询中以其他方式表达。</p>
<p>用户定义函数可以用 JVM 语言（如 Java 或 Scala）或 Python 实现。实现者可以在 UDF 中使用任意的第三方库。本页将重点介绍基于 JVM 的语言。</p>
<h2 id="概述">概述</h2>
<p>目前，Flink 区分了以下几种函数。</p>
<ul>
<li>标量函数将标量值映射到一个新的标量值。</li>
<li>表函数将标量值映射到新的行(row)。</li>
<li>聚合函数将多行的标量值映射到新的标量值。</li>
<li>表聚合函数将多行的标量值映射到新的行上。</li>
<li>异步表函数是针对 table source 执行查找的特殊函数。</li>
</ul>
<p>注意: 标量函数和表函数已经更新为基于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/types.html">数据类型</a>的新类型系统。聚合函数仍然使用基于 TypeInformation 的旧类型系统。</p>
<p>下面的示例展示了如何创建一个简单的标量函数，以及如何在表 API 和 SQL 中调用该函数。</p>
<p>对于 SQL 查询，一个函数必须始终以一个名字注册。对于 Table API，函数可以被注册，也可以直接内联使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.table.api._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.ScalarFunction</span>

<span class="c1">// define function logic
</span><span class="c1"></span><span class="k">class</span> <span class="nc">SubstringFunction</span> <span class="k">extends</span> <span class="nc">ScalarFunction</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">begin</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">,</span> <span class="n">end</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">s</span><span class="o">.</span><span class="n">substring</span><span class="o">(</span><span class="n">begin</span><span class="o">,</span> <span class="n">end</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(...)</span>

<span class="c1">// call function &#34;inline&#34; without registration in Table API
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">SubstringFunction</span><span class="o">],</span> <span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">,</span> <span class="mi">5</span><span class="o">,</span> <span class="mi">12</span><span class="o">))</span>

<span class="c1">// register function
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">createTemporarySystemFunction</span><span class="o">(</span><span class="s">&#34;SubstringFunction&#34;</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">SubstringFunction</span><span class="o">])</span>

<span class="c1">// call registered function in Table API
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="s">&#34;SubstringFunction&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">,</span> <span class="mi">5</span><span class="o">,</span> <span class="mi">12</span><span class="o">))</span>

<span class="c1">// call registered function in SQL
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span><span class="s">&#34;SELECT SubstringFunction(myField, 5, 12) FROM MyTable&#34;</span><span class="o">)</span>
</code></pre></div><p>对于交互式会话，也可以在使用或注册函数之前对其进行参数化。在这种情况下，可以使用函数实例代替函数类作为临时函数。</p>
<p>它要求参数是可序列化的，以便将函数实例运送到集群。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.table.api._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.ScalarFunction</span>

<span class="c1">// define parameterizable function logic
</span><span class="c1"></span><span class="k">class</span> <span class="nc">SubstringFunction</span><span class="o">(</span><span class="k">val</span> <span class="n">endInclusive</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">ScalarFunction</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">begin</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">,</span> <span class="n">end</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">s</span><span class="o">.</span><span class="n">substring</span><span class="o">(</span><span class="n">endInclusive</span> <span class="o">?</span> <span class="n">end</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">:</span> <span class="kt">end</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(...)</span>

<span class="c1">// call function &#34;inline&#34; without registration in Table API
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="k">new</span> <span class="nc">SubstringFunction</span><span class="o">(</span><span class="kc">true</span><span class="o">),</span> <span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">,</span> <span class="mi">5</span><span class="o">,</span> <span class="mi">12</span><span class="o">))</span>

<span class="c1">// register function
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">createTemporarySystemFunction</span><span class="o">(</span><span class="s">&#34;SubstringFunction&#34;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SubstringFunction</span><span class="o">(</span><span class="kc">true</span><span class="o">))</span>
</code></pre></div><h2 id="实现指南">实现指南</h2>
<p>注意：本节目前只适用于标量函数和表函数；在集合函数更新到新的类型系统之前，本节只适用于标量函数。</p>
<p>无论函数的种类如何，所有用户定义的函数都遵循一些基本的实现原则。</p>
<h3 id="函数类">函数类</h3>
<p>一个实现类必须从一个可用的基类(例如 <code>org.apache.flink.table.function.ScalarFunction</code>)中扩展出来。</p>
<p>这个类必须被声明为 <code>public</code>，而不是 <code>abstract</code>，并且应该是全局访问的。因此，不允许使用非静态的内部类或匿名类。</p>
<p>对于在持久化目录中存储用户定义的函数，该类必须有一个默认的构造函数，并且在运行时必须是可实例化的。</p>
<h3 id="评估方法">评估方法</h3>
<p>基类提供了一组可以重写的方法，如 <code>open()</code>、<code>close()</code> 或 <code>isDeterministic()</code>。</p>
<p>然而，除了这些声明的方法外，应用于每个传入记录的主要运行时逻辑必须通过专门的评估方法来实现。</p>
<p>根据函数种类的不同，评价方法如 <code>eval()</code>、<code>accumulate()</code> 或 <code>retract()</code> 会在运行时被代码生成的操作符调用。</p>
<p>这些方法必须声明为 <code>public</code>，并接受一组定义明确的参数。</p>
<p>常规的 JVM 方法调用语义适用。因此，可以</p>
<ul>
<li>实现重载方法，如 <code>eval(Integer)</code> 和 <code>eval(LocalDateTime)</code>。</li>
<li>使用 var-args，如 <code>eval(Integer...)</code>。</li>
<li>使用对象继承，如 <code>eval(Object)</code>，它同时接受 <code>LocalDateTime</code> 和 <code>Integer</code>。</li>
<li>以及上述函数的组合，如 <code>eval(Object...)</code>，它可以接受所有类型的参数。</li>
</ul>
<p>如果你打算在 Scala 中实现函数，请在使用变量参数时添加 scala.annotation.varargs 注解。此外，建议使用盒状基元（如用 java.lang.Integer 代替 Int）来支持 NULL。</p>
<p>下面的代码段显示了一个重载函数的示例。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.table.functions.ScalarFunction</span>
<span class="k">import</span> <span class="nn">java.lang.Integer</span>
<span class="k">import</span> <span class="nn">java.lang.Double</span>
<span class="k">import</span> <span class="nn">scala.annotation.varargs</span>

<span class="c1">// function with overloaded evaluation methods
</span><span class="c1"></span><span class="k">class</span> <span class="nc">SumFunction</span> <span class="k">extends</span> <span class="nc">ScalarFunction</span> <span class="o">{</span>

  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Integer</span><span class="o">)</span><span class="k">:</span> <span class="kt">Integer</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Integer</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">Integer</span><span class="o">.</span><span class="n">valueOf</span><span class="o">(</span><span class="n">a</span><span class="o">)</span> <span class="o">+</span> <span class="nc">Integer</span><span class="o">.</span><span class="n">valueOf</span><span class="o">(</span><span class="n">b</span><span class="o">)</span>
  <span class="o">}</span>

  <span class="nd">@varargs</span> <span class="c1">// generate var-args like Java
</span><span class="c1"></span>  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">d</span><span class="k">:</span> <span class="kt">Double*</span><span class="o">)</span><span class="k">:</span> <span class="kt">Integer</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">d</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">toInt</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="类型推断">类型推断</h3>
<p>表生态系统（类似于 SQL 标准）是一个强类型的 API。因此，函数参数和返回类型都必须映射到<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/types.html">数据类型</a>。</p>
<p>从逻辑的角度来看，规划师需要关于预期类型、精度和规模的信息。从 JVM 的角度来看，规划师需要了解当调用用户定义的函数时，内部数据结构如何被表示为 JVM 对象。</p>
<p>验证输入参数和推导出函数的参数和结果的数据类型的逻辑被总结在类型推理这个术语下。</p>
<p>Flink 的用户定义函数实现了自动类型推理提取，通过反射从函数的类和它的评估方法中导出数据类型。如果这种隐式反射提取方法不成功，可以通过用 <code>@DataTypeHint</code> 和 <code>@FunctionHint</code> 注释受影响的参数、类或方法来支持提取过程。更多关于如何注释函数的例子如下所示。</p>
<p>如果需要更高级的类型推理逻辑，实现者可以在每个用户定义的函数中显式覆盖 <code>getTypeInference()</code> 方法。然而，推荐使用注释方法，因为它将自定义类型推理逻辑保持在受影响的位置附近，并回落到其余实现的默认行为。</p>
<h4 id="自动类型推断">自动类型推断</h4>
<p>自动类型推理检查函数的类和评估方法，从而得出函数的参数和结果的数据类型。<code>@DataTypeHint</code> 和 <code>@FunctionHint</code> 注解支持自动提取。</p>
<p>关于可以隐式映射到数据类型的类的完整列表，请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/types.html#data-type-extraction">数据类型提取</a>部分。</p>
<h5 id="datatypehint">@DataTypeHint</h5>
<p>在很多情况下，需要支持对函数的参数和返回类型进行在线自动提取。</p>
<p>下面的示例展示了如何使用数据类型提示。更多信息可以在注解类的文档中找到。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.table.annotation.DataTypeHint</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.annotation.InputGroup</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.ScalarFunction</span>
<span class="k">import</span> <span class="nn">org.apache.flink.types.Row</span>
<span class="k">import</span> <span class="nn">scala.annotation.varargs</span>

<span class="c1">// function with overloaded evaluation methods
</span><span class="c1"></span><span class="k">class</span> <span class="nc">OverloadedFunction</span> <span class="k">extends</span> <span class="nc">ScalarFunction</span> <span class="o">{</span>

  <span class="c1">// no hint required
</span><span class="c1"></span>  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
  <span class="o">}</span>

  <span class="c1">// define the precision and scale of a decimal
</span><span class="c1"></span>  <span class="nd">@DataTypeHint</span><span class="o">(</span><span class="s">&#34;DECIMAL(12, 3)&#34;</span><span class="o">)</span>
  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">double</span> <span class="n">a</span><span class="o">,</span> <span class="n">double</span> <span class="n">b</span><span class="o">)</span><span class="k">:</span> <span class="kt">BigDecimal</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">java</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="nc">BigDecimal</span><span class="o">.</span><span class="n">valueOf</span><span class="o">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span>
  <span class="o">}</span>

  <span class="c1">// define a nested data type
</span><span class="c1"></span>  <span class="nd">@DataTypeHint</span><span class="o">(</span><span class="s">&#34;ROW&lt;s STRING, t TIMESTAMP(3) WITH LOCAL TIME ZONE&gt;&#34;</span><span class="o">)</span>
  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="nc">Int</span> <span class="n">i</span><span class="o">)</span><span class="k">:</span> <span class="kt">Row</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">Row</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">java</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="nc">String</span><span class="o">.</span><span class="n">valueOf</span><span class="o">(</span><span class="n">i</span><span class="o">),</span> <span class="n">java</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="nc">Instant</span><span class="o">.</span><span class="n">ofEpochSecond</span><span class="o">(</span><span class="n">i</span><span class="o">))</span>
  <span class="o">}</span>

  <span class="c1">// allow wildcard input and customly serialized output
</span><span class="c1"></span>  <span class="nd">@DataTypeHint</span><span class="o">(</span><span class="n">value</span> <span class="k">=</span> <span class="s">&#34;RAW&#34;</span><span class="o">,</span> <span class="n">bridgedTo</span> <span class="k">=</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">java.nio.ByteBuffer</span><span class="o">])</span>
  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="nd">@DataTypeHint</span><span class="o">(</span><span class="n">inputGroup</span> <span class="k">=</span> <span class="nc">InputGroup</span><span class="o">.</span><span class="nc">ANY</span><span class="o">)</span> <span class="nc">Object</span> <span class="n">o</span><span class="o">)</span><span class="k">:</span> <span class="kt">java.nio.ByteBuffer</span> <span class="o">=</span> <span class="o">{</span>
    <span class="nc">MyUtils</span><span class="o">.</span><span class="n">serializeToByteBuffer</span><span class="o">(</span><span class="n">o</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h5 id="functionhint">@FunctionHint</h5>
<p>在某些场景下，一个评估方法同时处理多种不同的数据类型是可取的。此外，在某些场景中，重载的评估方法有一个共同的结果类型，应该只声明一次。</p>
<p><code>@FunctionHint</code> 注解可以提供从参数数据类型到结果数据类型的映射。它可以为输入、累加器和结果数据类型注释整个函数类或评估方法。一个或多个注解可以在一个类的顶部声明，也可以为每个评估方法单独声明，以便重载函数签名。所有的提示参数都是可选的。如果没有定义参数，则使用默认的基于反射的提取方式。在函数类之上定义的提示参数会被所有的评估方法继承。</p>
<p>下面的例子展示了如何使用函数提示。更多信息可以在注解类的文档中找到。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.table.annotation.DataTypeHint</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.annotation.FunctionHint</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.TableFunction</span>
<span class="k">import</span> <span class="nn">org.apache.flink.types.Row</span>

<span class="c1">// function with overloaded evaluation methods
</span><span class="c1">// but globally defined output type
</span><span class="c1"></span><span class="nd">@FunctionHint</span><span class="o">(</span><span class="n">output</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DataTypeHint</span><span class="o">(</span><span class="s">&#34;ROW&lt;s STRING, i INT&gt;&#34;</span><span class="o">))</span>
<span class="k">class</span> <span class="nc">OverloadedFunction</span> <span class="k">extends</span> <span class="nc">TableFunction</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">collect</span><span class="o">(</span><span class="nc">Row</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="s">&#34;Sum&#34;</span><span class="o">,</span> <span class="nc">Int</span><span class="o">.</span><span class="n">box</span><span class="o">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)))</span>
  <span class="o">}</span>

  <span class="c1">// overloading of arguments is still possible
</span><span class="c1"></span>  <span class="k">def</span> <span class="n">eval</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">collect</span><span class="o">(</span><span class="nc">Row</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="s">&#34;Empty args&#34;</span><span class="o">,</span> <span class="nc">Int</span><span class="o">.</span><span class="n">box</span><span class="o">(-</span><span class="mi">1</span><span class="o">)))</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// decouples the type inference from evaluation methods,
</span><span class="c1">// the type inference is entirely determined by the function hints
</span><span class="c1"></span><span class="nd">@FunctionHint</span><span class="o">(</span>
  <span class="n">input</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="k">new</span> <span class="nc">DataTypeHint</span><span class="o">(</span><span class="s">&#34;INT&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">DataTypeHint</span><span class="o">(</span><span class="s">&#34;INT&#34;</span><span class="o">)),</span>
  <span class="n">output</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DataTypeHint</span><span class="o">(</span><span class="s">&#34;INT&#34;</span><span class="o">)</span>
<span class="o">)</span>
<span class="nd">@FunctionHint</span><span class="o">(</span>
  <span class="n">input</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="k">new</span> <span class="nc">DataTypeHint</span><span class="o">(</span><span class="s">&#34;BIGINT&#34;</span><span class="o">),</span> <span class="k">new</span> <span class="nc">DataTypeHint</span><span class="o">(</span><span class="s">&#34;BIGINT&#34;</span><span class="o">)),</span>
  <span class="n">output</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DataTypeHint</span><span class="o">(</span><span class="s">&#34;BIGINT&#34;</span><span class="o">)</span>
<span class="o">)</span>
<span class="nd">@FunctionHint</span><span class="o">(</span>
  <span class="n">input</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(),</span>
  <span class="n">output</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DataTypeHint</span><span class="o">(</span><span class="s">&#34;BOOLEAN&#34;</span><span class="o">)</span>
<span class="o">)</span>
<span class="k">class</span> <span class="nc">OverloadedFunction</span> <span class="k">extends</span> <span class="nc">TableFunction</span><span class="o">[</span><span class="kt">AnyRef</span><span class="o">]</span> <span class="o">{</span>

  <span class="c1">// an implementer just needs to make sure that a method exists
</span><span class="c1"></span>  <span class="c1">// that can be called by the JVM
</span><span class="c1"></span>  <span class="nd">@varargs</span>
  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">o</span><span class="k">:</span> <span class="kt">AnyRef*</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">o</span><span class="o">.</span><span class="n">length</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">collect</span><span class="o">(</span><span class="nc">Boolean</span><span class="o">.</span><span class="n">box</span><span class="o">(</span><span class="kc">false</span><span class="o">))</span>
    <span class="o">}</span>
    <span class="n">collect</span><span class="o">(</span><span class="n">o</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h4 id="自定义类型推断">自定义类型推断</h4>
<p>对于大多数情况下，<code>@DataTypeHint</code> 和 <code>@FunctionHint</code> 应该足以为用户定义的函数建模。然而，通过覆盖 <code>getTypeInference()</code> 中定义的自动类型推理，实现者可以创建任意的函数，这些函数的行为就像内置的系统函数一样。</p>
<p>下面这个用 Java 实现的例子说明了自定义类型推理逻辑的潜力。它使用一个字符串文字参数来确定一个函数的结果类型。该函数需要两个字符串参数：第一个参数代表要解析的字符串，第二个参数代表目标类型。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.table.api.DataTypes</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.catalog.DataTypeFactory</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.functions.ScalarFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.table.types.inference.TypeInference</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.types.Row</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">LiteralFunction</span> <span class="kd">extends</span> <span class="n">ScalarFunction</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Object</span> <span class="nf">eval</span><span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">,</span> <span class="n">String</span> <span class="n">type</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">switch</span> <span class="o">(</span><span class="n">type</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">case</span> <span class="s">&#34;INT&#34;</span><span class="o">:</span>
        <span class="k">return</span> <span class="n">Integer</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">s</span><span class="o">);</span>
      <span class="k">case</span> <span class="s">&#34;DOUBLE&#34;</span><span class="o">:</span>
        <span class="k">return</span> <span class="n">Double</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">s</span><span class="o">);</span>
      <span class="k">case</span> <span class="s">&#34;STRING&#34;</span><span class="o">:</span>
      <span class="k">default</span><span class="o">:</span>
        <span class="k">return</span> <span class="n">s</span><span class="o">;</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="c1">// the automatic, reflection-based type inference is disabled and
</span><span class="c1"></span>  <span class="c1">// replaced by the following logic
</span><span class="c1"></span>  <span class="nd">@Override</span>
  <span class="kd">public</span> <span class="n">TypeInference</span> <span class="nf">getTypeInference</span><span class="o">(</span><span class="n">DataTypeFactory</span> <span class="n">typeFactory</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">TypeInference</span><span class="o">.</span><span class="na">newBuilder</span><span class="o">()</span>
      <span class="c1">// specify typed arguments
</span><span class="c1"></span>      <span class="c1">// parameters will be casted implicitly to those types if necessary
</span><span class="c1"></span>      <span class="o">.</span><span class="na">typedArguments</span><span class="o">(</span><span class="n">DataTypes</span><span class="o">.</span><span class="na">STRING</span><span class="o">(),</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">STRING</span><span class="o">())</span>
      <span class="c1">// specify a strategy for the result data type of the function
</span><span class="c1"></span>      <span class="o">.</span><span class="na">outputTypeStrategy</span><span class="o">(</span><span class="n">callContext</span> <span class="o">-&gt;</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(!</span><span class="n">callContext</span><span class="o">.</span><span class="na">isArgumentLiteral</span><span class="o">(</span><span class="n">1</span><span class="o">)</span> <span class="o">||</span> <span class="n">callContext</span><span class="o">.</span><span class="na">isArgumentNull</span><span class="o">(</span><span class="n">1</span><span class="o">))</span> <span class="o">{</span>
          <span class="k">throw</span> <span class="n">callContext</span><span class="o">.</span><span class="na">newValidationError</span><span class="o">(</span><span class="s">&#34;Literal expected for second argument.&#34;</span><span class="o">);</span>
        <span class="o">}</span>
        <span class="c1">// return a data type based on a literal
</span><span class="c1"></span>        <span class="kd">final</span> <span class="n">String</span> <span class="n">literal</span> <span class="o">=</span> <span class="n">callContext</span><span class="o">.</span><span class="na">getArgumentValue</span><span class="o">(</span><span class="n">1</span><span class="o">,</span> <span class="n">String</span><span class="o">.</span><span class="na">class</span><span class="o">).</span><span class="na">orElse</span><span class="o">(</span><span class="s">&#34;STRING&#34;</span><span class="o">);</span>
        <span class="k">switch</span> <span class="o">(</span><span class="n">literal</span><span class="o">)</span> <span class="o">{</span>
          <span class="k">case</span> <span class="s">&#34;INT&#34;</span><span class="o">:</span>
            <span class="k">return</span> <span class="n">Optional</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">DataTypes</span><span class="o">.</span><span class="na">INT</span><span class="o">().</span><span class="na">notNull</span><span class="o">());</span>
          <span class="k">case</span> <span class="s">&#34;DOUBLE&#34;</span><span class="o">:</span>
            <span class="k">return</span> <span class="n">Optional</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">DataTypes</span><span class="o">.</span><span class="na">DOUBLE</span><span class="o">().</span><span class="na">notNull</span><span class="o">());</span>
          <span class="k">case</span> <span class="s">&#34;STRING&#34;</span><span class="o">:</span>
          <span class="k">default</span><span class="o">:</span>
            <span class="k">return</span> <span class="n">Optional</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">DataTypes</span><span class="o">.</span><span class="na">STRING</span><span class="o">());</span>
        <span class="o">}</span>
      <span class="o">})</span>
      <span class="o">.</span><span class="na">build</span><span class="o">();</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h3 id="运行时集成">运行时集成</h3>
<p>有时可能需要用户自定义函数在实际工作前获取全局运行时信息或做一些设置/清理工作。用户自定义函数提供了 <code>open()</code> 和 <code>close()</code> 方法，这些方法可以被重写，并提供与 DataStream API 的 RichFunction 中的方法类似的功能。</p>
<p><code>open()</code> 方法在评估方法之前被调用一次。<code>close()</code> 方法在最后一次调用评估方法后调用。</p>
<p><code>open()</code> 方法提供了一个 FunctionContext，该 FunctionContext 包含了用户定义函数执行的上下文信息，如度量组、分布式缓存文件或全局作业参数。</p>
<p>通过调用 FunctionContext 的相应方法，可以获得以下信息。</p>
<table>
<thead>
<tr>
<th style="text-align:left">方法</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">getMetricGroup()</td>
<td style="text-align:left">该并行子任务的度量组。</td>
</tr>
<tr>
<td style="text-align:left">getCachedFile(name)</td>
<td style="text-align:left">分布式缓存文件的本地临时文件副本。</td>
</tr>
<tr>
<td style="text-align:left">getJobParameter(name, defaultValue)</td>
<td style="text-align:left">与给定键相关联的全局作业参数值。</td>
</tr>
</tbody>
</table>
<p>下面的示例片段展示了如何在标量函数中使用 FunctionContext 来访问全局工作参数。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.table.api._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.FunctionContext</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.ScalarFunction</span>

<span class="k">class</span> <span class="nc">HashCodeFunction</span> <span class="k">extends</span> <span class="nc">ScalarFunction</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">var</span> <span class="n">factor</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">context</span><span class="k">:</span> <span class="kt">FunctionContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// access the global &#34;hashcode_factor&#34; parameter
</span><span class="c1"></span>    <span class="c1">// &#34;12&#34; would be the default value if the parameter does not exist
</span><span class="c1"></span>    <span class="n">factor</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">getJobParameter</span><span class="o">(</span><span class="s">&#34;hashcode_factor&#34;</span><span class="o">,</span> <span class="s">&#34;12&#34;</span><span class="o">).</span><span class="n">toInt</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">s</span><span class="o">.</span><span class="n">hashCode</span> <span class="o">*</span> <span class="n">factor</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(...)</span>

<span class="c1">// add job parameter
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">getConfig</span><span class="o">.</span><span class="n">addJobParameter</span><span class="o">(</span><span class="s">&#34;hashcode_factor&#34;</span><span class="o">,</span> <span class="s">&#34;31&#34;</span><span class="o">)</span>

<span class="c1">// register the function
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">createTemporarySystemFunction</span><span class="o">(</span><span class="s">&#34;hashCode&#34;</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">HashCodeFunction</span><span class="o">])</span>

<span class="c1">// use the function
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span><span class="s">&#34;SELECT myField, hashCode(myField) FROM MyTable&#34;</span><span class="o">)</span>
</code></pre></div><h2 id="标量函数">标量函数</h2>
<p>用户定义的标量函数可以将零、一或多个标量值映射到一个新的标量值。数据类型一节中列出的任何数据类型都可以作为一个评估方法的参数或返回类型。</p>
<p>为了定义一个标量函数，必须扩展 org.apache.flink.table.function 中的基类 ScalarFunction，并实现一个或多个名为 <code>eval(...)</code> 的评估方法。</p>
<p>下面的例子展示了如何定义自己的哈希码函数并在查询中调用它。更多细节请参见实施指南。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.table.annotation.InputGroup</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.ScalarFunction</span>

<span class="k">class</span> <span class="nc">HashFunction</span> <span class="k">extends</span> <span class="nc">ScalarFunction</span> <span class="o">{</span>

  <span class="c1">// take any data type and return INT
</span><span class="c1"></span>  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="nd">@DataTypeHint</span><span class="o">(</span><span class="n">inputGroup</span> <span class="k">=</span> <span class="nc">InputGroup</span><span class="o">.</span><span class="nc">ANY</span><span class="o">)</span> <span class="n">o</span><span class="k">:</span> <span class="kt">AnyRef</span><span class="o">)</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">{</span>
    <span class="kt">return</span> <span class="kt">o.hashCode</span><span class="o">();</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(...)</span>

<span class="c1">// call function &#34;inline&#34; without registration in Table API
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">HashFunction</span><span class="o">],</span> <span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">))</span>

<span class="c1">// register function
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">createTemporarySystemFunction</span><span class="o">(</span><span class="s">&#34;HashFunction&#34;</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">HashFunction</span><span class="o">])</span>

<span class="c1">// call registered function in Table API
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">).</span><span class="n">select</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="s">&#34;HashFunction&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">))</span>

<span class="c1">// call registered function in SQL
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span><span class="s">&#34;SELECT HashFunction(myField) FROM MyTable&#34;</span><span class="o">)</span>
</code></pre></div><p>如果你打算用 Python 实现或调用函数，请参考 Python Scalar Functions 文档了解更多细节。</p>
<h2 id="表函数">表函数</h2>
<p>与用户定义的标量函数类似，用户定义的表函数将零、一个或多个标量值作为输入参数。然而，与标量函数不同的是，它可以返回任意数量的行（或结构化类型）作为输出，而不是单个值。返回的记录可能由一个或多个字段组成。如果一条输出记录只由一个字段组成，则可以省略结构化记录，并发出一个标量值。它将被运行时包装成一个隐式行。</p>
<p>为了定义一个表函数，必须扩展 org.apache.flink.table.function 中的基类 TableFunction，并实现一个或多个名为 <code>eval(...)</code> 的评估方法。与其他函数类似，输入和输出数据类型也是使用反射自动提取的。这包括类的通用参数 T，用于确定输出数据类型。与标量函数不同的是，评价方法本身不能有返回类型，相反，表函数提供了一个 <code>collect(T)</code> 方法，可以在每个评价方法内调用，用于发出零、一条或多条记录。</p>
<p>在表 API 中，表函数的使用方法是 <code>.joinLateral(...)</code> 或 <code>.leftOuterJoinLateral(...)</code>。joinLateral 运算符（cross）将外表（运算符左边的表）的每条记录与表值函数产生的所有记录（表值函数在运算符的右边）连接起来。leftOuterJoinLateral 操作符将外表（操作符左边的表）的每一条记录与表值函数产生的所有记录（它在操作符的右边）连接起来，并且保留那些表函数返回空表的外表。</p>
<p>在 SQL 中，使用 <code>LATERAL TABLE(&lt;TableFunction&gt;)</code> 与 JOIN 或 LEFT JOIN 与 ON TRUE 连接条件。</p>
<p>下面的示例展示了如何定义自己的拆分函数并在查询中调用它。更多细节请参见《实现指南》。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.table.annotation.DataTypeHint</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.annotation.FunctionHint</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api._</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.TableFunction</span>
<span class="k">import</span> <span class="nn">org.apache.flink.types.Row</span>

<span class="nd">@FunctionHint</span><span class="o">(</span><span class="n">output</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DataTypeHint</span><span class="o">(</span><span class="s">&#34;ROW&lt;word STRING, length INT&gt;&#34;</span><span class="o">))</span>
<span class="k">class</span> <span class="nc">SplitFunction</span> <span class="k">extends</span> <span class="nc">TableFunction</span><span class="o">[</span><span class="kt">Row</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">def</span> <span class="n">eval</span><span class="o">(</span><span class="n">str</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// use collect(...) to emit a row
</span><span class="c1"></span>    <span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&#34; &#34;</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="n">collect</span><span class="o">(</span><span class="nc">Row</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">s</span><span class="o">,</span> <span class="nc">Int</span><span class="o">.</span><span class="n">box</span><span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="n">length</span><span class="o">))))</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(...)</span>

<span class="c1">// call function &#34;inline&#34; without registration in Table API
</span><span class="c1"></span><span class="n">env</span>
  <span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">joinLateral</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">SplitFunction</span><span class="o">],</span> <span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;word&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;length&#34;</span><span class="o">)</span>
<span class="n">env</span>
  <span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">leftOuterJoinLateral</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">SplitFunction</span><span class="o">],</span> <span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;word&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;length&#34;</span><span class="o">)</span>

<span class="c1">// rename fields of the function in Table API
</span><span class="c1"></span><span class="n">env</span>
  <span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">leftOuterJoinLateral</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">SplitFunction</span><span class="o">],</span> <span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">).</span><span class="n">as</span><span class="o">(</span><span class="s">&#34;newWord&#34;</span><span class="o">,</span> <span class="s">&#34;newLength&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;newWord&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;newLength&#34;</span><span class="o">)</span>

<span class="c1">// register function
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">createTemporarySystemFunction</span><span class="o">(</span><span class="s">&#34;SplitFunction&#34;</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">SplitFunction</span><span class="o">])</span>

<span class="c1">// call registered function in Table API
</span><span class="c1"></span><span class="n">env</span>
  <span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">joinLateral</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="s">&#34;SplitFunction&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;word&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;length&#34;</span><span class="o">)</span>
<span class="n">env</span>
  <span class="o">.</span><span class="n">from</span><span class="o">(</span><span class="s">&#34;MyTable&#34;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">leftOuterJoinLateral</span><span class="o">(</span><span class="n">call</span><span class="o">(</span><span class="s">&#34;SplitFunction&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">$</span><span class="s">&#34;myField&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;word&#34;</span><span class="o">,</span> <span class="n">$</span><span class="s">&#34;length&#34;</span><span class="o">)</span>

<span class="c1">// call registered function in SQL
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
  <span class="s">&#34;SELECT myField, word, length &#34;</span> <span class="o">+</span>
  <span class="s">&#34;FROM MyTable, LATERAL TABLE(SplitFunction(myField))&#34;</span><span class="o">);</span>
<span class="n">env</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
  <span class="s">&#34;SELECT myField, word, length &#34;</span> <span class="o">+</span>
  <span class="s">&#34;FROM MyTable &#34;</span> <span class="o">+</span>
  <span class="s">&#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) ON TRUE&#34;</span><span class="o">)</span>

<span class="c1">// rename fields of the function in SQL
</span><span class="c1"></span><span class="n">env</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span>
  <span class="s">&#34;SELECT myField, newWord, newLength &#34;</span> <span class="o">+</span>
  <span class="s">&#34;FROM MyTable &#34;</span> <span class="o">+</span>
  <span class="s">&#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) AS T(newWord, newLength) ON TRUE&#34;</span><span class="o">)</span>
</code></pre></div><p>如果你打算在 Scala 中实现函数，不要将表函数实现为 Scala 对象。Scala 对象是单子，会导致并发问题。</p>
<p>如果你打算用 Python 实现或调用函数，请参考 Python 表函数文档了解更多细节。</p>
<h2 id="聚合函数">聚合函数</h2>
<p>用户自定义聚合函数（UDAGG）将一个表（一个或多个具有一个或多个属性的行）聚合成一个标量值。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/udagg-mechanism.png" alt="img"></p>
<p>上图显示了一个聚合的例子。假设你有一个包含饮料数据的表。该表由 id、名称和价格三列和 5 行组成。想象一下，你需要找到表中所有饮料的最高价格，即执行 <code>max()</code> 聚合。你需要对 5 行中的每一行进行检查，结果将是一个单一的数值。</p>
<p>用户定义的聚合函数是通过扩展 AggregateFunction 类来实现的。AggregateFunction 的工作原理如下。首先，它需要一个累加器，它是存放聚合中间结果的数据结构。通过调用 AggregateFunction 的 <code>createAccumulator()</code> 方法创建一个空的累加器。随后，函数的 <code>accumulate()</code> 方法对每一条输入行进行调用，以更新累加器。一旦所有的行都被处理完毕，函数的 <code>getValue()</code> 方法就会被调用来计算并返回最终结果。</p>
<p>以下方法是每个 AggregateFunction 必须使用的。</p>
<ul>
<li>createAccumulator()</li>
<li>accumulate()</li>
<li>getValue()</li>
</ul>
<p>Flink 的类型提取设施可能无法识别复杂的数据类型，例如，如果它们不是基本类型或简单的 POJOs。所以与 ScalarFunction 和 TableFunction 类似，AggregateFunction 提供了指定结果类型（通过 AggregateFunction#getResultType()）和累加器类型（通过 AggregateFunction#getAccumulatorType()）的方法。</p>
<p>除了上述方法外，还有一些签约方法可以选择实现。这些方法中的一些方法可以让系统更高效地执行查询，而另一些方法则是某些用例所必须的。例如，如果聚合函数应该在会话组窗口的上下文中应用，那么 <code>merge()</code> 方法是强制性的（当观察到有一行 &ldquo;连接 &ldquo;它们时，需要将两个会话窗口的累加器连接起来）。</p>
<p>AggregateFunction 的以下方法是根据用例需要的。</p>
<ul>
<li><code>retract()</code> 对于有界 OVER 窗口上的聚合是需要的。</li>
<li><code>merge()</code> 是许多批次聚合和会话窗口聚合所需要的。</li>
<li><code>resetAccumulator()</code> 是许多批处理聚合所需要的。</li>
</ul>
<p>AggregateFunction 的所有方法都必须声明为 public，而不是 static，并且命名与上述名称完全一致。方法 createAccumulator、getValue、getResultType 和 getAccumulatorType 是在 AggregateFunction 抽象类中定义的，而其他方法则是合同方法。为了定义一个聚合函数，必须扩展基类 org.apache.flink.table.function.AggregateFunction，并实现一个（或多个）accumulate 方法。方法 accumulate 可以用不同的参数类型重载，并支持变量参数。</p>
<p>下面给出了 AggregateFunction 所有方法的详细文档。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="cm">/**
</span><span class="cm">  * Base class for user-defined aggregates and table aggregates.
</span><span class="cm">  *
</span><span class="cm">  * @tparam T   the type of the aggregation result.
</span><span class="cm">  * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the
</span><span class="cm">  *             aggregated values which are needed to compute an aggregation result.
</span><span class="cm">  */</span>
<span class="k">abstract</span> <span class="k">class</span> <span class="nc">UserDefinedAggregateFunction</span><span class="o">[</span><span class="kt">T</span>, <span class="kt">ACC</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">UserDefinedFunction</span> <span class="o">{</span>

  <span class="cm">/**
</span><span class="cm">    * Creates and init the Accumulator for this (table)aggregate function.
</span><span class="cm">    *
</span><span class="cm">    * @return the accumulator with the initial value
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span><span class="k">:</span> <span class="kt">ACC</span> <span class="c1">// MANDATORY
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Returns the TypeInformation of the (table)aggregate function&#39;s result.
</span><span class="cm">    *
</span><span class="cm">    * @return The TypeInformation of the (table)aggregate function&#39;s result or null if the result
</span><span class="cm">    *         type should be automatically inferred.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">getResultType</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span> <span class="c1">// PRE-DEFINED
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Returns the TypeInformation of the (table)aggregate function&#39;s accumulator.
</span><span class="cm">    *
</span><span class="cm">    * @return The TypeInformation of the (table)aggregate function&#39;s accumulator or null if the
</span><span class="cm">    *         accumulator type should be automatically inferred.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">getAccumulatorType</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[</span><span class="kt">ACC</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span> <span class="c1">// PRE-DEFINED
</span><span class="c1"></span><span class="o">}</span>

<span class="cm">/**
</span><span class="cm">  * Base class for aggregation functions. 
</span><span class="cm">  *
</span><span class="cm">  * @tparam T   the type of the aggregation result
</span><span class="cm">  * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the
</span><span class="cm">  *             aggregated values which are needed to compute an aggregation result.
</span><span class="cm">  *             AggregateFunction represents its state using accumulator, thereby the state of the
</span><span class="cm">  *             AggregateFunction must be put into the accumulator.
</span><span class="cm">  */</span>
<span class="k">abstract</span> <span class="k">class</span> <span class="nc">AggregateFunction</span><span class="o">[</span><span class="kt">T</span>, <span class="kt">ACC</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">UserDefinedAggregateFunction</span><span class="o">[</span><span class="kt">T</span>, <span class="kt">ACC</span><span class="o">]</span> <span class="o">{</span>

  <span class="cm">/**
</span><span class="cm">    * Processes the input values and update the provided accumulator instance. The method
</span><span class="cm">    * accumulate can be overloaded with different custom types and arguments. An AggregateFunction
</span><span class="cm">    * requires at least one accumulate() method.
</span><span class="cm">    *
</span><span class="cm">    * @param accumulator           the accumulator which contains the current aggregated results
</span><span class="cm">    * @param [user defined inputs] the input value (usually obtained from a new arrived data).
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">accumulate</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="kt">ACC</span><span class="o">,</span> <span class="o">[</span><span class="kt">user</span> <span class="kt">defined</span> <span class="kt">inputs</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="c1">// MANDATORY
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Retracts the input values from the accumulator instance. The current design assumes the
</span><span class="cm">    * inputs are the values that have been previously accumulated. The method retract can be
</span><span class="cm">    * overloaded with different custom types and arguments. This function must be implemented for
</span><span class="cm">    * datastream bounded over aggregate.
</span><span class="cm">    *
</span><span class="cm">    * @param accumulator           the accumulator which contains the current aggregated results
</span><span class="cm">    * @param [user defined inputs] the input value (usually obtained from a new arrived data).
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">retract</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="kt">ACC</span><span class="o">,</span> <span class="o">[</span><span class="kt">user</span> <span class="kt">defined</span> <span class="kt">inputs</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="c1">// OPTIONAL
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Merges a group of accumulator instances into one accumulator instance. This function must be
</span><span class="cm">    * implemented for datastream session window grouping aggregate and dataset grouping aggregate.
</span><span class="cm">    *
</span><span class="cm">    * @param accumulator  the accumulator which will keep the merged aggregate results. It should
</span><span class="cm">    *                     be noted that the accumulator may contain the previous aggregated
</span><span class="cm">    *                     results. Therefore user should not replace or clean this instance in the
</span><span class="cm">    *                     custom merge method.
</span><span class="cm">    * @param its          an [[java.lang.Iterable]] pointed to a group of accumulators that will be
</span><span class="cm">    *                     merged.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="kt">ACC</span><span class="o">,</span> <span class="n">its</span><span class="k">:</span> <span class="kt">java.lang.Iterable</span><span class="o">[</span><span class="kt">ACC</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="c1">// OPTIONAL
</span><span class="c1"></span>  
  <span class="cm">/**
</span><span class="cm">    * Called every time when an aggregation result should be materialized.
</span><span class="cm">    * The returned value could be either an early and incomplete result
</span><span class="cm">    * (periodically emitted as data arrive) or the final result of the
</span><span class="cm">    * aggregation.
</span><span class="cm">    *
</span><span class="cm">    * @param accumulator the accumulator which contains the current
</span><span class="cm">    *                    aggregated results
</span><span class="cm">    * @return the aggregation result
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">getValue</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="kt">ACC</span><span class="o">)</span><span class="k">:</span> <span class="kt">T</span> <span class="c1">// MANDATORY
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Resets the accumulator for this [[AggregateFunction]]. This function must be implemented for
</span><span class="cm">    * dataset grouping aggregate.
</span><span class="cm">    *
</span><span class="cm">    * @param accumulator  the accumulator which needs to be reset
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">resetAccumulator</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="kt">ACC</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="c1">// OPTIONAL
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Returns true if this AggregateFunction can only be applied in an OVER window.
</span><span class="cm">    *
</span><span class="cm">    * @return true if the AggregateFunction requires an OVER window, false otherwise.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">requiresOver</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="kc">false</span> <span class="c1">// PRE-DEFINED
</span><span class="c1"></span><span class="o">}</span>
</code></pre></div><p>下面的例子说明了如何进行</p>
<ul>
<li>定义一个 AggregateFunction，用于计算给定列的加权平均值。</li>
<li>在 TableEnvironment 中注册该函数，并且</li>
<li>在查询中使用该函数。</li>
</ul>
<p>为了计算加权平均值，累加器需要存储所有已积累的数据的加权和和计数。在我们的例子中，我们定义了一个类 WeightedAvgAccum 作为累加器。累积器由 Flink 的检查点机制自动备份，并在故障时恢复，以保证精确的唯一性语义。</p>
<p>我们 WeightedAvg AggregateFunction 的 <code>accumulate()</code> 方法有三个输入。第一个是 WeightedAvgAccum 累加器，另外两个是用户自定义的输入：输入值 ivalue 和输入的权重 iweight。虽然 <code>retract()</code>、<code>merge()</code> 和 <code>resetAccumulator()</code> 方法对于大多数聚合类型来说并不是强制性的，但我们在下面提供它们作为例子。请注意，我们在 Scala 示例中使用了 Java 基元类型，并定义了 <code>getResultType()</code> 和  <code>getAccumulatorType()</code> 方法，因为 Flink 类型提取对于 Scala 类型并不十分有效。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">java.lang.</span><span class="o">{</span><span class="nc">Long</span> <span class="k">=&gt;</span> <span class="nc">JLong</span><span class="o">,</span> <span class="nc">Integer</span> <span class="k">=&gt;</span> <span class="nc">JInteger</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.java.tuple.</span><span class="o">{</span><span class="nc">Tuple1</span> <span class="k">=&gt;</span> <span class="nc">JTuple1</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.java.typeutils.TupleTypeInfo</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.Types</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.AggregateFunction</span>

<span class="cm">/**
</span><span class="cm"> * Accumulator for WeightedAvg.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">WeightedAvgAccum</span> <span class="k">extends</span> <span class="nc">JTuple1</span><span class="o">[</span><span class="kt">JLong</span>, <span class="kt">JInteger</span><span class="o">]</span> <span class="o">{</span>
  <span class="n">sum</span> <span class="k">=</span> <span class="mi">0L</span>
  <span class="n">count</span> <span class="k">=</span> <span class="mi">0</span>
<span class="o">}</span>

<span class="cm">/**
</span><span class="cm"> * Weighted Average user-defined aggregate function.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">WeightedAvg</span> <span class="k">extends</span> <span class="nc">AggregateFunction</span><span class="o">[</span><span class="kt">JLong</span>, <span class="kt">CountAccumulator</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span><span class="k">:</span> <span class="kt">WeightedAvgAccum</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">new</span> <span class="nc">WeightedAvgAccum</span>
  <span class="o">}</span>
  
  <span class="k">override</span> <span class="k">def</span> <span class="n">getValue</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">WeightedAvgAccum</span><span class="o">)</span><span class="k">:</span> <span class="kt">JLong</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">count</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
        <span class="kc">null</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
        <span class="n">acc</span><span class="o">.</span><span class="n">sum</span> <span class="o">/</span> <span class="n">acc</span><span class="o">.</span><span class="n">count</span>
    <span class="o">}</span>
  <span class="o">}</span>
  
  <span class="k">def</span> <span class="n">accumulate</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">WeightedAvgAccum</span><span class="o">,</span> <span class="n">iValue</span><span class="k">:</span> <span class="kt">JLong</span><span class="o">,</span> <span class="n">iWeight</span><span class="k">:</span> <span class="kt">JInteger</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">sum</span> <span class="o">+=</span> <span class="n">iValue</span> <span class="o">*</span> <span class="n">iWeight</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">iWeight</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">retract</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">WeightedAvgAccum</span><span class="o">,</span> <span class="n">iValue</span><span class="k">:</span> <span class="kt">JLong</span><span class="o">,</span> <span class="n">iWeight</span><span class="k">:</span> <span class="kt">JInteger</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">sum</span> <span class="o">-=</span> <span class="n">iValue</span> <span class="o">*</span> <span class="n">iWeight</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">count</span> <span class="o">-=</span> <span class="n">iWeight</span>
  <span class="o">}</span>
    
  <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">WeightedAvgAccum</span><span class="o">,</span> <span class="n">it</span><span class="k">:</span> <span class="kt">java.lang.Iterable</span><span class="o">[</span><span class="kt">WeightedAvgAccum</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">iter</span> <span class="k">=</span> <span class="n">it</span><span class="o">.</span><span class="n">iterator</span><span class="o">()</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="n">hasNext</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">a</span> <span class="k">=</span> <span class="n">iter</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">a</span><span class="o">.</span><span class="n">count</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">sum</span> <span class="o">+=</span> <span class="n">a</span><span class="o">.</span><span class="n">sum</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">resetAccumulator</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">WeightedAvgAccum</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">count</span> <span class="k">=</span> <span class="mi">0</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">sum</span> <span class="k">=</span> <span class="mi">0L</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getAccumulatorType</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[</span><span class="kt">WeightedAvgAccum</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">new</span> <span class="nc">TupleTypeInfo</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">WeightedAvgAccum</span><span class="o">],</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">LONG</span><span class="o">,</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">INT</span><span class="o">)</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getResultType</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[</span><span class="kt">JLong</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Types</span><span class="o">.</span><span class="nc">LONG</span>
<span class="o">}</span>

<span class="c1">// register function
</span><span class="c1"></span><span class="k">val</span> <span class="n">tEnv</span><span class="k">:</span> <span class="kt">StreamTableEnvironment</span> <span class="o">=</span> <span class="o">???</span>
<span class="n">tEnv</span><span class="o">.</span><span class="n">registerFunction</span><span class="o">(</span><span class="s">&#34;wAvg&#34;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">WeightedAvg</span><span class="o">())</span>

<span class="c1">// use function
</span><span class="c1"></span><span class="n">tEnv</span><span class="o">.</span><span class="n">sqlQuery</span><span class="o">(</span><span class="s">&#34;SELECT user, wAvg(points, level) AS avgPoints FROM userScores GROUP BY user&#34;</span><span class="o">)</span>
</code></pre></div><h2 id="表聚合函数">表聚合函数</h2>
<p>用户定义表聚合函数(UDTAGGs)将一个表(具有一个或多个属性的一行或多行)聚合到一个具有多行和多列的结果表。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/udtagg-mechanism.png" alt="img"></p>
<p>上图显示了一个表聚合的例子。假设你有一个包含饮料数据的表。该表由 id、名称和价格三列和 5 行组成。设想你需要找到表中所有饮料中价格最高的前 2 名，即执行 <code>top2()</code> 表聚合。你需要对 5 行中的每一行进行检查，结果将是一个具有前 2 个值的表。</p>
<p>用户定义的表聚合函数是通过扩展 TableAggregateFunction 类来实现的。TableAggregateFunction 的工作原理如下。首先，它需要一个累加器，它是存放聚合中间结果的数据结构。通过调用 TableAggregateFunction 的 <code>createAccumulator()</code> 方法创建一个空的累加器。随后，对每一条输入行调用函数的 <code>accumulate()</code> 方法来更新累加器。一旦所有的行都被处理完毕，函数的 <code>emitValue()</code> 方法就会被调用来计算并返回最终结果。</p>
<p>以下方法是每个 TableAggregateFunction 必须使用的。</p>
<ul>
<li>createAccumulator()</li>
<li>accumulate()</li>
</ul>
<p>Flink 的类型提取设施可能无法识别复杂的数据类型，例如，如果它们不是基本类型或简单的 POJOs。因此，与 ScalarFunction 和 TableFunction 类似，TableAggregateFunction 提供了指定结果类型（通过 <code>TableAggregateFunction#getResultType()</code>）和累积器类型（通过 <code>TableAggregateFunction#getAccumulatorType()</code>）的方法。</p>
<p>除了上述方法外，还有一些签约方法可以选择实现。这些方法中的一些方法可以让系统更高效地执行查询，而另一些方法则是某些用例所必须的。例如，如果聚合函数应该在会话组窗口的上下文中应用，那么 <code>merge()</code> 方法是强制性的（当观察到有一条记录&quot;连接&quot;它们时，需要将两个会话窗口的累加器连接起来）。</p>
<p>TableAggregateFunction 的以下方法是需要的，这取决于用例。</p>
<ul>
<li><code>retract()</code> 对于有界 OVER 窗口上的聚合是需要的。</li>
<li><code>merge()</code> 是许多批次聚合和会话窗口聚合所需要的。</li>
<li><code>resetAccumulator()</code> 是许多批处理聚合所需要的。</li>
<li><code>emitValue()</code> 是批处理和窗口聚合所需要的。</li>
</ul>
<p>TableAggregateFunction 的以下方法用于提高流作业的性能。</p>
<ul>
<li><code>emitUpdateWithRetract()</code> 用于发射在伸缩模式下更新的值。</li>
</ul>
<p>对于 emitValue 方法，则是根据累加器来发射完整的数据。以 TopN 为例，emitValue 每次都会发射所有前 n 个值。这可能会给流式作业带来性能问题。为了提高性能，用户也可以实现 emitUpdateWithRetract 方法来提高性能。该方法以回缩模式增量输出数据，即一旦有更新，我们必须在发送新的更新记录之前回缩旧记录。如果在表聚合函数中都定义了该方法，那么该方法将优先于 emitValue 方法使用，因为 emitUpdateWithRetract 被认为比 emitValue 更有效率，因为它可以增量输出值。</p>
<p>TableAggregateFunction 的所有方法都必须声明为 public，而不是 static，并完全按照上面提到的名字命名。方法 createAccumulator、getResultType 和 getAccumulatorType 是在 TableAggregateFunction 的父抽象类中定义的，而其他方法则是收缩的方法。为了定义一个表聚合函数，必须扩展基类 org.apache.flink.table.function.TableAggregateFunction，并实现一个（或多个）accumulate 方法。积累方法可以用不同的参数类型重载，并支持变量参数。</p>
<p>下面给出了 TableAggregateFunction 所有方法的详细文档。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="cm">/**
</span><span class="cm">  * Base class for user-defined aggregates and table aggregates.
</span><span class="cm">  *
</span><span class="cm">  * @tparam T   the type of the aggregation result.
</span><span class="cm">  * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the
</span><span class="cm">  *             aggregated values which are needed to compute an aggregation result.
</span><span class="cm">  */</span>
<span class="k">abstract</span> <span class="k">class</span> <span class="nc">UserDefinedAggregateFunction</span><span class="o">[</span><span class="kt">T</span>, <span class="kt">ACC</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">UserDefinedFunction</span> <span class="o">{</span>

  <span class="cm">/**
</span><span class="cm">    * Creates and init the Accumulator for this (table)aggregate function.
</span><span class="cm">    *
</span><span class="cm">    * @return the accumulator with the initial value
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span><span class="k">:</span> <span class="kt">ACC</span> <span class="c1">// MANDATORY
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Returns the TypeInformation of the (table)aggregate function&#39;s result.
</span><span class="cm">    *
</span><span class="cm">    * @return The TypeInformation of the (table)aggregate function&#39;s result or null if the result
</span><span class="cm">    *         type should be automatically inferred.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">getResultType</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span> <span class="c1">// PRE-DEFINED
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Returns the TypeInformation of the (table)aggregate function&#39;s accumulator.
</span><span class="cm">    *
</span><span class="cm">    * @return The TypeInformation of the (table)aggregate function&#39;s accumulator or null if the
</span><span class="cm">    *         accumulator type should be automatically inferred.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">getAccumulatorType</span><span class="k">:</span> <span class="kt">TypeInformation</span><span class="o">[</span><span class="kt">ACC</span><span class="o">]</span> <span class="k">=</span> <span class="kc">null</span> <span class="c1">// PRE-DEFINED
</span><span class="c1"></span><span class="o">}</span>

<span class="cm">/**
</span><span class="cm">  * Base class for table aggregation functions. 
</span><span class="cm">  *
</span><span class="cm">  * @tparam T   the type of the aggregation result
</span><span class="cm">  * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the
</span><span class="cm">  *             aggregated values which are needed to compute an aggregation result.
</span><span class="cm">  *             TableAggregateFunction represents its state using accumulator, thereby the state of
</span><span class="cm">  *             the TableAggregateFunction must be put into the accumulator.
</span><span class="cm">  */</span>
<span class="k">abstract</span> <span class="k">class</span> <span class="nc">TableAggregateFunction</span><span class="o">[</span><span class="kt">T</span>, <span class="kt">ACC</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">UserDefinedAggregateFunction</span><span class="o">[</span><span class="kt">T</span>, <span class="kt">ACC</span><span class="o">]</span> <span class="o">{</span>

  <span class="cm">/**
</span><span class="cm">    * Processes the input values and update the provided accumulator instance. The method
</span><span class="cm">    * accumulate can be overloaded with different custom types and arguments. A TableAggregateFunction
</span><span class="cm">    * requires at least one accumulate() method.
</span><span class="cm">    *
</span><span class="cm">    * @param accumulator           the accumulator which contains the current aggregated results
</span><span class="cm">    * @param [user defined inputs] the input value (usually obtained from a new arrived data).
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">accumulate</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="kt">ACC</span><span class="o">,</span> <span class="o">[</span><span class="kt">user</span> <span class="kt">defined</span> <span class="kt">inputs</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="c1">// MANDATORY
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Retracts the input values from the accumulator instance. The current design assumes the
</span><span class="cm">    * inputs are the values that have been previously accumulated. The method retract can be
</span><span class="cm">    * overloaded with different custom types and arguments. This function must be implemented for
</span><span class="cm">    * datastream bounded over aggregate.
</span><span class="cm">    *
</span><span class="cm">    * @param accumulator           the accumulator which contains the current aggregated results
</span><span class="cm">    * @param [user defined inputs] the input value (usually obtained from a new arrived data).
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">retract</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="kt">ACC</span><span class="o">,</span> <span class="o">[</span><span class="kt">user</span> <span class="kt">defined</span> <span class="kt">inputs</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="c1">// OPTIONAL
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Merges a group of accumulator instances into one accumulator instance. This function must be
</span><span class="cm">    * implemented for datastream session window grouping aggregate and dataset grouping aggregate.
</span><span class="cm">    *
</span><span class="cm">    * @param accumulator  the accumulator which will keep the merged aggregate results. It should
</span><span class="cm">    *                     be noted that the accumulator may contain the previous aggregated
</span><span class="cm">    *                     results. Therefore user should not replace or clean this instance in the
</span><span class="cm">    *                     custom merge method.
</span><span class="cm">    * @param its          an [[java.lang.Iterable]] pointed to a group of accumulators that will be
</span><span class="cm">    *                     merged.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="kt">ACC</span><span class="o">,</span> <span class="n">its</span><span class="k">:</span> <span class="kt">java.lang.Iterable</span><span class="o">[</span><span class="kt">ACC</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="c1">// OPTIONAL
</span><span class="c1"></span>  
  <span class="cm">/**
</span><span class="cm">    * Called every time when an aggregation result should be materialized. The returned value
</span><span class="cm">    * could be either an early and incomplete result  (periodically emitted as data arrive) or
</span><span class="cm">    * the final result of the  aggregation.
</span><span class="cm">    *
</span><span class="cm">    * @param accumulator the accumulator which contains the current
</span><span class="cm">    *                    aggregated results
</span><span class="cm">    * @param out         the collector used to output data
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">emitValue</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="kt">ACC</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="c1">// OPTIONAL
</span><span class="c1"></span>
  <span class="cm">/**
</span><span class="cm">    * Called every time when an aggregation result should be materialized. The returned value
</span><span class="cm">    * could be either an early and incomplete result (periodically emitted as data arrive) or
</span><span class="cm">    * the final result of the aggregation.
</span><span class="cm">    *
</span><span class="cm">    * Different from emitValue, emitUpdateWithRetract is used to emit values that have been updated.
</span><span class="cm">    * This method outputs data incrementally in retract mode, i.e., once there is an update, we
</span><span class="cm">    * have to retract old records before sending new updated ones. The emitUpdateWithRetract
</span><span class="cm">    * method will be used in preference to the emitValue method if both methods are defined in the
</span><span class="cm">    * table aggregate function, because the method is treated to be more efficient than emitValue
</span><span class="cm">    * as it can outputvalues incrementally.
</span><span class="cm">    *
</span><span class="cm">    * @param accumulator the accumulator which contains the current
</span><span class="cm">    *                    aggregated results
</span><span class="cm">    * @param out         the retractable collector used to output data. Use collect method
</span><span class="cm">    *                    to output(add) records and use retract method to retract(delete)
</span><span class="cm">    *                    records.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">emitUpdateWithRetract</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="kt">ACC</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">RetractableCollector</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="c1">// OPTIONAL
</span><span class="c1"></span> 
  <span class="cm">/**
</span><span class="cm">    * Collects a record and forwards it. The collector can output retract messages with the retract
</span><span class="cm">    * method. Note: only use it in `emitRetractValueIncrementally`.
</span><span class="cm">    */</span>
  <span class="k">trait</span> <span class="nc">RetractableCollector</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">Collector</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="o">{</span>
    
    <span class="cm">/**
</span><span class="cm">      * Retract a record.
</span><span class="cm">      *
</span><span class="cm">      * @param record The record to retract.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">retract</span><span class="o">(</span><span class="n">record</span><span class="k">:</span> <span class="kt">T</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>下面的例子说明了如何进行</p>
<ul>
<li>定义一个 TableAggregateFunction，用于计算给定列上的前 2 个值。</li>
<li>在 TableEnvironment 中注册该函数，并且</li>
<li>在 Table API 查询中使用该函数(TableAggregateFunction 仅由 Table API 支持)。</li>
</ul>
<p>为了计算前 2 名的值，累加器需要存储所有已积累的数据中最大的 2 个值。在我们的例子中，我们定义了一个类 Top2Accum 作为累加器。累积器会被 Flink 的检查点机制自动备份，并在故障时恢复，以保证精确的 once 语义。</p>
<p>我们 Top2 TableAggregateFunction 的 <code>accumulate()</code> 方法有两个输入。第一个是 Top2Accum 累加器，另一个是用户定义的输入：输入值 v，虽然 <code>merge()</code> 方法对于大多数表聚合类型来说不是强制性的，但我们在下面提供它作为例子。请注意，我们在 Scala 示例中使用了 Java 基元类型，并定义了 <code>getResultType()</code> 和 <code>getAccumulatorType()</code> 方法，因为 Flink 类型提取对 Scala 类型的效果并不好。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">java.lang.</span><span class="o">{</span><span class="nc">Integer</span> <span class="k">=&gt;</span> <span class="nc">JInteger</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.Types</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.TableAggregateFunction</span>

<span class="cm">/**
</span><span class="cm"> * Accumulator for top2.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">Top2Accum</span> <span class="o">{</span>
  <span class="k">var</span> <span class="n">first</span><span class="k">:</span> <span class="kt">JInteger</span> <span class="o">=</span> <span class="k">_</span>
  <span class="k">var</span> <span class="n">second</span><span class="k">:</span> <span class="kt">JInteger</span> <span class="o">=</span> <span class="k">_</span>
<span class="o">}</span>

<span class="cm">/**
</span><span class="cm"> * The top2 user-defined table aggregate function.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">Top2</span> <span class="k">extends</span> <span class="nc">TableAggregateFunction</span><span class="o">[</span><span class="kt">JTuple2</span><span class="o">[</span><span class="kt">JInteger</span>, <span class="kt">JInteger</span><span class="o">]</span>, <span class="kt">Top2Accum</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span><span class="k">:</span> <span class="kt">Top2Accum</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">acc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Top2Accum</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">first</span> <span class="k">=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="k">=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span>
    <span class="n">acc</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">accumulate</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">Top2Accum</span><span class="o">,</span> <span class="n">v</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">v</span> <span class="o">&gt;</span> <span class="n">acc</span><span class="o">.</span><span class="n">first</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="k">=</span> <span class="n">acc</span><span class="o">.</span><span class="n">first</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">first</span> <span class="k">=</span> <span class="n">v</span>
    <span class="o">}</span> <span class="k">else</span> <span class="k">if</span> <span class="o">(</span><span class="n">v</span> <span class="o">&gt;</span> <span class="n">acc</span><span class="o">.</span><span class="n">second</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="k">=</span> <span class="n">v</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">Top2Accum</span><span class="o">,</span> <span class="n">its</span><span class="k">:</span> <span class="kt">JIterable</span><span class="o">[</span><span class="kt">Top2Accum</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">iter</span> <span class="k">=</span> <span class="n">its</span><span class="o">.</span><span class="n">iterator</span><span class="o">()</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="n">hasNext</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">top2</span> <span class="k">=</span> <span class="n">iter</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
      <span class="n">accumulate</span><span class="o">(</span><span class="n">acc</span><span class="o">,</span> <span class="n">top2</span><span class="o">.</span><span class="n">first</span><span class="o">)</span>
      <span class="n">accumulate</span><span class="o">(</span><span class="n">acc</span><span class="o">,</span> <span class="n">top2</span><span class="o">.</span><span class="n">second</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">emitValue</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">Top2Accum</span><span class="o">,</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">JTuple2</span><span class="o">[</span><span class="kt">JInteger</span>, <span class="kt">JInteger</span><span class="o">]])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// emit the value and rank
</span><span class="c1"></span>    <span class="k">if</span> <span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">first</span> <span class="o">!=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">JTuple2</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">first</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
    <span class="o">}</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="o">!=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">JTuple2</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">second</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// init table
</span><span class="c1"></span><span class="k">val</span> <span class="n">tab</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// use function
</span><span class="c1"></span><span class="n">tab</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span>&#39;key<span class="o">)</span>
  <span class="o">.</span><span class="n">flatAggregate</span><span class="o">(</span><span class="n">top2</span><span class="o">(</span>&#39;a<span class="o">)</span> <span class="n">as</span> <span class="o">(</span>&#39;v<span class="o">,</span> &#39;rank<span class="o">))</span>
  <span class="o">.</span><span class="n">select</span><span class="o">(</span>&#39;key<span class="o">,</span> &#39;v<span class="o">,</span> &#39;rank<span class="o">)</span>
</code></pre></div><p>下面的例子展示了如何使用 emitUpdateWithRetract 方法来只发送更新。在我们的例子中，为了只发出更新，累加器同时保留新旧 top2 的值。注意：如果 topN 的 N 很大，那么同时保留新旧值的效率可能很低。解决这种情况的方法之一是在累加方法中把输入的记录存储到累加器中，然后在 emitUpdateWithRetract 中进行计算。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">java.lang.</span><span class="o">{</span><span class="nc">Integer</span> <span class="k">=&gt;</span> <span class="nc">JInteger</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.api.Types</span>
<span class="k">import</span> <span class="nn">org.apache.flink.table.functions.TableAggregateFunction</span>

<span class="cm">/**
</span><span class="cm"> * Accumulator for top2.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">Top2Accum</span> <span class="o">{</span>
  <span class="k">var</span> <span class="n">first</span><span class="k">:</span> <span class="kt">JInteger</span> <span class="o">=</span> <span class="k">_</span>
  <span class="k">var</span> <span class="n">second</span><span class="k">:</span> <span class="kt">JInteger</span> <span class="o">=</span> <span class="k">_</span>
  <span class="k">var</span> <span class="n">oldFirst</span><span class="k">:</span> <span class="kt">JInteger</span> <span class="o">=</span> <span class="k">_</span>
  <span class="k">var</span> <span class="n">oldSecond</span><span class="k">:</span> <span class="kt">JInteger</span> <span class="o">=</span> <span class="k">_</span>
<span class="o">}</span>

<span class="cm">/**
</span><span class="cm"> * The top2 user-defined table aggregate function.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">Top2</span> <span class="k">extends</span> <span class="nc">TableAggregateFunction</span><span class="o">[</span><span class="kt">JTuple2</span><span class="o">[</span><span class="kt">JInteger</span>, <span class="kt">JInteger</span><span class="o">]</span>, <span class="kt">Top2Accum</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span><span class="k">:</span> <span class="kt">Top2Accum</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">acc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Top2Accum</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">first</span> <span class="k">=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="k">=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">oldFirst</span> <span class="k">=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">oldSecond</span> <span class="k">=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span>
    <span class="n">acc</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">accumulate</span><span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="kt">Top2Accum</span><span class="o">,</span> <span class="n">v</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">v</span> <span class="o">&gt;</span> <span class="n">acc</span><span class="o">.</span><span class="n">first</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="k">=</span> <span class="n">acc</span><span class="o">.</span><span class="n">first</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">first</span> <span class="k">=</span> <span class="n">v</span>
    <span class="o">}</span> <span class="k">else</span> <span class="k">if</span> <span class="o">(</span><span class="n">v</span> <span class="o">&gt;</span> <span class="n">acc</span><span class="o">.</span><span class="n">second</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="k">=</span> <span class="n">v</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">emitUpdateWithRetract</span><span class="o">(</span>
    <span class="n">acc</span><span class="k">:</span> <span class="kt">Top2Accum</span><span class="o">,</span>
    <span class="n">out</span><span class="k">:</span> <span class="kt">RetractableCollector</span><span class="o">[</span><span class="kt">JTuple2</span><span class="o">[</span><span class="kt">JInteger</span>, <span class="kt">JInteger</span><span class="o">]])</span>
  <span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">first</span> <span class="o">!=</span> <span class="n">acc</span><span class="o">.</span><span class="n">oldFirst</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// if there is an update, retract old value then emit new value.
</span><span class="c1"></span>      <span class="k">if</span> <span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">oldFirst</span> <span class="o">!=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">out</span><span class="o">.</span><span class="n">retract</span><span class="o">(</span><span class="nc">JTuple2</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">oldFirst</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
      <span class="o">}</span>
      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">JTuple2</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">first</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">oldFirst</span> <span class="k">=</span> <span class="n">acc</span><span class="o">.</span><span class="n">first</span>
    <span class="o">}</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">second</span> <span class="o">!=</span> <span class="n">acc</span><span class="o">.</span><span class="n">oldSecond</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// if there is an update, retract old value then emit new value.
</span><span class="c1"></span>      <span class="k">if</span> <span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">oldSecond</span> <span class="o">!=</span> <span class="nc">Int</span><span class="o">.</span><span class="nc">MinValue</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">out</span><span class="o">.</span><span class="n">retract</span><span class="o">(</span><span class="nc">JTuple2</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">oldSecond</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
      <span class="o">}</span>
      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="nc">JTuple2</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="n">acc</span><span class="o">.</span><span class="n">second</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">oldSecond</span> <span class="k">=</span> <span class="n">acc</span><span class="o">.</span><span class="n">second</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// init table
</span><span class="c1"></span><span class="k">val</span> <span class="n">tab</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// use function
</span><span class="c1"></span><span class="n">tab</span>
  <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span>&#39;key<span class="o">)</span>
  <span class="o">.</span><span class="n">flatAggregate</span><span class="o">(</span><span class="n">top2</span><span class="o">(</span>&#39;a<span class="o">)</span> <span class="n">as</span> <span class="o">(</span>&#39;v<span class="o">,</span> &#39;rank<span class="o">))</span>
  <span class="o">.</span><span class="n">select</span><span class="o">(</span>&#39;key<span class="o">,</span> &#39;v<span class="o">,</span> &#39;rank<span class="o">)</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/function" term="function" label="Function" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[用户定义函数]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-user-defined-functions/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-user-defined-functions/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>User Defined Functions</blockquote><h1 id="用户自定义函数">用户自定义函数</h1>
<p>大多数操作符都需要用户定义的函数。本节列出了如何指定这些函数的不同方法。我们还涵盖了累加器，它可以用来深入了解您的 Flink 应用程序。</p>
<h2 id="lambda-函数">Lambda 函数</h2>
<p>在前面的例子中已经看到，所有的操作符都接受 lambda 函数来描述操作。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">_</span><span class="o">.</span><span class="n">startsWith</span><span class="o">(</span><span class="s">&#34;http://&#34;</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">data</span><span class="k">:</span> <span class="kt">DataSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="o">(</span><span class="n">i1</span><span class="o">,</span><span class="n">i2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i2</span> <span class="o">}</span>
<span class="c1">// 或
</span><span class="c1"></span><span class="n">data</span><span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="k">_</span> <span class="o">+</span> <span class="k">_</span> <span class="o">}</span>
</code></pre></div><h3 id="富函数rich-functions">富函数(Rich functions)</h3>
<p>所有以 lambda 函数作为参数的变换都可以以富函数作为参数。例如，我们可以不使用:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">toInt</span> <span class="o">}</span>
</code></pre></div><p>你可以编写:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">MyMapFunction</span> <span class="k">extends</span> <span class="nc">RichMapFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">in</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span><span class="kt">Int</span> <span class="o">=</span> <span class="o">{</span> <span class="n">in</span><span class="o">.</span><span class="n">toInt</span> <span class="o">}</span>
<span class="o">};</span>
</code></pre></div><p>并将该函数传递给 map 转换:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyMapFunction</span><span class="o">())</span>
</code></pre></div><p>丰富的函数也可以定义为匿名类:</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">(</span><span class="k">new</span> <span class="nc">RichMapFunction</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">map</span><span class="o">(</span><span class="n">in</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span><span class="kt">Int</span> <span class="o">=</span> <span class="o">{</span> <span class="n">in</span><span class="o">.</span><span class="n">toInt</span> <span class="o">}</span>
<span class="o">})</span>
</code></pre></div><p>丰富的函数除了提供用户定义的函数（map、reduce等）外，还提供了四个方法：<code>open</code>、<code>close</code>、<code>getRuntimeContext</code> 和 <code>setRuntimeContext</code>。这些方法可以用于为函数设置参数（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#passing-parameters-to-functions">Passing Parameters to Functions</a>）、创建和最终确定局部状态、访问广播变量（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#broadcast-variables">Broadcast Variables</a>）、访问运行时信息，如累加器和计数器（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html#accumulators--counters">Accumulators and Counters</a>）以及迭代信息（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">Iterations</a>）。</p>
<h3 id="累积器和计数器">累积器和计数器</h3>
<p>累积器是一个简单的构造，有一个加法运算和一个最终的累积结果，在作业结束后就可以使用。</p>
<p>最直接的累加器是一个计数器，你可以使用 <code>Accumulator.add(V value)</code> 方法对它进行增量。在作业结束时，Flink 将对所有部分结果进行加总（合并）并将结果发送给客户端。累积器在调试期间或如果你快速想了解更多的数据时是很有用的。</p>
<p>Flink 目前有以下内置的累加器。它们每个都实现了 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java">Accumulator</a> 接口。</p>
<ul>
<li><a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/IntCounter.java">IntCounter</a>、<a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/LongCounter.java">LongCounter</a> 和 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/DoubleCounter.java">DoubleCounter</a>。请看下面一个使用计数器的例子。</li>
<li><a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Histogram.java">直方图</a>。一个离散数量的直方块的直方图实现。在内部，它只是一个从 Integer 到 Integer 的映射。你可以用它来计算值的分布，例如字数程序的每行字数分布。</li>
</ul>
<p><strong>如何使用累加器:</strong></p>
<p>首先你必须在用户定义的转换函数中创建一个累加器对象(这里是一个计数器)，在你想使用它的地方。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">private</span> <span class="n">IntCounter</span> <span class="n">numLines</span> <span class="o">=</span> <span class="k">new</span> <span class="n">IntCounter</span><span class="o">();</span>
</code></pre></div><p>其次，你必须注册累加器对象，通常是在富函数的 <code>open()</code> 方法中。在这里你还需要定义名称。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">getRuntimeContext</span><span class="o">().</span><span class="n">addAccumulator</span><span class="o">(</span><span class="s">&#34;num-lines&#34;</span><span class="o">,</span> <span class="k">this</span><span class="o">.</span><span class="n">numLines</span><span class="o">);</span>
</code></pre></div><p>现在你可以在运算函数的任何地方使用累加器，包括在 <code>open()</code> 和 <code>close()</code> 方法中。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">this</span><span class="o">.</span><span class="n">numLines</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
</code></pre></div><p>整体结果将存储在 <code>JobExecutionResult</code> 对象中，该对象由执行环境的 <code>execute()</code> 方法返回（目前只有在执行等待作业完成的情况下才有效）。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="n">myJobExecutionResult</span><span class="o">.</span><span class="n">getAccumulatorResult</span><span class="o">(</span><span class="s">&#34;num-lines&#34;</span><span class="o">)</span>
</code></pre></div><p>所有的累加器在每个作业中共享一个命名空间。因此你可以在你的工作的不同操作函数中使用同一个累加器。Flink 会在内部合并所有同名的累加器。</p>
<p>关于累加器和迭代的说明。目前，累加器的结果只有在整个作业结束后才会出现。我们计划在下一次迭代中也能获得上一次迭代的结果。你可以使用 <a href="v">Aggregators</a> 来计算每次迭代的统计数据，并根据这些统计数据来终止迭代。</p>
<p><strong>自定义累加器:</strong></p>
<p>要实现你自己的累加器，你只需要编写你的 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java">Accumulator</a> 接口的实现。如果你认为你的自定义累加器应该和Flink一起发布，请随时创建一个pull request。</p>
<p>你可以选择实现 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java">Accumulator</a> 或 <a href="https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/SimpleAccumulator.java">SimpleAccumulator</a>。</p>
<p><code>Accumulator&lt;V,R&gt;</code> 是最灵活的。它为要添加的值定义了一个类型 V，为最终结果定义了一个结果类型 R。例如，对于一个直方图，V 是一个数字，R 是一个直方图。 <code>SimpleAccumulator</code> 适用于两种类型都相同的情况，例如计数器。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[窗口]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-windows/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-asynchronous-io-for-external-data-access/?utm_source=atom_feed" rel="related" type="text/html" title="用于外部数据访问的异步 I/O" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-windows/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Windows</blockquote><h1 id="窗口">窗口</h1>
<p>窗口是处理无限流的核心。窗口将流分割成有限大小的&quot;桶&quot;，我们可以对其应用计算。本文档主要介绍 Flink 中如何进行窗口化，以及程序员如何从其提供的功能中最大限度地受益。</p>
<p>下面介绍了一个窗口化 Flink 程序的一般结构。第一个片段指的是 keyed 流，而第二个片段指的是 non-keyed 流。正如人们所看到的那样，唯一的区别是 keyed 流的 <code>keyBy(...)</code> 调用和 non-keyed 流的 <code>window(...)</code> 变成了 <code>windowAll(...)</code>。这也将作为本页面其他内容的路线图。</p>
<p><strong>Keyed 窗口</strong></p>
<pre><code>stream
       .keyBy(...)               &lt;-  keyed 与 non-keyed 窗口的对比
       .window(...)              &lt;-  必须的: &quot;assigner&quot;
      [.trigger(...)]            &lt;-  可选的: &quot;trigger&quot; (否则使用默认的 trigger)
      [.evictor(...)]            &lt;-  可选的: &quot;evictor&quot; (否则没有 evictor)
      [.allowedLateness(...)]    &lt;-  可选的: &quot;lateness&quot; (否则为零)
      [.sideOutputLateData(...)] &lt;-  可选的: &quot;output tag&quot; (否则迟到数据无侧输出)
       .reduce/aggregate/fold/apply()      &lt;-  必须的: &quot;function&quot;
      [.getSideOutput(...)]      &lt;-  可选的: &quot;output tag&quot;
</code></pre><p><strong>Non-Keyed 窗口</strong></p>
<pre><code>stream
       .windowAll(...)           &lt;-  必须的: &quot;assigner&quot;
      [.trigger(...)]            &lt;-  可选的: &quot;trigger&quot; (否则使用默认的 trigger)
      [.evictor(...)]            &lt;-  可选的: &quot;evictor&quot; (否则没有 evictor)
      [.allowedLateness(...)]    &lt;-  可选的: &quot;lateness&quot; (否则为零)
      [.sideOutputLateData(...)] &lt;-  可选的: &quot;output tag&quot; (否则迟到数据无侧输出)
       .reduce/aggregate/fold/apply()      &lt;-  必须的: &quot;function&quot;
      [.getSideOutput(...)]      &lt;-  可选的: &quot;output tag&quot;
</code></pre><p>在上面，方括号中的命令(<code>[...]</code>)是可选的。这表明 Flink 允许你以多种不同的方式定制你的窗口逻辑，以便它最适合你的需求。</p>
<h2 id="窗口生命周期">窗口生命周期</h2>
<p>简而言之，当第一个应该属于这个窗口的元素到达时，就会创建一个窗口，当时间（事件时间或处理时间）经过(passes)它的结束时间戳加上用户指定的允许延迟时，这个窗口就会被完全移除（见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#allowed-lateness">允许延迟</a>）。Flink 只保证对基于时间的窗口进行移除，而不保证对其他类型的窗口，如全局窗口进行移除（见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-assigners">窗口分配器</a>）。例如，基于事件-时间的窗口策略每5分钟创建一个非重叠（或翻滚）的窗口，并且允许的延迟为1分钟，当第一个具有时间戳的元素落入这个区间时，Flink 将为 12:00 和 12:05 之间的区间创建一个新的窗口，当水印通过 12:06 的时间戳时，它将删除它。</p>
<p>此外，每个窗口将有一个触发器(见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#triggers">触发器</a>)和一个函数(ProcessWindowFunction、ReduceFunction、AggregateFunction或FoldFunction)(见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-functions">窗口函数</a>)。函数将包含要应用于窗口内容的计算，而触发器则指定了窗口被认为可以应用函数的条件。触发策略可能是&quot;当窗口中的元素数量超过4时&quot;，或者&quot;当水印经过窗口的末端时&quot;。触发器还可以决定在创建和删除窗口之间的任何时间(any time between its creation and removal)清除窗口的内容。在这种情况下，清除只指窗口中的元素，而不是窗口元数据。这意味着新的数据仍然可以被添加到该窗口中。</p>
<p>除上述之外，您还可以指定一个 Evictor(见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#evictors">Evictors</a>)，它将能够在触发器触发后以及在函数应用之前和/或之后从窗口中删除元素。</p>
<p>在下文中，我们将对上述每个组件进行更详细的介绍。我们先从上述代码段中必须的部分开始(参见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#keyed-vs-non-keyed-windows">Keyed vs Non-Keyed 窗口</a>、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-assigner">窗口分配器</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-function">窗口函数</a>)，然后再转向可选部分。</p>
<h2 id="keyed-与-non-keyed-窗口的对比">Keyed 与 Non-Keyed 窗口的对比</h2>
<p>首先要指定的是您的流是否应该是 keyed 的。这必须在定义窗口之前完成。使用 <code>keyBy(...)</code> 将把您的无限流分割成逻辑 keyed 流。如果没有调用 <code>keyBy(...)</code>，那么您的流就不是 keyed 流。</p>
<p>在 keyed 流的情况下，传入事件的任何属性都可以被用作键（更多细节在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html#keyed-datastream">这里</a>）。拥有一个 keyed 流将允许你的窗口计算由多个任务并行执行，因为每个逻辑 keyed 流可以独立于其他流进行处理。所有指向同一键的元素将被发送到同一个并行任务(task)。</p>
<p>在 non-keyed 流的情况下，您的原始流不会被分割成多个逻辑流，所有的窗口化逻辑将由一个任务(task)来执行，即并行度为1。</p>
<h2 id="窗口分配器">窗口分配器</h2>
<p>在指定流是否是 keyed 流之后，下一步是定义窗口分配器。窗口分配器定义了如何将元素分配给窗口。这是通过在 <code>window(...)</code>（对于 keyed 流）或 <code>windowAll()</code>（对于 non-keyed 流）调用中指定您所选择的 <code>WindowAssigner</code> 来实现的。</p>
<p><code>WindowAssigner</code> 负责将每个传入的元素分配给一个或多个窗口。Flink 为最常见的用例提供了预定义的窗口分配器，即滚动窗口、滑动窗口、会话窗口和全局窗口。您也可以通过扩展 <code>WindowAssigner</code> 类来实现自定义窗口分配器。所有内置的窗口分配器（除了全局窗口）都是基于时间将元素分配给窗口，时间可以是处理时间，也可以是事件时间。请查看我们关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间</a>的部分，了解处理时间和事件时间之间的区别，以及时间戳和水印是如何生成的。</p>
<p>基于时间的窗口有一个开始时间戳（包括）和结束时间戳（不包括），共同描述窗口的大小。在代码中，Flink 在处理基于时间的窗口时使用了 <code>TimeWindow</code>，它有查询开始和结束时间戳的方法，还有一个额外的方法 <code>maxTimestamp()</code>，可以返回给定窗口的最大允许时间戳。</p>
<p>在下文中，我们将展示 Flink 的预定义窗口分配器是如何工作的，以及如何在 DataStream 程序中使用它们。下图直观地展示了每个分配器的工作情况。紫色的圆圈代表流的元素，这些元素被某个键（在本例中是用户1、用户2和用户3）分割。x轴显示的是时间的进度。</p>
<h2 id="滚动窗口">滚动窗口</h2>
<p>滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口。滚动窗口有一个固定的大小，并且不重叠。例如，如果你指定了一个大小为5分钟的滚动窗口，那么当前的窗口将被评估，并且每隔5分钟就会启动一个新的窗口，如下图所示。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/tumbling-windows.svg" alt="img"></p>
<p>以下代码片段展示了如何使用滚动窗口。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// tumbling event-time windows
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// tumbling processing-time windows
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingProcessingTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// daily tumbling event-time windows offset by -8 hours.
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">days</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(-</span><span class="mi">8</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>
</code></pre></div><p>时间间隔可以使用 <code>Time.milliseconds(x)</code>, <code>Time.seconds(x)</code>, <code>Time.minutes(x)</code> 等中的一种来指定。</p>
<p>如最后一个例子所示，滚动窗口分配器还可以采用一个可选的偏移量(<code>offset</code>)参数，用于改变窗口的对齐方式。例如，在没有偏移量的情况下，每小时的滚动窗口与纪元对齐，也就是说，你会得到诸如 <code>1:00:00.000 - 1:59:59.999</code>，<code>2:00:00.000 - 2:59:59.999</code> 等窗口。如果你想改变这一点，你可以给出一个偏移量。例如，如果偏移量为15分钟，您将得到 <code>1:15:00.000 - 2:14:59.999</code>，<code>2:15:00.000 - 3:14:59.999</code> 等。偏移量的一个重要用途是调整窗口到 UTC-0 以外的时区。例如，在中国，你必须指定一个 <code>Time.hours(-8)</code> 的偏移量。</p>
<h2 id="滑动窗口">滑动窗口</h2>
<p>滑动窗口分配器将元素分配给固定长度的窗口。与滚动窗口分配器类似，窗口的大小由窗口大小(window size)参数配置。一个额外的窗口滑动(window slide)参数控制滑动窗口的启动频率。因此，如果滑动窗口的滑块小于窗口大小，滑动窗口可以重叠。在这种情况下，元素被分配到多个窗口。</p>
<p>例如，你可以有10分钟大小的窗口，滑动5分钟。这样，每隔5分钟就会有一个窗口，包含过去10分钟内到达的事件，如下图所示。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/sliding-windows.svg" alt="img"></p>
<p>以下代码片段展示了如何使用滑动窗口。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// sliding event-time windows
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">SlidingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// sliding processing-time windows
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">SlidingProcessingTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// sliding processing-time windows offset by -8 hours
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">SlidingProcessingTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(</span><span class="mi">12</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span> <span class="nc">Time</span><span class="o">.</span><span class="n">hours</span><span class="o">(-</span><span class="mi">8</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>
</code></pre></div><p>时间间隔可以通过使用 <code>Time.milliseconds(x)</code>, <code>Time.seconds(x)</code>, <code>Time.minutes(x)</code> 等中的一个来指定。</p>
<p>如上一个例子所示，滑动窗口分配器还可以采取一个可选的偏移量(<code>offset</code>)参数，用于改变窗口的对齐方式。例如，在没有偏移量的情况下，每小时滑动30分钟的窗口与纪元对齐，也就是说，你将得到 <code>1:00:00.000 - 1:59:59.999</code>，<code>1:30:00.000 - 2:29:59.999</code> 等窗口。如果你想改变这一点，你可以给出一个偏移量。例如，如果偏移量为15分钟，您将得到 <code>1:15:00.000 - 2:14:59.999</code>，<code>1:45:00.000 - 2:44:59.999</code> 等。偏移量的一个重要用途是调整窗口到 UTC-0 以外的时区。例如，在中国，你必须指定一个 <code>Time.hours(-8)</code> 的偏移。</p>
<h2 id="会话窗口">会话窗口</h2>
<p>会话窗口分配器按活动的会话对元素进行分组。与滚动窗口和滑动窗口不同，会话窗口不重叠，也没有固定的开始和结束时间。相反，当会话窗口在一定时间内没有接收到元素时，也就是在不活动的间隙发生时，会话窗口就会关闭。会话窗口分配器可以配置一个静态的会话间隙(session gap)，也可以配置一个会话间隙提取函数，该函数定义了多长时间的不活动期。当这个时间段(period)到期(expires)时，当前会话关闭，后续元素被分配到一个新的会话窗口。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/session-windows.svg" alt="img"></p>
<p>以下代码片段展示了如何使用会话窗口。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="c1">// event-time session windows with static gap
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">EventTimeSessionWindows</span><span class="o">.</span><span class="n">withGap</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">minutes</span><span class="o">(</span><span class="mi">10</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// event-time session windows with dynamic gap
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">EventTimeSessionWindows</span><span class="o">.</span><span class="n">withDynamicGap</span><span class="o">(</span><span class="k">new</span> <span class="nc">SessionWindowTimeGapExtractor</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="o">{</span>
      <span class="k">override</span> <span class="k">def</span> <span class="n">extract</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="o">{</span>
        <span class="c1">// determine and return session gap
</span><span class="c1"></span>      <span class="o">}</span>
    <span class="o">}))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="c1">// processing-time session windows with static gap
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">ProcessingTimeSessionWindows</span><span class="o">.</span><span class="n">withGap</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">minutes</span><span class="o">(</span><span class="mi">10</span><span class="o">)))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>


<span class="c1">// processing-time session windows with dynamic gap
</span><span class="c1"></span><span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">DynamicProcessingTimeSessionWindows</span><span class="o">.</span><span class="n">withDynamicGap</span><span class="o">(</span><span class="k">new</span> <span class="nc">SessionWindowTimeGapExtractor</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="o">{</span>
      <span class="k">override</span> <span class="k">def</span> <span class="n">extract</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="o">{</span>
        <span class="c1">// determine and return session gap
</span><span class="c1"></span>      <span class="o">}</span>
    <span class="o">}))</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>
</code></pre></div><p>静态间隙可以通过使用 <code>Time.milliseconds(x)</code>, <code>Time.seconds(x)</code>, <code>Time.minutes(x)</code> 等之一来指定。</p>
<p>动态间隙可以通过实现 <code>SessionWindowTimeGapExtractor</code> 接口来指定。</p>
<p>注意: 由于会话窗口没有固定的开始和结束，所以它们的评估方式与滚动和滑动窗口不同。在内部，会话窗口操作符为每个到达的记录创建一个新的窗口，如果它们彼此之间的距离比定义的间隙更近，就会将窗口合并在一起。为了能够合并，会话窗口操作符需要一个合并<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#triggers">触发器</a>和一个合并<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-functions">窗口函数</a>，如 ReduceFunction、AggregateFunction 或 ProcessWindowFunction(FoldFunction 不能合并)。</p>
<h2 id="全局窗口">全局窗口</h2>
<p>全局窗口分配器将具有相同键的所有元素分配到同一个全局窗口。只有当你还指定了一个自定义<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#triggers">触发器</a>时，这种窗口方案才有用。否则，任何计算都不会被执行，因为全局窗口没有一个自然的终点，我们可以在那里处理聚集的元素。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/non-windowed.svg" alt="img"></p>
<p>下面的代码片段展示了如何使用全局窗口。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">GlobalWindows</span><span class="o">.</span><span class="n">create</span><span class="o">())</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>
</code></pre></div><h2 id="窗口函数">窗口函数</h2>
<p>在定义了窗口分配器之后，我们需要指定我们要对这些窗口中的每一个窗口进行的计算。这是窗口函数的责任，一旦系统确定一个窗口准备好进行处理，它就会用来处理每个（可能是 keyed 的）窗口的元素（关于 Flink 如何确定窗口准备好，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#triggers">触发器</a>）。</p>
<p>窗口函数可以是 <code>ReduceFunction</code>、<code>AggregateFunction</code>、<code>FoldFunction</code> 或 <code>ProcessWindowFunction</code> 中的一种。前两个可以更有效地执行（见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#state%20size">状态大小</a>部分），因为 Flink 可以在每个窗口到达时增量地聚合元素。<code>ProcessWindowFunction</code> 可以为一个窗口中包含的所有元素获取一个 <code>Iterable</code>，以及关于元素所属窗口的附加元信息。</p>
<p>带有 <code>ProcessWindowFunction</code> 的窗口化转换不能像其他情况一样高效执行，因为 Flink 在调用函数之前必须在内部缓冲一个窗口的所有元素。通过将 <code>ProcessWindowFunction</code> 与 <code>ReduceFunction</code>、<code>AggregateFunction</code> 或 <code>FoldFunction</code> 结合起来，既可以得到窗口元素的增量聚合，也可以得到 <code>ProcessWindowFunction</code> 接收到的额外的窗口元数据，从而缓解这种情况。我们将查看这些变体的每个例子。</p>
<h3 id="reducefunction">ReduceFunction</h3>
<p><code>ReduceFunction</code> 指定了如何将输入的两个元素组合起来以产生相同类型的输出元素。Flink 使用 <code>ReduceFunction</code> 来增量聚合一个窗口的元素。</p>
<p><code>ReduceFunction</code> 可以这样定义和使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">reduce</span> <span class="o">{</span> <span class="o">(</span><span class="n">v1</span><span class="o">,</span> <span class="n">v2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">v1</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">v1</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">v2</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div><p>上面的例子把一个窗口中所有元素的元组的第二个字段相加起来。</p>
<h3 id="aggregatefunction">AggregateFunction</h3>
<p><code>AggregateFunction</code> 是 <code>ReduceFunction</code> 的通用版本，它有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型是输入流中元素的类型，AggregateFunction 有一个方法用于将一个输入元素添加到累加器中。该接口还有创建一个初始累加器、将两个累加器合并成一个累加器以及从一个累加器中提取一个输出（类型为 OUT）的方法。我们将在下面的例子中看到这些方法是如何工作的。</p>
<p>和 ReduceFunction 一样，Flink 会在窗口的输入元素到达时，对它们进行增量聚合。</p>
<p>AggregateFunction 可以这样定义和使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="cm">/**
</span><span class="cm"> * The accumulator is used to keep a running sum and a count. The [getResult] method
</span><span class="cm"> * computes the average.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">AverageAggregate</span> <span class="k">extends</span> <span class="nc">AggregateFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="kt">Double</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span> <span class="k">=</span> <span class="o">(</span><span class="mi">0L</span><span class="o">,</span> <span class="mi">0L</span><span class="o">)</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">add</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Long</span><span class="o">),</span> <span class="n">accumulator</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span>
    <span class="o">(</span><span class="n">accumulator</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="mi">1L</span><span class="o">)</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getResult</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_1</span> <span class="o">/</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_2</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">),</span> <span class="n">b</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span>
    <span class="o">(</span><span class="n">a</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">a</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="k">new</span> <span class="nc">AverageAggregate</span><span class="o">)</span>
</code></pre></div><p>上面的例子是计算窗口中元素的第二个字段的平均值。</p>
<h3 id="foldfunction">FoldFunction</h3>
<p>FoldFunction 指定了窗口的输入元素如何与输出类型的元素相结合。对于添加到窗口的每个元素和当前的输出值，都会递增地调用 FoldFunction。第一个元素与输出类型的预定义初始值相结合。</p>
<p>可以这样定义和使用 FoldFunction。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">fold</span><span class="o">(</span><span class="s">&#34;&#34;</span><span class="o">)</span> <span class="o">{</span> <span class="o">(</span><span class="n">acc</span><span class="o">,</span> <span class="n">v</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">acc</span> <span class="o">+</span> <span class="n">v</span><span class="o">.</span><span class="n">_2</span> <span class="o">}</span>
</code></pre></div><p>上面的例子将所有输入的 Long 值追加到一个初始的空字符串中。</p>
<p>注意 <code>fold()</code> 不能用于会话窗口或其他可合并窗口。</p>
<h3 id="processwindowfunction">ProcessWindowFunction</h3>
<p>ProcessWindowFunction 得到一个包含窗口所有元素的 Iterable，以及一个可以访问时间和状态信息的 Context 对象，这使得它能够提供比其他窗口函数更多的灵活性。这是以性能和资源消耗为代价的，因为元素不能增量聚合，而是需要在内部缓冲，直到窗口被认为可以处理为止。</p>
<p>ProcessWindowFunction 的签名如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">abstract</span> <span class="k">class</span> <span class="nc">ProcessWindowFunction</span><span class="o">[</span><span class="kt">IN</span>, <span class="kt">OUT</span>, <span class="kt">KEY</span>, <span class="kt">W</span> <span class="k">&lt;:</span> <span class="kt">Window</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">Function</span> <span class="o">{</span>

  <span class="cm">/**
</span><span class="cm">    * Evaluates the window and outputs none or several elements.
</span><span class="cm">    *
</span><span class="cm">    * @param key      The key for which this window is evaluated.
</span><span class="cm">    * @param context  The context in which the window is being evaluated.
</span><span class="cm">    * @param elements The elements in the window being evaluated.
</span><span class="cm">    * @param out      A collector for emitting elements.
</span><span class="cm">    * @throws Exception The function may throw exceptions to fail the program and trigger recovery.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">process</span><span class="o">(</span>
      <span class="n">key</span><span class="k">:</span> <span class="kt">KEY</span><span class="o">,</span>
      <span class="n">context</span><span class="k">:</span> <span class="kt">Context</span><span class="o">,</span>
      <span class="n">elements</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[</span><span class="kt">IN</span><span class="o">],</span>
      <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">OUT</span><span class="o">])</span>

  <span class="cm">/**
</span><span class="cm">    * The context holding window metadata
</span><span class="cm">    */</span>
  <span class="k">abstract</span> <span class="k">class</span> <span class="nc">Context</span> <span class="o">{</span>
    <span class="cm">/**
</span><span class="cm">      * Returns the window that is being evaluated.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">window</span><span class="k">:</span> <span class="kt">W</span>

    <span class="cm">/**
</span><span class="cm">      * Returns the current processing time.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">currentProcessingTime</span><span class="k">:</span> <span class="kt">Long</span>

    <span class="cm">/**
</span><span class="cm">      * Returns the current event-time watermark.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">currentWatermark</span><span class="k">:</span> <span class="kt">Long</span>

    <span class="cm">/**
</span><span class="cm">      * State accessor for per-key and per-window state.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">windowState</span><span class="k">:</span> <span class="kt">KeyedStateStore</span>

    <span class="cm">/**
</span><span class="cm">      * State accessor for per-key global state.
</span><span class="cm">      */</span>
    <span class="k">def</span> <span class="n">globalState</span><span class="k">:</span> <span class="kt">KeyedStateStore</span>
  <span class="o">}</span>

<span class="o">}</span>
</code></pre></div><p>注意 <code>key</code> 参数是通过为 <code>keyBy()</code> 调用指定的 <code>KeySelector</code> 提取的键。如果是元组索引键或字符串字段引用，这个键的类型总是 Tuple，你必须手动将其转换为一个正确大小的元组来提取键字段。</p>
<p><code>ProcessWindowFunction</code> 可以这样定义和使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
  <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
  <span class="o">.</span><span class="n">timeWindow</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">minutes</span><span class="o">(</span><span class="mi">5</span><span class="o">))</span>
  <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyProcessWindowFunction</span><span class="o">())</span>

<span class="cm">/* ... */</span>

<span class="k">class</span> <span class="nc">MyProcessWindowFunction</span> <span class="k">extends</span> <span class="nc">ProcessWindowFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="kt">String</span>, <span class="kt">String</span>, <span class="kt">TimeWindow</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">def</span> <span class="n">process</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">context</span><span class="k">:</span> <span class="kt">Context</span><span class="o">,</span> <span class="n">input</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)],</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">var</span> <span class="n">count</span> <span class="k">=</span> <span class="mi">0L</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">in</span> <span class="k">&lt;-</span> <span class="n">input</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">count</span> <span class="k">=</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="o">}</span>
    <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="s">s&#34;Window </span><span class="si">${</span><span class="n">context</span><span class="o">.</span><span class="n">window</span><span class="si">}</span><span class="s"> count: </span><span class="si">$count</span><span class="s">&#34;</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>这个例子显示了一个 <code>ProcessWindowFunction</code>，它可以计算一个窗口中的元素。此外，窗口函数还将窗口的信息添加到输出中。</p>
<p>注意，使用 ProcessWindowFunction 进行简单的聚合，如 <code>count</code>，效率相当低。下一节将展示如何将 <code>ReduceFunction</code> 或 <code>AggregateFunction</code> 与 <code>ProcessWindowFunction</code> 结合起来，以获得增量聚合和 <code>ProcessWindowFunction</code> 的附加信息。</p>
<h3 id="具有增量聚合功能的-processwindowfunction">具有增量聚合功能的 ProcessWindowFunction</h3>
<p><code>ProcessWindowFunction</code> 可以与 <code>ReduceFunction</code>、<code>AggregateFunction</code> 或 <code>FoldFunction</code> 相结合，以在元素到达窗口时进行增量聚合。当窗口关闭时，<code>ProcessWindowFunction</code> 将被提供聚合的结果。这使得它可以增量计算窗口，同时可以访问 <code>ProcessWindowFunction</code> 的附加窗口元信息。</p>
<p>注意 您也可以使用 legacy WindowFunction 代替 ProcessWindowFunction 进行增量窗口聚合。</p>
<h4 id="使用-reducefunction-进行增量窗口聚合">使用 ReduceFunction 进行增量窗口聚合</h4>
<p>下面的例子展示了如何将增量 ReduceFunction 与 ProcessWindowFunction 相结合，以返回窗口中最小的事件以及窗口的开始时间。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">SensorReading</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
  <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
  <span class="o">.</span><span class="n">timeWindow</span><span class="o">(&lt;</span><span class="n">duration</span><span class="o">&gt;)</span>
  <span class="o">.</span><span class="n">reduce</span><span class="o">(</span>
    <span class="o">(</span><span class="n">r1</span><span class="k">:</span> <span class="kt">SensorReading</span><span class="o">,</span> <span class="n">r2</span><span class="k">:</span> <span class="kt">SensorReading</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span> <span class="k">if</span> <span class="o">(</span><span class="n">r1</span><span class="o">.</span><span class="n">value</span> <span class="o">&gt;</span> <span class="n">r2</span><span class="o">.</span><span class="n">value</span><span class="o">)</span> <span class="n">r2</span> <span class="k">else</span> <span class="n">r1</span> <span class="o">},</span>
    <span class="o">(</span> <span class="n">key</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
      <span class="n">context</span><span class="k">:</span> <span class="kt">ProcessWindowFunction</span><span class="o">[</span><span class="k">_</span>, <span class="k">_</span>, <span class="k">_</span>, <span class="kt">TimeWindow</span><span class="o">]</span><span class="k">#</span><span class="nc">Context</span><span class="o">,</span>
      <span class="n">minReadings</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[</span><span class="kt">SensorReading</span><span class="o">],</span>
      <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">SensorReading</span><span class="o">)]</span> <span class="o">)</span> <span class="k">=&gt;</span>
      <span class="o">{</span>
        <span class="k">val</span> <span class="n">min</span> <span class="k">=</span> <span class="n">minReadings</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">context</span><span class="o">.</span><span class="n">window</span><span class="o">.</span><span class="n">getStart</span><span class="o">,</span> <span class="n">min</span><span class="o">))</span>
      <span class="o">}</span>
  <span class="o">)</span>
</code></pre></div><h4 id="用-aggregatefunction-进行增量窗口聚合">用 AggregateFunction 进行增量窗口聚合</h4>
<p>下面的例子展示了如何将增量的 AggregateFunction 与 ProcessWindowFunction 结合起来，计算平均值，同时将键和窗口与平均值一起发出。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
  <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
  <span class="o">.</span><span class="n">timeWindow</span><span class="o">(&lt;</span><span class="n">duration</span><span class="o">&gt;)</span>
  <span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="k">new</span> <span class="nc">AverageAggregate</span><span class="o">(),</span> <span class="k">new</span> <span class="nc">MyProcessWindowFunction</span><span class="o">())</span>

<span class="c1">// Function definitions
</span><span class="c1"></span>
<span class="cm">/**
</span><span class="cm"> * The accumulator is used to keep a running sum and a count. The [getResult] method
</span><span class="cm"> * computes the average.
</span><span class="cm"> */</span>
<span class="k">class</span> <span class="nc">AverageAggregate</span> <span class="k">extends</span> <span class="nc">AggregateFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="kt">Double</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">createAccumulator</span><span class="o">()</span> <span class="k">=</span> <span class="o">(</span><span class="mi">0L</span><span class="o">,</span> <span class="mi">0L</span><span class="o">)</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">add</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Long</span><span class="o">),</span> <span class="n">accumulator</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span>
    <span class="o">(</span><span class="n">accumulator</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="n">value</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="mi">1L</span><span class="o">)</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getResult</span><span class="o">(</span><span class="n">accumulator</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_1</span> <span class="o">/</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">_2</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">),</span> <span class="n">b</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">))</span> <span class="k">=</span>
    <span class="o">(</span><span class="n">a</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">a</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">class</span> <span class="nc">MyProcessWindowFunction</span> <span class="k">extends</span> <span class="nc">ProcessWindowFunction</span><span class="o">[</span><span class="kt">Double</span>, <span class="o">(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)</span>, <span class="kt">String</span>, <span class="kt">TimeWindow</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">def</span> <span class="n">process</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">context</span><span class="k">:</span> <span class="kt">Context</span><span class="o">,</span> <span class="n">averages</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)])</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">average</span> <span class="k">=</span> <span class="n">averages</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
    <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">key</span><span class="o">,</span> <span class="n">average</span><span class="o">))</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h4 id="用-foldfunction-进行增量窗口聚合">用 FoldFunction 进行增量窗口聚合</h4>
<p>下面的例子展示了如何将增量式 FoldFunction 与 ProcessWindowFunction 相结合，以提取窗口中的事件数量，并返回窗口的键和结束时间。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">SensorReading</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
 <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
 <span class="o">.</span><span class="n">timeWindow</span><span class="o">(&lt;</span><span class="n">duration</span><span class="o">&gt;)</span>
 <span class="o">.</span><span class="n">fold</span> <span class="o">(</span>
    <span class="o">(</span><span class="s">&#34;&#34;</span><span class="o">,</span> <span class="mi">0L</span><span class="o">,</span> <span class="mi">0</span><span class="o">),</span>
    <span class="o">(</span><span class="n">acc</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Long</span><span class="o">,</span> <span class="nc">Int</span><span class="o">),</span> <span class="n">r</span><span class="k">:</span> <span class="kt">SensorReading</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span> <span class="o">(</span><span class="s">&#34;&#34;</span><span class="o">,</span> <span class="mi">0L</span><span class="o">,</span> <span class="n">acc</span><span class="o">.</span><span class="n">_3</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="o">},</span>
    <span class="o">(</span> <span class="n">key</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
      <span class="n">window</span><span class="k">:</span> <span class="kt">TimeWindow</span><span class="o">,</span>
      <span class="n">counts</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span>, <span class="kt">Int</span><span class="o">)],</span>
      <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="o">)</span> <span class="k">=&gt;</span>
      <span class="o">{</span>
        <span class="k">val</span> <span class="n">count</span> <span class="k">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
        <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">key</span><span class="o">,</span> <span class="n">window</span><span class="o">.</span><span class="n">getEnd</span><span class="o">,</span> <span class="n">count</span><span class="o">.</span><span class="n">_3</span><span class="o">))</span>
      <span class="o">}</span>
  <span class="o">)</span>
</code></pre></div><h4 id="在-processwindowfunction-中使用-per-窗口状态">在 ProcessWindowFunction 中使用 per-窗口状态</h4>
<p>除了访问 keyed 状态（任何富函数都可以），ProcessWindowFunction 还可以使用 keyed 状态，该状态的作用域是函数当前正在处理的窗口。在这种情况下，理解每个窗口状态所指的窗口是什么很重要。这里涉及到不同的&quot;窗口&quot;。</p>
<ul>
<li>窗口是在指定窗口操作时定义的。这可能是1小时的滚动窗口或者2小时的滑动窗口，滑动1小时。</li>
<li>一个给定的键的定义窗口的实际实例。这可能是 12: 00 到 13: 00 的时间窗口，用户 ID xyz. 这是基于窗口定义的，会有很多窗口，基于作业当前正在处理的键的数量，基于事件属于什么时间段。</li>
</ul>
<p>每个窗口的状态与这两者中的后一种挂钩。意思是说，如果我们处理了1000个不同键的事件，并且所有键的事件当前都属于 <code>[12:00，13:00)</code> 时间窗口，那么将有1000个窗口实例，每个窗口都有自己的键的per-窗口状态。</p>
<p><code>process()</code> 调用接收到的 Context 对象上有两个方法允许访问这两种类型的状态。</p>
<ul>
<li><code>globalState()</code>，允许访问不在窗口范围内的 keyed 状态。</li>
<li><code>windowState()</code>，它允许访问同样作用于窗口的 keyed 状态。</li>
</ul>
<p>如果你预计同一窗口会有多次发射，那么这个功能是很有帮助的，因为当你对晚到的数据有晚发射的情况，或者当你有一个自定义的触发器，做投机性的早期发射时，可能会发生这种情况。在这种情况下，你会在每个窗口状态下存储之前的发射信息或发射次数。</p>
<p>当使用窗口状态时，重要的是当窗口被清除时也要清理该状态。这应该发生在 <code>clear()</code> 方法中。</p>
<h3 id="windowfunctionlegacy">WindowFunction(Legacy)</h3>
<p>在一些可以使用 <code>ProcessWindowFunction</code> 的地方，你也可以使用 <code>WindowFunction</code>。这是 <code>ProcessWindowFunction</code> 的旧版本，它提供的上下文信息较少，而且没有一些先进的功能，比如每个窗口的 keyed 状态。这个接口在某些时候会被废弃。</p>
<p>WindowFunction 的签名如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">trait</span> <span class="nc">WindowFunction</span><span class="o">[</span><span class="kt">IN</span>, <span class="kt">OUT</span>, <span class="kt">KEY</span>, <span class="kt">W</span> <span class="k">&lt;:</span> <span class="kt">Window</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">Function</span> <span class="k">with</span> <span class="nc">Serializable</span> <span class="o">{</span>

  <span class="cm">/**
</span><span class="cm">    * Evaluates the window and outputs none or several elements.
</span><span class="cm">    *
</span><span class="cm">    * @param key    The key for which this window is evaluated.
</span><span class="cm">    * @param window The window that is being evaluated.
</span><span class="cm">    * @param input  The elements in the window being evaluated.
</span><span class="cm">    * @param out    A collector for emitting elements.
</span><span class="cm">    * @throws Exception The function may throw exceptions to fail the program and trigger recovery.
</span><span class="cm">    */</span>
  <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">KEY</span><span class="o">,</span> <span class="n">window</span><span class="k">:</span> <span class="kt">W</span><span class="o">,</span> <span class="n">input</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[</span><span class="kt">IN</span><span class="o">],</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[</span><span class="kt">OUT</span><span class="o">])</span>
<span class="o">}</span>
</code></pre></div><p>可以这样使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="k">new</span> <span class="nc">MyWindowFunction</span><span class="o">())</span>
</code></pre></div><h3 id="触发器">触发器</h3>
<p>触发器决定一个窗口（由窗口分配器形成）何时可以被窗口函数处理。每个 <code>WindowAssigner</code> 都有一个默认的触发器。如果默认的触发器不符合你的需求，你可以使用 <code>trigger(...)</code> 指定一个自定义的触发器。</p>
<p>触发器接口有五个方法，允许 Trigger 对不同的事件做出反应。</p>
<ul>
<li><code>onElement()</code> 方法对每个添加到窗口的元素都会被调用。</li>
<li><code>onEventTime()</code> 方法在注册的事件时间定时器启动时被调用。</li>
<li><code>onProcessingTime()</code> 方法在注册的处理时间计时器启动时被调用。</li>
<li><code>onMerge()</code> 方法与有状态的触发器相关，当两个触发器的对应窗口合并时，例如使用会话窗口时，就会合并两个触发器的状态。</li>
<li>最后 <code>clear()</code> 方法在删除相应窗口时执行任何需要的操作。</li>
</ul>
<p>关于以上方法有两点需要注意。</p>
<p>1）前三个方法通过返回一个 <code>TriggerResult</code> 来决定如何对其调用事件采取行动。动作可以是以下之一。</p>
<ul>
<li>CONTINUE：什么也不做。</li>
<li>FIRE：触发计算。</li>
<li>PURGE：清除窗口中的元素，以及</li>
<li>FIRE_AND_PURGE：触发计算，之后清除窗口中的元素。</li>
</ul>
<ol start="2">
<li>这些方法中的任何一种都可以用来注册处理时间或事件时间的定时器，以备将来的操作。</li>
</ol>
<h3 id="fire-和-purge">Fire 和 Purge</h3>
<p>一旦触发器确定一个窗口可以处理，它就会发射，即返回 FIRE 或 FIRE_AND_PURGE。这是窗口操作者发出当前窗口结果的信号。给定一个带有 ProcessWindowFunction 的窗口，所有的元素都会被传递给 ProcessWindowFunction（可能是在将它们传递给 evictor 之后）。带有 ReduceFunction、AggregateFunction 或 FoldFunction 的窗口只是简单地发出它们急切的聚合结果。</p>
<p>当一个触发器发射时，它可以是 FIRE 或 FIRE_AND_PURGE。FIRE 保留窗口的内容，而 FIRE_AND_PURGE 则删除其内容。默认情况下，预先实现的触发器只是 FIRE 而不清除窗口状态。</p>
<p>注意 Purging 将简单地删除窗口的内容，并将完整地保留任何关于窗口和任何触发状态的潜在元信息。</p>
<h3 id="窗口分配器的默认触发器">窗口分配器的默认触发器</h3>
<p>WindowAssigner 的默认触发器适合于许多用例。例如，所有的事件时间窗口分配器都有一个 EventTimeTrigger 作为默认触发器。这个触发器仅仅是在水印通过窗口结束后就会触发。</p>
<p>注意：GlobalWindow 的默认触发器是 NeverTrigger，它永远不会触发。因此，在使用 GlobalWindow 时，您必须定义一个自定义的触发器。</p>
<p>注意：通过使用 trigger() 指定一个触发器，您将覆盖一个 WindowAssigner 的默认触发器。例如，如果你为 TumblingEventTimeWindows 指定了一个 CountTrigger，你将不再获得基于时间进度的窗口启动，而只能通过计数来获得。现在，如果你想同时基于时间和计数做出反应，你必须编写自己的自定义触发器。</p>
<h3 id="内置和自定义触发器">内置和自定义触发器</h3>
<p>Flink 内置了一些触发器。</p>
<ul>
<li>前面已经提到过的, EventTimeTrigger 会根据水印测量的事件时间的进展而触发。</li>
<li>处理时间触发器（ProcessingTimeTrigger）基于处理时间而触发。</li>
<li>CountTrigger 在一个窗口中的元素数量超过给定的限制时触发。</li>
<li>PurgingTrigger 将另一个触发器作为参数，并将其转换为一个清洗触发器。</li>
</ul>
<p>如果你需要实现一个自定义的触发器，你应该查看抽象的 <a href="https://github.com/apache/flink/blob/master//flink-streaming-java/src/main/java/org/apache/flink/streaming/api/windowing/triggers/Trigger.java">Trigger</a> 类。请注意，API 仍在不断发展，可能会在 Flink 的未来版本中改变。</p>
<h3 id="evictors">Evictors</h3>
<p>Flink 的窗口模型允许在 WindowAssigner 和 Trigger 之外指定一个可选的 Evictor。这可以通过 <code>evictor(...)</code> 方法来完成（如本文开头所示）。Evictor 能够在触发器触发后和应用窗口函数之前和/或之后从窗口中移除元素。要做到这一点，Evictor 接口有两个方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="cm">/**
</span><span class="cm"> * Optionally evicts elements. Called before windowing function.
</span><span class="cm"> *
</span><span class="cm"> * @param elements The elements currently in the pane.
</span><span class="cm"> * @param size The current number of elements in the pane.
</span><span class="cm"> * @param window The {@link Window}
</span><span class="cm"> * @param evictorContext The context for the Evictor
</span><span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">evictBefore</span><span class="o">(</span><span class="n">Iterable</span><span class="o">&lt;</span><span class="n">TimestampedValue</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;&gt;</span> <span class="n">elements</span><span class="o">,</span> <span class="kt">int</span> <span class="n">size</span><span class="o">,</span> <span class="n">W</span> <span class="n">window</span><span class="o">,</span> <span class="n">EvictorContext</span> <span class="n">evictorContext</span><span class="o">);</span>

<span class="cm">/**
</span><span class="cm"> * Optionally evicts elements. Called after windowing function.
</span><span class="cm"> *
</span><span class="cm"> * @param elements The elements currently in the pane.
</span><span class="cm"> * @param size The current number of elements in the pane.
</span><span class="cm"> * @param window The {@link Window}
</span><span class="cm"> * @param evictorContext The context for the Evictor
</span><span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">evictAfter</span><span class="o">(</span><span class="n">Iterable</span><span class="o">&lt;</span><span class="n">TimestampedValue</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;&gt;</span> <span class="n">elements</span><span class="o">,</span> <span class="kt">int</span> <span class="n">size</span><span class="o">,</span> <span class="n">W</span> <span class="n">window</span><span class="o">,</span> <span class="n">EvictorContext</span> <span class="n">evictorContext</span><span class="o">);</span>
</code></pre></div><p><code>evictBefore()</code> 包含在窗口函数之前应用的驱逐逻辑，而 <code>evictAfter()</code> 包含在窗口函数之后应用的逻辑。在应用窗口函数之前被驱逐的元素将不会被它处理。</p>
<p>Flink 自带了三个预先实现的驱逐器。这三个是:</p>
<ul>
<li>CountEvictor：从窗口中保留最多用户指定数量的元素，并从窗口缓冲区开始丢弃剩余的元素。</li>
<li>DeltaEvictor：取 DeltaFunction 和阈值，计算窗口缓冲区中最后一个元素和剩余元素之间的 delta，并删除 delta 大于或等于阈值的元素。</li>
<li>TimeEvictor：以毫秒为单位的时间间隔作为参数，对于一个给定的窗口，它在其元素中找到最大的时间戳 max_ts，并删除所有时间戳小于 max_ts - interval 的元素。</li>
</ul>
<p>默认情况下，所有预先实现的 evictor 都会在 window 函数之前应用其逻辑。</p>
<p>注意: 指定一个 evictor 可以防止任何预聚集，因为一个窗口的所有元素都必须在应用计算之前传递给 evictor。</p>
<p>注意 Flink 不保证窗口内元素的顺序。这意味着，虽然 evictor 可以从窗口的开头移除元素，但这些元素不一定是最先或最后到达的。</p>
<h2 id="允许的延迟">允许的延迟</h2>
<p>当使用事件时间窗口时，可能会发生元素迟到的情况，也就是说，Flink 用来跟踪事件时间进度的水印已经超过了元素所属窗口的结束时间戳。关于 Flink 如何处理事件时间，请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间</a>，尤其是<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html#late-elements">迟到元素</a>。</p>
<p>默认情况下，当水印超过窗口的结束时间时，晚期元素就会被删除。然而，Flink 允许为窗口操作者指定一个最大允许延迟。允许延迟指定了元素在被丢弃之前可以迟到多少时间，其默认值为0。 在水印通过窗口结束后但在其通过窗口结束前加上允许延迟之前到达的元素，仍然会被添加到窗口中。根据所使用的触发器，一个迟到但未被丢弃的元素可能会导致窗口再次启动。EventTimeTrigger 就属于这种情况。</p>
<p>为了使这个工作，Flink 会保持窗口的状态，直到它们的允许延迟过期。一旦发生这种情况，Flink 就会删除窗口并删除其状态，这一点在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-lifecycle">窗口生命周期</a>部分也有描述。</p>
<p>默认情况下，允许的延迟被设置为0，也就是说，到达水印后面的元素将被丢弃。</p>
<p>您可以像这样指定允许的延迟。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">allowedLateness</span><span class="o">(&lt;</span><span class="n">time</span><span class="o">&gt;)</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>
</code></pre></div><p>注意 当使用 GlobalWindows 窗口分配器时，由于全局窗口的结束时间戳是 Long.MAX_VALUE，因此没有数据被认为是迟到数据。</p>
<h3 id="作为侧输出获取迟到数据">作为侧输出获取迟到数据</h3>
<p>使用 Flink 的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/side_output.html">侧输出</a>功能，你可以得到一个被丢弃的迟到数据流。</p>
<p>首先，你需要在窗口化的数据流上使用 <code>sideOutputLateData(OutputTag)</code> 来指定你要获取迟到的数据。然后，你就可以在窗口化操作的结果上得到侧输出流。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">lateOutputTag</span> <span class="k">=</span> <span class="nc">OutputTag</span><span class="o">[</span><span class="kt">T</span><span class="o">](</span><span class="s">&#34;late-data&#34;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(&lt;</span><span class="n">window</span> <span class="n">assigner</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">allowedLateness</span><span class="o">(&lt;</span><span class="n">time</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">sideOutputLateData</span><span class="o">(</span><span class="n">lateOutputTag</span><span class="o">)</span>
    <span class="o">.&lt;</span><span class="n">windowed</span> <span class="n">transformation</span><span class="o">&gt;(&lt;</span><span class="n">window</span> <span class="n">function</span><span class="o">&gt;)</span>

<span class="k">val</span> <span class="n">lateStream</span> <span class="k">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getSideOutput</span><span class="o">(</span><span class="n">lateOutputTag</span><span class="o">)</span>
</code></pre></div><h4 id="迟到元素的考虑">迟到元素的考虑</h4>
<p>当指定允许的延迟大于0时，在水印通过窗口结束后，窗口及其内容将被保留。在这些情况下，当一个迟到但未被丢弃的元素到达时，它可能会触发窗口的另一次发射。这些发射被称为晚期发射，因为它们是由晚期事件触发的，与主发射相反，主发射是窗口的第一次发射。在会话窗口的情况下，迟发可能会进一步导致窗口的合并，因为它们可能会&quot;弥合&quot;两个已经存在的、未合并的窗口之间的差距。</p>
<p>注意：你应该意识到，晚点发射的元素应该被视为之前计算的更新结果，也就是说，你的数据流将包含同一计算的多个结果。根据你的应用，你需要考虑到这些重复的结果，或者对它们进行重复复制。</p>
<h3 id="处理窗口结果">处理窗口结果</h3>
<p>窗口化操作的结果又是一个 DataStream，在结果元素中没有保留任何关于窗口化操作的信息，所以如果你想保留窗口的元信息，你必须在你的 <code>ProcessWindowFunction</code> 的结果元素中手动编码这些信息。在结果元素上设置的唯一相关信息是元素的时间戳。这被设置为处理过的窗口的最大允许时间戳，也就是结束时间戳-1，因为窗口结束时间戳是独占的。注意，这对事件时间窗口和处理时间窗口都是如此，即在窗口化操作后元素总是有一个时间戳，但这个时间戳可以是事件时间时间戳，也可以是处理时间时间戳。对于处理时间窗口来说，这没有特别的影响，但是对于事件时间窗口来说，加上水印与窗口的交互方式，使得<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#consecutive-windowed-operations">连续的窗口化操作</a>具有相同的窗口大小。我们将在看完水印如何与窗口交互后再谈这个问题。</p>
<h4 id="水印和窗口的交互">水印和窗口的交互</h4>
<p>在继续本节之前，你可能想看看我们关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">事件时间和水印</a>的章节。</p>
<p>当水印到达窗口操作符时，会触发两件事。</p>
<ul>
<li>水印会触发计算所有窗口的最大时间戳（就是结束时间戳-1）小于新水印的窗口。</li>
<li>水印被转发到下游的操作中</li>
</ul>
<p>直观地说，水印会&quot;冲掉&quot;任何在下游操作中被认为是晚期的窗口，一旦它们收到该水印。</p>
<h4 id="连续的窗口操作">连续的窗口操作</h4>
<p>如前所述，计算窗口化结果的时间戳的方式以及水印与窗口的交互方式允许将连续的窗口化操作串在一起。当你想进行两个连续的窗口化操作时，如果你想使用不同的键，但仍然希望来自同一个上游窗口的元素最终出现在同一个下游窗口中，这就很有用。考虑这个例子。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">resultsPerKey</span> <span class="k">=</span> <span class="n">input</span>
    <span class="o">.</span><span class="n">keyBy</span><span class="o">(&lt;</span><span class="n">key</span> <span class="n">selector</span><span class="o">&gt;)</span>
    <span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">reduce</span><span class="o">(</span><span class="k">new</span> <span class="nc">Summer</span><span class="o">())</span>

<span class="k">val</span> <span class="n">globalResults</span> <span class="k">=</span> <span class="n">resultsPerKey</span>
    <span class="o">.</span><span class="n">windowAll</span><span class="o">(</span><span class="nc">TumblingEventTimeWindows</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">5</span><span class="o">)))</span>
    <span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="k">new</span> <span class="nc">TopKWindowFunction</span><span class="o">())</span>
</code></pre></div><p>在这个例子中，第一次操作的时间窗口 <code>[0，5)</code> 的结果也会在随后的窗口操作中最终出现在时间窗口 <code>[0，5)</code>。这样就可以计算每个键的和，然后在第二个操作中计算同一窗口内的 top-k 元素。</p>
<h3 id="有用的状态大小考虑">有用的状态大小考虑</h3>
<p>窗口可以在很长一段时间内（如几天、几周或几个月）被定义，因此会积累非常大的状态。在估算窗口计算的存储需求时，有几个规则需要牢记。</p>
<ol>
<li>
<p>Flink 为每个元素所属的窗口创建一个副本。鉴于此，翻滚窗口为每个元素保留一个副本（一个元素正好属于一个窗口，除非它被后期丢弃）。相比之下，滑动窗口会给每个元素创建若干个，这一点在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#window-assigners">窗口分配器</a>部分有解释。因此，大小为1天，滑动1秒的滑动窗口可能不是一个好主意。</p>
</li>
<li>
<p>ReduceFunction、AggregateFunction 和 FoldFunction 可以显著降低存储要求，因为它们热衷于聚合元素，每个窗口只存储一个值。相比之下，仅仅使用 ProcessWindowFunction 就需要累积所有元素。</p>
</li>
<li>
<p>使用 Evictor 可以防止任何预聚集，因为一个窗口的所有元素都必须在应用计算之前通过 evictor（见 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html#evictors">Evictor</a>）。</p>
</li>
</ol>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/operators" term="operators" label="Operators" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/windows" term="windows" label="Windows" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[自定义序列化管理状态]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-custom-serialization-for-managed-state/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-custom-serialization-for-managed-state/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Custom Serialization for Managed State</blockquote><p>本页面的目标是为需要使用自定义状态序列化的用户提供指导，涵盖了如何提供自定义状态序列化器，以及实现允许状态模式演化的序列化器的指南和最佳实践。</p>
<p>如果你只是简单地使用 Flink 自带的序列化器，这个页面是不相关的，可以忽略。</p>
<h2 id="使用自定义状态序列化器">使用自定义状态序列化器</h2>
<p>当注册一个 managed operator 或 keyed state时，需要一个 <code>StateDescriptor</code> 来指定状态的名称，以及状态的类型信息。类型信息被 Flink 的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/types_serialization.html">类型序列化框架</a>用来为状态创建合适的序列化器。</p>
<p>也可以完全绕过这一点，让 Flink 使用自己的自定义序列化器来序列化被管理的状态，只需用自己的 <code>TypeSerializer</code> 实现直接实例化 <code>StateDescriptor</code> 即可。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">CustomTypeSerializer</span> <span class="k">extends</span> <span class="nc">TypeSerializer</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Integer</span><span class="o">)]</span> <span class="o">{...}</span>

<span class="k">val</span> <span class="n">descriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ListStateDescriptor</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Integer</span><span class="o">)](</span>
    <span class="s">&#34;state-name&#34;</span><span class="o">,</span>
    <span class="k">new</span> <span class="nc">CustomTypeSerializer</span><span class="o">)</span>
<span class="o">)</span>

<span class="n">checkpointedState</span> <span class="k">=</span> <span class="n">getRuntimeContext</span><span class="o">.</span><span class="n">getListState</span><span class="o">(</span><span class="n">descriptor</span><span class="o">)</span>
</code></pre></div><h3 id="状态序列化器和模式演进">状态序列化器和模式演进</h3>
<p>本节解释了与状态序列化和模式演进相关的面向用户的抽象，以及关于 Flink 如何与这些抽象交互的必要内部细节。</p>
<p>当从保存点恢复时，Flink 允许改变用于读取和写入先前注册状态的序列化器，因此用户不会被锁定在任何特定的序列化模式上。当状态被还原时，将为该状态注册一个新的序列化器（即在还原作业中用于访问状态的 <code>StateDescriptor</code> 所附带的序列化器）。这个新的序列化器可能与之前的序列化器的模式不同。因此，在实现状态序列化器时，除了读取/写入数据的基本逻辑外，另一个需要注意的重要问题是未来如何改变序列化模式。</p>
<p>说到 schema，在这里，这个术语可以互换，指的是状态类型的数据模型和状态类型的序列化二进制格式。一般来说，模式，可以为少数情况而改变。</p>
<ol>
<li>状态类型的数据模式发生了变化，即从 POJO 中增加或删除一个作为状态的字段。</li>
<li>一般来说，数据模式发生变化后，需要升级序列器的序列化格式。</li>
<li>序列器的配置发生了变化。</li>
</ol>
<p>为了让新的执行有状态的写入模式的信息，并检测模式是否发生了变化，在对操作符的状态进行保存点时，需要将状态序列器的快照和状态字节一起写入。这就是抽象出来的一个 <code>TypeSerializerSnapshot</code>，在下一小节解释。</p>
<h3 id="typeserializersnapshot-抽象"><code>TypeSerializerSnapshot</code> 抽象</h3>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">TypeSerializerSnapshot</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kt">int</span> <span class="nf">getCurrentVersion</span><span class="o">();</span>
    <span class="kt">void</span> <span class="nf">writeSnapshot</span><span class="o">(</span><span class="n">DataOuputView</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="o">;</span>
    <span class="kt">void</span> <span class="nf">readSnapshot</span><span class="o">(</span><span class="kt">int</span> <span class="n">readVersion</span><span class="o">,</span> <span class="n">DataInputView</span> <span class="n">in</span><span class="o">,</span> <span class="n">ClassLoader</span> <span class="n">userCodeClassLoader</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="o">;</span>
    <span class="n">TypeSerializerSchemaCompatibility</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="nf">resolveSchemaCompatibility</span><span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="n">newSerializer</span><span class="o">);</span>
    <span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="nf">restoreSerializer</span><span class="o">();</span>
<span class="o">}</span>
<span class="kd">public</span> <span class="kd">abstract</span> <span class="kd">class</span> <span class="nc">TypeSerializer</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="o">{</span>    
    
    <span class="c1">// ...
</span><span class="c1"></span>    
    <span class="kd">public</span> <span class="kd">abstract</span> <span class="n">TypeSerializerSnapshot</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="nf">snapshotConfiguration</span><span class="o">();</span>
<span class="o">}</span>
</code></pre></div><p>序列器的 TypeSerializerSnapshot 是一个时间点信息，它作为状态序列器的写模式的唯一真理来源，以及还原一个序列器所必须的任何额外信息，这些信息将与给定的时间点相同。关于在还原时应该写入和读取什么作为序列器快照的逻辑是在 <code>writeSnapshot和readSnapshot</code> 方法中定义的。</p>
<p>请注意，快照本身的写模式也可能需要随着时间的推移而改变（例如，当你希望在快照中添加更多关于序列器的信息时）。为了方便，快照是有版本的，在 <code>getCurrentVersion</code> 方法中定义了当前的版本号。在还原时，当从保存点读取序列器快照时，将向 <code>readSnapshot</code> 方法提供写入快照的模式的版本，以便读取实现可以处理不同的版本。</p>
<p>在还原时，检测新的序列器的模式是否改变的逻辑应该在 <code>resolveSchemaCompatibility</code> 方法中实现。当之前的注册状态在还原执行的操作符中再次注册新的序列化器时，新的序列化器会通过这个方法提供给之前序列化器的快照。该方法返回一个代表兼容性解决结果的 <code>TypeSerializerSchemaCompatibility</code>，它可以是以下之一。</p>
<ol>
<li><code>TypeSerializerSchemaCompatibility.compatibleAsIs()</code>：这个结果标志着新的序列化器是兼容的，这意味着新的序列化器与之前的序列化器具有相同的模式。有可能在resolveSchemaCompatibility方法中重新配置了新的序列化器，使其兼容。</li>
<li><code>TypeSerializerSchemaCompatibility.compatibleAfterMigration()</code>：这个结果标志着新的序列化器具有不同的序列化模式，可以从旧的模式迁移，使用之前的序列化器（识别旧的模式）将字节读入状态对象，然后用新的序列化器（识别新的模式）将对象重新写回字节。</li>
<li><code>TypeSerializerSchemaCompatibility.incompatible()</code>：这个结果标志着新的序列化器有不同的序列化模式，但不可能从旧模式迁移。</li>
</ol>
<p>最后一点细节是在需要迁移的情况下，如何获得之前的序列化器。序列化器的 <code>TypeSerializerSnapshot</code> 的另一个重要作用是，它可以作为一个工厂来恢复以前的序列化器。更具体地说，<code>TypeSerializerSnapshot</code> 应该实现 <code>restoreSerializer</code> 方法来实例化一个序列化器实例，该实例能够识别之前序列化器的模式和配置，因此可以安全地读取之前序列化器写入的数据。</p>
<h4 id="flink-如何与-typeserializer-和-typeserializersnapshot-抽象进行交互">Flink 如何与 TypeSerializer 和 TypeSerializerSnapshot 抽象进行交互</h4>
<p>总结一下，本节总结了 Flink，或者更具体地说，状态后端如何与抽象进行交互。根据状态后端的不同，交互略有不同，但这与状态序列化器及其序列化器快照的实现是正交的。</p>
<p><strong>离堆状态后端(如 RocksDBStateBackend)</strong></p>
<ol>
<li>用具有模式A的状态序列器注册新的状态。</li>
</ol>
<ul>
<li>注册的 TypeSerializer 用于在每次状态访问时读取/写入状态。</li>
<li>状态被写入模式A中。</li>
</ul>
<ol start="2">
<li>拍摄一个保存点</li>
</ol>
<ul>
<li>序列器快照是通过 <code>TypeSerializer#snapshotConfiguration</code> 方法提取的。</li>
<li>序列器快照被写入保存点，以及已经序列化的状态字节（模式A）。</li>
</ul>
<ol start="3">
<li>恢复的执行用新的状态序列化器重新访问恢复的状态字节，新的状态序列化器具有模式B。</li>
</ol>
<ul>
<li>前一个状态序列器的快照被还原。</li>
<li>状态字节在还原时不被反序列化，只被加载回状态后端（因此，仍在模式A中）。</li>
<li>接收到新的序列化器后，通过 <code>TypeSerializer#resolveSchemaCompatibility</code> 提供给被还原的前一个序列化器的快照，检查模式是否兼容。</li>
</ul>
<ol start="4">
<li>将后端中的状态字节从模式A迁移到模式B。</li>
</ol>
<ul>
<li>如果兼容性决议反映模式已经改变，并且可以进行迁移，则进行模式迁移。通过 <code>TypeSerializerSnapshot#restoreSerializer()</code>，将从序列化器快照中获取之前识别模式A的状态序列化器，并用于反序列化状态字节到对象，进而用新的序列化器再次重写，识别模式B，完成迁移。在继续处理之前，所有访问状态的条目全部迁移完毕。</li>
<li>如果解析信号为不兼容，则状态访问失败，出现异常。</li>
</ul>
<p><strong>堆状态后端（如 MemoryStateBackend、FsStateBackend）</strong>:</p>
<ol>
<li>用具有模式A的状态序列器注册新的状态。</li>
</ol>
<ul>
<li>注册的 TypeSerializer 由状态后端维护。</li>
</ul>
<ol start="2">
<li>拍摄一个保存点，将所有状态用模式A序列化。</li>
</ol>
<ul>
<li>序列器快照是通过 <code>TypeSerializer#snapshotConfiguration</code> 方法提取的。</li>
<li>序列化器快照被写入保存点。</li>
<li>现在状态对象被序列化到保存点，写入模式A中。</li>
</ul>
<ol start="3">
<li>在还原时，将状态反序列化为堆中的对象。</li>
</ol>
<ul>
<li>前一个状态序列器的快照被恢复。</li>
<li>通过 <code>TypeSerializerSnapshot#restoreSerializer()</code> 从序列化器快照中获取之前的序列化器，该序列化器识别模式A，用于将状态字节反序列化为对象。</li>
<li>从现在开始，所有的状态都已经被反序列化了。</li>
</ul>
<ol start="4">
<li>恢复后的执行用新的状态序列化器重新访问以前的状态，新的状态序列化器具有模式B。</li>
</ol>
<ul>
<li>在接收到新的序列化器后，通过 <code>TypeSerializer#resolveSchemaCompatibility</code> 提供给恢复之前序列化器的快照，以检查模式的兼容性。</li>
<li>如果兼容性检查发出需要迁移的信号，在这种情况下什么都不会发生，因为对于堆后端来说，所有的状态已经被反序列化为对象。</li>
<li>如果解析信号为不兼容，则状态访问失败，出现异常。</li>
</ul>
<ol start="5">
<li>再拍摄一个保存点，将所有状态用模式B序列化。</li>
</ol>
<ul>
<li>与步骤2.相同，但现在状态字节都在模式B中。</li>
</ul>
<h3 id="预先定义方便的-typeserializersnapshot-类">预先定义方便的 TypeSerializerSnapshot 类</h3>
<p>Flink 提供了两个抽象的基础 TypeSerializerSnapshot 类，可以用于典型场景。SimpleTypeSerializerSnapshot 和 CompositeTypeSerializerSnapshot。</p>
<p>提供这些预定义快照作为其序列化器快照的序列化器必须始终有自己独立的子类实现。这与不在不同的序列化器之间共享快照类的最佳实践相对应，这将在下一节中得到更详尽的解释。</p>
<h4 id="实现-simpletypeserializersnapshot">实现 SimpleTypeSerializerSnapshot</h4>
<p>SimpleTypeSerializerSnapshot 是为没有任何状态或配置的序列化器准备的，本质上意味着序列化器的序列化模式完全由序列化器的类来定义。</p>
<p>当使用 SimpleTypeSerializerSnapshot 作为你的序列化器的快照类时，兼容性解决只有2种可能的结果。</p>
<ul>
<li>TypeSerializerSchemaCompatibility.compatibleAsIs()，如果新的序列化器类保持相同，或</li>
<li>TypeSerializerSchemaCompatibility.incompatible()，如果新的序列化器类与之前的序列化器类不同。</li>
</ul>
<p>下面以 Flink 的 <code>IntSerializer</code> 为例，介绍 <code>SimpleTypeSerializerSnapshot</code> 的使用方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">IntSerializerSnapshot</span> <span class="kd">extends</span> <span class="n">SimpleTypeSerializerSnapshot</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="nf">IntSerializerSnapshot</span><span class="o">()</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(()</span> <span class="o">-&gt;</span> <span class="n">IntSerializer</span><span class="o">.</span><span class="na">INSTANCE</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>IntSerializer 没有状态或配置。序列化格式完全由序列化器类自己定义，只能由另一个 IntSerializer 读取。因此，它适合 SimpleTypeSerializerSnapshot 的使用情况。</p>
<p>SimpleTypeSerializerSnapshot 的基础超级构造函数期望得到一个相应序列器实例的 Supplier，不管快照当前是在还原还是在快照期间写入。该 Supplier 用于创建还原序列化器，以及类型检查，以验证新序列化器是否属于相同的预期序列化器类。</p>
<h4 id="实现-compositetypeserializersnapshot">实现 CompositeTypeSerializerSnapshot</h4>
<p>CompositeTypeSerializerSnapshot 是为那些依赖于多个嵌套序列化器的序列化器而设计的。</p>
<p>在进一步解释之前，我们将依赖于多个嵌套序列化器的序列化器称为此上下文中的&quot;外部&quot;序列化器。这方面的例子可以是 MapSerializer、ListSerializer、GenericArraySerializer 等。例如，考虑 MapSerializer &ndash;键和值序列化器将是嵌套序列化器，而MapSerializer本身是 &ldquo;外部 &ldquo;序列化器。</p>
<p>在这种情况下，外层序列化器的快照也应该包含嵌套序列化器的快照，这样就可以独立检查嵌套序列化器的兼容性。在解决外层序列化器的兼容性时，需要考虑每个嵌套序列化器的兼容性。</p>
<p>提供 CompositeTypeSerializerSnapshot 是为了协助实现这类复合序列器的快照。它处理嵌套序列化器快照的读写，以及考虑到所有嵌套序列化器的兼容性，解析最终的兼容性结果。</p>
<p>下面以 Flink 的 MapSerializer 为例，介绍如何使用 CompositeTypeSerializerSnapshot。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">MapSerializerSnapshot</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="kd">extends</span> <span class="n">CompositeTypeSerializerSnapshot</span><span class="o">&lt;</span><span class="n">Map</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;,</span> <span class="n">MapSerializer</span><span class="o">&gt;</span> <span class="o">{</span>

    <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">CURRENT_VERSION</span> <span class="o">=</span> <span class="n">1</span><span class="o">;</span>

    <span class="kd">public</span> <span class="nf">MapSerializerSnapshot</span><span class="o">()</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span><span class="n">MapSerializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="nf">MapSerializerSnapshot</span><span class="o">(</span><span class="n">MapSerializer</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="n">mapSerializer</span><span class="o">)</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span><span class="n">mapSerializer</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">int</span> <span class="nf">getCurrentOuterSnapshotVersion</span><span class="o">()</span> <span class="o">{</span>
        <span class="k">return</span> <span class="n">CURRENT_VERSION</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">MapSerializer</span> <span class="nf">createOuterSerializerWithNestedSerializers</span><span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="n">nestedSerializers</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">K</span><span class="o">&gt;</span> <span class="n">keySerializer</span> <span class="o">=</span> <span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">K</span><span class="o">&gt;)</span> <span class="n">nestedSerializers</span><span class="o">[</span><span class="n">0</span><span class="o">];</span>
        <span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">V</span><span class="o">&gt;</span> <span class="n">valueSerializer</span> <span class="o">=</span> <span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">V</span><span class="o">&gt;)</span> <span class="n">nestedSerializers</span><span class="o">[</span><span class="n">1</span><span class="o">];</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">MapSerializer</span><span class="o">&lt;&gt;(</span><span class="n">keySerializer</span><span class="o">,</span> <span class="n">valueSerializer</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="n">getNestedSerializers</span><span class="o">(</span><span class="n">MapSerializer</span> <span class="n">outerSerializer</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="o">{</span> <span class="n">outerSerializer</span><span class="o">.</span><span class="na">getKeySerializer</span><span class="o">(),</span> <span class="n">outerSerializer</span><span class="o">.</span><span class="na">getValueSerializer</span><span class="o">()</span> <span class="o">};</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>当实现一个新的序列器快照作为 CompositeTypeSerializerSnapshot 的子类时，必须实现以下三个方法。</p>
<ul>
<li><code>#getCurrentOuterSnapshotVersion()</code>。该方法定义了当前外部序列化器快照的序列化二进制格式的版本。</li>
<li><code>#getNestedSerializers(TypeSerializer)</code>。给定外部序列化器，返回其嵌套的序列化器。</li>
<li><code>#createOuterSerializerWithNestedSerializers(TypeSerializer[])</code>。给定嵌套的序列化器，创建一个外部序列化器的实例。</li>
</ul>
<p>上面的例子是一个 CompositeTypeSerializerSnapshot，除了嵌套的序列化器的快照外，没有额外的信息需要快照。因此，可以预期其外部快照版本永远不需要上报。然而，其他一些序列化器，包含一些额外的静态配置，需要和嵌套的组件序列化器一起持久化。一个例子是 Flink 的 GenericArraySerializer，除了嵌套的元素序列化器之外，它还包含了数组元素类型的类作为配置。</p>
<p>在这些情况下，需要在 CompositeTypeSerializerSnapshot 上实现另外三个方法。</p>
<ul>
<li><code>#writeOuterSnapshot(DataOutputView)</code>：定义如何写入外部快照信息。</li>
<li><code>#readOuterSnapshot(int, DataInputView, ClassLoader)</code>：定义如何读取外部快照信息。</li>
<li><code>#resolveOuterSchemaCompatibility(TypeSerializer)</code>：根据外部快照信息检查兼容性。</li>
</ul>
<p>默认情况下，CompositeTypeSerializerSnapshot 假设没有任何外部快照信息可读/可写，因此上述方法的默认实现为空。如果子类有外部快照信息，那么这三个方法必须全部实现。</p>
<p>下面以 Flink 的 GenericArraySerializer 为例，说明 CompositeTypeSerializerSnapshot 如何用于确实有外部快照信息的复合序列器快照。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">final</span> <span class="kd">class</span> <span class="nc">GenericArraySerializerSnapshot</span><span class="o">&lt;</span><span class="n">C</span><span class="o">&gt;</span> <span class="kd">extends</span> <span class="n">CompositeTypeSerializerSnapshot</span><span class="o">&lt;</span><span class="n">C</span><span class="o">[],</span> <span class="n">GenericArraySerializer</span><span class="o">&gt;</span> <span class="o">{</span>

    <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">CURRENT_VERSION</span> <span class="o">=</span> <span class="n">1</span><span class="o">;</span>

    <span class="kd">private</span> <span class="n">Class</span><span class="o">&lt;</span><span class="n">C</span><span class="o">&gt;</span> <span class="n">componentClass</span><span class="o">;</span>

    <span class="kd">public</span> <span class="nf">GenericArraySerializerSnapshot</span><span class="o">()</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span><span class="n">GenericArraySerializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="nf">GenericArraySerializerSnapshot</span><span class="o">(</span><span class="n">GenericArraySerializer</span><span class="o">&lt;</span><span class="n">C</span><span class="o">&gt;</span> <span class="n">genericArraySerializer</span><span class="o">)</span> <span class="o">{</span>
        <span class="kd">super</span><span class="o">(</span><span class="n">genericArraySerializer</span><span class="o">);</span>
        <span class="k">this</span><span class="o">.</span><span class="na">componentClass</span> <span class="o">=</span> <span class="n">genericArraySerializer</span><span class="o">.</span><span class="na">getComponentClass</span><span class="o">();</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">int</span> <span class="nf">getCurrentOuterSnapshotVersion</span><span class="o">()</span> <span class="o">{</span>
        <span class="k">return</span> <span class="n">CURRENT_VERSION</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">writeOuterSnapshot</span><span class="o">(</span><span class="n">DataOutputView</span> <span class="n">out</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
        <span class="n">out</span><span class="o">.</span><span class="na">writeUTF</span><span class="o">(</span><span class="n">componentClass</span><span class="o">.</span><span class="na">getName</span><span class="o">());</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">readOuterSnapshot</span><span class="o">(</span><span class="kt">int</span> <span class="n">readOuterSnapshotVersion</span><span class="o">,</span> <span class="n">DataInputView</span> <span class="n">in</span><span class="o">,</span> <span class="n">ClassLoader</span> <span class="n">userCodeClassLoader</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
        <span class="k">this</span><span class="o">.</span><span class="na">componentClass</span> <span class="o">=</span> <span class="n">InstantiationUtil</span><span class="o">.</span><span class="na">resolveClassByName</span><span class="o">(</span><span class="n">in</span><span class="o">,</span> <span class="n">userCodeClassLoader</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">boolean</span> <span class="nf">resolveOuterSchemaCompatibility</span><span class="o">(</span><span class="n">GenericArraySerializer</span> <span class="n">newSerializer</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="na">componentClass</span> <span class="o">==</span> <span class="n">newSerializer</span><span class="o">.</span><span class="na">getComponentClass</span><span class="o">())</span>
            <span class="o">?</span> <span class="n">OuterSchemaCompatibility</span><span class="o">.</span><span class="na">COMPATIBLE_AS_IS</span>
            <span class="o">:</span> <span class="n">OuterSchemaCompatibility</span><span class="o">.</span><span class="na">INCOMPATIBLE</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">GenericArraySerializer</span> <span class="nf">createOuterSerializerWithNestedSerializers</span><span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="n">nestedSerializers</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">C</span><span class="o">&gt;</span> <span class="n">componentSerializer</span> <span class="o">=</span> <span class="o">(</span><span class="n">TypeSerializer</span><span class="o">&lt;</span><span class="n">C</span><span class="o">&gt;)</span> <span class="n">nestedSerializers</span><span class="o">[</span><span class="n">0</span><span class="o">];</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">GenericArraySerializer</span><span class="o">&lt;&gt;(</span><span class="n">componentClass</span><span class="o">,</span> <span class="n">componentSerializer</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="n">getNestedSerializers</span><span class="o">(</span><span class="n">GenericArraySerializer</span> <span class="n">outerSerializer</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="k">new</span> <span class="n">TypeSerializer</span><span class="o">&lt;?&gt;[]</span> <span class="o">{</span> <span class="n">outerSerializer</span><span class="o">.</span><span class="na">getComponentSerializer</span><span class="o">()</span> <span class="o">};</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>在上面的代码片段中，有两个重要的事情需要注意。首先，由于这个 <code>CompositeTypeSerializerSnapshot</code> 实现的外快照信息是作为快照的一部分写入的，所以每当外快照信息的序列化格式发生变化时，由 <code>getCurrentOuterSnapshotVersion()</code> 定义的外快照版本必须被上调。</p>
<p>其次，请注意我们在写组件类时避免使用 Java 序列化，只写类名，在读回快照时动态加载。避免使用 Java 序列化来编写序列化器快照的内容，总的来说是一个很好的做法。关于这方面的更多细节将在下一节介绍。</p>
<h3 id="实施说明和最佳实践">实施说明和最佳实践</h3>
<ol>
<li>Flink 通过将序列器快照实例化，恢复序列器快照，其类名为</li>
</ol>
<p>序列器的快照，是注册状态如何被序列化的唯一真实来源，是读取保存点中状态的入口。为了能够恢复和访问以前的状态，必须能够恢复以前状态序列化器的快照。</p>
<p>Flink 通过首先实例化 TypeSerializerSnapshot 与其类名（与快照字节一起写入）来恢复序列器快照。因此，为了避免受到意外的类名更改或实例化失败， TypeSerializerSnapshot 类应该。</p>
<ul>
<li>避免被实现为匿名类或嵌套类。</li>
<li>有一个公共的空值构造函数用于实例化。</li>
</ul>
<ol start="2">
<li>避免在不同的序列化器之间共享同一个 TypeSerializerSnapshot 类。</li>
</ol>
<p>由于模式兼容性检查要通过序列化器快照，让多个序列化器返回同一个 TypeSerializerSnapshot 类作为它们的快照，会使 <code>TypeSerializerSnapshot#resolveSchemaCompatibility</code> 和 <code>TypeSerializerSnapshot#restoreSerializer()</code> 方法的实现变得复杂。</p>
<p>这也将是一个不好的分离关注点，一个单一序列化器的序列化模式、配置以及如何恢复它，应该合并在自己专门的TypeSerializerSnapshot类中。</p>
<ol start="3">
<li>避免使用 Java 序列化来制作序列化器快照内容</li>
</ol>
<p>在编写持久化的序列化器快照的内容时，完全不应该使用 Java 序列化。例如，一个序列化器需要持久化一个目标类型的类作为其快照的一部分。关于类的信息应该通过写入类名来持久化，而不是直接使用 Java 将类序列化。在读取快照时，会读取类名，并通过名称来动态加载类。</p>
<p>这种做法保证了序列化器快照总是可以安全读取。在上面的例子中，如果类型类是使用 Java 序列化来持久化的，一旦类的实现发生了变化，根据 Java 序列化的具体规定，快照可能不再可读，不再二进制兼容。</p>
<h3 id="从-flink-17-之前的废弃序列化快照-api-迁移">从 Flink 1.7 之前的废弃序列化快照 API 迁移</h3>
<p>本节是一个从 Flink 1.7 之前存在的序列化器和序列化器快照的 API 迁移指南。</p>
<p>在 Flink 1.7 之前，序列化器快照是以 TypeSerializerConfigSnapshot 的形式实现的（现在已经被废弃了，将来最终会被移除，完全被新的 TypeSerializerSnapshot 接口取代）。此外，序列化器模式兼容性检查的责任住在 TypeSerializer  内部，在 <code>TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot)</code> 方法中实现。</p>
<p>新旧抽象之间的另一个主要区别是，被废弃的 <code>TypeSerializerConfigSnapshot</code> 不具备实例化之前的序列化器的能力。因此，在你的序列化器仍然返回 <code>TypeSerializerConfigSnapshot</code> 的子类作为它的快照的情况下，序列化器实例本身将总是使用 Java 序列化写入 savepoints，以便在还原时可以使用以前的序列化器。这是很不可取的，因为还原作业是否成功，很容易受到前一个序列化器类的可用性的影响，或者说，一般来说，序列化器实例是否可以在还原时使用 Java 序列化读回。这意味着你的状态只能使用同一个序列化器，一旦你想升级序列化器类或进行模式迁移，可能会出现问题。</p>
<p>为了面向未来，并能灵活地迁移你的状态序列器和模式，强烈建议从旧的抽象中迁移。做到这一点的步骤如下。</p>
<ol>
<li>实现 TypeSerializerSnapshot 的新子类。这将是你的序列化器的新快照。</li>
<li>在 <code>TypeSerializer#snapshotConfiguration()</code> 方法中返回新的 <code>TypeSerializerSnapshot</code> 作为你的 serializer 快照。</li>
<li>从 Flink 1.7 之前存在的保存点恢复作业，然后再取一个保存点。注意，在这一步，旧的序列化器的 <code>TypeSerializerConfigSnapshot</code> 必须仍然存在于 classpath 中，并且不能删除 <code>TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot)</code> 方法的实现。这个过程的目的是将旧保存点中写的 <code>TypeSerializerConfigSnapshot</code> 替换为序列化器新实现的 <code>TypeSerializerSnapshot</code>。</li>
<li>一旦你有一个用 Flink 1.7 拍摄的保存点，保存点将包含 TypeSerializerSnapshot 作为状态序列化器快照，序列化器实例将不再写入保存点中。在这一点上，现在可以安全地删除旧抽象的所有实现（从序列化器中删除旧的 TypeSerializerConfigSnapshot 实现，因为将作为 TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot)）。</li>
</ol>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[运行 Describe 语句]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-run-a-describe-statement/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-insert-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Insert 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-sql-hints/?utm_source=atom_feed" rel="related" type="text/html" title="SQL 提示" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-run-a-describe-statement/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Run a Describe Statement</blockquote><h1 id="describe-语句">DESCRIBE 语句</h1>
<p>DESCRIBE 语句用于描述表或视图的模式。</p>
<h2 id="运行一个describe语句">运行一个DESCRIBE语句</h2>
<p>DESCRIBE语句可以用TableEnvironment的executeSql()方法执行，也可以在SQL CLI中执行。executeSql()方法对于一个成功的DESCRIBE操作会返回给定表的模式，否则会抛出一个异常。</p>
<p>下面的例子展示了如何在TableEnvironment和SQL CLI中运行DESCRIBE语句。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">settings</span> <span class="k">=</span> <span class="nc">EnvironmentSettings</span><span class="o">.</span><span class="n">newInstance</span><span class="o">()...</span>
<span class="k">val</span> <span class="n">tableEnv</span> <span class="k">=</span> <span class="nc">TableEnvironment</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">settings</span><span class="o">)</span>

<span class="c1">// register a table named &#34;Orders&#34;
</span><span class="c1"></span> <span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span>
        <span class="s">&#34;CREATE TABLE Orders (&#34;</span> <span class="o">+</span>
        <span class="s">&#34; `user` BIGINT NOT NULl,&#34;</span> <span class="o">+</span>
        <span class="s">&#34; product VARCHAR(32),&#34;</span> <span class="o">+</span>
        <span class="s">&#34; amount INT,&#34;</span> <span class="o">+</span>
        <span class="s">&#34; ts TIMESTAMP(3),&#34;</span> <span class="o">+</span>
        <span class="s">&#34; ptime AS PROCTIME(),&#34;</span> <span class="o">+</span>
        <span class="s">&#34; PRIMARY KEY(`user`) NOT ENFORCED,&#34;</span> <span class="o">+</span>
        <span class="s">&#34; WATERMARK FOR ts AS ts - INTERVAL &#39;1&#39; SECONDS&#34;</span> <span class="o">+</span>
        <span class="s">&#34;) with (...)&#34;</span><span class="o">)</span>

<span class="c1">// print the schema
</span><span class="c1"></span><span class="n">tableEnv</span><span class="o">.</span><span class="n">executeSql</span><span class="o">(</span><span class="s">&#34;DESCRIBE Orders&#34;</span><span class="o">).</span><span class="n">print</span><span class="o">()</span>
</code></pre></div><pre><code>Flink SQL&gt; CREATE TABLE Orders (
&gt;  `user` BIGINT NOT NULl,
&gt;  product VARCHAR(32),
&gt;  amount INT,
&gt;  ts TIMESTAMP(3),
&gt;  ptime AS PROCTIME(),
&gt;  PRIMARY KEY(`user`) NOT ENFORCED,
&gt;  WATERMARK FOR ts AS ts - INTERVAL '1' SECONDS
&gt; ) with (
&gt;  ...
&gt; );
[INFO] Table has been created.

Flink SQL&gt; DESCRIBE Orders;
</code></pre><pre><code>root
 |-- user: BIGINT NOT NULL
 |-- product: VARCHAR(32)
 |-- amount: INT
 |-- ts: TIMESTAMP(3) *ROWTIME*
 |-- ptime: TIMESTAMP(3) NOT NULL *PROCTIME* AS PROCTIME()
 |-- WATERMARK FOR ts AS `ts` - INTERVAL '1' SECOND
 |-- CONSTRAINT PK_3599338 PRIMARY KEY (user)
</code></pre><p>上述例子的结果是：</p>
<pre><code>+---------+----------------------------------+-------+-----------+-----------------+----------------------------+
|    name |                             type |  null |       key | computed column |                  watermark |
+---------+----------------------------------+-------+-----------+-----------------+----------------------------+
|    user |                           BIGINT | false | PRI(user) |                 |                            |
| product |                      VARCHAR(32) |  true |           |                 |                            |
|  amount |                              INT |  true |           |                 |                            |
|      ts |           TIMESTAMP(3) *ROWTIME* |  true |           |                 | `ts` - INTERVAL '1' SECOND |
|   ptime | TIMESTAMP(3) NOT NULL *PROCTIME* | false |           |      PROCTIME() |                            |
+---------+----------------------------------+-------+-----------+-----------------+----------------------------+
5 rows in set
</code></pre><h2 id="语法">语法</h2>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">DESCRIBE</span><span class="w"> </span><span class="p">[</span><span class="k">catalog_name</span><span class="p">.][</span><span class="n">db_name</span><span class="p">.]</span><span class="k">table_name</span><span class="w">
</span></code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/describe.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/describe.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/sql" term="sql" label="SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[连续查询中的 Join]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-join-in-continuous-queries/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-explan-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Explan 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-join-in-continuous-queries/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Join in Continuous Queries</blockquote><h1 id="连续查询中的-join">连续查询中的 Join</h1>
<p>在批处理数据时，连接是一种常见的、好理解的操作，用来连接两个关系的行。然而，在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/dynamic_tables.html">动态表</a>上的连接的语义就不那么明显了，甚至是混乱的。</p>
<p>正因为如此，有几种方法可以使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#joins">Table API</a> 或 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#joins">SQL</a> 实际执行连接。</p>
<p>关于语法的更多信息，请查看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#joins">Table API</a> 和 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#joins">SQL</a> 中的连接部分。</p>
<h2 id="常规连接">常规连接</h2>
<p>常规联接是最通用的联接类型，联接输入的任何一条新记录或变化都是可见的，并影响整个联接结果。例如，如果左边有一条新记录，它将与右边所有以前和将来的记录一起连接。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w">
</span><span class="w"></span><span class="k">INNER</span><span class="w"> </span><span class="k">JOIN</span><span class="w"> </span><span class="n">Product</span><span class="w">
</span><span class="w"></span><span class="k">ON</span><span class="w"> </span><span class="n">Orders</span><span class="p">.</span><span class="n">productId</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Product</span><span class="p">.</span><span class="n">id</span><span class="w">
</span></code></pre></div><p>这些语义允许任何形式的更新（插入、更新、删除）输入表。</p>
<p>然而，这种操作有一个重要的含义：它需要将 <code>join</code> 输入的双方永远保持在 Flink 的状态中。因此，如果一个或两个输入表持续增长，资源使用量也会无限增长。</p>
<h2 id="区间连接">区间连接</h2>
<p>区间联接是由联接谓词定义的，它检查输入记录的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html">时间属性</a>是否在一定的时间限制内，即时间窗口。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w">
</span><span class="w">  </span><span class="n">Orders</span><span class="w"> </span><span class="n">o</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="n">Shipments</span><span class="w"> </span><span class="n">s</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">orderId</span><span class="w"> </span><span class="k">AND</span><span class="w">
</span><span class="w">      </span><span class="n">o</span><span class="p">.</span><span class="n">ordertime</span><span class="w"> </span><span class="k">BETWEEN</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">shiptime</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;4&#39;</span><span class="w"> </span><span class="n">HOUR</span><span class="w"> </span><span class="k">AND</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">shiptime</span><span class="w">
</span></code></pre></div><p>与普通的 join 操作相比，这种 join 只支持带有时间属性的 append-only 表。由于时间属性是准单调递增的，所以 Flink 可以在不影响结果正确性的情况下，从其状态中删除旧值。</p>
<h2 id="用临时表函数进行联接">用临时表函数进行联接</h2>
<p>使用时态表函数的连接，将一个只附加表（左输入/探针侧）与一个时态表（右输入/建立侧）连接起来，即一个随时间变化的表，并跟踪其变化。关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">临时表</a>的更多信息，请查看相应页面。</p>
<p>下面的例子显示了一个只附加表 Orders，它应该与不断变化的货币汇率表 RatesHistory 连接。</p>
<p>Orders 是一个只附加表，表示给定金额和给定货币的付款。例如在 10:15 有一个金额为 2 欧元的订单。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">rowtime</span><span class="w"> </span><span class="n">amount</span><span class="w"> </span><span class="n">currency</span><span class="w">
</span><span class="w"></span><span class="o">=======</span><span class="w"> </span><span class="o">======</span><span class="w"> </span><span class="o">=========</span><span class="w">
</span><span class="w"></span><span class="mi">10</span><span class="p">:</span><span class="mi">15</span><span class="w">        </span><span class="mi">2</span><span class="w"> </span><span class="n">Euro</span><span class="w">
</span><span class="w"></span><span class="mi">10</span><span class="p">:</span><span class="mi">30</span><span class="w">        </span><span class="mi">1</span><span class="w"> </span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">
</span><span class="w"></span><span class="mi">10</span><span class="p">:</span><span class="mi">32</span><span class="w">       </span><span class="mi">50</span><span class="w"> </span><span class="n">Yen</span><span class="w">
</span><span class="w"></span><span class="mi">10</span><span class="p">:</span><span class="mi">52</span><span class="w">        </span><span class="mi">3</span><span class="w"> </span><span class="n">Euro</span><span class="w">
</span><span class="w"></span><span class="mi">11</span><span class="p">:</span><span class="mi">04</span><span class="w">        </span><span class="mi">5</span><span class="w"> </span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">
</span></code></pre></div><p>RatesHistory 代表了一个不断变化的对日元（汇率为 1）的货币汇率附加表。例如，从 09:00 到 10:45，欧元对日元的汇率是 114，从 10:45 到 11:15 是 116。从 10:45 到 11:15 是 116。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">RatesHistory</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">rowtime</span><span class="w"> </span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">=======</span><span class="w"> </span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">Euro</span><span class="w">        </span><span class="mi">114</span><span class="w">
</span><span class="w"></span><span class="mi">09</span><span class="p">:</span><span class="mi">00</span><span class="w">   </span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span><span class="w"></span><span class="mi">10</span><span class="p">:</span><span class="mi">45</span><span class="w">   </span><span class="n">Euro</span><span class="w">        </span><span class="mi">116</span><span class="w">
</span><span class="w"></span><span class="mi">11</span><span class="p">:</span><span class="mi">15</span><span class="w">   </span><span class="n">Euro</span><span class="w">        </span><span class="mi">119</span><span class="w">
</span><span class="w"></span><span class="mi">11</span><span class="p">:</span><span class="mi">49</span><span class="w">   </span><span class="n">Pounds</span><span class="w">      </span><span class="mi">108</span><span class="w">
</span></code></pre></div><p>我们想计算所有订单的金额，并将其换算成一种通用货币（日元）。</p>
<p>例如，我们想使用给定行时间(114)的适当换算率换算以下订单。</p>
<pre><code>rowtime amount currency
======= ====== =========
10:15        2 Euro
</code></pre><p>如果不使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">临时表</a>的概念，就需要写一个类似的查询。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w">
</span><span class="w">  </span><span class="k">SUM</span><span class="p">(</span><span class="n">o</span><span class="p">.</span><span class="n">amount</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">rate</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">amount</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">o</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="n">RatesHistory</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">r</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">currency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">currency</span><span class="w">
</span><span class="w"></span><span class="k">AND</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">rowtime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="w">
</span><span class="w">  </span><span class="k">SELECT</span><span class="w"> </span><span class="k">MAX</span><span class="p">(</span><span class="n">rowtime</span><span class="p">)</span><span class="w">
</span><span class="w">  </span><span class="k">FROM</span><span class="w"> </span><span class="n">RatesHistory</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">r2</span><span class="w">
</span><span class="w">  </span><span class="k">WHERE</span><span class="w"> </span><span class="n">r2</span><span class="p">.</span><span class="n">currency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">currency</span><span class="w">
</span><span class="w">  </span><span class="k">AND</span><span class="w"> </span><span class="n">r2</span><span class="p">.</span><span class="n">rowtime</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">rowtime</span><span class="p">);</span><span class="w">
</span></code></pre></div><p>在临时表函数 Rates over RatesHistory 的帮助下，我们可以将这样的查询用 SQL 表达为:</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w">
</span><span class="w">  </span><span class="n">o</span><span class="p">.</span><span class="n">amount</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">rate</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">amount</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w">
</span><span class="w">  </span><span class="n">Orders</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">o</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="k">LATERAL</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="p">(</span><span class="n">Rates</span><span class="p">(</span><span class="n">o</span><span class="p">.</span><span class="n">rowtime</span><span class="p">))</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">r</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">currency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">currency</span><span class="w">
</span></code></pre></div><p>来自探针侧的每条记录将与构建侧表在探针侧记录的相关时间属性时的版本连接。为了支持更新（覆盖）构建侧表的先前值，表必须定义一个主键。</p>
<p>在我们的例子中，来自 Orders 的每条记录将与 Rates 的版本在时间 o.rowtime 连接。货币字段之前已经被定义为 Rates 的主键，在我们的例子中用来连接两个表。如果查询使用的是处理时间的概念，那么在执行操作时，新添加的订单将始终与 Rates 的最新版本连接。</p>
<p>与<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html#regular-joins">常规的连接</a>不同，这意味着如果在构建端有新的记录，不会影响之前的连接结果。这又使得 Flink 可以限制必须保留在状态中的元素数量。</p>
<p>与<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html#interval-joins">区间联接</a>相比，时间表联接并没有定义一个时间窗口，在这个时间窗口的范围内，记录将被加入。来自探针侧的记录总是在时间属性指定的时间与构建侧的版本进行连接。因此，构建侧的记录可能是任意的旧记录。随着时间的流逝，记录（对于给定的主键）以前的和不再需要的版本将从状态中删除。</p>
<p>这样的行为使得时间表连接成为用关系术语来表达流丰富的一个很好的候选。</p>
<h3 id="使用方法">使用方法</h3>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#defining-temporal-table-function">定义了临时表函数</a>之后，我们就可以开始使用它了。时间表函数的使用方法可以和普通表函数的使用方法一样。</p>
<p>下面的代码片段解决了我们的动机问题，即从订单表中转换货币。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w">
</span><span class="w">  </span><span class="k">SUM</span><span class="p">(</span><span class="n">o_amount</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">r_rate</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">amount</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w">
</span><span class="w">  </span><span class="n">Orders</span><span class="p">,</span><span class="w">
</span><span class="w">  </span><span class="k">LATERAL</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="p">(</span><span class="n">Rates</span><span class="p">(</span><span class="n">o_proctime</span><span class="p">))</span><span class="w">
</span><span class="w"></span><span class="k">WHERE</span><span class="w">
</span><span class="w">  </span><span class="n">r_currency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">o_currency</span><span class="w">
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// scala
</span><span class="c1"></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">orders</span>
    <span class="o">.</span><span class="n">joinLateral</span><span class="o">(</span><span class="n">rates</span><span class="o">(</span>&#39;o_proctime<span class="o">),</span> &#39;r_currency <span class="o">===</span> &#39;o_currency<span class="o">)</span>
    <span class="o">.</span><span class="n">select</span><span class="o">((</span>&#39;o_amount <span class="o">*</span> &#39;r_rate<span class="o">).</span><span class="n">sum</span> <span class="n">as</span> &#39;amount<span class="o">)</span>
</code></pre></div><p>注意：在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/query_configuration.html">查询配置</a>中定义的状态保留还没有实现时序连接。这意味着计算查询结果所需的状态可能会根据历史表的不同主键的数量而无限增长。</p>
<h3 id="处理时间的-temporal-连接">处理时间的 Temporal 连接</h3>
<p>有了处理时间时间属性，就不可能将过去的时间属性作为参数传递给时序表函数。根据定义，它总是当前的时间戳。因此，对处理时间时间表函数的调用将始终返回底层表的最新已知版本，底层历史表的任何更新也将立即覆盖当前值。</p>
<p>只有构建侧记录的最新版本（相对于定义的主键）才会保存在状态中。构建侧的更新不会对之前发出的连接结果产生影响。</p>
<p>我们可以把处理时的时空联接看成一个简单的 <code>HashMap&lt;K，V&gt;</code>，它存储了来自构建侧的所有记录。当来自构建侧的新记录与之前的某个记录具有相同的键时，旧的值只是简单地被覆盖。来自探针侧的每条记录总是根据 HashMap 的最近/当前状态进行评估。</p>
<h3 id="事件时间的-temporal-连接">事件时间的 Temporal 连接</h3>
<p>有了事件时间属性（即行时间属性），就可以将过去的时间属性传递给时间表函数。这样就可以在一个共同的时间点上连接两个表。</p>
<p>与处理时间的时空连接相比，时空表不仅保留了状态下构建方记录的最新版本（相对于定义的主键），而且还存储了自上次水印以来的所有版本（通过时间来识别）。</p>
<p>例如，一个事件时间时间戳为 12:30:00 的传入行被追加到探针侧表中，根据<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html">临时表的概念</a>，它与构建侧表中时间为 12:30:00 的版本连接。因此，传入的行只与时间戳小于或等于 12:30:00 的行连接，并根据主键应用更新，直到这个时间点。</p>
<p>根据事件时间的定义，<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/event_time.html">水印</a>允许联接操作在时间上向前移动，并丢弃不再需要的构建表的版本，因为预计不会有时间戳较低或相等的传入行。</p>
<h3 id="用时间表进行联接">用时间表进行联接</h3>
<p>带时态表的连接将一个任意表（左输入/探针侧）与一个时态表（右输入/建立侧）连接起来，即一个随时间变化的外部维度表。关于<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#temporal-table">时间表</a>的详细信息，请查看相应页面。</p>
<p>注意: 用户不能使用任意表作为时间表，而是需要使用一个由 LookupableTableSource 支持的表。一个 LookupableTableSource 只能作为一个时态表用于时态连接。有关如何定义 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sourceSinks.html#defining-a-tablesource-with-lookupable">LookupableTableSource</a> 的详细信息，请参见页面。</p>
<p>下面的示例显示了一个 Orders 流，它应该与不断变化的货币汇率表 LatestRates 进行连接。</p>
<p>LatestRates 是一个维度表，它是以最新的汇率来实现的。在时间 10:15、10:30、10:52，LatestRates 的内容如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="mi">10</span><span class="p">:</span><span class="mi">15</span><span class="o">&gt;</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">LatestRates</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="n">Euro</span><span class="w">        </span><span class="mi">114</span><span class="w">
</span><span class="w"></span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="mi">10</span><span class="p">:</span><span class="mi">30</span><span class="o">&gt;</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">LatestRates</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="n">Euro</span><span class="w">        </span><span class="mi">114</span><span class="w">
</span><span class="w"></span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="mi">10</span><span class="p">:</span><span class="mi">52</span><span class="o">&gt;</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">LatestRates</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">currency</span><span class="w">   </span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="o">========</span><span class="w"> </span><span class="o">======</span><span class="w">
</span><span class="w"></span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">   </span><span class="mi">102</span><span class="w">
</span><span class="w"></span><span class="n">Euro</span><span class="w">        </span><span class="mi">116</span><span class="w">     </span><span class="o">&lt;====</span><span class="w"> </span><span class="n">changed</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="mi">114</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="mi">116</span><span class="w">
</span><span class="w"></span><span class="n">Yen</span><span class="w">           </span><span class="mi">1</span><span class="w">
</span></code></pre></div><p>时间 10:15 和 10:30 的 LastestRates 内容相等。欧元汇率在 10:52 从 114 变为 116。</p>
<p>订单是一个只附加的表，表示给定金额和给定货币的支付。例如在 10:15 有一个金额为 2 欧元的订单。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">Orders</span><span class="p">;</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="n">amount</span><span class="w"> </span><span class="n">currency</span><span class="w">
</span><span class="w"></span><span class="o">======</span><span class="w"> </span><span class="o">=========</span><span class="w">
</span><span class="w">     </span><span class="mi">2</span><span class="w"> </span><span class="n">Euro</span><span class="w">             </span><span class="o">&lt;==</span><span class="w"> </span><span class="n">arrived</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="mi">10</span><span class="p">:</span><span class="mi">15</span><span class="w">
</span><span class="w">     </span><span class="mi">1</span><span class="w"> </span><span class="n">US</span><span class="w"> </span><span class="n">Dollar</span><span class="w">        </span><span class="o">&lt;==</span><span class="w"> </span><span class="n">arrived</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="mi">10</span><span class="p">:</span><span class="mi">30</span><span class="w">
</span><span class="w">     </span><span class="mi">2</span><span class="w"> </span><span class="n">Euro</span><span class="w">             </span><span class="o">&lt;==</span><span class="w"> </span><span class="n">arrived</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="mi">10</span><span class="p">:</span><span class="mi">52</span><span class="w">
</span></code></pre></div><p>我们想计算所有订单的金额，并将其兑换成一种通用货币（日元）。</p>
<p>例如，我们想使用 LatestRates 中的最新汇率来转换以下订单。结果将是：</p>
<pre><code>amount currency     rate   amout*rate
====== ========= ======= ============
     2 Euro          114          228    &lt;== arrived at time 10:15
     1 US Dollar     102          102    &lt;== arrived at time 10:30
     2 Euro          116          232    &lt;== arrived at time 10:52
</code></pre><p>在时间表连接的帮助下，我们可以将这样的查询用 SQL 表达为。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w">
</span><span class="w">  </span><span class="n">o</span><span class="p">.</span><span class="n">amout</span><span class="p">,</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">currency</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">rate</span><span class="p">,</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">amount</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">rate</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w">
</span><span class="w">  </span><span class="n">Orders</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">o</span><span class="w">
</span><span class="w">  </span><span class="k">JOIN</span><span class="w"> </span><span class="n">LatestRates</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">SYSTEM_TIME</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">OF</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">proctime</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">r</span><span class="w">
</span><span class="w">  </span><span class="k">ON</span><span class="w"> </span><span class="n">r</span><span class="p">.</span><span class="n">currency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">o</span><span class="p">.</span><span class="n">currency</span><span class="w">
</span></code></pre></div><p>来自探针侧的每一条记录都将与构建侧表的当前版本相连接。在我们的例子中，查询使用的是处理时间的概念，所以在执行操作时，新追加的订单将始终与最新版本的 LatestRates 连接。需要注意的是，结果并不是处理时间的确定性。</p>
<p>与<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html#regular-joins">常规联接</a>相比，尽管构建端发生了变化，但时态表联接之前的结果不会受到影响。另外，时态表连接操作符非常轻量级，不保留任何状态。</p>
<p>与<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html#interval-joins">区间联接</a>相比，时态表联接不定义记录联接的时间窗口。在处理时，来自探针侧的记录总是与构建侧的最新版本连接。因此，构建侧的记录可能是任意旧的。</p>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html#join-with-a-temporal-table-function">时态表函数连接</a>和时态表连接的动机都是一样的，但在 SQL 语法和运行时的实现上却有所不同。</p>
<ul>
<li>时间表函数 join 的 SQL 语法是 join UDTF，而时间表 join 使用 SQL:2011 中引入的标准时间表语法。</li>
<li>时态表函数 join 的实现实际上是将两个流连接起来并保持状态，而时态表 join 只是接收唯一的输入流，并根据记录中的键查找外部数据库。</li>
<li>时态表函数联接通常用于联接变更日志流，而时态表联接通常用于联接外部表（即维表）。</li>
</ul>
<p>这样的行为使得时态表连接成为用关系术语来表达流丰富的一个很好的候选。</p>
<p>在未来，时态表连接将支持时态表函数连接的特性，即支持时态连接 changelog 流。</p>
<h3 id="使用方法-1">使用方法</h3>
<p>时间表连接的语法如下。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w"> </span><span class="p">[</span><span class="n">column_list</span><span class="p">]</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w"> </span><span class="n">table1</span><span class="w"> </span><span class="p">[</span><span class="k">AS</span><span class="w"> </span><span class="o">&lt;</span><span class="n">alias1</span><span class="o">&gt;</span><span class="p">]</span><span class="w">
</span><span class="w"></span><span class="p">[</span><span class="k">LEFT</span><span class="p">]</span><span class="w"> </span><span class="k">JOIN</span><span class="w"> </span><span class="n">table2</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">SYSTEM_TIME</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">OF</span><span class="w"> </span><span class="n">table1</span><span class="p">.</span><span class="n">proctime</span><span class="w"> </span><span class="p">[</span><span class="k">AS</span><span class="w"> </span><span class="o">&lt;</span><span class="n">alias2</span><span class="o">&gt;</span><span class="p">]</span><span class="w">
</span><span class="w"></span><span class="k">ON</span><span class="w"> </span><span class="n">table1</span><span class="p">.</span><span class="k">column</span><span class="o">-</span><span class="n">name1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">table2</span><span class="p">.</span><span class="k">column</span><span class="o">-</span><span class="n">name1</span><span class="w">
</span></code></pre></div><p>目前只支持 INNER JOIN 和 LEFT JOIN。在 temporal 表后应跟上 FOR SYSTEM_TIME AS OF table1.proctime。proctime 是 table1 的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/time_attributes.html#processing-time">处理时间属性</a>。这意味着它在处理时间对时间表进行快照，当从左表连接每一条记录时，它就会对时间表进行快照。</p>
<p>例如，在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/temporal_tables.html#defining-temporal-table">定义了 temporal 表</a>之后，我们可以按以下方式使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">SELECT</span><span class="w">
</span><span class="w">  </span><span class="k">SUM</span><span class="p">(</span><span class="n">o_amount</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">r_rate</span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">amount</span><span class="w">
</span><span class="w"></span><span class="k">FROM</span><span class="w">
</span><span class="w">  </span><span class="n">Orders</span><span class="w">
</span><span class="w">  </span><span class="k">JOIN</span><span class="w"> </span><span class="n">LatestRates</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="n">SYSTEM_TIME</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">OF</span><span class="w"> </span><span class="n">o_proctime</span><span class="w">
</span><span class="w">  </span><span class="k">ON</span><span class="w"> </span><span class="n">r_currency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">o_currency</span><span class="w">
</span></code></pre></div><p>注意: 这只在 Blink 计划器中支持。</p>
<p>注意: 目前只在 SQL 中支持，在 Table API 中还不支持。</p>
<p>注意: Flink 目前不支持事件时间的表连接。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/table-api-sql" term="table-api-sql" label="Table API &amp; SQL" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[迭代]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-iterations/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-flink-dataset-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Dataset API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-hadoop-compatibility-beta/?utm_source=atom_feed" rel="related" type="text/html" title="Hadoop 的兼容性" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-batch-examples/?utm_source=atom_feed" rel="related" type="text/html" title="批处理例子" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-zipping-elements-in-a-dataset/?utm_source=atom_feed" rel="related" type="text/html" title="数据集中的 zipping 元素" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-iterations/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Iterations</blockquote><h2 id="迭代">迭代</h2>
<p>迭代算法出现在数据分析的许多领域，如机器学习或图形分析。为了实现大数据的承诺，从数据中提取有意义的信息，此类算法至关重要。随着人们对在非常大的数据集上运行这类算法的兴趣越来越大，就需要以大规模并行的方式执行迭代。</p>
<p>Flink 程序通过定义一个步骤函数并将其嵌入到一个特殊的迭代运算符中来实现迭代算法。这个运算符有两个变体。Iterate 和 Delta Iterate。这两个运算符都是在当前的迭代状态上反复调用步骤函数，直到达到某个终止条件。</p>
<p>在这里，我们提供了这两个操作符变体的背景，并概述了它们的用法。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">编程指南</a>解释了如何在 Scala 和 Java 中实现这些操作符。我们还通过 Flink 的图处理 API <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/gelly/index.html">Gelly</a> 支持以顶点为中心的迭代和集和应用迭代。</p>
<p>下表提供了这两种运算符的概述:</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">Iterate</th>
<th style="text-align:left">Delta Iterate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Iteration 输入</td>
<td style="text-align:left">Partial Solution</td>
<td style="text-align:left">Workset and Solution Set</td>
</tr>
<tr>
<td style="text-align:left">Step 函数</td>
<td style="text-align:left">Arbitrary Data Flows</td>
<td style="text-align:left">Arbitrary Data Flows</td>
</tr>
<tr>
<td style="text-align:left">State Update</td>
<td style="text-align:left">Next partial solution</td>
<td style="text-align:left">Next workset,Changes to solution set</td>
</tr>
<tr>
<td style="text-align:left">Iteration Result</td>
<td style="text-align:left">Last partial solution</td>
<td style="text-align:left">Solution set state after last iteration</td>
</tr>
<tr>
<td style="text-align:left">Termination</td>
<td style="text-align:left">Maximum number of iterations (default),Custom aggregator convergence</td>
<td style="text-align:left">Maximum number of iterations or empty workset (default),Custom aggregator convergence</td>
</tr>
</tbody>
</table>
<h2 id="iterate-operator">Iterate Operator</h2>
<p>迭代运算符涵盖了简单的迭代形式：在每一次迭代中，step 函数都会消耗整个输入（上一次迭代的结果，或初始数据集），并计算出下一个版本的部分解（如 <code>map</code>, <code>reduce</code>, <code>join</code> 等）。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/iterations_iterate_operator.png" alt="img"></p>
<ol>
<li>迭代输入。第一次迭代的初始输入，来自数据源或之前的运算符。</li>
<li>step 函数。步骤函数将在每次迭代中执行。它是一个任意的数据流，由 map、reduce、join 等运算符组成，取决于你手头的具体任务。</li>
<li>下一个部分解决方案。在每次迭代中，步骤函数的输出将被反馈到下一次迭代中。</li>
<li>迭代结果。上一次迭代的输出会被写入数据接收器，或者作为后续运算符的输入。</li>
</ol>
<p>有多个选项可以指定迭代的终止条件。</p>
<ul>
<li>最大迭代次数。没有任何进一步的条件，迭代将被执行这么多次。</li>
<li>自定义聚合器收敛。迭代允许指定自定义聚合器和收敛标准，比如对发出的记录数量进行加总（聚合器），如果这个数字为零就终止（收敛标准）。</li>
</ul>
<p>你也可以用伪代码来思考迭代操作符。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">IterationState</span> <span class="n">state</span> <span class="o">=</span> <span class="n">getInitialState</span><span class="o">();</span>

<span class="k">while</span> <span class="o">(!</span><span class="n">terminationCriterion</span><span class="o">())</span> <span class="o">{</span>
	<span class="n">state</span> <span class="o">=</span> <span class="n">step</span><span class="o">(</span><span class="n">state</span><span class="o">);</span>
<span class="o">}</span>

<span class="n">setFinalState</span><span class="o">(</span><span class="n">state</span><span class="o">);</span>
</code></pre></div><p>详情和代码示例请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">编程指南</a>。</p>
<h2 id="例子-数字递增">例子: 数字递增</h2>
<p>在下面的例子中，我们对一组数字进行迭代递增。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/iterations_iterate_operator_example.png" alt="img"></p>
<ol>
<li>迭代输入。初始输入是从数据源读取的，由5个单字段记录组成（整数1至5）。</li>
<li>step 函数。步进函数是一个单一的 map 运算符，它将整数字段从i递增到i+1。它将被应用于输入的每一条记录。</li>
<li>下一个部分解。step 函数的输出将是 map 运算符的输出，也就是整数递增的记录。</li>
<li>迭代结果。经过十次迭代，初始数字将被递增十倍，结果是整数11到15。</li>
</ol>
<pre><code>// 1st           2nd                       10th
map(1) -&gt; 2      map(2) -&gt; 3      ...      map(10) -&gt; 11
map(2) -&gt; 3      map(3) -&gt; 4      ...      map(11) -&gt; 12
map(3) -&gt; 4      map(4) -&gt; 5      ...      map(12) -&gt; 13
map(4) -&gt; 5      map(5) -&gt; 6      ...      map(13) -&gt; 14
map(5) -&gt; 6      map(6) -&gt; 7      ...      map(14) -&gt; 15
</code></pre><p>请注意，1、2和4可以是任意的数据流。</p>
<h2 id="增量迭代运算符">增量迭代运算符</h2>
<p>delta 迭代算子涵盖了增量迭代的情况。增量迭代有选择地修改其解的元素，并对解进行演化，而不是完全重新计算。</p>
<p>在适用的情况下，这将导致更高效的算法，因为在每次迭代中，并不是解集中的每个元素都会改变。这样就可以把注意力集中在解的热点部分，而对冷点部分不加处理。通常情况下，大部分解的冷却速度比较快，后面的迭代只对一小部分数据进行操作。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/iterations_delta_iterate_operator.png" alt="img"></p>
<ol>
<li>迭代输入。从数据源或以前的运算符中读取初始工作集和解决方案集，作为第一次迭代的输入。</li>
<li>step 函数。在每次迭代中，步骤函数将被执行。它是一个任意的数据流，由 map、reduce、join 等运算符组成，取决于你手头的具体任务。</li>
<li>下一个工作集/更新解决方案集。下一个工作集驱动迭代计算，并将反馈到下一个迭代中。此外，解决方案集将被更新并隐式转发（它不需要被重建）。这两个数据集都可以通过步长函数的不同运算符进行更新。</li>
<li>迭代结果。最后一次迭代后，解集被写入数据接收器，或作为下面运算符的输入。</li>
</ol>
<p>delta 迭代的默认终止条件由空工作集收敛准则和最大迭代次数指定。当产生的下一个工作集为空或达到最大迭代次数时，迭代将终止。也可以指定一个自定义的聚合器和收敛准则。</p>
<p>你也可以用伪代码来思考迭代操作符。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="n">IterationState</span> <span class="n">workset</span> <span class="o">=</span> <span class="n">getInitialState</span><span class="o">();</span>
<span class="n">IterationState</span> <span class="n">solution</span> <span class="o">=</span> <span class="n">getInitialSolution</span><span class="o">();</span>

<span class="k">while</span> <span class="o">(!</span><span class="n">terminationCriterion</span><span class="o">())</span> <span class="o">{</span>
	<span class="o">(</span><span class="n">delta</span><span class="o">,</span> <span class="n">workset</span><span class="o">)</span> <span class="o">=</span> <span class="n">step</span><span class="o">(</span><span class="n">workset</span><span class="o">,</span> <span class="n">solution</span><span class="o">);</span>

	<span class="n">solution</span><span class="o">.</span><span class="na">update</span><span class="o">(</span><span class="n">delta</span><span class="o">)</span>
<span class="o">}</span>

<span class="n">setFinalState</span><span class="o">(</span><span class="n">solution</span><span class="o">);</span>
</code></pre></div><p>详情和代码示例请参见<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html">编程指南</a>。</p>
<h2 id="例子-在图中传播最小值">例子: 在图中传播最小值</h2>
<p>在下面的例子中，每个顶点都有一个ID和一个着色。每个顶点将把它的顶点ID传播给邻近的顶点。目标是给子图中的每个顶点分配最小的ID。如果一个接收到的ID比当前的ID小，它就会改变成接收到ID的顶点的颜色。这在社区分析或连接组件计算中可以找到一个应用。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/iterations_delta_iterate_operator_example.png" alt="img"></p>
<p>初始输入被设定为工作集和解决方案集。在上图中，颜色直观地显示了解决方案集的演变。随着每次迭代，最小ID的颜色在各自的子图中蔓延。同时，每一次迭代，工作量（交换和比较顶点ID）都在减少。这对应于工作集的大小递减，在三次迭代后，工作集从所有七个顶点变为零，此时迭代终止。重要的观察是，下半子图在上半子图之前收敛，而delta迭代能够用工作集抽象捕捉到这一点。</p>
<p>在上子图中，ID 1（橙色）是最小ID。在第一次迭代中，它将被传播到顶点2，随后它的颜色将变为橙色。顶点3和4将收到ID 2（黄色）作为它们当前的最小ID，并改变为黄色。因为顶点1的颜色在第一次迭代中没有改变，所以在下一个工作集中可以跳过它。</p>
<p>在下层子图中，ID 5（青色）是最小ID。下层子图的所有顶点都会在第一次迭代中收到它。同样，我们可以在下一个工作集中跳过没有变化的顶点（顶点5）。</p>
<p>在第2次迭代中，工作集大小已经从7个元素减少到5个元素（顶点2、3、4、6和7）。这些都是迭代的一部分，并进一步传播它们当前的最小ID。在这次迭代之后，下半部分子图已经收敛了（图的冷部分），因为它在工作集中没有元素，而上半部分则需要对剩下的两个工作集元素（顶点3和4）进行进一步的迭代（图的热部分）。</p>
<p>当第3次迭代后工作集为空时，迭代终止。</p>
<h2 id="superstep-同步">Superstep 同步</h2>
<p>我们将迭代操作符的步骤函数的每次执行称为单次迭代。在并行设置中，步骤函数的多个实例在迭代状态的不同分区上并行评估。在许多设置中，在所有并行实例上对步骤函数的一次评估形成一个所谓的超级步骤，这也是同步的粒度。因此，一个迭代的所有并行任务都需要完成 superstep，才会初始化下一个 superstep。终止标准也将在 superstep 障碍处进行评估。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/iterations_supersteps.png" alt="img"></p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/iterations.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/dataset-api" term="dataset-api" label="DataSet API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[集群 Execution]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-cluster-execution/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-alter-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Alter 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-catalogs/?utm_source=atom_feed" rel="related" type="text/html" title="Catalogs" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-create-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Create 语句" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-dataset-transformations/?utm_source=atom_feed" rel="related" type="text/html" title="Dataset 变换" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-drop-statements/?utm_source=atom_feed" rel="related" type="text/html" title="Drop 语句" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-cluster-execution/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Cluster Execution</blockquote><p>Flink 程序可以在许多机器组成的集群上分布式运行。有两种方法可以将程序发送到集群上执行。</p>
<h2 id="命令行接口">命令行接口</h2>
<p>命令行界面让您可以将打包的程序（JAR）提交到集群（或单机设置）。</p>
<p>详情请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/cli.html">命令行接口</a>文档。</p>
<h2 id="远程环境">远程环境</h2>
<p>远程环境可以让你直接在集群上执行 Flink Java 程序。远程环境指向你要执行程序的集群。</p>
<h3 id="maven-依赖">Maven 依赖</h3>
<p>如果你是以 Maven 项目的形式开发程序，你必须使用这个依赖关系添加 flink-clients 模块。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-clients_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><h2 id="例子">例子</h2>
<p>以下说明了 RemoteEnvironment 的使用。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
    <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span>
        <span class="o">.</span><span class="na">createRemoteEnvironment</span><span class="o">(</span><span class="s">&#34;flink-jobmanager&#34;</span><span class="o">,</span> <span class="n">8081</span><span class="o">,</span> <span class="s">&#34;/home/user/udfs.jar&#34;</span><span class="o">);</span>

    <span class="n">DataSet</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">readTextFile</span><span class="o">(</span><span class="s">&#34;hdfs://path/to/file&#34;</span><span class="o">);</span>

    <span class="n">data</span>
        <span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="k">new</span> <span class="n">FilterFunction</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;()</span> <span class="o">{</span>
            <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">filter</span><span class="o">(</span><span class="n">String</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
                <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="na">startsWith</span><span class="o">(</span><span class="s">&#34;http://&#34;</span><span class="o">);</span>
            <span class="o">}</span>
        <span class="o">})</span>
        <span class="o">.</span><span class="na">writeAsText</span><span class="o">(</span><span class="s">&#34;hdfs://path/to/result&#34;</span><span class="o">);</span>

    <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">();</span>
<span class="o">}</span>
</code></pre></div><p>请注意，该程序包含自定义用户代码，因此需要一个包含代码类的 JAR 文件。远程环境的构造函数使用 JAR 文件的路径。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/cluster_execution.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/cluster_execution.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/cluster-execution" term="cluster-execution" label="Cluster Execution" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[项目配置]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-22-project-configuration/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-java-lambda-expressions/?utm_source=atom_feed" rel="related" type="text/html" title="Java Lambda 表达式" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-joining/?utm_source=atom_feed" rel="related" type="text/html" title="Joining" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-operators/?utm_source=atom_feed" rel="related" type="text/html" title="Operators" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-scala-api-extensions/?utm_source=atom_feed" rel="related" type="text/html" title="Scala API 扩展" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-22-side-outputs/?utm_source=atom_feed" rel="related" type="text/html" title="侧输出" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-22-project-configuration/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-22T00:00:00+08:00</published>
            <updated>2020-08-22T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Project Configuration</blockquote><h2 id="项目配置">项目配置</h2>
<p>每个 Flink 应用都依赖于一组 Flink 库。最起码，应用程序依赖于 Flink APIs。许多应用还依赖于某些连接器库（如 Kafka、Cassandra 等）。当运行 Flink 应用时（无论是在分布式部署中，还是在 IDE 中进行测试），Flink 运行时库也必须是可用的。</p>
<h3 id="flink-核心和应用依赖性">Flink 核心和应用依赖性</h3>
<p>与大多数运行用户定义应用的系统一样，Flink 中的依赖和库有两大类。</p>
<ul>
<li>Flink 核心依赖。Flink 本身由一组运行系统所需的类和依赖关系组成，例如协调、网络、检查点、故障转移、API、操作（如窗口化）、资源管理等。所有这些类和依赖项的集合构成了 Flink 运行时的核心，在 Flink 应用启动时必须存在。</li>
</ul>
<p>这些核心类和依赖项被打包在 flink-dist jar 中。它们是 Flink 的 lib 文件夹的一部分，也是基本的 Flink 容器镜像的一部分。把这些依赖关系想象成类似于 Java 的核心库（rt.jar，charsets.jar 等），其中包含了 String 和 List 等类。</p>
<p>Flink Core Dependencies 不包含任何连接器或库（CEP、SQL、ML 等），以避免默认情况下 classpath 中的依赖关系和类数量过多。事实上，我们尽量让核心依赖关系保持纤细，以保持默认 classpath 小，避免依赖冲突。</p>
<ul>
<li>用户应用依赖是指特定用户应用所需要的所有连接器、格式或库。</li>
</ul>
<p>用户应用程序通常被打包成一个应用程序 jar，其中包含了应用程序代码和所需的连接器和库依赖。</p>
<p>用户应用依赖关系明确不包括 Flink DataStream API 和运行时依赖关系，因为这些已经是 Flink 核心依赖关系的一部分。</p>
<h3 id="设置一个项目-基本依赖性">设置一个项目: 基本依赖性</h3>
<p>每一个 Flink 应用都需要最低限度的 API 依赖关系，来进行开发。</p>
<p>当手动设置项目时，你需要为 Java/Scala API 添加以下依赖关系（这里用 Maven 语法表示，但同样的依赖关系也适用于其他构建工具（Gradle、SBT 等）。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-streaming-scala_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;scope&gt;</span>provided<span class="nt">&lt;/scope&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>重要：请注意，所有这些依赖关系的范围都被设置为 <em>provided</em>。这意味着它们需要被编译，但它们不应该被打包到项目的应用程序 jar 文件中&ndash;这些依赖是 Flink 核心依赖，在任何设置中都是可用的。</p>
<p>强烈建议将这些依赖关系保持在 <em>provid</em> 的作用域内。如果它们没有被设置为 <em>provided</em>，最好的情况是生成的 JAR 变得过大，因为它也包含了所有 Flink 核心依赖。最坏的情况是，添加到应用程序的 jar 文件中的 Flink 核心依赖与你自己的一些依赖版本发生冲突（通常通过倒类加载来避免）。</p>
<p>关于 IntelliJ 的说明：要使应用程序在 IntelliJ IDEA 中运行，就必须在运行配置中勾选 Include dependencies with &ldquo;Provided&rdquo; scope box。如果这个选项不可用（可能是由于使用了旧的 IntelliJ IDEA 版本），那么一个简单的变通方法是创建一个调用应用程序 <code>main()</code> 方法的测试。</p>
<h3 id="添加连接器和库依赖性">添加连接器和库依赖性</h3>
<p>大多数应用都需要特定的连接器或库来运行，例如与 Kafka、Cassandra 等的连接器。这些连接器不是 Flink 核心依赖的一部分，必须作为依赖关系添加到应用程序中。</p>
<p>下面是一个将 Kafka 的连接器作为依赖项添加的例子（Maven 语法）。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>flink-connector-kafka_2.11<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>1.11.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div><p>我们建议将应用程序代码和所有需要的依赖关系打包成一个带有依赖关系的 jar，我们称之为应用 jar。应用 jar 可以提交给一个已经运行的 Flink 集群，或者添加到 Flink 应用容器镜像中。</p>
<p>从 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html">Java 项目模板</a>或 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration">Scala 项目模板</a>创建的项目被配置为在运行 <code>mvn clean package</code> 时自动将应用依赖关系包含到应用 jar 中。对于没有从这些模板中设置的项目，我们建议添加 Maven Shade Plugin（如下文附录中所列）来构建包含所有所需依赖项的应用 jar。</p>
<p>重要的是。为了让 Maven（和其他构建工具）正确地将依赖关系打包到应用 jar 中，这些应用依赖关系必须在编译范围中指定（与核心依赖关系不同，后者必须在提供的范围中指定）。</p>
<h3 id="scala-版本">Scala 版本</h3>
<p>Scala 版本(2.11, 2.12 等)彼此之间不是二进制兼容的。因此，Flink for Scala 2.11 不能用于使用 Scala 2.12 的应用程序。</p>
<p>所有的 Flink 依赖性都是以 Scala 版本为后缀的，例如 flink-streaming-scala_2.11。</p>
<p>只使用 Java 的开发者可以选择任何 Scala 版本，Scala 开发者需要选择与其应用的 Scala 版本相匹配的 Scala 版本。</p>
<p>请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/flinkDev/building.html#scala-versions">构建指南</a>，了解如何为特定的 Scala 版本构建 Flink。</p>
<h3 id="hadoop-依赖性">Hadoop 依赖性</h3>
<p>一般规则：永远不需要直接将 Hadoop 依赖关系添加到您的应用程序中。唯一的例外是当使用现有的 Hadoop 输入/输出格式和 Flink 的 Hadoop  兼容性包装时。</p>
<p>如果您想将 Flink 与 Hadoop 一起使用，您需要有一个包含 Hadoop 依赖的 Flink 设置，而不是将 Hadoop 添加为应用程序依赖。详情请参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html">Hadoop 设置指南</a>。</p>
<p>这种设计主要有两个原因。</p>
<ul>
<li>
<p>一些 Hadoop 交互发生在 Flink 的核心中，可能是在用户应用启动之前，例如为检查点设置 HDFS，通过 Hadoop 的 Kerberos 令牌进行认证，或者在 YARN 上进行部署。</p>
</li>
<li>
<p>Flink 的倒类加载方法将许多过渡性依赖从核心依赖中隐藏起来。这不仅适用于 Flink 自身的核心依赖，也适用于 Hadoop 在设置中存在的依赖。这样一来，应用程序可以使用相同依赖的不同版本，而不会遇到依赖冲突（相信我们，这是一个大问题，因为 Hadoop 的依赖树是巨大的）。</p>
</li>
</ul>
<p>如果你在 IDE 内部的测试或开发过程中需要 Hadoop 依赖关系（例如用于 HDFS 访问），请将这些依赖关系配置成类似于要测试或提供的依赖关系的范围。</p>
<h3 id="maven-快速入门">Maven 快速入门</h3>
<p><strong>所需</strong></p>
<p>唯一的要求是工作中的 Maven 3.0.4（或更高）和 Java 8.x 的安装。</p>
<p><strong>创建项目</strong></p>
<p>使用以下命令之一来创建项目。</p>
<ul>
<li>使用 Maven 原型</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ mvn archetype:generate                           <span class="se">\
</span><span class="se"></span>  -DarchetypeGroupId<span class="o">=</span>org.apache.flink              <span class="se">\
</span><span class="se"></span>  -DarchetypeArtifactId<span class="o">=</span>flink-quickstart-java      <span class="se">\
</span><span class="se"></span>  -DarchetypeVersion<span class="o">=</span>1.11.0
</code></pre></div><p>这可以让你为新创建的项目命名，它将交互式地要求你提供 groupId、artifactId 和包名。</p>
<ul>
<li>运行快速启动脚本</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ curl https://flink.apache.org/q/quickstart.sh <span class="p">|</span> bash -s 1.11.0
</code></pre></div><p>我们建议您将该项目导入到您的 IDE 中进行开发和测试。IntelliJ IDEA 支持开箱即用的 Maven 项目。如果您使用 Eclipse，<a href="http://www.eclipse.org/m2e/">m2e 插件</a>允许<a href="http://books.sonatype.com/m2eclipse-book/reference/creating-sect-importing-projects.html#fig-creating-import">导入 Maven 项目</a>。有些 Eclipse 捆绑包默认包含该插件，有些则需要您手动安装。</p>
<p>请注意：Java 默认的 JVM 堆大小对 Flink 来说可能太小。你必须手动增加它。在 Eclipse 中，选择 Run Configurations -&gt; Arguments，并在 VM Arguments 框中写下 -Xmx800m。在 IntelliJ IDEA 中推荐的改变 JVM 选项的方法是来自 Help | Edit Custom VM Options 菜单。详情请看<a href="https://intellij-support.jetbrains.com/hc/en-us/articles/206544869-Configuring-JVM-options-and-platform-properties">这篇文章</a>。</p>
<h4 id="构建项目">构建项目</h4>
<p>如果你想构建/打包你的项目，进入你的项目目录并运行 &ldquo;mvn clean package&rdquo; 命令。你会发现一个 JAR 文件，其中包含了你的应用程序，加上你可能已经添加的连接器和库作为应用程序的依赖关系：<code>target/&lt;artifact-id&gt;-&lt;version&gt;.jar</code>。</p>
<p>注意：如果您使用与 StreamingJob 不同的类作为应用程序的主类/入口点，我们建议您相应地更改 pom.xml 文件中的 mainClass 设置。这样，Flink 就可以从 JAR 文件中运行应用程序，而不需要额外指定主类。</p>
<h3 id="gradle">Gradle</h3>
<p><strong>需求</strong></p>
<p>唯一的要求是工作的 Gradle 3.x（或更高）和 Java 8.x 安装。</p>
<p><strong>创建项目</strong></p>
<p>使用以下命令之一来创建一个项目。</p>
<ul>
<li>Gradle 例子</li>
</ul>
<p><strong>build.gradle</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="err">buildscript</span> <span class="p">{</span>
    <span class="err">repositories</span> <span class="err">{</span>
        <span class="err">jcenter()</span> <span class="err">//</span> <span class="err">this</span> <span class="err">applies</span> <span class="err">only</span> <span class="err">to</span> <span class="err">the</span> <span class="err">Gradle</span> <span class="err">&#39;Shadow&#39;</span> <span class="err">plugin</span>
    <span class="p">}</span>
    <span class="err">dependencies</span> <span class="p">{</span>
        <span class="err">classpath</span> <span class="err">&#39;com.github.jengelman.gradle.plugins:shadow:2.0.4&#39;</span>
    <span class="p">}</span>
<span class="err">}</span>

<span class="err">plugins</span> <span class="p">{</span>
    <span class="err">id</span> <span class="err">&#39;java&#39;</span>
    <span class="err">id</span> <span class="err">&#39;application&#39;</span>
    <span class="err">//</span> <span class="err">shadow</span> <span class="err">plugin</span> <span class="err">to</span> <span class="err">produce</span> <span class="err">fat</span> <span class="err">JARs</span>
    <span class="err">id</span> <span class="err">&#39;com.github.johnrengelman.shadow&#39;</span> <span class="err">version</span> <span class="err">&#39;2.0.4&#39;</span>
<span class="p">}</span>


<span class="err">//</span> <span class="err">artifact</span> <span class="err">properties</span>
<span class="err">group</span> <span class="err">=</span> <span class="err">&#39;org.myorg.quickstart&#39;</span>
<span class="err">version</span> <span class="err">=</span> <span class="err">&#39;</span><span class="mf">0.1</span><span class="err">-SNAPSHOT&#39;</span>
<span class="err">mainClassName</span> <span class="err">=</span> <span class="err">&#39;org.myorg.quickstart.StreamingJob&#39;</span>
<span class="err">description</span> <span class="err">=</span> <span class="s2">&#34;&#34;&#34;Flink Quickstart Job&#34;&#34;&#34;</span>

<span class="err">ext</span> <span class="p">{</span>
    <span class="err">javaVersion</span> <span class="err">=</span> <span class="err">&#39;1.8&#39;</span>
    <span class="err">flinkVersion</span> <span class="err">=</span> <span class="err">&#39;1.11.0&#39;</span>
    <span class="err">scalaBinaryVersion</span> <span class="err">=</span> <span class="err">&#39;2.11&#39;</span>
    <span class="err">slf4jVersion</span> <span class="err">=</span> <span class="err">&#39;1.7.15&#39;</span>
    <span class="err">log4jVersion</span> <span class="err">=</span> <span class="err">&#39;2.12.1&#39;</span>
<span class="p">}</span>


<span class="err">sourceCompatibility</span> <span class="err">=</span> <span class="err">javaVersion</span>
<span class="err">targetCompatibility</span> <span class="err">=</span> <span class="err">javaVersion</span>
<span class="err">tasks.withType(JavaCompile)</span> <span class="p">{</span>
	<span class="err">options.encoding</span> <span class="err">=</span> <span class="err">&#39;UTF-8&#39;</span>
<span class="p">}</span>

<span class="err">applicationDefaultJvmArgs</span> <span class="err">=</span> <span class="p">[</span><span class="s2">&#34;-Dlog4j.configurationFile=log4j2.properties&#34;</span><span class="p">]</span>

<span class="err">task</span> <span class="err">wrapper(type:</span> <span class="err">Wrapper)</span> <span class="p">{</span>
    <span class="err">gradleVersion</span> <span class="err">=</span> <span class="err">&#39;3.1&#39;</span>
<span class="p">}</span>

<span class="err">//</span> <span class="err">declare</span> <span class="err">where</span> <span class="err">to</span> <span class="err">find</span> <span class="err">the</span> <span class="err">dependencies</span> <span class="err">of</span> <span class="err">your</span> <span class="err">project</span>
<span class="err">repositories</span> <span class="p">{</span>
    <span class="err">mavenCentral()</span>
    <span class="err">maven</span> <span class="err">{</span> <span class="err">url</span> <span class="nt">&#34;https://repository.apache.org/content/repositories/snapshots/&#34;</span> <span class="p">}</span>
<span class="err">}</span>

<span class="err">//</span> <span class="err">NOTE:</span> <span class="err">We</span> <span class="err">cannot</span> <span class="err">use</span> <span class="s2">&#34;compileOnly&#34;</span> <span class="err">or</span> <span class="s2">&#34;shadow&#34;</span> <span class="err">configurations</span> <span class="err">since</span> <span class="err">then</span> <span class="err">we</span> <span class="err">could</span> <span class="err">not</span> <span class="err">run</span> <span class="err">code</span>
<span class="err">//</span> <span class="err">in</span> <span class="err">the</span> <span class="err">IDE</span> <span class="err">or</span> <span class="err">with</span> <span class="s2">&#34;gradle run&#34;</span><span class="err">.</span> <span class="err">We</span> <span class="err">also</span> <span class="err">cannot</span> <span class="err">exclude</span> <span class="err">transitive</span> <span class="err">dependencies</span> <span class="err">from</span> <span class="err">the</span>
<span class="err">//</span> <span class="err">shadowJar</span> <span class="err">yet</span> <span class="err">(see</span> <span class="err">https://github.com/johnrengelman/shadow/issues/</span><span class="mi">159</span><span class="err">).</span>
<span class="err">//</span> <span class="err">-&gt;</span> <span class="err">Explicitly</span> <span class="err">define</span> <span class="err">the</span> <span class="err">//</span> <span class="err">libraries</span> <span class="err">we</span> <span class="err">want</span> <span class="err">to</span> <span class="err">be</span> <span class="err">included</span> <span class="err">in</span> <span class="err">the</span> <span class="s2">&#34;flinkShadowJar&#34;</span> <span class="err">configuration!</span>
<span class="err">configurations</span> <span class="p">{</span>
    <span class="err">flinkShadowJar</span> <span class="err">//</span> <span class="err">dependencies</span> <span class="err">which</span> <span class="err">go</span> <span class="err">into</span> <span class="err">the</span> <span class="err">shadowJar</span>

    <span class="err">//</span> <span class="err">always</span> <span class="err">exclude</span> <span class="err">these</span> <span class="err">(also</span> <span class="err">from</span> <span class="err">transitive</span> <span class="err">dependencies)</span> <span class="err">since</span> <span class="err">they</span> <span class="err">are</span> <span class="err">provided</span> <span class="err">by</span> <span class="err">Flink</span>
    <span class="err">flinkShadowJar.exclude</span> <span class="err">group:</span> <span class="err">&#39;org.apache.flink&#39;,</span> <span class="err">module:</span> <span class="err">&#39;force-shading&#39;</span>
    <span class="err">flinkShadowJar.exclude</span> <span class="err">group:</span> <span class="err">&#39;com.google.code.findbugs&#39;,</span> <span class="err">module:</span> <span class="err">&#39;jsr305&#39;</span>
    <span class="err">flinkShadowJar.exclude</span> <span class="err">group:</span> <span class="err">&#39;org.slf4j&#39;</span>
    <span class="err">flinkShadowJar.exclude</span> <span class="err">group:</span> <span class="err">&#39;org.apache.logging.log4j&#39;</span>
<span class="p">}</span>

<span class="err">//</span> <span class="err">declare</span> <span class="err">the</span> <span class="err">dependencies</span> <span class="err">for</span> <span class="err">your</span> <span class="err">production</span> <span class="err">and</span> <span class="err">test</span> <span class="err">code</span>
<span class="err">dependencies</span> <span class="p">{</span>
    <span class="err">//</span> <span class="err">--------------------------------------------------------------</span>
    <span class="err">//</span> <span class="err">Compile-time</span> <span class="err">dependencies</span> <span class="err">that</span> <span class="err">should</span> <span class="err">NOT</span> <span class="err">be</span> <span class="err">part</span> <span class="err">of</span> <span class="err">the</span>
    <span class="err">//</span> <span class="err">shadow</span> <span class="err">jar</span> <span class="err">and</span> <span class="err">are</span> <span class="err">provided</span> <span class="err">in</span> <span class="err">the</span> <span class="err">lib</span> <span class="err">folder</span> <span class="err">of</span> <span class="err">Flink</span>
    <span class="err">//</span> <span class="err">--------------------------------------------------------------</span>
    <span class="err">compile</span> <span class="nt">&#34;org.apache.flink:flink-streaming-java_${scalaBinaryVersion}:${flinkVersion}&#34;</span>

    <span class="err">//</span> <span class="err">--------------------------------------------------------------</span>
    <span class="err">//</span> <span class="err">Dependencies</span> <span class="err">that</span> <span class="err">should</span> <span class="err">be</span> <span class="err">part</span> <span class="err">of</span> <span class="err">the</span> <span class="err">shadow</span> <span class="err">jar</span><span class="p">,</span> <span class="err">e.g.</span>
    <span class="err">//</span> <span class="err">connectors.</span> <span class="err">These</span> <span class="err">must</span> <span class="err">be</span> <span class="err">in</span> <span class="err">the</span> <span class="err">flinkShadowJar</span> <span class="err">configuration!</span>
    <span class="err">//</span> <span class="err">--------------------------------------------------------------</span>
    <span class="err">//flinkShadowJar</span> <span class="nt">&#34;org.apache.flink:flink-connector-kafka-0.11_${scalaBinaryVersion}:${flinkVersion}&#34;</span>

    <span class="err">compile</span> <span class="s2">&#34;org.apache.logging.log4j:log4j-api:${log4jVersion}&#34;</span>
    <span class="err">compile</span> <span class="s2">&#34;org.apache.logging.log4j:log4j-core:${log4jVersion}&#34;</span>
    <span class="err">compile</span> <span class="s2">&#34;org.apache.logging.log4j:log4j-slf4j-impl:${log4jVersion}&#34;</span>
    <span class="err">compile</span> <span class="s2">&#34;org.slf4j:slf4j-log4j12:${slf4jVersion}&#34;</span>

    <span class="err">//</span> <span class="err">Add</span> <span class="err">test</span> <span class="err">dependencies</span> <span class="err">here.</span>
    <span class="err">//</span> <span class="err">testCompile</span> <span class="s2">&#34;junit:junit:4.12&#34;</span>
<span class="p">}</span>

<span class="err">//</span> <span class="err">make</span> <span class="err">compileOnly</span> <span class="err">dependencies</span> <span class="err">available</span> <span class="err">for</span> <span class="err">tests:</span>
<span class="err">sourceSets</span> <span class="p">{</span>
    <span class="err">main.compileClasspath</span> <span class="err">+=</span> <span class="err">configurations.flinkShadowJar</span>
    <span class="err">main.runtimeClasspath</span> <span class="err">+=</span> <span class="err">configurations.flinkShadowJar</span>

    <span class="err">test.compileClasspath</span> <span class="err">+=</span> <span class="err">configurations.flinkShadowJar</span>
    <span class="err">test.runtimeClasspath</span> <span class="err">+=</span> <span class="err">configurations.flinkShadowJar</span>

    <span class="err">javadoc.classpath</span> <span class="err">+=</span> <span class="err">configurations.flinkShadowJar</span>
<span class="p">}</span>

<span class="err">run.classpath</span> <span class="err">=</span> <span class="err">sourceSets.main.runtimeClasspath</span>

<span class="err">jar</span> <span class="p">{</span>
    <span class="err">manifest</span> <span class="err">{</span>
        <span class="err">attributes</span> <span class="err">&#39;Built-By&#39;:</span> <span class="err">System.getProperty(&#39;user.name&#39;),</span>
                <span class="err">&#39;Build-Jdk&#39;:</span> <span class="err">System.getProperty(&#39;java.version&#39;)</span>
    <span class="p">}</span>
<span class="err">}</span>

<span class="err">shadowJar</span> <span class="p">{</span>
    <span class="err">configurations</span> <span class="err">=</span> <span class="err">[project.configurations.flinkShadowJar]</span>
<span class="p">}</span>
</code></pre></div><p><strong>setting.gradle</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="err">rootProject.name</span> <span class="err">=</span> <span class="err">&#39;quickstart&#39;</span>
</code></pre></div><p>这允许你为你新创建的项目命名，它将交互式地询问你项目的名称、组织（也用于包名）、项目版本、Scala 和 Flink。它将交互式地要求你提供项目名称、组织（也用于包名）、项目版本、Scala 和 Flink 版本。</p>
<ul>
<li>运行快速启动脚本</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">bash -c <span class="s2">&#34;</span><span class="k">$(</span>curl https://flink.apache.org/q/gradle-quickstart.sh<span class="k">)</span><span class="s2">&#34;</span> -- 1.11.0 2.11
</code></pre></div><p>我们建议你将这个项目导入到你的 IDE 中进行开发和测试。IntelliJ IDEA 在安装 Gradle 插件后，支持 Gradle 项目。Eclipse 通过 <a href="https://projects.eclipse.org/projects/tools.buildship">Eclipse Buildship</a> 插件来实现（确保在导入向导的最后一步指定 Gradle 版本&gt;=3.0，影子插件需要它）。你也可以使用 <a href="https://docs.gradle.org/current/userguide/userguide.html#ide-integration">Gradle 的 IDE 集成</a>来从 Gradle 创建项目文件。</p>
<p>请注意：Java 默认的 JVM 堆大小对 Flink 来说可能太小。你必须手动增加它。在 Eclipse 中，选择 Run Configurations -&gt; Arguments，并在 VM Arguments 框中写下 <code>-Xmx800m</code>。在 IntelliJ IDEA 中推荐的改变 JVM 选项的方法是来自 Help | Edit Custom VM Options 菜单。详情请看<a href="https://intellij-support.jetbrains.com/hc/en-us/articles/206544869-Configuring-JVM-options-and-platform-properties">这篇文章</a>。</p>
<h4 id="构建项目-1">构建项目</h4>
<p>如果你想构建/打包你的项目，去你的项目目录下运行 &ldquo;gradle clean shadowJar&rdquo; 命令，你会发现一个 JAR 文件，其中包含了你的应用程序，以及你可能已经添加到应用程序中作为依赖的连接器和库：<code>build/libs/&lt;project-name&gt;-&lt;version&gt;-all.jar</code>。</p>
<p>注意：如果你使用与 StreamingJob 不同的类作为应用程序的主类/入口点，我们建议你相应地更改 build.gradle 文件中的 mainClassName 设置。这样，Flink 就可以从 JAR 文件中运行应用程序，而无需额外指定主类。</p>
<h3 id="sbt">SBT</h3>
<h4 id="创建项目">创建项目</h4>
<p>您可以通过以下两种方法中的任何一种来构建一个新项目。</p>
<ul>
<li>使用 sbt 模板</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ sbt new tillrohrmann/flink-project.g8
</code></pre></div><ul>
<li>运行快速启动脚本</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ bash &lt;<span class="o">(</span>curl https://flink.apache.org/q/sbt-quickstart.sh<span class="o">)</span>
</code></pre></div><p>这将在指定的项目目录下创建一个 Flink 项目。</p>
<h4 id="构建项目-2">构建项目</h4>
<p>为了建立你的项目，你只需要发出 sbt clean assembly 命令。这将在 <code>target/scala_your-major-scala-version/</code> 目录下创建 fat-jar <code>your-project-name-assembly-0.1-SNAPSHOT.jar</code>。</p>
<p><strong>运行项目</strong></p>
<p>为了运行你的项目，你必须发出 sbt 运行命令。</p>
<p>默认情况下，这将在 sbt 运行的同一个 JVM 中运行你的工作。为了在不同的 JVM 中运行你的工作，请在 build.sbt 中添加以下行。</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">fork in run :<span class="o">=</span> <span class="nb">true</span>
</code></pre></div><h4 id="intellij">IntelliJ</h4>
<p>我们推荐您使用 <a href="https://www.jetbrains.com/idea/">IntelliJ</a> 进行 Flink 作业开发。为了开始，您必须将新创建的项目导入到 IntelliJ 中。您可以通过 File -&gt; New -&gt; Project from Existing Sources&hellip;然后选择您的项目目录。IntelliJ 会自动检测 build.sbt 文件，并设置好一切。</p>
<p>为了运行 Flink 作业，建议选择 mainRunner 模块作为运行/调试配置的 classpath。这将确保所有被设置为提供的依赖关系在执行时都是可用的。您可以通过 Run -&gt; Edit Configurations&hellip;配置 Run/Debug 配置，然后从 Use classpath of module dropbox 中选择 mainRunner。</p>
<h4 id="eclipse">Eclipse</h4>
<p>为了将新创建的项目导入到 <a href="https://eclipse.org/">Eclipse</a> 中，首先必须为其创建 Eclipse 项目文件。这些项目文件可以通过  <a href="https://github.com/typesafehub/sbteclipse">sbteclipse</a> 插件来创建。在 PROJECT_DIR/project/plugins.sbt 文件中添加以下一行。</p>
<pre><code>addSbtPlugin(&quot;com.typeafe.sbteclipse&quot; % &quot;sbteclipse-plugin&quot; % &quot;4.0.0&quot;)
</code></pre><p>在 sbt 中使用下面的命令来创建 Eclipse 项目文件</p>
<pre><code>&gt; eclipse
</code></pre><p>现在你可以通过 File-&gt;Import&hellip;-&gt;Existing Projects into Workspace 导入 Eclipse，然后选择项目目录。</p>
<h2 id="附录-用依赖关系构建-jar-的模板">附录: 用依赖关系构建 Jar 的模板</h2>
<p>要构建一个包含声明的连接器和库所需的所有依赖关系的应用程序 JAR，可以使用以下 shade 插件定义。</p>
<div class="highlight"><pre class="chroma"><code class="language-xml" data-lang="xml"><span class="nt">&lt;build&gt;</span>
    <span class="nt">&lt;plugins&gt;</span>
        <span class="nt">&lt;plugin&gt;</span>
            <span class="nt">&lt;groupId&gt;</span>org.apache.maven.plugins<span class="nt">&lt;/groupId&gt;</span>
            <span class="nt">&lt;artifactId&gt;</span>maven-shade-plugin<span class="nt">&lt;/artifactId&gt;</span>
            <span class="nt">&lt;version&gt;</span>3.1.1<span class="nt">&lt;/version&gt;</span>
            <span class="nt">&lt;executions&gt;</span>
                <span class="nt">&lt;execution&gt;</span>
                    <span class="nt">&lt;phase&gt;</span>package<span class="nt">&lt;/phase&gt;</span>
                    <span class="nt">&lt;goals&gt;</span>
                        <span class="nt">&lt;goal&gt;</span>shade<span class="nt">&lt;/goal&gt;</span>
                    <span class="nt">&lt;/goals&gt;</span>
                    <span class="nt">&lt;configuration&gt;</span>
                        <span class="nt">&lt;artifactSet&gt;</span>
                            <span class="nt">&lt;excludes&gt;</span>
                                <span class="nt">&lt;exclude&gt;</span>com.google.code.findbugs:jsr305<span class="nt">&lt;/exclude&gt;</span>
                                <span class="nt">&lt;exclude&gt;</span>org.slf4j:*<span class="nt">&lt;/exclude&gt;</span>
                                <span class="nt">&lt;exclude&gt;</span>log4j:*<span class="nt">&lt;/exclude&gt;</span>
                            <span class="nt">&lt;/excludes&gt;</span>
                        <span class="nt">&lt;/artifactSet&gt;</span>
                        <span class="nt">&lt;filters&gt;</span>
                            <span class="nt">&lt;filter&gt;</span>
                                <span class="c">&lt;!-- Do not copy the signatures in the META-INF folder.
</span><span class="c">                                Otherwise, this might cause SecurityExceptions when using the JAR. --&gt;</span>
                                <span class="nt">&lt;artifact&gt;</span>*:*<span class="nt">&lt;/artifact&gt;</span>
                                <span class="nt">&lt;excludes&gt;</span>
                                    <span class="nt">&lt;exclude&gt;</span>META-INF/*.SF<span class="nt">&lt;/exclude&gt;</span>
                                    <span class="nt">&lt;exclude&gt;</span>META-INF/*.DSA<span class="nt">&lt;/exclude&gt;</span>
                                    <span class="nt">&lt;exclude&gt;</span>META-INF/*.RSA<span class="nt">&lt;/exclude&gt;</span>
                                <span class="nt">&lt;/excludes&gt;</span>
                            <span class="nt">&lt;/filter&gt;</span>
                        <span class="nt">&lt;/filters&gt;</span>
                        <span class="nt">&lt;transformers&gt;</span>
                            <span class="nt">&lt;transformer</span> <span class="na">implementation=</span><span class="s">&#34;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&#34;</span><span class="nt">&gt;</span>
                                <span class="nt">&lt;mainClass&gt;</span>my.programs.main.clazz<span class="nt">&lt;/mainClass&gt;</span>
                            <span class="nt">&lt;/transformer&gt;</span>
                        <span class="nt">&lt;/transformers&gt;</span>
                    <span class="nt">&lt;/configuration&gt;</span>
                <span class="nt">&lt;/execution&gt;</span>
            <span class="nt">&lt;/executions&gt;</span>
        <span class="nt">&lt;/plugin&gt;</span>
    <span class="nt">&lt;/plugins&gt;</span>
<span class="nt">&lt;/build&gt;</span>
</code></pre></div><p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/project-configuration.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[使用状态]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-event-time/?utm_source=atom_feed" rel="related" type="text/html" title="Event Time" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-flink-datastream-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Datastream API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-generating-watermarks/?utm_source=atom_feed" rel="related" type="text/html" title="Generating Watermarks" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-built-in-watermark-generators/?utm_source=atom_feed" rel="related" type="text/html" title="内置的水印生成器" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/?utm_source=atom_feed" rel="related" type="text/html" title="状态和容错性" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-21T00:00:00+08:00</published>
            <updated>2020-08-21T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Working With State</blockquote><h2 id="使用状态httpsciapacheorgprojectsflinkflink-docs-release-111devstreamstatestatehtml"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">使用状态</a></h2>
<p>在本节中，您将了解 Flink 为编写有状态程序提供的 API。请看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/concepts/stateful-stream-processing.html">Stateful Stream Processing</a> 来了解有状态流处理背后的概念。</p>
<h3 id="keyed-datastream">Keyed DataStream</h3>
<p>如果要使用 keyed state，首先需要在 DataStream 上指定一个键，这个键应该用来分隔(partition)状态（也包括流中的记录本身）。你可以在 DataStream 上使用 <code>keyBy(KeySelector)</code> 指定一个键。这将产生一个 <code>KeyedDataStream</code>，然后允许使用 keyed state 的操作。</p>
<p>键选择函数将一条记录作为输入，并返回该记录的键。键可以是任何类型的，并且必须从确定性计算中导出。</p>
<p>Flink 的数据模型不是基于键值对的。因此，您不需要将数据集类型物理地打包成键和值。键是&quot;虚拟&quot;的：它们被定义为实际数据上的函数，以指导分组操作符。</p>
<p>下面的例子显示了一个键选择函数，它只是返回对象的字段。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="c1">// 普通的 case 类
</span><span class="c1"></span><span class="k">case</span> <span class="k">class</span> <span class="nc">WC</span><span class="o">(</span><span class="n">word</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">WC</span><span class="o">]</span> <span class="k">=</span> <span class="c1">// [...]
</span><span class="c1"></span><span class="k">val</span> <span class="n">keyed</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">keyBy</span><span class="o">(</span> <span class="k">_</span><span class="o">.</span><span class="n">word</span> <span class="o">)</span>
</code></pre></div><h4 id="元组键和表达式键">元组键和表达式键</h4>
<p>Flink 还有两种定义键的方法：元组键和表达式键。有了它，你可以使用元组字段索引或表达式来指定键，用于选择对象的字段。我们今天不推荐使用这些，但你可以参考 DataStream 的 Javadoc 来了解它们。严格来说，使用 <code>KeySelector</code> 函数更胜一筹：使用 Java lambdas，它们很容易使用，而且它们在运行时的开销可能更少。</p>
<h3 id="使用-keyed-state">使用 Keyed State</h3>
<p>keyed State 接口提供了对不同类型的状态的访问，这些状态的作用域都是当前输入元素的键。这意味着，这种类型的状态只能在 <code>KeyedStream</code> 上使用，它可以通过 <code>stream.keyBy(...)</code> 来创建。</p>
<p>现在，我们将首先看看不同类型的状态有哪些，然后我们会看看如何在程序中使用它们。可用的状态原语有:</p>
<ul>
<li>
<p><code>ValueState&lt;T&gt;</code>：它保留了一个可更新和检索的值（如上所述，作用域为输入元素的键，因此操作符所看到的每个键都可能有一个值）。这个值可以使用 <code>update(T)</code> 来设置，也可以使用 <code>T value()</code> 来检索。</p>
</li>
<li>
<p><code>ListState&lt;T&gt;</code>：这保存了一个元素列表。你可以在所有当前存储的元素上追加元素和检索一个 <code>Iterable</code>。使用 <code>add(T)</code> 或 <code>addAll(List&lt;T&gt;)</code> 添加元素，可以使用 <code>Iterable&lt;T&gt; get()</code> 检索 <code>Iterable</code>。你也可以用 <code>update(List&lt;T&gt;)</code> 覆盖现有的列表。</p>
</li>
<li>
<p><code>ReducingState&lt;T&gt;</code>: 这保留了一个单一的值，代表所有添加到状态的值的集合。该接口类似于 <code>ListState</code>，但使用 <code>add(T)</code> 添加的元素会使用指定的 <code>ReduceFunction</code> 被化简成一个总计。</p>
</li>
<li>
<p><code>AggregatingState&lt;IN，OUT&gt;</code>：这保留了一个单一的值，代表所有添加到状态的值的聚合。与 <code>ReducingState</code> 相反，<code>aggregate</code> 类型可能与添加到状态中的元素类型不同。接口与 <code>ListState</code> 相同，但使用 <code>add(IN)</code> 添加的元素会使用指定的 <code>AggregateFunction</code> 进行聚合。</p>
</li>
<li>
<p><code>MapState&lt;UK, UV&gt;</code>: 它保存了一个映射列表。你可以将键值对放入状态中，并在所有当前存储的映射上检索一个 <code>Iterable</code>。使用 <code>put(UK, UV)</code> 或 <code>putAll(Map&lt;UK, UV&gt;)</code> 可以添加映射。与用户键相关联的值可以使用 <code>get(UK)</code> 来检索。可以分别使用 <code>entries()</code>、<code>keys()</code> 和 <code>values()</code> 检索映射、键和值的可迭代视图。你也可以使用 <code>isEmpty()</code> 来检查这个映射是否包含任何键值映射。</p>
</li>
</ul>
<p>所有类型的状态也都有一个方法 <code>clear()</code>，可以清除当前活动键的状态，也就是输入元素的键。</p>
<p>需要注意的是，这些状态对象只用于带状态的接口。状态不一定存储在里面，而可能驻留在磁盘或其他地方。第二件要记住的事情是，你从状态中得到的值取决于输入元素的键。因此，如果所涉及的键不同，你在用户函数的一次调用中得到的值可能与另一次调用中的值不同。</p>
<p>为了得到一个状态句柄，你必须创建一个 <code>StateDescriptor</code>。这里面包含了状态的名称(我们稍后会看到，你可以创建多个状态，而且它们必须有独特的名称，这样你才能引用它们)，状态所拥有的值的类型，可能还有一个用户指定的函数，比如 <code>ReduceFunction</code>。根据你要检索的状态类型，你可以创建一个 <code>ValueStateDescriptor</code>、一个 <code>ListStateDescriptor</code>、一个 <code>ReducingStateDescriptor</code> 或一个 <code>MapStateDescriptor</code>。</p>
<p>状态是使用 <code>RuntimeContext</code> 访问的，所以只有在富函数(<em>rich functions</em>)中才有可能。请看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html#rich-functions">这里</a>了解相关信息，但我们也会很快看到一个例子。<code>RichFunction</code> 中可用的 <code>RuntimeContext</code> 有这些方法来访问状态。</p>
<ul>
<li>ValueState<!-- raw HTML omitted --> getState(ValueStateDescriptor<!-- raw HTML omitted -->)</li>
<li>ReducingState<!-- raw HTML omitted --> getReducingState(ReducingStateDescriptor<!-- raw HTML omitted -->)</li>
<li>ListState<!-- raw HTML omitted --> getListState(ListStateDescriptor<!-- raw HTML omitted -->)</li>
<li>AggregatingState&lt;IN, OUT&gt; getAggregatingState(AggregatingStateDescriptor&lt;IN, ACC, OUT&gt;)</li>
<li>MapState&lt;UK, UV&gt; getMapState(MapStateDescriptor&lt;UK, UV&gt;)</li>
</ul>
<p>这是一个 <code>FlatMapFunction</code> 的例子，它展示了所有的部分是如何结合在一起的。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">CountWindowAverage</span> <span class="k">extends</span> <span class="nc">RichFlatMapFunction</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)</span>, <span class="o">(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">var</span> <span class="n">sum</span><span class="k">:</span> <span class="kt">ValueState</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)]</span> <span class="k">=</span> <span class="k">_</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">flatMap</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Long</span><span class="o">),</span> <span class="n">out</span><span class="k">:</span> <span class="kt">Collector</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>

    <span class="c1">// 访问状态值
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">tmpCurrentSum</span> <span class="k">=</span> <span class="n">sum</span><span class="o">.</span><span class="n">value</span>

    <span class="c1">// 如果之前没有使用过，则为 null。
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">currentSum</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">tmpCurrentSum</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">tmpCurrentSum</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="o">(</span><span class="mi">0L</span><span class="o">,</span> <span class="mi">0L</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="c1">// 更新次数
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">newSum</span> <span class="k">=</span> <span class="o">(</span><span class="n">currentSum</span><span class="o">.</span><span class="n">_1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="n">currentSum</span><span class="o">.</span><span class="n">_2</span> <span class="o">+</span> <span class="n">input</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>

    <span class="c1">// 更新状态
</span><span class="c1"></span>    <span class="n">sum</span><span class="o">.</span><span class="n">update</span><span class="o">(</span><span class="n">newSum</span><span class="o">)</span>

    <span class="c1">// 如果计数达到2，则发出平均数，并清除状态。
</span><span class="c1"></span>    <span class="k">if</span> <span class="o">(</span><span class="n">newSum</span><span class="o">.</span><span class="n">_1</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">out</span><span class="o">.</span><span class="n">collect</span><span class="o">((</span><span class="n">input</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">newSum</span><span class="o">.</span><span class="n">_2</span> <span class="o">/</span> <span class="n">newSum</span><span class="o">.</span><span class="n">_1</span><span class="o">))</span>
      <span class="n">sum</span><span class="o">.</span><span class="n">clear</span><span class="o">()</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">parameters</span><span class="k">:</span> <span class="kt">Configuration</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">sum</span> <span class="k">=</span> <span class="n">getRuntimeContext</span><span class="o">.</span><span class="n">getState</span><span class="o">(</span>
      <span class="k">new</span> <span class="nc">ValueStateDescriptor</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)](</span><span class="s">&#34;average&#34;</span><span class="o">,</span> <span class="n">createTypeInformation</span><span class="o">[(</span><span class="kt">Long</span>, <span class="kt">Long</span><span class="o">)])</span>
    <span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>


<span class="k">object</span> <span class="nc">ExampleCountWindowAverage</span> <span class="k">extends</span> <span class="nc">App</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">env</span> <span class="k">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="n">getExecutionEnvironment</span>

  <span class="n">env</span><span class="o">.</span><span class="n">fromCollection</span><span class="o">(</span><span class="nc">List</span><span class="o">(</span>
    <span class="o">(</span><span class="mi">1L</span><span class="o">,</span> <span class="mi">3L</span><span class="o">),</span>
    <span class="o">(</span><span class="mi">1L</span><span class="o">,</span> <span class="mi">5L</span><span class="o">),</span>
    <span class="o">(</span><span class="mi">1L</span><span class="o">,</span> <span class="mi">7L</span><span class="o">),</span>
    <span class="o">(</span><span class="mi">1L</span><span class="o">,</span> <span class="mi">4L</span><span class="o">),</span>
    <span class="o">(</span><span class="mi">1L</span><span class="o">,</span> <span class="mi">2L</span><span class="o">)</span>
  <span class="o">)).</span><span class="n">keyBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
    <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="nc">CountWindowAverage</span><span class="o">())</span>
    <span class="o">.</span><span class="n">print</span><span class="o">()</span>
  <span class="c1">// the printed output will be (1,4) and (1,5)
</span><span class="c1"></span>
  <span class="n">env</span><span class="o">.</span><span class="n">execute</span><span class="o">(</span><span class="s">&#34;ExampleKeyedState&#34;</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div><p>这个例子实现了一个穷人的计数窗口。我们用第一个字段对元组进行 keyed 操作（在本例中，所有元组都有相同的键 <code>1</code>）。该函数将计数和运行的总和存储在一个 <code>ValueState</code>  中。一旦计数达到 2，它就会发出平均数并清除状态，这样我们就可以从 0 开始。注意，如果我们在第一个字段中的元组具有不同的值，那么这将为每个不同的输入键保持不同的状态值。</p>
<h4 id="状态存活时间ttl">状态存活时间(TTL)</h4>
<p>可以为任何类型的 keyed state 分配一个生存时间（TTL）。如果配置了 TTL，并且状态值已经过期，存储的值将在尽力的基础上进行清理，这将在下面详细讨论。</p>
<p>所有状态集合类型都支持每个条目的 TTL。这意味着列表元素和映射条目独立过期。</p>
<p>为了使用状态 TTL，必须首先建立一个 <code>StateTtlConfig</code> 配置对象。然后可以通过传递配置在任何状态描述符中启用 TTL 功能。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.StateTtlConfig</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.ValueStateDescriptor</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.common.time.Time</span>

<span class="k">val</span> <span class="n">ttlConfig</span> <span class="k">=</span> <span class="nc">StateTtlConfig</span>
    <span class="o">.</span><span class="n">newBuilder</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">setUpdateType</span><span class="o">(</span><span class="nc">StateTtlConfig</span><span class="o">.</span><span class="nc">UpdateType</span><span class="o">.</span><span class="nc">OnCreateAndWrite</span><span class="o">)</span>
    <span class="o">.</span><span class="n">setStateVisibility</span><span class="o">(</span><span class="nc">StateTtlConfig</span><span class="o">.</span><span class="nc">StateVisibility</span><span class="o">.</span><span class="nc">NeverReturnExpired</span><span class="o">)</span>
    <span class="o">.</span><span class="n">build</span>
    
<span class="k">val</span> <span class="n">stateDescriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ValueStateDescriptor</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">&#34;text state&#34;</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span>
<span class="n">stateDescriptor</span><span class="o">.</span><span class="n">enableTimeToLive</span><span class="o">(</span><span class="n">ttlConfig</span><span class="o">)</span>
</code></pre></div><p>配置有几个选项需要考虑。</p>
<p><code>newBuilder</code> 方法的第一个参数是强制性的，它是存活的时间值。</p>
<p>更新类型配置状态 TTL 何时被刷新（默认为 <code>OnCreateAndWrite</code>）。</p>
<ul>
<li>StateTtlConfig.UpdateType.OnCreateAndWrite - 仅在创建和写入访问时才会出现</li>
<li>StateTtlConfig.UpdateType.OnReadAndWrite - 也是在读的时候。</li>
</ul>
<p>状态可见性配置如果过期值尚未清理，是否在读取访问时返回（默认为 <code>NeverReturnExpired</code>）。</p>
<ul>
<li>StateTtlConfig.StateVisibility.NeverReturnExpired - 过期值永不返回</li>
<li>StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp - 如果仍然可用则返回。</li>
</ul>
<p>在 <code>NeverReturnExpired</code> 的情况下，过期状态就像不存在一样，即使它仍然必须被删除。这个选项对于数据在 TTL 之后必须严格地成为不可读的访问状态的用例是很有用的，例如处理隐私敏感数据的应用程序。</p>
<p>另一个选项 <code>ReturnExpiredIfNotCleanedUp</code> 允许在清理之前返回过期状态。</p>
<p>注意:</p>
<ul>
<li>
<p>状态后端存储最后一次修改的时间戳和用户值，这意味着启用该功能会增加状态存储的消耗。Heap 状态后端在内存中存储了一个额外的 Java 对象，该对象有一个对用户状态对象的引用和一个原始的长值。RocksDB 状态后端每存储一个值、列表项或映射项增加8个字节。</p>
</li>
<li>
<p>目前只支持参考处理时间的 TTL。</p>
</li>
<li>
<p>试图使用启用 TTL 的描述符来恢复之前没有配置 TTL 的状态，或者反之，将导致兼容性失败和 <code>StateMigrationException</code>。</p>
</li>
<li>
<p>TTL 配置不是检查点或保存点的一部分，而是 Flink 在当前运行的作业中如何处理的一种方式。</p>
</li>
<li>
<p>带 TTL 的映射状态目前只有在用户值序列化器能够处理 null 值的情况下才支持 null 用户值。如果序列化器不支持空值，可以用 <code>NullableSerializer</code> 包装，代价是在序列化形式中多出一个字节。</p>
</li>
</ul>
<h4 id="过期状态的清理">过期状态的清理</h4>
<p>默认情况下，过期的值会在读取时显式删除，如 <code>ValueState#value</code>，如果配置的状态后台支持，则会定期在后台进行垃圾回收。后台清理可以在 <code>StateTtlConfig</code> 中禁用。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.StateTtlConfig</span>
<span class="k">val</span> <span class="n">ttlConfig</span> <span class="k">=</span> <span class="nc">StateTtlConfig</span>
    <span class="o">.</span><span class="n">newBuilder</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">disableCleanupInBackground</span>
    <span class="o">.</span><span class="n">build</span>
</code></pre></div><p>如果想对后台的一些特殊清理进行更精细的控制，可以按照下面的描述单独配置。目前，堆状态后台依靠增量清理，RocksDB 后台使用压实过滤器进行后台清理。</p>
<h5 id="全快照中的清理">全快照中的清理</h5>
<p>此外，您可以在拍摄完整状态快照的瞬间激活清理，这将减少其大小。在当前的实现下，本地状态不会被清理，但在从上一个快照恢复的情况下，它将不包括删除的过期状态。可以在 <code>StateTtlConfig</code> 中进行配置。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.StateTtlConfig</span>
<span class="k">import</span> <span class="nn">org.apache.flink.api.common.time.Time</span>

<span class="k">val</span> <span class="n">ttlConfig</span> <span class="k">=</span> <span class="nc">StateTtlConfig</span>
    <span class="o">.</span><span class="n">newBuilder</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">cleanupFullSnapshot</span>
    <span class="o">.</span><span class="n">build</span>
</code></pre></div><p>此选项不适用于 RocksDB 状态后端的增量检查点。</p>
<p><strong>注意:</strong></p>
<ul>
<li>对于现有的作业，这个清理策略可以在 <code>StateTtlConfig</code> 中随时激活或停用，例如从保存点重新启动后。</li>
</ul>
<h5 id="增量清理">增量清理</h5>
<p>另一种选择是逐步触发一些状态条目的清理。触发器可以是每次状态访问或/和每次记录处理的回调。如果这种清理策略对某些状态是激活的，存储后端就会为这个状态的所有条目保留一个惰性的全局迭代器。每次触发增量清理时，迭代器都会被提前。对遍历过的状态条目进行检查，对过期的条目进行清理。</p>
<p>这个功能可以在 <code>StateTtlConfig</code> 中配置。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.StateTtlConfig</span>
<span class="k">val</span> <span class="n">ttlConfig</span> <span class="k">=</span> <span class="nc">StateTtlConfig</span>
    <span class="o">.</span><span class="n">newBuilder</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">cleanupIncrementally</span><span class="o">(</span><span class="mi">10</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
    <span class="o">.</span><span class="n">build</span>
</code></pre></div><p>这个策略有两个参数。第一个是每次清理触发的检查状态条目数。它总是在每次状态访问时触发。第二个参数定义是否在每次记录处理中额外触发清理。堆后端默认的后台清理每次记录处理检查5个条目而不进行清理。</p>
<p><strong>注意:</strong></p>
<ul>
<li>如果没有发生对状态的访问或者没有处理记录，过期状态将持续存在。</li>
<li>增量清理所花费的时间会增加记录处理的延迟。</li>
<li>目前，增量清理只在堆状态后端实现。对 RocksDB 的设置不会有影响。</li>
<li>如果堆状态后端与同步快照一起使用，全局迭代器在迭代的时候会保留所有键的副本，因为它的具体实现不支持并发修改。那么启用这个功能会增加内存消耗。异步快照则不存在这个问题。</li>
<li>对于现有的作业，这个清理策略可以在 <code>StateTtlConfig</code> 中随时激活或停用，例如从保存点重新启动后。</li>
</ul>
<h5 id="rocksdb-压缩过程中的清理">RocksDB 压缩过程中的清理</h5>
<p>如果使用 RocksDB 状态后端，将调用 Flink 特定的压实过滤器进行后台清理。RocksDB  会定期运行异步压实来合并状态更新，减少存储量。Flink 压实过滤器通过 TTL 检查状态条目的过期时间戳，排除过期值。</p>
<p>这个功能可以在 <code>StateTtlConfig</code> 中配置。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.state.StateTtlConfig</span>

<span class="k">val</span> <span class="n">ttlConfig</span> <span class="k">=</span> <span class="nc">StateTtlConfig</span>
    <span class="o">.</span><span class="n">newBuilder</span><span class="o">(</span><span class="nc">Time</span><span class="o">.</span><span class="n">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
    <span class="o">.</span><span class="n">cleanupInRocksdbCompactFilter</span><span class="o">(</span><span class="mi">1000</span><span class="o">)</span>
    <span class="o">.</span><span class="n">build</span>
</code></pre></div><p>RocksDB 压实过滤器在处理一定数量的状态条目后，每次都会从 Flink 中查询当前的时间戳，用于检查过期情况，你可以改变它，并传递自定义值给 <code>StateTtlConfig.newBuilder(...).cleanupInRocksdbCompactFilter(long queryTimeAfterNumEntries)</code> 方法。更频繁地更新时间戳可以提高清理速度，但由于它使用了来自本地代码的 JNI 调用，因此降低了压缩性能。RocksDB 后台默认的清理方式是每次处理1000个条目后查询当前时间戳。</p>
<p>你可以通过激活 <code>FlinkCompactionFilter</code> 的调试级别来激活 RocksDB 过滤器原生代码的调试日志。</p>
<pre><code>log4j.logger.org.rocksdb.FlinkCompactionFilter=DEBUG
</code></pre><p><strong>注意:</strong></p>
<ul>
<li>在压实过程中调用 TTL 过滤器会使其速度减慢。TTL 过滤器必须解析最后一次访问的时间戳，并检查每个被压缩的键的存储状态条目的到期时间。如果是集合状态类型(list 或 map)，每个存储元素的检查也会被调用。</li>
<li>如果该功能用于具有非固定字节长度元素的列表状态，则原生 TTL 过滤器必须额外调用每个至少第一个元素已过期的状态条目中元素在 JNI 上的 Flink java 类型序列化器，以确定下一个未过期元素的偏移。</li>
<li>对于现有的作业，这种清理策略可以在 <code>StateTtlConfig</code> 中随时激活或停用，例如从保存点重新启动后。</li>
</ul>
<h3 id="scala-datastream-api-中的状态">Scala DataStream API 中的状态</h3>
<p>除了上面描述的接口外，Scala API 还为 KeyedStream 上具有单个 <code>ValueState</code> 的有状态 <code>map()</code> 或 <code>flatMap()</code> 函数提供了快捷方式。用户函数在 <code>Option</code> 中获取 <code>ValueState</code> 的当前值，并且必须返回一个更新的值，该值将用于更新状态。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="n">counts</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="n">stream</span>
  <span class="o">.</span><span class="n">keyBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
  <span class="o">.</span><span class="n">mapWithState</span><span class="o">((</span><span class="n">in</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">),</span> <span class="n">count</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span> <span class="k">=&gt;</span>
    <span class="n">count</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="nc">Some</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span> <span class="o">(</span><span class="n">in</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">c</span><span class="o">),</span> <span class="nc">Some</span><span class="o">(</span><span class="n">c</span> <span class="o">+</span> <span class="n">in</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">)</span>
      <span class="k">case</span> <span class="nc">None</span> <span class="k">=&gt;</span> <span class="o">(</span> <span class="o">(</span><span class="n">in</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="mi">0</span><span class="o">),</span> <span class="nc">Some</span><span class="o">(</span><span class="n">in</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span> <span class="o">)</span>
    <span class="o">})</span>
</code></pre></div><h3 id="operator-state">Operator State</h3>
<p>Operator State（或 non-keyed state）是指绑定到一个并行操作符实例的状态。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/kafka.html">Kafka 连接器</a>是 Flink 中使用 Operator State 的一个很好的激励例子。Kafka 消费者的每个并行实例都维护着一个主题分区和偏移的映射作为其 Operator State。</p>
<p>Operator State 接口支持在并行操作符实例之间重新分配状态，当并行性发生变化时。有不同的方案来进行这种重新分配。</p>
<p>在典型的有状态的 Flink 应用中，你不需要操作符状态。它主要是一种特殊类型的状态，用于源/接收器实现和你没有键的情况下，可以通过它来分隔状态。</p>
<h3 id="广播状态">广播状态</h3>
<p>Broadcast State 是 Operator State 的一种特殊类型。引入它是为了支持这样的用例：一个流的记录(records)需要被广播到所有下游任务，它们被用来在所有子任务中保持相同的状态。然后在处理第二个流的记录时可以访问这个状态。作为一个广播状态可以自然出现的例子，我们可以想象一个低吞吐量的流，其中包含一组规则，我们希望对来自另一个流的所有元素进行评估。考虑到上述类型的用例，广播状态与其余运算符状态的不同之处在于。</p>
<ul>
<li>它有一个 map 格式。</li>
<li>它只适用于有广播流和非广播流作为输入的特定操作符，以及</li>
<li>这样的操作符可以拥有多个不同名称的广播状态。</li>
</ul>
<h3 id="使用-operator-state">使用 Operator State</h3>
<p>要使用运算符状态，有状态函数可以实现 <code>CheckpointedFunction</code> 接口。</p>
<h4 id="checkpointedfunction">CheckpointedFunction</h4>
<p><code>CheckpointedFunction</code> 接口提供了对不同重分配方案的 non-keyed 的访问。它需要实现两个方法。</p>
<div class="highlight"><pre class="chroma"><code class="language-java" data-lang="java"><span class="kt">void</span> <span class="nf">snapshotState</span><span class="o">(</span><span class="n">FunctionSnapshotContext</span> <span class="n">context</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>

<span class="kt">void</span> <span class="nf">initializeState</span><span class="o">(</span><span class="n">FunctionInitializationContext</span> <span class="n">context</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span><span class="o">;</span>
</code></pre></div><p>每当需要执行一个检查点时，就会调用 <code>snapshotState()</code>。与之对应的 <code>initializeState()</code>，在每次用户定义的函数被初始化时都会被调用，不管是在函数首次初始化时，还是在函数实际从早期的检查点恢复时。鉴于此，<code>initializeState()</code> 不仅是初始化不同类型状态的地方，也是包含状态恢复逻辑的地方。</p>
<p>目前，支持列表式操作符状态。状态有望成为一个可序列化对象的 <code>List</code>，彼此独立，因此在重新缩放时有资格重新分配。换句话说，这些对象是 non-keyed state 可以重新分配的最细粒度。根据状态访问方法的不同，定义了以下重分布方案。</p>
<ul>
<li>
<p>均分重分配: 每个操作符都会返回一个状态元素列表。整个状态在逻辑上是所有列表的连接(concatenation)。在还原/再分配时，列表被平均分成有多少个并行操作符就有多少个子列表。每个操作符都会得到一个子列表，这个子列表可以是空的，也可以包含一个或多个元素。举个例子，如果在并行度为1的情况下，一个操作符的检查点状态包含元素1和元素2，当把并行度增加到2时，元素1可能最终进入操作符实例0，而元素2将进入操作符实例1。</p>
</li>
<li>
<p>联盟再分配。每个操作符都会返回一个状态元素列表。整个状态在逻辑上是所有 <code>List</code> 的连接(concatenation)。在还原/再分配时，每个操作符都会得到完整的状态元素列表。如果你的列表可能有很高的基数(cardinality)，请不要使用这个功能。检查点元数据将为每个列表条目存储一个偏移，这可能会导致 RPC 帧大小或内存外错误。</p>
</li>
</ul>
<p>下面是一个有状态的 <code>SinkFunction</code> 的例子，它使用 <code>CheckpointedFunction</code> 来缓冲元素，然后再将它们发送到外界。它演示了基本的均分重分配列表状态。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">BufferingSink</span><span class="o">(</span><span class="n">threshold</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">0</span><span class="o">)</span>
  <span class="k">extends</span> <span class="nc">SinkFunction</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span>
    <span class="k">with</span> <span class="nc">CheckpointedFunction</span> <span class="o">{</span>

  <span class="nd">@transient</span>
  <span class="k">private</span> <span class="k">var</span> <span class="n">checkpointedState</span><span class="k">:</span> <span class="kt">ListState</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="k">_</span>

  <span class="k">private</span> <span class="k">val</span> <span class="n">bufferedElements</span> <span class="k">=</span> <span class="nc">ListBuffer</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]()</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">invoke</span><span class="o">(</span><span class="n">value</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">),</span> <span class="n">context</span><span class="k">:</span> <span class="kt">Context</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">bufferedElements</span> <span class="o">+=</span> <span class="n">value</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">bufferedElements</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">threshold</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">for</span> <span class="o">(</span><span class="n">element</span> <span class="k">&lt;-</span> <span class="n">bufferedElements</span><span class="o">)</span> <span class="o">{</span>
        <span class="c1">// send it to the sink
</span><span class="c1"></span>      <span class="o">}</span>
      <span class="n">bufferedElements</span><span class="o">.</span><span class="n">clear</span><span class="o">()</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">snapshotState</span><span class="o">(</span><span class="n">context</span><span class="k">:</span> <span class="kt">FunctionSnapshotContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">checkpointedState</span><span class="o">.</span><span class="n">clear</span><span class="o">()</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">element</span> <span class="k">&lt;-</span> <span class="n">bufferedElements</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">checkpointedState</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">element</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">initializeState</span><span class="o">(</span><span class="n">context</span><span class="k">:</span> <span class="kt">FunctionInitializationContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">descriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ListStateDescriptor</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)](</span>
      <span class="s">&#34;buffered-elements&#34;</span><span class="o">,</span>
      <span class="nc">TypeInformation</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="k">new</span> <span class="nc">TypeHint</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]()</span> <span class="o">{})</span>
    <span class="o">)</span>

    <span class="n">checkpointedState</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">getOperatorStateStore</span><span class="o">.</span><span class="n">getListState</span><span class="o">(</span><span class="n">descriptor</span><span class="o">)</span>

    <span class="k">if</span><span class="o">(</span><span class="n">context</span><span class="o">.</span><span class="n">isRestored</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">for</span><span class="o">(</span><span class="n">element</span> <span class="k">&lt;-</span> <span class="n">checkpointedState</span><span class="o">.</span><span class="n">get</span><span class="o">())</span> <span class="o">{</span>
        <span class="n">bufferedElements</span> <span class="o">+=</span> <span class="n">element</span>
      <span class="o">}</span>
    <span class="o">}</span>
  <span class="o">}</span>

<span class="o">}</span>
</code></pre></div><p><code>initializeState</code> 方法的参数是一个 <code>FunctionInitializationContext</code>。它用于初始化 non-keyed &ldquo;容器&rdquo;。这些容器是一个 <code>ListState</code> 类型的容器，在检查点时，non-keyed 对象将被存储在那里。</p>
<p>请注意如何初始化状态，类似于 keyed state，用一个 <code>StateDescriptor</code> 来初始化，这个 <code>StateDescriptor</code> 包含了状态名称和状态所持有的值的类型信息。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">descriptor</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ListStateDescriptor</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)](</span>
    <span class="s">&#34;buffered-elements&#34;</span><span class="o">,</span>
    <span class="nc">TypeInformation</span><span class="o">.</span><span class="n">of</span><span class="o">(</span><span class="k">new</span> <span class="nc">TypeHint</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Long</span><span class="o">)]()</span> <span class="o">{})</span>
<span class="o">)</span>

<span class="n">checkpointedState</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">getOperatorStateStore</span><span class="o">.</span><span class="n">getListState</span><span class="o">(</span><span class="n">descriptor</span><span class="o">)</span>
</code></pre></div><p>状态访问方法的命名约定包含其重分配模式，然后是其状态结构。例如，如果要在还原时使用 union 重分配方案的列表状态，则使用 <code>getUnionListState(descriptor)</code> 访问状态。如果方法名中不包含重分配模式，例如 <code>getListState(descriptor)</code>，则仅仅意味着将使用基本的均分重分配方案。</p>
<p>在初始化容器后，我们使用上下文的 <code>isRestored()</code> 方法来检查是否在故障后恢复。如果为真，即我们正在恢复，则应用还原逻辑。</p>
<p>如修改后的 <code>BufferingSink</code> 的代码所示，在状态初始化过程中恢复的这个 <code>ListState</code> 被保存在一个类变量中，以便将来在 <code>snapshotState()</code> 中使用。在那里，<code>ListState</code> 会被清除掉之前检查点所包含的所有对象，然后用我们要检查点的新对象来填充。</p>
<p>顺便说一下， keyed state 也可以在 <code>initializeState()</code> 方法中初始化。这可以使用提供的 <code>FunctionInitializationContext</code> 来完成。</p>
<h3 id="有状态的源函数">有状态的源函数</h3>
<p>与其他操作符相比，有状态的源需要更多的小心。为了使状态和输出集合的更新是原子性的（对于失败/恢复时的精确一次性语义来说是必需的），用户需要从源的上下文中获得一个锁。</p>
<div class="highlight"><pre class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">CounterSource</span>
       <span class="k">extends</span> <span class="nc">RichParallelSourceFunction</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span>
       <span class="k">with</span> <span class="nc">CheckpointedFunction</span> <span class="o">{</span>

  <span class="nd">@volatile</span>
  <span class="k">private</span> <span class="k">var</span> <span class="n">isRunning</span> <span class="k">=</span> <span class="kc">true</span>

  <span class="k">private</span> <span class="k">var</span> <span class="n">offset</span> <span class="k">=</span> <span class="mi">0L</span>
  <span class="k">private</span> <span class="k">var</span> <span class="n">state</span><span class="k">:</span> <span class="kt">ListState</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="k">_</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">run</span><span class="o">(</span><span class="n">ctx</span><span class="k">:</span> <span class="kt">SourceFunction.SourceContext</span><span class="o">[</span><span class="kt">Long</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">lock</span> <span class="k">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">getCheckpointLock</span>

    <span class="k">while</span> <span class="o">(</span><span class="n">isRunning</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// output and state update are atomic
</span><span class="c1"></span>      <span class="n">lock</span><span class="o">.</span><span class="n">synchronized</span><span class="o">({</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">offset</span><span class="o">)</span>

        <span class="n">offset</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="o">})</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">cancel</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="n">isRunning</span> <span class="k">=</span> <span class="kc">false</span>
  
  <span class="k">override</span> <span class="k">def</span> <span class="n">initializeState</span><span class="o">(</span><span class="n">context</span><span class="k">:</span> <span class="kt">FunctionInitializationContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">state</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">getOperatorStateStore</span><span class="o">.</span><span class="n">getListState</span><span class="o">(</span>
      <span class="k">new</span> <span class="nc">ListStateDescriptor</span><span class="o">[</span><span class="kt">Long</span><span class="o">](</span><span class="s">&#34;state&#34;</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">Long</span><span class="o">]))</span>

    <span class="k">for</span> <span class="o">(</span><span class="n">l</span> <span class="k">&lt;-</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="o">().</span><span class="n">asScala</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">offset</span> <span class="k">=</span> <span class="n">l</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">snapshotState</span><span class="o">(</span><span class="n">context</span><span class="k">:</span> <span class="kt">FunctionSnapshotContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">state</span><span class="o">.</span><span class="n">clear</span><span class="o">()</span>
    <span class="n">state</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">offset</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div><p>一些运算符可能需要检查点被 Flink 完全承认时的信息来与外界沟通。在这种情况下，请参见 <code>org.apache.flink.runtime.state.CheckpointListener</code> 接口。</p>
<p>原文链接: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html</a></p>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/state" term="state" label="State" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[状态和容错性]]></title>
            <link href="https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-event-time/?utm_source=atom_feed" rel="related" type="text/html" title="Event Time" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-flink-datastream-api-programming-guide/?utm_source=atom_feed" rel="related" type="text/html" title="Flink Datastream API 编程指南" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-generating-watermarks/?utm_source=atom_feed" rel="related" type="text/html" title="Generating Watermarks" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-working-with-state/?utm_source=atom_feed" rel="related" type="text/html" title="使用状态" />
                <link href="https://ohmyweekly.github.io/notes/2020-08-21-built-in-watermark-generators/?utm_source=atom_feed" rel="related" type="text/html" title="内置的水印生成器" />
            
                <id>https://ohmyweekly.github.io/notes/2020-08-21-state-and-fault-tolerance/</id>
            
            
                    <author>
                        <name>焉知非鱼</name>
                    </author>
            <published>2020-08-21T00:00:00+08:00</published>
            <updated>2020-08-21T00:00:00+08:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>State &amp; Fault Tolerance</blockquote><h2 id="状态和容错性httpsciapacheorgprojectsflinkflink-docs-release-111devstreamstate"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/">状态和容错性</a></h2>
<p>在本节中，您将了解 Flink 为编写有状态程序提供的 API。请看一下 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/concepts/stateful-stream-processing.html">Stateful Stream Processing</a>，了解有状态流处理背后的概念。</p>
<h2 id="下一步怎么走">下一步怎么走？</h2>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html">使用状态</a>。展示如何在 Flink 应用中使用状态，并解释不同类型的状态。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/broadcast_state.html">广播状态模式</a>。解释如何连接一个广播流和一个非广播流，并使用状态在它们之间交换信息。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/checkpointing.html">检查点</a>。描述了如何启用和配置检查点以实现容错。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/queryable_state.html">可查询状态</a>。说明如何在运行时从 Flink 外部访问状态。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/schema_evolution.html">状态模式演化</a>：介绍如何在运行时从外部访问状态。展示了状态类型的模式如何演化。</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/custom_serialization.html">管理状态的自定义序列化</a>。讨论如何实现自定义序列化，特别是针对模式演化。</li>
</ul>
]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/categories/flink" term="flink" label="Flink" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink" term="flink" label="Flink" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" term="flink-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" label="Flink 官方文档" />
                             
                                <category scheme="https://ohmyweekly.github.io/tags/datastream-api" term="datastream-api" label="DataStream API" />
                            
                        
                    
                
            
        </entry>
    
</feed>
