{"author":{"name":null,"type":"card","url":"https://ohmycloud.github.io/"},"content":{"html":"\u003cp\u003e重启 hdfs 集群后，出现的standy 错误的原因:\u003c/p\u003e\n\u003cp\u003e是因为没有启动zookeeper, zookeeper不会自动重启, zook的启动命令是zkServer.sh.\u003c/p\u003e\n\u003cp\u003e以后启动集群时，先启动那slave上安装了 zookeeper 的 zookeeper , 然后再启动 hdfs。\u003c/p\u003e\n\u003cp\u003eMaster 上 的  zoomkeeper,   \u003ccode\u003evim /opt/zookeeper-3.4.9/conf/zoom.cfg\u003c/code\u003e:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e# The number of milliseconds of each tick\ntickTime=2000\n# The number of ticks that the initial \n# synchronization phase can take\ninitLimit=10\n# The number of ticks that can pass between \n# sending a request and getting an acknowledgement\nsyncLimit=5\n# the directory where the snapshot is stored.\n# do not use /tmp for storage, /tmp here is just \n# example sakes.\ndataDir=/data/zkdata\ndataLogDir=/data/zklog\n# the port at which the clients will connect\nclientPort=2181\n# the maximum number of client connections.\n# increase this if you need to handle more clients\n#maxClientCnxns=60\n#\n# Be sure to read the maintenance section of the \n# administrator guide before turning on autopurge.\n#\n# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance\n#\n# The number of snapshots to retain in dataDir\n#autopurge.snapRetainCount=3\n# Purge task interval in hours\n# Set to \u0026#34;0\u0026#34; to disable auto purge feature\n#autopurge.purgeInterval=1\nserver.1=slave1:2888:3888\nserver.2=slave2:2888:3888\nserver.3=slave3:2888:3888\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSalve2 上: \u003ccode\u003ell /opt\u003c/code\u003e:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edrwxr-xr-x  9 root root 4096 Apr 13 18:02 apache-hive-2.1.1-bin/\ndrwxrwxr-x 10 root root 4096 Apr 13 01:35 hadoop-2.6.5/\ndrwxr-xr-x  7 root root 4096 May 24 22:40 kafka_2.11-0.10.1.0/\ndrwxrwxr-x  6 root root 4096 Apr 13 01:36 scala-2.11.8/\ndrwxr-xr-x 14 root root 4096 Apr 13 03:45 spark-2.1.0-bin-hadoop2.6/\ndrwxr-xr-x 10 root root 4096 Apr 13 02:06 zookeeper-3.4.9/\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ecd /opt/zookeeper-3.4.9 \u0026amp;\u0026amp;  vim conf/zoo.cfg :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e# The number of milliseconds of each tick\ntickTime=2000\n# The number of ticks that the initial \n# synchronization phase can take\ninitLimit=10\n# The number of ticks that can pass between \n# sending a request and getting an acknowledgement\nsyncLimit=5\n# the directory where the snapshot is stored.\n# do not use /tmp for storage, /tmp here is just \n# example sakes.\ndataDir=/data/zkdata\ndataLogDir=/data/zklog\n# the port at which the clients will connect\nclientPort=2181\n# the maximum number of client connections.\n# increase this if you need to handle more clients\n#maxClientCnxns=60\n#\n# Be sure to read the maintenance section of the \n# administrator guide before turning on autopurge.\n#\n# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance\n#\n# The number of snapshots to retain in dataDir\n#autopurge.snapRetainCount=3\n# Purge task interval in hours\n# Set to \u0026#34;0\u0026#34; to disable auto purge feature\n#autopurge.purgeInterval=1\nserver.1=slave1:2888:3888\nserver.2=slave2:2888:3888\nserver.3=slave3:2888:3888\n\u003c/code\u003e\u003c/pre\u003e","text":"重启 hdfs 集群后，出现的standy 错误的原因:\n是因为没有启动zookeeper, zookeeper不会自动重启, zook的启动命令是zkServer.sh.\n以后启动集群时，先启动那slave上安装了 zookeeper 的 zookeeper , 然后再启动 hdfs。\nMaster 上 的 zoomkeeper, vim /opt/zookeeper-3.4.9/conf/zoom.cfg:\n# The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take initLimit=10 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=5 # the directory where the snapshot is stored. # do not use /tmp for storage, /tmp here is just # example sakes. dataDir=/data/zkdata dataLogDir=/data/zklog # the port at which the clients will connect clientPort=2181 # the maximum number of client connections. # increase this if you need to handle more clients #maxClientCnxns=60 # # Be sure to read the maintenance section of the # administrator guide before turning on autopurge. # # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance # # The number of snapshots to retain in dataDir #autopurge.snapRetainCount=3 # Purge task interval in hours # Set to \u0026#34;0\u0026#34; to disable auto purge feature #autopurge.purgeInterval=1 server.1=slave1:2888:3888 server.2=slave2:2888:3888 server.3=slave3:2888:3888 Salve2 上: ll /opt:\ndrwxr-xr-x 9 root root 4096 Apr 13 18:02 apache-hive-2.1.1-bin/ drwxrwxr-x 10 root root 4096 Apr 13 01:35 hadoop-2.6.5/ drwxr-xr-x 7 root root 4096 May 24 22:40 kafka_2.11-0.10.1.0/ drwxrwxr-x 6 root root 4096 Apr 13 01:36 scala-2.11.8/ drwxr-xr-x 14 root root 4096 Apr 13 03:45 spark-2.1.0-bin-hadoop2.6/ drwxr-xr-x 10 root root 4096 Apr 13 02:06 zookeeper-3.4.9/ cd /opt/zookeeper-3.4.9 \u0026amp;\u0026amp; vim conf/zoo.cfg :\n# The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take initLimit=10 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=5 # the directory where the snapshot is stored. # do not use /tmp for storage, /tmp here is just # example sakes. dataDir=/data/zkdata dataLogDir=/data/zklog # the port at which the clients will connect clientPort=2181 # the maximum number of client connections. # increase this if you need to handle more clients #maxClientCnxns=60 # # Be sure to read the maintenance section of the # administrator guide before turning on autopurge. # # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance # # The number of snapshots to retain in dataDir #autopurge.snapRetainCount=3 # Purge task interval in hours # Set to \u0026#34;0\u0026#34; to disable auto purge feature #autopurge.purgeInterval=1 server.1=slave1:2888:3888 server.2=slave2:2888:3888 server.3=slave3:2888:3888 "},"name":"Spark 集群出现 standby 的问题","published":"2017-04-01T16:36:25Z","summary":"重启 hdfs 集群后，出现的standy 错误的原因:\n是因为没有启动zookeeper, zookeeper不会自动重启, zook的启动命令是zkServer.sh.\n以后启动集群时，先启动那slave上安装了 zookeeper 的 zookeeper , 然后再启动 hdfs。\nMaster 上 的 zoomkeeper, vim /opt/zookeeper-3.4.9/conf/zoom.cfg:\n# The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take initLimit=10 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=5 # the directory where the snapshot is stored. # do not use /tmp for storage, /tmp here is just # example sakes.","type":"entry","url":"https://ohmycloud.github.io/notes/standby/"}