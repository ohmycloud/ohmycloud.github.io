{"author":{"name":null,"type":"card","url":"http://localhost:1313/"},"content":{"html":"\u003ch2 id=\"table-api-和-sql\"\u003eTable API 和 SQL\u003c/h2\u003e\n\u003cp\u003eApache Flink 具有两个关系型 API - Table API 和 SQL - 用于统一的流和批处理。Table API 是 Scala 和 Java 的语言集成查询 API，它允许用非常直观的方式从关系运算符（如选择、过滤和连接）组成查询。Flink 的 SQL 支持是基于 \u003ca href=\"https://calcite.apache.org/\"\u003eApache Calcite\u003c/a\u003e，它实现了 SQL 标准。无论输入是批处理输入（DataSet）还是流输入（DataStream），在任一接口中指定的查询都具有相同的语义，并指定相同的结果。\u003c/p\u003e\n\u003cp\u003e表 API 和 SQL 接口与 Flink 的 DataStream 和 DataSet API 紧密集成。你可以很容易地在所有 API 和建立在 API 基础上的库之间切换。例如，您可以使用 \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/cep.html\"\u003eCEP 库\u003c/a\u003e 从 DataStream 中提取模式，随后使用 Table API 来分析模式，或者您可能会在预处理数据上运行 \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/gelly\"\u003eGelly 图算法\u003c/a\u003e之前，使用 SQL 查询扫描、过滤和聚合一个批处理表。\u003c/p\u003e\n\u003cp\u003e请注意，Table API 和 SQL 的功能还不完善，正在积极开发中。并非所有的操作都被 [Table API, SQL] 和 [stream, batch] 输入的每个组合所支持。\u003c/p\u003e\n\u003ch3 id=\"依赖结构\"\u003e依赖结构\u003c/h3\u003e\n\u003cp\u003e从 Flink 1.9 开始，Flink 为评估 Table \u0026amp; SQL API 程序提供了两种不同的规划器实现：Blink planner 和 Flink 1.9 之前的旧 planner。Planner 负责将关系运算符转化为可执行的、优化的 Flink 作业。这两种 planner 都有不同的优化规则和运行时类。它们在支持的功能集上也可能有所不同。\u003c/p\u003e\n\u003cp\u003e注意: 对于生产用例，我们推荐 blink planner，它从 1.11 开始成为默认 planner。\u003c/p\u003e\n\u003cp\u003e所有的 Table API 和 SQL 组件都捆绑在 flink-table 或 flink-table-blink Maven 构件中。\u003c/p\u003e\n\u003cp\u003e以下是与大多数项目相关的依赖关系。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eflink-table-common: 一个通用模块，用于通过自定义函数、格式等扩展表生态系统。\u003c/li\u003e\n\u003cli\u003eflink-table-api-java: 使用 Java 编程语言的纯表程序的 Table \u0026amp; SQL API（处于早期开发阶段，不推荐！）。\u003c/li\u003e\n\u003cli\u003eflink-table-api-scala: Table 和 SQL API，用于使用 Java 编程语言的纯表程序（处于早期开发阶段，不推荐）。\u003c/li\u003e\n\u003cli\u003eflink-table-api-java-bridge: 使用 Java 编程语言支持 DataStream/DataSet API 的 Table \u0026amp; SQL API。\u003c/li\u003e\n\u003cli\u003eflink-table-api-scala-bridge: 使用 Scala 编程语言，支持 DataStream/DataSet API 的表和 SQL API。\u003c/li\u003e\n\u003cli\u003eflink-table-planner: 表程序 planner 和运行时。这是在 1.9 版本之前 Flink 唯一的 planner。从 Flink 1.11 开始，不再推荐使用它。\u003c/li\u003e\n\u003cli\u003eflink-table-planner-link: 新的 Blink 计划器，从 Flink 1.11 开始成为默认的。\u003c/li\u003e\n\u003cli\u003eflink-table-runtim-blink: 新的 Blink 运行时。\u003c/li\u003e\n\u003cli\u003eflink-table-uber: 将上面的 API 模块加上旧的规划器打包成一个适用于大多数 Table \u0026amp; SQL API 使用案例的发行版。uber JAR 文件 \u003ccode\u003eflink-table-*.jar\u003c/code\u003e 默认位于 Flink 版本的 \u003ccode\u003e/lib\u003c/code\u003e 目录下。\u003c/li\u003e\n\u003cli\u003eflink-table-uber-blink: 将上面的 API 模块加上 Blink 的特定模块打包成一个适用于大多数 Table \u0026amp; SQL API 用例的发行版。uber JAR 文件 \u003ccode\u003eflink-table-blink-*.jar\u003c/code\u003e 默认位于 Flink 版本的 \u003ccode\u003e/lib\u003c/code\u003e 目录下。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e关于如何在表程序中切换新旧 Blink planner，请参见\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html\"\u003e通用 API\u003c/a\u003e 页面。\u003c/p\u003e\n\u003ch3 id=\"表程序依赖\"\u003e表程序依赖\u003c/h3\u003e\n\u003cp\u003e根据目标编程语言的不同，您需要将 Java 或 Scala API 添加到项目中，以便使用 Table API 和 SQL 来定义管道。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-xml\" data-lang=\"xml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c\"\u003e\u0026lt;!-- Either... --\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.flink\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003eflink-table-api-java-bridge_2.11\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e1.11.0\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c\"\u003e\u0026lt;!-- or... --\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.flink\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003eflink-table-api-scala-bridge_2.11\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e1.11.0\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e此外，如果你想在 IDE 中本地运行 Table API 和 SQL 程序，你必须添加以下一组模块，这取决于你想使用的计划器。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-xml\" data-lang=\"xml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c\"\u003e\u0026lt;!-- Either... (for the old planner that was available before Flink 1.9) --\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.flink\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003eflink-table-planner_2.11\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e1.11.0\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c\"\u003e\u0026lt;!-- or.. (for the new Blink planner) --\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.flink\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003eflink-table-planner-blink_2.11\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e1.11.0\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e在内部，表生态系统的部分内容是在 Scala 中实现的。因此，请确保为批处理和流应用添加以下依赖关系。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-xml\" data-lang=\"xml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.flink\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003eflink-streaming-scala_2.11\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e1.11.0\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"扩展依赖性\"\u003e扩展依赖性\u003c/h3\u003e\n\u003cp\u003e如果你想实现与 Kafka 交互的\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sourceSinks.html#define-a-tablefactory\"\u003e自定义格式\u003c/a\u003e或一组\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/systemFunctions.html\"\u003e用户定义的函数\u003c/a\u003e，下面的依赖就足够了，可以用于 SQL 客户端的 JAR 文件。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-xml\" data-lang=\"xml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.flink\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003eflink-table-common\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e1.11.0\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e目前，该模块包括以下扩展点：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSerializationSchemaFactory\u003c/li\u003e\n\u003cli\u003eDeserializationSchemaFactory\u003c/li\u003e\n\u003cli\u003eScalarFunction\u003c/li\u003e\n\u003cli\u003eTableFunction\u003c/li\u003e\n\u003cli\u003eAggregateFunction\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"下一步怎么走\"\u003e下一步怎么走？\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/common.html\"\u003e概念与通用 API\u003c/a\u003e: Table API 和 SQL 的共享概念和 API。\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/types.html\"\u003e数据类型\u003c/a\u003e: 列出了预先定义的数据类型及其属性。\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming\"\u003e流概念\u003c/a\u003e: 表 API 或 SQL 的流特定文档，如时间属性的配置和更新结果的处理。\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connect.html\"\u003e连接到外部系统\u003c/a\u003e: 可用的连接器和格式，用于向外部系统读写数据。\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html\"\u003eTable API\u003c/a\u003e。支持的操作和表 API 的 API。\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/index.html\"\u003eSQL\u003c/a\u003e。支持 SQL 的操作和语法。\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/systemFunctions.html\"\u003e内置函数\u003c/a\u003e: 表 API 和 SQL 中支持的函数。\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sqlClient.html\"\u003eSQL 客户端\u003c/a\u003e: 玩转 Flink SQL，并向集群提交表格程序，无需编程知识。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e原文链接: \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/\"\u003ehttps://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/\u003c/a\u003e\u003c/p\u003e\n","text":"Table API 和 SQL Apache Flink 具有两个关系型 API - Table API 和 SQL - 用于统一的流和批处理。Table API 是 Scala 和 Java 的语言集成查询 API，它允许用非常直观的方式从关系运算符（如选择、过滤和连接）组成查询。Flink 的 SQL 支持是基于 Apache Calcite，它实现了 SQL 标准。无论输入是批处理输入（DataSet）还是流输入（DataStream），在任一接口中指定的查询都具有相同的语义，并指定相同的结果。\n表 API 和 SQL 接口与 Flink 的 DataStream 和 DataSet API 紧密集成。你可以很容易地在所有 API 和建立在 API 基础上的库之间切换。例如，您可以使用 CEP 库 从 DataStream 中提取模式，随后使用 Table API 来分析模式，或者您可能会在预处理数据上运行 Gelly 图算法之前，使用 SQL 查询扫描、过滤和聚合一个批处理表。\n请注意，Table API 和 SQL 的功能还不完善，正在积极开发中。并非所有的操作都被 [Table API, SQL] 和 [stream, batch] 输入的每个组合所支持。\n依赖结构 从 Flink 1.9 开始，Flink 为评估 Table \u0026amp; SQL API 程序提供了两种不同的规划器实现：Blink planner 和 Flink 1.9 之前的旧 planner。Planner 负责将关系运算符转化为可执行的、优化的 Flink 作业。这两种 planner 都有不同的优化规则和运行时类。它们在支持的功能集上也可能有所不同。\n注意: 对于生产用例，我们推荐 blink planner，它从 1.11 开始成为默认 planner。\n所有的 Table API 和 SQL 组件都捆绑在 flink-table 或 flink-table-blink Maven 构件中。\n以下是与大多数项目相关的依赖关系。\nflink-table-common: 一个通用模块，用于通过自定义函数、格式等扩展表生态系统。 flink-table-api-java: 使用 Java 编程语言的纯表程序的 Table \u0026amp; SQL API（处于早期开发阶段，不推荐！）。 flink-table-api-scala: Table 和 SQL API，用于使用 Java 编程语言的纯表程序（处于早期开发阶段，不推荐）。 flink-table-api-java-bridge: 使用 Java 编程语言支持 DataStream/DataSet API 的 Table \u0026amp; SQL API。 flink-table-api-scala-bridge: 使用 Scala 编程语言，支持 DataStream/DataSet API 的表和 SQL API。 flink-table-planner: 表程序 planner 和运行时。这是在 1.9 版本之前 Flink 唯一的 planner。从 Flink 1.11 开始，不再推荐使用它。 flink-table-planner-link: 新的 Blink 计划器，从 Flink 1.11 开始成为默认的。 flink-table-runtim-blink: 新的 Blink 运行时。 flink-table-uber: 将上面的 API 模块加上旧的规划器打包成一个适用于大多数 Table \u0026amp; SQL API 使用案例的发行版。uber JAR 文件 flink-table-*.jar 默认位于 Flink 版本的 /lib 目录下。 flink-table-uber-blink: 将上面的 API 模块加上 Blink 的特定模块打包成一个适用于大多数 Table \u0026amp; SQL API 用例的发行版。uber JAR 文件 flink-table-blink-*.jar 默认位于 Flink 版本的 /lib 目录下。 关于如何在表程序中切换新旧 Blink planner，请参见通用 API 页面。\n表程序依赖 根据目标编程语言的不同，您需要将 Java 或 Scala API 添加到项目中，以便使用 Table API 和 SQL 来定义管道。\n\u0026lt;!-- Either... --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- or... --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-scala-bridge_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 此外，如果你想在 IDE 中本地运行 Table API 和 SQL 程序，你必须添加以下一组模块，这取决于你想使用的计划器。\n\u0026lt;!-- Either... (for the old planner that was available before Flink 1.9) --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-planner_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- or.. (for the new Blink planner) --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-planner-blink_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 在内部，表生态系统的部分内容是在 Scala 中实现的。因此，请确保为批处理和流应用添加以下依赖关系。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-streaming-scala_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 扩展依赖性 如果你想实现与 Kafka 交互的自定义格式或一组用户定义的函数，下面的依赖就足够了，可以用于 SQL 客户端的 JAR 文件。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-common\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 目前，该模块包括以下扩展点：\nSerializationSchemaFactory DeserializationSchemaFactory ScalarFunction TableFunction AggregateFunction 下一步怎么走？ 概念与通用 API: Table API 和 SQL 的共享概念和 API。 数据类型: 列出了预先定义的数据类型及其属性。 流概念: 表 API 或 SQL 的流特定文档，如时间属性的配置和更新结果的处理。 连接到外部系统: 可用的连接器和格式，用于向外部系统读写数据。 Table API。支持的操作和表 API 的 API。 SQL。支持 SQL 的操作和语法。 内置函数: 表 API 和 SQL 中支持的函数。 SQL 客户端: 玩转 Flink SQL，并向集群提交表格程序，无需编程知识。 原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/\n"},"name":"Table API 和 SQL","published":"2020-08-22T00:00:00Z","summary":"Table API \u0026amp; SQL","type":"entry","url":"http://localhost:1313/notes/2020-08-22-table-api-and-sql/"}