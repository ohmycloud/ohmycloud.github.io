{"author":{"name":null,"type":"card","url":"https://ohmyweekly.github.io/"},"content":{"html":"\u003ch1 id=\"安装-airflow\"\u003e安装 AirFlow\u003c/h1\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo pip install airflow --upgrade --ignore-installed\nmkdir -p /Users/ohmycloud/airflow/dags\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003edags 目录中放入该文件：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e# -*- coding:utf-8 -*-\n# airflowPysparkDagTest.py\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\nimport os\n\nsparkSubmit = \u0026#39;/Users/ohmycloud/opt/spark-2.1.0-bin-hadoop2.6/bin/spark-submit\u0026#39;\n\n## Define the DAG object\ndefault_args = {\n    \u0026#39;owner\u0026#39;: \u0026#39;焉知非鱼\u0026#39;,\n    \u0026#39;depends_on_past\u0026#39;: False,\n    \u0026#39;start_date\u0026#39;: datetime(2017, 12, 27),\n    \u0026#39;retries\u0026#39;: 5,\n    \u0026#39;retry_delay\u0026#39;: timedelta(minutes=1),\n}\ndag = DAG(\u0026#39;PysparkTest\u0026#39;, default_args=default_args, schedule_interval=timedelta(1))\n\nnumUniqueAuthors = BashOperator(\n    task_id=\u0026#39;unique-event\u0026#39;,\n    bash_command=sparkSubmit + \u0026#39; \u0026#39; + \u0026#39;/Users/ohmycloud/scripts/Python/spark-json/test.py\u0026#39;,\n    dag=dag)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003etest.py 里面写入：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e# -*- coding:utf-8 -*-\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import countDistinct,count,sum,when,col,current_date,date_sub,unix_timestamp,lit,from_unixtime\nimport sys\nimport time\nimport datetime\n\n# 从 json 数据源读取数据\ndef read_from_json(spark):\n    df=spark.read.json(\u0026#34;/Users/ohmycloud/scripts/Python/ald_py/etl.json\u0026#34;)\n    event_logs_df=df.filter(df.ev==\u0026#34;event\u0026#34;) \\\n                         .select(\n                             df.ak.alias(\u0026#34;app_key\u0026#34;),\n                             df.at.alias(\u0026#34;access_token\u0026#34;),\n                             df.tp.alias(\u0026#34;ev\u0026#34;)\n                         )\n    event_logs_df.show()\n\nif __name__ == \u0026#39;__main__\u0026#39;:\n    spark = SparkSession \\\n        .builder \\\n        .appName(\u0026#34;aldstat\u0026#34;) \\\n        .config(\u0026#34;spark.sql.shuffle.partitions\u0026#34;, 48) \\\n        .master(\u0026#34;local\u0026#34;) \\\n        .getOrCreate()\n    read_from_json(spark)\n    spark.stop()\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e测试一次：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eairflow list_dags\nairflow backfill PysparkTest -s 2017-12-27\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e在 UI 界面中会看到运行成功了一次， 日志中也能看见打印出来了结果：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"http://upload-images.jianshu.io/upload_images/326727-27ab43a6bdefdf27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"WX20171227-154224@2x.png\"\u003e\u003c/p\u003e\n","text":"安装 AirFlow sudo pip install airflow --upgrade --ignore-installed mkdir -p /Users/ohmycloud/airflow/dags dags 目录中放入该文件：\n# -*- coding:utf-8 -*- # airflowPysparkDagTest.py from airflow import DAG from airflow.operators.bash_operator import BashOperator from datetime import datetime, timedelta import os sparkSubmit = \u0026#39;/Users/ohmycloud/opt/spark-2.1.0-bin-hadoop2.6/bin/spark-submit\u0026#39; ## Define the DAG object default_args = { \u0026#39;owner\u0026#39;: \u0026#39;焉知非鱼\u0026#39;, \u0026#39;depends_on_past\u0026#39;: False, \u0026#39;start_date\u0026#39;: datetime(2017, 12, 27), \u0026#39;retries\u0026#39;: 5, \u0026#39;retry_delay\u0026#39;: timedelta(minutes=1), } dag = DAG(\u0026#39;PysparkTest\u0026#39;, default_args=default_args, schedule_interval=timedelta(1)) numUniqueAuthors = BashOperator( task_id=\u0026#39;unique-event\u0026#39;, bash_command=sparkSubmit + \u0026#39; \u0026#39; + \u0026#39;/Users/ohmycloud/scripts/Python/spark-json/test.py\u0026#39;, dag=dag) test.py 里面写入：\n# -*- coding:utf-8 -*- from pyspark.sql import SparkSession from pyspark.sql import Row from pyspark.sql.functions import countDistinct,count,sum,when,col,current_date,date_sub,unix_timestamp,lit,from_unixtime import sys import time import datetime # 从 json 数据源读取数据 def read_from_json(spark): df=spark.read.json(\u0026#34;/Users/ohmycloud/scripts/Python/ald_py/etl.json\u0026#34;) event_logs_df=df.filter(df.ev==\u0026#34;event\u0026#34;) \\ .select( df.ak.alias(\u0026#34;app_key\u0026#34;), df.at.alias(\u0026#34;access_token\u0026#34;), df.tp.alias(\u0026#34;ev\u0026#34;) ) event_logs_df.show() if __name__ == \u0026#39;__main__\u0026#39;: spark = SparkSession \\ .builder \\ .appName(\u0026#34;aldstat\u0026#34;) \\ .config(\u0026#34;spark.sql.shuffle.partitions\u0026#34;, 48) \\ .master(\u0026#34;local\u0026#34;) \\ .getOrCreate() read_from_json(spark) spark.stop() 测试一次：\nairflow list_dags airflow backfill PysparkTest -s 2017-12-27 在 UI 界面中会看到运行成功了一次， 日志中也能看见打印出来了结果：\n"},"name":"AirFlow 教程","published":"2017-03-01T16:36:25Z","summary":"安装 AirFlow sudo pip install airflow --upgrade --ignore-installed mkdir -p /Users/ohmycloud/airflow/dags dags 目录中放入该文件：\n# -*- coding:utf-8 -*- # airflowPysparkDagTest.py from airflow import DAG from airflow.operators.bash_operator import BashOperator from datetime import datetime, timedelta import os sparkSubmit = \u0026#39;/Users/ohmycloud/opt/spark-2.1.0-bin-hadoop2.6/bin/spark-submit\u0026#39; ## Define the DAG object default_args = { \u0026#39;owner\u0026#39;: \u0026#39;焉知非鱼\u0026#39;, \u0026#39;depends_on_past\u0026#39;: False, \u0026#39;start_date\u0026#39;: datetime(2017, 12, 27), \u0026#39;retries\u0026#39;: 5, \u0026#39;retry_delay\u0026#39;: timedelta(minutes=1), } dag = DAG(\u0026#39;PysparkTest\u0026#39;, default_args=default_args, schedule_interval=timedelta(1)) numUniqueAuthors = BashOperator( task_id=\u0026#39;unique-event\u0026#39;, bash_command=sparkSubmit + \u0026#39; \u0026#39; + \u0026#39;/Users/ohmycloud/scripts/Python/spark-json/test.","type":"entry","url":"https://ohmyweekly.github.io/notes/airflow-%E6%95%99%E7%A8%8B/"}