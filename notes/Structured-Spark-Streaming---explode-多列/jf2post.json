{"author":{"name":null,"type":"card","url":"https://ohmyweekly.github.io/"},"content":{"html":"\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epackage ohmysummer\n\nimport ohmysummer.model.SourceCan\nimport ohmysummer.pipeline.kafka.WmKafkaDeserializer\nimport ohmysummer.pipeline.schema.{CanSignalSchema, DcSaleData}\nimport org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\nimport org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.functions._\n\n/**\n  * 从 Kafka 读取  JSON 数据\n  *  https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n  *  https://stackoverflow.com/questions/43297973/how-to-read-records-in-json-format-from-kafka-using-structured-streaming\n  *  https://stackoverflow.com/questions/48361177/spark-structured-streaming-kafka-convert-json-without-schema-infer-schema\n  *  https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\n  *  https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\n  */\n\n\nobject WmDcSaleApplication {\n  def main(args: Array[String]) {\n\n    val spark = SparkSession\n      .builder\n      .appName(\u0026#34;ReadFromKafka\u0026#34;)\n      .master(\u0026#34;local[2]\u0026#34;)\n      .getOrCreate()\n\n    object KafkaDeserializerWrapper {\n      val deser = new WmKafkaDeserializer\n    }\n    spark.udf.register(\u0026#34;deserialize\u0026#34;, (topic: String, bytes: Array[Byte]) =\u0026gt;\n      KafkaDeserializerWrapper.deser.deserialize(topic, bytes)\n    )\n\n    val df = spark.readStream\n      .format(\u0026#34;kafka\u0026#34;)\n      .option(\u0026#34;kafka.bootstrap.servers\u0026#34;, \u0026#34;localhost:9092\u0026#34;)\n      .option(\u0026#34;subscribe\u0026#34;, \u0026#34;dc-sale-data\u0026#34;)\n      .option(\u0026#34;startingOffsets\u0026#34;, \u0026#34;earliest\u0026#34;) \n      .load()\n\n    import spark.implicits._\n    // 反序列化 value 中的字节数组, 得到原始 JSON\n    val result: Dataset[(String, String)] = df.selectExpr(\u0026#34;CAST(key AS STRING)\u0026#34;, \u0026#34;\u0026#34;\u0026#34;deserialize(\u0026#34;dc-sale-data\u0026#34;, value) AS message\u0026#34;\u0026#34;\u0026#34;)\n      .as[(String, String)]\n\n    val schema = (new DcSaleData).schema\n    val canSchema = (new CanSignalSchema).schema\n\n    val parsed = result.select($\u0026#34;key\u0026#34;, from_json($\u0026#34;message\u0026#34;, canSchema) as \u0026#34;data\u0026#34;)\n//    val event: DataFrame = parsed.select($\u0026#34;data.vin\u0026#34;, $\u0026#34;data.version\u0026#34;, $\u0026#34;data.tboxSn\u0026#34;, $\u0026#34;data.iccid\u0026#34;, $\u0026#34;data.createTime\u0026#34;, explode(array($\u0026#34;data.event.info\u0026#34;))).select(\u0026#34;vin\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;tboxSn\u0026#34;, \u0026#34;iccid\u0026#34;, \u0026#34;createTime\u0026#34;, \u0026#34;col.*\u0026#34;)\n    val event: Dataset[SourceCan] = parsed.select($\u0026#34;data.vin\u0026#34;,$\u0026#34;data.version\u0026#34;,$\u0026#34;data.tboxSn\u0026#34;,$\u0026#34;data.iccid\u0026#34;,$\u0026#34;data.createTime\u0026#34;,$\u0026#34;data.event.info\u0026#34; as \u0026#34;event\u0026#34;,$\u0026#34;data.signal1s.info\u0026#34; as \u0026#34;signal1s\u0026#34;, $\u0026#34;data.signal30s.info\u0026#34; as \u0026#34;signal30s\u0026#34;)\n      .select($\u0026#34;vin\u0026#34;, $\u0026#34;version\u0026#34;, $\u0026#34;tboxSn\u0026#34;, $\u0026#34;iccid\u0026#34;, $\u0026#34;createTime\u0026#34;,              explode(array($\u0026#34;event\u0026#34;)) as \u0026#34;eventcol\u0026#34;, $\u0026#34;signal1s\u0026#34;, $\u0026#34;signal30s\u0026#34;)\n      .select($\u0026#34;vin\u0026#34;, $\u0026#34;version\u0026#34;, $\u0026#34;tboxSn\u0026#34;, $\u0026#34;iccid\u0026#34;, $\u0026#34;createTime\u0026#34;, $\u0026#34;eventcol\u0026#34;, explode(array($\u0026#34;signal1s\u0026#34;)) as \u0026#34;signal1scol\u0026#34;, $\u0026#34;signal30s\u0026#34;)\n      .select($\u0026#34;vin\u0026#34;, $\u0026#34;version\u0026#34;, $\u0026#34;tboxSn\u0026#34;, $\u0026#34;iccid\u0026#34;, $\u0026#34;createTime\u0026#34;, $\u0026#34;eventcol\u0026#34;, $\u0026#34;signal1scol\u0026#34;, explode(array($\u0026#34;signal30s\u0026#34;)) as \u0026#34;signal30scol\u0026#34;)\n      .select($\u0026#34;vin\u0026#34;, $\u0026#34;version\u0026#34;, $\u0026#34;tboxSn\u0026#34;, $\u0026#34;iccid\u0026#34;, $\u0026#34;createTime\u0026#34;, $\u0026#34;eventcol.*\u0026#34;, $\u0026#34;signal1scol.*\u0026#34;, $\u0026#34;signal30scol.*\u0026#34;)\n      .select($\u0026#34;vin\u0026#34;, $\u0026#34;createTime\u0026#34;, $\u0026#34;HU_TargetSOC\u0026#34; as \u0026#34;targetSoc\u0026#34; , $\u0026#34;ICU_ICUTotalOdometer\u0026#34; as \u0026#34;totalOdometer\u0026#34;)\n      .filter($\u0026#34;vin\u0026#34;.isNotNull)\n      .as[SourceCan]\n\n//    parsed.printSchema()\n//    event.printSchema()\n\n    val console = event.writeStream\n      .format(\u0026#34;console\u0026#34;)\n      .option(\u0026#34;truncate\u0026#34;, \u0026#34;false\u0026#34;)\n      .outputMode(OutputMode.Append())\n\n    val query = console.start()\n\n    query.awaitTermination()\n\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e","text":"package ohmysummer import ohmysummer.model.SourceCan import ohmysummer.pipeline.kafka.WmKafkaDeserializer import ohmysummer.pipeline.schema.{CanSignalSchema, DcSaleData} import org.apache.spark.sql.{DataFrame, Dataset, SparkSession} import org.apache.spark.sql.streaming.OutputMode import org.apache.spark.sql.functions._ /** * 从 Kafka 读取 JSON 数据 * https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html * https://stackoverflow.com/questions/43297973/how-to-read-records-in-json-format-from-kafka-using-structured-streaming * https://stackoverflow.com/questions/48361177/spark-structured-streaming-kafka-convert-json-without-schema-infer-schema * https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html * https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html */ object WmDcSaleApplication { def main(args: Array[String]) { val spark = SparkSession .builder .appName(\u0026#34;ReadFromKafka\u0026#34;) .master(\u0026#34;local[2]\u0026#34;) .getOrCreate() object KafkaDeserializerWrapper { val deser = new WmKafkaDeserializer } spark.udf.register(\u0026#34;deserialize\u0026#34;, (topic: String, bytes: Array[Byte]) =\u0026gt; KafkaDeserializerWrapper.deser.deserialize(topic, bytes) ) val df = spark.readStream .format(\u0026#34;kafka\u0026#34;) .option(\u0026#34;kafka.bootstrap.servers\u0026#34;, \u0026#34;localhost:9092\u0026#34;) .option(\u0026#34;subscribe\u0026#34;, \u0026#34;dc-sale-data\u0026#34;) .option(\u0026#34;startingOffsets\u0026#34;, \u0026#34;earliest\u0026#34;) .load() import spark.implicits._ // 反序列化 value 中的字节数组, 得到原始 JSON val result: Dataset[(String, String)] = df.selectExpr(\u0026#34;CAST(key AS STRING)\u0026#34;, \u0026#34;\u0026#34;\u0026#34;deserialize(\u0026#34;dc-sale-data\u0026#34;, value) AS message\u0026#34;\u0026#34;\u0026#34;) .as[(String, String)] val schema = (new DcSaleData).schema val canSchema = (new CanSignalSchema).schema val parsed = result.select($\u0026#34;key\u0026#34;, from_json($\u0026#34;message\u0026#34;, canSchema) as \u0026#34;data\u0026#34;) // val event: DataFrame = parsed.select($\u0026#34;data.vin\u0026#34;, $\u0026#34;data.version\u0026#34;, $\u0026#34;data.tboxSn\u0026#34;, $\u0026#34;data.iccid\u0026#34;, $\u0026#34;data.createTime\u0026#34;, explode(array($\u0026#34;data.event.info\u0026#34;))).select(\u0026#34;vin\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;tboxSn\u0026#34;, \u0026#34;iccid\u0026#34;, \u0026#34;createTime\u0026#34;, \u0026#34;col.*\u0026#34;) val event: Dataset[SourceCan] = parsed.select($\u0026#34;data.vin\u0026#34;,$\u0026#34;data.version\u0026#34;,$\u0026#34;data.tboxSn\u0026#34;,$\u0026#34;data.iccid\u0026#34;,$\u0026#34;data.createTime\u0026#34;,$\u0026#34;data.event.info\u0026#34; as \u0026#34;event\u0026#34;,$\u0026#34;data.signal1s.info\u0026#34; as \u0026#34;signal1s\u0026#34;, $\u0026#34;data.signal30s.info\u0026#34; as \u0026#34;signal30s\u0026#34;) .select($\u0026#34;vin\u0026#34;, $\u0026#34;version\u0026#34;, $\u0026#34;tboxSn\u0026#34;, $\u0026#34;iccid\u0026#34;, $\u0026#34;createTime\u0026#34;, explode(array($\u0026#34;event\u0026#34;)) as \u0026#34;eventcol\u0026#34;, $\u0026#34;signal1s\u0026#34;, $\u0026#34;signal30s\u0026#34;) .select($\u0026#34;vin\u0026#34;, $\u0026#34;version\u0026#34;, $\u0026#34;tboxSn\u0026#34;, $\u0026#34;iccid\u0026#34;, $\u0026#34;createTime\u0026#34;, $\u0026#34;eventcol\u0026#34;, explode(array($\u0026#34;signal1s\u0026#34;)) as \u0026#34;signal1scol\u0026#34;, $\u0026#34;signal30s\u0026#34;) .select($\u0026#34;vin\u0026#34;, $\u0026#34;version\u0026#34;, $\u0026#34;tboxSn\u0026#34;, $\u0026#34;iccid\u0026#34;, $\u0026#34;createTime\u0026#34;, $\u0026#34;eventcol\u0026#34;, $\u0026#34;signal1scol\u0026#34;, explode(array($\u0026#34;signal30s\u0026#34;)) as \u0026#34;signal30scol\u0026#34;) .select($\u0026#34;vin\u0026#34;, $\u0026#34;version\u0026#34;, $\u0026#34;tboxSn\u0026#34;, $\u0026#34;iccid\u0026#34;, $\u0026#34;createTime\u0026#34;, $\u0026#34;eventcol.*\u0026#34;, $\u0026#34;signal1scol.*\u0026#34;, $\u0026#34;signal30scol.*\u0026#34;) .select($\u0026#34;vin\u0026#34;, $\u0026#34;createTime\u0026#34;, $\u0026#34;HU_TargetSOC\u0026#34; as \u0026#34;targetSoc\u0026#34; , $\u0026#34;ICU_ICUTotalOdometer\u0026#34; as \u0026#34;totalOdometer\u0026#34;) .filter($\u0026#34;vin\u0026#34;.isNotNull) .as[SourceCan] // parsed.printSchema() // event.printSchema() val console = event.writeStream .format(\u0026#34;console\u0026#34;) .option(\u0026#34;truncate\u0026#34;, \u0026#34;false\u0026#34;) .outputMode(OutputMode.Append()) val query = console.start() query.awaitTermination() } } "},"name":"Spark Structured Streaming 之 explode 多列","published":"2017-04-02T16:36:25Z","summary":"package ohmysummer import ohmysummer.model.SourceCan import ohmysummer.pipeline.kafka.WmKafkaDeserializer import ohmysummer.pipeline.schema.{CanSignalSchema, DcSaleData} import org.apache.spark.sql.{DataFrame, Dataset, SparkSession} import org.apache.spark.sql.streaming.OutputMode import org.apache.spark.sql.functions._ /** * 从 Kafka 读取 JSON 数据 * https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html * https://stackoverflow.com/questions/43297973/how-to-read-records-in-json-format-from-kafka-using-structured-streaming * https://stackoverflow.com/questions/48361177/spark-structured-streaming-kafka-convert-json-without-schema-infer-schema * https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html * https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html */ object WmDcSaleApplication { def main(args: Array[String]) { val spark = SparkSession .builder .appName(\u0026#34;ReadFromKafka\u0026#34;) .master(\u0026#34;local[2]\u0026#34;) .getOrCreate() object KafkaDeserializerWrapper { val deser = new WmKafkaDeserializer } spark.udf.register(\u0026#34;deserialize\u0026#34;, (topic: String, bytes: Array[Byte]) =\u0026gt; KafkaDeserializerWrapper.deser.deserialize(topic, bytes) ) val df = spark.","type":"entry","url":"https://ohmyweekly.github.io/notes/structured-spark-streaming---explode-%E5%A4%9A%E5%88%97/"}