{"author":{"name":null,"type":"card","url":"https://ohmycloud.github.io/"},"content":{"html":"\u003cp\u003e以下所有操作都是在 Master 上。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e停掉所有的 spark crontab 任务\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003estop crontab\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo rm /etc/cron.d/crontab_ald_spark\n\u003c/code\u003e\u003c/pre\u003e\u003col start=\"2\"\u003e\n\u003cli\u003e杀掉所有的正在运行的 spark 进程\nkill spark progress\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eps aux | grep \u0026#39;spark\u0026#39; | awk \u0026#39;{print $2}\u0026#39; | xargs kill -9\n\u003c/code\u003e\u003c/pre\u003e\u003col start=\"3\"\u003e\n\u003cli\u003e杀掉 Master(10.0.0.247)\nstop-master.sh\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003espark/sbin/stop-master.sh -h 10.0.0.247 -p 7077\n\u003c/code\u003e\u003c/pre\u003e\u003col start=\"4\"\u003e\n\u003cli\u003e停掉所有的 slaves\nstop-slaves.sh\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003espark/sbin/stop-slaves.sh -h 10.0.0.247 -p 7077\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e停掉 slaves,  注意是 s, 需要先做 Master 到各 slave 的免密码登陆, 即把 Master 下的  \u003ccode\u003e~/.ssh/id_rsa.pub\u003c/code\u003e 中的公钥复制到各 slave 下(用sudo su切换到root)的  ~/.ssh/authorized_keys 中。\u003c/p\u003e\n\u003cp\u003e在 Master 下的 \u003ccode\u003e/etc/hosts\u003c/code\u003e 文件中, 加上各 slave 的主机名:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e127.0.0.1  localhost  localhost.localdomain  VM-0-247-ubuntu\n10.0.0.51  VM-0-51-ubuntu\n10.0.0.71  VM-0-71-ubuntu\n10.0.0.134 VM-0-134-ubuntu\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e然后手动登陆各个 slave:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003essh VM-0-51-ubuntu\nssh VM-0-71-ubuntu\nssh VM-0-134-ubuntu\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e第一次免密码登陆会让你输入 YES 来确认,  以后就不用输入了。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e集群的启动：\u003c/p\u003e\n\u003cp\u003e启动 master:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003espark/sbin/start-master.sh -h 10.0.0.247 -p 7077\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e启动 slaves:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003espark/sbin/start-slaves.sh -h 10.0.0.247 -p 7077\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e启动 slaves 后, 在 web ui 界面并没有发现 Alive 的 slaves,  登陆到 slave 查看:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eps aux | grep \u0026#39;Worker\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e发现:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java -cp /data/app/ald_spark/spark/conf/:/data/app/ald_spark/spark/jars/* -Xmx2g -XX:MaxPermSize=256m org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://localhost:7077\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e发现 Worker 连接 localhost 作为 Master 了。\u003ccode\u003evim spark/sbin/start-slaves.sh\u003c/code\u003e 看到：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eif [ \u0026#34;$SPARK_MASTER_HOST\u0026#34; = \u0026#34;\u0026#34; ]; then\n  case `uname` in\n      (SunOS)\n          SPARK_MASTER_HOST=\u0026#34;`/usr/sbin/check-hostname | awk \u0026#39;{print $NF}\u0026#39;`\u0026#34;\n          ;;\n      (*)\n          SPARK_MASTER_HOST=\u0026#34;`hostname -f`\u0026#34;\n          ;;\n  esac\nfi\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e我们是 ubuntu 系统, 运行  \u003ccode\u003ehostname -f\u003c/code\u003e 得到 localhost, 所以 slave 自然连接到的也是 localhost。\u003c/p\u003e\n\u003cp\u003e有几种解决办法：\na) 直接在上面的代码中把 SPARK_MASTER_HOST 改为 10.0.0.247\nb) 在 \u003ccode\u003e~/.bashrc\u003c/code\u003e 或 \u003ccode\u003e~/.profile\u003c/code\u003e 文件中加入 SPARK_MASTER_HOST 变量\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eexport SPARK_MASTER_HOST=10.0.0.247\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ec) 自己在 spark 同级目录下创建一个 \u003ccode\u003estart-slaves.sh\u003c/code\u003e 文件， 文件中的内容如下：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#!/bin/bash\n. ./conf.sh\nspark/sbin/start-slaves.sh spark://$MASTER_HOST:7077\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e其中 conf.sh 的内容如下:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eMASTER_HOST=10.0.0.247\n\nif [ \u0026#34;$ALD_ENV\u0026#34;x = \u0026#34;test\u0026#34;x ]; then\n    MASTER_HOST=127.0.0.1;\nfi\n\necho \u0026#34;MASTER_HOST=$MASTER_HOST\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e然后运行  \u003ccode\u003e./start-slaves.sh\u003c/code\u003e 即可。\u003c/p\u003e\n\u003cp\u003eStandalone 模式下任务是 FIFO, first in, first out, 正在运行的任务会尽可能的使用所有可用资源（cpu/内存）, 后面的任务需要等到前面的任务执行完才开始, 如果任务过于密集, 会造成排队现象，　某些任务会　Ｗａｉｔｎｇ\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e参考 \u003ca href=\"http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts\"\u003ehttp://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","text":"以下所有操作都是在 Master 上。\n停掉所有的 spark crontab 任务 stop crontab\nsudo rm /etc/cron.d/crontab_ald_spark 杀掉所有的正在运行的 spark 进程 kill spark progress ps aux | grep \u0026#39;spark\u0026#39; | awk \u0026#39;{print $2}\u0026#39; | xargs kill -9 杀掉 Master(10.0.0.247) stop-master.sh spark/sbin/stop-master.sh -h 10.0.0.247 -p 7077 停掉所有的 slaves stop-slaves.sh spark/sbin/stop-slaves.sh -h 10.0.0.247 -p 7077 停掉 slaves, 注意是 s, 需要先做 Master 到各 slave 的免密码登陆, 即把 Master 下的 ~/.ssh/id_rsa.pub 中的公钥复制到各 slave 下(用sudo su切换到root)的 ~/.ssh/authorized_keys 中。\n在 Master 下的 /etc/hosts 文件中, 加上各 slave 的主机名:\n127.0.0.1 localhost localhost.localdomain VM-0-247-ubuntu 10.0.0.51 VM-0-51-ubuntu 10.0.0.71 VM-0-71-ubuntu 10.0.0.134 VM-0-134-ubuntu 然后手动登陆各个 slave:\nssh VM-0-51-ubuntu ssh VM-0-71-ubuntu ssh VM-0-134-ubuntu 第一次免密码登陆会让你输入 YES 来确认, 以后就不用输入了。\n集群的启动：\n启动 master:\nspark/sbin/start-master.sh -h 10.0.0.247 -p 7077 启动 slaves:\nspark/sbin/start-slaves.sh -h 10.0.0.247 -p 7077 启动 slaves 后, 在 web ui 界面并没有发现 Alive 的 slaves, 登陆到 slave 查看:\nps aux | grep \u0026#39;Worker\u0026#39; 发现:\n/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java -cp /data/app/ald_spark/spark/conf/:/data/app/ald_spark/spark/jars/* -Xmx2g -XX:MaxPermSize=256m org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://localhost:7077 发现 Worker 连接 localhost 作为 Master 了。vim spark/sbin/start-slaves.sh 看到：\nif [ \u0026#34;$SPARK_MASTER_HOST\u0026#34; = \u0026#34;\u0026#34; ]; then case `uname` in (SunOS) SPARK_MASTER_HOST=\u0026#34;`/usr/sbin/check-hostname | awk \u0026#39;{print $NF}\u0026#39;`\u0026#34; ;; (*) SPARK_MASTER_HOST=\u0026#34;`hostname -f`\u0026#34; ;; esac fi 我们是 ubuntu 系统, 运行 hostname -f 得到 localhost, 所以 slave 自然连接到的也是 localhost。\n有几种解决办法： a) 直接在上面的代码中把 SPARK_MASTER_HOST 改为 10.0.0.247 b) 在 ~/.bashrc 或 ~/.profile 文件中加入 SPARK_MASTER_HOST 变量\nexport SPARK_MASTER_HOST=10.0.0.247 c) 自己在 spark 同级目录下创建一个 start-slaves.sh 文件， 文件中的内容如下：\n#!/bin/bash . ./conf.sh spark/sbin/start-slaves.sh spark://$MASTER_HOST:7077 其中 conf.sh 的内容如下:\nMASTER_HOST=10.0.0.247 if [ \u0026#34;$ALD_ENV\u0026#34;x = \u0026#34;test\u0026#34;x ]; then MASTER_HOST=127.0.0.1; fi echo \u0026#34;MASTER_HOST=$MASTER_HOST\u0026#34; 然后运行 ./start-slaves.sh 即可。\nStandalone 模式下任务是 FIFO, first in, first out, 正在运行的任务会尽可能的使用所有可用资源（cpu/内存）, 后面的任务需要等到前面的任务执行完才开始, 如果任务过于密集, 会造成排队现象，　某些任务会　Ｗａｉｔｎｇ\n参考 http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts "},"name":"Standalone 模式下 Spark 集群的启动和停止","published":"2017-03-20T16:36:25Z","summary":"以下所有操作都是在 Master 上。\n停掉所有的 spark crontab 任务 stop crontab\nsudo rm /etc/cron.d/crontab_ald_spark 杀掉所有的正在运行的 spark 进程 kill spark progress ps aux | grep \u0026#39;spark\u0026#39; | awk \u0026#39;{print $2}\u0026#39; | xargs kill -9 杀掉 Master(10.0.0.247) stop-master.sh spark/sbin/stop-master.sh -h 10.0.0.247 -p 7077 停掉所有的 slaves stop-slaves.sh spark/sbin/stop-slaves.sh -h 10.0.0.247 -p 7077 停掉 slaves, 注意是 s, 需要先做 Master 到各 slave 的免密码登陆, 即把 Master 下的 ~/.ssh/id_rsa.pub 中的公钥复制到各 slave 下(用sudo su切换到root)的 ~/.ssh/authorized_keys 中。\n在 Master 下的 /etc/hosts 文件中, 加上各 slave 的主机名:","type":"entry","url":"https://ohmycloud.github.io/notes/standalone%E6%A8%A1%E5%BC%8F%E4%B8%8B%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E5%81%9C%E6%AD%A2/"}