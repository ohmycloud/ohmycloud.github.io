{"author":{"name":null,"type":"card","url":"https://ohmyweekly.github.io/"},"content":{"html":"\u003cp\u003e使用 Spark Structured Streaming 消费 Kafka 数据并实时保存为 parquet 文件时, 出现一个问题, 代码如下:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-scala\" data-lang=\"scala\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003eres\u003c/span\u003e\u003cspan class=\"k\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eStreamingQuery\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eadapterData\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erepartition\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esparkConf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enumOfPartitions\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewriteStream\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eformat\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;parquet\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoption\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;path\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;/parquet_data/enterprise/\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epartitionBy\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;vintype\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;dt\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoutputMode\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eOutputMode\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nc\"\u003eAppend\u003c/span\u003e\u003cspan class=\"o\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etrigger\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eTrigger\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nc\"\u003eProcessingTime\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;1200 seconds\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003equeryName\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;enterprise kafka data saving as parquet\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"o\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e问题：\u003c/p\u003e\n\u003cp\u003e在线上运行一段时间后, 发现 \u003cstrong\u003e/parquet_data/enterprise/\u003c/strong\u003e 目录下不再有新的文件追加了, 查日志没发现原因。隔一段时间就出现一次。临时解决办法是删除并重建 \u003cstrong\u003e_spark_metadata\u003c/strong\u003e 目录和 checkpoint 目录。\n这样会丢数据。\u003c/p\u003e\n","text":"使用 Spark Structured Streaming 消费 Kafka 数据并实时保存为 parquet 文件时, 出现一个问题, 代码如下:\nval res: StreamingQuery = adapterData .repartition(sparkConf.numOfPartitions) .writeStream .format(\u0026#34;parquet\u0026#34;) .option(\u0026#34;path\u0026#34;, \u0026#34;/parquet_data/enterprise/\u0026#34;) .partitionBy(\u0026#34;vintype\u0026#34;, \u0026#34;dt\u0026#34;) .outputMode(OutputMode.Append()) .trigger(Trigger.ProcessingTime(\u0026#34;1200 seconds\u0026#34;)) .queryName(\u0026#34;enterprise kafka data saving as parquet\u0026#34;) .start() 问题：\n在线上运行一段时间后, 发现 /parquet_data/enterprise/ 目录下不再有新的文件追加了, 查日志没发现原因。隔一段时间就出现一次。临时解决办法是删除并重建 _spark_metadata 目录和 checkpoint 目录。 这样会丢数据。\n"},"name":"parquet-format-not-work-in-spark-structured-streaming","published":"2020-02-26T14:48:38Z","summary":"使用 Spark Structured Streaming 消费 Kafka 数据并实时保存为 parquet 文件时, 出现一个问题, 代码如下:\nval res: StreamingQuery = adapterData .repartition(sparkConf.numOfPartitions) .writeStream .format(\u0026#34;parquet\u0026#34;) .option(\u0026#34;path\u0026#34;, \u0026#34;/parquet_data/enterprise/\u0026#34;) .partitionBy(\u0026#34;vintype\u0026#34;, \u0026#34;dt\u0026#34;) .outputMode(OutputMode.Append()) .trigger(Trigger.ProcessingTime(\u0026#34;1200 seconds\u0026#34;)) .queryName(\u0026#34;enterprise kafka data saving as parquet\u0026#34;) .start() 问题：\n在线上运行一段时间后, 发现 /parquet_data/enterprise/ 目录下不再有新的文件追加了, 查日志没发现原因。隔一段时间就出现一次。临时解决办法是删除并重建 _spark_metadata 目录和 checkpoint 目录。 这样会丢数据。","type":"entry","url":"https://ohmyweekly.github.io/notes/parquet-format-not-work-in-spark-structured-streaming/"}