{"author":{"name":null,"type":"card","url":"https://ohmycloud.github.io/"},"content":{"html":"\u003ch1 id=\"hive-读写\"\u003eHive 读写\u003c/h1\u003e\n\u003cp\u003e使用 HiveCatalog 和 Flink 与 Hive 的连接器，Flink 可以从 Hive 数据中读取和写入数据，作为 Hive 批处理引擎的替代。请务必按照说明在你的应用中加入正确的\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#depedencies\"\u003e依赖关系\u003c/a\u003e。同时请注意，Hive 连接器只适用于 blink planner。\u003c/p\u003e\n\u003ch2 id=\"从-hive-读取数据\"\u003e从 Hive 读取数据\u003c/h2\u003e\n\u003cp\u003e假设 Hive 在其默认的数据库中包含一个名为 people 的单表，该表包含多条记录。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehive\u0026gt; show databases\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eOK\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edefault\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eTime taken: 0.841 seconds, Fetched: \u003cspan class=\"m\"\u003e1\u003c/span\u003e row\u003cspan class=\"o\"\u003e(\u003c/span\u003es\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehive\u0026gt; show tables\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eOK\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eTime taken: 0.087 seconds\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehive\u0026gt; CREATE TABLE mytable\u003cspan class=\"o\"\u003e(\u003c/span\u003ename string, value double\u003cspan class=\"o\"\u003e)\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eOK\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eTime taken: 0.127 seconds\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehive\u0026gt; SELECT * FROM mytable\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eOK\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eTom   4.72\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eJohn  8.0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eTom   24.2\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eBob   3.14\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eBob   4.72\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eTom   34.9\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eMary  4.79\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eTiff  2.72\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eBill  4.33\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eMary  77.7\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eTime taken: 0.097 seconds, Fetched: \u003cspan class=\"m\"\u003e10\u003c/span\u003e row\u003cspan class=\"o\"\u003e(\u003c/span\u003es\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e数据准备好后，你可以\u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#connecting-to-hive\"\u003e连接到现有的 Hive 安装\u003c/a\u003e并开始查询。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; show catalogs\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emyhive\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edefault_catalog\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ------ Set the current catalog to be \u0026#39;myhive\u0026#39; catalog if you haven\u0026#39;t set it in the yaml file ------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; use catalog myhive\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ------ See all registered database in catalog \u0026#39;mytable\u0026#39; ------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; show databases\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edefault\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ------ See the previously registered table \u0026#39;mytable\u0026#39; ------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; show tables\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emytable\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ------ The table schema that Flink sees is the same that we created in Hive, two columns - name as string and value as double ------ \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; describe mytable\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eroot\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e \u003cspan class=\"p\"\u003e|\u003c/span\u003e-- name: name\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e \u003cspan class=\"p\"\u003e|\u003c/span\u003e-- type: STRING\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e \u003cspan class=\"p\"\u003e|\u003c/span\u003e-- name: value\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e \u003cspan class=\"p\"\u003e|\u003c/span\u003e-- type: DOUBLE\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ------ Select from hive table or hive view ------ \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; SELECT * FROM mytable\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e   name      value\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e__________ __________\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Tom      4.72\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    John     8.0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Tom      24.2\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Bob      3.14\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Bob      4.72\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Tom      34.9\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Mary     4.79\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Tiff     2.72\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Bill     4.33\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Mary     77.7\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"查询-hive-视图\"\u003e查询 Hive 视图\u003c/h3\u003e\n\u003cp\u003e如果你需要查询 Hive 视图，请注意。\u003c/p\u003e\n\u003cp\u003e在查询该目录中的视图之前，必须先使用 Hive 目录作为当前目录。可以通过 Table API 中的 \u003ccode\u003etableEnv.useCatalog(...)\u003c/code\u003e 或者 SQL Client 中的 USE CATALOG \u0026hellip;来实现。\nHive 和 Flink SQL 有不同的语法，例如，不同的保留关键字和字元。请确保视图的查询与 Flink 语法兼容。\u003c/p\u003e\n\u003ch2 id=\"写入-hive\"\u003e写入 Hive\u003c/h2\u003e\n\u003cp\u003e同样，也可以使用 INSERT 子句将数据写入 hive 中。\u003c/p\u003e\n\u003cp\u003e考虑有一个名为 \u0026ldquo;mytable \u0026ldquo;的示例表，表中有两列：name 和 age，类型为 string 和 int。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ------ INSERT INTO will append to the table or partition, keeping the existing data intact ------ \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; INSERT INTO mytable SELECT \u003cspan class=\"s1\"\u003e\u0026#39;Tom\u0026#39;\u003c/span\u003e, 25\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ------ INSERT OVERWRITE will overwrite any existing data in the table or partition ------ \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; INSERT OVERWRITE mytable SELECT \u003cspan class=\"s1\"\u003e\u0026#39;Tom\u0026#39;\u003c/span\u003e, 25\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e我们也支持分区表，考虑有一个名为 myparttable 的分区表，有四列：name、age、my_type 和 my_date，在 type 中\u0026hellip;\u003ccode\u003emy_type\u003c/code\u003e 和 \u003ccode\u003emy_date\u003c/code\u003e 是分区键。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ------ Insert with static partition ------ \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION \u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003emy_type\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;type_1\u0026#39;\u003c/span\u003e, \u003cspan class=\"nv\"\u003emy_date\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;2019-08-08\u0026#39;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e SELECT \u003cspan class=\"s1\"\u003e\u0026#39;Tom\u0026#39;\u003c/span\u003e, 25\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ------ Insert with dynamic partition ------ \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; INSERT OVERWRITE myparttable SELECT \u003cspan class=\"s1\"\u003e\u0026#39;Tom\u0026#39;\u003c/span\u003e, 25, \u003cspan class=\"s1\"\u003e\u0026#39;type_1\u0026#39;\u003c/span\u003e, \u003cspan class=\"s1\"\u003e\u0026#39;2019-08-08\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ------ Insert with static(my_type) and dynamic(my_date) partition ------ \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFlink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION \u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003emy_type\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;type_1\u0026#39;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e SELECT \u003cspan class=\"s1\"\u003e\u0026#39;Tom\u0026#39;\u003c/span\u003e, 25, \u003cspan class=\"s1\"\u003e\u0026#39;2019-08-08\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"格式\"\u003e格式\u003c/h2\u003e\n\u003cp\u003e我们测试了以下表格存储格式：文本、csv、SequenceFile、ORC 和 Parquet。\u003c/p\u003e\n\u003ch2 id=\"优化\"\u003e优化\u003c/h2\u003e\n\u003ch3 id=\"分区修剪\"\u003e分区修剪\u003c/h3\u003e\n\u003cp\u003eFlink 使用分区修剪作为一种性能优化，以限制 Flink 在查询 Hive 表时读取的文件和分区的数量。当你的数据被分区后，当查询符合某些过滤条件时，Flink 只会读取 Hive 表中的分区子集。\u003c/p\u003e\n\u003ch3 id=\"投影下推\"\u003e投影下推\u003c/h3\u003e\n\u003cp\u003eFlink 利用投影下推，通过从表扫描中省略不必要的字段，最大限度地减少 Flink 和 Hive 表之间的数据传输。\u003c/p\u003e\n\u003cp\u003e当一个表包含许多列时，它尤其有利。\u003c/p\u003e\n\u003ch3 id=\"限制下推\"\u003e限制下推\u003c/h3\u003e\n\u003cp\u003e对于带有 LIMIT 子句的查询，Flink 会尽可能地限制输出记录的数量，以减少跨网络传输的数据量。\u003c/p\u003e\n\u003ch3 id=\"读取时的向量优化\"\u003e读取时的向量优化\u003c/h3\u003e\n\u003cp\u003e当满足以下条件时，会自动使用优化功能。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e格式： ORC 或 Parquet。\u003c/li\u003e\n\u003cli\u003e没有复杂数据类型的列，如 hive 类型: List, Map, Struct, Union。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这个功能默认是开启的。如果出现问题，可以使用这个配置选项来关闭 Vectorized Optimization。\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003etable.exec.hive.fallback-mapred-reader=true\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"source-并行性推断\"\u003eSource 并行性推断\u003c/h3\u003e\n\u003cp\u003e默认情况下，Flink 根据分割次数来推断 hive 源的并行度，分割次数是根据文件的数量和文件中的块数来推断的。\u003c/p\u003e\n\u003cp\u003eFlink 允许你灵活配置并行度推断的策略。你可以在 TableConfig 中配置以下参数（注意，这些参数会影响作业的所有源）。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align:left\"\u003eKey\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eDefault\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eType\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003etable.exec.hive.infer-source-parallelism\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003etrue\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eBoolean\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e如果为真，则根据分割数来推断源的并行度，如果为假，则根据配置来设置源的并行度。如果为 false，则通过配置来设置源的并行度。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003etable.exec.hive.infer-source-parallelism.max\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e1000\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eInteger\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e设置源运算符的最大推断并行度。\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"路线图\"\u003e路线图\u003c/h2\u003e\n\u003cp\u003e我们正在规划并积极开发支持功能，如:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eACID 表\u003c/li\u003e\n\u003cli\u003e分桶表\u003c/li\u003e\n\u003cli\u003e更多格式\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e更多功能需求请联系社区 \u003ca href=\"https://flink.apache.org/community.html#mailing-lists\"\u003ehttps://flink.apache.org/community.html#mailing-lists\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e原文链接: \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_read_write.html\"\u003ehttps://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_read_write.html\u003c/a\u003e\u003c/p\u003e\n","text":"Hive 读写 使用 HiveCatalog 和 Flink 与 Hive 的连接器，Flink 可以从 Hive 数据中读取和写入数据，作为 Hive 批处理引擎的替代。请务必按照说明在你的应用中加入正确的依赖关系。同时请注意，Hive 连接器只适用于 blink planner。\n从 Hive 读取数据 假设 Hive 在其默认的数据库中包含一个名为 people 的单表，该表包含多条记录。\nhive\u0026gt; show databases; OK default Time taken: 0.841 seconds, Fetched: 1 row(s) hive\u0026gt; show tables; OK Time taken: 0.087 seconds hive\u0026gt; CREATE TABLE mytable(name string, value double); OK Time taken: 0.127 seconds hive\u0026gt; SELECT * FROM mytable; OK Tom 4.72 John 8.0 Tom 24.2 Bob 3.14 Bob 4.72 Tom 34.9 Mary 4.79 Tiff 2.72 Bill 4.33 Mary 77.7 Time taken: 0.097 seconds, Fetched: 10 row(s) 数据准备好后，你可以连接到现有的 Hive 安装并开始查询。\nFlink SQL\u0026gt; show catalogs; myhive default_catalog # ------ Set the current catalog to be \u0026#39;myhive\u0026#39; catalog if you haven\u0026#39;t set it in the yaml file ------ Flink SQL\u0026gt; use catalog myhive; # ------ See all registered database in catalog \u0026#39;mytable\u0026#39; ------ Flink SQL\u0026gt; show databases; default # ------ See the previously registered table \u0026#39;mytable\u0026#39; ------ Flink SQL\u0026gt; show tables; mytable # ------ The table schema that Flink sees is the same that we created in Hive, two columns - name as string and value as double ------ Flink SQL\u0026gt; describe mytable; root |-- name: name |-- type: STRING |-- name: value |-- type: DOUBLE # ------ Select from hive table or hive view ------ Flink SQL\u0026gt; SELECT * FROM mytable; name value __________ __________ Tom 4.72 John 8.0 Tom 24.2 Bob 3.14 Bob 4.72 Tom 34.9 Mary 4.79 Tiff 2.72 Bill 4.33 Mary 77.7 查询 Hive 视图 如果你需要查询 Hive 视图，请注意。\n在查询该目录中的视图之前，必须先使用 Hive 目录作为当前目录。可以通过 Table API 中的 tableEnv.useCatalog(...) 或者 SQL Client 中的 USE CATALOG \u0026hellip;来实现。 Hive 和 Flink SQL 有不同的语法，例如，不同的保留关键字和字元。请确保视图的查询与 Flink 语法兼容。\n写入 Hive 同样，也可以使用 INSERT 子句将数据写入 hive 中。\n考虑有一个名为 \u0026ldquo;mytable \u0026ldquo;的示例表，表中有两列：name 和 age，类型为 string 和 int。\n# ------ INSERT INTO will append to the table or partition, keeping the existing data intact ------ Flink SQL\u0026gt; INSERT INTO mytable SELECT \u0026#39;Tom\u0026#39;, 25; # ------ INSERT OVERWRITE will overwrite any existing data in the table or partition ------ Flink SQL\u0026gt; INSERT OVERWRITE mytable SELECT \u0026#39;Tom\u0026#39;, 25; 我们也支持分区表，考虑有一个名为 myparttable 的分区表，有四列：name、age、my_type 和 my_date，在 type 中\u0026hellip;my_type 和 my_date 是分区键。\n# ------ Insert with static partition ------ Flink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION (my_type=\u0026#39;type_1\u0026#39;, my_date=\u0026#39;2019-08-08\u0026#39;) SELECT \u0026#39;Tom\u0026#39;, 25; # ------ Insert with dynamic partition ------ Flink SQL\u0026gt; INSERT OVERWRITE myparttable SELECT \u0026#39;Tom\u0026#39;, 25, \u0026#39;type_1\u0026#39;, \u0026#39;2019-08-08\u0026#39;; # ------ Insert with static(my_type) and dynamic(my_date) partition ------ Flink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION (my_type=\u0026#39;type_1\u0026#39;) SELECT \u0026#39;Tom\u0026#39;, 25, \u0026#39;2019-08-08\u0026#39;; 格式 我们测试了以下表格存储格式：文本、csv、SequenceFile、ORC 和 Parquet。\n优化 分区修剪 Flink 使用分区修剪作为一种性能优化，以限制 Flink 在查询 Hive 表时读取的文件和分区的数量。当你的数据被分区后，当查询符合某些过滤条件时，Flink 只会读取 Hive 表中的分区子集。\n投影下推 Flink 利用投影下推，通过从表扫描中省略不必要的字段，最大限度地减少 Flink 和 Hive 表之间的数据传输。\n当一个表包含许多列时，它尤其有利。\n限制下推 对于带有 LIMIT 子句的查询，Flink 会尽可能地限制输出记录的数量，以减少跨网络传输的数据量。\n读取时的向量优化 当满足以下条件时，会自动使用优化功能。\n格式： ORC 或 Parquet。 没有复杂数据类型的列，如 hive 类型: List, Map, Struct, Union。 这个功能默认是开启的。如果出现问题，可以使用这个配置选项来关闭 Vectorized Optimization。\ntable.exec.hive.fallback-mapred-reader=true Source 并行性推断 默认情况下，Flink 根据分割次数来推断 hive 源的并行度，分割次数是根据文件的数量和文件中的块数来推断的。\nFlink 允许你灵活配置并行度推断的策略。你可以在 TableConfig 中配置以下参数（注意，这些参数会影响作业的所有源）。\nKey Default Type Description table.exec.hive.infer-source-parallelism true Boolean 如果为真，则根据分割数来推断源的并行度，如果为假，则根据配置来设置源的并行度。如果为 false，则通过配置来设置源的并行度。 table.exec.hive.infer-source-parallelism.max 1000 Integer 设置源运算符的最大推断并行度。 路线图 我们正在规划并积极开发支持功能，如:\nACID 表 分桶表 更多格式 更多功能需求请联系社区 https://flink.apache.org/community.html#mailing-lists\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_read_write.html\n"},"name":"Hive Read and Write","published":"2020-08-25T00:00:00Z","summary":"Hive Read and Write","type":"entry","url":"https://ohmycloud.github.io/notes/2020-08-25-hive-read-and-write/"}