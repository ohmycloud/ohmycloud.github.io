{"author":{"name":null,"type":"card","url":"https://ohmyweekly.github.io/"},"content":{"html":"\u003ch1 id=\"hive-集成\"\u003eHive 集成\u003c/h1\u003e\n\u003cp\u003eApache Hive 已经确立了自己作为数据仓库生态系统的焦点。它不仅是大数据分析和 ETL 的 SQL 引擎，也是一个数据管理平台，在这里，数据被发现、定义和发展。\u003c/p\u003e\n\u003cp\u003eFlink 与 Hive 提供了两方面的整合。\u003c/p\u003e\n\u003cp\u003e第一是利用 Hive 的 Metastore 作为一个持久性目录，与 Flink 的 HiveCatalog 进行跨会话存储 Flink 特定的元数据。例如，用户可以通过使用 HiveCatalog 将 Kafka 或 ElasticSearch 表存储在 Hive Metastore 中，并在以后的 SQL 查询中重复使用。\u003c/p\u003e\n\u003cp\u003e二是提供 Flink 作为读写 Hive 表的替代引擎。\u003c/p\u003e\n\u003cp\u003eHiveCatalog 的设计是 \u0026ldquo;开箱即用\u0026rdquo;，与现有的 Hive 安装兼容。您不需要修改现有的 Hive Metastore，也不需要改变数据位置或表的分区。\u003c/p\u003e\n\u003ch2 id=\"支持的-hive-版本\"\u003e支持的 Hive 版本\u003c/h2\u003e\n\u003cp\u003eFlink 支持以下 Hive 版本。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1.0\n\u003cul\u003e\n\u003cli\u003e1.0.0\u003c/li\u003e\n\u003cli\u003e1.0.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e1.1\n\u003cul\u003e\n\u003cli\u003e1.1.0\u003c/li\u003e\n\u003cli\u003e1.1.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e1.2\n\u003cul\u003e\n\u003cli\u003e1.2.0\u003c/li\u003e\n\u003cli\u003e1.2.1\u003c/li\u003e\n\u003cli\u003e1.2.2\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2.0\n\u003cul\u003e\n\u003cli\u003e2.0.0\u003c/li\u003e\n\u003cli\u003e2.0.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2.1\n\u003cul\u003e\n\u003cli\u003e2.1.0\u003c/li\u003e\n\u003cli\u003e2.1.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2.2\n\u003cul\u003e\n\u003cli\u003e2.2.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2.3\n\u003cul\u003e\n\u003cli\u003e2.3.0\u003c/li\u003e\n\u003cli\u003e2.3.1\u003c/li\u003e\n\u003cli\u003e2.3.2\u003c/li\u003e\n\u003cli\u003e2.3.3\u003c/li\u003e\n\u003cli\u003e2.3.4\u003c/li\u003e\n\u003cli\u003e2.3.5\u003c/li\u003e\n\u003cli\u003e2.3.6\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e3.1\n\u003cul\u003e\n\u003cli\u003e3.1.0\u003c/li\u003e\n\u003cli\u003e3.1.1\u003c/li\u003e\n\u003cli\u003e3.1.2\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e请注意 Hive 本身在不同的版本有不同的功能，这些问题不是 Flink 造成的。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1.2.0 及以后版本支持 Hive 内置函数。\u003c/li\u003e\n\u003cli\u003e3.1.0 及以后版本支持列约束，即 PRIMARY KEY 和 NOT NULL。\u003c/li\u003e\n\u003cli\u003e在 1.2.0 及以后的版本中，支持修改表的统计数据。\u003c/li\u003e\n\u003cli\u003e在 1.2.0 及以后的版本中支持 DATE 列统计。\u003c/li\u003e\n\u003cli\u003e在 2.0.x 中不支持写入 ORC 表。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"依赖性\"\u003e依赖性\u003c/h3\u003e\n\u003cp\u003e为了与 Hive 集成，你需要在 Flink 发行版的 \u003ccode\u003e/lib/\u003c/code\u003e 目录下添加一些额外的依赖关系，以使集成工作在 Table API 程序或 SQL 客户端中。另外，你也可以将这些依赖项放在一个专门的文件夹中，并分别用 \u003ccode\u003e-C\u003c/code\u003e 或 \u003ccode\u003e-l\u003c/code\u003e 选项将它们添加到 \u003ccode\u003eclasspath\u003c/code\u003e 中，用于 Table API 程序或 SQL Client。\u003c/p\u003e\n\u003cp\u003eApache Hive 是建立在 Hadoop 上的，所以首先需要 Hadoop 依赖，请参考提供 Hadoop 类。\u003c/p\u003e\n\u003cp\u003e有两种方法可以添加 Hive 依赖。首先是使用 Flink 的捆绑式 Hive jars。你可以根据你使用的 metastore 的版本来选择捆绑的 Hive jar。第二种是分别添加每个所需的 jar。如果你使用的 Hive 版本没有在这里列出，第二种方式就会很有用。\u003c/p\u003e\n\u003ch4 id=\"使用捆绑的-hive-jar\"\u003e使用捆绑的 Hive jar\u003c/h4\u003e\n\u003cp\u003e下表列出了所有可用的捆绑的 hive jar，你可以选择一个到 Flink 发行版的 \u003ccode\u003e/lib/\u003c/code\u003e 目录下。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align:left\"\u003eMetastore version\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eMaven dependency\u003c/th\u003e\n\u003cth style=\"text-align:left\"\u003eSQL Client JAR\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e1.0.0 - 1.2.2\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eflink-sql-connector-hive-1.2.2\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003ca href=\"https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-1.2.2_2.11/1.11.0/flink-sql-connector-hive-1.2.2_2.11-1.11.0.jar\"\u003eDownload\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e2.0.0 - 2.2.0\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eflink-sql-connector-hive-2.2.0\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003ca href=\"https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-2.2.0_2.11/1.11.0/flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar\"\u003eDownload\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e2.3.0 - 2.3.6\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eflink-sql-connector-hive-2.3.6\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003ca href=\"https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-2.3.6_2.11/1.11.0/flink-sql-connector-hive-2.3.6_2.11-1.11.0.jar\"\u003eDownload\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e3.0.0 - 3.1.2\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003eflink-sql-connector-hive-3.1.2\u003c/td\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003ca href=\"https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-3.1.2_2.11/1.11.0/flink-sql-connector-hive-3.1.2_2.11-1.11.0.jar\"\u003eDownload\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch4 id=\"用户定义的依赖性\"\u003e用户定义的依赖性\u003c/h4\u003e\n\u003cp\u003e请在下面找到不同 Hive 主要版本所需的依赖关系。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHive 2.3.4\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink\u0026#39;s Hive connector.Contains flink-hadoop-compatibility and flink-orc jars\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-exec-2.3.4.jar\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 1.0.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink\u0026#39;s Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-metastore-1.0.0.jar\n       hive-exec-1.0.0.jar\n       libfb303-0.9.0.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately\n       \n       // Orc dependencies -- required by the ORC vectorized optimizations\n       orc-core-1.4.3-nohive.jar\n       aircompressor-0.8.jar // transitive dependency of orc-core\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 1.1.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink\u0026#39;s Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-metastore-1.1.0.jar\n       hive-exec-1.1.0.jar\n       libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately\n\n       // Orc dependencies -- required by the ORC vectorized optimizations\n       orc-core-1.4.3-nohive.jar\n       aircompressor-0.8.jar // transitive dependency of orc-core\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 1.2.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink\u0026#39;s Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-metastore-1.2.1.jar\n       hive-exec-1.2.1.jar\n       libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately\n\n       // Orc dependencies -- required by the ORC vectorized optimizations\n       orc-core-1.4.3-nohive.jar\n       aircompressor-0.8.jar // transitive dependency of orc-core\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 2.0.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink\u0026#39;s Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-exec-2.0.0.jar\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 2.1.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink\u0026#39;s Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-exec-2.1.0.jar\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 2.2.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink\u0026#39;s Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-exec-2.2.0.jar\n\n       // Orc dependencies -- required by the ORC vectorized optimizations\n       orc-core-1.4.3.jar\n       aircompressor-0.8.jar // transitive dependency of orc-core\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eHive 3.1.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e/flink-1.11.0\n   /lib\n\n       // Flink\u0026#39;s Hive connector\n       flink-connector-hive_2.11-1.11.0.jar\n\n       // Hive dependencies\n       hive-exec-3.1.0.jar\n       libfb303-0.9.3.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"program-maven\"\u003eProgram maven\u003c/h3\u003e\n\u003cp\u003e如果你正在构建你自己的程序，你需要在你的 mvn 文件中加入以下依赖关系。建议不要在生成的 jar 文件中包含这些依赖关系。你应该在运行时添加上面所说的依赖关系。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-xml\" data-lang=\"xml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c\"\u003e\u0026lt;!-- Flink Dependency --\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.flink\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003eflink-connector-hive_2.11\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e1.11.0\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.flink\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003eflink-table-api-java-bridge_2.11\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e1.11.0\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c\"\u003e\u0026lt;!-- Hive Dependency --\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026lt;groupId\u0026gt;\u003c/span\u003eorg.apache.hive\u003cspan class=\"nt\"\u003e\u0026lt;/groupId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026lt;artifactId\u0026gt;\u003c/span\u003ehive-exec\u003cspan class=\"nt\"\u003e\u0026lt;/artifactId\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026lt;version\u0026gt;\u003c/span\u003e${hive.version}\u003cspan class=\"nt\"\u003e\u0026lt;/version\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026lt;scope\u0026gt;\u003c/span\u003eprovided\u003cspan class=\"nt\"\u003e\u0026lt;/scope\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003e\u0026lt;/dependency\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"连接到-hive\"\u003e连接到 Hive\u003c/h2\u003e\n\u003cp\u003e通过表环境或 YAML 配置，使用目录接口和 HiveCatalog 连接到现有的 Hive 安装。\u003c/p\u003e\n\u003cp\u003e如果 \u003ccode\u003ehive-conf/hive-site.xml\u003c/code\u003e 文件存储在远程存储系统中，用户应先将 hive 配置文件下载到本地环境中。\u003c/p\u003e\n\u003cp\u003e请注意，虽然 HiveCatalog 不需要特定的规划师，但读/写 Hive 表只适用于 blink 规划师。因此强烈建议您在连接 Hive 仓库时使用 blink planner。\u003c/p\u003e\n\u003cp\u003eHiveCatalog 能够自动检测使用中的 Hive 版本。建议不要指定 Hive 版本，除非自动检测失败。\u003c/p\u003e\n\u003cp\u003e以 Hive 2.3.4 版本为例。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-scala\" data-lang=\"scala\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003esettings\u003c/span\u003e \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eEnvironmentSettings\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enewInstance\u003c/span\u003e\u003cspan class=\"o\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003einBatchMode\u003c/span\u003e\u003cspan class=\"o\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003ebuild\u003c/span\u003e\u003cspan class=\"o\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003etableEnv\u003c/span\u003e \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eTableEnvironment\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esettings\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e            \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;myhive\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003edefaultDatabase\u003c/span\u003e \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;mydatabase\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003ehiveConfDir\u003c/span\u003e     \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026#34;/opt/hive-conf\u0026#34;\u003c/span\u003e \u003cspan class=\"c1\"\u003e// a local path\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eval\u003c/span\u003e \u003cspan class=\"n\"\u003ehive\u003c/span\u003e \u003cspan class=\"k\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nc\"\u003eHiveCatalog\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"o\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edefaultDatabase\u003c/span\u003e\u003cspan class=\"o\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ehiveConfDir\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etableEnv\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eregisterCatalog\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;myhive\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ehive\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// set the HiveCatalog as the current catalog of the session\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003etableEnv\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003euseCatalog\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;myhive\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ddl\"\u003eDDL\u003c/h2\u003e\n\u003cp\u003e建议使用 \u003ca href=\"https://ohmyweekly.github.io/notes/2020-08-25-hive-dialect\"\u003eHive 方言\u003c/a\u003e在 Flink 中执行 DDL 来创建 Hive 表、视图、分区、函数。\u003c/p\u003e\n\u003ch2 id=\"dml\"\u003eDML\u003c/h2\u003e\n\u003cp\u003eFlink 支持 DML 写入 Hive 表。请参考\u003ca href=\"https://ohmyweekly.github.io/notes/2020-08-25-hive-read-and-write\"\u003e读写 Hive 表\u003c/a\u003e的细节。\u003c/p\u003e\n\u003cp\u003e原文链接: \u003ca href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/\"\u003ehttps://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/\u003c/a\u003e\u003c/p\u003e\n","text":"Hive 集成 Apache Hive 已经确立了自己作为数据仓库生态系统的焦点。它不仅是大数据分析和 ETL 的 SQL 引擎，也是一个数据管理平台，在这里，数据被发现、定义和发展。\nFlink 与 Hive 提供了两方面的整合。\n第一是利用 Hive 的 Metastore 作为一个持久性目录，与 Flink 的 HiveCatalog 进行跨会话存储 Flink 特定的元数据。例如，用户可以通过使用 HiveCatalog 将 Kafka 或 ElasticSearch 表存储在 Hive Metastore 中，并在以后的 SQL 查询中重复使用。\n二是提供 Flink 作为读写 Hive 表的替代引擎。\nHiveCatalog 的设计是 \u0026ldquo;开箱即用\u0026rdquo;，与现有的 Hive 安装兼容。您不需要修改现有的 Hive Metastore，也不需要改变数据位置或表的分区。\n支持的 Hive 版本 Flink 支持以下 Hive 版本。\n1.0 1.0.0 1.0.1 1.1 1.1.0 1.1.1 1.2 1.2.0 1.2.1 1.2.2 2.0 2.0.0 2.0.1 2.1 2.1.0 2.1.1 2.2 2.2.0 2.3 2.3.0 2.3.1 2.3.2 2.3.3 2.3.4 2.3.5 2.3.6 3.1 3.1.0 3.1.1 3.1.2 请注意 Hive 本身在不同的版本有不同的功能，这些问题不是 Flink 造成的。\n1.2.0 及以后版本支持 Hive 内置函数。 3.1.0 及以后版本支持列约束，即 PRIMARY KEY 和 NOT NULL。 在 1.2.0 及以后的版本中，支持修改表的统计数据。 在 1.2.0 及以后的版本中支持 DATE 列统计。 在 2.0.x 中不支持写入 ORC 表。 依赖性 为了与 Hive 集成，你需要在 Flink 发行版的 /lib/ 目录下添加一些额外的依赖关系，以使集成工作在 Table API 程序或 SQL 客户端中。另外，你也可以将这些依赖项放在一个专门的文件夹中，并分别用 -C 或 -l 选项将它们添加到 classpath 中，用于 Table API 程序或 SQL Client。\nApache Hive 是建立在 Hadoop 上的，所以首先需要 Hadoop 依赖，请参考提供 Hadoop 类。\n有两种方法可以添加 Hive 依赖。首先是使用 Flink 的捆绑式 Hive jars。你可以根据你使用的 metastore 的版本来选择捆绑的 Hive jar。第二种是分别添加每个所需的 jar。如果你使用的 Hive 版本没有在这里列出，第二种方式就会很有用。\n使用捆绑的 Hive jar 下表列出了所有可用的捆绑的 hive jar，你可以选择一个到 Flink 发行版的 /lib/ 目录下。\nMetastore version Maven dependency SQL Client JAR 1.0.0 - 1.2.2 flink-sql-connector-hive-1.2.2 Download 2.0.0 - 2.2.0 flink-sql-connector-hive-2.2.0 Download 2.3.0 - 2.3.6 flink-sql-connector-hive-2.3.6 Download 3.0.0 - 3.1.2 flink-sql-connector-hive-3.1.2 Download 用户定义的依赖性 请在下面找到不同 Hive 主要版本所需的依赖关系。\nHive 2.3.4 /flink-1.11.0 /lib // Flink\u0026#39;s Hive connector.Contains flink-hadoop-compatibility and flink-orc jars flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.3.4.jar Hive 1.0.0 /flink-1.11.0 /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-metastore-1.0.0.jar hive-exec-1.0.0.jar libfb303-0.9.0.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core Hive 1.1.0 /flink-1.11.0 /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-metastore-1.1.0.jar hive-exec-1.1.0.jar libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core Hive 1.2.1 /flink-1.11.0 /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-metastore-1.2.1.jar hive-exec-1.2.1.jar libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core Hive 2.0.0 /flink-1.11.0 /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.0.0.jar Hive 2.1.0 /flink-1.11.0 /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.1.0.jar Hive 2.2.0 /flink-1.11.0 /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-2.2.0.jar // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3.jar aircompressor-0.8.jar // transitive dependency of orc-core Hive 3.1.0 /flink-1.11.0 /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.11-1.11.0.jar // Hive dependencies hive-exec-3.1.0.jar libfb303-0.9.3.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately Program maven 如果你正在构建你自己的程序，你需要在你的 mvn 文件中加入以下依赖关系。建议不要在生成的 jar 文件中包含这些依赖关系。你应该在运行时添加上面所说的依赖关系。\n\u0026lt;!-- Flink Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-hive_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.11.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Hive Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hive-exec\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${hive.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 连接到 Hive 通过表环境或 YAML 配置，使用目录接口和 HiveCatalog 连接到现有的 Hive 安装。\n如果 hive-conf/hive-site.xml 文件存储在远程存储系统中，用户应先将 hive 配置文件下载到本地环境中。\n请注意，虽然 HiveCatalog 不需要特定的规划师，但读/写 Hive 表只适用于 blink 规划师。因此强烈建议您在连接 Hive 仓库时使用 blink planner。\nHiveCatalog 能够自动检测使用中的 Hive 版本。建议不要指定 Hive 版本，除非自动检测失败。\n以 Hive 2.3.4 版本为例。\nval settings = EnvironmentSettings.newInstance().inBatchMode().build() val tableEnv = TableEnvironment.create(settings) val name = \u0026#34;myhive\u0026#34; val defaultDatabase = \u0026#34;mydatabase\u0026#34; val hiveConfDir = \u0026#34;/opt/hive-conf\u0026#34; // a local path val hive = new HiveCatalog(name, defaultDatabase, hiveConfDir) tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, hive) // set the HiveCatalog as the current catalog of the session tableEnv.useCatalog(\u0026#34;myhive\u0026#34;) DDL 建议使用 Hive 方言在 Flink 中执行 DDL 来创建 Hive 表、视图、分区、函数。\nDML Flink 支持 DML 写入 Hive 表。请参考读写 Hive 表的细节。\n原文链接: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/\n"},"name":"Hive 集成 - 概览","published":"2020-08-25T00:00:00Z","summary":"Overview","type":"entry","url":"https://ohmyweekly.github.io/notes/2020-08-25-hive-integration-overview/"}