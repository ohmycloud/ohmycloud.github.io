{"author":{"name":null,"type":"card","url":"https://ohmycloud.github.io/"},"content":{"html":"\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epackage wmstat.trip\n\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.execution.datasources.hbase._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport scopt.OptionParser\nimport wmutils.WmTimeUtil._\nimport scala.collection.mutable.ArrayBuffer\n\n/**\n  * 提交方式\n  * spark2-submit  --name HBASE-CONNECTOR --files /etc/hbase/conf/hbase-site.xml --class wmhbase.MileageAnxiety  --master yarn    --deploy-mode client  --driver-memory 2g    --driver-cores 2    --executor-memory 2g    --executor-cores 1    --num-executors 2 wmanxiety-1.0-SNAPSHOT.jar --day 20180810 --repartition 500 --interval 7\n  */\n\nobject MileageAnxiety {\n  def cat = s\u0026#34;\u0026#34;\u0026#34;{\n               |\u0026#34;table\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;trip_signal\u0026#34;, \u0026#34;tableCoder\u0026#34;:\u0026#34;PrimitiveType\u0026#34;},\n               |\u0026#34;rowkey\u0026#34;:\u0026#34;key\u0026#34;,\n               |\u0026#34;columns\u0026#34;:{\n               |\u0026#34;rowkey\u0026#34;       :{\u0026#34;cf\u0026#34;:\u0026#34;rowkey\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;key\u0026#34;,            \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\n               |\u0026#34;vin\u0026#34;          :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;,   \u0026#34;col\u0026#34;:\u0026#34;vin\u0026#34;,            \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\n               |\u0026#34;tripStatus\u0026#34;   :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;,   \u0026#34;col\u0026#34;:\u0026#34;tripStatus\u0026#34;,     \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\n               |\u0026#34;tripStartTime\u0026#34;:{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;,   \u0026#34;col\u0026#34;:\u0026#34;tripStartTime\u0026#34;,  \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\n               |\u0026#34;tripEndTime\u0026#34;  :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;,   \u0026#34;col\u0026#34;:\u0026#34;tripEndTime\u0026#34;,    \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\n               |\u0026#34;tripDistance\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;,   \u0026#34;col\u0026#34;:\u0026#34;tripDistance\u0026#34;,   \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\n               |\u0026#34;startSoc\u0026#34;     :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;,   \u0026#34;col\u0026#34;:\u0026#34;startSoc\u0026#34;,       \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\n               |\u0026#34;endSoc\u0026#34;       :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;,   \u0026#34;col\u0026#34;:\u0026#34;endSoc\u0026#34;,         \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\n               |\u0026#34;maxSpeed\u0026#34;     :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;,   \u0026#34;col\u0026#34;:\u0026#34;maxSpeed\u0026#34;,       \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\n               |\u0026#34;startMileage\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;,   \u0026#34;col\u0026#34;:\u0026#34;startMileage\u0026#34;,   \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\n               |\u0026#34;coordinate\u0026#34;   :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;,   \u0026#34;col\u0026#34;:\u0026#34;coordinate\u0026#34;,     \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}\n               |}\n               |}\u0026#34;\u0026#34;\u0026#34;.stripMargin\n\n  def main(args: Array[String]): Unit = {\n    Logger.getLogger(\u0026#34;org\u0026#34;).setLevel(Level.WARN)\n    val spark = SparkSession.builder()\n      .appName(this.getClass.getName)\n      .master(\u0026#34;local[2]\u0026#34;)\n      .getOrCreate()\n\n    case class Params(day: String = \u0026#34;\u0026#34;, repartition: Int = 200, interval: Int = 7) // 设置日期和分区和间隔的默认值\n\n    // 定义一个命令行解析器\n    val parser = new OptionParser[Params](\u0026#34;MileageAnxiety\u0026#34;) { // OptionParser 用于解析命名行参数, 生成 help 信息\n      head(\u0026#34;MileageAnxiety: pull data from HBase\u0026#34;) // 打印用途信息\n\n      opt[String](\u0026#39;d\u0026#39;, \u0026#34;day\u0026#34;)\n        .text(\u0026#34;fill data on demand day, yyyymmss format\u0026#34;)\n        .action((x, c) =\u0026gt; c.copy(day = x))\n\n      opt[Int](\u0026#39;r\u0026#39;, \u0026#34;repartition\u0026#34;)\n        .text(\u0026#34;the partition of dataframe\u0026#34;)\n        .action((x, c) =\u0026gt; c.copy(repartition = x))\n\n      opt[Int](\u0026#39;i\u0026#39;, \u0026#34;interval\u0026#34;)\n        .text(\u0026#34;the interval of days, 7,30 or 90\u0026#34;)\n        .action((x, c) =\u0026gt; c.copy(interval = x))\n\n      help(\u0026#34;help\u0026#34;).text(\u0026#34;prints this usage text\u0026#34;)\n    }\n\n    // 解析给定的 args\n    parser.parse(args, Params()) match { // 解析命令行参数\n      case Some(params) =\u0026gt; run(params)   // 如果传参, 将参数传递给 run 方法\n      case _ =\u0026gt; sys.exit(1)              // 没有传就退出\n    }\n\n    def run(params: Params): Unit = {\n      println(params)\n\n    val sc = spark.sparkContext\n    val sqlContext = spark.sqlContext\n    import sqlContext.implicits._\n\n    def withCatalog(cat: String): DataFrame = {\n      sqlContext\n        .read\n        .options(Map(HBaseTableCatalog.tableCatalog -\u0026gt; cat))\n        .format(\u0026#34;org.apache.spark.sql.execution.datasources.hbase\u0026#34;)\n        .load()\n    }\n\n    val df = withCatalog(cat)\n      df.show()\n\n    var dtype: Int = 0\n    params.interval match {\n      case 7  =\u0026gt; dtype = 0\n      case 30 =\u0026gt; dtype = 1\n      case 90 =\u0026gt; dtype = 2\n      case _  =\u0026gt; dtype = 3\n    }\n\n    val arrayWeek: ArrayBuffer[String] =  lastestNdays(params.day, params.interval)\n    val function: (String =\u0026gt; Boolean) = (col: String) =\u0026gt; arrayWeek.contains( col.split(\u0026#34;_\u0026#34;)(1).substring(0,8))\n    val udfFiltering = udf(function)\n\n    // 筛选 Row Key\n    val dfWeek = df.repartition(params.repartition).filter(udfFiltering(col(\u0026#34;ROWKEY\u0026#34;)))\n\n    // 里程值\n    val dfTrip: DataFrame = dfWeek.select(\u0026#34;rowkey\u0026#34;, \u0026#34;vin\u0026#34;, \u0026#34;tripDistance\u0026#34;, \u0026#34;tripStartTime\u0026#34;, \u0026#34;tripEndTime\u0026#34;)\n      .withColumn(\u0026#34;driverTime\u0026#34;, $\u0026#34;tripEndTime\u0026#34; - $\u0026#34;tripStartTime\u0026#34;)\n      .groupBy(\u0026#34;vin\u0026#34;)\n      .agg(\n        sum(\u0026#34;tripDistance\u0026#34;) as \u0026#34;mileage\u0026#34;, // 每辆车的里程\n        sum($\u0026#34;driverTime\u0026#34;) as \u0026#34;driverTime\u0026#34;,            // 每辆车的行驶时长\n        count($\u0026#34;rowkey\u0026#34;) as \u0026#34;tripCount\u0026#34;                // 每辆车的行程次数\n      )\n      .agg(\n        sum(\u0026#34;mileage\u0026#34;) as \u0026#34;totalMileage\u0026#34;,        // 全国所有车辆周里程总和\n        sum(\u0026#34;driverTime\u0026#34;) as \u0026#34;totalDriverTime\u0026#34;,  // 全国所有车辆行驶时长总和\n        sum(\u0026#34;tripCount\u0026#34;) as \u0026#34;totalTripCount\u0026#34;,   // 全国所有车辆行程次数总和\n        countDistinct(\u0026#34;vin\u0026#34;) as \u0026#34;totalVin\u0026#34;      // 全国所有车辆总和\n      ).withColumn(\u0026#34;dtype\u0026#34;, lit(dtype))            // 0-最近1周, 1-最近一个月, 2-最近3个月\n       .withColumn(\u0026#34;dtime\u0026#34;, lit(arrayWeek(0)))     // 昨天的日期或命令行指定的日期\n\n     val dfDao:TripAnalysis = new TripDao()\n     dfDao.trip2db(dfTrip)\n     spark.stop()\n  }\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e","text":"package wmstat.trip import org.apache.log4j.{Level, Logger} import org.apache.spark.sql.execution.datasources.hbase._ import org.apache.spark.sql.functions._ import org.apache.spark.sql.{DataFrame, SparkSession} import scopt.OptionParser import wmutils.WmTimeUtil._ import scala.collection.mutable.ArrayBuffer /** * 提交方式 * spark2-submit --name HBASE-CONNECTOR --files /etc/hbase/conf/hbase-site.xml --class wmhbase.MileageAnxiety --master yarn --deploy-mode client --driver-memory 2g --driver-cores 2 --executor-memory 2g --executor-cores 1 --num-executors 2 wmanxiety-1.0-SNAPSHOT.jar --day 20180810 --repartition 500 --interval 7 */ object MileageAnxiety { def cat = s\u0026#34;\u0026#34;\u0026#34;{ |\u0026#34;table\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;trip_signal\u0026#34;, \u0026#34;tableCoder\u0026#34;:\u0026#34;PrimitiveType\u0026#34;}, |\u0026#34;rowkey\u0026#34;:\u0026#34;key\u0026#34;, |\u0026#34;columns\u0026#34;:{ |\u0026#34;rowkey\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;rowkey\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;key\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;vin\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;vin\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;tripStatus\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;tripStatus\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;tripStartTime\u0026#34;:{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;tripStartTime\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;tripEndTime\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;tripEndTime\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;tripDistance\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;tripDistance\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;startSoc\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;startSoc\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;endSoc\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;endSoc\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;maxSpeed\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;maxSpeed\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;startMileage\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;startMileage\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;coordinate\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;coordinate\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;} |} |}\u0026#34;\u0026#34;\u0026#34;.stripMargin def main(args: Array[String]): Unit = { Logger.getLogger(\u0026#34;org\u0026#34;).setLevel(Level.WARN) val spark = SparkSession.builder() .appName(this.getClass.getName) .master(\u0026#34;local[2]\u0026#34;) .getOrCreate() case class Params(day: String = \u0026#34;\u0026#34;, repartition: Int = 200, interval: Int = 7) // 设置日期和分区和间隔的默认值 // 定义一个命令行解析器 val parser = new OptionParser[Params](\u0026#34;MileageAnxiety\u0026#34;) { // OptionParser 用于解析命名行参数, 生成 help 信息 head(\u0026#34;MileageAnxiety: pull data from HBase\u0026#34;) // 打印用途信息 opt[String](\u0026#39;d\u0026#39;, \u0026#34;day\u0026#34;) .text(\u0026#34;fill data on demand day, yyyymmss format\u0026#34;) .action((x, c) =\u0026gt; c.copy(day = x)) opt[Int](\u0026#39;r\u0026#39;, \u0026#34;repartition\u0026#34;) .text(\u0026#34;the partition of dataframe\u0026#34;) .action((x, c) =\u0026gt; c.copy(repartition = x)) opt[Int](\u0026#39;i\u0026#39;, \u0026#34;interval\u0026#34;) .text(\u0026#34;the interval of days, 7,30 or 90\u0026#34;) .action((x, c) =\u0026gt; c.copy(interval = x)) help(\u0026#34;help\u0026#34;).text(\u0026#34;prints this usage text\u0026#34;) } // 解析给定的 args parser.parse(args, Params()) match { // 解析命令行参数 case Some(params) =\u0026gt; run(params) // 如果传参, 将参数传递给 run 方法 case _ =\u0026gt; sys.exit(1) // 没有传就退出 } def run(params: Params): Unit = { println(params) val sc = spark.sparkContext val sqlContext = spark.sqlContext import sqlContext.implicits._ def withCatalog(cat: String): DataFrame = { sqlContext .read .options(Map(HBaseTableCatalog.tableCatalog -\u0026gt; cat)) .format(\u0026#34;org.apache.spark.sql.execution.datasources.hbase\u0026#34;) .load() } val df = withCatalog(cat) df.show() var dtype: Int = 0 params.interval match { case 7 =\u0026gt; dtype = 0 case 30 =\u0026gt; dtype = 1 case 90 =\u0026gt; dtype = 2 case _ =\u0026gt; dtype = 3 } val arrayWeek: ArrayBuffer[String] = lastestNdays(params.day, params.interval) val function: (String =\u0026gt; Boolean) = (col: String) =\u0026gt; arrayWeek.contains( col.split(\u0026#34;_\u0026#34;)(1).substring(0,8)) val udfFiltering = udf(function) // 筛选 Row Key val dfWeek = df.repartition(params.repartition).filter(udfFiltering(col(\u0026#34;ROWKEY\u0026#34;))) // 里程值 val dfTrip: DataFrame = dfWeek.select(\u0026#34;rowkey\u0026#34;, \u0026#34;vin\u0026#34;, \u0026#34;tripDistance\u0026#34;, \u0026#34;tripStartTime\u0026#34;, \u0026#34;tripEndTime\u0026#34;) .withColumn(\u0026#34;driverTime\u0026#34;, $\u0026#34;tripEndTime\u0026#34; - $\u0026#34;tripStartTime\u0026#34;) .groupBy(\u0026#34;vin\u0026#34;) .agg( sum(\u0026#34;tripDistance\u0026#34;) as \u0026#34;mileage\u0026#34;, // 每辆车的里程 sum($\u0026#34;driverTime\u0026#34;) as \u0026#34;driverTime\u0026#34;, // 每辆车的行驶时长 count($\u0026#34;rowkey\u0026#34;) as \u0026#34;tripCount\u0026#34; // 每辆车的行程次数 ) .agg( sum(\u0026#34;mileage\u0026#34;) as \u0026#34;totalMileage\u0026#34;, // 全国所有车辆周里程总和 sum(\u0026#34;driverTime\u0026#34;) as \u0026#34;totalDriverTime\u0026#34;, // 全国所有车辆行驶时长总和 sum(\u0026#34;tripCount\u0026#34;) as \u0026#34;totalTripCount\u0026#34;, // 全国所有车辆行程次数总和 countDistinct(\u0026#34;vin\u0026#34;) as \u0026#34;totalVin\u0026#34; // 全国所有车辆总和 ).withColumn(\u0026#34;dtype\u0026#34;, lit(dtype)) // 0-最近1周, 1-最近一个月, 2-最近3个月 .withColumn(\u0026#34;dtime\u0026#34;, lit(arrayWeek(0))) // 昨天的日期或命令行指定的日期 val dfDao:TripAnalysis = new TripDao() dfDao.trip2db(dfTrip) spark.stop() } } } "},"name":"使用 Spark 读取 HBase","published":"2018-12-10T16:16:21Z","summary":"package wmstat.trip import org.apache.log4j.{Level, Logger} import org.apache.spark.sql.execution.datasources.hbase._ import org.apache.spark.sql.functions._ import org.apache.spark.sql.{DataFrame, SparkSession} import scopt.OptionParser import wmutils.WmTimeUtil._ import scala.collection.mutable.ArrayBuffer /** * 提交方式 * spark2-submit --name HBASE-CONNECTOR --files /etc/hbase/conf/hbase-site.xml --class wmhbase.MileageAnxiety --master yarn --deploy-mode client --driver-memory 2g --driver-cores 2 --executor-memory 2g --executor-cores 1 --num-executors 2 wmanxiety-1.0-SNAPSHOT.jar --day 20180810 --repartition 500 --interval 7 */ object MileageAnxiety { def cat = s\u0026#34;\u0026#34;\u0026#34;{ |\u0026#34;table\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;trip_signal\u0026#34;, \u0026#34;tableCoder\u0026#34;:\u0026#34;PrimitiveType\u0026#34;}, |\u0026#34;rowkey\u0026#34;:\u0026#34;key\u0026#34;, |\u0026#34;columns\u0026#34;:{ |\u0026#34;rowkey\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;rowkey\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;key\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;vin\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;vin\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;tripStatus\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;tripStatus\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;tripStartTime\u0026#34;:{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;tripStartTime\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;tripEndTime\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;tripEndTime\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;tripDistance\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;tripDistance\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;startSoc\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;startSoc\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;endSoc\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;endSoc\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;maxSpeed\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;maxSpeed\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;startMileage\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;startMileage\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, |\u0026#34;coordinate\u0026#34; :{\u0026#34;cf\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;col\u0026#34;:\u0026#34;coordinate\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;} |} |}\u0026#34;\u0026#34;\u0026#34;.","type":"entry","url":"https://ohmycloud.github.io/notes/use-spark-to-read-hbase/"}