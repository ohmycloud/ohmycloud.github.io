<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">

    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
    
    
    

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5">

    
    <meta name="referrer" content="no-referrer">

    <title>
        
            焉知非鱼 ❚
        
    </title>

    
    


    
    
    
    

    
    
    
    

    
    
    

    
    
    
    <style>
     
     
     :root {
         --theme-color: #ac4142;
         --theme-color-light: rgba(172, 65, 66, 0.2);
     }
     
     html {
         line-height: 1.5;
     }
    </style>

    
    

    
    
    
    
    <link rel="stylesheet" href="/css/refined.min.7f6d3ee611034e4ebcbc063f1db3bc042fecdc8901afbedad80ff02bae409204.css">
    
    <link rel="preload" href="/css/refined.min.7f6d3ee611034e4ebcbc063f1db3bc042fecdc8901afbedad80ff02bae409204.css" as="style">

    



    
        <style>
         
         /* Background */ .chroma { background-color: #ffffff }
/* Error */ .chroma .err { color: #a61717; background-color: #e3d2d2 }
/* LineTableTD */ .chroma .lntd { vertical-align: top; padding: 0; margin: 0; border: 0; }
/* LineTable */ .chroma .lntable { border-spacing: 0; padding: 0; margin: 0; border: 0; width: auto; overflow: auto; display: block; }
/* LineHighlight */ .chroma .hl { display: block; width: 100%;background-color: #ffffcc }
/* LineNumbersTable */ .chroma .lnt { margin-right: 0.4em; padding: 0 0.4em 0 0.4em; }
/* LineNumbers */ .chroma .ln { margin-right: 0.4em; padding: 0 0.4em 0 0.4em; }
/* Keyword */ .chroma .k { color: #000000; font-weight: bold }
/* KeywordConstant */ .chroma .kc { color: #000000; font-weight: bold }
/* KeywordDeclaration */ .chroma .kd { color: #000000; font-weight: bold }
/* KeywordNamespace */ .chroma .kn { color: #000000; font-weight: bold }
/* KeywordPseudo */ .chroma .kp { color: #000000; font-weight: bold }
/* KeywordReserved */ .chroma .kr { color: #000000; font-weight: bold }
/* KeywordType */ .chroma .kt { color: #445588; font-weight: bold }
/* NameAttribute */ .chroma .na { color: #008080 }
/* NameBuiltin */ .chroma .nb { color: #0086b3 }
/* NameBuiltinPseudo */ .chroma .bp { color: #999999 }
/* NameClass */ .chroma .nc { color: #445588; font-weight: bold }
/* NameConstant */ .chroma .no { color: #008080 }
/* NameDecorator */ .chroma .nd { color: #3c5d5d; font-weight: bold }
/* NameEntity */ .chroma .ni { color: #800080 }
/* NameException */ .chroma .ne { color: #990000; font-weight: bold }
/* NameFunction */ .chroma .nf { color: #990000; font-weight: bold }
/* NameLabel */ .chroma .nl { color: #990000; font-weight: bold }
/* NameNamespace */ .chroma .nn { color: #555555 }
/* NameTag */ .chroma .nt { color: #000080 }
/* NameVariable */ .chroma .nv { color: #008080 }
/* NameVariableClass */ .chroma .vc { color: #008080 }
/* NameVariableGlobal */ .chroma .vg { color: #008080 }
/* NameVariableInstance */ .chroma .vi { color: #008080 }
/* LiteralString */ .chroma .s { color: #dd1144 }
/* LiteralStringAffix */ .chroma .sa { color: #dd1144 }
/* LiteralStringBacktick */ .chroma .sb { color: #dd1144 }
/* LiteralStringChar */ .chroma .sc { color: #dd1144 }
/* LiteralStringDelimiter */ .chroma .dl { color: #dd1144 }
/* LiteralStringDoc */ .chroma .sd { color: #dd1144 }
/* LiteralStringDouble */ .chroma .s2 { color: #dd1144 }
/* LiteralStringEscape */ .chroma .se { color: #dd1144 }
/* LiteralStringHeredoc */ .chroma .sh { color: #dd1144 }
/* LiteralStringInterpol */ .chroma .si { color: #dd1144 }
/* LiteralStringOther */ .chroma .sx { color: #dd1144 }
/* LiteralStringRegex */ .chroma .sr { color: #009926 }
/* LiteralStringSingle */ .chroma .s1 { color: #dd1144 }
/* LiteralStringSymbol */ .chroma .ss { color: #990073 }
/* LiteralNumber */ .chroma .m { color: #009999 }
/* LiteralNumberBin */ .chroma .mb { color: #009999 }
/* LiteralNumberFloat */ .chroma .mf { color: #009999 }
/* LiteralNumberHex */ .chroma .mh { color: #009999 }
/* LiteralNumberInteger */ .chroma .mi { color: #009999 }
/* LiteralNumberIntegerLong */ .chroma .il { color: #009999 }
/* LiteralNumberOct */ .chroma .mo { color: #009999 }
/* Operator */ .chroma .o { color: #000000; font-weight: bold }
/* OperatorWord */ .chroma .ow { color: #000000; font-weight: bold }
/* Comment */ .chroma .c { color: #999988; font-style: italic }
/* CommentHashbang */ .chroma .ch { color: #999988; font-style: italic }
/* CommentMultiline */ .chroma .cm { color: #999988; font-style: italic }
/* CommentSingle */ .chroma .c1 { color: #999988; font-style: italic }
/* CommentSpecial */ .chroma .cs { color: #999999; font-weight: bold; font-style: italic }
/* CommentPreproc */ .chroma .cp { color: #999999; font-weight: bold; font-style: italic }
/* CommentPreprocFile */ .chroma .cpf { color: #999999; font-weight: bold; font-style: italic }
/* GenericDeleted */ .chroma .gd { color: #000000; background-color: #ffdddd }
/* GenericEmph */ .chroma .ge { color: #000000; font-style: italic }
/* GenericError */ .chroma .gr { color: #aa0000 }
/* GenericHeading */ .chroma .gh { color: #999999 }
/* GenericInserted */ .chroma .gi { color: #000000; background-color: #ddffdd }
/* GenericOutput */ .chroma .go { color: #888888 }
/* GenericPrompt */ .chroma .gp { color: #555555 }
/* GenericStrong */ .chroma .gs { font-weight: bold }
/* GenericSubheading */ .chroma .gu { color: #aaaaaa }
/* GenericTraceback */ .chroma .gt { color: #aa0000 }
/* GenericUnderline */ .chroma .gl { text-decoration: underline }
/* TextWhitespace */ .chroma .w { color: #bbbbbb }

         
         /* Overrides on top of the theme and Chroma CSS */
/* Chroma-based lines highlighting in code blocks */
.chroma .hl {
    background-color: #e8e8e8;
    /* Extend highlight up to 100 characters (assuming that the code blocks never have more than 100 characters in a line) */
    min-width: 100ch;
}
/* GenericHeading */ .chroma .gh { color: #999999; font-weight: bold }
/* GenericSubheading */ .chroma .gu { color: #aaaaaa; font-weight: bold }

         
        </style>
    

    

    
    
    

    
    <script src="/js/responsive-nav-orig.min.e2b5f2a956b488f466da513820636134defdc38b90ed566248960593f2bb4ba5.js"></script>
    
    <link rel="preload" href="/js/responsive-nav-orig.min.e2b5f2a956b488f466da513820636134defdc38b90ed566248960593f2bb4ba5.js" as="script">

    
    
    <script defer src="/js/libs/fa/fontawesome-all.min.08916ac0fd078adfb58edc890460e2c8990729aee02bca7586404b56805f5219.js"></script>
    
    <link rel="preload" href="/js/libs/fa/fontawesome-all.min.08916ac0fd078adfb58edc890460e2c8990729aee02bca7586404b56805f5219.js" as="script">

    

    

    
    
    

    
    
<!-- rel="me" links for IndieAuth -->







    
 
<meta property="og:title" content="焉知非鱼" />
<meta property="og:description"
      content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://ohmyweekly.github.io/" />


    
        <meta property="og:updated_time" content="2022-12-27T00:00:00&#43;00:00"/>
    












     <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="焉知非鱼"/>
<meta name="twitter:description" content=""/>


    
    
    <link rel="alternate" type="application/rss+xml" href="https://ohmyweekly.github.io/index.xml" title="Rss for 焉知非鱼" />
    <link rel="alternate" type="application/atom+xml" href="https://ohmyweekly.github.io/atom.xml" title="Atom for 焉知非鱼" />
    <link rel="alternate" type="application/jf2feed+json" href="https://ohmyweekly.github.io/jf2feed.json" title="Jf2feed for 焉知非鱼" />
    
     






    
    
    
    <meta name="hugo-build-date" content="2024-03-01T16:16:06Z"/>
    <meta name="hugo-commit-hash" content="312735366b20d64bd61bff8627f593749f86c964"/>
    <meta name="generator" content="Hugo 0.123.7">
</head>


    
        <body lang="en">
    

        
        <div class="border" id="home"></div>

        <div class="wrapper">   
            
<nav id="nav" class="nav-collapse opened" aria-hidden="false">
    <ul class="navbar">
        <li><a class="active" href="/">Home</a></li>
        
            
                <li><a class="" href="https://ohmyweekly.github.io/posts/">Posts</a></li>
            
        
            
                <li><a class="" href="https://ohmyweekly.github.io/notes/">Notes</a></li>
            
        
        
            <li><a class="" href="https://ohmyweekly.github.io/search/">Search</a></li>
        
    </ul>
</nav>

            <div class="container">
                <header class="masthead">
                    <div class="masthead-title no-text-decoration">
                        <a href="/">焉知非鱼</a> <span class="blinking-cursor">❚</span>
                    </div>
                    <div class="masthead-tagline">
                        Wait the light to fall
                    </div>
                </header>

                

<div class="posts h-feed">
    <header>
        
        <data class="u-url" value="https://ohmyweekly.github.io/"></data>
         





        <data class="p-name" value="焉知非鱼"></data>
    </header>
    
    
        <article class="summary h-entry notes">
    <header>
        <h2 class="post-title p-name no-text-decoration"><a class="u-url" href='https://ohmyweekly.github.io/notes/outer-joins-in-spark-structured-streaming/'> Spark Structured Streaming 中的外连接</a> </h2>
        
    <div class="taxo no-text-decoration">
         
            
                <ul class="no-bullets inline categories">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts categorized in ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/categories/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
         
            
                <ul class="no-bullets inline tags">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts tagged with ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/tags/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
        
    </div>


        


    
    
    <div class="post-date">
        
        <time datetime="2019-10-04T08:25:57+0000" class="dt-published">Fri Oct 4, 2019</time>
        
        
    </div>


    </header>

    <div class="post p-summary" >
        <p>以前，我们在 Apache Spark 中发现了流到流的内连接，但是它们不是唯一受支持的类型。另一个是外连接，使我们可以在不匹配行的情况下连接流。
这篇文章是关于结构化流模块中的外连接的。它的第一部分介绍了有关这种连接的一些理论观点。第二篇展示了如何通过一些 Scala 示例来实现它。
当匹配是可选的 流式外连接与经典的，类似批处理的连接没有什么不同。有了它们，我们总是从一侧获得所有行，即使其中一些行在连接数据集中没有匹配项也是如此。对于 RDBMS 之类的有限数据源，此类不匹配将直接返回，其中 null 表示另一侧的行。但是，无限来源的逻辑是不同的。由于不同的特性，例如网络延迟影响或脱机设备产生事件，因此在给定时刻我们可能没有所有连接的元素。因此，我们必须能够将物理连接推迟到我们确定要连接的大多数行都将到来的那一刻。为此，我们需要将一侧的行存储在某处。并且，如果您还记得 Apache Spark 结构化流中流之间的内连接的文章中的一些注释，则 Apache Spark 为此使用状态存储。下图从鸟瞰显示了这一点：
该图像清楚地表明，与内连接水位的情况一样，行被缓冲在状态存储中。 外连接还使用水位和范围查询条件的概念来确定何时不应在给定的行中接收第二个流中的任何新匹配。 这就是为什么完全没有水位的外连接是不可能的：
it should &#34;fail without watermark and range condition on watermark in the query&#34; in { val mainEventsStream = new MemoryStream[MainEvent](1, sparkSession.sqlContext) val joinedEventsStream = new MemoryStream[JoinedEvent](2, sparkSession.sqlContext) val mainEventsDataset = mainEventsStream.toDS().select($&#34;mainKey&#34;, $&#34;mainEventTime&#34;, $&#34;mainEventTimeWatermark&#34;) .withWatermark(&#34;mainEventTimeWatermark&#34;, &#34;2 seconds&#34;) val joinedEventsDataset = joinedEventsStream.toDS().select($&#34;joinedKey&#34;, $&#34;joinedEventTime&#34;, $&#34;joinedEventTimeWatermark&#34;) .withWatermark(&#34;joinedEventTimeWatermark&#34;, &#34;2 seconds&#34;) val stream = mainEventsDataset.</p>

    </div>

    
    
    
        <div class="readmore no-text-decoration">
            <a href='https://ohmyweekly.github.io/notes/outer-joins-in-spark-structured-streaming/'
               title="Read more about Spark Structured Streaming 中的外连接">
                read <span class="more"> more</span>
                <span class="screen-readers-only" > about Spark Structured Streaming 中的外连接</span>
            </a>
        </div>
    
</article>
      
    
        <article class="summary h-entry notes">
    <header>
        <h2 class="post-title p-name no-text-decoration"><a class="u-url" href='https://ohmyweekly.github.io/notes/stream-to-stream-state-management/'> 流到流的状态管理</a> </h2>
        
    <div class="taxo no-text-decoration">
         
            
                <ul class="no-bullets inline categories">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts categorized in ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/categories/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
         
            
                <ul class="no-bullets inline tags">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts tagged with ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/tags/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
        
    </div>


        


    
    
    <div class="post-date">
        
        <time datetime="2019-10-04T08:02:39+0000" class="dt-published">Fri Oct 4, 2019</time>
        
        
    </div>


    </header>

    <div class="post p-summary" >
        <p>上周，我们在 Apache Spark 结构化流中发现了 2 种流对流连接类型。如这些帖子所述，有时可能会省略状态管理逻辑（对于内连接），但通常建议减少内存压力。 Apache Spark 提出了 3 种不同的状态管理策略，这些策略将在以下各节中详细介绍。
这篇文章分为 4 部分。第一个回顾了流对流连接情况下的状态特异性。接下来的 3 个讨论 3 种状态管理策略。
状态和流连接 如 Apache Spark 结构化流中的外连接这篇文章所述，每个潜在的可连接行都缓存在状态存储中。每当找到匹配的行时，都会进行连接并发射结果。内连接类型就是这种情况。对于外连接，逻辑略有不同，由于匹配的行或由于过期状态而发出结果。过期状态表示我们不希望收到给定条目的匹配事件的时刻。希望此行为也适用于内连接，但不同之处在于其可选特性。
没有这种“过期状态”的概念，引擎将使行无限期地匹配，并且由于数据源是无界的，因此不可避免地迟早会失败。因此，Apache Spark 提供了 3 种不同的策略来管理状态过期（水位）。
状态键水位 这些策略中的第一个称为状态键水位。在以下情况下将其应用于查询：
在至少一个连接流中之一中定义了一个水位列-它可以是时间戳列或窗口列。如果仅在一侧定义水位，则 Apache Spark 能够从该水位推导另一侧的水位。 水位列在 JOIN 子句中用作相等约束 此策略的名称来自直接在 JOIN 子句条件中使用水位-因此是状态键。为了说明这一点，我们可以用下面的代码片段举几个例子：
&#34;state key watermark&#34; should &#34;be built from watermark used in join&#34; in { val mainEventsStream = new MemoryStream[MainEvent](1, sparkSession.sqlContext) val joinedEventsStream = new MemoryStream[JoinedEvent](2, sparkSession.sqlContext) val mainEventsDataset = mainEventsStream.toDS().select($&#34;mainKey&#34;, $&#34;mainEventTime&#34;, $&#34;mainEventTimeWatermark&#34;) .</p>

    </div>

    
    
    
        <div class="readmore no-text-decoration">
            <a href='https://ohmyweekly.github.io/notes/stream-to-stream-state-management/'
               title="Read more about 流到流的状态管理">
                read <span class="more"> more</span>
                <span class="screen-readers-only" > about 流到流的状态管理</span>
            </a>
        </div>
    
</article>
      
    
        <article class="summary h-entry notes">
    <header>
        <h2 class="post-title p-name no-text-decoration"><a class="u-url" href='https://ohmyweekly.github.io/notes/stream-to-stream-joins-internals/'> 流和流之间的连接内部研讨</a> </h2>
        
    <div class="taxo no-text-decoration">
         
            
                <ul class="no-bullets inline categories">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts categorized in ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/categories/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
         
            
                <ul class="no-bullets inline tags">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts tagged with ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/tags/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
        
    </div>


        


    
    
    <div class="post-date">
        
        <time datetime="2019-10-03T17:36:48+0000" class="dt-published">Thu Oct 3, 2019</time>
        
        
    </div>


    </header>

    <div class="post p-summary" >
        <p>在最近关于Apache Spark结构化流的3篇文章中，我们发现了流连接：内部连接，外部连接和状态管理策略。发现所有这些操作背后发生的事情是总结该系列的一个好方法。 这篇文章首先介绍了流连接过程中涉及的类。接下来是专注于与联接有关的状态管理内部的部分。文章以关于连接机制的一小段结尾。
所涉及的类 在流连接所涉及的类中，我们可以区分3个非常重要的类：SymmetricHashJoinStateManager，StreamingSymmetricHashJoinExec 和 StreamingJoinHelper。所有这些都用于流查询执行的不同阶段。
首先，查询的流式表示形式 IncrementalExecution 实例存储对该状态的引用。如果查询具有某些流间连接，则此状态在每次执行时都表示为 StreamingSymmetricHashJoinExec 的实例。该实例在每次执行中都不同，不同点是偏移量统计信息和状态水位谓词。谓词的计算公式为：
def getStateWatermarkPredicates( leftAttributes: Seq[Attribute], rightAttributes: Seq[Attribute], leftKeys: Seq[Expression], rightKeys: Seq[Expression], condition: Option[Expression], eventTimeWatermark: Option[Long]): JoinStateWatermarkPredicates 此方法通过应用不同的规则来计算用于从状态存储中丢弃太晚的行的状态水位谓词。 它首先检查查询的相等性 JOIN 中涉及的所有列中是否至少有一个用水位注释标记的列。 如果是，它将自动认为必须将状态键水位策略应用于迟到的行（您可以在 Apache Spark 结构化流的外连接中了解它们）。 如果不是，则检查一个连接侧是否定义了水位列。 如果满足上述条件之一，并且优先选择前者，则使用常规的 org.apache.spark.sql.execution.streaming.WatermarkSupport＃watermarkExpression(optionalWatermarkExpression: Option[Expression], optionalWatermarkMs: Option[Long]) 方法。
JOIN 子句中的相等重要性 流到流连接的实际实现仅接受相等关系作为连接键。 这意味着如果我们有2个流：stream#1(field1[int], field2[timestamp]), stream#2(field10[int], field20[timestamp]), ，则只有 field1 和 field10 之间的相等关系以及 field2 和 field20 将被视为连接键。 如果在 JOIN 的 ON 部分中表示不等式，则会将其转换为 WHERE 条件。
例如，以下查询：
val mainEventsDataset = mainEventsStream.toDS().select($&#34;mainKey&#34;, $&#34;mainEventTime&#34;, $&#34;mainEventTimeWatermark&#34;, window($&#34;mainEventTimeWatermark&#34;, &#34;3 seconds&#34;).</p>

    </div>

    
    
    
        <div class="readmore no-text-decoration">
            <a href='https://ohmyweekly.github.io/notes/stream-to-stream-joins-internals/'
               title="Read more about 流和流之间的连接内部研讨">
                read <span class="more"> more</span>
                <span class="screen-readers-only" > about 流和流之间的连接内部研讨</span>
            </a>
        </div>
    
</article>
      
    
        <article class="summary h-entry notes">
    <header>
        <h2 class="post-title p-name no-text-decoration"><a class="u-url" href='https://ohmyweekly.github.io/notes/spark-structured-streaming-and-kafka-offests-management/'> Spark Structured Streaming 和 Kafka 偏移量管理</a> </h2>
        
    <div class="taxo no-text-decoration">
         
            
                <ul class="no-bullets inline categories">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts categorized in ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/categories/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
         
            
                <ul class="no-bullets inline tags">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts tagged with ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/tags/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
        
    </div>


        


    
    
    <div class="post-date">
        
        <time datetime="2019-10-03T17:04:09+0000" class="dt-published">Thu Oct 3, 2019</time>
        
        
    </div>


    </header>

    <div class="post p-summary" >
        <p>前段时间，我对 Apache Spark 结构化流中 Apache Kafka 连接器的实现提出了3个有趣的问题。 我将在这篇文章中回答他们。
您可以想象，该帖子分为3个部分。 每个人都会回答一个问题。 在文章结尾，您应该更好地了解谁负责 Apache Kafka 连接器中的内容。
问题1：偏移量跟踪 第一个问题是关于 Apache Kafka 偏移量跟踪的。 谁跟踪他们，driver 或 executor？ 在深入探讨该问题之前，让我们回顾一下我在分析结构化流式 Kafka 集成-Kafka源帖子中介绍的一些基础知识。 Apache Kafka 源代码首先从 driver 读取要处理的偏移量，然后将其分配给 executor 以进行实际处理。 因此，我们可以推断出，从这个角度来看，偏移量是由 driver 跟踪的。 您会注意到在 KafkaSource 类内部创建 KafkaSourceRDD 的代码中：
// Calculate offset ranges val offsetRanges = topicPartitions.map { tp =&gt; // ... } // Create an RDD that reads from Kafka and get the (key, value) pair as byte arrays.</p>

    </div>

    
    
    
        <div class="readmore no-text-decoration">
            <a href='https://ohmyweekly.github.io/notes/spark-structured-streaming-and-kafka-offests-management/'
               title="Read more about Spark Structured Streaming 和 Kafka 偏移量管理">
                read <span class="more"> more</span>
                <span class="screen-readers-only" > about Spark Structured Streaming 和 Kafka 偏移量管理</span>
            </a>
        </div>
    
</article>
      
    
        <article class="summary h-entry notes">
    <header>
        <h2 class="post-title p-name no-text-decoration"><a class="u-url" href='https://ohmyweekly.github.io/notes/initializing-state-in-structured-streaming/'> Spark Structured Streaming 中的初始化状态</a> </h2>
        
    <div class="taxo no-text-decoration">
         
            
                <ul class="no-bullets inline categories">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts categorized in ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/categories/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
         
            
                <ul class="no-bullets inline tags">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts tagged with ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/tags/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
        
    </div>


        


    
    
    <div class="post-date">
        
        <time datetime="2019-10-03T16:43:59+0000" class="dt-published">Thu Oct 3, 2019</time>
        
        
    </div>


    </header>

    <div class="post p-summary" >
        <p>不久前，Sunil 询问我是否可以像基于 DStream 的 API 一样在 Apache Spark 结构化流中加载初始状态。 由于回应并不明显，因此我决定调查并通过这篇文章分享调查结果。
文章首先简短地回忆了 Apache Spark Streaming 模块中的状态初始化。 下一节将讨论可用于在 Apache Spark 结构化流库中执行相同操作的方法。
流中的初始化状态 在基于 DStream 的库中初始化状态很简单。 您只需要创建一个基于键的 RDD 并将其传递给 StateSpec 的 initialState 方法：
&#34;streaming processing&#34; should &#34;start with initialized state&#34; in { val conf = new SparkConf().setAppName(&#34;DStream initialState test&#34;).setMaster(&#34;local[*]&#34;) val streamingContext = new StreamingContext(conf, Durations.seconds(1)) streamingContext.checkpoint(&#34;/tmp/spark-initialstate-test&#34;) val dataQueue = new mutable.Queue[RDD[OneVisit]]() // A mapping function that maintains an integer state and return a UserVisit def mappingFunction(key: String, value: Option[OneVisit], state: State[UserVisit]): Option[(String, String)] = { var visitedPages = state.</p>

    </div>

    
    
    
        <div class="readmore no-text-decoration">
            <a href='https://ohmyweekly.github.io/notes/initializing-state-in-structured-streaming/'
               title="Read more about Spark Structured Streaming 中的初始化状态">
                read <span class="more"> more</span>
                <span class="screen-readers-only" > about Spark Structured Streaming 中的初始化状态</span>
            </a>
        </div>
    
</article>
      
    
        <article class="summary h-entry notes">
    <header>
        <h2 class="post-title p-name no-text-decoration"><a class="u-url" href='https://ohmyweekly.github.io/notes/foreach-batch/'> Apache Spark 2.4.0 特性  - foreachBatch</a> </h2>
        
    <div class="taxo no-text-decoration">
         
            
                <ul class="no-bullets inline categories">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts categorized in ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/categories/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
         
            
                <ul class="no-bullets inline tags">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts tagged with ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/tags/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
        
    </div>


        


    
    
    <div class="post-date">
        
        <time datetime="2019-10-03T15:13:39+0000" class="dt-published">Thu Oct 3, 2019</time>
        
        
    </div>


    </header>

    <div class="post p-summary" >
        <p>当我第一次听说 foreachBatch 功能时，我以为这是结构化流模块中 foreachPartition 的实现。但是，经过一些分析，我发现我错了，因为此新功能解决了其他但也很重要的问题。您会发现更多。
在 Apache Spark 2.4.0 功能系列的这一新文章中，我将展示 foreachBatch 方法的实现。在第一部分中，我将简要介绍有关此功能的要点。我还将在其中添加有关实现的一些详细信息。在接下来的2部分中，我将展示.foreachBatch 数据接收器解决的问题。
定义 在 2.4.0 发行版之前，foreach 是我们可以放置一些自定义逻辑的单一接收器。它很容易使用，因为它看起来像包装在类中的 foreach 循环。另外，foreach 接收器非常适合连续执行，因为我们将重点放在每次一行所带来的信息上。但是，由于基于微批处理的管道的适应性通常更差一些，因为我们经常需要对整个累积的微批处理进行某些处理。
2.4.0 版本使用新的 org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink 接收器解决了微批处理的这些问题。它的主要思想很简单。引擎累积在给定的微批次中处理的数据，并将其作为数据集传递到接收器。这不仅意味着您可以对整个数据应用一种逻辑，而且还可以在流传输管道中执行一些纯批处理，例如将数据写入不可流式的数据存储中。
除了具体的数据集之外，foreachBatch 消费者方法还接受一个名为 batchId 的属性。此参数包含生成数据集的微批处理的 ID。您可以使用此属性实现一次传递语义，因为默认情况下，引擎以至少一次语义运行。
最后，由于 ForeachBatchSink 解决了微批次问题，因此您不能将其与连续触发器一起使用。在 DataStreamWriter 内部进行的简单检查显示：
} else if (source == &#34;foreachBatch&#34;) { assertNotPartitioned(&#34;foreachBatch&#34;) if (trigger.isInstanceOf[ContinuousTrigger]) { throw new AnalysisException(&#34;&#39;foreachBatch&#39; is not supported with continuous trigger&#34;) } 您还可以看到接收器不支持分区管道（.partitionBy(...)）。 如果您对引入此逻辑的任务感兴趣，可以在“另请阅读”部分中找到一个链接。
用例：不流式接收器 我已经在上一节中提到了 foreachBatch 的第一个用例。 当您要将处理后的数据保存到关系数据库或键值存储之类的不可流式接收器中时，此新接收器很有用。 为了简单起见，我将使用内存中单例键值存储：
&#34;foreachBatch&#34; should &#34;save the data into a key-value memory store&#34; in { val inputStream = new MemoryStream[Int](1, sparkSession.</p>

    </div>

    
    
    
        <div class="readmore no-text-decoration">
            <a href='https://ohmyweekly.github.io/notes/foreach-batch/'
               title="Read more about Apache Spark 2.4.0 特性  - foreachBatch">
                read <span class="more"> more</span>
                <span class="screen-readers-only" > about Apache Spark 2.4.0 特性  - foreachBatch</span>
            </a>
        </div>
    
</article>
      
    
        <article class="summary h-entry notes">
    <header>
        <h2 class="post-title p-name no-text-decoration"><a class="u-url" href='https://ohmyweekly.github.io/notes/continuous-execution/'> Spark Structured Streaming 中的持续执行</a> </h2>
        
    <div class="taxo no-text-decoration">
         
            
                <ul class="no-bullets inline categories">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts categorized in ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/categories/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
         
            
                <ul class="no-bullets inline tags">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts tagged with ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/tags/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
        
    </div>


        


    
    
    <div class="post-date">
        
        <time datetime="2019-10-03T14:35:22+0000" class="dt-published">Thu Oct 3, 2019</time>
        
        
    </div>


    </header>

    <div class="post p-summary" >
        <p>这些年来，Apache Spark 的流媒体被认为是与微批处理一起工作的。但是，版本 2.3.0 试图对此进行更改，并提出了一种称为“连续”的新执行模型。即使它仍处于实验状态，还是值得更多了解它。
这篇文章介绍了连续流处理的新实验功能。第一部分解释并与微批执行策略进行比较。下一个显示一些内部细节。在第三部分中，我们可以了解至少一次保证。最后一部分在基于比率的源示例中显示了一个简单的连续执行用例。
连续流 毕竟为什么要替换微批​​处理的 Spark 区别标记（与允许连续流处理的 Apache Flink 或 Apache Beam 不同）？答案很简单-延迟。使用微批处理时，延迟很高。在最坏的情况下，等待时间是批处理时间与任务启动时间的总和，估计为几毫秒。连续处理将数据到达与其处理之间的等待时间减少到几毫秒。延迟减少是此（仍）实验特性背后的主要动机。
从鸟瞰来看，连续查询在多个线程中执行。每个线程负责不同的分区。因此，为了保证最佳的并行性，集群中可用核心的数量必须等于要处理的分区的数量。在处理期间，这些线程将结果连续写入结果表。
从技术上讲，可以通过 .trigger(Trigger.Continuous(&quot;1 second&quot;)) 使用连续触发来启用连续处理。但必须强调的是，在当前（2.3.0）Spark 版本中，并非所有操作都暴露于此模式下。在可用的转换中，我们只能区分投影（select，映射函数）和选择（过滤器子句）。
但是，降低延迟并不是没有代价的。实际上，更快的处理将交付保证从正好一次下降到了至少一次。因此，对于处理延迟比交付保证更为重要的系统，建议执行连续执行。
ContinuousExecution 类 最初，在结构化流中，StreamExecution 包含整个执行逻辑。仅在 2.3.0 版本中，它才成为由 MicroBatchExecution 和 ContinuousExecution 扩展的抽象类。这两个类的出发点都是 org.apache.spark.sql.execution.streaming.StreamExecution#runStream()，在执行一些初始化步骤之后，该类将激活每个执行策略中实现的流查询，方法为 runActivatedStream(sparkSessionForStream: SparkSession)` 方法。
由于以下代码，ContinuousExecution 连续运行查询：
do { runContinuous(sparkSessionForStream) } while (state.updateAndGet(stateUpdate) == ACTIVE) 在此 runContinuous 方法内部，发生了许多操作。第一个步骤包括将查询逻辑计划转换为一系列 ContinuousReaders。此接口定义是否可以连续方式读取给定的数据源。当前仅支持 Apache Kafka 和基于速率的源（该源以固定的Y间隔每秒生成 X（行，计数器）行，其中 X 和 Y 是配置参数）。如果某些源或操作不支持连续模式，则在启动查询之前会引发 UnsupportedOperationException。
在创建源之后，将解析起始偏移量，以确定何时开始执行给定执行的处理。如果查询是第一次启动，则偏移量自然为空。但是，再次执行查询时，将从提交日志中检索偏移量。稍后，将为每个映射到读取偏移量的数据源创建一个 StreamingDataSourceV2Relation 实例。接下来，创建数据读取器和写入器，并以与微批处理方法完全相同的方式生成执行计划。
在下一步中，引擎通过 EpochCoordinator 实例以纪元（与微批处理中的批处理类似）进行播放。这是连续处理的杰作。此 RPC 端点负责处理以下消息：
生成新的纪元ID-通过消息 IncrementAndGetEpoch，该纪元 ID 会自动增加。然后，在 runContinuous 方法中返回新值。 返回当前纪元ID-表示为 GetCurrentEpoch 实例的此 getter 消息由定期执行 EpochPollRunnable 中定义的逻辑的线程使用。在此逻辑中，引擎检索当前已知的纪元并将其添加到要处理的消息的内部队列中。在此列表下面的模式中将对此进行详细说明。 在纪元下提交分区-在给定时期内对分区的处理终止时，会将 CommitPartitionEpoch 消息发送给 EpochCoordinator 以发出信号。如果所有纪元的分区都已处理，则 ContinuousExecution 将此纪元标记为已提交。这意味着该纪元被保留在提交日志中，并且在重新处理的情况下，与之关联的偏移量被用作起始偏移量（请参见上一段）。 这些涉及纪元协调器的连续处理逻辑可以在以下简化模式中恢复：</p>

    </div>

    
    
    
        <div class="readmore no-text-decoration">
            <a href='https://ohmyweekly.github.io/notes/continuous-execution/'
               title="Read more about Spark Structured Streaming 中的持续执行">
                read <span class="more"> more</span>
                <span class="screen-readers-only" > about Spark Structured Streaming 中的持续执行</span>
            </a>
        </div>
    
</article>
      
    
        <article class="summary h-entry notes">
    <header>
        <h2 class="post-title p-name no-text-decoration"><a class="u-url" href='https://ohmyweekly.github.io/notes/watermark-configuration/'> Apache Spark 2.4.0 特性 - Watermark Configuration</a> </h2>
        
    <div class="taxo no-text-decoration">
         
            
                <ul class="no-bullets inline categories">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts categorized in ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/categories/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
         
            
                <ul class="no-bullets inline tags">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts tagged with ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/tags/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
        
    </div>


        


    
    
    <div class="post-date">
        
        <time datetime="2019-10-03T14:09:24+0000" class="dt-published">Thu Oct 3, 2019</time>
        
        
    </div>


    </header>

    <div class="post p-summary" >
        <p>有关 Apache Spark 2.4.0 功能的系列继续进行。在上周发现了桶剪枝之后，现在该切换到结构化流模块并查看其主要进展了。
这篇文章介绍了 2.4.0 版本中添加的水位配置功能。第一部分解释了它的变化。第二个给出了实现细节，其中显示了用于保证向后兼容性的策略之一。最后一部分包含说明水位配置功能的测试用例。
最小或最大水位？ 2.3.0 版可以连接流。如 Apache Spark 结构化流中的流之间的内联中所述，这样做不是一件容易的事，因为联接的流可能具有不同的延迟。水位是控制哪些数据可以联接的方法之一，即使它来得很晚。在 2.4.0 发行版之前，定义了多个水位时，默认情况下使用最小值。它使我们能够以最慢的流移动，并且不会丢失太多数据：
通过 Apache Spark 2.4.0 中引入的更改，我们可以在这种情况下配置使用的水位，并采用最小或最大水位。 当然，选择最大值将导致应用程序以最快的速度向前移动，从而丢失更多数据：
水位配置实现 水位管理是通过 spark.sql.streaming.multipleWatermarkPolicy 配置条目实现的。它采用“最小”或“最大”值之一。它们中的每一个都导致相应的 MultipleWatermarkPolicy 实例的初始化：MinWatermark 和 MaxWatermark。两者都附带一种解决使用的水位值的方法。根据使用的策略，此方法仅是对 Seq 的 min 或 max 方法的调用。负责调用已配置策略的类为 WatermarkTracker。
向后兼容性由 MultipleWatermarkPolicy 条目的默认值 &lsquo;min&rsquo; 保证。除非您明确更改此值，否则您应该能够在新的 Spark 版本上运行旧的流传输管道，而不会出现问题。甚至那些从 Apache Spark 2.4.0 之前的检查点恢复的管道。而且由于我们在谈论检查点，所以要记住的一件事很重要。从检查点还原管道后，我们无法修改水位策略。任何更改都将被忽略。
最大水位示例 为了看到这个新功能的实际效果，我们将执行2个测试用例。前者将具有最小水位，因此可以接受更多迟到数据：
object FirstWatermark { var FirstKnownValue = &#34;&#34; } def launchDataInjection(mainEventsStream: MemoryStream[MainEvent], joinedEventsStream: MemoryStream[JoinedEvent], query: StreamingQuery): Unit = { new Thread(new Runnable() { override def run(): Unit = { val stateManagementHelper = new StateManagementHelper(mainEventsStream, joinedEventsStream) var key = 0 val processingTimeFrom1970 = 10000L // 10 sec stateManagementHelper.</p>

    </div>

    
    
    
        <div class="readmore no-text-decoration">
            <a href='https://ohmyweekly.github.io/notes/watermark-configuration/'
               title="Read more about Apache Spark 2.4.0 特性 - Watermark Configuration">
                read <span class="more"> more</span>
                <span class="screen-readers-only" > about Apache Spark 2.4.0 特性 - Watermark Configuration</span>
            </a>
        </div>
    
</article>
      
    
        <article class="summary h-entry notes">
    <header>
        <h2 class="post-title p-name no-text-decoration"><a class="u-url" href='https://ohmyweekly.github.io/notes/query-metrics-in-spark-structured-streaming/'> Spark Structured Streaming 中的查询指标</a> </h2>
        
    <div class="taxo no-text-decoration">
         
            
                <ul class="no-bullets inline categories">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts categorized in ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/categories/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
         
            
                <ul class="no-bullets inline tags">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts tagged with ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/tags/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
        
    </div>


        


    
    
    <div class="post-date">
        
        <time datetime="2019-10-03T13:12:56+0000" class="dt-published">Thu Oct 3, 2019</time>
        
        
    </div>


    </header>

    <div class="post p-summary" >
        <p>长查询的重点之一是跟踪。知道查询的执行方式总是很重要的。在结构化流中，由于有了称为 ProgressReporter 的特殊对象，我们可以追踪此执行。
在本文中，我们将重点介绍 ProgressReporter 对象收集的指标。在第一部分中，我们将解释其生命周期以及一些实现细节。下一部分将涵盖公开的信息，而最后一部分将通过一些测试展示 ProgressReporter 的行为。
ProgressReporter 首先，让我们定义这个著名的 ProgressReporter。它是 org.apache.spark.sql.execution.streaming 中的一个 trait，由 StreamExecution 抽象类直接继承，因此由其实现间接继承：org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution 和 org.apache.spark.sql.execution.streaming.MicroBatchExecution。 ProgressReporter 的作用是提供一个接口，该接口一旦实现，便可以自由用于报告有关流查询执行情况的统计信息。
ProgressReporter 定义了严格的生命周期阶段：
1、一切开始于流查询触发器（处理时间或事件时间）执行时。触发器要做的第一件事是对 ProgressReporter 的 startTrigger 方法的调用。此方法使报告程序准备好累积刚刚开始执行的统计信息。
2、稍后，根据选择的流模式（微批或连续），报告器将记录有关几个不同步骤执行情况的统计信息。下一部分将详细介绍这些步骤。为此，它使用了方法 reportTimeTaken[T](triggerDetailKey: String)(body: =&gt; T)，该方法将有关执行这些步骤的度量添加到 currentDurationsMs: mutable.HashMap[String，Long]` 字段中。
3、下一步是数据处理，报告者还将收集一些统计信息。
4、将这些统计信息添加到 currentDurationMs 映射后，如果执行微批处理，则 ProgressReporter 调用 finishTrigger(hasNewData: Boolean)。此方法完成触发器的执行，并创建保存执行统计信息的对象，这些统计信息放入 progressBuffer = new mutable.Queue[StreamingQueryProgress]() 中。之后，客户端可以通过公共访问器方法直接从那里检索更新（或最后一个更新）。
在 ProgressReporter 中，我们还可以找到其他一些指标，例如：
newData - 它是一个 Map[BaseStreamingSource, LogicalPlan]，其中包含每个源的最新数据。 availableOffsets - 这是一个类似于 map 的 StreamProgress 对象，用于存储未提交到接收器的可用于处理的偏移量。 commitOffsets - 类似于 availableOffset。不同之处在于，它存储已处理和已提交数据的偏移量。 currentBatchId - 当前处理批次的 ID。 currentStatus - org.</p>

    </div>

    
    
    
        <div class="readmore no-text-decoration">
            <a href='https://ohmyweekly.github.io/notes/query-metrics-in-spark-structured-streaming/'
               title="Read more about Spark Structured Streaming 中的查询指标">
                read <span class="more"> more</span>
                <span class="screen-readers-only" > about Spark Structured Streaming 中的查询指标</span>
            </a>
        </div>
    
</article>
      
    
        <article class="summary h-entry notes">
    <header>
        <h2 class="post-title p-name no-text-decoration"><a class="u-url" href='https://ohmyweekly.github.io/notes/stateful-aggregations-in-spark-structured-streaming/'> Spark Structured Streaming 中的状态聚合</a> </h2>
        
    <div class="taxo no-text-decoration">
         
            
                <ul class="no-bullets inline categories">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts categorized in ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/categories/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
         
            
                <ul class="no-bullets inline tags">
                    
                        
                        
                        
                            
                            
                            
                            
                            
                            <li class="__rakulang__"
                                
                                
                                title="See all 0 posts tagged with ‘rakulang’"
                                
                            >
                                <a class="p-category" href="https://ohmyweekly.github.io/tags/rakulang/">rakulang</a>
                            </li>
                        
                    
                </ul>
            
        
    </div>


        


    
    
    <div class="post-date">
        
        <time datetime="2019-10-02T22:32:41+0000" class="dt-published">Wed Oct 2, 2019</time>
        
        
    </div>


    </header>

    <div class="post p-summary" >
        <p>https://www.waitingforcode.com/apache-spark-structured-streaming/stateful-aggregations-apache-spark-structured-streaming/read
最近，我们发现了用于处理结构化流中状态聚合的状态存储的概念。但是那时我们还没有花时间在这些聚合上。按照承诺，现在将对其进行描述。 这篇文章开始于聚合与有状态聚合之间的简短比较。在本文结尾之后，直接进行了一些测试，这些测试说明了状态聚合。
聚合与有状态聚合 聚合是从多个值中计算单个值的操作。在 Apache Spark 中，这种操作的示例可以是 count 或 sum 方法。在流式应用程序上下文中，我们谈论有状态聚合，即具有这些值的状态的聚合在一段时间内逐渐增长。
在对结构化流中的输出模式，状态存储和触发器给出了解释之后，就更容易理解状态聚合的特殊性。如所告知的，输出模式不仅确定数据量，而且还决定是否可以丢弃中间状态（如果与水印一起使用的话）。然后，所有这些中间状态都保留在容错状态存储中。上一次触发器执行后，触发器会按固定的时间间隔执行状态计算，这些数据是从上一次触发器执行中累积的。以下架构显示了所有这些部分如何协同工作：
因此，为了简单起见，我们可以将状态聚合定义为结果随时间变化的聚合。 结果计算由触发器启动，并保存在状态存储中，在经过处理的数据通过水印之前（如果已定义），可以从状态存储中删除结果。
有状态聚合示例 两项测试显示了 Apache Spark 结构化流中的状态聚合：
&#34;stateful count aggregation&#34; should &#34;succeed after grouping by id&#34; in { val testKey = &#34;stateful-aggregation-count&#34; val inputStream = new MemoryStream[(Long, String)](1, sparkSession.sqlContext) val aggregatedStream = inputStream.toDS().toDF(&#34;id&#34;, &#34;name&#34;) .groupBy(&#34;id&#34;) .agg(count(&#34;*&#34;)) val query = aggregatedStream.writeStream.trigger(Trigger.ProcessingTime(1000)).outputMode(&#34;update&#34;) .foreach( new InMemoryStoreWriter[Row](testKey, (row) =&gt; s&#34;${row.getAs[Long](&#34;id&#34;)} -&gt; ${row.getAs[Long](&#34;count(1)&#34;)}&#34;)) .start() new Thread(new Runnable() { override def run(): Unit = { inputStream.</p>

    </div>

    
    
    
        <div class="readmore no-text-decoration">
            <a href='https://ohmyweekly.github.io/notes/stateful-aggregations-in-spark-structured-streaming/'
               title="Read more about Spark Structured Streaming 中的状态聚合">
                read <span class="more"> more</span>
                <span class="screen-readers-only" > about Spark Structured Streaming 中的状态聚合</span>
            </a>
        </div>
    
</article>
      
    
</div>



                <footer>
                    

<div class="pagination no-text-decoration">
    
        <a class="pagination-item newer" href="/page/26/">Newer</a>
    
    
        <a class="pagination-item older" href="/page/28/">Older</a>
    
</div>




                    <ul class="no-bullets feed right inline">
    
        
        
            <li class="inline no-text-decoration">
                <a href="https://ohmyweekly.github.io/index.xml">RSS</a>
            </li>
        
    
        
        
            <li class="inline no-text-decoration">
                <a href="https://ohmyweekly.github.io/atom.xml">ATOM</a>
            </li>
        
    
        
        
    
</ul>
<div class="clear-float"></div>

                </footer>
                <hr />
            </div>               

            <footer> 
                

<ul class="social no-text-decoration">
    
</ul>










 
    
    



<p class="generated no-text-decoration">
    Generated using  <a href="https://gitlab.com/kaushalmodi/hugo-theme-refined"><code class="nobr">hugo-theme-refined</code></a> + <span class="nobr">Hugo <a href="https://github.com/gohugoio/hugo/commit/312735366b20d64bd61bff8627f593749f86c964">0.123.7</a></span>
</p>

<p>
    
</p>




<div class="badges no-text-decoration">
    
    

    
</div>




<script type="application/javascript">var nav=responsiveNav("#nav");</script>




<script defer src="/js/libs/fragmentions/wrapper.min.e8c468c89edc4f5dccaa8c720c6b220b3088a16cd7b1e4a1e3345985788260c9.js"></script>









            </footer>
        </div> 
    </body>
</html>
